<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9897189</article-id>
    <article-id pub-id-type="pmid">36707990</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btad059</article-id>
    <article-id pub-id-type="publisher-id">btad059</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Paper</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Data and Text Mining</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Potent antibiotic design via guided search from antibacterial activity evaluations</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Chen</surname>
          <given-names>Lu</given-names>
        </name>
        <aff><institution>School of Computer Science and Technology, Xidian University</institution>, Xi’an 710071, Shaanxi, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-8351-3332</contrib-id>
        <name>
          <surname>Yu</surname>
          <given-names>Liang</given-names>
        </name>
        <xref rid="btad059-cor1" ref-type="corresp"/>
        <!--lyu@xidian.edu.cn-->
        <aff><institution>School of Computer Science and Technology, Xidian University</institution>, Xi’an 710071, Shaanxi, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-6396-0787</contrib-id>
        <name>
          <surname>Gao</surname>
          <given-names>Lin</given-names>
        </name>
        <aff><institution>School of Computer Science and Technology, Xidian University</institution>, Xi’an 710071, Shaanxi, <country country="CN">China</country></aff>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Wren</surname>
          <given-names>Jonathan</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btad059-cor1">To whom correspondence should be addressed. <email>lyu@xidian.edu.cn</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2023-01-27">
      <day>27</day>
      <month>1</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>27</day>
      <month>1</month>
      <year>2023</year>
    </pub-date>
    <volume>39</volume>
    <issue>2</issue>
    <elocation-id>btad059</elocation-id>
    <history>
      <date date-type="received">
        <day>26</day>
        <month>9</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>14</day>
        <month>1</month>
        <year>2023</year>
      </date>
      <date date-type="editorial-decision">
        <day>22</day>
        <month>1</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>25</day>
        <month>1</month>
        <year>2023</year>
      </date>
      <date date-type="corrected-typeset">
        <day>02</day>
        <month>2</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btad059.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>The emergence of drug-resistant bacteria makes the discovery of new antibiotics an urgent issue, but finding new molecules with the desired antibacterial activity is an extremely difficult task. To address this challenge, we established a framework, MDAGS (Molecular Design via Attribute-Guided Search), to optimize and generate potent antibiotic molecules.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>By designing the antibacterial activity latent space and guiding the optimization of functional compounds based on this space, the model MDAGS can generate novel compounds with desirable antibacterial activity without the need for extensive expensive and time-consuming evaluations. Compared with existing antibiotics, candidate antibacterial compounds generated by MDAGS always possessed significantly better antibacterial activity and ensured high similarity. Furthermore, although without explicit constraints on similarity to known antibiotics, these candidate antibacterial compounds all exhibited the highest structural similarity to antibiotics of expected function in the DrugBank database query. Overall, our approach provides a viable solution to the problem of bacterial drug resistance.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>Code of the model and datasets can be downloaded from GitHub (<ext-link xlink:href="https://github.com/LiangYu-Xidian/MDAGS" ext-link-type="uri">https://github.com/LiangYu-Xidian/MDAGS</ext-link>).</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Natural Science Foundation of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>62072353</award-id>
        <award-id>62132015</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="9"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Bacterial drug resistance has led to an increasing number of health, social and economic problems that are becoming more prominent globally (<xref rid="btad059-B54" ref-type="bibr">Sugden <italic toggle="yes">et al.</italic>, 2016</xref>). Studies have shown that by 2050, if the global problem of antimicrobial resistance (AMR) is not addressed, 10 million people will die from AMR annually (<xref rid="btad059-B34" ref-type="bibr">Mulpuru <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btad059-B36" ref-type="bibr">Nieuwlaat <italic toggle="yes">et al.</italic>, 2021</xref>). The development of new antibiotics is typically abandoned due to unresolved regulatory issues and lack of commercial appeal (<xref rid="btad059-B1" ref-type="bibr">Årdal <italic toggle="yes">et al.</italic>, 2020</xref>). Furthermore, industry experts have analyzed unsuccessful antibacterial programs and found that high-throughput screening of synthetic chemicals often fails to find promising compounds with expected properties (<xref rid="btad059-B5" ref-type="bibr">Brown and Wright, 2016</xref>). Therefore, identifying and developing novel antibiotic drugs remains very challenging (<xref rid="btad059-B18" ref-type="bibr">Ismail <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btad059-B28" ref-type="bibr">Li <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btad059-B33" ref-type="bibr">Miethke <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btad059-B35" ref-type="bibr">Murray <italic toggle="yes">et al.</italic>, 2022</xref>; <xref rid="btad059-B55" ref-type="bibr">Theuretzbacher <italic toggle="yes">et al.</italic>, 2020</xref>).</p>
    <p>In recent years, computational methods, especially deep learning, have emerged as a promising avenue for drug discovery by combining domain knowledge and data-driven learning (<xref rid="btad059-B7" ref-type="bibr">Button <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btad059-B8" ref-type="bibr">Camacho <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btad059-B16" ref-type="bibr">Hoffman <italic toggle="yes">et al.</italic>, 2022</xref>; <xref rid="btad059-B21" ref-type="bibr">Joshi <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btad059-B22" ref-type="bibr">Kotsias <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btad059-B26" ref-type="bibr">LeCun <italic toggle="yes">et al.</italic>, 2015</xref>; <xref rid="btad059-B30" ref-type="bibr">Lv <italic toggle="yes">et al.</italic>, 2022</xref>; <xref rid="btad059-B53" ref-type="bibr">Su <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btad059-B57" ref-type="bibr">von Lilienfeld and Burke, 2020</xref>). Among them, deep generative methods have been shown to be ideal for drug candidate discovery (<xref rid="btad059-B14" ref-type="bibr">Gómez-Bombarelli <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btad059-B42" ref-type="bibr">Putin <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btad059-B43" ref-type="bibr">Ru <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btad059-B45" ref-type="bibr">Sanchez-Lengeling and Aspuru-Guzik, 2018</xref>; <xref rid="btad059-B47" ref-type="bibr">Segler <italic toggle="yes">et al.</italic>, 2018b</xref>; <xref rid="btad059-B49" ref-type="bibr">Shaker <italic toggle="yes">et al.</italic>, 2021</xref>). In view of this, deep generative methods can be applied to generate ideal antibiotic molecules, thereby increasing the success rate of new antibiotic discovery and reducing the cost in the discovery process. To date, the related works of deep generative methods that have been reported in industry and academia (<xref rid="btad059-B10" ref-type="bibr">Dao <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btad059-B65" ref-type="bibr">Zulfiqar <italic toggle="yes">et al.</italic>, 2022</xref>) can be divided into generation-based and guide optimization-based, which are not mutually exclusive.</p>
    <p>In generation-based work, many models represent a molecule with the corresponding simplified molecular input line entry specification (SMILES) notation so that the molecule generation task is transformed into a sequence-to-sequence generation task (<xref rid="btad059-B4" ref-type="bibr">Born <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btad059-B23" ref-type="bibr">Krishnan <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btad059-B27" ref-type="bibr">Lee <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btad059-B37" ref-type="bibr">Olivecrona <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btad059-B39" ref-type="bibr">Popova <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btad059-B46" ref-type="bibr">Segler <italic toggle="yes">et al.</italic>, 2018a</xref>; <xref rid="btad059-B58" ref-type="bibr">Wang <italic toggle="yes">et al.</italic>, 2021</xref>). However, this approach often requires large-scale pretraining, and grammatical errors will often lead to the generation of invalid SMILES, which cannot be converted into realistic molecular structures. There are also models that are represented as molecular graphs so that the task of generating molecules is transformed into a graph-to-graph generation task. Graph-based generation methods are further divided into autoregressive(<xref rid="btad059-B29" ref-type="bibr">Li <italic toggle="yes">et al.</italic>, 2018</xref>) and non-autoregressive (<xref rid="btad059-B12" ref-type="bibr">De Cao and Kipf, 2018</xref>; <xref rid="btad059-B20" ref-type="bibr">Jin <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btad059-B25" ref-type="bibr">Kwon <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btad059-B31" ref-type="bibr">Maziarka <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btad059-B50" ref-type="bibr">Simonovsky and Komodakis, 2018</xref>) generation. Autoregressive generation generates one atom at a time, borrowing ideas from sequence generation, and similarly, this method of generation also leads to chemically invalid intermediate graphs. Non-autoregressive generation generates the entire graph at once; this method has also been proven to be feasible, but its disadvantage is that it requires an additional graph matching process (<xref rid="btad059-B25" ref-type="bibr">Kwon <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btad059-B50" ref-type="bibr">Simonovsky and Komodakis, 2018</xref>).</p>
    <p>In guided optimization-based work, the optimization can be in the generated molecular map or SMILES space or in the encoder–decoder latent space. For the former, the combination of a generative model and a property prediction model or scoring function for reinforcement learning is a common paradigm (<xref rid="btad059-B4" ref-type="bibr">Born <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btad059-B12" ref-type="bibr">De Cao and Kipf, 2018</xref>; <xref rid="btad059-B23" ref-type="bibr">Krishnan <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btad059-B27" ref-type="bibr">Lee <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btad059-B37" ref-type="bibr">Olivecrona <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btad059-B39" ref-type="bibr">Popova <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btad059-B46" ref-type="bibr">Segler <italic toggle="yes">et al.</italic>, 2018a</xref>; <xref rid="btad059-B58" ref-type="bibr">Wang <italic toggle="yes">et al.</italic>, 2021</xref>), but when the generative model is more complex, the generated results are poor since the prediction or scoring results cannot be efficiently fed back to the generative model (<xref rid="btad059-B58" ref-type="bibr">Wang <italic toggle="yes">et al.</italic>, 2021</xref>). For the latter, many models utilize different optimization strategies or sampling strategies for molecule generation (<xref rid="btad059-B7" ref-type="bibr">Button <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btad059-B9" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btad059-B11" ref-type="bibr">Das <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btad059-B13" ref-type="bibr">Godinez <italic toggle="yes">et al.</italic>, 2022</xref>; <xref rid="btad059-B15" ref-type="bibr">Griffiths and Hernández-Lobato, 2020</xref>; <xref rid="btad059-B16" ref-type="bibr">Hoffman <italic toggle="yes">et al.</italic>, 2022</xref>; <xref rid="btad059-B19" ref-type="bibr">Jiménez-Luna <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btad059-B51" ref-type="bibr">Skalic <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btad059-B63" ref-type="bibr">You <italic toggle="yes">et al.</italic>, 2018</xref>).</p>
    <p>In this article, we develop a novel generative method that combines attribute prediction models with an efficient guided search strategy to optimize and generate potent antibiotic molecules with expected antibacterial activity. Inspired by Graph2Seq models in the natural language domain (<xref rid="btad059-B56" ref-type="bibr">Tu and Coley, 2021</xref>; <xref rid="btad059-B60" ref-type="bibr">Xu <italic toggle="yes">et al.</italic>, 2018</xref>), we employ graph-to-sequence generation to exploit the advantages and avoid the disadvantages of existing sequence-to-sequence and graph-to-graph generation methods. We also decouple the encoder and decoder to reduce the complexity of the model. Furthermore, the encoder and attribute prediction model (<xref rid="btad059-B61" ref-type="bibr">Yang <italic toggle="yes">et al.</italic>, 2019</xref>) are jointly trained to learn a latent attribute space—the antibacterial activity space. In this latent space, a guided search based on attribute evaluation (<xref rid="btad059-B62" ref-type="bibr">Yang <italic toggle="yes">et al.</italic>, 2020</xref>) is used to guide the exploration of candidate antibiotic molecules in a meaningful direction (enhancing growth inhibition). We applied this method to generate antibiotic molecules with enhanced antibacterial activity compared to existing antibiotics, and these generated molecules achieved the greatest similarity to the expected functional antibiotic, although we did not explicitly constrain the similarity. Our approach combined a designed antibacterial activity space with guided searches to produce antibiotic molecules with enhanced inhibition for the first time. We believe our work will contribute to the development of new antibiotics.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Data sources</title>
      <sec>
        <title>2.1.1 Drugs and natural products</title>
        <p>We obtained the antibiotic and natural product dataset from <xref rid="btad059-B52" ref-type="bibr">Stokes <italic toggle="yes">et al.</italic> (2020)</xref> and obtained a primary training set containing 2560 compounds, of which 1760 compounds were screened for growth inhibition against <italic toggle="yes">Escherichia coli BW25113</italic> (<xref rid="btad059-B64" ref-type="bibr">Zampieri <italic toggle="yes">et al.</italic>, 2017</xref>) using a widely available US Food and Drug Administration-approved drug library. Another 800 are natural products isolated from plant, animal and microbial sources. After deduplication, 2334 compounds remained. For convenience, we call these 2334 compounds the main dataset. The mean inhibitory concentration—Mean_Inhibition (averaged over two rounds of experiments)—is a measure of the compound’s growth-inhibitory effect on <italic toggle="yes">E.coli BW25113</italic>. The distribution of growth inhibition values in the main dataset is shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S1</xref>.</p>
      </sec>
      <sec>
        <title>2.1.2 MOSES dataset</title>
        <p>The MOSES set is based on the ZINC (<xref rid="btad059-B17" ref-type="bibr">Irwin and Shoichet, 2005</xref>) Clean Leads collection. It contains a total of 4 591 276 molecules, filtered by molecular weight (MW) in the range 250–350 Da, with no more than seven rotatable bonds and XlogP≤3.5. The set excludes molecules containing charged atoms or atoms other than C, N, S, O, F, Cl, Br, H or cycles of more than eight atoms. The molecules were filtered via two filters—medicinal chemistry filters and PAINS filters (<xref rid="btad059-B38" ref-type="bibr">Polykovskiy <italic toggle="yes">et al.</italic>, 2020</xref>). The dataset contains 1 936 962 molecular structures. For experiments, the dataset is divided into training, test and scaffold test sets containing ∼1.6 M, 176 k and 176 k molecules, respectively.</p>
      </sec>
      <sec>
        <title>2.1.3 GuacaMol dataset</title>
        <p>The GuacaMol dataset is derived from the ChEMBL 24 database (<xref rid="btad059-B32" ref-type="bibr">Mendez <italic toggle="yes">et al.</italic>, 2019</xref>). The advantage of ChEMBL is that it contains only molecules that have been synthesized and tested against biological targets (<xref rid="btad059-B6" ref-type="bibr">Brown <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btad059-B44" ref-type="bibr">Ru <italic toggle="yes">et al.</italic>, 2022</xref>). The dataset is divided into training, validation and test sets containing ∼1.3 M, 80 k and 240 k molecules, respectively.</p>
      </sec>
    </sec>
    <sec>
      <title>2.2 Our MDAGS model</title>
      <p>The overall framework of the MDAGS model is shown in <xref rid="btad059-F1" ref-type="fig">Figure 1</xref>.</p>
      <fig position="float" id="btad059-F1">
        <label>Fig. 1.</label>
        <caption>
          <p>Overview of MDAGS. (<bold>A</bold>) The encoder and the predictor are jointly trained to obtain the latent vector representation of the input molecular graph and the predicted growth inhibition value, respectively, and the combination of the two constructs the potential growth inhibition space. The generator generates SMILES corresponding to the latent space optimization result after being pretrained with a large number of unlabeled molecules. (<bold>B</bold>) Latent space optimization consists of two parts: (i) sampling and (ii) updating. A given seed molecule is sampled and the sampled molecule is updated to find the molecule with the global minimum growth inhibition. Molecules satisfying the growth inhibition threshold during sampling and updating are considered successful molecules and are generated</p>
        </caption>
        <graphic xlink:href="btad059f1" position="float"/>
      </fig>
      <sec>
        <title>2.2.1 Architecture of the generative model</title>
        <p><bold>Encoder–predictor for learning inhibition space.</bold> We apply the directed message-passing neural network Chemprop (<xref rid="btad059-B61" ref-type="bibr">Yang <italic toggle="yes">et al.</italic>, 2019</xref>) to learn the inhibition space. The model consists of two parts: an encoder and a predictor. The encoder uses a directed message-passing mechanism to convert the molecular graph into a continuous latent space vector. The atomic features and edge features corresponding to the molecular graph are listed in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref>. By giving directions to the edges of the molecular graph and updating iterative messages based on the edges, unnecessary loops and jitter are avoided. During each iteration, the molecular graph substructure information is passed through the edges, and the node features are updated with the edge features (sum of the bond features). After multiple rounds of iterations, both edge and node features contain larger molecular graph substructure information and even global information, and the node feature aggregation (sum of the atom hidden states) can obtain the representation of the molecular graph. The equations for message passing and updating are:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mo>α</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E2"><label>(2)</label><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>\</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E3"><label>(3)</label><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>α</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E4"><label>(4)</label><mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>a</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E5"><label>(5)</label><mml:math id="M5" display="block" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>a</mml:mi><mml:mrow/></mml:msubsup><mml:mo>=</mml:mo><mml:mo>α</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>a</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>where <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> represents hidden states and <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> represents messages, <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>a</mml:mi><mml:mrow/></mml:msubsup></mml:mrow></mml:math></inline-formula> represents the atom features and <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mrow/></mml:msubsup></mml:mrow></mml:math></inline-formula> represents the bond features, <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow/></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> are learned matrixes, <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mo>α</mml:mo></mml:math></inline-formula> is the activation function ReLU, <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>⋅</mml:mo><mml:mo>,</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> is concatenation and <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the length of the message-passing step. And the readout phase operates according to
<disp-formula id="E6"><label>(6)</label><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>a</mml:mi><mml:mrow/></mml:msubsup></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>The predictor is a feed-forward network (FFN) that predicts the growth inhibition value corresponding to the latent space vector
<disp-formula id="E7"><label>(7)</label><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>A latent property (growth inhibition) space of molecular graphs is learned by joint training of the encoder and predictor and optimized based on this space.</p>
        <p><bold>Generator.</bold> Inspired by the work of <xref rid="btad059-B2" ref-type="bibr">Bagal <italic toggle="yes">et al.</italic> (2022)</xref>, we train a GPT model as the generator to generate molecular SMILES sequences corresponding to the latent space optimization results (see Section 2.2.2 for the optimization procedure). The model utilizes a masked self-attention mechanism and consists of multiple stacked decoding blocks. Each stacked decoding block consists of a self-attention layer and fully connected layers. The long-range dependencies of sequences are modeled through a self-attention module. The implementation of the self-attention mechanism and the fully connected layer for each decoding block can be expressed by the following formulas:
<disp-formula id="E8"><label>(8)</label><mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">atten</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="italic">softmax</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:msup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo stretchy="true">)</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E9"><label>(9)</label><mml:math id="M9" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">hea</mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="italic">atten</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Q</mml:mi><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>Q</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>K</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>V</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E10"><label>(10)</label><mml:math id="M10" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">MultiHead</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="italic">hea</mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="italic">hea</mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E11"><label>(11)</label><mml:math id="M11" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">FNN</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>σ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>where <italic toggle="yes">Q</italic> is query, <italic toggle="yes">K</italic> is key and <italic toggle="yes">V</italic> is value vectors, respectively. <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the dimension of query and key vectors, and <italic toggle="yes">T</italic> is transpose of the matrix. <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">hea</mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the <italic toggle="yes">i</italic>-th attentional head, <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>⋅</mml:mo><mml:mo>,</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> is concatenation and all of the <italic toggle="yes">W</italic>'s above are learnable matrices.<inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mo>σ</mml:mo></mml:math></inline-formula> is the activation function GeLU and <italic toggle="yes">b</italic> is bias.</p>
        <p>For training, we feed the molecular graph latent vector and the molecular SMILES into the model, and the model is then trained on the next character prediction task so that the latent vector guides the model generation. The generator is trained to maximize the conditional likelihood:
<disp-formula id="E12"><label>(12)</label><mml:math id="M12" display="block" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">out</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mtext>latent</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo> </mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mo>.</mml:mo><mml:mo> </mml:mo><mml:mo>.</mml:mo><mml:mo> </mml:mo><mml:mo>.</mml:mo><mml:mo> </mml:mo><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mtext>latent</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mtext>latent</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
        <p>We feed the latent vector into the model, and given a start marker, the model predicts the next marker in the sequence until a stop symbol is generated, thus generating a molecular SMILES.</p>
        <p>Since the latent representation of the molecular graph and the input dimension of the decoding block are inconsistent, we add a fully connected layer between the two to map the latent space vector representation to the corresponding dimension vector of the decoding block.</p>
      </sec>
      <sec>
        <title>2.2.2 Guided search from predicted inhibition</title>
        <p>Although the latent space is already dimensionally reduced relative to the broad drug space, it is still a high-dimensional space. With high-dimensional data, it is difficult to mitigate the impact of dimensional disaster and to continue to explore in the desired direction of growth inhibition in the process of optimization. Inspired by the work of <xref rid="btad059-B62" ref-type="bibr">Yang <italic toggle="yes">et al.</italic> (2020)</xref>, we employ stochastic dominant learning swarm optimization strategy in the latent space. We define a particle as a point in latent space and the fitness as the growth inhibition corresponding to that point. For a seed molecule, the initial population is obtained by sampling around the potential representation, and the population is updated in the direction of minimizing growth inhibition under the guidance of fitness until a point with the minimum global growth inhibition is found. During the update process, points that satisfy the threshold range for growth inhibition are saved and generated (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Algorithm S1</xref> for the detailed process). The optimization mainly includes two parts: sampling and updating.</p>
        <p><bold>Sampling.</bold> If the similarity between the sampling point and the seed is too high, the latent space search is inefficient; if the similarity is too low, the sampling point will not be able to keep the characteristics of the seed. We design the following sampling strategy: for a given point, we randomly sample number of populations (<inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mrow><mml:mi>N</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula>) (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Algorithm S1</xref>) Gaussian noises <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>P</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is calculated as follows: the encoder is used to obtain the latent representation of the main dataset and compute the variance <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi>cov</mml:mi></mml:mrow></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi>cov</mml:mi></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi>cov</mml:mi></mml:mrow></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> for each dimension, where <italic toggle="yes">n</italic> is the dimension of the latent vector. This yields the following:
<disp-formula id="E13"><label>(13)</label><mml:math id="M13" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>max</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi>cov</mml:mi></mml:mrow></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi>cov</mml:mi></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi>cov</mml:mi></mml:mrow></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E14"><label>(14)</label><mml:math id="M14" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E15"><label>(15)</label><mml:math id="M15" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>×</mml:mo><mml:mi>I</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic toggle="yes">I</italic> is an identity matrix and <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> represents the latent space representation of the seed. Gaussian noises are then added to the latent space representation vector of the seed:
<disp-formula id="E16"><label>(16)</label><mml:math id="M16" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mo>θ</mml:mo><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2…</mml:mn><mml:mi>N</mml:mi><mml:mi>P</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>We set the parameter <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mo>θ</mml:mo></mml:math></inline-formula> (<inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mrow><mml:mo>θ</mml:mo><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>) to make the noise range controllable. <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>P</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> form the initial population (see <xref rid="btad059-F1" ref-type="fig">Fig. 1B</xref>).</p>
        <p><bold>Updating.</bold> For a given point <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the update is done as follows:
<disp-formula id="E17"><label>(17)</label><mml:math id="M17" display="block" overflow="scroll"><mml:mrow><mml:mo>β</mml:mo><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mo>−</mml:mo><mml:mn>0.2</mml:mn><mml:mo>×</mml:mo><mml:mtext>exp</mml:mtext><mml:mo>×</mml:mo><mml:mo stretchy="true">(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo>ξ</mml:mo></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo>ξ</mml:mo></mml:mrow></mml:mfrac><mml:mo stretchy="true">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E18"><label>(18)</label><mml:math id="M18" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>w</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo>β</mml:mo><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E19"><label>(19)</label><mml:math id="M19" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>g</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is a function to calculate individual fitness. We design two <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>g</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> functions: one is a jointly trained FFN predictor, and the other is a Gaussian process (GP) model trained using the training set of the main dataset; both models are used to achieve inhibition consistency. The GP model is a non-parametric model (<xref rid="btad059-B59" ref-type="bibr">Williams and Rasmussen, 2006</xref>). It is a very lightweight model that takes only a few minutes to train. <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mo>ξ</mml:mo></mml:math></inline-formula> is a very small number to avoid the denominator being zero. <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mrow><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are hyperparameters. It is worth noting that only one fitness function is required in a complete scheme. Designing two and doing experiments is to consider both neural network parametric models (FFN) and non-parametric models (GP), which is an extension of the entire experimental scheme.</p>
      </sec>
    </sec>
    <sec>
      <title>2.3 Experimental settings</title>
      <p>The experimental settings for MDAGS model could be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref>.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 Encoder–predictor joint training analysis</title>
      <sec>
        <title>3.1.1 Choice of hyperparameter</title>
        <p>In machine learning, especially in the field of deep learning, the choice of hyperparameters has a great impact on the performance of the model. Therefore, in our work, the joint model is also optimized with hyperparameters before jointly training the encoder and predictor. We apply Bayesian optimization (BO) to perform this process (Shahriari <italic toggle="yes">et al.</italic>, 2016). The main idea of BO is that the posterior distribution of a given complex objective function of optimization is computed with a GP. The objective function can have just the input and output specified with unknown internal structure and mathematical properties. The posterior distribution of the objective function is modified until the posterior distribution reasonably fits the real objective distribution so that the current parameters can be better adjusted. In our experiments, the optimized hyperparameters are the number of message-passing steps, neural network hidden size, number of feed-forward layers and dropout probability. We performed 20 optimizations on the training set of the main dataset and used the validation set to select the best hyperparameter combination. The result of hyperparameter optimization is as follows: the number of message-passing steps is 5, the neural network hidden size is 1900, the number of feed-forward layers is 2 and the dropout probability is 0.30000000000000004.</p>
      </sec>
      <sec>
        <title>3.1.2 Learned latent space visualization</title>
        <p>The generation and optimization of molecules is often concerned with maximizing or minimizing certain properties. In this article, we jointly train an encoder and a predictor to learn the growth-inhibitory property of molecules. In this way, the distribution of molecules in the latent space is organized by growth inhibition.</p>
        <p>We apply the results of hyperparameter optimization to set up the model and train the model to generate the latent space with the main dataset. We analyzed the latent space’s ability to capture molecular features. <xref rid="btad059-F2" ref-type="fig">Figure 2</xref> shows the mapping of attribute values to latent space attribute values, visualized using t-SNE. From <xref rid="btad059-F2" ref-type="fig">Figure 2A</xref>, it can be seen that the molecular representations learned by joint training of the encoder–predictor have a more concentrated distribution for the molecules with obvious growth inhibition, and the molecules with low values are almost all clustered directly below the latent space. To compare with this distribution, we chose the Morgan fingerprint, which is one of the classic fingerprints for feature vectorization of molecules, to compute the fingerprint of the main dataset. We performed the same visualization and found that the calculated molecular latent characterization does not possess the ability to aggregate molecular inhibition (<xref rid="btad059-F2" ref-type="fig">Fig. 2B</xref>).</p>
        <fig position="float" id="btad059-F2">
          <label>Fig. 2.</label>
          <caption>
            <p>Comparison of the latent space learned by the encoder–predictor and the latent space corresponding to the Morgan fingerprint. (<bold>A</bold>) Learned by the encoder–predictor. (<bold>B</bold>) Calculated using Morgan fingerprints. The value of the color bar represents the strength of antibacterial activity, the stronger the antibacterial activity, the smaller the value</p>
          </caption>
          <graphic xlink:href="btad059f2" position="float"/>
        </fig>
      </sec>
    </sec>
    <sec>
      <title>3.2 Latent space optimization analysis</title>
      <sec>
        <title>3.2.1 Fitness function evaluation</title>
        <p>We analyzed the predictive performance of the two designed fitness functions GP and FFN to ensure that the two fitness functions supported the prediction of growth inhibition (<xref rid="btad059-T1" ref-type="table">Table 1</xref>). The experimental growth inhibition <italic toggle="yes">R</italic><sup>2</sup> (correlation values) of the two fitness functions were 0.421 and 0.523, the root mean square error (RMSE) were 0.197 and 0.179 and the mean absolute error (MAE) were 0.130 and 0.118, respectively. It can be seen from this table that the performance of the two fitness functions is similar in these indicators, and both have good predictive performance. Therefore, it can be determined that the two fitness functions support the prediction of growth inhibition and can be used for the subsequent potential space-guided search optimization.</p>
        <table-wrap position="float" id="btad059-T1">
          <label>Table 1.</label>
          <caption>
            <p>Performance of fitness functions GP and FFN</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Fitness function type</th>
                <th rowspan="1" colspan="1">MAE</th>
                <th rowspan="1" colspan="1">RMSE</th>
                <th rowspan="1" colspan="1">
                  <italic toggle="yes">R</italic>
                  <sup>2</sup>
                </th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">GP</td>
                <td rowspan="1" colspan="1">0.130</td>
                <td rowspan="1" colspan="1">0.197</td>
                <td rowspan="1" colspan="1">0.421</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">FFN</td>
                <td rowspan="1" colspan="1">0.118</td>
                <td rowspan="1" colspan="1">0.179</td>
                <td rowspan="1" colspan="1">0.523</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
      <sec>
        <title>3.2.2 Optimization results analysis for selected seeds</title>
        <p>In the latent space, we selected four compound molecules from the test set of the main dataset as seeds for optimization: <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">cefazolin</mml:mi><mml:mo> </mml:mo><mml:mi mathvariant="italic">sodium</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">cefotaxime</mml:mi><mml:mo> </mml:mo><mml:mi mathvariant="italic">sodium</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">ceftazidime</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">minocycline</mml:mi><mml:mo> </mml:mo><mml:mi mathvariant="italic">hydrochloride</mml:mi></mml:mrow></mml:math></inline-formula>. The first three of these four compounds are cephalosporin antibiotics and the fourth is a tetracycline antibiotic. Among them, Compounds 1 and 3 have broad-spectrum antibacterial ability and Compounds 2 and 4 are active against gram-positive and gram-negative bacteria.</p>
        <p>For the four selected seed compounds, predicted growth inhibition was calculated with GP and FFN and compared with the actual growth inhibition (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref>). The prediction results of the two functions are slightly different. In addition, we counted the number of potential threshold range representations corresponding to the four seeds generated by the GP and FFN fitness function optimization (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S3</xref>). The FFN fitness function has more latent space representations than the GP optimization. We believe that this is related not only to setting different thresholds but also to the prediction accuracy of the two fitness functions. Exploring more successful latent space representations may mean exploring a wider space. <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S2</xref> verifies our conjecture. The latent representations optimized with the GP fitness function are more evenly spread around the seeds, but the latent representations optimized with the FFN fitness function have been further explored in a certain direction.</p>
        <p>Our goal is to find new molecules with stronger growth inhibition. We plotted the inhibition values corresponding to the main dataset and the latent representations optimized by the GP and FFN fitness functions (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S3</xref>). It can be seen from the figure that the predicted inhibition value of the latent representation obtained by the optimization of the two fitness functions is significantly lower than that of the main dataset.</p>
      </sec>
    </sec>
    <sec>
      <title>3.3 Generation analysis</title>
      <sec>
        <title>3.3.1 Generation performance comparison</title>
        <p>It is important for the generative model to generate chemically valuable, novel and unique molecules. This ensures that the generator has the ability to broadly explore new spaces beyond the existing chemical space without overfitting existing data. Therefore, we evaluate the generative ability of the model on multiple metrics and compare it with two benchmarks—MOSES and GuacaMol. It is worth noting that although we use the MolGPT framework in this article, our generation idea and method differ from the original. We believe these have an impact on the generative power of the model. Therefore, we also compared with the MolGPT model.</p>
        <p>On the MOSES benchmark, we compare our model with five benchmarks, CharRNN, AAE, VAE, JTN-VAE and LatentGAN, in addition to the MolGPT model (<xref rid="btad059-T2" ref-type="table">Table 2</xref>). In addition to validity, novelty and uniqueness, evaluation metrics include filters and internal diversity. Filters are mainly used to evaluate the proportion of filtered generated molecules applied during dataset construction. The resulting molecules, while chemically valid, may contain unnecessary fragments. Therefore, when building the MOSES training dataset, molecules with these fragments were removed, and the model is expected to avoid producing them (<xref rid="btad059-B38" ref-type="bibr">Polykovskiy <italic toggle="yes">et al.</italic>, 2020</xref>). Internal diversity assesses the chemical diversity within the generated set of molecules to avoid model collapse (models tend to produce several fixed types of data) (<xref rid="btad059-B3" ref-type="bibr">Benhenda, 2017</xref>). The results showed that our model significantly outperformed the other models on the novelty metric, suggesting that the model can generate new molecules that are completely different from the training set. In addition, it also has comparable performance to the baseline model on the other four metrics.</p>
        <table-wrap position="float" id="btad059-T2">
          <label>Table 2.</label>
          <caption>
            <p>Performance comparison on the MOSES benchmark</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Model</th>
                <th rowspan="1" colspan="1">Validity (↑)</th>
                <th rowspan="1" colspan="1">Uniqueness@10k (↑)</th>
                <th rowspan="1" colspan="1">Novelty (↑)</th>
                <th rowspan="1" colspan="1">Filters (↑)</th>
                <th rowspan="1" colspan="1">IntDiv (↑)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">CharRNN</td>
                <td rowspan="1" colspan="1">0.9748</td>
                <td rowspan="1" colspan="1">
                  <bold>0.9994</bold>
                </td>
                <td rowspan="1" colspan="1">0.8419</td>
                <td rowspan="1" colspan="1">0.9943</td>
                <td rowspan="1" colspan="1">0.8562</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">AAE</td>
                <td rowspan="1" colspan="1">0.9368</td>
                <td rowspan="1" colspan="1">0.9973</td>
                <td rowspan="1" colspan="1">0.7931</td>
                <td rowspan="1" colspan="1">0.9960</td>
                <td rowspan="1" colspan="1">0.8557</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">VAE</td>
                <td rowspan="1" colspan="1">0.9767</td>
                <td rowspan="1" colspan="1">0.9984</td>
                <td rowspan="1" colspan="1">0.6949</td>
                <td rowspan="1" colspan="1">0.9970</td>
                <td rowspan="1" colspan="1">
                  <bold>0.8558</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">JTN-VAE</td>
                <td rowspan="1" colspan="1">
                  <bold>1.0000</bold>
                </td>
                <td rowspan="1" colspan="1">0.9996</td>
                <td rowspan="1" colspan="1">0.9143</td>
                <td rowspan="1" colspan="1">0.9760</td>
                <td rowspan="1" colspan="1">0.8551</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">LatentGAN</td>
                <td rowspan="1" colspan="1">0.8966</td>
                <td rowspan="1" colspan="1">0.9968</td>
                <td rowspan="1" colspan="1">0.9498</td>
                <td rowspan="1" colspan="1">0.9735</td>
                <td rowspan="1" colspan="1">0.8565</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">MolGPT</td>
                <td rowspan="1" colspan="1">0.9900</td>
                <td rowspan="1" colspan="1">0.9986</td>
                <td rowspan="1" colspan="1">0.8093</td>
                <td rowspan="1" colspan="1">
                  <bold>0.9972</bold>
                </td>
                <td rowspan="1" colspan="1">0.8526</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">MDAGS</td>
                <td rowspan="1" colspan="1">0.9556</td>
                <td rowspan="1" colspan="1">0.9984</td>
                <td rowspan="1" colspan="1">
                  <bold>0.9925</bold>
                </td>
                <td rowspan="1" colspan="1">0.9948</td>
                <td rowspan="1" colspan="1">0.8534</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn1">
              <p>The bold value in each column corresponds to the optimal value in all comparison models under this indicator.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <p>On the GuacaMol benchmark, we compare our model with four benchmarks, Graph MCTS, AAE, ORGAN and VAE, in addition to the MolGPT model (<xref rid="btad059-T3" ref-type="table">Table 3</xref>). In addition to validity, novelty and uniqueness, the evaluation metrics also include KL divergence and FCD metrics. KL divergence measures how well one probability distribution approximates another (<xref rid="btad059-B24" ref-type="bibr">Kullback and Leibler, 1951</xref>). This metric reflects the ability of the model to fit the distribution of the data. FCD is a measure of how close the distribution of the generated data is to the distribution of molecules in the training set (<xref rid="btad059-B40" ref-type="bibr">Preuer <italic toggle="yes">et al.</italic>, 2018</xref>). This metric reflects the model’s ability to incorporate important chemical and biological features. The results show that our model has the best performance on uniqueness, KL divergence and FCD. This shows that the model can generate novel new molecules while fitting the distribution of the dataset.</p>
        <table-wrap position="float" id="btad059-T3">
          <label>Table 3.</label>
          <caption>
            <p>Performance comparison on the GuacaMol benchmark</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Model</th>
                <th rowspan="1" colspan="1">Graph MCTS</th>
                <th rowspan="1" colspan="1">AAE</th>
                <th rowspan="1" colspan="1">ORGAN</th>
                <th rowspan="1" colspan="1">VAE</th>
                <th rowspan="1" colspan="1">MolGPT</th>
                <th rowspan="1" colspan="1">MDAGS</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">Validity</td>
                <td rowspan="1" colspan="1">
                  <bold>1.000</bold>
                </td>
                <td rowspan="1" colspan="1">0.822</td>
                <td rowspan="1" colspan="1">0.379</td>
                <td rowspan="1" colspan="1">0.870</td>
                <td rowspan="1" colspan="1">0.970</td>
                <td rowspan="1" colspan="1">0.929</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Uniqueness</td>
                <td rowspan="1" colspan="1">
                  <bold>1.000</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>1.000</bold>
                </td>
                <td rowspan="1" colspan="1">0.841</td>
                <td rowspan="1" colspan="1">0.999</td>
                <td rowspan="1" colspan="1">0.998</td>
                <td rowspan="1" colspan="1">
                  <bold>1.000</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Novelty</td>
                <td rowspan="1" colspan="1">0.994</td>
                <td rowspan="1" colspan="1">0.998</td>
                <td rowspan="1" colspan="1">0.687</td>
                <td rowspan="1" colspan="1">0.974</td>
                <td rowspan="1" colspan="1">
                  <bold>1.000</bold>
                </td>
                <td rowspan="1" colspan="1">0.984</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">KLD</td>
                <td rowspan="1" colspan="1">0.522</td>
                <td rowspan="1" colspan="1">0.886</td>
                <td rowspan="1" colspan="1">0.267</td>
                <td rowspan="1" colspan="1">
                  <bold>0.982</bold>
                </td>
                <td rowspan="1" colspan="1">0.981</td>
                <td rowspan="1" colspan="1">
                  <bold>0.982</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">FCD</td>
                <td rowspan="1" colspan="1">0.015</td>
                <td rowspan="1" colspan="1">0.529</td>
                <td rowspan="1" colspan="1">0.000</td>
                <td rowspan="1" colspan="1">0.863</td>
                <td rowspan="1" colspan="1">0.736</td>
                <td rowspan="1" colspan="1">
                  <bold>0.874</bold>
                </td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn2">
              <p>The bold value in each row corresponds to the optimal value in all comparison models under this indicator.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
      </sec>
      <sec>
        <title>3.3.2 The impact of transfer learning</title>
        <p>For language generation models, it is necessary to train on a large dataset to learn grammar rules. Considering the small number of main datasets, direct training cannot learn SMILES grammar rules well, which affects the generation effect. We chose to learn grammar rules on two large datasets, MOSES and GuacaMol, and then use the main dataset for fine-tuning to generate antibiotic-specific compounds.</p>
        <p>We investigated whether generators benefit from transfer learning. We plotted the loss curves of the model trained directly and the models fine-tuned after pretraining on the MOSES and GuacaMol datasets, and then evaluated each model’s ability to fit the data (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S4</xref>). In addition, the test set of the main dataset was generated with three trained generators after removing the duplicate data in the MOSES and GuacaMol datasets, and the ability of the generators to regenerate was evaluated (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S4</xref>). The results show that the model trained directly on the master dataset performs less well than the pretrained and fine-tuned models. Additionally, we found that the model pretrained and fine-tuned on the GuacaMol dataset outperformed its MOSES dataset counterpart. We speculate that this may because the GuacaMol dataset has a more similar distribution to the main dataset. <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S5</xref> verifies our estimate. Considering the excellent performance of the model on the GuacaMol-pretrained dataset, we use this model in subsequent generation tasks.</p>
      </sec>
    </sec>
    <sec>
      <title>3.4 Generated results verification</title>
      <sec>
        <title>3.4.1 Generative spaces analysis</title>
        <p>We generated 7496 and 11 924 chemically usable molecules with the trained generators for GP and FFN, respectively, as successful latent representations of fitness optimization. We explored the spatial intersection generated by the two methods (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S6a</xref>); there were 476 repeated molecules explored in the two methods, and 7020 and 11 448 different molecules were generated by each exploration, respectively. The two spanning spaces are more intuitively displayed using TMAP (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S6b</xref>), which calculates the similarity based on ECFP fingerprints to construct a minimum spanning tree. The tree is drawn and displayed by Fearun, and a point in the figure represents a molecule (<xref rid="btad059-B41" ref-type="bibr">Probst and Reymond, 2018</xref>). Since the two modalities explore different chemical spaces, further analysis of both chemical spaces is warranted.</p>
        <p>We further explored the property distribution of the molecules in the two generative spaces with the main dataset (<xref rid="btad059-F3" ref-type="fig">Fig. 3</xref>). The distribution of the two generated datasets is almost the same as that of the main datasets of several important attributes [MW, LogP, number of hydrogen bond donors (HBD), number of hydrogen bond acceptors (HBA), topological polar surface area (TPSA) and synthetic accessibility (SAS)] on the whole, but the individual properties are slightly different: for MW, LogP and HBD, the generated molecules are consistent with the main datasets. However, half of the generated molecules do not conform to the principle of HBA &lt; 10 in the five principles of drug like. And the TPSA of the resulting molecule is more difficult to penetrate the cell membrane than the dataset. The SAS distribution of the generated molecules is more like 10, which means that the generated molecules are relatively difficult to synthesize compared to the dataset. For the performance on HBA, TPSA and SAS, we considered that the generated molecules were affected by the properties of the corresponding seeds.</p>
        <fig position="float" id="btad059-F3">
          <label>Fig. 3.</label>
          <caption>
            <p>Probability distributions of molecular properties for the two generated spaces compared to the main dataset. Calculated property distributions: MW, LogP, HBD, HBA, TPSA and SAS. Lines represent kernel density estimates for individual attributes of the main dataset and the two generated datasets</p>
          </caption>
          <graphic xlink:href="btad059f3" position="float"/>
        </fig>
      </sec>
      <sec>
        <title>3.4.2 Generated potent antibiotic verification</title>
        <p>In this section, we further verify the properties of the generated molecules. For the two generative spaces, we removed the molecules with a similarity &gt;0.80 from the generation results of each seed and then sorted the molecules in the order of Tanimoto similarity from high to low and inhibition value from small to large. The compounds at the resulting first position were further analyzed (Columns 2 and 4 of <xref rid="btad059-F4" ref-type="fig">Fig. 4</xref>). These eight compounds obtained stronger growth inhibition ability (lower PI value) under the condition of ensuring higher similarity with seeds. In other words, these molecules have the potential to have stronger antibacterial activity while their other properties remain similar to those of existing antibiotics. The third and fifth columns of <xref rid="btad059-F4" ref-type="fig">Figure 4</xref> show the similarity maps of the eight molecules and the corresponding seeds so that similar substructures between them can be visually observed. The SMILES corresponding to the seeds in <xref rid="btad059-F4" ref-type="fig">Figure 4</xref> and the eight sampled seeds are shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S5</xref>. We also calculated several properties of the seed molecule and the eight sampled compounds—MW, Log P, HBD, HBA, TPSA and SAS (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S6</xref>)—and found that the eight sampled molecules and the corresponding seed molecules have very similar properties.</p>
        <fig position="float" id="btad059-F4">
          <label>Fig. 4.</label>
          <caption>
            <p>Sample results for potent antibiotic generation. Each row corresponds to the result of a seed. The molecules in the first column are the seeds, and MI is the Mean_Inhibition value of the experiment. The molecule in the second column is the molecule obtained by the GP fitness function and the molecule in the fourth column is the molecule obtained by the FFN fitness function. <italic toggle="yes">S</italic> is the Tanimoto similarity between the molecule and the corresponding seed, QED is the drug-like property of the molecule and PI is the predicted inhibition value. The third and fifth columns are the similarity maps between the molecules in the second and fourth columns and the corresponding seeds. The darker the color is, the more similar the substructure of the molecule and the seed is. The first column in bold is the name of each antibiotic molecule</p>
          </caption>
          <graphic xlink:href="btad059f4" position="float"/>
        </fig>
        <p>In addition, we queried the top three nearest neighbors of these eight compounds in DrugBank (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S7</xref>). The first three nearest neighbors of these eight compounds are almost all antibiotics, and the functions of the three nearest neighbors are almost the same as the corresponding seed of the compound [e.g. the first compound was optimized by the first seed (cefazolin, a cephalosporin antibiotic), and its three nearest neighbors are also cephalosporin antibiotics, which is consistent with the antibiotic function in the seeds]. This further demonstrates the bacteriostatic potential of these eight sampled molecules.</p>
        <p>We also calculated the Tanimoto similarity between these eight compounds and the training set of the main dataset (<xref rid="btad059-F5" ref-type="fig">Fig. 5</xref>). We found that the similarities between the generated molecules and the dataset have a consistent distribution, and the similarities are relatively low, almost all closer to 0 rather than 1, indicating that these molecules are not simply replicating the dataset.</p>
        <fig position="float" id="btad059-F5">
          <label>Fig. 5.</label>
          <caption>
            <p>Tanimoto similarity of the eight sample compounds to the training set of the main dataset. For each seed, histograms show the similarity of molecules sampled from the chemical space generated by GP or FFN fitness optimization to the training set of the main dataset, and lines represent the kernel density estimates for the respective histograms</p>
          </caption>
          <graphic xlink:href="btad059f5" position="float"/>
        </fig>
      </sec>
      <sec>
        <title>3.4.3 Molecular visualization generated along the optimized direction</title>
        <p>The goal of latent space optimization is to make the population move in the direction of stronger inhibition under the guidance of fitness to explore compounds with stronger antibacterial activity. We sampled the molecules generated by latent space exploration in sequential equal steps and observed the similarity of the sampled molecules to the corresponding seeds and their inhibitory properties (<xref rid="sup1" ref-type="supplementary-material">Supplementary Figs S8 and S9</xref>). We found that the population is always able to continue exploring in a more inhibitory direction while guaranteeing a higher similarity to the seed. The corresponding SMILES of the compounds in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figs S8 and S9</xref> are shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S7 and S8</xref>, respectively.</p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>4 Conclusion</title>
    <p>In this article, we report on MDAGS, a generation method of potent antibiotic design. The method incorporates two novel ideas: the encoder and predictor are jointly trained to learn a latent property space, and an attribute-guided optimization strategy is employed in the latent space to enable the model to explore in the direction of the expected properties of the molecule. Doing so aggregates molecules with similar properties, thereby reducing exploration of the vast space and avoiding exploration in directions that may not make sense. In addition, considering that the generator does not contribute to the learning of the latent space, we decouple the encoder and generator, which greatly reduces the model complexity.</p>
    <p>Our findings suggest that MDAGS can serve as a novel and practical method for effective antibiotic design. Visualization of the designed latent space shows that the joint training of the model captures molecular properties so that molecules cluster in spatially distinct locations according to antibacterial activity. Validation of the two fitness functions demonstrates the ability of the model to predict the antibacterial activity of the underlying representation, thereby supporting the optimization of molecules in the potential property space. On the benchmark task of unlabeled molecule generation, MDAGS shows comparable or even better performance than the benchmark model. Furthermore, in the antibiotic molecule generation task, the analysis of the generated molecules shows that our method is able to generate molecules with other properties comparable to existing antibiotics but with better antibacterial activity, and there is a large difference between these generated molecules and existing real data. Finally, the equal-step sampling of the generated results provides insight into how efficiently the method can navigate the property space to find improved molecules with desired properties.</p>
    <p>Given the generality of MDAGS, we believe it will impact other drug discovery cases. For example, designing a more inhibitory antitumor drug requires little to no model changes, just data replacement. However, the method also has limitations. In this work, we only focus on and optimize one property, i.e. antibacterial activity; however, when a drug acts on an organism, many factors need to be considered, such as low toxicity and better pharmacokinetic properties. Our current method does not take these factors into account.</p>
    <p>Overall, our work provides a new approach for the generation and optimization of molecules, especially potent antibiotic compounds. This is undoubtedly of great significance for addressing today’s increasingly serious problem of antibiotic resistance. At the same time, we believe our work will positively impact other drug discovery cases. In future work, we will further explore optimizing our method to consider multi-objective optimization to improve the success rate of compounds in clinical trials.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btad059_Supplementary_Data</label>
      <media xlink:href="btad059_supplementary_data.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>Thanks to all those who maintain excellent databases and to all experimentalists who enabled this work by making their data publicly available.</p>
  </ack>
  <sec>
    <title>Funding</title>
    <p>This work was supported by the National Natural Science Foundation of China [62072353 and 62132015].</p>
    <p><italic toggle="yes">Conflict of Interest</italic>: The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
  </sec>
  <sec sec-type="data-availability">
    <title>Data availability</title>
    <p>The Drugs and Natural Products data underlying this article are avaliable in <ext-link xlink:href="https://github.com/LiangYu-Xidian/MDAGS" ext-link-type="uri">https://github.com/LiangYu-Xidian/MDAGS</ext-link>. The MOSES data underlying this article are avaliable in <ext-link xlink:href="https://github.com/molecularsets/moses" ext-link-type="uri">https://github.com/molecularsets/moses</ext-link>. The GuacaMol data underlying this article are avaliable in <ext-link xlink:href="https://github.com/BenevolentAI/guacamol" ext-link-type="uri">https://github.com/BenevolentAI/guacamol</ext-link>.</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btad059-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Årdal</surname><given-names>C.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>Antibiotic development—economic, regulatory and societal challenges</article-title>. <source>Nat. Rev. Microbiol</source>., <volume>18</volume>, <fpage>267</fpage>–<lpage>274</lpage>.<pub-id pub-id-type="pmid">31745330</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bagal</surname><given-names>V.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>MolGPT: molecular generation using a transformer-decoder model</article-title>. <source>J. Chem. Inf. Model</source>., <volume>62</volume>, <fpage>2064</fpage>–<lpage>2076</lpage>.<pub-id pub-id-type="pmid">34694798</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B3">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Benhenda</surname><given-names>M.</given-names></string-name></person-group> (<year>2017</year>) ChemGAN challenge for drug discovery: can AI reproduce natural chemical diversity? arXiv preprint arXiv:1708.08227.</mixed-citation>
    </ref>
    <ref id="btad059-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Born</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>PaccMann<sup>RL</sup>: de novo generation of hit-like anticancer molecules from transcriptomic data via reinforcement learning</article-title>. <source>iScience</source>, <volume>24</volume>, <fpage>102269</fpage>.<pub-id pub-id-type="pmid">33851095</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brown</surname><given-names>E.D.</given-names></string-name>, <string-name><surname>Wright</surname><given-names>G.D.</given-names></string-name></person-group> (<year>2016</year>) <article-title>Antibacterial drug discovery in the resistance era</article-title>. <source>Nature</source>, <volume>529</volume>, <fpage>336</fpage>–<lpage>343</lpage>.<pub-id pub-id-type="pmid">26791724</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brown</surname><given-names>N.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>GuacaMol: benchmarking models for de novo molecular design</article-title>. <source>J. Chem. Inf. Model</source>., <volume>59</volume>, <fpage>1096</fpage>–<lpage>1108</lpage>.<pub-id pub-id-type="pmid">30887799</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Button</surname><given-names>A.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Automated de novo molecular design by hybrid machine intelligence and rule-driven chemical synthesis</article-title>. <source>Nat. Mach. Intell</source>., <volume>1</volume>, <fpage>307</fpage>–<lpage>315</lpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Camacho</surname><given-names>D.M.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>Next-generation machine learning for biological networks</article-title>. <source>Cell</source>, <volume>173</volume>, <fpage>1581</fpage>–<lpage>1592</lpage>.<pub-id pub-id-type="pmid">29887378</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B9">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>Z.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) CELLS: cost-effective evolution in latent space for goal-directed molecular generation. arXiv preprint arXiv:2112.00905.</mixed-citation>
    </ref>
    <ref id="btad059-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dao</surname><given-names>F.Y.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>DeepYY1: a deep learning approach to identify YY1-mediated chromatin loops</article-title>. <source>Brief. Bioinform</source>., <volume>22</volume>, <fpage>bbaa356</fpage>.<pub-id pub-id-type="pmid">33279983</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Das</surname><given-names>P.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Accelerated antimicrobial discovery via deep generative models and molecular dynamics simulations</article-title>. <source>Nat. Biomed. Eng</source>., <volume>5</volume>, <fpage>613</fpage>–<lpage>623</lpage>.<pub-id pub-id-type="pmid">33707779</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B12">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>De Cao</surname><given-names>N.</given-names></string-name>, <string-name><surname>Kipf</surname><given-names>T.</given-names></string-name></person-group> (<year>2018</year>) MolGAN: an implicit generative model for small molecular graphs. arXiv preprint arXiv:1805.11973.</mixed-citation>
    </ref>
    <ref id="btad059-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Godinez</surname><given-names>W.J.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>Design of potent antimalarials with generative chemistry</article-title>. <source>Nat. Mach. Intell</source>., <volume>4</volume>, <fpage>180</fpage>–<lpage>186</lpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gómez-Bombarelli</surname><given-names>R.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>Automatic chemical design using a data-driven continuous representation of molecules</article-title>. <source>ACS Cent. Sci</source>., <volume>4</volume>, <fpage>268</fpage>–<lpage>276</lpage>.<pub-id pub-id-type="pmid">29532027</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Griffiths</surname><given-names>R.-R.</given-names></string-name>, <string-name><surname>Hernández-Lobato</surname><given-names>J.M.</given-names></string-name></person-group> (<year>2020</year>) <article-title>Constrained Bayesian optimization for automatic chemical design using variational autoencoders</article-title>. <source>Chem. Sci</source>., <volume>11</volume>, <fpage>577</fpage>–<lpage>586</lpage>.<pub-id pub-id-type="pmid">32190274</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hoffman</surname><given-names>S.C.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>Optimizing molecules using efficient queries from property evaluations</article-title>. <source>Nat. Mach. Intell</source>., <volume>4</volume>, <fpage>21</fpage>–<lpage>31</lpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Irwin</surname><given-names>J.J.</given-names></string-name>, <string-name><surname>Shoichet</surname><given-names>B.K.</given-names></string-name></person-group> (<year>2005</year>) <article-title>ZINC− a free database of commercially available compounds for virtual screening</article-title>. <source>J. Chem. Inf. Model</source>., <volume>45</volume>, <fpage>177</fpage>–<lpage>182</lpage>.<pub-id pub-id-type="pmid">15667143</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ismail</surname><given-names>S.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Pan-vaccinomics approach towards a universal vaccine candidate against WHO priority pathogens to address growing global antibiotic resistance</article-title>. <source>Comput. Biol. Med</source>., <volume>136</volume>, <fpage>104705</fpage>.<pub-id pub-id-type="pmid">34340127</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jiménez-Luna</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>DeltaDelta neural networks for lead optimization of small molecule potency</article-title>. <source>Chem. Sci</source>., <volume>10</volume>, <fpage>10911</fpage>–<lpage>10918</lpage>.<pub-id pub-id-type="pmid">32190246</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B20">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Jin</surname><given-names>W.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) Junction tree variational autoencoder for molecular graph generation. In: <italic toggle="yes">International Conference on Machine Learning</italic>, Stockholm, Sweden. pp. <fpage>2323</fpage>–<lpage>2332</lpage>. <publisher-name>PMLR</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btad059-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Joshi</surname><given-names>P.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>An ensembled SVM based approach for predicting adverse drug reactions</article-title>. <source>Curr. Bioinform</source>., <volume>16</volume>, <fpage>422</fpage>–<lpage>432</lpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kotsias</surname><given-names>P.-C.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>Direct steering of de novo molecular generation with descriptor conditional recurrent neural networks</article-title>. <source>Nat. Mach. Intell</source>., <volume>2</volume>, <fpage>254</fpage>–<lpage>265</lpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Krishnan</surname><given-names>S.R.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>De novo structure-based drug design using deep learning</article-title>. <source>J. Chem. Inf. Model</source>., <volume>61</volume>, <fpage>621</fpage>–<lpage>630</lpage>.<pub-id pub-id-type="pmid">33491455</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kullback</surname><given-names>S.</given-names></string-name>, <string-name><surname>Leibler</surname><given-names>R.A.</given-names></string-name></person-group> (<year>1951</year>) <article-title>On information and sufficiency</article-title>. <source>Ann. Math. Stat</source>., <volume>22</volume>, <fpage>79</fpage>–<lpage>86</lpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kwon</surname><given-names>Y.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Efficient learning of non-autoregressive graph variational autoencoders for molecular graph generation</article-title>. <source>J. Cheminform</source>., <volume>11</volume>, <fpage>1</fpage>–<lpage>10</lpage>.<pub-id pub-id-type="pmid">30604073</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>LeCun</surname><given-names>Y.</given-names></string-name></person-group><etal>et al</etal> (<year>2015</year>) <article-title>Deep learning</article-title>. <source>Nature</source>, <volume>521</volume>, <fpage>436</fpage>–<lpage>444</lpage>.<pub-id pub-id-type="pmid">26017442</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname><given-names>Y.J.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Generative adversarial networks for de novo molecular design</article-title>. <source>Mol. Inf</source>., <volume>40</volume>, <fpage>2100045</fpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>W.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Surface design for antibacterial materials: from fundamentals to advanced strategies</article-title>. <source>Adv. Sci</source>., <volume>8</volume>, <fpage>2100368</fpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>Y.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>Multi-objective de novo drug design with conditional graph generative model</article-title>. <source>J. Cheminform</source>., <volume>10</volume>, <fpage>1</fpage>–<lpage>24</lpage>.<pub-id pub-id-type="pmid">29340790</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lv</surname><given-names>H.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>DeepKla: an attention mechanism-based deep neural network for protein lysine lactylation site prediction</article-title>. <source>iMeta</source>, <volume>1</volume>, <fpage>e11</fpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Maziarka</surname><given-names>Ł.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>Mol-CycleGAN: a generative model for molecular optimization</article-title>. <source>J. Cheminform</source>., <volume>12</volume>, <fpage>2</fpage>–<lpage>18</lpage>.<pub-id pub-id-type="pmid">33431006</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mendez</surname><given-names>D.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>ChEMBL: towards direct deposition of bioassay data</article-title>. <source>Nucleic Acids Res</source>., <volume>47</volume>, <fpage>D930</fpage>–<lpage>D940</lpage>.<pub-id pub-id-type="pmid">30398643</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Miethke</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Towards the sustainable discovery and development of new antibiotics</article-title>. <source>Nat. Rev. Chem</source>., <volume>5</volume>, <fpage>726</fpage>–<lpage>749</lpage>.<pub-id pub-id-type="pmid">34426795</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mulpuru</surname><given-names>V.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>HAMP: a knowledgebase of antimicrobial peptides from human microbiome</article-title>. <source>Curr. Bioinform</source>., <volume>16</volume>, <fpage>534</fpage>–<lpage>540</lpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Murray</surname><given-names>C.J.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>Global burden of bacterial antimicrobial resistance in 2019: a systematic analysis</article-title>. <source>Lancet</source>, <volume>399</volume>, <fpage>629</fpage>–<lpage>655</lpage>.<pub-id pub-id-type="pmid">35065702</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nieuwlaat</surname><given-names>R.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Coronavirus disease 2019 and antimicrobial resistance: parallel and interacting health emergencies</article-title>. <source>Clin. Infect. Dis</source>., <volume>72</volume>, <fpage>1657</fpage>–<lpage>1659</lpage>.<pub-id pub-id-type="pmid">32544232</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Olivecrona</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) <article-title>Molecular de-novo design through deep reinforcement learning</article-title>. <source>J. Cheminform</source>., <volume>9</volume>, <fpage>1</fpage>–<lpage>14</lpage>.<pub-id pub-id-type="pmid">28316652</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Polykovskiy</surname><given-names>D.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>Molecular sets (MOSES): a benchmarking platform for molecular generation models</article-title>. <source>Front. Pharmacol</source>., <volume>11</volume>, <fpage>565644</fpage>.<pub-id pub-id-type="pmid">33390943</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Popova</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>Deep reinforcement learning for de novo drug design</article-title>. <source>Sci. Adv</source>., <volume>4</volume>, <fpage>eaap7885</fpage>.<pub-id pub-id-type="pmid">30050984</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Preuer</surname><given-names>K.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>Fréchet ChemNet distance: a metric for generative models for molecules in drug discovery</article-title>. <source>J. Chem. Inf. Model</source>., <volume>58</volume>, <fpage>1736</fpage>–<lpage>1741</lpage>.<pub-id pub-id-type="pmid">30118593</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Probst</surname><given-names>D.</given-names></string-name>, <string-name><surname>Reymond</surname><given-names>J.-L.</given-names></string-name></person-group> (<year>2018</year>) <article-title>FUn: a framework for interactive visualizations of large, high-dimensional datasets on the web</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>1433</fpage>–<lpage>1435</lpage>.<pub-id pub-id-type="pmid">29186333</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Putin</surname><given-names>E.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>Reinforced adversarial neural computer for de novo molecular design</article-title>. <source>J. Chem. Inf. Model</source>., <volume>58</volume>, <fpage>1194</fpage>–<lpage>1204</lpage>.<pub-id pub-id-type="pmid">29762023</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ru</surname><given-names>X.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>Exploration of the correlation between GPCRs and drugs based on a learning to rank algorithm</article-title>. <source>Comput. Biol. Med</source>., <volume>119</volume>, <fpage>103660</fpage>.<pub-id pub-id-type="pmid">32090901</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ru</surname><given-names>X.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>NerLTR-DTA: drug-target binding affinity prediction based on neighbor relationship and learning to rank</article-title>. <source>Bioinformatics</source>, <volume>38</volume>, <fpage>1964</fpage>–<lpage>1971</lpage>.<pub-id pub-id-type="pmid">35134828</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sanchez-Lengeling</surname><given-names>B.</given-names></string-name>, <string-name><surname>Aspuru-Guzik</surname><given-names>A.</given-names></string-name></person-group> (<year>2018</year>) <article-title>Inverse molecular design using machine learning: generative models for matter engineering</article-title>. <source>Science</source>, <volume>361</volume>, <fpage>360</fpage>–<lpage>365</lpage>.<pub-id pub-id-type="pmid">30049875</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Segler</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2018a</year>) Generating focussed molecule libraries for drug discovery with recurrent neural networks. <italic toggle="yes">ACS central science</italic>, <volume>4</volume>(1): <fpage>120</fpage>–<lpage>131</lpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Segler</surname><given-names>M.H.</given-names></string-name></person-group><etal>et al</etal> (<year>2018b</year>) <article-title>Planning chemical syntheses with deep neural networks and symbolic AI</article-title>. <source>Nature</source>, <volume>555</volume>, <fpage>604</fpage>–<lpage>610</lpage>.<pub-id pub-id-type="pmid">29595767</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shahriari</surname><given-names>B.</given-names></string-name></person-group><etal>et al</etal> (<year>2016</year>) <article-title>Taking the human out of the loop: a review of Bayesian optimization</article-title>. <source>Proc. IEEE</source>, <volume>104</volume>, <fpage>148</fpage>–<lpage>175</lpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B49">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shaker</surname><given-names>B.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Introduction of advanced methods for structure-based drug discovery</article-title>. <source>Curr. Bioinform</source>., <volume>16</volume>, <fpage>351</fpage>–<lpage>363</lpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B50">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Simonovsky</surname><given-names>M.</given-names></string-name>, <string-name><surname>Komodakis</surname><given-names>N.</given-names></string-name></person-group> (<year>2018</year>) GraphVAE: towards generation of small graphs using variational autoencoders. In: <italic toggle="yes">International Conference on Artificial Neural Networks</italic>, Rhodes, Greece. pp. <fpage>412</fpage>–<lpage>422</lpage>. <publisher-name>Springer</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btad059-B51">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Skalic</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Shape-based generative modeling for de novo drug design</article-title>. <source>J. Chem. Inf. Model</source>., <volume>59</volume>, <fpage>1205</fpage>–<lpage>1214</lpage>.<pub-id pub-id-type="pmid">30762364</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B52">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stokes</surname><given-names>J.M.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>A deep learning approach to antibiotic discovery</article-title>. <source>Cell</source>, <volume>180</volume>, <fpage>688</fpage>–<lpage>702.e13</lpage>.<pub-id pub-id-type="pmid">32084340</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B53">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Su</surname><given-names>R.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Deep-Resp-Forest: a deep forest model to predict anti-cancer drug response</article-title>. <source>Methods (San Diego, Calif.)</source>, <volume>166</volume>, <fpage>91</fpage>–<lpage>102</lpage>.<pub-id pub-id-type="pmid">30772464</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B54">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sugden</surname><given-names>R.</given-names></string-name></person-group><etal>et al</etal> (<year>2016</year>) <article-title>Combatting antimicrobial resistance globally</article-title>. <source>Nat. Microbiol</source>., <volume>1</volume>, <fpage>1</fpage>–<lpage>2</lpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B55">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Theuretzbacher</surname><given-names>U.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>The global preclinical antibacterial pipeline</article-title>. <source>Nat. Rev. Microbiol</source>., <volume>18</volume>, <fpage>275</fpage>–<lpage>285</lpage>.<pub-id pub-id-type="pmid">31745331</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B56">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Tu</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Coley</surname><given-names>C.W.</given-names></string-name></person-group> (<year>2021</year>) Permutation invariant graph-to-sequence model for template-free retrosynthesis and reaction prediction. arXiv preprint arXiv:2110.09681.</mixed-citation>
    </ref>
    <ref id="btad059-B57">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>von Lilienfeld</surname><given-names>O.A.</given-names></string-name>, <string-name><surname>Burke</surname><given-names>K.</given-names></string-name></person-group> (<year>2020</year>) <article-title>Retrospective on a decade of machine learning for chemical discovery</article-title>. <source>Nat. Commun</source>., <volume>11</volume>, <fpage>1</fpage>–<lpage>4</lpage>.<pub-id pub-id-type="pmid">31911652</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B58">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Multi-constraint molecular generation based on conditional transformer, knowledge distillation and reinforcement learning</article-title>. <source>Nat. Mach. Intell</source>., <volume>3</volume>, <fpage>914</fpage>–<lpage>922</lpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B59">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Williams</surname><given-names>C.K.</given-names></string-name>, <string-name><surname>Rasmussen</surname><given-names>C.E.</given-names></string-name></person-group> (<year>2006</year>) <source>Gaussian Processes for Machine Learning</source>. <publisher-name>MIT Press</publisher-name>, <publisher-loc>Cambridge, MA</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btad059-B60">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Xu</surname><given-names>K.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) Graph2Seq: graph to sequence learning with attention-based neural networks. arXi preprint arXiv:1804.00823.</mixed-citation>
    </ref>
    <ref id="btad059-B61">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yang</surname><given-names>K.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Analyzing learned molecular representations for property prediction</article-title>. <source>J. Chem. Inf. Model</source>., <volume>59</volume>, <fpage>3370</fpage>–<lpage>3388</lpage>.<pub-id pub-id-type="pmid">31361484</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B62">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yang</surname><given-names>Q.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>An adaptive stochastic dominant learning swarm optimizer for high-dimensional optimization</article-title>. <source>IEEE Trans. Cybern</source>., <volume>99</volume>, <fpage>1</fpage>–<lpage>17</lpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B63">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>You</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) Graph convolutional policy network for goal-directed molecular graph generation. In: <italic toggle="yes">Advances in Neural Information Processing Systems</italic>, Montréal, Canada. p.<fpage>31</fpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B64">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zampieri</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) <article-title>Nontargeted metabolomics reveals the multilevel response to antibiotic perturbations</article-title>. <source>Cell Rep</source>., <volume>19</volume>, <fpage>1214</fpage>–<lpage>1228</lpage>.<pub-id pub-id-type="pmid">28494870</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B65">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zulfiqar</surname><given-names>H.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>Deep-4mCGP: a deep learning approach to predict 4mC sites in <italic toggle="yes">Geobacter pickeringii</italic> by using correlation-based feature selection technique</article-title>. <source>Int. J. Mol. Sci</source>., <volume>23</volume>, <fpage>1251</fpage>.<pub-id pub-id-type="pmid">35163174</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9897189</article-id>
    <article-id pub-id-type="pmid">36707990</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btad059</article-id>
    <article-id pub-id-type="publisher-id">btad059</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Paper</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Data and Text Mining</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Potent antibiotic design via guided search from antibacterial activity evaluations</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Chen</surname>
          <given-names>Lu</given-names>
        </name>
        <aff><institution>School of Computer Science and Technology, Xidian University</institution>, Xi’an 710071, Shaanxi, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-8351-3332</contrib-id>
        <name>
          <surname>Yu</surname>
          <given-names>Liang</given-names>
        </name>
        <xref rid="btad059-cor1" ref-type="corresp"/>
        <!--lyu@xidian.edu.cn-->
        <aff><institution>School of Computer Science and Technology, Xidian University</institution>, Xi’an 710071, Shaanxi, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-6396-0787</contrib-id>
        <name>
          <surname>Gao</surname>
          <given-names>Lin</given-names>
        </name>
        <aff><institution>School of Computer Science and Technology, Xidian University</institution>, Xi’an 710071, Shaanxi, <country country="CN">China</country></aff>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Wren</surname>
          <given-names>Jonathan</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btad059-cor1">To whom correspondence should be addressed. <email>lyu@xidian.edu.cn</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2023-01-27">
      <day>27</day>
      <month>1</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>27</day>
      <month>1</month>
      <year>2023</year>
    </pub-date>
    <volume>39</volume>
    <issue>2</issue>
    <elocation-id>btad059</elocation-id>
    <history>
      <date date-type="received">
        <day>26</day>
        <month>9</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>14</day>
        <month>1</month>
        <year>2023</year>
      </date>
      <date date-type="editorial-decision">
        <day>22</day>
        <month>1</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>25</day>
        <month>1</month>
        <year>2023</year>
      </date>
      <date date-type="corrected-typeset">
        <day>02</day>
        <month>2</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btad059.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>The emergence of drug-resistant bacteria makes the discovery of new antibiotics an urgent issue, but finding new molecules with the desired antibacterial activity is an extremely difficult task. To address this challenge, we established a framework, MDAGS (Molecular Design via Attribute-Guided Search), to optimize and generate potent antibiotic molecules.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>By designing the antibacterial activity latent space and guiding the optimization of functional compounds based on this space, the model MDAGS can generate novel compounds with desirable antibacterial activity without the need for extensive expensive and time-consuming evaluations. Compared with existing antibiotics, candidate antibacterial compounds generated by MDAGS always possessed significantly better antibacterial activity and ensured high similarity. Furthermore, although without explicit constraints on similarity to known antibiotics, these candidate antibacterial compounds all exhibited the highest structural similarity to antibiotics of expected function in the DrugBank database query. Overall, our approach provides a viable solution to the problem of bacterial drug resistance.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>Code of the model and datasets can be downloaded from GitHub (<ext-link xlink:href="https://github.com/LiangYu-Xidian/MDAGS" ext-link-type="uri">https://github.com/LiangYu-Xidian/MDAGS</ext-link>).</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Natural Science Foundation of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>62072353</award-id>
        <award-id>62132015</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="9"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Bacterial drug resistance has led to an increasing number of health, social and economic problems that are becoming more prominent globally (<xref rid="btad059-B54" ref-type="bibr">Sugden <italic toggle="yes">et al.</italic>, 2016</xref>). Studies have shown that by 2050, if the global problem of antimicrobial resistance (AMR) is not addressed, 10 million people will die from AMR annually (<xref rid="btad059-B34" ref-type="bibr">Mulpuru <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btad059-B36" ref-type="bibr">Nieuwlaat <italic toggle="yes">et al.</italic>, 2021</xref>). The development of new antibiotics is typically abandoned due to unresolved regulatory issues and lack of commercial appeal (<xref rid="btad059-B1" ref-type="bibr">Årdal <italic toggle="yes">et al.</italic>, 2020</xref>). Furthermore, industry experts have analyzed unsuccessful antibacterial programs and found that high-throughput screening of synthetic chemicals often fails to find promising compounds with expected properties (<xref rid="btad059-B5" ref-type="bibr">Brown and Wright, 2016</xref>). Therefore, identifying and developing novel antibiotic drugs remains very challenging (<xref rid="btad059-B18" ref-type="bibr">Ismail <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btad059-B28" ref-type="bibr">Li <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btad059-B33" ref-type="bibr">Miethke <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btad059-B35" ref-type="bibr">Murray <italic toggle="yes">et al.</italic>, 2022</xref>; <xref rid="btad059-B55" ref-type="bibr">Theuretzbacher <italic toggle="yes">et al.</italic>, 2020</xref>).</p>
    <p>In recent years, computational methods, especially deep learning, have emerged as a promising avenue for drug discovery by combining domain knowledge and data-driven learning (<xref rid="btad059-B7" ref-type="bibr">Button <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btad059-B8" ref-type="bibr">Camacho <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btad059-B16" ref-type="bibr">Hoffman <italic toggle="yes">et al.</italic>, 2022</xref>; <xref rid="btad059-B21" ref-type="bibr">Joshi <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btad059-B22" ref-type="bibr">Kotsias <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btad059-B26" ref-type="bibr">LeCun <italic toggle="yes">et al.</italic>, 2015</xref>; <xref rid="btad059-B30" ref-type="bibr">Lv <italic toggle="yes">et al.</italic>, 2022</xref>; <xref rid="btad059-B53" ref-type="bibr">Su <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btad059-B57" ref-type="bibr">von Lilienfeld and Burke, 2020</xref>). Among them, deep generative methods have been shown to be ideal for drug candidate discovery (<xref rid="btad059-B14" ref-type="bibr">Gómez-Bombarelli <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btad059-B42" ref-type="bibr">Putin <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btad059-B43" ref-type="bibr">Ru <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btad059-B45" ref-type="bibr">Sanchez-Lengeling and Aspuru-Guzik, 2018</xref>; <xref rid="btad059-B47" ref-type="bibr">Segler <italic toggle="yes">et al.</italic>, 2018b</xref>; <xref rid="btad059-B49" ref-type="bibr">Shaker <italic toggle="yes">et al.</italic>, 2021</xref>). In view of this, deep generative methods can be applied to generate ideal antibiotic molecules, thereby increasing the success rate of new antibiotic discovery and reducing the cost in the discovery process. To date, the related works of deep generative methods that have been reported in industry and academia (<xref rid="btad059-B10" ref-type="bibr">Dao <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btad059-B65" ref-type="bibr">Zulfiqar <italic toggle="yes">et al.</italic>, 2022</xref>) can be divided into generation-based and guide optimization-based, which are not mutually exclusive.</p>
    <p>In generation-based work, many models represent a molecule with the corresponding simplified molecular input line entry specification (SMILES) notation so that the molecule generation task is transformed into a sequence-to-sequence generation task (<xref rid="btad059-B4" ref-type="bibr">Born <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btad059-B23" ref-type="bibr">Krishnan <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btad059-B27" ref-type="bibr">Lee <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btad059-B37" ref-type="bibr">Olivecrona <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btad059-B39" ref-type="bibr">Popova <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btad059-B46" ref-type="bibr">Segler <italic toggle="yes">et al.</italic>, 2018a</xref>; <xref rid="btad059-B58" ref-type="bibr">Wang <italic toggle="yes">et al.</italic>, 2021</xref>). However, this approach often requires large-scale pretraining, and grammatical errors will often lead to the generation of invalid SMILES, which cannot be converted into realistic molecular structures. There are also models that are represented as molecular graphs so that the task of generating molecules is transformed into a graph-to-graph generation task. Graph-based generation methods are further divided into autoregressive(<xref rid="btad059-B29" ref-type="bibr">Li <italic toggle="yes">et al.</italic>, 2018</xref>) and non-autoregressive (<xref rid="btad059-B12" ref-type="bibr">De Cao and Kipf, 2018</xref>; <xref rid="btad059-B20" ref-type="bibr">Jin <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btad059-B25" ref-type="bibr">Kwon <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btad059-B31" ref-type="bibr">Maziarka <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btad059-B50" ref-type="bibr">Simonovsky and Komodakis, 2018</xref>) generation. Autoregressive generation generates one atom at a time, borrowing ideas from sequence generation, and similarly, this method of generation also leads to chemically invalid intermediate graphs. Non-autoregressive generation generates the entire graph at once; this method has also been proven to be feasible, but its disadvantage is that it requires an additional graph matching process (<xref rid="btad059-B25" ref-type="bibr">Kwon <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btad059-B50" ref-type="bibr">Simonovsky and Komodakis, 2018</xref>).</p>
    <p>In guided optimization-based work, the optimization can be in the generated molecular map or SMILES space or in the encoder–decoder latent space. For the former, the combination of a generative model and a property prediction model or scoring function for reinforcement learning is a common paradigm (<xref rid="btad059-B4" ref-type="bibr">Born <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btad059-B12" ref-type="bibr">De Cao and Kipf, 2018</xref>; <xref rid="btad059-B23" ref-type="bibr">Krishnan <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btad059-B27" ref-type="bibr">Lee <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btad059-B37" ref-type="bibr">Olivecrona <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btad059-B39" ref-type="bibr">Popova <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btad059-B46" ref-type="bibr">Segler <italic toggle="yes">et al.</italic>, 2018a</xref>; <xref rid="btad059-B58" ref-type="bibr">Wang <italic toggle="yes">et al.</italic>, 2021</xref>), but when the generative model is more complex, the generated results are poor since the prediction or scoring results cannot be efficiently fed back to the generative model (<xref rid="btad059-B58" ref-type="bibr">Wang <italic toggle="yes">et al.</italic>, 2021</xref>). For the latter, many models utilize different optimization strategies or sampling strategies for molecule generation (<xref rid="btad059-B7" ref-type="bibr">Button <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btad059-B9" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btad059-B11" ref-type="bibr">Das <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btad059-B13" ref-type="bibr">Godinez <italic toggle="yes">et al.</italic>, 2022</xref>; <xref rid="btad059-B15" ref-type="bibr">Griffiths and Hernández-Lobato, 2020</xref>; <xref rid="btad059-B16" ref-type="bibr">Hoffman <italic toggle="yes">et al.</italic>, 2022</xref>; <xref rid="btad059-B19" ref-type="bibr">Jiménez-Luna <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btad059-B51" ref-type="bibr">Skalic <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btad059-B63" ref-type="bibr">You <italic toggle="yes">et al.</italic>, 2018</xref>).</p>
    <p>In this article, we develop a novel generative method that combines attribute prediction models with an efficient guided search strategy to optimize and generate potent antibiotic molecules with expected antibacterial activity. Inspired by Graph2Seq models in the natural language domain (<xref rid="btad059-B56" ref-type="bibr">Tu and Coley, 2021</xref>; <xref rid="btad059-B60" ref-type="bibr">Xu <italic toggle="yes">et al.</italic>, 2018</xref>), we employ graph-to-sequence generation to exploit the advantages and avoid the disadvantages of existing sequence-to-sequence and graph-to-graph generation methods. We also decouple the encoder and decoder to reduce the complexity of the model. Furthermore, the encoder and attribute prediction model (<xref rid="btad059-B61" ref-type="bibr">Yang <italic toggle="yes">et al.</italic>, 2019</xref>) are jointly trained to learn a latent attribute space—the antibacterial activity space. In this latent space, a guided search based on attribute evaluation (<xref rid="btad059-B62" ref-type="bibr">Yang <italic toggle="yes">et al.</italic>, 2020</xref>) is used to guide the exploration of candidate antibiotic molecules in a meaningful direction (enhancing growth inhibition). We applied this method to generate antibiotic molecules with enhanced antibacterial activity compared to existing antibiotics, and these generated molecules achieved the greatest similarity to the expected functional antibiotic, although we did not explicitly constrain the similarity. Our approach combined a designed antibacterial activity space with guided searches to produce antibiotic molecules with enhanced inhibition for the first time. We believe our work will contribute to the development of new antibiotics.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Data sources</title>
      <sec>
        <title>2.1.1 Drugs and natural products</title>
        <p>We obtained the antibiotic and natural product dataset from <xref rid="btad059-B52" ref-type="bibr">Stokes <italic toggle="yes">et al.</italic> (2020)</xref> and obtained a primary training set containing 2560 compounds, of which 1760 compounds were screened for growth inhibition against <italic toggle="yes">Escherichia coli BW25113</italic> (<xref rid="btad059-B64" ref-type="bibr">Zampieri <italic toggle="yes">et al.</italic>, 2017</xref>) using a widely available US Food and Drug Administration-approved drug library. Another 800 are natural products isolated from plant, animal and microbial sources. After deduplication, 2334 compounds remained. For convenience, we call these 2334 compounds the main dataset. The mean inhibitory concentration—Mean_Inhibition (averaged over two rounds of experiments)—is a measure of the compound’s growth-inhibitory effect on <italic toggle="yes">E.coli BW25113</italic>. The distribution of growth inhibition values in the main dataset is shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S1</xref>.</p>
      </sec>
      <sec>
        <title>2.1.2 MOSES dataset</title>
        <p>The MOSES set is based on the ZINC (<xref rid="btad059-B17" ref-type="bibr">Irwin and Shoichet, 2005</xref>) Clean Leads collection. It contains a total of 4 591 276 molecules, filtered by molecular weight (MW) in the range 250–350 Da, with no more than seven rotatable bonds and XlogP≤3.5. The set excludes molecules containing charged atoms or atoms other than C, N, S, O, F, Cl, Br, H or cycles of more than eight atoms. The molecules were filtered via two filters—medicinal chemistry filters and PAINS filters (<xref rid="btad059-B38" ref-type="bibr">Polykovskiy <italic toggle="yes">et al.</italic>, 2020</xref>). The dataset contains 1 936 962 molecular structures. For experiments, the dataset is divided into training, test and scaffold test sets containing ∼1.6 M, 176 k and 176 k molecules, respectively.</p>
      </sec>
      <sec>
        <title>2.1.3 GuacaMol dataset</title>
        <p>The GuacaMol dataset is derived from the ChEMBL 24 database (<xref rid="btad059-B32" ref-type="bibr">Mendez <italic toggle="yes">et al.</italic>, 2019</xref>). The advantage of ChEMBL is that it contains only molecules that have been synthesized and tested against biological targets (<xref rid="btad059-B6" ref-type="bibr">Brown <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btad059-B44" ref-type="bibr">Ru <italic toggle="yes">et al.</italic>, 2022</xref>). The dataset is divided into training, validation and test sets containing ∼1.3 M, 80 k and 240 k molecules, respectively.</p>
      </sec>
    </sec>
    <sec>
      <title>2.2 Our MDAGS model</title>
      <p>The overall framework of the MDAGS model is shown in <xref rid="btad059-F1" ref-type="fig">Figure 1</xref>.</p>
      <fig position="float" id="btad059-F1">
        <label>Fig. 1.</label>
        <caption>
          <p>Overview of MDAGS. (<bold>A</bold>) The encoder and the predictor are jointly trained to obtain the latent vector representation of the input molecular graph and the predicted growth inhibition value, respectively, and the combination of the two constructs the potential growth inhibition space. The generator generates SMILES corresponding to the latent space optimization result after being pretrained with a large number of unlabeled molecules. (<bold>B</bold>) Latent space optimization consists of two parts: (i) sampling and (ii) updating. A given seed molecule is sampled and the sampled molecule is updated to find the molecule with the global minimum growth inhibition. Molecules satisfying the growth inhibition threshold during sampling and updating are considered successful molecules and are generated</p>
        </caption>
        <graphic xlink:href="btad059f1" position="float"/>
      </fig>
      <sec>
        <title>2.2.1 Architecture of the generative model</title>
        <p><bold>Encoder–predictor for learning inhibition space.</bold> We apply the directed message-passing neural network Chemprop (<xref rid="btad059-B61" ref-type="bibr">Yang <italic toggle="yes">et al.</italic>, 2019</xref>) to learn the inhibition space. The model consists of two parts: an encoder and a predictor. The encoder uses a directed message-passing mechanism to convert the molecular graph into a continuous latent space vector. The atomic features and edge features corresponding to the molecular graph are listed in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref>. By giving directions to the edges of the molecular graph and updating iterative messages based on the edges, unnecessary loops and jitter are avoided. During each iteration, the molecular graph substructure information is passed through the edges, and the node features are updated with the edge features (sum of the bond features). After multiple rounds of iterations, both edge and node features contain larger molecular graph substructure information and even global information, and the node feature aggregation (sum of the atom hidden states) can obtain the representation of the molecular graph. The equations for message passing and updating are:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mo>α</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E2"><label>(2)</label><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>\</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E3"><label>(3)</label><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>α</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E4"><label>(4)</label><mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>a</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E5"><label>(5)</label><mml:math id="M5" display="block" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>a</mml:mi><mml:mrow/></mml:msubsup><mml:mo>=</mml:mo><mml:mo>α</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>a</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>where <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> represents hidden states and <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> represents messages, <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>a</mml:mi><mml:mrow/></mml:msubsup></mml:mrow></mml:math></inline-formula> represents the atom features and <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mrow/></mml:msubsup></mml:mrow></mml:math></inline-formula> represents the bond features, <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow/></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> are learned matrixes, <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mo>α</mml:mo></mml:math></inline-formula> is the activation function ReLU, <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>⋅</mml:mo><mml:mo>,</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> is concatenation and <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the length of the message-passing step. And the readout phase operates according to
<disp-formula id="E6"><label>(6)</label><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>a</mml:mi><mml:mrow/></mml:msubsup></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>The predictor is a feed-forward network (FFN) that predicts the growth inhibition value corresponding to the latent space vector
<disp-formula id="E7"><label>(7)</label><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>A latent property (growth inhibition) space of molecular graphs is learned by joint training of the encoder and predictor and optimized based on this space.</p>
        <p><bold>Generator.</bold> Inspired by the work of <xref rid="btad059-B2" ref-type="bibr">Bagal <italic toggle="yes">et al.</italic> (2022)</xref>, we train a GPT model as the generator to generate molecular SMILES sequences corresponding to the latent space optimization results (see Section 2.2.2 for the optimization procedure). The model utilizes a masked self-attention mechanism and consists of multiple stacked decoding blocks. Each stacked decoding block consists of a self-attention layer and fully connected layers. The long-range dependencies of sequences are modeled through a self-attention module. The implementation of the self-attention mechanism and the fully connected layer for each decoding block can be expressed by the following formulas:
<disp-formula id="E8"><label>(8)</label><mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">atten</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="italic">softmax</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:msup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo stretchy="true">)</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E9"><label>(9)</label><mml:math id="M9" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">hea</mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="italic">atten</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Q</mml:mi><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>Q</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>K</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>V</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E10"><label>(10)</label><mml:math id="M10" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">MultiHead</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="italic">hea</mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="italic">hea</mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E11"><label>(11)</label><mml:math id="M11" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">FNN</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>σ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>where <italic toggle="yes">Q</italic> is query, <italic toggle="yes">K</italic> is key and <italic toggle="yes">V</italic> is value vectors, respectively. <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the dimension of query and key vectors, and <italic toggle="yes">T</italic> is transpose of the matrix. <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">hea</mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the <italic toggle="yes">i</italic>-th attentional head, <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>⋅</mml:mo><mml:mo>,</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> is concatenation and all of the <italic toggle="yes">W</italic>'s above are learnable matrices.<inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mo>σ</mml:mo></mml:math></inline-formula> is the activation function GeLU and <italic toggle="yes">b</italic> is bias.</p>
        <p>For training, we feed the molecular graph latent vector and the molecular SMILES into the model, and the model is then trained on the next character prediction task so that the latent vector guides the model generation. The generator is trained to maximize the conditional likelihood:
<disp-formula id="E12"><label>(12)</label><mml:math id="M12" display="block" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">out</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mtext>latent</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo> </mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mo>.</mml:mo><mml:mo> </mml:mo><mml:mo>.</mml:mo><mml:mo> </mml:mo><mml:mo>.</mml:mo><mml:mo> </mml:mo><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mtext>latent</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mtext>latent</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
        <p>We feed the latent vector into the model, and given a start marker, the model predicts the next marker in the sequence until a stop symbol is generated, thus generating a molecular SMILES.</p>
        <p>Since the latent representation of the molecular graph and the input dimension of the decoding block are inconsistent, we add a fully connected layer between the two to map the latent space vector representation to the corresponding dimension vector of the decoding block.</p>
      </sec>
      <sec>
        <title>2.2.2 Guided search from predicted inhibition</title>
        <p>Although the latent space is already dimensionally reduced relative to the broad drug space, it is still a high-dimensional space. With high-dimensional data, it is difficult to mitigate the impact of dimensional disaster and to continue to explore in the desired direction of growth inhibition in the process of optimization. Inspired by the work of <xref rid="btad059-B62" ref-type="bibr">Yang <italic toggle="yes">et al.</italic> (2020)</xref>, we employ stochastic dominant learning swarm optimization strategy in the latent space. We define a particle as a point in latent space and the fitness as the growth inhibition corresponding to that point. For a seed molecule, the initial population is obtained by sampling around the potential representation, and the population is updated in the direction of minimizing growth inhibition under the guidance of fitness until a point with the minimum global growth inhibition is found. During the update process, points that satisfy the threshold range for growth inhibition are saved and generated (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Algorithm S1</xref> for the detailed process). The optimization mainly includes two parts: sampling and updating.</p>
        <p><bold>Sampling.</bold> If the similarity between the sampling point and the seed is too high, the latent space search is inefficient; if the similarity is too low, the sampling point will not be able to keep the characteristics of the seed. We design the following sampling strategy: for a given point, we randomly sample number of populations (<inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mrow><mml:mi>N</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula>) (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Algorithm S1</xref>) Gaussian noises <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>P</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is calculated as follows: the encoder is used to obtain the latent representation of the main dataset and compute the variance <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi>cov</mml:mi></mml:mrow></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi>cov</mml:mi></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi>cov</mml:mi></mml:mrow></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> for each dimension, where <italic toggle="yes">n</italic> is the dimension of the latent vector. This yields the following:
<disp-formula id="E13"><label>(13)</label><mml:math id="M13" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>max</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi>cov</mml:mi></mml:mrow></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi>cov</mml:mi></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi>cov</mml:mi></mml:mrow></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E14"><label>(14)</label><mml:math id="M14" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E15"><label>(15)</label><mml:math id="M15" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>×</mml:mo><mml:mi>I</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic toggle="yes">I</italic> is an identity matrix and <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> represents the latent space representation of the seed. Gaussian noises are then added to the latent space representation vector of the seed:
<disp-formula id="E16"><label>(16)</label><mml:math id="M16" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mo>θ</mml:mo><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2…</mml:mn><mml:mi>N</mml:mi><mml:mi>P</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>We set the parameter <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mo>θ</mml:mo></mml:math></inline-formula> (<inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mrow><mml:mo>θ</mml:mo><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>) to make the noise range controllable. <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>P</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> form the initial population (see <xref rid="btad059-F1" ref-type="fig">Fig. 1B</xref>).</p>
        <p><bold>Updating.</bold> For a given point <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the update is done as follows:
<disp-formula id="E17"><label>(17)</label><mml:math id="M17" display="block" overflow="scroll"><mml:mrow><mml:mo>β</mml:mo><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mo>−</mml:mo><mml:mn>0.2</mml:mn><mml:mo>×</mml:mo><mml:mtext>exp</mml:mtext><mml:mo>×</mml:mo><mml:mo stretchy="true">(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo>ξ</mml:mo></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo>ξ</mml:mo></mml:mrow></mml:mfrac><mml:mo stretchy="true">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E18"><label>(18)</label><mml:math id="M18" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>w</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo>β</mml:mo><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E19"><label>(19)</label><mml:math id="M19" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>g</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is a function to calculate individual fitness. We design two <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>g</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> functions: one is a jointly trained FFN predictor, and the other is a Gaussian process (GP) model trained using the training set of the main dataset; both models are used to achieve inhibition consistency. The GP model is a non-parametric model (<xref rid="btad059-B59" ref-type="bibr">Williams and Rasmussen, 2006</xref>). It is a very lightweight model that takes only a few minutes to train. <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mo>ξ</mml:mo></mml:math></inline-formula> is a very small number to avoid the denominator being zero. <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mrow><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are hyperparameters. It is worth noting that only one fitness function is required in a complete scheme. Designing two and doing experiments is to consider both neural network parametric models (FFN) and non-parametric models (GP), which is an extension of the entire experimental scheme.</p>
      </sec>
    </sec>
    <sec>
      <title>2.3 Experimental settings</title>
      <p>The experimental settings for MDAGS model could be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref>.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 Encoder–predictor joint training analysis</title>
      <sec>
        <title>3.1.1 Choice of hyperparameter</title>
        <p>In machine learning, especially in the field of deep learning, the choice of hyperparameters has a great impact on the performance of the model. Therefore, in our work, the joint model is also optimized with hyperparameters before jointly training the encoder and predictor. We apply Bayesian optimization (BO) to perform this process (Shahriari <italic toggle="yes">et al.</italic>, 2016). The main idea of BO is that the posterior distribution of a given complex objective function of optimization is computed with a GP. The objective function can have just the input and output specified with unknown internal structure and mathematical properties. The posterior distribution of the objective function is modified until the posterior distribution reasonably fits the real objective distribution so that the current parameters can be better adjusted. In our experiments, the optimized hyperparameters are the number of message-passing steps, neural network hidden size, number of feed-forward layers and dropout probability. We performed 20 optimizations on the training set of the main dataset and used the validation set to select the best hyperparameter combination. The result of hyperparameter optimization is as follows: the number of message-passing steps is 5, the neural network hidden size is 1900, the number of feed-forward layers is 2 and the dropout probability is 0.30000000000000004.</p>
      </sec>
      <sec>
        <title>3.1.2 Learned latent space visualization</title>
        <p>The generation and optimization of molecules is often concerned with maximizing or minimizing certain properties. In this article, we jointly train an encoder and a predictor to learn the growth-inhibitory property of molecules. In this way, the distribution of molecules in the latent space is organized by growth inhibition.</p>
        <p>We apply the results of hyperparameter optimization to set up the model and train the model to generate the latent space with the main dataset. We analyzed the latent space’s ability to capture molecular features. <xref rid="btad059-F2" ref-type="fig">Figure 2</xref> shows the mapping of attribute values to latent space attribute values, visualized using t-SNE. From <xref rid="btad059-F2" ref-type="fig">Figure 2A</xref>, it can be seen that the molecular representations learned by joint training of the encoder–predictor have a more concentrated distribution for the molecules with obvious growth inhibition, and the molecules with low values are almost all clustered directly below the latent space. To compare with this distribution, we chose the Morgan fingerprint, which is one of the classic fingerprints for feature vectorization of molecules, to compute the fingerprint of the main dataset. We performed the same visualization and found that the calculated molecular latent characterization does not possess the ability to aggregate molecular inhibition (<xref rid="btad059-F2" ref-type="fig">Fig. 2B</xref>).</p>
        <fig position="float" id="btad059-F2">
          <label>Fig. 2.</label>
          <caption>
            <p>Comparison of the latent space learned by the encoder–predictor and the latent space corresponding to the Morgan fingerprint. (<bold>A</bold>) Learned by the encoder–predictor. (<bold>B</bold>) Calculated using Morgan fingerprints. The value of the color bar represents the strength of antibacterial activity, the stronger the antibacterial activity, the smaller the value</p>
          </caption>
          <graphic xlink:href="btad059f2" position="float"/>
        </fig>
      </sec>
    </sec>
    <sec>
      <title>3.2 Latent space optimization analysis</title>
      <sec>
        <title>3.2.1 Fitness function evaluation</title>
        <p>We analyzed the predictive performance of the two designed fitness functions GP and FFN to ensure that the two fitness functions supported the prediction of growth inhibition (<xref rid="btad059-T1" ref-type="table">Table 1</xref>). The experimental growth inhibition <italic toggle="yes">R</italic><sup>2</sup> (correlation values) of the two fitness functions were 0.421 and 0.523, the root mean square error (RMSE) were 0.197 and 0.179 and the mean absolute error (MAE) were 0.130 and 0.118, respectively. It can be seen from this table that the performance of the two fitness functions is similar in these indicators, and both have good predictive performance. Therefore, it can be determined that the two fitness functions support the prediction of growth inhibition and can be used for the subsequent potential space-guided search optimization.</p>
        <table-wrap position="float" id="btad059-T1">
          <label>Table 1.</label>
          <caption>
            <p>Performance of fitness functions GP and FFN</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Fitness function type</th>
                <th rowspan="1" colspan="1">MAE</th>
                <th rowspan="1" colspan="1">RMSE</th>
                <th rowspan="1" colspan="1">
                  <italic toggle="yes">R</italic>
                  <sup>2</sup>
                </th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">GP</td>
                <td rowspan="1" colspan="1">0.130</td>
                <td rowspan="1" colspan="1">0.197</td>
                <td rowspan="1" colspan="1">0.421</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">FFN</td>
                <td rowspan="1" colspan="1">0.118</td>
                <td rowspan="1" colspan="1">0.179</td>
                <td rowspan="1" colspan="1">0.523</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
      <sec>
        <title>3.2.2 Optimization results analysis for selected seeds</title>
        <p>In the latent space, we selected four compound molecules from the test set of the main dataset as seeds for optimization: <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">cefazolin</mml:mi><mml:mo> </mml:mo><mml:mi mathvariant="italic">sodium</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">cefotaxime</mml:mi><mml:mo> </mml:mo><mml:mi mathvariant="italic">sodium</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">ceftazidime</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">minocycline</mml:mi><mml:mo> </mml:mo><mml:mi mathvariant="italic">hydrochloride</mml:mi></mml:mrow></mml:math></inline-formula>. The first three of these four compounds are cephalosporin antibiotics and the fourth is a tetracycline antibiotic. Among them, Compounds 1 and 3 have broad-spectrum antibacterial ability and Compounds 2 and 4 are active against gram-positive and gram-negative bacteria.</p>
        <p>For the four selected seed compounds, predicted growth inhibition was calculated with GP and FFN and compared with the actual growth inhibition (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref>). The prediction results of the two functions are slightly different. In addition, we counted the number of potential threshold range representations corresponding to the four seeds generated by the GP and FFN fitness function optimization (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S3</xref>). The FFN fitness function has more latent space representations than the GP optimization. We believe that this is related not only to setting different thresholds but also to the prediction accuracy of the two fitness functions. Exploring more successful latent space representations may mean exploring a wider space. <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S2</xref> verifies our conjecture. The latent representations optimized with the GP fitness function are more evenly spread around the seeds, but the latent representations optimized with the FFN fitness function have been further explored in a certain direction.</p>
        <p>Our goal is to find new molecules with stronger growth inhibition. We plotted the inhibition values corresponding to the main dataset and the latent representations optimized by the GP and FFN fitness functions (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S3</xref>). It can be seen from the figure that the predicted inhibition value of the latent representation obtained by the optimization of the two fitness functions is significantly lower than that of the main dataset.</p>
      </sec>
    </sec>
    <sec>
      <title>3.3 Generation analysis</title>
      <sec>
        <title>3.3.1 Generation performance comparison</title>
        <p>It is important for the generative model to generate chemically valuable, novel and unique molecules. This ensures that the generator has the ability to broadly explore new spaces beyond the existing chemical space without overfitting existing data. Therefore, we evaluate the generative ability of the model on multiple metrics and compare it with two benchmarks—MOSES and GuacaMol. It is worth noting that although we use the MolGPT framework in this article, our generation idea and method differ from the original. We believe these have an impact on the generative power of the model. Therefore, we also compared with the MolGPT model.</p>
        <p>On the MOSES benchmark, we compare our model with five benchmarks, CharRNN, AAE, VAE, JTN-VAE and LatentGAN, in addition to the MolGPT model (<xref rid="btad059-T2" ref-type="table">Table 2</xref>). In addition to validity, novelty and uniqueness, evaluation metrics include filters and internal diversity. Filters are mainly used to evaluate the proportion of filtered generated molecules applied during dataset construction. The resulting molecules, while chemically valid, may contain unnecessary fragments. Therefore, when building the MOSES training dataset, molecules with these fragments were removed, and the model is expected to avoid producing them (<xref rid="btad059-B38" ref-type="bibr">Polykovskiy <italic toggle="yes">et al.</italic>, 2020</xref>). Internal diversity assesses the chemical diversity within the generated set of molecules to avoid model collapse (models tend to produce several fixed types of data) (<xref rid="btad059-B3" ref-type="bibr">Benhenda, 2017</xref>). The results showed that our model significantly outperformed the other models on the novelty metric, suggesting that the model can generate new molecules that are completely different from the training set. In addition, it also has comparable performance to the baseline model on the other four metrics.</p>
        <table-wrap position="float" id="btad059-T2">
          <label>Table 2.</label>
          <caption>
            <p>Performance comparison on the MOSES benchmark</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Model</th>
                <th rowspan="1" colspan="1">Validity (↑)</th>
                <th rowspan="1" colspan="1">Uniqueness@10k (↑)</th>
                <th rowspan="1" colspan="1">Novelty (↑)</th>
                <th rowspan="1" colspan="1">Filters (↑)</th>
                <th rowspan="1" colspan="1">IntDiv (↑)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">CharRNN</td>
                <td rowspan="1" colspan="1">0.9748</td>
                <td rowspan="1" colspan="1">
                  <bold>0.9994</bold>
                </td>
                <td rowspan="1" colspan="1">0.8419</td>
                <td rowspan="1" colspan="1">0.9943</td>
                <td rowspan="1" colspan="1">0.8562</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">AAE</td>
                <td rowspan="1" colspan="1">0.9368</td>
                <td rowspan="1" colspan="1">0.9973</td>
                <td rowspan="1" colspan="1">0.7931</td>
                <td rowspan="1" colspan="1">0.9960</td>
                <td rowspan="1" colspan="1">0.8557</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">VAE</td>
                <td rowspan="1" colspan="1">0.9767</td>
                <td rowspan="1" colspan="1">0.9984</td>
                <td rowspan="1" colspan="1">0.6949</td>
                <td rowspan="1" colspan="1">0.9970</td>
                <td rowspan="1" colspan="1">
                  <bold>0.8558</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">JTN-VAE</td>
                <td rowspan="1" colspan="1">
                  <bold>1.0000</bold>
                </td>
                <td rowspan="1" colspan="1">0.9996</td>
                <td rowspan="1" colspan="1">0.9143</td>
                <td rowspan="1" colspan="1">0.9760</td>
                <td rowspan="1" colspan="1">0.8551</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">LatentGAN</td>
                <td rowspan="1" colspan="1">0.8966</td>
                <td rowspan="1" colspan="1">0.9968</td>
                <td rowspan="1" colspan="1">0.9498</td>
                <td rowspan="1" colspan="1">0.9735</td>
                <td rowspan="1" colspan="1">0.8565</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">MolGPT</td>
                <td rowspan="1" colspan="1">0.9900</td>
                <td rowspan="1" colspan="1">0.9986</td>
                <td rowspan="1" colspan="1">0.8093</td>
                <td rowspan="1" colspan="1">
                  <bold>0.9972</bold>
                </td>
                <td rowspan="1" colspan="1">0.8526</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">MDAGS</td>
                <td rowspan="1" colspan="1">0.9556</td>
                <td rowspan="1" colspan="1">0.9984</td>
                <td rowspan="1" colspan="1">
                  <bold>0.9925</bold>
                </td>
                <td rowspan="1" colspan="1">0.9948</td>
                <td rowspan="1" colspan="1">0.8534</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn1">
              <p>The bold value in each column corresponds to the optimal value in all comparison models under this indicator.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <p>On the GuacaMol benchmark, we compare our model with four benchmarks, Graph MCTS, AAE, ORGAN and VAE, in addition to the MolGPT model (<xref rid="btad059-T3" ref-type="table">Table 3</xref>). In addition to validity, novelty and uniqueness, the evaluation metrics also include KL divergence and FCD metrics. KL divergence measures how well one probability distribution approximates another (<xref rid="btad059-B24" ref-type="bibr">Kullback and Leibler, 1951</xref>). This metric reflects the ability of the model to fit the distribution of the data. FCD is a measure of how close the distribution of the generated data is to the distribution of molecules in the training set (<xref rid="btad059-B40" ref-type="bibr">Preuer <italic toggle="yes">et al.</italic>, 2018</xref>). This metric reflects the model’s ability to incorporate important chemical and biological features. The results show that our model has the best performance on uniqueness, KL divergence and FCD. This shows that the model can generate novel new molecules while fitting the distribution of the dataset.</p>
        <table-wrap position="float" id="btad059-T3">
          <label>Table 3.</label>
          <caption>
            <p>Performance comparison on the GuacaMol benchmark</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Model</th>
                <th rowspan="1" colspan="1">Graph MCTS</th>
                <th rowspan="1" colspan="1">AAE</th>
                <th rowspan="1" colspan="1">ORGAN</th>
                <th rowspan="1" colspan="1">VAE</th>
                <th rowspan="1" colspan="1">MolGPT</th>
                <th rowspan="1" colspan="1">MDAGS</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">Validity</td>
                <td rowspan="1" colspan="1">
                  <bold>1.000</bold>
                </td>
                <td rowspan="1" colspan="1">0.822</td>
                <td rowspan="1" colspan="1">0.379</td>
                <td rowspan="1" colspan="1">0.870</td>
                <td rowspan="1" colspan="1">0.970</td>
                <td rowspan="1" colspan="1">0.929</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Uniqueness</td>
                <td rowspan="1" colspan="1">
                  <bold>1.000</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>1.000</bold>
                </td>
                <td rowspan="1" colspan="1">0.841</td>
                <td rowspan="1" colspan="1">0.999</td>
                <td rowspan="1" colspan="1">0.998</td>
                <td rowspan="1" colspan="1">
                  <bold>1.000</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Novelty</td>
                <td rowspan="1" colspan="1">0.994</td>
                <td rowspan="1" colspan="1">0.998</td>
                <td rowspan="1" colspan="1">0.687</td>
                <td rowspan="1" colspan="1">0.974</td>
                <td rowspan="1" colspan="1">
                  <bold>1.000</bold>
                </td>
                <td rowspan="1" colspan="1">0.984</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">KLD</td>
                <td rowspan="1" colspan="1">0.522</td>
                <td rowspan="1" colspan="1">0.886</td>
                <td rowspan="1" colspan="1">0.267</td>
                <td rowspan="1" colspan="1">
                  <bold>0.982</bold>
                </td>
                <td rowspan="1" colspan="1">0.981</td>
                <td rowspan="1" colspan="1">
                  <bold>0.982</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">FCD</td>
                <td rowspan="1" colspan="1">0.015</td>
                <td rowspan="1" colspan="1">0.529</td>
                <td rowspan="1" colspan="1">0.000</td>
                <td rowspan="1" colspan="1">0.863</td>
                <td rowspan="1" colspan="1">0.736</td>
                <td rowspan="1" colspan="1">
                  <bold>0.874</bold>
                </td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn2">
              <p>The bold value in each row corresponds to the optimal value in all comparison models under this indicator.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
      </sec>
      <sec>
        <title>3.3.2 The impact of transfer learning</title>
        <p>For language generation models, it is necessary to train on a large dataset to learn grammar rules. Considering the small number of main datasets, direct training cannot learn SMILES grammar rules well, which affects the generation effect. We chose to learn grammar rules on two large datasets, MOSES and GuacaMol, and then use the main dataset for fine-tuning to generate antibiotic-specific compounds.</p>
        <p>We investigated whether generators benefit from transfer learning. We plotted the loss curves of the model trained directly and the models fine-tuned after pretraining on the MOSES and GuacaMol datasets, and then evaluated each model’s ability to fit the data (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S4</xref>). In addition, the test set of the main dataset was generated with three trained generators after removing the duplicate data in the MOSES and GuacaMol datasets, and the ability of the generators to regenerate was evaluated (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S4</xref>). The results show that the model trained directly on the master dataset performs less well than the pretrained and fine-tuned models. Additionally, we found that the model pretrained and fine-tuned on the GuacaMol dataset outperformed its MOSES dataset counterpart. We speculate that this may because the GuacaMol dataset has a more similar distribution to the main dataset. <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S5</xref> verifies our estimate. Considering the excellent performance of the model on the GuacaMol-pretrained dataset, we use this model in subsequent generation tasks.</p>
      </sec>
    </sec>
    <sec>
      <title>3.4 Generated results verification</title>
      <sec>
        <title>3.4.1 Generative spaces analysis</title>
        <p>We generated 7496 and 11 924 chemically usable molecules with the trained generators for GP and FFN, respectively, as successful latent representations of fitness optimization. We explored the spatial intersection generated by the two methods (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S6a</xref>); there were 476 repeated molecules explored in the two methods, and 7020 and 11 448 different molecules were generated by each exploration, respectively. The two spanning spaces are more intuitively displayed using TMAP (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S6b</xref>), which calculates the similarity based on ECFP fingerprints to construct a minimum spanning tree. The tree is drawn and displayed by Fearun, and a point in the figure represents a molecule (<xref rid="btad059-B41" ref-type="bibr">Probst and Reymond, 2018</xref>). Since the two modalities explore different chemical spaces, further analysis of both chemical spaces is warranted.</p>
        <p>We further explored the property distribution of the molecules in the two generative spaces with the main dataset (<xref rid="btad059-F3" ref-type="fig">Fig. 3</xref>). The distribution of the two generated datasets is almost the same as that of the main datasets of several important attributes [MW, LogP, number of hydrogen bond donors (HBD), number of hydrogen bond acceptors (HBA), topological polar surface area (TPSA) and synthetic accessibility (SAS)] on the whole, but the individual properties are slightly different: for MW, LogP and HBD, the generated molecules are consistent with the main datasets. However, half of the generated molecules do not conform to the principle of HBA &lt; 10 in the five principles of drug like. And the TPSA of the resulting molecule is more difficult to penetrate the cell membrane than the dataset. The SAS distribution of the generated molecules is more like 10, which means that the generated molecules are relatively difficult to synthesize compared to the dataset. For the performance on HBA, TPSA and SAS, we considered that the generated molecules were affected by the properties of the corresponding seeds.</p>
        <fig position="float" id="btad059-F3">
          <label>Fig. 3.</label>
          <caption>
            <p>Probability distributions of molecular properties for the two generated spaces compared to the main dataset. Calculated property distributions: MW, LogP, HBD, HBA, TPSA and SAS. Lines represent kernel density estimates for individual attributes of the main dataset and the two generated datasets</p>
          </caption>
          <graphic xlink:href="btad059f3" position="float"/>
        </fig>
      </sec>
      <sec>
        <title>3.4.2 Generated potent antibiotic verification</title>
        <p>In this section, we further verify the properties of the generated molecules. For the two generative spaces, we removed the molecules with a similarity &gt;0.80 from the generation results of each seed and then sorted the molecules in the order of Tanimoto similarity from high to low and inhibition value from small to large. The compounds at the resulting first position were further analyzed (Columns 2 and 4 of <xref rid="btad059-F4" ref-type="fig">Fig. 4</xref>). These eight compounds obtained stronger growth inhibition ability (lower PI value) under the condition of ensuring higher similarity with seeds. In other words, these molecules have the potential to have stronger antibacterial activity while their other properties remain similar to those of existing antibiotics. The third and fifth columns of <xref rid="btad059-F4" ref-type="fig">Figure 4</xref> show the similarity maps of the eight molecules and the corresponding seeds so that similar substructures between them can be visually observed. The SMILES corresponding to the seeds in <xref rid="btad059-F4" ref-type="fig">Figure 4</xref> and the eight sampled seeds are shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S5</xref>. We also calculated several properties of the seed molecule and the eight sampled compounds—MW, Log P, HBD, HBA, TPSA and SAS (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S6</xref>)—and found that the eight sampled molecules and the corresponding seed molecules have very similar properties.</p>
        <fig position="float" id="btad059-F4">
          <label>Fig. 4.</label>
          <caption>
            <p>Sample results for potent antibiotic generation. Each row corresponds to the result of a seed. The molecules in the first column are the seeds, and MI is the Mean_Inhibition value of the experiment. The molecule in the second column is the molecule obtained by the GP fitness function and the molecule in the fourth column is the molecule obtained by the FFN fitness function. <italic toggle="yes">S</italic> is the Tanimoto similarity between the molecule and the corresponding seed, QED is the drug-like property of the molecule and PI is the predicted inhibition value. The third and fifth columns are the similarity maps between the molecules in the second and fourth columns and the corresponding seeds. The darker the color is, the more similar the substructure of the molecule and the seed is. The first column in bold is the name of each antibiotic molecule</p>
          </caption>
          <graphic xlink:href="btad059f4" position="float"/>
        </fig>
        <p>In addition, we queried the top three nearest neighbors of these eight compounds in DrugBank (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S7</xref>). The first three nearest neighbors of these eight compounds are almost all antibiotics, and the functions of the three nearest neighbors are almost the same as the corresponding seed of the compound [e.g. the first compound was optimized by the first seed (cefazolin, a cephalosporin antibiotic), and its three nearest neighbors are also cephalosporin antibiotics, which is consistent with the antibiotic function in the seeds]. This further demonstrates the bacteriostatic potential of these eight sampled molecules.</p>
        <p>We also calculated the Tanimoto similarity between these eight compounds and the training set of the main dataset (<xref rid="btad059-F5" ref-type="fig">Fig. 5</xref>). We found that the similarities between the generated molecules and the dataset have a consistent distribution, and the similarities are relatively low, almost all closer to 0 rather than 1, indicating that these molecules are not simply replicating the dataset.</p>
        <fig position="float" id="btad059-F5">
          <label>Fig. 5.</label>
          <caption>
            <p>Tanimoto similarity of the eight sample compounds to the training set of the main dataset. For each seed, histograms show the similarity of molecules sampled from the chemical space generated by GP or FFN fitness optimization to the training set of the main dataset, and lines represent the kernel density estimates for the respective histograms</p>
          </caption>
          <graphic xlink:href="btad059f5" position="float"/>
        </fig>
      </sec>
      <sec>
        <title>3.4.3 Molecular visualization generated along the optimized direction</title>
        <p>The goal of latent space optimization is to make the population move in the direction of stronger inhibition under the guidance of fitness to explore compounds with stronger antibacterial activity. We sampled the molecules generated by latent space exploration in sequential equal steps and observed the similarity of the sampled molecules to the corresponding seeds and their inhibitory properties (<xref rid="sup1" ref-type="supplementary-material">Supplementary Figs S8 and S9</xref>). We found that the population is always able to continue exploring in a more inhibitory direction while guaranteeing a higher similarity to the seed. The corresponding SMILES of the compounds in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figs S8 and S9</xref> are shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S7 and S8</xref>, respectively.</p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>4 Conclusion</title>
    <p>In this article, we report on MDAGS, a generation method of potent antibiotic design. The method incorporates two novel ideas: the encoder and predictor are jointly trained to learn a latent property space, and an attribute-guided optimization strategy is employed in the latent space to enable the model to explore in the direction of the expected properties of the molecule. Doing so aggregates molecules with similar properties, thereby reducing exploration of the vast space and avoiding exploration in directions that may not make sense. In addition, considering that the generator does not contribute to the learning of the latent space, we decouple the encoder and generator, which greatly reduces the model complexity.</p>
    <p>Our findings suggest that MDAGS can serve as a novel and practical method for effective antibiotic design. Visualization of the designed latent space shows that the joint training of the model captures molecular properties so that molecules cluster in spatially distinct locations according to antibacterial activity. Validation of the two fitness functions demonstrates the ability of the model to predict the antibacterial activity of the underlying representation, thereby supporting the optimization of molecules in the potential property space. On the benchmark task of unlabeled molecule generation, MDAGS shows comparable or even better performance than the benchmark model. Furthermore, in the antibiotic molecule generation task, the analysis of the generated molecules shows that our method is able to generate molecules with other properties comparable to existing antibiotics but with better antibacterial activity, and there is a large difference between these generated molecules and existing real data. Finally, the equal-step sampling of the generated results provides insight into how efficiently the method can navigate the property space to find improved molecules with desired properties.</p>
    <p>Given the generality of MDAGS, we believe it will impact other drug discovery cases. For example, designing a more inhibitory antitumor drug requires little to no model changes, just data replacement. However, the method also has limitations. In this work, we only focus on and optimize one property, i.e. antibacterial activity; however, when a drug acts on an organism, many factors need to be considered, such as low toxicity and better pharmacokinetic properties. Our current method does not take these factors into account.</p>
    <p>Overall, our work provides a new approach for the generation and optimization of molecules, especially potent antibiotic compounds. This is undoubtedly of great significance for addressing today’s increasingly serious problem of antibiotic resistance. At the same time, we believe our work will positively impact other drug discovery cases. In future work, we will further explore optimizing our method to consider multi-objective optimization to improve the success rate of compounds in clinical trials.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btad059_Supplementary_Data</label>
      <media xlink:href="btad059_supplementary_data.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>Thanks to all those who maintain excellent databases and to all experimentalists who enabled this work by making their data publicly available.</p>
  </ack>
  <sec>
    <title>Funding</title>
    <p>This work was supported by the National Natural Science Foundation of China [62072353 and 62132015].</p>
    <p><italic toggle="yes">Conflict of Interest</italic>: The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
  </sec>
  <sec sec-type="data-availability">
    <title>Data availability</title>
    <p>The Drugs and Natural Products data underlying this article are avaliable in <ext-link xlink:href="https://github.com/LiangYu-Xidian/MDAGS" ext-link-type="uri">https://github.com/LiangYu-Xidian/MDAGS</ext-link>. The MOSES data underlying this article are avaliable in <ext-link xlink:href="https://github.com/molecularsets/moses" ext-link-type="uri">https://github.com/molecularsets/moses</ext-link>. The GuacaMol data underlying this article are avaliable in <ext-link xlink:href="https://github.com/BenevolentAI/guacamol" ext-link-type="uri">https://github.com/BenevolentAI/guacamol</ext-link>.</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btad059-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Årdal</surname><given-names>C.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>Antibiotic development—economic, regulatory and societal challenges</article-title>. <source>Nat. Rev. Microbiol</source>., <volume>18</volume>, <fpage>267</fpage>–<lpage>274</lpage>.<pub-id pub-id-type="pmid">31745330</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bagal</surname><given-names>V.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>MolGPT: molecular generation using a transformer-decoder model</article-title>. <source>J. Chem. Inf. Model</source>., <volume>62</volume>, <fpage>2064</fpage>–<lpage>2076</lpage>.<pub-id pub-id-type="pmid">34694798</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B3">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Benhenda</surname><given-names>M.</given-names></string-name></person-group> (<year>2017</year>) ChemGAN challenge for drug discovery: can AI reproduce natural chemical diversity? arXiv preprint arXiv:1708.08227.</mixed-citation>
    </ref>
    <ref id="btad059-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Born</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>PaccMann<sup>RL</sup>: de novo generation of hit-like anticancer molecules from transcriptomic data via reinforcement learning</article-title>. <source>iScience</source>, <volume>24</volume>, <fpage>102269</fpage>.<pub-id pub-id-type="pmid">33851095</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brown</surname><given-names>E.D.</given-names></string-name>, <string-name><surname>Wright</surname><given-names>G.D.</given-names></string-name></person-group> (<year>2016</year>) <article-title>Antibacterial drug discovery in the resistance era</article-title>. <source>Nature</source>, <volume>529</volume>, <fpage>336</fpage>–<lpage>343</lpage>.<pub-id pub-id-type="pmid">26791724</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brown</surname><given-names>N.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>GuacaMol: benchmarking models for de novo molecular design</article-title>. <source>J. Chem. Inf. Model</source>., <volume>59</volume>, <fpage>1096</fpage>–<lpage>1108</lpage>.<pub-id pub-id-type="pmid">30887799</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Button</surname><given-names>A.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Automated de novo molecular design by hybrid machine intelligence and rule-driven chemical synthesis</article-title>. <source>Nat. Mach. Intell</source>., <volume>1</volume>, <fpage>307</fpage>–<lpage>315</lpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Camacho</surname><given-names>D.M.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>Next-generation machine learning for biological networks</article-title>. <source>Cell</source>, <volume>173</volume>, <fpage>1581</fpage>–<lpage>1592</lpage>.<pub-id pub-id-type="pmid">29887378</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B9">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>Z.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) CELLS: cost-effective evolution in latent space for goal-directed molecular generation. arXiv preprint arXiv:2112.00905.</mixed-citation>
    </ref>
    <ref id="btad059-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dao</surname><given-names>F.Y.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>DeepYY1: a deep learning approach to identify YY1-mediated chromatin loops</article-title>. <source>Brief. Bioinform</source>., <volume>22</volume>, <fpage>bbaa356</fpage>.<pub-id pub-id-type="pmid">33279983</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Das</surname><given-names>P.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Accelerated antimicrobial discovery via deep generative models and molecular dynamics simulations</article-title>. <source>Nat. Biomed. Eng</source>., <volume>5</volume>, <fpage>613</fpage>–<lpage>623</lpage>.<pub-id pub-id-type="pmid">33707779</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B12">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>De Cao</surname><given-names>N.</given-names></string-name>, <string-name><surname>Kipf</surname><given-names>T.</given-names></string-name></person-group> (<year>2018</year>) MolGAN: an implicit generative model for small molecular graphs. arXiv preprint arXiv:1805.11973.</mixed-citation>
    </ref>
    <ref id="btad059-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Godinez</surname><given-names>W.J.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>Design of potent antimalarials with generative chemistry</article-title>. <source>Nat. Mach. Intell</source>., <volume>4</volume>, <fpage>180</fpage>–<lpage>186</lpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gómez-Bombarelli</surname><given-names>R.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>Automatic chemical design using a data-driven continuous representation of molecules</article-title>. <source>ACS Cent. Sci</source>., <volume>4</volume>, <fpage>268</fpage>–<lpage>276</lpage>.<pub-id pub-id-type="pmid">29532027</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Griffiths</surname><given-names>R.-R.</given-names></string-name>, <string-name><surname>Hernández-Lobato</surname><given-names>J.M.</given-names></string-name></person-group> (<year>2020</year>) <article-title>Constrained Bayesian optimization for automatic chemical design using variational autoencoders</article-title>. <source>Chem. Sci</source>., <volume>11</volume>, <fpage>577</fpage>–<lpage>586</lpage>.<pub-id pub-id-type="pmid">32190274</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hoffman</surname><given-names>S.C.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>Optimizing molecules using efficient queries from property evaluations</article-title>. <source>Nat. Mach. Intell</source>., <volume>4</volume>, <fpage>21</fpage>–<lpage>31</lpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Irwin</surname><given-names>J.J.</given-names></string-name>, <string-name><surname>Shoichet</surname><given-names>B.K.</given-names></string-name></person-group> (<year>2005</year>) <article-title>ZINC− a free database of commercially available compounds for virtual screening</article-title>. <source>J. Chem. Inf. Model</source>., <volume>45</volume>, <fpage>177</fpage>–<lpage>182</lpage>.<pub-id pub-id-type="pmid">15667143</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ismail</surname><given-names>S.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Pan-vaccinomics approach towards a universal vaccine candidate against WHO priority pathogens to address growing global antibiotic resistance</article-title>. <source>Comput. Biol. Med</source>., <volume>136</volume>, <fpage>104705</fpage>.<pub-id pub-id-type="pmid">34340127</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jiménez-Luna</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>DeltaDelta neural networks for lead optimization of small molecule potency</article-title>. <source>Chem. Sci</source>., <volume>10</volume>, <fpage>10911</fpage>–<lpage>10918</lpage>.<pub-id pub-id-type="pmid">32190246</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B20">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Jin</surname><given-names>W.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) Junction tree variational autoencoder for molecular graph generation. In: <italic toggle="yes">International Conference on Machine Learning</italic>, Stockholm, Sweden. pp. <fpage>2323</fpage>–<lpage>2332</lpage>. <publisher-name>PMLR</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btad059-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Joshi</surname><given-names>P.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>An ensembled SVM based approach for predicting adverse drug reactions</article-title>. <source>Curr. Bioinform</source>., <volume>16</volume>, <fpage>422</fpage>–<lpage>432</lpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kotsias</surname><given-names>P.-C.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>Direct steering of de novo molecular generation with descriptor conditional recurrent neural networks</article-title>. <source>Nat. Mach. Intell</source>., <volume>2</volume>, <fpage>254</fpage>–<lpage>265</lpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Krishnan</surname><given-names>S.R.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>De novo structure-based drug design using deep learning</article-title>. <source>J. Chem. Inf. Model</source>., <volume>61</volume>, <fpage>621</fpage>–<lpage>630</lpage>.<pub-id pub-id-type="pmid">33491455</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kullback</surname><given-names>S.</given-names></string-name>, <string-name><surname>Leibler</surname><given-names>R.A.</given-names></string-name></person-group> (<year>1951</year>) <article-title>On information and sufficiency</article-title>. <source>Ann. Math. Stat</source>., <volume>22</volume>, <fpage>79</fpage>–<lpage>86</lpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kwon</surname><given-names>Y.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Efficient learning of non-autoregressive graph variational autoencoders for molecular graph generation</article-title>. <source>J. Cheminform</source>., <volume>11</volume>, <fpage>1</fpage>–<lpage>10</lpage>.<pub-id pub-id-type="pmid">30604073</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>LeCun</surname><given-names>Y.</given-names></string-name></person-group><etal>et al</etal> (<year>2015</year>) <article-title>Deep learning</article-title>. <source>Nature</source>, <volume>521</volume>, <fpage>436</fpage>–<lpage>444</lpage>.<pub-id pub-id-type="pmid">26017442</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname><given-names>Y.J.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Generative adversarial networks for de novo molecular design</article-title>. <source>Mol. Inf</source>., <volume>40</volume>, <fpage>2100045</fpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>W.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Surface design for antibacterial materials: from fundamentals to advanced strategies</article-title>. <source>Adv. Sci</source>., <volume>8</volume>, <fpage>2100368</fpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>Y.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>Multi-objective de novo drug design with conditional graph generative model</article-title>. <source>J. Cheminform</source>., <volume>10</volume>, <fpage>1</fpage>–<lpage>24</lpage>.<pub-id pub-id-type="pmid">29340790</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lv</surname><given-names>H.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>DeepKla: an attention mechanism-based deep neural network for protein lysine lactylation site prediction</article-title>. <source>iMeta</source>, <volume>1</volume>, <fpage>e11</fpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Maziarka</surname><given-names>Ł.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>Mol-CycleGAN: a generative model for molecular optimization</article-title>. <source>J. Cheminform</source>., <volume>12</volume>, <fpage>2</fpage>–<lpage>18</lpage>.<pub-id pub-id-type="pmid">33431006</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mendez</surname><given-names>D.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>ChEMBL: towards direct deposition of bioassay data</article-title>. <source>Nucleic Acids Res</source>., <volume>47</volume>, <fpage>D930</fpage>–<lpage>D940</lpage>.<pub-id pub-id-type="pmid">30398643</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Miethke</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Towards the sustainable discovery and development of new antibiotics</article-title>. <source>Nat. Rev. Chem</source>., <volume>5</volume>, <fpage>726</fpage>–<lpage>749</lpage>.<pub-id pub-id-type="pmid">34426795</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mulpuru</surname><given-names>V.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>HAMP: a knowledgebase of antimicrobial peptides from human microbiome</article-title>. <source>Curr. Bioinform</source>., <volume>16</volume>, <fpage>534</fpage>–<lpage>540</lpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Murray</surname><given-names>C.J.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>Global burden of bacterial antimicrobial resistance in 2019: a systematic analysis</article-title>. <source>Lancet</source>, <volume>399</volume>, <fpage>629</fpage>–<lpage>655</lpage>.<pub-id pub-id-type="pmid">35065702</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nieuwlaat</surname><given-names>R.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Coronavirus disease 2019 and antimicrobial resistance: parallel and interacting health emergencies</article-title>. <source>Clin. Infect. Dis</source>., <volume>72</volume>, <fpage>1657</fpage>–<lpage>1659</lpage>.<pub-id pub-id-type="pmid">32544232</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Olivecrona</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) <article-title>Molecular de-novo design through deep reinforcement learning</article-title>. <source>J. Cheminform</source>., <volume>9</volume>, <fpage>1</fpage>–<lpage>14</lpage>.<pub-id pub-id-type="pmid">28316652</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Polykovskiy</surname><given-names>D.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>Molecular sets (MOSES): a benchmarking platform for molecular generation models</article-title>. <source>Front. Pharmacol</source>., <volume>11</volume>, <fpage>565644</fpage>.<pub-id pub-id-type="pmid">33390943</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Popova</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>Deep reinforcement learning for de novo drug design</article-title>. <source>Sci. Adv</source>., <volume>4</volume>, <fpage>eaap7885</fpage>.<pub-id pub-id-type="pmid">30050984</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Preuer</surname><given-names>K.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>Fréchet ChemNet distance: a metric for generative models for molecules in drug discovery</article-title>. <source>J. Chem. Inf. Model</source>., <volume>58</volume>, <fpage>1736</fpage>–<lpage>1741</lpage>.<pub-id pub-id-type="pmid">30118593</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Probst</surname><given-names>D.</given-names></string-name>, <string-name><surname>Reymond</surname><given-names>J.-L.</given-names></string-name></person-group> (<year>2018</year>) <article-title>FUn: a framework for interactive visualizations of large, high-dimensional datasets on the web</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>1433</fpage>–<lpage>1435</lpage>.<pub-id pub-id-type="pmid">29186333</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Putin</surname><given-names>E.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>Reinforced adversarial neural computer for de novo molecular design</article-title>. <source>J. Chem. Inf. Model</source>., <volume>58</volume>, <fpage>1194</fpage>–<lpage>1204</lpage>.<pub-id pub-id-type="pmid">29762023</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ru</surname><given-names>X.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>Exploration of the correlation between GPCRs and drugs based on a learning to rank algorithm</article-title>. <source>Comput. Biol. Med</source>., <volume>119</volume>, <fpage>103660</fpage>.<pub-id pub-id-type="pmid">32090901</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ru</surname><given-names>X.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>NerLTR-DTA: drug-target binding affinity prediction based on neighbor relationship and learning to rank</article-title>. <source>Bioinformatics</source>, <volume>38</volume>, <fpage>1964</fpage>–<lpage>1971</lpage>.<pub-id pub-id-type="pmid">35134828</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sanchez-Lengeling</surname><given-names>B.</given-names></string-name>, <string-name><surname>Aspuru-Guzik</surname><given-names>A.</given-names></string-name></person-group> (<year>2018</year>) <article-title>Inverse molecular design using machine learning: generative models for matter engineering</article-title>. <source>Science</source>, <volume>361</volume>, <fpage>360</fpage>–<lpage>365</lpage>.<pub-id pub-id-type="pmid">30049875</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Segler</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2018a</year>) Generating focussed molecule libraries for drug discovery with recurrent neural networks. <italic toggle="yes">ACS central science</italic>, <volume>4</volume>(1): <fpage>120</fpage>–<lpage>131</lpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Segler</surname><given-names>M.H.</given-names></string-name></person-group><etal>et al</etal> (<year>2018b</year>) <article-title>Planning chemical syntheses with deep neural networks and symbolic AI</article-title>. <source>Nature</source>, <volume>555</volume>, <fpage>604</fpage>–<lpage>610</lpage>.<pub-id pub-id-type="pmid">29595767</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shahriari</surname><given-names>B.</given-names></string-name></person-group><etal>et al</etal> (<year>2016</year>) <article-title>Taking the human out of the loop: a review of Bayesian optimization</article-title>. <source>Proc. IEEE</source>, <volume>104</volume>, <fpage>148</fpage>–<lpage>175</lpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B49">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shaker</surname><given-names>B.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Introduction of advanced methods for structure-based drug discovery</article-title>. <source>Curr. Bioinform</source>., <volume>16</volume>, <fpage>351</fpage>–<lpage>363</lpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B50">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Simonovsky</surname><given-names>M.</given-names></string-name>, <string-name><surname>Komodakis</surname><given-names>N.</given-names></string-name></person-group> (<year>2018</year>) GraphVAE: towards generation of small graphs using variational autoencoders. In: <italic toggle="yes">International Conference on Artificial Neural Networks</italic>, Rhodes, Greece. pp. <fpage>412</fpage>–<lpage>422</lpage>. <publisher-name>Springer</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btad059-B51">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Skalic</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Shape-based generative modeling for de novo drug design</article-title>. <source>J. Chem. Inf. Model</source>., <volume>59</volume>, <fpage>1205</fpage>–<lpage>1214</lpage>.<pub-id pub-id-type="pmid">30762364</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B52">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stokes</surname><given-names>J.M.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>A deep learning approach to antibiotic discovery</article-title>. <source>Cell</source>, <volume>180</volume>, <fpage>688</fpage>–<lpage>702.e13</lpage>.<pub-id pub-id-type="pmid">32084340</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B53">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Su</surname><given-names>R.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Deep-Resp-Forest: a deep forest model to predict anti-cancer drug response</article-title>. <source>Methods (San Diego, Calif.)</source>, <volume>166</volume>, <fpage>91</fpage>–<lpage>102</lpage>.<pub-id pub-id-type="pmid">30772464</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B54">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sugden</surname><given-names>R.</given-names></string-name></person-group><etal>et al</etal> (<year>2016</year>) <article-title>Combatting antimicrobial resistance globally</article-title>. <source>Nat. Microbiol</source>., <volume>1</volume>, <fpage>1</fpage>–<lpage>2</lpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B55">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Theuretzbacher</surname><given-names>U.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>The global preclinical antibacterial pipeline</article-title>. <source>Nat. Rev. Microbiol</source>., <volume>18</volume>, <fpage>275</fpage>–<lpage>285</lpage>.<pub-id pub-id-type="pmid">31745331</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B56">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Tu</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Coley</surname><given-names>C.W.</given-names></string-name></person-group> (<year>2021</year>) Permutation invariant graph-to-sequence model for template-free retrosynthesis and reaction prediction. arXiv preprint arXiv:2110.09681.</mixed-citation>
    </ref>
    <ref id="btad059-B57">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>von Lilienfeld</surname><given-names>O.A.</given-names></string-name>, <string-name><surname>Burke</surname><given-names>K.</given-names></string-name></person-group> (<year>2020</year>) <article-title>Retrospective on a decade of machine learning for chemical discovery</article-title>. <source>Nat. Commun</source>., <volume>11</volume>, <fpage>1</fpage>–<lpage>4</lpage>.<pub-id pub-id-type="pmid">31911652</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B58">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Multi-constraint molecular generation based on conditional transformer, knowledge distillation and reinforcement learning</article-title>. <source>Nat. Mach. Intell</source>., <volume>3</volume>, <fpage>914</fpage>–<lpage>922</lpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B59">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Williams</surname><given-names>C.K.</given-names></string-name>, <string-name><surname>Rasmussen</surname><given-names>C.E.</given-names></string-name></person-group> (<year>2006</year>) <source>Gaussian Processes for Machine Learning</source>. <publisher-name>MIT Press</publisher-name>, <publisher-loc>Cambridge, MA</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btad059-B60">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Xu</surname><given-names>K.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) Graph2Seq: graph to sequence learning with attention-based neural networks. arXi preprint arXiv:1804.00823.</mixed-citation>
    </ref>
    <ref id="btad059-B61">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yang</surname><given-names>K.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Analyzing learned molecular representations for property prediction</article-title>. <source>J. Chem. Inf. Model</source>., <volume>59</volume>, <fpage>3370</fpage>–<lpage>3388</lpage>.<pub-id pub-id-type="pmid">31361484</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B62">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yang</surname><given-names>Q.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>An adaptive stochastic dominant learning swarm optimizer for high-dimensional optimization</article-title>. <source>IEEE Trans. Cybern</source>., <volume>99</volume>, <fpage>1</fpage>–<lpage>17</lpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B63">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>You</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) Graph convolutional policy network for goal-directed molecular graph generation. In: <italic toggle="yes">Advances in Neural Information Processing Systems</italic>, Montréal, Canada. p.<fpage>31</fpage>.</mixed-citation>
    </ref>
    <ref id="btad059-B64">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zampieri</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) <article-title>Nontargeted metabolomics reveals the multilevel response to antibiotic perturbations</article-title>. <source>Cell Rep</source>., <volume>19</volume>, <fpage>1214</fpage>–<lpage>1228</lpage>.<pub-id pub-id-type="pmid">28494870</pub-id></mixed-citation>
    </ref>
    <ref id="btad059-B65">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zulfiqar</surname><given-names>H.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>Deep-4mCGP: a deep learning approach to predict 4mC sites in <italic toggle="yes">Geobacter pickeringii</italic> by using correlation-based feature selection technique</article-title>. <source>Int. J. Mol. Sci</source>., <volume>23</volume>, <fpage>1251</fpage>.<pub-id pub-id-type="pmid">35163174</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
