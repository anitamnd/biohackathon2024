<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Psychol</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Psychol</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Psychol.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Psychology</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1664-1078</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">5440596</article-id>
    <article-id pub-id-type="doi">10.3389/fpsyg.2017.00696</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Psychology</subject>
        <subj-group>
          <subject>Methods</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Testing Separability and Independence of Perceptual Dimensions with General Recognition Theory: A Tutorial and New R Package (<italic>grtools</italic>)</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Soto</surname>
          <given-names>Fabian A.</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="author-notes" rid="fn001">
          <sup>*</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/79891/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zheng</surname>
          <given-names>Emily</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/384123/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Fonseca</surname>
          <given-names>Johnny</given-names>
        </name>
        <xref ref-type="aff" rid="aff3">
          <sup>3</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/433132/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ashby</surname>
          <given-names>F. Gregory</given-names>
        </name>
        <xref ref-type="aff" rid="aff4">
          <sup>4</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/110763/overview"/>
      </contrib>
    </contrib-group>
    <aff id="aff1">
      <sup>1</sup>
      <institution>Department of Psychology, Florida International University, Miami</institution>
      <country>FL, USA</country>
    </aff>
    <aff id="aff2">
      <sup>2</sup>
      <institution>Department of Statistics and Applied Probability, University of California, Santa Barbara, Santa Barbara</institution>
      <country>CA, USA</country>
    </aff>
    <aff id="aff3">
      <sup>3</sup>
      <institution>Department of Mathematics and Statistics, Florida International University, Miami</institution>
      <country>FL, USA</country>
    </aff>
    <aff id="aff4">
      <sup>4</sup>
      <institution>Department of Psychological and Brain Sciences, University of California, Santa Barbara, Santa Barbara</institution>
      <country>CA, USA</country>
    </aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: <italic>Thomas Serre, Brown University, USA</italic></p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: <italic>Michael L. Mack, University of Toronto, Canada; John Ritchie, KU Leuven, Belgium</italic></p>
      </fn>
      <corresp id="fn001">*Correspondence: <italic>Fabian A. Soto, <email xlink:type="simple">fabian.soto@fiu.edu</email></italic></corresp>
      <fn fn-type="other" id="fn002">
        <p>This article was submitted to Perception Science, a section of the journal Frontiers in Psychology</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>23</day>
      <month>5</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2017</year>
    </pub-date>
    <volume>8</volume>
    <elocation-id>696</elocation-id>
    <history>
      <date date-type="received">
        <day>10</day>
        <month>10</month>
        <year>2016</year>
      </date>
      <date date-type="accepted">
        <day>21</day>
        <month>4</month>
        <year>2017</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2017 Soto, Zheng, Fonseca and Ashby.</copyright-statement>
      <copyright-year>2017</copyright-year>
      <copyright-holder>Soto, Zheng, Fonseca and Ashby</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) or licensor are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Determining whether perceptual properties are processed independently is an important goal in perceptual science, and tools to test independence should be widely available to experimental researchers. The best analytical tools to test for perceptual independence are provided by General Recognition Theory (GRT), a multidimensional extension of signal detection theory. Unfortunately, there is currently a lack of software implementing GRT analyses that is ready-to-use by experimental psychologists and neuroscientists with little training in computational modeling. This paper presents <italic>grtools</italic>, an R package developed with the explicit aim of providing experimentalists with the ability to perform full GRT analyses using only a couple of command lines. We describe the software and provide a practical tutorial on how to perform each of the analyses available in <italic>grtools</italic>. We also provide advice to researchers on best practices for experimental design and interpretation of results when applying GRT and <italic>grtools</italic></p>
    </abstract>
    <kwd-group>
      <kwd>general recognition theory</kwd>
      <kwd>R package</kwd>
      <kwd>perceptual separability</kwd>
      <kwd>perceptual independence</kwd>
      <kwd>Garner interference</kwd>
      <kwd>identification task</kwd>
      <kwd>decisional separability</kwd>
      <kwd>signal detection theory</kwd>
    </kwd-group>
    <counts>
      <fig-count count="10"/>
      <table-count count="1"/>
      <equation-count count="0"/>
      <ref-count count="46"/>
      <page-count count="18"/>
      <word-count count="0"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>Introduction</title>
    <p>In perceptual science, an important amount of effort has been dedicated to understanding what aspects of stimuli are represented independently (e.g., <xref rid="B8" ref-type="bibr">Bruce and Young, 1986</xref>; <xref rid="B43" ref-type="bibr">Ungerleider and Haxby, 1994</xref>; <xref rid="B16" ref-type="bibr">Haxby et al., 2000</xref>; <xref rid="B22" ref-type="bibr">Kanwisher, 2000</xref>; <xref rid="B44" ref-type="bibr">Vogels et al., 2001</xref>; <xref rid="B41" ref-type="bibr">Stankiewicz, 2002</xref>; <xref rid="B32" ref-type="bibr">Op de Beeck et al., 2008</xref>). Independent processing of two stimulus properties is interesting, because it implies that those properties are given priority by the perceptual system, which allocates a specific set of resources to the processing of each. On the other hand, some properties are said to be processed “holistically” or “configurally,” which is equivalent to saying that they cannot be processed independently (e.g., <xref rid="B42" ref-type="bibr">Thomas, 2001</xref>; <xref rid="B34" ref-type="bibr">Richler et al., 2008</xref>; <xref rid="B29" ref-type="bibr">Mestry et al., 2012</xref>). Such holistic processing is also important to understand perception (see <xref rid="B12" ref-type="bibr">Farah et al., 1998</xref>; <xref rid="B28" ref-type="bibr">Maurer et al., 2002</xref>; <xref rid="B35" ref-type="bibr">Richler et al., 2012</xref>). In sum, determining whether perceptual properties are processed independently is an important goal in perception science, and tools to test independence should be widely available to experimental researchers.</p>
    <p>Currently, the best analytical tools to test for independence of perceptual processing are provided by General Recognition Theory (GRT; <xref rid="B5" ref-type="bibr">Ashby and Townsend, 1986</xref>; for a tutorial review, see <xref rid="B4" ref-type="bibr">Ashby and Soto, 2015</xref>). GRT is an extension of signal detection theory to cases in which stimuli vary along two or more dimensions. It offers a framework in which different types of dimensional interactions can be defined formally and studied, while inheriting from signal detection theory the ability to dissociate perceptual from decisional sources for such interactions. Still, the implementation of GRT analyses requires an important amount of technical knowledge, so this task has been almost exclusively performed by computational modelers. The lack of software implementing GRT analyses that is ready-to-use by untrained researchers has probably reduced the application of GRT among experimental psychologists and neuroscientists.</p>
    <p>This paper presents <italic>grtools</italic>, an R package developed with the explicit aim of providing experimentalists with the ability to perform full GRT analyses using only a couple of command lines. We describe the software and provide a practical tutorial on how to perform each of the analyses available in <italic>grtools</italic>. The goal is to give a step-by-step guide for researchers interested in applying GRT to their own research problems. Readers interested in the theory behind these analyses should consult the recent review by <xref rid="B4" ref-type="bibr">Ashby and Soto (2015)</xref> and the papers referenced therein.</p>
  </sec>
  <sec>
    <title>General Recognition Theory</title>
    <p>General recognition theory is a multivariate extension of signal detection theory to cases in which stimuli vary on more than one dimension (<xref rid="B5" ref-type="bibr">Ashby and Townsend, 1986</xref>; for a review, see <xref rid="B4" ref-type="bibr">Ashby and Soto, 2015</xref>). As in signal detection theory, GRT assumes that different presentations of the same stimulus produce slightly different perceptual representations. However, because stimuli vary in multiple dimensions, perceptual representations vary along multiple dimensions at the same time. For example, imagine that you are interested in the dimensions of gender (male vs. female) and emotional expression (neutral vs. sad) in faces. A single presentation of a happy female (<bold>Figure <xref ref-type="fig" rid="F1">1A</xref></bold>) would produce a perceptual effect—a single point in the two-dimensional space of perceived gender and emotional expression. <bold>Figure <xref ref-type="fig" rid="F1">1B</xref></bold> shows an example of such a two-dimensional perceptual space; each point in the figure represents a percept evoked by a single stimulus presentation, and the tick marks on the axes represent the corresponding percept values on both perceptual dimensions. The green dotted lines show this correspondence between points and tick marks more clearly for a single perceptual effect.</p>
    <fig id="F1" position="float">
      <label>FIGURE 1</label>
      <caption>
        <p><bold>Stimulus representation in general recognition theory</bold>. For <bold>(A)</bold> faces varying in gender (male or female) and emotional expression (sad or happy), the representation is two-dimensional. Each stimulus presentation produces <bold>(B)</bold> a point in the two-dimensional space. The whole distribution of such perceptual effects can be described through <bold>(C)</bold> a two-dimensional probability distribution. Face photograph in <bold>(A)</bold> obtained from the Extended Cohn-Kanade (CK+) database (S74, Jeffrey Cohn; <xref rid="B21" ref-type="bibr">Kanade et al., 2000</xref>; <xref rid="B25" ref-type="bibr">Lucey et al., 2010</xref>), consent for publication obtained by the original authors.</p>
      </caption>
      <graphic xlink:href="fpsyg-08-00696-g001"/>
    </fig>
    <p>Thus, the representation of a happy female face in <bold>Figure <xref ref-type="fig" rid="F1">1</xref></bold> is probabilistic across trials and can be summarized through a probability distribution. Assuming that this distribution is a two-dimensional Gaussian (a common assumption in GRT, as well as in signal detection theory and many other statistical models), we can represent it graphically by the ellipse in <bold>Figure <xref ref-type="fig" rid="F1">1C</xref></bold>, which is the shape of the cloud of points that are produced by the presented face. The plus sign inside the ellipse represents the mean of the distribution. The bell-shaped unidimensional Gaussians plotted on the axes of <bold>Figure <xref ref-type="fig" rid="F1">1C</xref></bold> represent the distribution of tick marks along a single dimension. These are called <italic>marginal distributions</italic>.</p>
    <sec>
      <title>Forms of Independence Defined within GRT</title>
      <p><bold>Figure <xref ref-type="fig" rid="F2">2A</xref></bold> shows two distributions that are equivalent except for the correlation between the values of percepts on the two dimensions. The distribution to the left shows no correlation; this is a case of <italic>perceptual independence</italic> (PI), where the perceived value of expression does not depend on the perceived value of gender. The distribution to the right shows a positive correlation, which is an example of a <italic>failure of PI.</italic> In this case, whenever the face is presented, the more it is perceived as female-looking, the more likely it will also be perceived as more sad. It is important to note that PI is defined for a single stimulus.</p>
      <fig id="F2" position="float">
        <label>FIGURE 2</label>
        <caption>
          <p><bold>Schematic representation of different forms of interaction between dimensions defined in general recognition theory: (A)</bold> perceptual independence, <bold>(B)</bold> perceptual separability, and <bold>(C)</bold> decisional separability.</p>
        </caption>
        <graphic xlink:href="fpsyg-08-00696-g002"/>
      </fig>
      <p>Another important concept in GRT is related to how two or more stimuli are perceived. Each stimulus with a unique combination of gender and emotional expression can be represented by its own probability distribution. For example, <bold>Figure <xref ref-type="fig" rid="F2">2B</xref></bold> shows perceptual distributions for two happy faces that differ in their gender. Note how the distributions in the left panel are aligned on the <italic>x</italic>-axis, representing expression, and how their marginal distributions overlap. This is a case of <italic>perceptual separability</italic> (PS) of emotional expression from gender; the perception of happiness is not affected by a change in gender. The right panel of <bold>Figure <xref ref-type="fig" rid="F2">2B</xref></bold> shows an example of the opposite case, a <italic>failure of PS</italic>. In this case, the two distributions do not align and their marginal distributions do not overlap. The male face is perceived as more “happy” than the female face.</p>
      <p>In a typical behavioral experiment, participants would be asked to identify something about the presented stimulus, like a face’s expression, gender, or both. According to GRT, participants achieve this by dividing the perceptual space into response regions. For example, a participant asked to report gender might divide the perceptual space as shown in <bold>Figure <xref ref-type="fig" rid="F2">2C</xref></bold>, using a linear bound. When a percept lands in the left area, the participant reports “happy,” and when the percept lands in the right area, the participant reports “sad.” Note how the decision bound in the left panel of <bold>Figure <xref ref-type="fig" rid="F2">2C</xref></bold>, which is orthogonal to the expression dimension, divides the space in the same way across all values of gender. This is a case of <italic>decisional separability</italic> (DS) of emotional expression from gender; the decisions about expression are not affected by the face’s gender. The right panel of <bold>Figure <xref ref-type="fig" rid="F2">2C</xref></bold> shows an example of the opposite case, a <italic>failure of DS.</italic> In this case, the bound is tilted instead of orthogonal, and the area given to the “happy” response is much smaller in the “male” end of the identity dimension than in the “female” end. That is, the observer is biased to answer “happy” more often for the female faces than for male faces.</p>
    </sec>
    <sec>
      <title>The 2 × 2 Identification Task</title>
      <p>The most widely used task to study the independence of stimulus dimensions using GRT is the 2 × 2 identification task. On each trial of an identification task, a stimulus is presented and it must be identified by pressing a specific response button. Each stimulus must have a value on at least two dimensions or features (that we want to test for independence), A and B. If there are only two values per dimension, we obtain the 2 × 2 design with stimuli A<sub>1</sub>B<sub>1</sub>, A<sub>1</sub>B<sub>2</sub>, A<sub>2</sub>B<sub>1</sub>, and A<sub>2</sub>B<sub>2</sub>.</p>
      <p>For example, consider a 2 × 2 face identification experiment where the two varying dimensions are face emotional expression (dimension A) and gender (dimension B). Assume that the levels for the emotion dimension are happy (A<sub>1</sub>) and sad (A<sub>2</sub>), whereas the levels for the gender dimension are male (B<sub>1</sub>) and female (B<sub>2</sub>). Thus, a 2 × 2 identification task would create four face stimuli: happy-male (A<sub>1</sub>B<sub>1</sub>), sad-male (A<sub>1</sub>B<sub>2</sub>), happy-female (A<sub>2</sub>B<sub>1</sub>), and sad female (A<sub>2</sub>B<sub>2</sub>). On a given trial, a participant is shown one of these faces, and must identify the face accordingly. <bold>Figure <xref ref-type="fig" rid="F3">3A</xref></bold> illustrates a hypothetical GRT model for this example. In the figure, gender is perceptually separable from emotional expression, as indicated by the overlapping marginal distributions along the <italic>y</italic>-axis. On the other hand, emotional expression is not perceptually separable from gender, as the marginal distributions along the <italic>x</italic>-axis are not overlapping. There is a violation of PI for the sad-female stimulus (green distribution, showing a positive correlation), but not for the other stimuli. DS holds for emotional expression, as the bound used for this classification is orthogonal to the <italic>y</italic>-axis, but it does not hold for gender, as the bound used for this classification is not orthogonal to the <italic>x</italic>-axis.</p>
      <fig id="F3" position="float">
        <label>FIGURE 3</label>
        <caption>
          <p><bold>Illustration of a General Recognition Theory (GRT) model for the (A)</bold> 2 × 2 identification task and the <bold>(B)</bold> 2 × 2 Garner filtering task. Both models use the exact same stimulus representations, but the decision bounds represent the different demands of the tasks. The identification task requires dividing the space into four response regions, one for each stimulus, whereas the Garner filtering task requires dividing the space into two regions, one for each level of the relevant dimension.</p>
        </caption>
        <graphic xlink:href="fpsyg-08-00696-g003"/>
      </fig>
      <p>Due to its simplicity, the 2 × 2 task is by far the most widely used design in the field. It requires presenting only four stimuli and measuring only four responses. Other designs, such as a 3 × 3 task (two dimensions, each with three levels), are also possible. However, these tasks are less practical because the considerable learning required by participants means that the experiment usually will require multiple experimental sessions (e.g., 5 days in Experiment 1 of <xref rid="B2" ref-type="bibr">Ashby and Lee, 1991</xref>). The working memory load of a 3 × 3 task is also taxing, as participants are required to remember nine unique stimuli and their unique responses. Because of these and other reasons (see <xref rid="B4" ref-type="bibr">Ashby and Soto, 2015</xref>; <xref rid="B39" ref-type="bibr">Soto et al., 2015</xref>), functions in the <italic>grtools</italic> package were developed specifically to deal with the 2 × 2 identification task.</p>
      <p>The most common way to analyze the data from an identification task is through a <italic>summary statistics</italic> analysis, in which the researcher draws inferences about PI, PS, and DS by using summary statistics like proportion of correct responses, and measures of sensitivity and bias. An introductory tutorial about the exact statistics computed in this analysis can be found elsewhere (<xref rid="B4" ref-type="bibr">Ashby and Soto, 2015</xref>). For a rigorous treatment of the theory behind these analyses, see the following references: (<xref rid="B5" ref-type="bibr">Ashby and Townsend, 1986</xref>; <xref rid="B19" ref-type="bibr">Kadlec and Townsend, 1992a</xref>,<xref rid="B20" ref-type="bibr">b</xref>). Below, we focus only on how to obtain the results from summary statistics analyses using <italic>grtools</italic> and how they should be interpreted.</p>
      <p>A second approach is <italic>model-based analyses</italic>, which consist of fitting one or more GRT models to the data obtained from an identification experiment. In traditional model-based analyses, several models are fit and then model selection approaches are used to determine which of those models provides the best account of the data (<xref rid="B2" ref-type="bibr">Ashby and Lee, 1991</xref>; <xref rid="B42" ref-type="bibr">Thomas, 2001</xref>). The models used implement different assumptions about PS, PI, and DS. Thus, selecting a particular model is equivalent to selecting those assumptions that explain the data best. An introductory tutorial to model fitting and selection with traditional GRT models of the 2 × 2 identification task can be found elsewhere (<xref rid="B4" ref-type="bibr">Ashby and Soto, 2015</xref>). Below, we focus on showing how to easily obtain the results from such a procedure using <italic>grtools</italic>.</p>
      <p>Several problems have been identified with traditional GRT model-based analyses. The most important are that traditional analyses (1) do not allow a clear dissociation between PS and DS (<xref rid="B37" ref-type="bibr">Silbert and Thomas, 2013</xref>), (2) do not allow the full model to be fit to the data (because the full model has too many free parameters), and (3) are prone to over-fitting the data (<xref rid="B39" ref-type="bibr">Soto et al., 2015</xref>). All these problems are solved by the recently proposed General Recognition Theory with Individual Differences (GRT-wIND) model (<xref rid="B39" ref-type="bibr">Soto et al., 2015</xref>). GRT-wIND is fit simultaneously to the data from all participants in a particular experiment, because it assumes that some aspects of perception are common to all people. In particular, the model assumes that PS and PI either hold or do not hold across all participants, and that all participants perceive the same set of stimuli in a similar manner. Other aspects of the model are not common to all participants, but reflect individual differences. The model assumes individual differences in decisional strategies (e.g., in whether DS holds) and in the level to which people pay more or less attention to different dimensions. We recommend using GRT-wIND rather than traditional GRT models for the analysis of 2 × 2 identification experiments, as currently this is the only way to dissociate between perceptual and decisional processes in the 2 × 2 design.</p>
    </sec>
    <sec>
      <title>The 2 × 2 Garner Filtering Task</title>
      <p>A task related to the 2 × 2 identification task is the 2 × 2 Garner filtering task (<xref rid="B13" ref-type="bibr">Garner, 1974</xref>; for a review, see <xref rid="B1" ref-type="bibr">Algom and Fitousi, 2016</xref>). The stimuli are identical to those used in the identification task, but the Garner filtering task asks participants to classify stimuli based on their value on a single dimension, while ignoring stimulus values on the second dimension. For example, a 2 × 2 Garner filtering task may require participants to classify faces according to their gender while ignoring their emotional expression (gender task), or to classify faces according to emotional expression while ignoring their gender (emotion task). This differs slightly yet significantly from the identification task; instead of identifying a unique stimulus, participants are now asked to group the stimuli based on a single dimension (e.g., gender or emotion).</p>
      <p><bold>Figure <xref ref-type="fig" rid="F3">3B</xref></bold> illustrates a hypothetical GRT model for a Garner filtering task in which emotional expression is the relevant classification dimension, and gender is the irrelevant dimension. Assuming that the stimuli are the same as those from our previous example of the 2 × 2 identification task, the perceptual representations are also the same in this model. The main difference is in the decision bounds: rather than two bounds dividing space into four areas (one response per stimulus), there is now a single bound dividing space into only two areas (one response per expression level).</p>
      <p>There are two important conditions in a Garner filtering task, that are presented to participants in separated experimental blocks. In baseline blocks, the value of the irrelevant dimension (the dimension that participants must ignore) is fixed across trials. For example, participants may have to classify faces according to their gender, while emotional expression is fixed at “happy.” In filtering blocks, the value of the irrelevant dimension is varied across trials. For example, participants may have to classify faces according to their gender, while emotional expression randomly varies between “happy” and “sad.”</p>
      <p>If participants have difficulty separating one dimension from another, then a “Garner interference effect” (<xref rid="B13" ref-type="bibr">Garner, 1974</xref>) is expected: slower response times and lower accuracy are likely to occur on filtering blocks, compared to baseline blocks. Accuracy in the Garner filtering task is usually at ceiling levels, so most studies have measured the Garner interference effect in terms of response times.</p>
      <p>Because response times are the most common dependent variable used for the filtering task, using GRT for data analysis requires making further assumptions about the way in which parameters in a GRT model are related to observed response time distributions. The simplest possible assumption is the <italic>RT-distance hypothesis</italic> (<xref rid="B30" ref-type="bibr">Murdock, 1985</xref>), which proposes that perceptual effects that are more distant from the decision bound produce faster responses. In <bold>Figure <xref ref-type="fig" rid="F3">3B</xref></bold>, the representation for female/sad (green distribution) is closer to the bound than the representation for male/sad (orange distribution). This means that response times to classify sad faces should be slower when faces are female than when they are male. <xref rid="B3" ref-type="bibr">Ashby and Maddox (1994)</xref> showed that, if the RT-distance hypothesis is assumed, the data from a filtering task can be used to compute tests of dimensional separability that are <italic>more diagnostic</italic> than the Garner interference test.</p>
      <p>In particular, whereas a violation of separability (perceptual or decisional) is likely to produce a Garner interference effect, a <italic>context effect</italic> is also likely to produce an interference effect (<xref rid="B3" ref-type="bibr">Ashby and Maddox, 1994</xref>). Context effects refer to the case in which the perception of a particular stimulus is changed depending on other stimuli presented closely in time. In the Garner task, such a change in context happens between the baseline (two stimuli are presented) and the filtering (four stimuli are presented) blocks. The effect of context on stimulus perception does not need to be related to separability at all. For example, variability in the irrelevant dimension during filtering blocks could produce spontaneous switches of attention toward that dimension, which are quickly remedied by switching attention back to the relevant dimension (see <xref rid="B46" ref-type="bibr">Yankouskaya et al., 2014</xref>). Such switches of attention would increase response times in the filtering blocks compared to the baseline blocks, even when dimensions are perceptually separable.</p>
      <p><xref rid="B3" ref-type="bibr">Ashby and Maddox (1994)</xref> proposed two tests of separability that are not affected by context effects: marginal response invariance (mRi) and marginal response time invariance (mRTi) tests. Context effects do not influence mRi and mRTi because these tests are computed from data originating from a single block type. Usually, the data should come from filtering blocks, because in these blocks all stimuli are presented at the same time, which reduces the likelihood of participants changing their response strategy depending on the specific stimuli presented.</p>
      <p>The mRi test compares the proportion of correct responses for the relevant dimension across the two levels of the irrelevant dimension. If both PS and DS hold, then the mRi test should indicate no significant differences across levels of the irrelevant dimension. For example, mRi for gender across changes in emotion means that the probability of correctly classifying a face as male does not change depending on whether the face is sad or happy.</p>
      <p>The mRTi test compares the full distribution of response times for correct classifications of the relevant dimension across the two levels of the irrelevant dimension. If both PS and DS hold, then the mRTi test should indicate no significant differences across levels of the irrelevant dimension. For example, mRTi for gender across changes in emotion means that the time it takes to correctly decide that a face is male does not change depending on whether the face is sad or happy. Because mRTi involves a comparison of whole response time distributions, it is the strongest test of separability available for the Garner filtering task (<xref rid="B3" ref-type="bibr">Ashby and Maddox, 1994</xref>).</p>
      <p>As with traditional GRT analyses of the identification task, but unlike analyses based on GRT-wIND, the summary statistics analyses of the Garner interference task cannot determine whether violations of separability are due to perceptual or decisional factors. The usual approach is to assume that DS holds and make conclusions about PS. As indicated in the “Discussion” section below, there are ways to design the task so that DS is more likely to hold.</p>
      <p>Although model-based analyses of data from the Garner filtering task are possible (<xref rid="B27" ref-type="bibr">Maddox and Ashby, 1996</xref>), they require previous knowledge about the perceptual distributions involved, which can be acquired by fitting a GRT model to the data from an identification task. That makes the analyses rather redundant, so applications of GRT to the filtering task have not used model-based analyses.</p>
    </sec>
  </sec>
  <sec>
    <title>Using <italic>grtools</italic></title>
    <sec>
      <title>Installing <italic>grtools</italic></title>
      <p>The <italic>grtools</italic> package uses the computer programming language R (<xref rid="B33" ref-type="bibr">R Core Team, 2016</xref>). R is an open-source statistical software used by many researchers across a variety of scientific fields. R can be downloaded for free at <ext-link ext-link-type="uri" xlink:href="http://cran.rstudio.com">http://cran.rstudio.com</ext-link>.</p>
      <p>We highly recommend using RStudio to work with R. RStudio is an integrated development environment (IDE) for R, which provides a more user-friendly experience that includes a package manager, syntax-highlighting editor, tools for plotting, history, debugging, and workspace management. RStudio is also free and open-source, and it can be downloaded at <ext-link ext-link-type="uri" xlink:href="https://www.rstudio.com/products/rstudio/download/">https://www.rstudio.com/products/rstudio/download/</ext-link>.</p>
      <p>The <italic>grtools</italic> package will require that you have a C++ compiler installed on your computer. For Windows users, the C++ compiler Rtools<sup><xref ref-type="fn" rid="fn01">1</xref></sup> can be used. For Mac users, a C++ compiler can be installed through Xcode (found in the Apple Application Store). More instructions on how to install these compilers can be found in the <italic>grtools</italic> webpage: <ext-link ext-link-type="uri" xlink:href="https://github.com/fsotoc/grtools">https://github.com/fsotoc/grtools</ext-link>.</p>
      <p>To install <italic>grtools</italic>, start a session in R (or RStudio) and in the console type the following:</p>
      <p>
        <inline-graphic xlink:href="fpsyg-08-00696-e001.jpg"/>
      </p>
      <p>Now <italic>grtools</italic> is installed and available for use. To have access to <italic>grtools</italic> functions and analyses, you have to load the package into your current R session. Type the following in the console to load the <italic>grtools</italic> package:</p>
      <p>
        <inline-graphic xlink:href="fpsyg-08-00696-e002.jpg"/>
      </p>
      <p>To access documentation that provides explanations and examples for the analyses in <italic>grtools</italic>, type the following in the console:</p>
      <p>
        <inline-graphic xlink:href="fpsyg-08-00696-e003.jpg"/>
      </p>
    </sec>
    <sec>
      <title>Entering Data in R from a 2 × 2 Identification Task</title>
      <p>Some data gathered through the 2 × 2 identification task must be excluded to allow accurate analyses using <italic>grtools</italic>. First, data from participants in the first few blocks of trials must usually be excluded. These data represent the participants’ learning periods for the task. GRT models only decisional and perceptual processes during steady-state performance; it is unable to model learning processes. A good approach is to set a learning criterion (e.g., a specific number of blocks finished, or a percentage of correct responses within the last <italic>n</italic> trials) and use data only after the criterion has been reached by a participant. Second, data from participants that perform at chance levels in the task should also be excluded. This is usually an indicator that the participant did not understand or learn the task well. Lastly, data from participants that perform near-perfectly should also be excluded. The GRT analyses included in <italic>grtools</italic> extract information about dimensional independence from the pattern of errors shown by participants. If there are only a few errors, <italic>grtools</italic> analyses will be misleading and inaccurate. This requirement is common in psychophysics, as other tools like signal detection theory and classification image techniques also work only when errors are made.</p>
      <p>Because some stimuli (e.g., faces) are very easy to identify, obtaining identification errors might require manipulating the stimuli to increase task difficulty. Manipulations that increase errors include decreasing image contrast, decreasing presentation times, and increasing stimulus similarity through morphing and other image manipulation techniques.</p>
      <p>The data from an identification experiment are summarized in a confusion matrix, which contains a row for each stimulus and a column for each response. In the 2 × 2 design, there are four stimuli and four responses, resulting in a 4 × 4 confusion matrix with a total of 16 data elements for each test participant. An example from our hypothetical experiment dealing with face gender and emotion is shown in <bold>Table <xref ref-type="table" rid="T1">1</xref></bold>. The entry in each cell of this matrix represents the number of trials in which a particular stimulus (row) was presented and a particular response (column) was given. Thus, entries on the main diagonal represent the frequency of each correct response and off-diagonal entries describe the various errors (or confusions). For example, the number in the top-left in <bold>Table <xref ref-type="table" rid="T1">1</xref></bold> represents the number of trials on which stimulus A<sub>1</sub>B<sub>1</sub> was presented and correctly identified (140 trials). The next cell to the right shows the number of trials on which stimulus A<sub>1</sub>B<sub>1</sub> was presented, but the participant incorrectly reported seeing stimulus A<sub>2</sub>B<sub>1</sub> (36 trials). Each row of values must sum up to the total number of trials on which the corresponding stimulus was presented.</p>
      <table-wrap id="T1" position="float">
        <label>Table 1</label>
        <caption>
          <p>Data from a simulated identification experiment with four face stimuli, created by factorially combining two levels of emotion (happy and sad) and two levels of gender (male and female).</p>
        </caption>
        <table frame="hsides" rules="groups" cellspacing="5" cellpadding="5">
          <thead>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"/>
              <th valign="top" align="center" colspan="4" rowspan="1">Response<hr/></th>
            </tr>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">Stimulus</th>
              <th valign="top" align="center" rowspan="1" colspan="1">Happy/Male</th>
              <th valign="top" align="center" rowspan="1" colspan="1">Sad/Male</th>
              <th valign="top" align="center" rowspan="1" colspan="1">Happy/Female</th>
              <th valign="top" align="center" rowspan="1" colspan="1">Sad/Female</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Happy/Male</td>
              <td valign="top" align="center" rowspan="1" colspan="1">140</td>
              <td valign="top" align="center" rowspan="1" colspan="1">36</td>
              <td valign="top" align="center" rowspan="1" colspan="1">34</td>
              <td valign="top" align="center" rowspan="1" colspan="1">40</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Sad/Male</td>
              <td valign="top" align="center" rowspan="1" colspan="1">89</td>
              <td valign="top" align="center" rowspan="1" colspan="1">91</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">66</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Happy/Female</td>
              <td valign="top" align="center" rowspan="1" colspan="1">85</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">90</td>
              <td valign="top" align="center" rowspan="1" colspan="1">70</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Sad/Female</td>
              <td valign="top" align="center" rowspan="1" colspan="1">20</td>
              <td valign="top" align="center" rowspan="1" colspan="1">59</td>
              <td valign="top" align="center" rowspan="1" colspan="1">8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">163</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>When entering the data to R, be careful to order rows and columns in the same way as shown in <bold>Table <xref ref-type="table" rid="T1">1</xref></bold>; that is, beginning with the first row/column and ending with the last row/column, the correct order is: A<sub>1</sub>B<sub>1</sub>, A<sub>2</sub>B<sub>1</sub>, A<sub>1</sub>B<sub>2</sub>, and A<sub>2</sub>B<sub>2</sub>.</p>
      <p>There are at least two ways in which the data from a confusion matrix can be entered into R for analysis with <italic>grtools</italic>. The first option is to directly enter the data as an R object in the matrix class. This can be done through the R function <inline-graphic xlink:href="fpsyg-08-00696-i001.jpg"/>, as in the following example:</p>
      <p>
        <inline-graphic xlink:href="fpsyg-08-00696-e004.jpg"/>
      </p>
      <p>Note that we have entered the data as a single vector inside the function <inline-graphic xlink:href="fpsyg-08-00696-i002.jpg"/>, starting with the data from the first row, then the second row, and so on. The function <inline-graphic xlink:href="fpsyg-08-00696-i003.jpg"/> is then used to shape this vector into an actual confusion matrix.</p>
      <p>The second option is to input the confusion matrix into a spreadsheet application, such as Microsoft Excel. As before, the data must be ordered into a 4 × 4 matrix, as shown in <bold>Table <xref ref-type="table" rid="T1">1</xref></bold>. After entering the data, go to the “File” menu and choose “Save as…”. In the drop-down menu titled “Format” choose “Comma Separated Values (.csv).” Name your file, choose the folder where you want to store it, and click “Save.” In our example, we will name the file “data.csv” and store it in the directory “home/Documents.”</p>
      <p>To import the data to R, make sure first that your working directory is the folder that contains your data file. This is easy to do in RStudio, where you can simply navigate through your folders using the “Files” panel at the bottom-right of the screen. Once you get to your destination folder, click in the “More” menu of the “Files” panel, and choose “Set as Working Directory.”</p>
      <p>Read the file using the function <inline-graphic xlink:href="fpsyg-08-00696-i003.jpg"/>, as in the following example:</p>
      <p>
        <inline-graphic xlink:href="fpsyg-08-00696-e005.jpg"/>
      </p>
      <p>Here, the <inline-graphic xlink:href="fpsyg-08-00696-i004.jpg"/> argument specifies the separator used in the data file, which is a comma. Alternatively, if you know the full path of your file, you can include it in the <inline-graphic xlink:href="fpsyg-08-00696-i005.jpg"/> argument (e.g., <inline-graphic xlink:href="fpsyg-08-00696-i006.jpg"/>) and skip the step of changing the working directory.</p>
      <p>The data table is now available as a data frame named <inline-graphic xlink:href="fpsyg-08-00696-i007.jpg"/>. The final step is to convert the data into a matrix object, using the <inline-graphic xlink:href="fpsyg-08-00696-i008.jpg"/> function. The following command reassigns <inline-graphic xlink:href="fpsyg-08-00696-i007.jpg"/> as a matrix:</p>
      <p>
        <inline-graphic xlink:href="fpsyg-08-00696-e006.jpg"/>
      </p>
    </sec>
    <sec>
      <title>Summary Statistics Analysis of the 2 × 2 Identification Task</title>
      <p>There are two types of summary statistics analyses that can be performed for the 2 × 2 identification design: macro- and micro-analyses (<xref rid="B19" ref-type="bibr">Kadlec and Townsend, 1992a</xref>,<xref rid="B20" ref-type="bibr">b</xref>). Both depend on the computation of only three statistics–proportion of correct responses, sensitivity, and bias– for different combinations of stimuli and responses. The main difference is that macro-analyses compare statistics linked to different stimuli in the task, whereas micro-analyses compare statistics linked to the same stimulus in the task.</p>
      <p>Both the computation of sensitivity measures and the statistical tests for proportions require that proportions are larger than zero and smaller than one. For this reason, <italic>grtools</italic> replaces zeros and ones with values arbitrarily close to them (zero is replaced with 10<sup>-10</sup>, one with 1–10<sup>-10</sup>) and issues a warning in the R console.</p>
      <p>A complete set of macro-analyses can be performed in <italic>grtools</italic> with only three lines of code executed in the R console. The first line performs the actual analysis, using the data that we previously stored in the matrix named <inline-graphic xlink:href="fpsyg-08-00696-i007.jpg"/>:</p>
      <p>
        <inline-graphic xlink:href="fpsyg-08-00696-e007.jpg"/>
      </p>
      <p>This stores the results in an object named <inline-graphic xlink:href="fpsyg-08-00696-i009.jpg"/>, of class <inline-graphic xlink:href="fpsyg-08-00696-i010.jpg"/>. Our second line of code allows us to see a summary of the results:</p>
      <p>
        <inline-graphic xlink:href="fpsyg-08-00696-e008.jpg"/>
      </p>
      <p>This should produce an output table like the one reproduced in <bold>Figure <xref ref-type="fig" rid="F4">4A</xref></bold>. The interpretation of this table is straightforward. Each row represents a particular dimension, and the columns titled “MRI,” “Marginal d′” and “Marginal c” include information about whether or not each of these conditions holds according to the statistical tests performed (see <xref rid="B4" ref-type="bibr">Ashby and Soto, 2015</xref>). Only two values will be displayed in these columns: “YES” to indicate that the condition holds, and “NO” to indicate that the condition does not hold (i.e., a significant failure was detected in the relevant test).</p>
      <fig id="F4" position="float">
        <label>FIGURE 4</label>
        <caption>
          <p><bold>(A)</bold> Summary and <bold>(B)</bold> full results of a macro-analysis of data from a 2 × 2 identification design with <italic>grtools</italic>.</p>
        </caption>
        <graphic xlink:href="fpsyg-08-00696-g004"/>
      </fig>
      <p>Information about the analyses’ conclusions is stored in the final two columns, named “PS” for perceptual separability, and “DS” for decisional separability. The results in these columns are formatted similarly to the results from Kadlec’s classic msda2 software (<xref rid="B17" ref-type="bibr">Kadlec, 1995</xref>; <bold>Table <xref ref-type="table" rid="T1">1</xref></bold>):</p>
      <list list-type="simple" prefix-word="simple">
        <list-item>
          <label>•</label>
          <p> yes means that PS/DS may hold (weak evidence).</p>
        </list-item>
        <list-item>
          <label>•</label>
          <p> NO means that PS/DS does not hold.</p>
        </list-item>
        <list-item>
          <label>•</label>
          <p> ? yes means that PS/DS is unknown but possibly yes</p>
        </list-item>
        <list-item>
          <label>•</label>
          <p> ? no means that PS/DS is unknown but possibly no</p>
        </list-item>
        <list-item>
          <label>•</label>
          <p> ? means that PS/DS is unknown</p>
        </list-item>
      </list>
      <p>In our specific example (<bold>Figure <xref ref-type="fig" rid="F4">4A</xref></bold>), we conclude that dimension A is not perceptually separable from dimension B (NO), but dimension B may be perceptually separable from dimension A (yes). We also conclude that dimension B may be decisionally separable from dimension A (yes), but there is not enough information to reach a conclusion about whether dimension A is decisionally separable from dimension B (?).</p>
      <p>If we want to obtain more specific information about the tests performed in the macro-analysis, we simply type the name of the object in which we stored our results:</p>
      <p>
        <inline-graphic xlink:href="fpsyg-08-00696-e009.jpg"/>
      </p>
      <p>This produces the output in <bold>Figure <xref ref-type="fig" rid="F4">4B</xref></bold>. Results from each of the tests summarized in <bold>Figure <xref ref-type="fig" rid="F4">4A</xref></bold> are now displayed in full detail, including the specific subtests, computed statistics, <italic>p</italic>-values and conclusions.</p>
      <p>Micro-analyses are another method of summary statistic analyses available in <italic>grtools</italic> (see <xref rid="B4" ref-type="bibr">Ashby and Soto, 2015</xref>). These analyses use a different set of summary statistics to test assumptions about both PI and PS. The sequence of commands required to perform a full set of micro-analyses is similar to the macro-analyses described above. The first line performs the micro-analyses using the data stored in the matrix <inline-graphic xlink:href="fpsyg-08-00696-i007.jpg"/>:</p>
      <p>
        <inline-graphic xlink:href="fpsyg-08-00696-e010.jpg"/>
      </p>
      <p>This stores the results in an object named <inline-graphic xlink:href="fpsyg-08-00696-i009.jpg"/>. To see a summary of the results, you can use this <inline-graphic xlink:href="fpsyg-08-00696-i009.jpg"/> object as input to the <inline-graphic xlink:href="fpsyg-08-00696-i011.jpg"/> function:</p>
      <p>
        <inline-graphic xlink:href="fpsyg-08-00696-e011.jpg"/>
      </p>
      <p>This produces an output table like the one reproduced in <bold>Figure <xref ref-type="fig" rid="F5">5A</xref></bold>. Each row represents a unique stimulus, and the columns titled “Sampling Independence,” “Equal Cond d’” (Equal Conditional d’), “Equal Cond c” (Equal Conditional c), include information about whether or not each of these conditions holds according to the statistical tests performed (<xref rid="B4" ref-type="bibr">Ashby and Soto, 2015</xref>). Conclusions regarding PI and DS are stored in the final two columns, labeled PI and DS, respectively. The format of the results is the same as for the macro-analyses. In our example (<bold>Figure <xref ref-type="fig" rid="F5">5A</xref></bold>), both PI and DS are unknown, but possibly do not hold.</p>
      <fig id="F5" position="float">
        <label>FIGURE 5</label>
        <caption>
          <p><bold>(A)</bold> Summary and <bold>(B)</bold> full results of a macro-analysis of data from a 2 × 2 identification design with <italic>grtools</italic>.</p>
        </caption>
        <graphic xlink:href="fpsyg-08-00696-g005"/>
      </fig>
      <p>If you want more specific information about the tests conducted, simply type the name of the object previously used to store the results of this analysis:</p>
      <p>
        <inline-graphic xlink:href="fpsyg-08-00696-e012.jpg"/>
      </p>
      <p>This produces the output in <bold>Figure <xref ref-type="fig" rid="F5">5B</xref></bold>. Results from each one of the tests summarized in <bold>Figure <xref ref-type="fig" rid="F5">5A</xref></bold> are now displayed in full detail, including the specific subtests, computed statistics, <italic>p</italic>-values, and conclusions.</p>
      <p>Recent research has found that summary statistic analyses of the 2 × 2 identification design can sometimes lead to wrong conclusions about separability and independence (<xref rid="B26" ref-type="bibr">Mack et al., 2011</xref>; <xref rid="B37" ref-type="bibr">Silbert and Thomas, 2013</xref>). For this reason, it is good practice to perform the additional model-based analysis of the data that is described in the following two sections.</p>
    </sec>
    <sec>
      <title>Model-Based Analyses of the 2 × 2 Identification Task Using Traditional GRT Models</title>
      <p>Model-based GRT analyses have been used less often than summary statistics in the literature, probably due to the fact that implementing a variety of models, fitting them to data, and selecting among them requires quantitative skill and a deep understanding of the theory. One of the goals of developing <italic>grtools</italic> was to provide experimental psychologists who lack formal quantitative training with tools to easily perform and interpret model-based analyses. Thus, we wrote the software placing ease-of-use above flexibility: a full model-based analysis can be performed with only three lines of code, but the researcher is not free to choose what models to test or the procedures used for model fit and selection<sup><xref ref-type="fn" rid="fn02">2</xref></sup>. We start by briefly reviewing our choices regarding these points.</p>
      <p>Because past research almost exclusively has used the 2 × 2 identification task (for an exception, see <xref rid="B2" ref-type="bibr">Ashby and Lee, 1991</xref>), <italic>grtools</italic> includes functions to model data only from that task. The design is popular because it has a number of advantages: it requires a relatively short experiment, it does not tax the participants’ working memory as larger designs do, and it allows a study of stimulus “components” that cannot be ordered along a continuous dimension (e.g., face identity; see <xref rid="B39" ref-type="bibr">Soto et al., 2015</xref>).</p>
      <p>Our focus on the 2 × 2 identification task means that DS must be assumed throughout the analysis and cannot be tested. The reason is that it has been shown that it is impossible to perform a valid test of DS using traditional modeling of the 2 × 2 identification task (<xref rid="B37" ref-type="bibr">Silbert and Thomas, 2013</xref>). Additionally, the 2 × 2 identification task generates only 16 data points per participant. Due to this small sample size, complex models (i.e., models with many parameters) cannot be adequately fit, and additional assumptions must be made to simplify all models. As in previous related research, the model-based analysis implemented in <italic>grtools</italic> assumes that all variances of the four perceptual distributions are equal to one. All of these problems are solved by analyses using GRT-wIND (see <xref rid="B39" ref-type="bibr">Soto et al., 2015</xref>), which we cover in the next section.</p>
      <p><bold>Figure <xref ref-type="fig" rid="F6">6A</xref></bold> shows the 12 models used in the <italic>grtools</italic> analysis, ordered in a hierarchy in which models that make more assumptions are placed higher, and models that make fewer assumptions are placed progressively lower. This hierarchy has been modified from one presented earlier by <xref rid="B4" ref-type="bibr">Ashby and Soto (2015)</xref>, which in turn was based on an earlier hierarchy by <xref rid="B42" ref-type="bibr">Thomas (2001)</xref>.</p>
      <fig id="F6" position="float">
        <label>FIGURE 6</label>
        <caption>
          <p><bold>(A)</bold> Hierarchy of models used in a model-based analysis of data from a 2 × 2 identification task using traditional general recognition theory with <italic>grtools.</italic> PI stands for perceptual independence, PS for perceptual separability, DS for decisional separability, and 1_RHO for a single correlation in all distributions. The number of free parameters in each model is indicated below its label. <bold>(B)</bold> Initial configuration assuming PS, PI, and DS.</p>
        </caption>
        <graphic xlink:href="fpsyg-08-00696-g006"/>
      </fig>
      <p>In <bold>Figure <xref ref-type="fig" rid="F6">6A</xref></bold>, DS stands for decisional separability. As it can be seen, this property is assumed in all models. PS stands for perceptual separability and PI stands for perceptual independence. 1_RHO stands for a specific violation of PI in which the correlation (i.e., which measures the strength of the PI violation) is the same for all four perceptual distributions. At the top of the hierarchy in <bold>Figure <xref ref-type="fig" rid="F6">6A</xref></bold> is the most constrained model, which assumes PI and PS in addition to DS. This model has a total of four parameters that can be varied to provide a good fit to the data (so there are four “free parameters”). Each of the models one step lower in the hierarchy relaxes a single assumption of the more general model. Relaxing an assumption requires adding one or more free parameters to the model. The process of relaxing assumptions by adding parameters continues until we arrive at the least constrained model at the bottom, which assumes only DS.</p>
      <p>In past applications (e.g., <xref rid="B2" ref-type="bibr">Ashby and Lee, 1991</xref>; <xref rid="B42" ref-type="bibr">Thomas, 2001</xref>; <xref rid="B39" ref-type="bibr">Soto et al., 2015</xref>) model selection proceeded by a series of likelihood ratio tests comparing models joined by arrows in <bold>Figure <xref ref-type="fig" rid="F6">6A</xref></bold> (for details, see <xref rid="B4" ref-type="bibr">Ashby and Soto, 2015</xref>). The use of the likelihood ratio tests allowed the researcher to fit only some of the models in the hierarchy to the data, speeding up the analysis. However, the increase in speed came at the cost of consistency in the model-selection procedure: in most cases, the likelihood ratio tests could not decide between several candidate models, which then had to be compared by some other criterion. Fortunately, this is no longer necessary, as the <italic>grtools</italic> code is fast enough<sup><xref ref-type="fn" rid="fn03">3</xref></sup> that the process of fitting the whole hierarchy to the data of a participant and selecting the best model can be executed in about 20 s. This is done through a single line of code:</p>
      <p>
        <inline-graphic xlink:href="fpsyg-08-00696-e013.jpg"/>
      </p>
      <p>Here we have used again the data previously stored in the matrix <inline-graphic xlink:href="fpsyg-08-00696-i007.jpg"/>. What the function <inline-graphic xlink:href="fpsyg-08-00696-i012.jpg"/> does in the background is fit all the models in <bold>Figure <xref ref-type="fig" rid="F6">6</xref></bold> to the data in <inline-graphic xlink:href="fpsyg-08-00696-i007.jpg"/>, using maximum likelihood estimation (see <xref rid="B31" ref-type="bibr">Myung, 2003</xref>) through the R function <inline-graphic xlink:href="fpsyg-08-00696-i013.jpg"/>. The likelihood function of a GRT model may have several “hills” and “valleys.” Ideally, the optimization algorithm would only stop at the top of the highest hill (the maximum likelihood), but it is also possible for the algorithm to stop at one of the smaller hills, which is known as a local maximum (see <xref rid="B31" ref-type="bibr">Myung, 2003</xref>; <xref rid="B4" ref-type="bibr">Ashby and Soto, 2015</xref>). To avoid this problem, the search for the best set of parameters for a particular model is performed 10 times, each time starting from a different set of initial parameter values. The best solution is kept from these 10 runs of the algorithm.</p>
      <p>Each set of initial parameters is determined by adding a random perturbation to a fixed configuration, shown in <bold>Figure <xref ref-type="fig" rid="F6">6B</xref></bold>. The maximum value of the random perturbation for each parameter is set by default to 0.3.</p>
      <p>Although the default settings for <inline-graphic xlink:href="fpsyg-08-00696-i012.jpg"/> have worked well in our own research, users can change all of them. For example, to fit each model 20 times and use a maximum random perturbation of 0.5, one can use the following command:</p>
      <p>
        <inline-graphic xlink:href="fpsyg-08-00696-e014.jpg"/>
      </p>
      <p>Furthermore, users can have additional control over the optimization process by passing a list of control parameters as final argument to <inline-graphic xlink:href="fpsyg-08-00696-i014.jpg"/>, in the same way in which this is usually done for the optim function [<inline-graphic xlink:href="fpsyg-08-00696-i015.jpg"/>; for more information, type <inline-graphic xlink:href="fpsyg-08-00696-i016.jpg"/> in the R console].</p>
      <p>After finding maximum likelihood estimates for all models, the best model is selected by computing the (corrected) Akaike information criterion (AIC) of each model (<xref rid="B7" ref-type="bibr">Bozdogan, 2000</xref>; <xref rid="B45" ref-type="bibr">Wagenmakers and Farrell, 2004</xref>; for a tutorial on its application to GRT, see <xref rid="B4" ref-type="bibr">Ashby and Soto, 2015</xref>). A summary of the results of the model fitting and selection procedures can be obtained through the following command:</p>
      <p>
        <inline-graphic xlink:href="fpsyg-08-00696-e015.jpg"/>
      </p>
      <p>This should produce an output similar to that shown in <bold>Figure <xref ref-type="fig" rid="F7">7A</xref></bold>. The first column of <bold>Figure <xref ref-type="fig" rid="F7">7A</xref></bold> lists the different models tested, ordered from best to worst fit to the data. <inline-graphic xlink:href="fpsyg-08-00696-i017.jpg"/> stands for perceptual separability, <inline-graphic xlink:href="fpsyg-08-00696-i018.jpg"/> for perceptual independence, <inline-graphic xlink:href="fpsyg-08-00696-i019.jpg"/> for decisional separability, and <inline-graphic xlink:href="fpsyg-08-00696-i020.jpg"/> describes a model with a single correlation parameter for all distributions. In our example, the model that fit the data best is summarized as <inline-graphic xlink:href="fpsyg-08-00696-i021.jpg"/>. This is a model that assumes a single correlation parameter (and so a violation of perceptual independence), assumes PS for dimension B, and assumes DS.</p>
      <fig id="F7" position="float">
        <label>FIGURE 7</label>
        <caption>
          <p><bold>(A)</bold> Summary of the results of a model-based analysis of data from a 2 × 2 identification task using traditional general recognition theory with <italic>grtools</italic>, and <bold>(B)</bold> plot of the best-fitting model resulting from such analysis, where marginal distributions for a given dimension are plotted using either solid or dotted lines depending on the level of the opposite dimension, and the bottom-left insert is a plot of the observed response proportions against those predicted by the model.</p>
        </caption>
        <graphic xlink:href="fpsyg-08-00696-g007"/>
      </fig>
      <p>The following two columns list the log-likelihood and AIC values for each of the models fitted to data. These values are used to assess which models provide the best description of the observed data. In the table produced by <italic>grtools</italic>, models are ranked based on the AIC value, which takes into account both the model’s fit to data as well as the model’s complexity. A relatively low AIC value represents a model that provides a good description of the data without being overly complex and flexible (see <xref rid="B7" ref-type="bibr">Bozdogan, 2000</xref>; <xref rid="B45" ref-type="bibr">Wagenmakers and Farrell, 2004</xref>). The final column represents the AIC weights (see <xref rid="B45" ref-type="bibr">Wagenmakers and Farrell, 2004</xref>), which can be interpreted as the probability that a given model is closest to the true model among those tested. In the case in <bold>Figure <xref ref-type="fig" rid="F7">7A</xref></bold>, the probability of model <inline-graphic xlink:href="fpsyg-08-00696-i021.jpg"/> being closest to the true model is so high that it receives an AIC weight of one after rounding, with all other models receiving a weight of 0. Thus, in this case we can be very confident that model <inline-graphic xlink:href="fpsyg-08-00696-i021.jpg"/> provides the best account of the data of all the models that we tested.</p>
      <p>Underneath the data table, the code also provides a summary table that lists the best-fitting model, and whether PS of A, PS of B, and PI were violated.</p>
      <p>To help visualize what the best-fit model looks like, <italic>grtools</italic> provides aid through the <inline-graphic xlink:href="fpsyg-08-00696-i022.jpg"/> function:</p>
      <p>
        <inline-graphic xlink:href="fpsyg-08-00696-e016.jpg"/>
      </p>
      <p>This produces a graphical representation of the best-fitting model, like the one shown in <bold>Figure <xref ref-type="fig" rid="F7">7B</xref></bold>. The elements in this figure should be interpreted as explained earlier: each ellipse represents a single Gaussian distribution, with the plus sign representing its mean. The tilt of the ellipse represents the correlation between the perceptual effects in the two dimensions. Any tilt is indicative of a failure of PI. It can be seen that the distributions show a positive correlation (represented by <inline-graphic xlink:href="fpsyg-08-00696-i023.jpg"/> in the best-fitting model’s label), indicative of a failure of PI.</p>
      <p>Marginal distributions are plotted to the left and bottom of the figure. The marginal distributions for a given dimension are plotted using either solid or dotted lines depending on the level of the opposite dimension. These provide a visual test of whether the data suggest violations of PS: failures of PS are suggested by non-overlapping solid and dotted marginal distributions. In <bold>Figure <xref ref-type="fig" rid="F7">7B</xref></bold>, the non-overlapping marginal distributions along the expression dimension suggest a failure of PS of expression from gender [which is why <inline-graphic xlink:href="fpsyg-08-00696-i024.jpg"/> is not included in the best-fitting model’s label]. The overlapping marginal distributions along the gender dimension suggest PS of gender from expression [which is why <inline-graphic xlink:href="fpsyg-08-00696-i025.jpg"/> is included in the best-fitting model’s label].</p>
      <p>The insert at the bottom-left shows a plot of the observed response proportions against the response probabilities predicted by the best-fitting model. This plot allows a visual evaluation of how well the model fit the data. In a perfect fit, all the dots would land on the diagonal.</p>
    </sec>
    <sec>
      <title>Model-Based Analyses of the 2 × 2 Identification Task Using GRT-wIND</title>
      <p>Unlike the other analyses that we have illustrated, GRT-wIND accounts for the data from all participants in an experimental group who completed the 2 × 2 identification task. Suppose we have data from five participants that we have collected in five confusion matrices, one for each participant. The first step in applying GRT-wIND is to take all of our confusion matrices and concatenate them in a list:</p>
      <p>
        <inline-graphic xlink:href="fpsyg-08-00696-e017.jpg"/>
      </p>
      <p>An example of properly formatted data is included with <italic>grtools</italic>, and can be accessed by typing the following in the R console:</p>
      <p>
        <inline-graphic xlink:href="fpsyg-08-00696-e018.jpg"/>
      </p>
      <p>You can look at the loaded data by typing <inline-graphic xlink:href="fpsyg-08-00696-i026.jpg"/> in the R console. These data have been sampled directly from the model shown in <bold>Figure <xref ref-type="fig" rid="F8">8A</xref></bold>. This model was built to show PS of dimension B from dimension A, but failure of PS of dimension A from dimension B (marginal distributions along the <italic>x</italic>-axis are not aligned, see <bold>Figure <xref ref-type="fig" rid="F8">8A</xref></bold>). There are also violations of PI (i.e., negative correlations) for stimuli A<sub>2</sub>B<sub>1</sub> and A<sub>1</sub>B<sub>2</sub>. Furthermore, the decision strategies of hypothetical participants randomly deviated from DS on both dimensions.</p>
      <fig id="F8" position="float">
        <label>FIGURE 8</label>
        <caption>
          <p><bold>(A)</bold> Original GRT-wIND model used to generate the data in our example (see main text) and <bold>(B)</bold> model recovered by <italic>grtools</italic>, where marginal distributions for a given dimension are plotted using either solid or dotted lines depending on the level of the opposite dimension, and the bottom-left insert is a plot of the observed response proportions against those predicted by the model.</p>
        </caption>
        <graphic xlink:href="fpsyg-08-00696-g008"/>
      </fig>
      <p>A model-based GRT-wIND analysis starts by fitting the full model to the data in <inline-graphic xlink:href="fpsyg-08-00696-i026.jpg"/>, using the following command:</p>
      <p>
        <inline-graphic xlink:href="fpsyg-08-00696-e019.jpg"/>
      </p>
      <p>This line of code will fit a full GRT-wIND model to the experimental data, using maximum likelihood estimation (see <xref rid="B39" ref-type="bibr">Soto et al., 2015</xref>). By default, the algorithm starts with a set of parameters reflecting PS, PI, and DS (see <bold>Figure <xref ref-type="fig" rid="F6">6B</xref></bold>), which are slightly changed by adding or subtracting a random value. The algorithm then efficiently searches the parameter space to determine the parameter values that maximize the likelihood of the data given the model. As indicated above, to avoid finding a local maximum, one solution is to fit the model to data several times, each time with a different set of starting parameters. This procedure is time consuming, but it provides more valid results, so we recommend using it. <italic>grtools</italic> includes a special function to perform such multiple fits to a GRT-wIND model:</p>
      <p>
        <inline-graphic xlink:href="fpsyg-08-00696-e020.jpg"/>
      </p>
      <p><inline-graphic xlink:href="fpsyg-08-00696-i027.jpg"/> must be provided and it represents the number of times that the model will be fit to data. In our own practice, we have chosen a rather high value of 60 for this parameter (<xref rid="B38" ref-type="bibr">Soto and Ashby, 2015</xref>; <xref rid="B39" ref-type="bibr">Soto et al., 2015</xref>). Good results (i.e., recovery of the true model producing the data) can be sometimes obtained with 20–30 repetitions. Future simulation work will be necessary to determine the minimum number of repetitions that produces good results when fitting GRT-wIND across a variety of circumstances. In the meanwhile, our recommendation is to proceed with caution and use a high value for <inline-graphic xlink:href="fpsyg-08-00696-i027.jpg"/>. This will considerably increase the time that the analysis will take to finish, but it will ensure more valid results.</p>
      <p>The function <inline-graphic xlink:href="fpsyg-08-00696-i028.jpg"/> takes advantage of multiple processing cores in the computer when these are available. By default, the function will use all the available cores minus one. This means that using a machine with multiple cores will considerably speed up the analysis.</p>
      <p>The default settings for <inline-graphic xlink:href="fpsyg-08-00696-i029.jpg"/> and <inline-graphic xlink:href="fpsyg-08-00696-i028.jpg"/> have worked well for us, and we recommend that researchers with limited modeling experience use them. However, options to control the optimization algorithm and starting parameters are available for both functions. Interested researchers should read the documentation for each function, which is available by executing the commands <inline-graphic xlink:href="fpsyg-08-00696-i030.jpg"/> or <inline-graphic xlink:href="fpsyg-08-00696-i031.jpg"/> in the R console.</p>
      <p>To visualize the best-fitting model found by the GRT-wIND analyses, you can call the <inline-graphic xlink:href="fpsyg-08-00696-i022.jpg"/> function:</p>
      <p>
        <inline-graphic xlink:href="fpsyg-08-00696-e021.jpg"/>
      </p>
      <p>This produces <bold>Figure <xref ref-type="fig" rid="F8">8B</xref></bold>, a representation of the recovered model that has nearly the same features as the true model shown in <bold>Figure <xref ref-type="fig" rid="F8">8A</xref></bold>, and can be interpreted in the same way.</p>
      <p>Additionally, you can run statistical tests to determine whether PS, DS, or PI were violated. Currently, <italic>grtools</italic> offers two ways to perform such statistical tests: Wald tests (see <xref rid="B39" ref-type="bibr">Soto et al., 2015</xref>) and likelihood ratio tests (see <xref rid="B4" ref-type="bibr">Ashby and Soto, 2015</xref>).</p>
      <p>The Wald tests have the advantage of being relatively fast to compute. However, they require an estimate of the Hessian matrix of the likelihood function associated with the maximum likelihood estimates (see appendix of <xref rid="B39" ref-type="bibr">Soto et al., 2015</xref>), which is computationally costly and therefore may take several minutes. Even so, once the Hessian is obtained, all tests can be computed in seconds.</p>
      <p>Unfortunately, some estimates of the Hessian are not useful for the computation of a Wald test (i.e., when the Hessian is not positive definite, see <xref rid="B15" ref-type="bibr">Gill and King, 2004</xref>). In our experience, the estimates provided by R (e.g., through the package “numDeriv,” see <xref rid="B14" ref-type="bibr">Gilbert and Varadhan, 2015</xref>) are often problematic. We are currently working on implementing procedures to estimate the Hessian that have proven more successful in our experience (e.g., DERIVEST; <xref rid="B9" ref-type="bibr">D’Errico, 2006</xref>).</p>
      <p>Because of these problems with the Wald test, we recommend researchers perform likelihood ratio tests instead. These tests are slow to compute, but they do not require numerical estimation of the Hessian. A full series of likelihood ratio tests for PS, PI, and DS is performed by using the following line of code:</p>
      <p>
        <inline-graphic xlink:href="fpsyg-08-00696-e022.jpg"/>
      </p>
      <p>Each likelihood ratio test requires refitting GRT-wIND to the data an additional time, except this time with a model in which the assumption being tested (e.g., PS of dimension A) is assumed to hold. As before, this process is repeated a number of times with different starting parameters, obtained by adding random perturbations to the parameters estimated earlier with <inline-graphic xlink:href="fpsyg-08-00696-i028.jpg"/>. Because these parameters are likely to be a better starting guess than the model in <bold>Figure <xref ref-type="fig" rid="F6">6B</xref></bold>, for most cases we can obtain good results without having to run a high number of repetitions. By default, the function <inline-graphic xlink:href="fpsyg-08-00696-i032.jpg"/> runs the optimization algorithm 20 times for each test. This value can be changed by explicitly setting <inline-graphic xlink:href="fpsyg-08-00696-i027.jpg"/> to a different value, as you did in the call to <inline-graphic xlink:href="fpsyg-08-00696-i028.jpg"/>.</p>
      <p>Using <inline-graphic xlink:href="fpsyg-08-00696-i033.jpg"/> should now print to screen an output similar to that shown in <bold>Figure <xref ref-type="fig" rid="F9">9</xref></bold>. There are three parts to this output. The first line is a message indicating whether the optimization algorithm was successful in finding the maximum likelihood estimate of the GRT-wIND parameters. Any problem in the optimization process (which may invalidate the results of this analysis) will be described in this line. This is followed by a summary of the fit of the full model to data, including both the obtained log-likelihood and R-squared (the proportion of the variance in the data explained by the model) of the best-fitting model. The third part of this summary output includes the results for all likelihood ratio tests, in a table with columns including a description of the test, the Chi-squared test statistic, degrees of freedom, <italic>p</italic>-value, and the test’s conclusion (i.e., whether or not PS, PI or DS is violated). In our example, the likelihood ratio tests accurately conclude that PS holds for dimension B (i.e., NO violation) but fails for dimension A (YES violation), that PI does not hold (YES violation), and that DS does not hold for either dimension (YES violation).</p>
      <fig id="F9" position="float">
        <label>FIGURE 9</label>
        <caption>
          <p><bold>Summary of the results of a model-based analysis of data from a 2 × 2 identification task using general recognition theory with individual differences (GRT-wIND)</bold>.</p>
        </caption>
        <graphic xlink:href="fpsyg-08-00696-g009"/>
      </fig>
      <p>If only some likelihood ratio tests are of interest, it is possible to explicitly indicate what tests to run:</p>
      <p>
        <inline-graphic xlink:href="fpsyg-08-00696-e023.jpg"/>
      </p>
      <p>To run only some of these tests, simply keep the strings in the test array that correspond to a test that you want to run, and delete other strings. For example, to run tests of PS and DS of dimension A only, you should use</p>
      <p>
        <inline-graphic xlink:href="fpsyg-08-00696-e024.jpg"/>
      </p>
    </sec>
    <sec>
      <title>Entering Data in R from a 2 × 2 Garner Filtering Task</title>
      <p>Researchers should consider excluding some data from a Garner filtering task to allow accurate analyses using <italic>grtools</italic>. As with the identification ask, initial practice trials–used to teach participants the mapping between levels of the relevant dimension and response keys–should not be included in the analysis, and data from participants that perform at chance levels in the task should also be excluded. Unlike analysis of the identification task, researchers might choose to include the data from participants that perform near-perfectly. If such participants are included in the analysis, then all conclusions should focus on the analysis of response time data. Tests based on the analysis of proportion of correct choices (the Garner interference test on proportion of correct responses and the MRI test) would be invalid and should be ignored.</p>
      <p>The statistical tests for proportions require that proportions are larger than zero and smaller than one. For this reason, <italic>grtools</italic> replaces zeros and ones with values arbitrarily close to them (zero is replaced with 10<sup>-10</sup>, one with 1–10<sup>-10</sup>) and issues a warning in the R console.</p>
      <p><italic>grtools</italic> includes a routine to perform all the summary statistic analyses for the 2 × 2 Garner filtering task described above using only a couple of commands. Data are analyzed for each participant individually, and the first step in the analysis is to create a data frame with the participant’s data. Each row in the data frame includes data from one trial, with the following column organization (for a description of the task, see section “The 2 × 2 Garner Filtering Task”):</p>
      <list list-type="simple" prefix-word="simple">
        <list-item>
          <label>•</label>
          <p> Column 1: Block type, with a value of 1 for baseline blocks and a value of 2 for filtering blocks</p>
        </list-item>
        <list-item>
          <label>•</label>
          <p> Column 2: Level of the relevant dimension, with values 1 and 2</p>
        </list-item>
        <list-item>
          <label>•</label>
          <p> Column 3: Level of the irrelevant dimension, with values 1 and 2</p>
        </list-item>
        <list-item>
          <label>•</label>
          <p> Column 4: Accuracy, with a value of 0 for incorrect trials and 1 for correct trials</p>
        </list-item>
        <list-item>
          <label>•</label>
          <p> Column 5: Response time</p>
        </list-item>
      </list>
      <p>The data can be entered into a data spreadsheet processor like Microsoft Excel, saved as a comma-separated file, and then imported to R using the function <inline-graphic xlink:href="fpsyg-08-00696-i003.jpg"/>, as explained for <inline-graphic xlink:href="fpsyg-08-00696-i007.jpg"/> above. For this analysis, however, the imported data should not be converted to a matrix. An example of properly formatted data is included with <italic>grtools</italic>, and can be accessed by typing the following in the R console:</p>
      <p>
        <inline-graphic xlink:href="fpsyg-08-00696-e025.jpg"/>
      </p>
      <p>The first line of code loads the <inline-graphic xlink:href="fpsyg-08-00696-i034.jpg"/> data frame into our current R session, and the second line of code prints the column names and the first few rows on the console.</p>
    </sec>
    <sec>
      <title>Summary Statistics Analysis of the 2 × 2 Garner Filtering Task</title>
      <p>We perform the summary statistic analysis on these data using the following line of code:</p>
      <p>
        <inline-graphic xlink:href="fpsyg-08-00696-e026.jpg"/>
      </p>
      <p>The following line of code provides a comprehensive summary of the results:</p>
      <p>
        <inline-graphic xlink:href="fpsyg-08-00696-e027.jpg"/>
      </p>
      <p>This produces the output shown in <bold>Figure <xref ref-type="fig" rid="F10">10A</xref></bold>. The first column lists the name of the separability test, while the second column shows whether the test was passed (yes?) or not (NO) according to the analysis. The statistical test used by default to compare response time distributions in the mRTi test is the well-known Kolmogorov–Smirnov test, or KS test. If the conclusions from these analyses are that all tests are passed (yes?), then this suggests that the relevant dimension might be separable from the irrelevant dimension. If any of the conclusions from these analyses indicate that the test is not passed (NO), then this suggests that the relevant dimension is not separable from the irrelevant dimension.</p>
      <fig id="F10" position="float">
        <label>FIGURE 10</label>
        <caption>
          <p><bold>(A)</bold> Summary and <bold>(B)</bold> full results of a summary statistics analysis of data from a 2 × 2 Garner filtering task with <italic>grtools</italic>.</p>
        </caption>
        <graphic xlink:href="fpsyg-08-00696-g010"/>
      </fig>
      <p>To view a more detailed description of the results of this analysis, simply call the R object in which you stored the results:</p>
      <p>
        <inline-graphic xlink:href="fpsyg-08-00696-e028.jpg"/>
      </p>
      <p>This produces the output shown in <bold>Figure <xref ref-type="fig" rid="F10">10B</xref></bold>. Full details and information of the analysis are shown, including the specific subtests, computed statistics, <italic>p</italic>-values, and conclusions.</p>
    </sec>
  </sec>
  <sec>
    <title>Discussion</title>
    <p>We have described an R package with functions that perform a variety of statistical analyses to determine independence and separability of perceptual dimensions according to GRT. Our package focuses heavily on analyses of the 2 × 2 design, which is easier to implement and can be applied more broadly than larger designs. Still, we expect that future releases of <italic>grtools</italic> will include analyses for larger designs, as well as better algorithms for the estimation of the Hessian in Wald tests (<xref rid="B39" ref-type="bibr">Soto et al., 2015</xref>), tests to compare the results of two groups (<xref rid="B38" ref-type="bibr">Soto and Ashby, 2015</xref>), and model-based analyses of response time data (<xref rid="B27" ref-type="bibr">Maddox and Ashby, 1996</xref>).</p>
    <p>Before concluding, we would like to provide some general recommendations regarding design and analyses aimed at determining dimensional independence and separability:</p>
    <list list-type="simple" prefix-word="simple">
      <list-item>
        <label>1.</label>
        <p><italic>For best results, run a 2 × 2 identification experiment and analyze it using GRT-wIND.</italic> The 2<italic> ×</italic> 2 identification task is appealing due to its simplicity and because it is easy to run. Currently, model-based analysis with GRT-wIND is the only way to dissociate perceptual and decisional processes in the 2 × 2 identification task. Although the Garner filtering task is very popular among researchers, this task does not allow one to dissociate perceptual and decisional types of separability, as does the identification task.</p>
      </list-item>
      <list-item>
        <label>2.</label>
        <p><italic>Calibrate stimulus values in the identification task to ensure that all participants produce a moderate rate of errors.</italic> Participants that show perfect performance or performance near chance levels contribute no information to the analysis. For this reason, you should calibrate your stimuli to make sure that participants show a moderate error rate (∼25–35%). Common ways in which this can be achieved are by changing presentation times, contrast levels, and morphing levels (for faces and other objects). Usually, running some pilot participants is enough to settle on a particular set of stimuli. One possibility is using adaptive procedures (for reviews, see <xref rid="B23" ref-type="bibr">Leek, 2001</xref>; <xref rid="B24" ref-type="bibr">Lu and Dosher, 2013</xref>) to bring the error rates of all participants to the same level, by varying some stimulus feature that is unrelated to the dimensions under study.</p>
      </list-item>
      <list-item>
        <label>3.</label>
        <p><italic>If you have the resources, use a concurrent operations approach by also running a 2 × 2 Garner filtering task</italic>. In theory, the results from this second task cannot give more information about dimensional independence than the results from the identification task. In practice, it is advantageous to verify that the results obtained with the identification task, and analyses based mostly on choice proportions, are in line with results obtained with the Garner filtering task and analyses based mostly on response times.</p>
      </list-item>
      <list-item>
        <label>4.</label>
        <p><italic>Use instructions that will increase the likelihood of DS</italic>. Many of the analyses described here are only valid if DS holds. Fortunately, experimental results have shown that there are ways to encourage participants to use such decisional strategies, such as providing instructions indicating that stimuli follow a grid configuration (e.g., a 2 × 2 grid) and spatially positioning response buttons in that configuration (<xref rid="B6" ref-type="bibr">Ashby et al., 2001</xref>).</p>
      </list-item>
      <list-item>
        <label>5.</label>
        <p><italic>Use all available tests of separability for the Garner filtering task.</italic> The most common test of separability for the Garner task is the Garner interference test. As we have seen before, two other tests of separability are available –mRi and mRTi– and both are more diagnostic about dimensional separability than the Garner interference test. Given that there is no additional cost to perform such tests, we believe that most researchers do not perform them because they simply do not know how. Fortunately, <italic>grtools</italic> allows all three tests to be performed using a single command.</p>
      </list-item>
      <list-item>
        <label>6.</label>
        <p><italic>Include a control group that provides a benchmark of separability</italic>. Violations of separability are common, and features of your task and design might produce such violations. Factors such as training in a categorization task can influence PS as well (<xref rid="B38" ref-type="bibr">Soto and Ashby, 2015</xref>). For these reasons, it is a good idea to include a group that would serve as a benchmark, if the design does not already involve more than one group. That benchmark could be very simple; examples include a group presented with dimensions known to be separable (e.g., orientation and width of gratings, shape, and color) or integral (e.g., brightness and saturation; unfamiliar face identities).</p>
      </list-item>
      <list-item>
        <label>7.</label>
        <p><italic>Make sure that your results are generalizable, rather than explainable by task features</italic>. The simplicity of the 2 × 2 tasks comes at a cost: only four stimuli are studied, although the number of combinations of levels of the two dimensions under study could be infinite. You should be careful to not over generalize when interpreting the results from a single set of stimuli. Ideally, several experiments should be performed before reaching a conclusion, perhaps parametrically varying task factors such as difficulty (<xref rid="B46" ref-type="bibr">Yankouskaya et al., 2014</xref>). A good and low-cost way of learning whether task features can explain performance in a visual task is by analyzing the performance of an ideal observer in the task (for a GRT analysis of an ideal observer’s performance, see Experiment 2 of <xref rid="B40" ref-type="bibr">Soto and Wasserman, 2011</xref>).</p>
      </list-item>
    </list>
    <p>In the past, GRT has been popular among mathematical psychologists who have the technical knowledge to implement statistical analyses using it. Most of the research using GRT outside this community was facilitated by Kadlec’s publication of the <italic>mdsda</italic> program (<xref rid="B17" ref-type="bibr">Kadlec, 1995</xref>, <xref rid="B18" ref-type="bibr">1999</xref>), which performed summary statistics analyses of data from the identification tasks. Recently, another R package has been made public that allows data analyses using traditional GRT models (<xref rid="B36" ref-type="bibr">Silbert and Hawkins, 2016</xref>). However, this package requires a more active involvement from the researcher during the analysis. The researcher must have a good understanding of the relation between a variety of tests and the concepts of PI, PS, and DS. For model-based analyses, the researcher must decide what models to fit and compare, and make a selection based on measures of fit. On the other hand, <italic>grtools</italic> was developed with the typical experimental psychologist in mind, someone who wants to make use of the sophisticated analytical tools offered by GRT, but does not have the training to implement the analyses, make critical decisions about procedures, and interpret the overall pattern of results. Our package allows researchers to perform full GRT analyses with only three commands, and explore the results in a way that highlights the most important conclusions from the study. Importantly, <italic>grtools</italic> is the only currently available package implementing summary statistics analyses of the widely used Garner filtering task and model-based analyses with GRT-wIND (<xref rid="B39" ref-type="bibr">Soto et al., 2015</xref>). The latter are the only analyses capable of dissociating perceptual and decisional forms of separability with a 2 × 2 identification design. We hope that the availability of <italic>grtools</italic> will lead to a wider application of GRT to the analysis of dimensional interactions.</p>
  </sec>
  <sec>
    <title>Author Contributions</title>
    <p>FS programmed the software described in this article, created examples and figures, and wrote the article. EZ programmed the software described in this article. JF programmed the software described in this article. FA wrote the article and supervised the project.</p>
  </sec>
  <sec>
    <title>Conflict of Interest Statement</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn fn-type="financial-disclosure">
      <p><bold>Funding</bold>. This work was supported in part by grant no. W911NF-07-1-0072 from the U.S. Army Research Office through the Institute for Collaborative Biotechnologies, by NIH grant 2R01MH063760, and by the UCSB Open Access Fund Pilot Program.</p>
    </fn>
  </fn-group>
  <fn-group>
    <fn id="fn01">
      <label>1</label>
      <p>
        <ext-link ext-link-type="uri" xlink:href="http://cran.r-project.org/bin/windows/Rtools/">http://cran.r-project.org/bin/windows/Rtools/</ext-link>
      </p>
    </fn>
    <fn id="fn02">
      <label>2</label>
      <p>However, since our software is open-source and distributed under a creative commons license, any researcher can modify the original code to fit his/her own needs.</p>
    </fn>
    <fn id="fn03">
      <label>3</label>
      <p>The code in <italic>grtools</italic> is relatively fast for two reasons: (1) we use an efficient method of numerical integration proposed by <xref rid="B11" ref-type="bibr">Ennis and Ashby (2003)</xref> to compute the likelihood of the data for a given GRT model, and (2) we implement this method through a C++ function, which is called from R using the package Rcpp (<xref rid="B10" ref-type="bibr">Eddelbuettel et al., 2011</xref>).</p>
    </fn>
  </fn-group>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Algom</surname><given-names>D.</given-names></name><name><surname>Fitousi</surname><given-names>D.</given-names></name></person-group> (<year>2016</year>). <article-title>Half a century of research on Garner interference and the separability–integrality distinction.</article-title>
<source><italic>Psych. Bull.</italic></source>
<volume>142</volume>
<fpage>1352</fpage>–<lpage>1383</lpage>. <pub-id pub-id-type="doi">10.1037/bul0000072</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashby</surname><given-names>F. G.</given-names></name><name><surname>Lee</surname><given-names>W. W.</given-names></name></person-group> (<year>1991</year>). <article-title>Predicting similarity and categorization from identification.</article-title>
<source><italic>J. Exp. Psychol. Gen.</italic></source>
<volume>120</volume>
<fpage>150</fpage>–<lpage>172</lpage>. <pub-id pub-id-type="doi">10.1037/0096-3445.120.2.150</pub-id><pub-id pub-id-type="pmid">1830609</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashby</surname><given-names>F. G.</given-names></name><name><surname>Maddox</surname><given-names>W. T.</given-names></name></person-group> (<year>1994</year>). <article-title>A response time theory of separability and integrality in speeded classification.</article-title>
<source><italic>J. Math. Psychol.</italic></source>
<volume>38</volume>
<fpage>423</fpage>–<lpage>466</lpage>. <pub-id pub-id-type="doi">10.1006/jmps.1994.1032</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ashby</surname><given-names>F. G.</given-names></name><name><surname>Soto</surname><given-names>F. A.</given-names></name></person-group> (<year>2015</year>). <article-title>“Multidimensional signal detection theory,” in</article-title>
<source><italic>Oxford Handbook of Computational and Mathematical Psychology</italic></source>, <role>eds</role>
<person-group person-group-type="editor"><name><surname>Busemeyer</surname><given-names>J.</given-names></name><name><surname>Townsend</surname><given-names>J. T.</given-names></name><name><surname>Wang</surname><given-names>Z. J.</given-names></name><name><surname>Eidels</surname><given-names>A.</given-names></name></person-group> (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>), <fpage>13</fpage>–<lpage>34</lpage>.</mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashby</surname><given-names>F. G.</given-names></name><name><surname>Townsend</surname><given-names>J. T.</given-names></name></person-group> (<year>1986</year>). <article-title>Varieties of perceptual independence.</article-title>
<source><italic>Psychol. Rev.</italic></source>
<volume>93</volume>
<fpage>154</fpage>–<lpage>179</lpage>. <pub-id pub-id-type="doi">10.1037/0033-295X.93.2.154</pub-id><pub-id pub-id-type="pmid">3714926</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashby</surname><given-names>F. G.</given-names></name><name><surname>Waldron</surname><given-names>E. M.</given-names></name><name><surname>Lee</surname><given-names>W. W.</given-names></name><name><surname>Berkman</surname><given-names>A.</given-names></name></person-group> (<year>2001</year>). <article-title>Suboptimality in human categorization and identification.</article-title>
<source><italic>J. Exp. Psychol. Gen.</italic></source>
<volume>130</volume>
<fpage>77</fpage>–<lpage>96</lpage>. <pub-id pub-id-type="doi">10.1037/0096-3445.130.1.77</pub-id><pub-id pub-id-type="pmid">11293461</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bozdogan</surname><given-names>H.</given-names></name></person-group> (<year>2000</year>). <article-title>Akaike’s information criterion and recent developments in information complexity.</article-title>
<source><italic>J. Math. Psychol.</italic></source>
<volume>44</volume>
<fpage>62</fpage>–<lpage>91</lpage>. <pub-id pub-id-type="doi">10.1006/jmps.1999.1277</pub-id><pub-id pub-id-type="pmid">10733858</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruce</surname><given-names>V.</given-names></name><name><surname>Young</surname><given-names>A.</given-names></name></person-group> (<year>1986</year>). <article-title>Understanding face recognition.</article-title>
<source><italic>Br. J. Psychol.</italic></source>
<volume>77</volume>
<fpage>305</fpage>–<lpage>327</lpage>. <pub-id pub-id-type="doi">10.1111/j.2044-8295.1986.tb02199.x</pub-id><pub-id pub-id-type="pmid">3756376</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>D’Errico</surname><given-names>J.</given-names></name></person-group> (<year>2006</year>). <source><italic>Adaptive Robust Numerical Differentiation.</italic></source> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.mathworks.com/matlabcentral/fileexchange/file_infos/13490-adaptive-robust-numerical-differentiation">http://www.mathworks.com/matlabcentral/fileexchange/file_infos/13490-adaptive-robust-numerical-differentiation</ext-link>
<comment>[accessed April 19 2014]</comment></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eddelbuettel</surname><given-names>D.</given-names></name><name><surname>François</surname><given-names>R.</given-names></name><name><surname>Allaire</surname><given-names>J.</given-names></name><name><surname>Chambers</surname><given-names>J.</given-names></name><name><surname>Bates</surname><given-names>D.</given-names></name><name><surname>Ushey</surname><given-names>K.</given-names></name></person-group> (<year>2011</year>). <article-title>Rcpp: seamless R and C++ integration.</article-title>
<source><italic>J. Stat. Softw.</italic></source>
<volume>40</volume>
<fpage>1</fpage>–<lpage>18</lpage>. <pub-id pub-id-type="doi">10.18637/jss.v040.i08</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ennis</surname><given-names>D. M.</given-names></name><name><surname>Ashby</surname><given-names>F. G.</given-names></name></person-group> (<year>2003</year>). <source><italic>Fitting the decision bound models to identification categorization data.</italic></source>
<publisher-name>Technical Report. University of California Santa Barbara, Santa Barbara, CA</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Farah</surname><given-names>M. J.</given-names></name><name><surname>Wilson</surname><given-names>K. D.</given-names></name><name><surname>Drain</surname><given-names>M.</given-names></name><name><surname>Tanaka</surname><given-names>J. N.</given-names></name></person-group> (<year>1998</year>). <article-title>What is “special” about face perception?</article-title>
<source><italic>Psychol. Rev.</italic></source>
<volume>105</volume>
<fpage>482</fpage>–<lpage>498</lpage>. <pub-id pub-id-type="doi">10.1037/0033-295X.105.3.482</pub-id><pub-id pub-id-type="pmid">9697428</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Garner</surname><given-names>W. R.</given-names></name></person-group> (<year>1974</year>). <source><italic>The Processing of Information and Structure.</italic></source>
<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Lawrence Erlbaum Associates</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilbert</surname><given-names>P.</given-names></name><name><surname>Varadhan</surname><given-names>R.</given-names></name></person-group> (<year>2015</year>). <source><italic>numDeriv: Accurate Numerical Derivatives.</italic></source> Available at: <ext-link ext-link-type="uri" xlink:href="http://CRAN.R-project.org/package">http://CRAN.R-project.org/package</ext-link> = numDeriv</mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gill</surname><given-names>J.</given-names></name><name><surname>King</surname><given-names>G.</given-names></name></person-group> (<year>2004</year>). <article-title>What to do when your hessian is not invertible: alternatives to model respecification in nonlinear estimation.</article-title>
<source><italic>Sociol. Methods Res.</italic></source>
<volume>33</volume>
<fpage>54</fpage>–<lpage>87</lpage>. <pub-id pub-id-type="doi">10.1177/0049124103262681</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname><given-names>J. V.</given-names></name><name><surname>Hoffman</surname><given-names>E. A.</given-names></name><name><surname>Gobbini</surname><given-names>M. I.</given-names></name></person-group> (<year>2000</year>). <article-title>The distributed human neural system for face perception.</article-title>
<source><italic>Trends Cogn. Sci.</italic></source>
<volume>4</volume>
<fpage>223</fpage>–<lpage>232</lpage>. <pub-id pub-id-type="doi">10.1016/S1364-6613(00)01482-0</pub-id><pub-id pub-id-type="pmid">10827445</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kadlec</surname><given-names>H.</given-names></name></person-group> (<year>1995</year>). <article-title>Multidimensional signal detection analyses (MSDA) for testing separability and independence: a Pascal program.</article-title>
<source><italic>Behav. Res. Methods</italic></source>
<volume>27</volume>
<fpage>442</fpage>–<lpage>458</lpage>. <pub-id pub-id-type="doi">10.3758/BF03200443</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kadlec</surname><given-names>H.</given-names></name></person-group> (<year>1999</year>). <article-title>MSDA_2: updated version of software for multidimensional signal detection analyses.</article-title>
<source><italic>Behav. Res. Methods</italic></source>
<volume>31</volume>
<fpage>384</fpage>–<lpage>385</lpage>. <pub-id pub-id-type="doi">10.3758/BF03207737</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kadlec</surname><given-names>H.</given-names></name><name><surname>Townsend</surname><given-names>J. T.</given-names></name></person-group> (<year>1992a</year>). <article-title>Implications of marginal and conditional detection parameters for the separabilities and independence of perceptual dimensions.</article-title>
<source><italic>J. Math. Psychol.</italic></source>
<volume>36</volume>
<fpage>325</fpage>–<lpage>374</lpage>. <pub-id pub-id-type="doi">10.1016/0022-2496(92)90027-5</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kadlec</surname><given-names>H.</given-names></name><name><surname>Townsend</surname><given-names>J. T.</given-names></name></person-group> (<year>1992b</year>). “<article-title>Signal detection analysis of multidimensional interactions</article-title>,” in <source><italic>Multidimensional Models of Perception and Cognition</italic></source>, <role>ed.</role>
<person-group person-group-type="editor"><name><surname>Ashby</surname><given-names>F. G.</given-names></name></person-group> (<publisher-loc>Hillsdale, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum Associates</publisher-name>), <fpage>181</fpage>–<lpage>231</lpage>.</mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kanade</surname><given-names>T.</given-names></name><name><surname>Cohn</surname><given-names>J. F.</given-names></name><name><surname>Tian</surname><given-names>Y.</given-names></name></person-group> (<year>2000</year>). <article-title>“Comprehensive database for facial expression analysis,” in</article-title>
<source><italic>Proceedings of the Fourth IEEE International Conference on Automatic Face and Gesture Recognition (FG’00)</italic></source>, <publisher-loc>Grenoble</publisher-loc>, <fpage>46</fpage>–<lpage>53</lpage>. <pub-id pub-id-type="doi">10.1109/AFGR.2000.840611</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname><given-names>N.</given-names></name></person-group> (<year>2000</year>). <article-title>Domain specificity in face perception.</article-title>
<source><italic>Nat. Neurosci.</italic></source>
<volume>3</volume>
<fpage>759</fpage>–<lpage>763</lpage>. <pub-id pub-id-type="doi">10.1038/77664</pub-id><pub-id pub-id-type="pmid">10903567</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leek</surname><given-names>M. R.</given-names></name></person-group> (<year>2001</year>). <article-title>Adaptive procedures in psychophysical research.</article-title>
<source><italic>Percept. Psychophys.</italic></source>
<volume>63</volume>
<fpage>1279</fpage>–<lpage>1292</lpage>. <pub-id pub-id-type="doi">10.3758/BF03194543</pub-id><pub-id pub-id-type="pmid">11800457</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>Z.-L.</given-names></name><name><surname>Dosher</surname><given-names>B.</given-names></name></person-group> (<year>2013</year>). <source><italic>Visual Psychophysics: From Laboratory to Theory.</italic></source>
<publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>
<pub-id pub-id-type="doi">10.7551/mitpress/9780262019453.001.0001</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lucey</surname><given-names>P.</given-names></name><name><surname>Cohn</surname><given-names>J. F.</given-names></name><name><surname>Kanade</surname><given-names>T.</given-names></name><name><surname>Saragih</surname><given-names>J.</given-names></name><name><surname>Ambadar</surname><given-names>Z.</given-names></name><name><surname>Matthews</surname><given-names>I.</given-names></name></person-group> (<year>2010</year>). “<article-title>The extended cohn-kanade dataset (CK+): a complete expression dataset for action unit and emotion-specified expression</article-title>,” in <source><italic>Proceedings of the Third International Workshop on CVPR for Human Communicative Behavior Analysis (CVPR</italic></source>4HB 2010), San Francisco, CA, <fpage>94</fpage>–<lpage>101</lpage>. <pub-id pub-id-type="doi">10.1109/cvprw.2010.5543262</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mack</surname><given-names>M. L.</given-names></name><name><surname>Richler</surname><given-names>J. J.</given-names></name><name><surname>Gauthier</surname><given-names>I.</given-names></name><name><surname>Palmeri</surname><given-names>T. J.</given-names></name></person-group> (<year>2011</year>). <article-title>Indecision on decisional separability.</article-title>
<source><italic>Psychon. Bull. Rev.</italic></source>
<volume>18</volume>
<fpage>1</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.3758/s13423-010-0017-1</pub-id><pub-id pub-id-type="pmid">21327365</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maddox</surname><given-names>W. T.</given-names></name><name><surname>Ashby</surname><given-names>F. G.</given-names></name></person-group> (<year>1996</year>). <article-title>Perceptual separability, decisional separability, and the identification–speeded classification relationship.</article-title>
<source><italic>J. Exp. Psychol. Hum. Percept. Perform.</italic></source>
<volume>22</volume>
<fpage>795</fpage>–<lpage>817</lpage>. <pub-id pub-id-type="doi">10.1037/0096-1523.22.4.795</pub-id><pub-id pub-id-type="pmid">8756953</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maurer</surname><given-names>D.</given-names></name><name><surname>Grand</surname><given-names>R. L.</given-names></name><name><surname>Mondloch</surname><given-names>C. J.</given-names></name></person-group> (<year>2002</year>). <article-title>The many faces of configural processing.</article-title>
<source><italic>Trends Cogn. Sci.</italic></source>
<volume>6</volume>
<fpage>255</fpage>–<lpage>260</lpage>. <pub-id pub-id-type="doi">10.1016/S1364-6613(02)01903-4</pub-id><pub-id pub-id-type="pmid">12039607</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mestry</surname><given-names>N.</given-names></name><name><surname>Wenger</surname><given-names>M. J.</given-names></name><name><surname>Donnelly</surname><given-names>N.</given-names></name></person-group> (<year>2012</year>). <article-title>Identifying sources of configurality in three face processing tasks.</article-title>
<source><italic>Front. Percept. Sci.</italic></source>
<volume>3</volume>:<issue>456</issue>
<pub-id pub-id-type="doi">10.3389/fpsyg.2012.00456</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murdock</surname><given-names>B. B.</given-names></name></person-group> (<year>1985</year>). <article-title>An analysis of the strength-latency relationship.</article-title>
<source><italic>Mem. Cognit.</italic></source>
<volume>13</volume>
<fpage>511</fpage>–<lpage>521</lpage>.</mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Myung</surname><given-names>I. J.</given-names></name></person-group> (<year>2003</year>). <article-title>Tutorial on maximum likelihood estimation.</article-title>
<source><italic>J. Math. Psychol.</italic></source>
<volume>47</volume>
<fpage>90</fpage>–<lpage>100</lpage>. <pub-id pub-id-type="doi">10.1016/S0022-2496(02)00028-7</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Op de Beeck</surname><given-names>H. P.</given-names></name><name><surname>Haushofer</surname><given-names>J.</given-names></name><name><surname>Kanwisher</surname><given-names>N. G.</given-names></name></person-group> (<year>2008</year>). <article-title>Interpreting fMRI data: maps, modules and dimensions.</article-title>
<source><italic>Nat. Rev. Neurosci.</italic></source>
<volume>9</volume>
<fpage>123</fpage>–<lpage>135</lpage>. <pub-id pub-id-type="doi">10.1038/nrn2314</pub-id><pub-id pub-id-type="pmid">18200027</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="book"><collab>R Core Team</collab> (<year>2016</year>). <source><italic>R: A Language and Environment for Statistical Computing.</italic></source>
<publisher-loc>Vienna</publisher-loc>: <publisher-name>R Foundation for Statistical Computing</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richler</surname><given-names>J. J.</given-names></name><name><surname>Gauthier</surname><given-names>I.</given-names></name><name><surname>Wenger</surname><given-names>M. J.</given-names></name><name><surname>Palmeri</surname><given-names>T. J.</given-names></name></person-group> (<year>2008</year>). <article-title>Holistic processing of faces: perceptual and decisional components.</article-title>
<source><italic>J. Exp. Psychol. Learn. Mem. Cogn.</italic></source>
<volume>34</volume>
<fpage>328</fpage>–<lpage>342</lpage>. <pub-id pub-id-type="doi">10.1037/0278-7393.34.2.328</pub-id><pub-id pub-id-type="pmid">18315409</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richler</surname><given-names>J. J.</given-names></name><name><surname>Palmeri</surname><given-names>T. J.</given-names></name><name><surname>Gauthier</surname><given-names>I.</given-names></name></person-group> (<year>2012</year>). <article-title>Meanings, mechanisms, and measures of holistic processing.</article-title>
<source><italic>Front. Percept. Sci.</italic></source>
<volume>3</volume>:<issue>553</issue>
<pub-id pub-id-type="doi">10.3389/fpsyg.2012.00553</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silbert</surname><given-names>N. H.</given-names></name><name><surname>Hawkins</surname><given-names>R. X. D.</given-names></name></person-group> (<year>2016</year>). <article-title>A tutorial on General Recognition Theory.</article-title>
<source><italic>J. Math. Psychol.</italic></source>
<volume>73</volume>
<fpage>94</fpage>–<lpage>109</lpage>. <pub-id pub-id-type="doi">10.1016/j.jmp.2016.04.011</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silbert</surname><given-names>N. H.</given-names></name><name><surname>Thomas</surname><given-names>R.</given-names></name></person-group> (<year>2013</year>). <article-title>Decisional separability, model identification, and statistical inference in the general recognition theory framework.</article-title>
<source><italic>Psychon. Bull. Rev.</italic></source>
<volume>20</volume>
<fpage>1</fpage>–<lpage>20</lpage>. <pub-id pub-id-type="doi">10.3758/s13423-012-0329-4</pub-id><pub-id pub-id-type="pmid">23090749</pub-id></mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soto</surname><given-names>F. A.</given-names></name><name><surname>Ashby</surname><given-names>F. G.</given-names></name></person-group> (<year>2015</year>). <article-title>Categorization training increases the perceptual separability of novel dimensions.</article-title>
<source><italic>Cognition</italic></source>
<volume>139</volume>
<fpage>105</fpage>–<lpage>129</lpage>. <pub-id pub-id-type="doi">10.1016/j.cognition.2015.02.006</pub-id><pub-id pub-id-type="pmid">25817370</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soto</surname><given-names>F. A.</given-names></name><name><surname>Vucovich</surname><given-names>L.</given-names></name><name><surname>Musgrave</surname><given-names>R.</given-names></name><name><surname>Ashby</surname><given-names>F. G.</given-names></name></person-group> (<year>2015</year>). <article-title>General recognition theory with individual differences: a new method for examining perceptual and decisional interactions with an application to face perception.</article-title>
<source><italic>Psychon. Bull. Rev.</italic></source>
<volume>22</volume>
<fpage>88</fpage>–<lpage>111</lpage>. <pub-id pub-id-type="doi">10.3758/s13423-014-0661-y</pub-id><pub-id pub-id-type="pmid">24841236</pub-id></mixed-citation>
    </ref>
    <ref id="B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soto</surname><given-names>F. A.</given-names></name><name><surname>Wasserman</surname><given-names>E. A.</given-names></name></person-group> (<year>2011</year>). <article-title>Asymmetrical interactions in the perception of face identity and emotional expression are not unique to the primate visual system.</article-title>
<source><italic>J. Vis.</italic></source>
<volume>11</volume>:<issue>24</issue>
<pub-id pub-id-type="doi">10.1167/11.3.24</pub-id></mixed-citation>
    </ref>
    <ref id="B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stankiewicz</surname><given-names>B. J.</given-names></name></person-group> (<year>2002</year>). <article-title>Empirical evidence for independent dimensions in the visual representation of three-dimensional shape.</article-title>
<source><italic>J. Exp. Psychol. Hum. Percept. Perform.</italic></source>
<volume>28</volume>
<fpage>913</fpage>–<lpage>932</lpage>. <pub-id pub-id-type="doi">10.1037/0096-1523.28.4.913</pub-id><pub-id pub-id-type="pmid">12190258</pub-id></mixed-citation>
    </ref>
    <ref id="B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thomas</surname><given-names>R.</given-names></name></person-group> (<year>2001</year>). <article-title>Perceptual interactions of facial dimensions in speeded classification and identification.</article-title>
<source><italic>Atten. Percept. Psychophys.</italic></source>
<volume>63</volume>
<fpage>625</fpage>–<lpage>650</lpage>. <pub-id pub-id-type="doi">10.3758/BF03194426</pub-id></mixed-citation>
    </ref>
    <ref id="B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ungerleider</surname><given-names>L. G.</given-names></name><name><surname>Haxby</surname><given-names>J. V.</given-names></name></person-group> (<year>1994</year>). <article-title>“What” and “where” in the human brain.</article-title>
<source><italic>Curr. Opin. Neurobiol.</italic></source>
<volume>4</volume>
<fpage>157</fpage>–<lpage>165</lpage>. <pub-id pub-id-type="doi">10.1016/0959-4388(94)90066-3</pub-id><pub-id pub-id-type="pmid">8038571</pub-id></mixed-citation>
    </ref>
    <ref id="B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogels</surname><given-names>R.</given-names></name><name><surname>Biederman</surname><given-names>I.</given-names></name><name><surname>Bar</surname><given-names>M.</given-names></name><name><surname>Lorincz</surname><given-names>A.</given-names></name></person-group> (<year>2001</year>). <article-title>Inferior temporal neurons show greater sensitivity to nonaccidental than to metric shape differences.</article-title>
<source><italic>J. Cogn. Neurosci.</italic></source>
<volume>13</volume>
<fpage>444</fpage>–<lpage>453</lpage>. <pub-id pub-id-type="doi">10.1162/08989290152001871</pub-id><pub-id pub-id-type="pmid">11388918</pub-id></mixed-citation>
    </ref>
    <ref id="B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagenmakers</surname><given-names>E.-J.</given-names></name><name><surname>Farrell</surname><given-names>S.</given-names></name></person-group> (<year>2004</year>). <article-title>AIC model selection using Akaike weights.</article-title>
<source><italic>Psychon. Bull. Rev.</italic></source>
<volume>11</volume>
<fpage>192</fpage>–<lpage>196</lpage>. <pub-id pub-id-type="doi">10.3758/BF03206482</pub-id><pub-id pub-id-type="pmid">15117008</pub-id></mixed-citation>
    </ref>
    <ref id="B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yankouskaya</surname><given-names>A.</given-names></name><name><surname>Humphreys</surname><given-names>G. W.</given-names></name><name><surname>Rotshtein</surname><given-names>P.</given-names></name></person-group> (<year>2014</year>). <article-title>The processing of facial identity and expression is interactive, but dependent on task and experience.</article-title>
<source><italic>Front. Hum. Neurosci.</italic></source>
<volume>8</volume>:<issue>920</issue>
<pub-id pub-id-type="doi">10.3389/fnhum.2014.00920</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
