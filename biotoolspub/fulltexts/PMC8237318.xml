<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PeerJ Comput Sci</journal-id>
    <journal-id journal-id-type="iso-abbrev">PeerJ Comput Sci</journal-id>
    <journal-id journal-id-type="publisher-id">peerj-cs</journal-id>
    <journal-id journal-id-type="pmc">peerj-cs</journal-id>
    <journal-title-group>
      <journal-title>PeerJ Computer Science</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2376-5992</issn>
    <publisher>
      <publisher-name>PeerJ Inc.</publisher-name>
      <publisher-loc>San Diego, USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8237318</article-id>
    <article-id pub-id-type="publisher-id">cs-593</article-id>
    <article-id pub-id-type="doi">10.7717/peerj-cs.593</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Bioinformatics</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Distributed and Parallel Computing</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Scientific Computing and Simulation</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Software Engineering</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Compi: a framework for portable and reproducible pipelines</article-title>
    </title-group>
    <contrib-group>
      <contrib id="author-1" contrib-type="author">
        <name>
          <surname>López-Fernández</surname>
          <given-names>Hugo</given-names>
        </name>
        <xref ref-type="aff" rid="aff-1">1</xref>
        <xref ref-type="aff" rid="aff-2">2</xref>
        <xref ref-type="aff" rid="aff-3">3</xref>
      </contrib>
      <contrib id="author-2" contrib-type="author">
        <name>
          <surname>Graña-Castro</surname>
          <given-names>Osvaldo</given-names>
        </name>
        <xref ref-type="aff" rid="aff-4">4</xref>
      </contrib>
      <contrib id="author-3" contrib-type="author">
        <name>
          <surname>Nogueira-Rodríguez</surname>
          <given-names>Alba</given-names>
        </name>
        <xref ref-type="aff" rid="aff-1">1</xref>
        <xref ref-type="aff" rid="aff-2">2</xref>
        <xref ref-type="aff" rid="aff-3">3</xref>
      </contrib>
      <contrib id="author-4" contrib-type="author">
        <name>
          <surname>Reboiro-Jato</surname>
          <given-names>Miguel</given-names>
        </name>
        <xref ref-type="aff" rid="aff-1">1</xref>
        <xref ref-type="aff" rid="aff-2">2</xref>
        <xref ref-type="aff" rid="aff-3">3</xref>
      </contrib>
      <contrib id="author-5" contrib-type="author" corresp="yes">
        <name>
          <surname>Glez-Peña</surname>
          <given-names>Daniel</given-names>
        </name>
        <email>dgpena@uvigo.es</email>
        <xref ref-type="aff" rid="aff-1">1</xref>
        <xref ref-type="aff" rid="aff-2">2</xref>
        <xref ref-type="aff" rid="aff-3">3</xref>
      </contrib>
      <aff id="aff-1"><label>1</label><institution>ESEI: Escuela Superior de Ingeniería Informática, University of Vigo</institution>, <city>Ourense</city>, <state>Galicia</state>, <country>Spain</country></aff>
      <aff id="aff-2"><label>2</label><institution>CINBIO - Centro de Investigaciones Biomédicas, University of Vigo</institution>, <city>Vigo</city>, <state>Galicia</state>, <country>Spain</country></aff>
      <aff id="aff-3"><label>3</label><institution>SING Research Group, Galicia Sur Health Research Institute (IIS Galicia Sur). SERGAS-UVIGO</institution>, <city>Vigo</city>, <state>Galicia</state>, <country>Spain</country></aff>
      <aff id="aff-4"><label>4</label><institution>Bioinformatics Unit, Structural Biology Programme, Spanish National Cancer Research Centre (CNIO)</institution>, <city>Madrid</city>, <state>Madrid</state>, <country>Spain</country></aff>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Winkler</surname>
          <given-names>Robert</given-names>
        </name>
      </contrib>
    </contrib-group>
    <pub-date pub-type="epub" date-type="pub" iso-8601-date="2021-06-18">
      <day>18</day>
      <month>6</month>
      <year iso-8601-date="2021">2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>7</volume>
    <elocation-id>e593</elocation-id>
    <history>
      <date date-type="received" iso-8601-date="2021-01-28">
        <day>28</day>
        <month>1</month>
        <year iso-8601-date="2021">2021</year>
      </date>
      <date date-type="accepted" iso-8601-date="2021-05-21">
        <day>21</day>
        <month>5</month>
        <year iso-8601-date="2021">2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>©2021 López-Fernández et al.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>López-Fernández et al.</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, reproduction and adaptation in any medium and for any purpose provided that it is properly attributed. For attribution, the original author(s), title, publication source (PeerJ Computer Science) and either DOI or URL of the article must be cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="https://peerj.com/articles/cs-593"/>
    <abstract>
      <p>Compi is an application framework to develop end-user, pipeline-based applications with a primary emphasis on: (i) user interface generation, by automatically generating a command-line interface based on the pipeline specific parameter definitions; (ii) application packaging, with compi-dk, which is a version-control-friendly tool to package the pipeline application and its dependencies into a Docker image; and (iii) application distribution provided through a public repository of Compi pipelines, named Compi Hub, which allows users to discover, browse and reuse them easily. By addressing these three aspects, Compi goes beyond traditional workflow engines, having been specially designed for researchers who want to take advantage of common workflow engine features (such as automatic job scheduling or logging, among others) while keeping the simplicity and readability of shell scripts without the need to learn a new programming language. Here we discuss the design of various pipelines developed with Compi to describe its main functionalities, as well as to highlight the similarities and differences with similar tools that are available. An open-source distribution under the Apache 2.0 License is available from GitHub (available at <ext-link ext-link-type="uri" xlink:href="https://github.com/sing-group/compi">https://github.com/sing-group/compi</ext-link>). Documentation and installers are available from <uri xlink:href="https://www.sing-group.org/compi">https://www.sing-group.org/compi</uri>. A specific repository for Compi pipelines is available from Compi Hub (available at <ext-link ext-link-type="uri" xlink:href="https://www.sing-group.org/compihub">https://www.sing-group.org/compihub</ext-link>.</p>
    </abstract>
    <kwd-group kwd-group-type="author">
      <kwd>Computational pipelines</kwd>
      <kwd>Workflow management systems</kwd>
      <kwd>Application development framework</kwd>
    </kwd-group>
    <funding-group>
      <award-group id="fund-1">
        <funding-source> Consellería de Educación, Universidades e Formación Profesional (Xunta de Galicia)</funding-source>
        <award-id> ED431C2018/55-GRC</award-id>
      </award-group>
      <award-group id="fund-2">
        <funding-source> Ministerio de Economía, Industria y Competitividad, Gobierno de España under the scope of the PolyDeep project</funding-source>
        <award-id> DPI2017-87494-R</award-id>
      </award-group>
      <award-group id="fund-3">
        <funding-source> Xunta de Galicia</funding-source>
        <award-id> ED481A-2019/299</award-id>
      </award-group>
      <funding-statement>This work was supported by the Consellería de Educación, Universidades e Formación Profesional (Xunta de Galicia) under the scope of the strategic funding ED431C2018/55-GRC Competitive Reference Group and Ministerio de Economía, Industria y Competitividad, Gobierno de España under the scope of the PolyDeep project (DPI2017-87494-R). Alba Nogueira-Rodríguez is supported by a pre-doctoral fellowship from Xunta de Galicia (ED481A-2019/299). There was no additional external funding received for this study. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
  </article-meta>
</front>
<body>
  <sec sec-type="intro">
    <title>Introduction</title>
    <p>Bioinformatics units routinely deal with massive data analyses, which require combining multiple sequential or parallel steps using specific software tools (<xref rid="ref-13" ref-type="bibr">Perkel, 2019</xref>). Many of these computational pipelines are published regularly in the form of protocols, best practices or even fully runnable pipelines. They implement all the required steps and dependencies in order to ensure the reproducibility of the analyses and facilitate job automation (<xref rid="ref-5" ref-type="bibr">Grüning et al., 2018</xref>). Thus, scientific computational pipelines must provide three key features: reproducibility, portability and scalability. Container technologies such as Docker or Singularity are the most widely used tools to ensure that pipelines run in stable environments (i.e., always using the exact same version of pipeline dependencies) and make it easy to run the pipeline on multiple hardware platforms (e.g., workstations or cloud infrastructures), enforcing reproducibility and portability. Moreover, scalable pipelines must support running on HPC (High Performance Computing) resources using cluster management and job scheduling systems such as Slurm or SGE.</p>
    <p>For the above reasons, a wide variety of workflow management systems have been released in recent years that address these issues in different ways. Tools with graphical user interfaces, such as Galaxy (<xref rid="ref-1" ref-type="bibr">Afgan et al., 2018</xref>), are designed for scientists with little or no programming experience, although such tools can be difficult to set up and configure. Furthermore, command-line based applications such as Nextflow (<xref rid="ref-2" ref-type="bibr">Di Tommaso et al., 2017</xref>), Snakemake (<xref rid="ref-6" ref-type="bibr">Köster &amp; Rahmann, 2012</xref>), or SciPipe (<xref rid="ref-7" ref-type="bibr">Lampa et al., 2019</xref>), provide feature-rich workflow engines oriented to bioinformaticians with medium-to-high programming skills. The Common Workflow Language (CWL; <ext-link ext-link-type="uri" xlink:href="https://www.commonwl.org/">https://www.commonwl.org/</ext-link>) definition represents another alternative, since it defines a specification and offers a reference implementation, but does not provide a complete framework (<xref rid="ref-8" ref-type="bibr">Leipzig, 2017</xref>). Other frameworks like Galaxy or Taverna made significant progress to support the execution of workflows defined in CWL and other tools allow to export their workflows into CWL (e.g., Snakemake) or import them from CWL.</p>
    <p>Despite the existence of such remarkable workflow management systems, scientists with basic scripting skills (e.g., able to create shell scripts invoking command-line tools) but lacking advanced programming skills (e.g., knowledge of programming languages such as Python or Go), are usually overwhelmed due to the high complexity of these systems, and could be hampered to use or create their own workflows. In this sense, tools such as Bpipe (<xref rid="ref-14" ref-type="bibr">Sadedin, Pope &amp; Oshlack, 2012</xref>) help to assemble shell scripts into workflows to aid in job automation, logging and reproducibility.</p>
    <p>Compi is specially designed for researchers who want to take advantage of common workflow engine features (e.g., automatic job scheduling, restart from point of failures, etc.) while maintaining the simplicity and readability of shell scripts, without the need to learn a new programming language. Nevertheless, Compi also incorporates several features that meet the needs of the most advanced users, such as support for multiple programming languages or advanced management and control of workflow execution. In this sense, Compi is more than a workflow engine, it was created as an application framework for developing pipeline-based end-user applications by providing: (i) automatic user interface generation—generates a classical command-line interface (CLI) for the entire pipeline based on its parameter specifications; (ii) application packaging—provides a version-control-friendly mechanism to package the pipeline application and its dependencies into a Docker image; and (iii) application distribution, supported by Compi Hub (<xref rid="ref-12" ref-type="bibr">Nogueira-Rodríguez et al., 2021</xref>)—a public repository where researchers can easily and freely publish their Compi pipelines and related documentation, making them available for other researchers.</p>
    <p>Compi has been adopted by our research group to create pipelines for multiple research projects where other systems were not appropriate enough (e.g., the creation of complex pipelines for phylogenomics or training of models based on deep learning for image classification). Likewise, pipelines developed with Compi in collaboration with other research groups have already been published. These are: Metatax, a pipeline to analyze biological samples based on 16S rRNA gene sequencing (<xref rid="ref-3" ref-type="bibr">Graña Castro et al., 2020a</xref>; <xref rid="ref-4" ref-type="bibr">Graña Castro et al., 2020b</xref>), FastScreen, a pipeline for inferring positive selection in large viral datasets (<xref rid="ref-10" ref-type="bibr">López-Fernández et al., 2020</xref>), and GenomeFastScreen, an extension of the FastScreen pipeline (<xref rid="ref-11" ref-type="bibr">López-Fernández et al., 2021</xref>).</p>
  </sec>
  <sec sec-type="materials|methods">
    <title>Materials &amp; Methods</title>
    <p>This section depicts the most relevant technical details regarding the implementation of Compi. An open-source distribution under the Apache 2.0 License is available from GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/sing-group/compi">https://github.com/sing-group/compi</ext-link>). Documentation and installers are available from <ext-link ext-link-type="uri" xlink:href="https://www.sing-group.org/compi">https://www.sing-group.org/compi</ext-link>.</p>
    <sec>
      <title>XML for pipeline definition</title>
      <p>Compi pipelines are defined in a single XML (eXtensible Markup Language) file that includes: (i) tasks and dependencies between them, (ii) pipeline input parameters that are forwarded to tasks, and (iii) metadata, including parameters and tasks descriptions, which are useful for automatic generation of the user interface, as well as for pipeline documentation.</p>
      <p>We have chosen XML instead of JSON, YAML, or a custom DSL (Domain Specific Language) to reconcile the various requirements simultaneously. First, we wanted a high degree of interoperability. XML, JSON, and YAML can be easily generated and parsed in almost any programming language, although YAML can pose portability issues in some cases as YAML parsers in different languages can produce different results. In contrast, a DSL (<xref rid="ref-2" ref-type="bibr">Di Tommaso et al., 2017</xref>) is less interoperable, being difficult to produce or consume from languages other than the one on which the DSL is based. Second, XML is appropriate for dealing with long chunks of text, such as the embedded source code for pipeline tasks. Since Compi is language agnostic, these tasks can be defined in any programming language. Thanks to the XML CDATA (character data) blocks, which allow declaring a section of the XML that should not be parsed as XML, it is possible to include source code without any alteration. Embedding task code in JSON is virtually not feasible, as tabs and line breaks, which are key characters in languages like Python, must be escaped in JSON files. In YAML, however, it is easier thanks to its multiline literals. In DSLs it depends on whether the programming language the DSL is based on allows you to declare multiline strings easily, as Python does. Third, XML is easy to validate syntactically and semantically through schemas, which are also present in JSON and YAML. DSLs take advantage of the language parser the DSL is based on. Fourth, for security reasons, since Compi is designed to run pipelines defined by third-party programmers, YAML is a less secure language in this regard, as runnable code could be embedded in fields that were not intended for this purpose (see <ext-link ext-link-type="uri" xlink:href="https://www.arp242.net/yaml-config.html">https://www.arp242.net/yaml-config.html</ext-link>). Fifth comes readability. YAML is the clearer winner in this regard, as readability is the key feature of this format. Although XML is less readable than YAML, the comparison with JSON and DSLs is a more subjective matter. Based on these five requirements, the choice was between XML and YAML: while XML is more secure and portable, YAML is more readable. Finally, we selected XML, prioritizing its security, portability and popularity. Also, while writing XML files could be verbose, YAML is syntactically aware of whitespace, which is a welcome feature in the Python community, but still debated outside of it.</p>
    </sec>
    <sec>
      <title>The workflow execution engine</title>
      <p>The workflow execution engine of Compi is implemented purely in Java and is responsible for multi-thread task scheduling, monitoring, and standard error and output logging of tasks. First, it computes the DAG (Direct Acyclic Graph) of the task dependencies, since a task may depend on a set of tasks that must be run before the given task. Right after starting, or whenever a task finishes its execution, the engine reacts and all the tasks that can be run, i.e., the tasks whose dependencies are now complete, are sent to a worker thread pool that has a parameterizable size. When there are no more tasks to run, the pipeline execution ends.</p>
      <p>Every time a task is about to run and there is a free thread in the pool, a new subprocess is spawned by invoking the system Bash interpreter to execute the task script. Pipeline parameters are passed to the task script through environment variables, which is a robust, standard mechanism with two main benefits. On the one hand, environment variables are easily accessed via “$variable_name” or more complex expressions, so pipeline parameters are available directly within the task script. On the other hand, this allows Compi to pass parameters to scripts written in languages other than Bash, because virtually all programming languages give access to environment variables.</p>
      <p>The way to execute languages differently from Bash is through task interpreters. Any task can have a task interpreter defined in the pipeline specification, which is an intermediate, user-defined bash script intended to take the task script as input and call an interpreter from a different programming language.</p>
      <p>Moreover, it is possible to define task runners, which are the same concept as interpreters, but with a different purpose. They are not defined within the pipeline specification, but rather at execution time, and are intended to tailor task execution to specific computing resources, without modifying the pipeline itself. In this way, the workflow definition is decoupled from the workflow execution, making it possible to change the way tasks are executed without modifying the workflow XML file. For instance, if the tasks must be run in a cluster environment such as SGE or Slurm, computations must be initiated via a submission command (qsub in SGE or srun in Slurm). Task runners intercept task execution by changing the default Bash interpreter with queue submissions.</p>
    </sec>
    <sec>
      <title>Compi project architecture</title>
      <p>Compi comprises three main modules. The most important is the <italic>core</italic> module, which contains the workflow execution engine, with its main data structures. On top of it, there are two additional modules. On the one hand, the <italic>cli</italic> (command-line interface) module contains the command-line user interface for running pipelines, which generates a specific pipeline application tailored for pipeline tasks and parameters. On the other hand, the <italic>dk</italic> (Development Kit) module allows to create a portable application in the form of a Docker image and publishing the pipelines at Compi Hub.</p>
    </sec>
    <sec>
      <title>Compi Hub</title>
      <p>As discussed above, Compi Hub is a public repository where Compi pipelines can be published. The Compi Hub front-end was implemented using the Angular v7 web application framework, while the back-end was implemented using TypeScript and offers a RESTful API that supports all the functionality of the front-end. This REST API is also used by the <italic>compi-dk</italic> tool to allow pipeline developers to publish their pipelines from the command line. The Compi Hub back-end runs in a Node.js server and uses a MongoDB database to store the data.</p>
    </sec>
  </sec>
  <sec sec-type="results">
    <title>Results</title>
    <p>Compi can be seen as an ecosystem comprising: (i) <italic>compi</italic>, the workflow engine with a command-line user interface to control the pipeline execution, (ii) <italic>compi-dk</italic>, a command-line tool to assist in the development and packaging of Compi-based applications, and (iii) Compi Hub, a public repository of Compi pipelines that allows users to easily discover, navigate and reuse them (<xref rid="ref-12" ref-type="bibr">Nogueira-Rodríguez et al., 2021</xref>). <xref ref-type="fig" rid="fig-1">Figure 1</xref> illustrates the development and deployment lifecycle of Compi-based applications. A Compi-based application is a CLI application in which a user can run a pipeline in whole or in part by providing only the parameters of each task. This CLI displays all parameter names and descriptions following standard CLI conventions to help users use the pipeline application more easily.</p>
    <fig id="fig-1" orientation="portrait" position="float">
      <object-id pub-id-type="doi">10.7717/peerjcs.593/fig-1</object-id>
      <label>Figure 1</label>
      <caption>
        <title>Lifecycle of developing, packaging, distributing, and running Compi-based applications.</title>
        <p>For developers, this lifecycle comprises three main stages: pipeline development (including testing the pipeline as pipeline user), application packaging, and distribution.</p>
      </caption>
      <graphic xlink:href="peerj-cs-07-593-g001"/>
    </fig>
    <p>Compi helps pipeline developers by automatically determining the parameters required by a pipeline. This implies, on the one hand, that comprehensive help is generated for the user on how to use the application and, on the other hand, that the application parameters are automatically parsed when the pipeline is executed. This is complemented by several built-in features added to the CLI application, such as an advanced task execution control, allowing to run only specific tasks in a pipeline, or log management support per task. Therefore, building a CLI application from a Compi pipeline is a straightforward process that allows Compi users to focus on pipeline development. The “Pipeline design” section explains how Compi users can design their pipelines.</p>
    <p>Once the pipeline is developed, it can take two complementary paths: it can be executed directly with <italic>compi</italic> (top right of <xref ref-type="fig" rid="fig-1">Fig. 1</xref>) or it can be packaged as a portable end-user CLI application in a Docker image using <italic>compi-dk</italic> (application packaging area in <xref ref-type="fig" rid="fig-1">Fig. 1</xref>), which can be then executed via Docker. Pipeline execution and dependency management are explained in the “Pipeline execution and dependency management” section, while the application packaging in Docker images is explained in the “Reproducible Application Packaging with Docker” section.</p>
    <p>When an end-user pipeline application is ready to be published, Compi provides pipeline developers with a distribution platform called Compi Hub, where they can share different versions of their pipelines along with usage documentation, example datasets, parameter values, as well as links of interest (e.g., Docker Hub, Github). Community users can then browse the Compi Hub for pipelines and can explore the helpful documentation generated by the authors of each pipeline, as well as other information automatically generated by the platform, such as an interactive DAG representation of the pipeline. This topic is covered in the “Pipeline distribution via Compi Hub” section.</p>
    <p>Finally, it is important to note that <italic>compi-dk</italic> projects are particularly designed to be compatible with version control systems, as the required configuration and pipeline files are text files. Dependency management via a Dockerfile is key to achieve this, complemented by Compi-specific dependency management performed by <italic>compi-dk</italic> (e.g., the <italic>compi</italic> executable added to the Docker image of the pipeline). Therefore, a <italic>compi-dk</italic> project only requires that <italic>compi</italic>, <italic>compi-dk</italic> and Docker are installed in order to be built or run. With this feature, Compi gives pipeline developers the ability to use version control systems to keep their pipeline safe and to version their code.</p>
    <sec>
      <title>Pipeline design</title>
      <p>As explained previously, Compi pipelines are defined in an XML document that includes user parameters, tasks, and metadata. <xref ref-type="fig" rid="fig-2">Figure 2</xref> shows an example of a minimal pipeline. This sample pipeline defines two parameters (“name” and “output”), described in the corresponding “params” section, and two tasks (“greetings” and “bye”) described in the “tasks” section. The parameter descriptions are used to automatically generate both the user interface and the documentation for the pipeline. In the same way and for the same purpose, tasks are described in the “metadata” section, which allows the pipeline developer to describe them in a human-readable manner.</p>
      <fig id="fig-2" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerjcs.593/fig-2</object-id>
        <label>Figure 2</label>
        <caption>
          <title>Minimal pipeline example of a Compi pipeline with two tasks.</title>
          <p>A specific section (“params”) defines two parameters (“name” and “output”), used in the two pipeline tasks, named “greetings” and “bye”. Both tasks print the value of the “name” parameter to the path specified in the “output” parameter. By default, tasks are executed as Bash commands and this is the case of the “greetings” task. The “bye” task is written in Perl and the “interpreter” parameter of the task definition indicates how to invoke the Perl interpreter to run the task code.</p>
        </caption>
        <graphic xlink:href="peerj-cs-07-593-g002"/>
      </fig>
      <p>Tasks are defined as “task” elements within the “tasks” section. The main components of a task are: source code, parameters and dependencies. The source code is placed inside a “task” element, and the parameters used by a task are defined within the “params” attribute, and those tasks that the current task depends on are defined in the “after” attribute. For instance, in <xref ref-type="fig" rid="fig-2">Fig. 2</xref>, the “bye” task, uses the “name” and “output” parameters, and depends on the “greetings” task. Parameter values are passed to the tasks as environment variables, which is a standard method that any programming language gives access to. In this way, the Compi workflow execution engine does not need to process the task code in any way (e.g., to perform variable substitution), guaranteeing the possibility of using any programming language to define the code of the task. In this sense, the task code is written in Bash by default, although other scripting languages (e.g., Python, R, AWK, etc.) can be used under a suitable interpreter through an “interpreter” parameter. For example, in <xref ref-type="fig" rid="fig-2">Fig. 2</xref>, the “bye” task is written in Perl and the “interpreter” parameter indicates how to invoke the Perl interpreter to run the task code.</p>
      <p>A special type of tasks are parallel iterative tasks (or loop tasks), defined via “foreach” elements, which spawn multiple parallel processes with the same code on a collection of items. This collection of items is provided by a user-specified source of items, which could be a comma-separated list of values, a range of numbers, the files in a specific directory, a parameter whose value is a list of values separated by commas, or even a custom command whose output lines are taken as items. Each item is available to the task code as an environment variable.</p>
      <p>When all spawned tasks have finished, the foreach task ends as well and subsequent dependent tasks can be run. Nevertheless, there are several scenarios in which a pipeline developer can define multiple consecutive foreach tasks intended to iterate over the same collection of independent items. In these scenarios, when one iteration of a loop has finished, the corresponding iteration of the next loop could start without waiting for the entire previous loop to finish. Compi supports this type of interaction between foreach tasks by simply adding the ‘*’ prefix to the task name when declaring the dependency on the “after” attribute. For example, in a scenario where a pipeline must process a set of samples (e.g: “case-1”, “case-2”, “control-1”, “control-2”) by performing two consecutive operations: preprocess and analyze, the dependency of the “preprocess” task on the “analyze” task can be prefixed with ‘*’ (i.e., <italic>after</italic> =<italic>”*preprocess”</italic>) to tell Compi that iterations over the “analyze” loop could start when the corresponding iterations over the “preprocess” loop have finished.</p>
      <p><xref ref-type="fig" rid="fig-3">Figure 3</xref> shows an example of this, where two foreach tasks “preprocess” and “analyze” iterate over the same set of items (“samples”). The “analyze” task depends on the “preprocess” task, but at an iteration-level (<italic>after</italic> =<italic>”*preprocess”</italic>). Without the ‘*’ prefix, the “analyze” task will only start when the whole “preprocess” task has finished.</p>
    </sec>
    <sec>
      <title>Pipeline execution and dependency management</title>
      <p>One of the main features of Compi is the creation of a classic CLI for the entire pipeline based on its parameter specifications to aid users run the pipeline. This CLI is displayed when a user executes <italic>“compi run -p pipeline.xml –help”</italic>, which describes the Compi execution parameters and specification of each pipeline task. <xref ref-type="supplementary-material" rid="supp-1">File S1</xref> shows this CLI for the RNA-Seq Compi pipeline (<ext-link ext-link-type="uri" xlink:href="https://www.sing-group.org/compihub/explore/5d09fb2a1713f3002fde86e2">https://www.sing-group.org/compihub/explore/5d09fb2a1713f3002fde86e2</ext-link>).</p>
      <p>As <xref ref-type="fig" rid="fig-4">Fig. 4</xref> illustrates, the execution of Compi pipelines can be controlled using multiple parameters that fall in three main categories: pipeline inputs (i.e., the pipeline definition and its input parameters), logging, and execution control. In <xref ref-type="fig" rid="fig-4">Fig. 4A</xref>, the “<italic>compi run</italic>” command receives the pipeline definition file explicitly, while the <italic>“–params”</italic> option indicates that the input parameters must be read from the “compi.params” file. On the other hand, in <xref ref-type="fig" rid="fig-4">Fig. 4B</xref> the pipeline definition file is omitted and <italic>compi</italic> assumes that the pipeline definition must be read from a file named “pipeline.xml“ located in the current working directory. Also, pipeline parameters are passed on the command line after all the “<italic>compi run</italic>” parameters and separated by the ‘–’ delimiter.</p>
      <fig id="fig-3" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerjcs.593/fig-3</object-id>
        <label>Figure 3</label>
        <caption>
          <title>Example of Compi pipeline using iterative foreach tasks.</title>
          <p>Note the “*” character when the dependency of the “analyze” task on the “preprocess” task is defined. This way, the second foreach is binded to the first one, meaning that the two foreach tasks iterate over the same collection of elements and allowing Compi to start the execution of each iteration of the second foreach right after the corresponding iterations of the first foreach had finished.</p>
        </caption>
        <graphic xlink:href="peerj-cs-07-593-g003"/>
      </fig>
      <fig id="fig-4" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerjcs.593/fig-4</object-id>
        <label>Figure 4</label>
        <caption>
          <title>Examples of Compi parameters to control how pipelines must be executed.</title>
          <p>Compi parameters belong to three main categories: pipeline inputs (i.e., the pipeline definition and its input parameters), logging, and execution control (i.e., specify whith tasks must be executed).</p>
        </caption>
        <graphic xlink:href="peerj-cs-07-593-g004"/>
      </fig>
      <p>Regarding logging options, both include the <italic>“–logs”</italic> option to specify a directory to save the standard (stdout) and error (stderr) outputs of each task along with the specific parameter values used in each execution. They are saved in three different files named with the name of the corresponding task as prefix (e.g., <italic>“task-name.out.log“</italic>,<italic>“task-name.err.log“</italic>, and <italic>“task-name.params“</italic>). To avoid unnecessary file creation, Compi does not save task outputs unless the “<italic>–logs</italic>” option is used. However, it is possible to specify which tasks should be logged using the <italic>“–log-only-task”</italic> or <italic>“–no-log-task”</italic> parameters. In addition to the task specific logs, Compi displays its own log messages during pipeline execution. These messages can be disabled by including <italic>“–quiet“</italic> as shown in <xref ref-type="fig" rid="fig-4">Fig. 4B</xref>. In contrast, in the example given in <xref ref-type="fig" rid="fig-4">Fig. 4A</xref>, the “<italic>–show-std-outs</italic>” option forces Compi to forward each task log to the corresponding Compi output, which is very useful for debugging purposes during pipeline development.</p>
      <p>The third group of options allows to control the execution of the pipeline. For instance, the <italic>“–num-tasks”</italic> parameter used in <xref ref-type="fig" rid="fig-4">Fig. 4A</xref> sets the maximum number of tasks that can be run in parallel. It is important to note that this is not necessarily equivalent to the number of threads the pipeline will use, as some tasks may use parallel processes themselves. The <italic>“–abort-if-warnings”</italic> option, also used in the example in <xref ref-type="fig" rid="fig-4">Fig. 4A</xref>, tells Compi to abort the pipeline execution if there are warnings in the pipeline validation. This is a useful and recommended option for pipeline testing during development, to avoid undesired effects that may arise from ignoring such warnings. A typical scenario that causes a warning is when the name of a pipeline parameter is found inside the task code, but the task does not have access to it, because it is not a global parameter nor defined in the set of task parameters.</p>
      <p>One of the most notable features of the Compi workflow execution engine is that it allows a fine-grained control over the execution of the pipeline tasks. While many workflow engines only allow launching the entire pipeline or partially relaunching a pipeline from a point of failure, which can also be done in Compi using the <italic>“resume”</italic> command, Compi also allows launching sub-pipelines using modifiers such as <italic>“–from”</italic>, <italic>“–after“</italic>, <italic>“–until“</italic> or <italic>“–before“</italic>. In the example shown in <xref ref-type="fig" rid="fig-4">Fig. 4B</xref>, a combination of the <italic>“–from“</italic> and <italic>“–until“</italic> modifiers is used, resulting in the execution of all tasks in the path between “task-1” and “task-10”, including “task-1” and “task-10”. If the <italic>“–after“</italic> and <italic>“–before“</italic> are used instead, then “task-1” and “task-10” are not executed. A fifth modifier is <italic>“–single-task“</italic>, which allows only the specified task to execute and is not compatible with the other four modifiers. The Compi documentation (<ext-link ext-link-type="uri" xlink:href="https://www.sing-group.org/compi/docs/running_pipelines.html#examples">https://www.sing-group.org/compi/docs/running_pipelines.html#examples</ext-link>) includes several examples to illustrate how each of these options work.</p>
      <p>Compi runs each task as a local command by default (<xref ref-type="fig" rid="fig-5">Fig. 5A</xref>). This means that if a task invokes a certain tool (e.g., a ClustalOmega alignment running <italic>“clustalo -i /path/to/input.fasta -o /path/to/output.fasta”</italic>), this tool must be available either from the path environment variable or by including the absolute path to the binary executable. Since dependency management is always cumbersome, a special effort has been made to offer developers and users various alternatives to deal with this problem, which are explained below.</p>
      <fig id="fig-5" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerjcs.593/fig-5</object-id>
        <label>Figure 5</label>
        <caption>
          <title>Different ways of executing tasks involving external dependencies in Compi.</title>
          <p>The upper left quadrant shows the simplest case (scenario A), where the pipeline is run on a single machine and external dependencies must be available on the system. One way to manage external dependencies is by using containers like the one shown in the bottom left quadrant. While scenario C shows the case of a Compi application which runs in a container with all dependencies, scenario D shows a Compi application that uses a Docker image for running the external dependencies. Custom Compi runners can also be used to submit tasks into an HPC cluster, as scenario B shows. Finally, it is also possible to combine containerized execution of Compi tasks with Docker in a clustered environment by using a container orchestration system such as Kubernetes.</p>
        </caption>
        <graphic xlink:href="peerj-cs-07-593-g005"/>
      </fig>
      <p>One way to address this issue is by means of a file with a custom XML runner definition, as done in the example shown in <xref ref-type="fig" rid="fig-4">Fig. 4A</xref> with <italic>“–runners-config pipeline-runners.xml”</italic>. Individual runners are defined through a “runner“ element within a runners file, where the “task“ attribute is used to specify the list of tasks that the runner must execute. In this way, when a task identifier is assigned to a runner, Compi will ask the runner to run the corresponding task code (instead of running it as a local command). The usage of pipeline runners to handle dependencies allows Docker images to take responsibility for them (<xref ref-type="fig" rid="fig-5">Fig. 5D</xref>). For instance, <xref ref-type="fig" rid="fig-6">Fig. 6A</xref> shows a pipeline task named “align” that uses a tool (defined by the pipeline parameter “clustalomega”) that receives a file as input and produces an output. The runner defined in <xref ref-type="fig" rid="fig-6">Fig. 6B</xref> for the same task runs the specified task code (available in the environment variable “task_code”) using a Docker image. The runner here is almost a generic Docker runner, and the key points are:</p>
      <fig id="fig-6" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerjcs.593/fig-6</object-id>
        <label>Figure 6</label>
        <caption>
          <title>Use of custom Compi runners.</title>
          <p>(A) Task of a Compi pipeline that invokes a command defined in the “clustalomega” pipeline parameter. This command must be available in the system. (B) Example of Compi runner for the “align” task that runs the task source code (available in the “task_code” environment variable) using a Docker image that has the command installed.</p>
        </caption>
        <graphic xlink:href="peerj-cs-07-593-g006"/>
      </fig>
      <list list-type="simple" id="list-1">
        <list-item>
          <label> •</label>
          <p>First, the creation of a variable (“$envs”) with the list of parameters that must be passed as environment variables to the Docker container.</p>
        </list-item>
        <list-item>
          <label> •</label>
          <p>Second, run the Docker image with the list of environment variables and mount the directory where the command has the input and output files (“workingDir” in this example).</p>
        </list-item>
      </list>
      <p>Such a Docker runner would allow to follow an image-per-task execution pattern, where each task is executed using a different container image (<xref rid="ref-15" ref-type="bibr">Spjuth et al., 2018</xref>). An example of this execution pattern can be found in the GenomeFastScreen pipeline (<ext-link ext-link-type="uri" xlink:href="https://sing-group.org/compihub/explore/5e2eaacce1138700316488c1">https://sing-group.org/compihub/explore/5e2eaacce1138700316488c1</ext-link>), although in this case <italic>“docker run”</italic> commands are included in each task rather than provided in a runners file for the sake of simplicity. Following this image-per-task execution pattern, it is possible for a pipeline to use different versions of the same software or two tools that require different versions of some dependencies.</p>
      <p>In addition, custom runners can also be used to submit pipeline tasks to a job scheduler such as SGE, Torque or SLURM in supercomputers or computer clusters (<xref ref-type="fig" rid="fig-5">Fig. 5B</xref>). For instance, <xref ref-type="fig" rid="fig-7">Fig. 7</xref> shows a generic Slurm runner. Some <italic>srun</italic> parameters may need to be adjusted for each specific cluster and the <italic>“–export”</italic> parameter must be used to export all environment variables to the process to be executed, as the task parameters are declared as environment variables.</p>
      <p>In the same way, when using Compi runners, it is also possible to combine containerized execution of Compi tasks with Docker in a clustered environment, using a container orchestration system such as Kubernetes (<xref ref-type="fig" rid="fig-5">Fig. 5E</xref>).</p>
      <p>Another way to achieve dependency management is by building a monolithic Docker image with Compi, the pipeline itself and all its dependencies (<xref ref-type="fig" rid="fig-5">Fig. 5C</xref>). This topic is explained in “Reproducible Application Packaging with Docker”.</p>
      <p>Finally, dependency management can be also delegated to external systems. For example, Compi allows the use of Conda/Bioconda packages seamlessly. Each task can use them simply by activating and deactivating the corresponding Conda environments before executing the specific commands. The Metatax pipeline (<ext-link ext-link-type="uri" xlink:href="https://www.sing-group.org/compihub/explore/5d807e5590f1ec002fc6dd83">https://www.sing-group.org/compihub/explore/5d807e5590f1ec002fc6dd83</ext-link>) illustrates this. For instance, <xref ref-type="fig" rid="fig-8">Fig. 8</xref> shows the execution of a script (whose name is defined by the “validate_mapping_file” parameter, highlighted in bold) from the Qiime Bioconda package.</p>
      <fig id="fig-7" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerjcs.593/fig-7</object-id>
        <label>Figure 7</label>
        <caption>
          <title>Generic Compi runner for submitting tasks into a Slurm system.</title>
        </caption>
        <graphic xlink:href="peerj-cs-07-593-g007"/>
      </fig>
      <fig id="fig-8" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerjcs.593/fig-8</object-id>
        <label>Figure 8</label>
        <caption>
          <title>Dependency management in Compi pipelines using Conda/Bioconda packages.</title>
          <p>In this example from the Metatax pipeline, the “validate_mapping” task activates the qiime1 environment to load the Qiime Bioconda package, executes a script from this environment, and finally deactivates the environment. Each task can use them by simply activating and deactivating the corresponding environments before executing the specific commands.</p>
        </caption>
        <graphic xlink:href="peerj-cs-07-593-g008"/>
      </fig>
    </sec>
    <sec>
      <title>Reproducible application packaging with docker</title>
      <p>Compi enables the creation of portable end-user CLI applications for pipelines that can be distributed as Docker images. As noted in the previous section, this is another way to deal with dependency management, as such Docker images contain all the dependencies required by the pipeline. Pipelines distributed in this way follow an image-per-pipeline execution pattern in which all tasks are executed using the same image container (<xref rid="ref-15" ref-type="bibr">Spjuth et al., 2018</xref>) (<xref ref-type="fig" rid="fig-5">Fig. 5C</xref>), and can even be run using Docker-compatible container technologies such as Singularity.</p>
      <p>The <italic>compi-dk</italic> command-line tool is provided to assist in the development and packaging of Compi-based applications into Docker images. Pipeline development starts with the creation of a new <italic>compi-dk</italic> project with the “<italic>compi-dk new-project</italic>” command, which creates a project directory and inizialites two template files: <italic>pipeline.xml</italic> and <italic>Dockerfile</italic>. After this, the definition of the pipeline can start by modifying the <italic>pipeline.xml</italic> template and the subsequent local testing (using the <italic>compi</italic> command). Also, it can be tested by building a Docker image (using <italic>compi-dk</italic>) and running the containerized pipeline. As for the latter case, when the <italic>compi-dk build</italic> command is executed on the project directory, a Docker image for the pipeline is created. This image contains the <italic>compi</italic> executable file and a specific <italic>pipeline.xml file,</italic> along with the pipeline dependencies as defined in the <italic>Dockerfile</italic>. <xref ref-type="fig" rid="fig-9">Figure 9</xref> shows the <italic>Dockerfile</italic> of the MINC Computer Vision pipeline for image classification based on Deep Learning (<ext-link ext-link-type="uri" xlink:href="https://www.sing-group.org/compihub/explore/5d08a9e41713f3002fde86d5">https://www.sing-group.org/compihub/explore/5d08a9e41713f3002fde86d5</ext-link>). The <italic>Dockerfile</italic> skeleton was automatically generated by <italic>compi-dk</italic>, being only necessary to add the “RUN” commands for the installation of “gluoncv” and “gnuplot” dependencies. In this regard, it is important to note that special attention was placed on the ability to derive a pipeline application image from any preexisting Docker image of interest (e.g., images with bioinformatics packages). For instance, the RNA-Seq Compi pipeline discussed above was created using the DEWE (<xref rid="ref-9" ref-type="bibr">López-Fernández et al., 2019</xref>) Docker image as a base image and the MINC Computer Vision was created using one of the Apache MXNet Docker images as a base image.</p>
      <fig id="fig-9" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerjcs.593/fig-9</object-id>
        <label>Figure 9</label>
        <caption>
          <title>Docker image for the MINC computer vision pipeline (<ext-link ext-link-type="uri" xlink:href="https://sing-group.org/compihub/explore/5d08a9e41713f3002fde86d5">https://sing-group.org/compihub/explore/5d08a9e41713f3002fde86d5</ext-link>).</title>
        </caption>
        <graphic xlink:href="peerj-cs-07-593-g009"/>
      </fig>
      <p>When working with a <italic>compi-dk</italic> project, it is also possible to create a Docker image for a pipeline that follows the image-per-task execution pattern. This is the case of the GenomeFastScreen pipeline (<ext-link ext-link-type="uri" xlink:href="https://sing-group.org/compihub/explore/5e2eaacce1138700316488c1">https://sing-group.org/compihub/explore/5e2eaacce1138700316488c1</ext-link>), in which, as explained above, a <italic>“docker run”</italic> command is included within each task instead of managing the execution of Docker using an external runner file. Regardless, most of the tasks within this pipeline run under external Docker images, thus following the image-per-task execution pattern. As the GenomeFastScreen pipeline itself is also distributed as a Docker image, it must be able to run Docker images as well (please refer to the pipeline documentation for details on how to do this).</p>
    </sec>
    <sec>
      <title>Pipeline distribution via Compi Hub</title>
      <p>Compi enables the creation of portable end-user CLI applications for pipelines that can be distributed as Docker images. As noted in the previous section, this is another way to deal with dependency management, as such Docker images contain all the dependencies required by the pipeline. Pipelines distributed this way follow an image-per-pipeline execution pattern in which all tasks are executed using the same image container (<xref rid="ref-15" ref-type="bibr">Spjuth et al., 2018</xref>) (<xref ref-type="fig" rid="fig-5">Fig. 5C</xref>), and can even be run using Docker-compatible container technologies such as Singularity.</p>
      <p>Once the pipeline development is completed, it can be released through Compi Hub to increase the visibility and benefit from the Compi Hub features. Pipelines can be registered using the Compi Hub web interface (<ext-link ext-link-type="uri" xlink:href="https://www.sing-group.org/compihub">https://www.sing-group.org/compihub</ext-link>) or using the “<italic>compi-dk hub-push</italic>” command. Compi Hub can store several versions of a pipeline, each of them associated to a <italic>pipeline.xml</italic> file where the specific pipeline version is defined. In addition, since Compi Hub does not store either the full source code (e.g., scripts included in the <italic>pipeline.xml</italic> as source files) or the Docker images themselves, pipeline publishers are encouraged to: (i) publish the source code (i.e., the <italic>compi-dk</italic> project) in public repositories such as GitHub or GitLab to allow users to re-build the project locally at any time; and (ii) push the corresponding Docker image to the Docker Hub registry so that users can pull the image and follow the instructions to run the pipeline application.</p>
      <p>The Compi Hub website lists publicly available pipelines and gives access to all pipelines. When a pipeline is selected, the main pipeline information is displayed, including title and description, creation date, as well as links to external repositories in GitHub or Docker Hub. In addition, for each pipeline version, Compi Hub displays the following information:</p>
      <list list-type="simple" id="list-2">
        <list-item>
          <label> •</label>
          <p>Overview: this section is headed by the pipeline DAG, generated in the backend using the “<italic>compi export-graph</italic>” command. Since it is an interactive graph, visitors can use it to navigate to each task description. <xref ref-type="fig" rid="fig-10">Figure 10</xref> shows the Metatax pipeline DAG (<ext-link ext-link-type="uri" xlink:href="https://www.sing-group.org/compihub/explore/5d807e5590f1ec002fc6dd83">https://www.sing-group.org/compihub/explore/5d807e5590f1ec002fc6dd83</ext-link>). The DAG is followed by two tables, one containing the pipeline tasks and their associated descriptions, and a second one containing global parameters of the pipeline. Finally, this section encloses one table for each task with descriptions and specific parameters. It is important to note that all this information is automatically generated from the pipeline XML.</p>
        </list-item>
        <list-item>
          <label> •</label>
          <p>Readme: this section shows the content of the README.md file when it is present in the <italic>compi-dk</italic> project. This file should be used to provide a comprehensive description of the pipeline, as well as instructions on how to use it.</p>
        </list-item>
        <list-item>
          <label> •</label>
          <p>Dependencies: this section shows the content of the DEPENDENCIES.md file when it is present in the <italic>compi-dk</italic> project. We recommend that pipeline developers include this file with a human-readable description of the pipeline dependencies and the specific versions used to develop the pipeline.</p>
        </list-item>
        <list-item>
          <label> •</label>
          <p>License: this section shows the content of the LICENSE file when it is present in the <italic>compi-dk</italic> project. We encourage pipeline developers to include this file in their <italic>compi-dk</italic> projects so that the terms of use of the pipeline are clear.</p>
        </list-item>
        <list-item>
          <label> •</label>
          <p>Dataset: this section contains a list of datasets that can be used to test the different versions of the pipeline. It is shown when a pipeline publisher associates a test dataset with the displayed pipeline. In addition to the instructions given in the Readme section, we also recommend that pipeline publishers provide test datasets to help users test the pipelines themselves.</p>
        </list-item>
        <list-item>
          <label> •</label>
          <p>Runners: this section displays a list of example runner configurations when they are present in the <italic>compi-dk</italic> project. Runner configurations must be stored as XML files within the “runners-example” directory of the project.</p>
        </list-item>
        <list-item>
          <label> •</label>
          <p>Params: this section shows a list of example parameter configurations when they are present in the <italic>compi-dk</italic> project. Parameter configurations must be stored as plain-text key-value files in the “params-example” directory of the project.</p>
        </list-item>
      </list>
      <p>As can be seen, Compi Hub was not designed to be a merely pipeline repository. We seek that developers accompany each pipeline with all the necessary information to ensure its portability and reproducibility by other researchers.</p>
      <fig id="fig-10" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerjcs.593/fig-10</object-id>
        <label>Figure 10</label>
        <caption>
          <title>DAG of Metatax as shown in Compi Hub (<ext-link ext-link-type="uri" xlink:href="https://www.sing-group.org/compihub/explore/5d807e5590f1ec002fc6dd83">https://www.sing-group.org/compihub/explore/5d807e5590f1ec002fc6dd83</ext-link>).</title>
          <p>Compi Hub automatically generates it by running the “compi export-graph” command in the backend. For each, tasks are drawn with a dashed border to differentiate them from regular tasks.</p>
        </caption>
        <graphic xlink:href="peerj-cs-07-593-g010"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="discussion">
    <title>Discussion</title>
    <p>Workflow management systems play a key role in the development of data science processing pipelines in multiple fields, such as bioinformatics or machine learning, among others. There are multiple solutions and approaches to develop flexible, portable, usable, maintainable and reproducible analysis pipelines in an easy way. With Compi, we progressed from a state-of-the-art capable workflow management system to an entire application framework, focusing on the transition from pipeline development to end users and the community. In this sense, we have put special emphasis on providing pipelines with an advanced and automatically generated CLI, which is aware of the pipeline structure, parameters and tasks descriptions, to facilitate its adoption by final users. Moreover, Compi aids pipeline developers in creating all-in-one distributable Docker images, as well as share the pipeline in an online automatically documented hub where it will be available to the community. This combination with Docker has the added benefit that Compi projects can be built into runnable CLI applications using only text files (i.e., a “pipeline.xml” file, a Dockerfile, and other project files), allowing pipeline developers to use control version systems to keep track of their development. Compi pipelines can be executed in multiple computing layouts without any modification, from running natively on a single machine to a high-performance, fully containerized cluster environment, tailored to the needs of the end user. At design level, we have prioritized the use of well-known standards, such as XML, low intrusiveness and language agnosticism, targeting a broader user community. In this sense, we avoid defining pipelines via DSLs based on a specific programming language, forcing a scripting language to define each task code, or be coupled to specific dependency management systems, such as Python (Conda), R (Cran) or Java (Maven).</p>
    <p>Unlike Snakemake, the Nextflow and SciPipe tools allow dynamic scheduling, that is, the ability to change the pipeline structure dynamically to schedule a different number of tasks based on the results of a previous step or any other parameter. Compi allows dynamic scheduling via (i) the “<italic>if</italic> “ attribute of tasks, which executes a command just before the task is about to run, allowing the task to be skipped dinamically, and (ii) foreach loops, which can take their iteration values from the output of a command, which is executed just before the foreach loop is about to run, allowing, for instance, to do more parallel iterations depending on the number of files generated by a previous task.</p>
    <p>Compi, Snakemake, and Nextflow are language independent, allowing external scripts written in any programming language to be invoked.</p>
    <p>Similar to Compi runners, Nextflow defines executors, which are the components that determine where a pipeline process runs and its execution is supervised. It provides multiple built-in executors to manage execution on SGE, SLURM, Kubernetes, and many others. In SciPipe, this can be achieved by using the <italic>Preprend</italic> field when defining processes, similarly to how Compi runners work.</p>
    <p>Regarding containerization, as explained above, when pipeline tasks need to run in isolated containers, Compi users must include the corresponding command (e.g., “<italic>docker run</italic>”) in the task code or in a runner that tackles the execution of such tasks. Additionally, pipeline developers can create a Docker image for the entire pipeline so that all tasks are executed in the same container. In both cases, the developer must mount the paths to the input and output files of each task when using Docker. In this sense, Nextflow provides built-in support to run individual tasks or complete pipelines using Docker, Singularity or Podman images. In the case of Docker, Nextflow is able to mount the input and output paths automatically, since it is aware of the files needed by tasks. Similarly, Snakemake has built-in support to execute complete pipelines on Docker images and individual rules in isolated Conda environments. The SciPipe documentation does not provide information on how to containerize pipelines.</p>
    <p>Logging is another important feature of workflow management systems, allowing pipeline users to see how execution went and determine causes of errors if necessary. SciPipe has been designed with special care on logging and collecting metadata about each executed task. Following a data-centric audit logging approach, SciPipe generates a JSON file for each output file that contains the full trace of the tasks that were executed to generate it. Nextflow provides a log command that returns useful information about a specific pipeline execution, and incorporates a “<italic>–with-report</italic>” option in the run command that instructs Nextflow to generate an HTML execution report that includes many useful metrics about a specific workflow execution. It also supports a “<italic>–with-trace</italic>” option in the run command that generates an execution trace file that contains useful information about each process executed as part of the pipeline (e.g., submission time, start time, completion time, CPU, and memory usage). As previously stated, the “compi run” command provides the “<italic>–logs</italic>” option to specify a directory to save the stdout and stderr outputs and the specific parameter values of each task execution in separated files prefixed with the corresponding task name.</p>
    <p>Nextflow has nf-core, an environment with a dual purpose: to provide an online repository of Nextflow pipelines and to provide a command-line tool to interact with the repository and manage the execution of the hosted pipelines. Similarly, the Compi Hub repository allows users to discover and explore pipelines, and the <italic>compi-dk</italic> tool allows to push pipelines to the hub via command line. Snakemake does not provide a similar public repository, but a dedicated GitHub project exists (<ext-link ext-link-type="uri" xlink:href="https://github.com/snakemake-workflows/docs">https://github.com/snakemake-workflows/docs</ext-link>).</p>
    <p>After these considerations, and given the choices available, we believe that Compi may be a reasonable choice for researchers with CLI skills looking to create medium complexity pipelines without requiring them to learn new programming languages (e.g., to learn the Nextflow DSL or Go for SciPipe pipeline development). In this way, researchers would benefit from the common features of the workflow management engine that Compi offers without the need for much training. In addition, thanks to the auto-generated CLI, Compi would be the most suitable solution for those pipelines meant to be used by researchers as end-user applications. As the existing pipeline examples presented in the previous section demonstrate, pipeline-based applications developed in this way can easily be distributed as Docker images. Finally, to illustrate the way of creating Compi pipelines and the main differences with other workflow management systems, we have implemented a Nextflow example pipeline in Compi (<xref ref-type="supplementary-material" rid="supp-2">File S2</xref>). This simple example, the description made in the previous section, and the public pipelines available on Compi Hub, will allow potential users to determine when Compi may be the most suitable option.</p>
  </sec>
  <sec sec-type="conclusions">
    <title>Conclusions</title>
    <p>Compi is an application framework for developing pipeline-based end-user applications in bioinformatics and data science. Two Compi design principles are low intrusiveness and language agnosis, with the goal of covering a wide variety of scenarios and providing the most flexibility to pipeline developers. Noteworthy, Compi pipelines can be executed in multiple computing layouts without the need to modify the pipeline definition, from running natively on a single machine to a fully-containerized, high-performance cluster environment, tuned for the needs of the end user.</p>
    <p>To complement the Compi workflow execution engine, we have also created Compi Development Kit (<italic>compi-dk</italic>) and Compi Hub. Thanks to the Compi Development Kit, pipelines can be packaged as self-contained Docker images that also include pipeline dependencies, and can be shared publicly with minimal effort using Compi Hub.</p>
    <p>Future work include, among other tasks, the following issues: (i) improving the metadata section of the pipelines to allow the inclusion of more information (e.g., licensing, attributions, or custom information); (ii) including the possibility of creating stackable runners that gives more flexibility and power to customize task execution; and (iii) enhancing the logging reports generated by Compi.</p>
  </sec>
  <sec sec-type="supplementary-material" id="supplemental-information">
    <title> Supplemental Information</title>
    <supplementary-material content-type="local-data" id="supp-1">
      <object-id pub-id-type="doi">10.7717/peerj-cs.593/supp-1</object-id>
      <label>Supplemental Information 1</label>
      <caption>
        <title>Command-line interface for the RNA-Seq Compi pipeline</title>
      </caption>
      <media xlink:href="peerj-cs-07-593-s001.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="supp-2">
      <object-id pub-id-type="doi">10.7717/peerj-cs.593/supp-2</object-id>
      <label>Supplemental Information 2</label>
      <caption>
        <title>Implementation of a Nextflow example pipeline in Compi</title>
      </caption>
      <media xlink:href="peerj-cs-07-593-s002.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack>
    <p>SING group thanks the CITI (Centro de Investigación, Transferencia e Innovación) from the University of Vigo for hosting its IT infrastructure.</p>
  </ack>
  <sec sec-type="additional-information">
    <title>Additional Information and Declarations</title>
    <fn-group content-type="competing-interests">
      <title>Competing Interests</title>
      <fn id="conflict-1" fn-type="COI-statement">
        <p>The authors declare there are no competing interests.</p>
      </fn>
    </fn-group>
    <fn-group content-type="author-contributions">
      <title>Author Contributions</title>
      <fn id="contribution-5" fn-type="con">
        <p><xref ref-type="contrib" rid="author-1">Hugo López-Fernández</xref> and <xref ref-type="contrib" rid="author-5">Daniel Glez-Peña</xref> performed the computation work, prepared figures and/or tables, authored or reviewed drafts of the paper, and approved the final draft.</p>
      </fn>
      <fn id="contribution-4" fn-type="con">
        <p><xref ref-type="contrib" rid="author-2">Osvaldo Graña-Castro</xref>, <xref ref-type="contrib" rid="author-3">Alba Nogueira-Rodríguez</xref> and <xref ref-type="contrib" rid="author-4">Miguel Reboiro-Jato</xref> performed the computation work, authored or reviewed drafts of the paper, and approved the final draft.</p>
      </fn>
    </fn-group>
    <fn-group content-type="other">
      <title>Data Availability</title>
      <fn id="addinfo-1">
        <p>The following information was supplied regarding data availability:</p>
        <p>The Compi code is available at GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/sing-group/compi">https://github.com/sing-group/compi</ext-link>). The code of referenced Compi pipelines is available in CompiHub (<ext-link ext-link-type="uri" xlink:href="http://sing-group.org/compihub">http://sing-group.org/compihub</ext-link>).</p>
      </fn>
    </fn-group>
  </sec>
  <ref-list content-type="authoryear">
    <title>References</title>
    <ref id="ref-1">
      <label>Afgan et al. (2018)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Afgan</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Baker</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Batut</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>van den Beek</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Bouvier</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Čech</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Chilton</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Clements</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Coraor</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Grüning</surname>
            <given-names>BA</given-names>
          </name>
          <name>
            <surname>Guerler</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Hillman-Jackson</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hiltemann</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Jalili</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Rasche</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Soranzo</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Goecks</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Taylor</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Nekrutenko</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Blankenberg</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <year>2018</year>
        <article-title>The Galaxy platform for accessible, reproducible and collaborative biomedical analyses: 2018 update</article-title>
        <source>Nucleic Acids Research</source>
        <volume>46</volume>
        <fpage>W537</fpage>
        <lpage>W544</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gky379</pub-id>
        <pub-id pub-id-type="pmid">29790989</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-2">
      <label>Di Tommaso et al. (2017)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Di Tommaso</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Chatzou</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Floden</surname>
            <given-names>EW</given-names>
          </name>
          <name>
            <surname>Barja</surname>
            <given-names>PP</given-names>
          </name>
          <name>
            <surname>Palumbo</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Notredame</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <year>2017</year>
        <article-title>Nextflow enables reproducible computational workflows</article-title>
        <source>Nature Biotechnology</source>
        <volume>35</volume>
        <fpage>316</fpage>
        <lpage>319</lpage>
        <pub-id pub-id-type="doi">10.1038/nbt.3820</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-3">
      <label>Graña Castro et al. (2020a)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Graña Castro</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>López-Fernández</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Fdez-Riverola</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Al-Shahrour</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Glez-Peña</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Fdez-Riverola</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Rocha</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Mohamad</surname>
            <given-names>MS</given-names>
          </name>
          <name>
            <surname>Zaki</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Castellanos-Garzón</surname>
            <given-names>JA</given-names>
          </name>
        </person-group>
        <year>2020a</year>
        <article-title>Proposal of a new bioinformatics pipeline for metataxonomics in precision medicine</article-title>
        <conf-name>Practical applications of computational biology and bioinformatics, 13th international conference</conf-name>
        <conf-sponsor>Springer International Publishing</conf-sponsor>
        <conf-loc>Cham</conf-loc>
        <fpage>8</fpage>
        <lpage>15</lpage>
        <pub-id pub-id-type="doi">10.1007/978-3-030-23873-5_2</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-4">
      <label>Graña Castro et al. (2020b)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Graña Castro</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>López-Fernández</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Nogueira-Rodríguez</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Fdez-Riverola</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Al-Shahrour</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Glez-Peña</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <year>2020b</year>
        <article-title>Metatax: metataxonomics with a compi-based pipeline for precision medicine</article-title>
        <source>Interdisciplinary Sciences: Computational Life Sciences</source>
        <volume>12</volume>
        <fpage>252</fpage>
        <lpage>257</lpage>
        <pub-id pub-id-type="doi">10.1007/s12539-020-00368-6</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-5">
      <label>Grüning et al. (2018)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Grüning</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Chilton</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Köster</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Dale</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Soranzo</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Van den Beek</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Goecks</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Backofen</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Nekrutenko</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Taylor</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <year>2018</year>
        <article-title>Practical computational reproducibility in the life sciences</article-title>
        <source>Cell Systems</source>
        <volume>6</volume>
        <fpage>631</fpage>
        <lpage>635</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cels.2018.03.014</pub-id>
        <pub-id pub-id-type="pmid">29953862</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-6">
      <label>Köster &amp; Rahmann (2012)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Köster</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Rahmann</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <year>2012</year>
        <article-title>Snakemake—a scalable bioinformatics workflow engine</article-title>
        <source>Bioinformatics</source>
        <volume>28</volume>
        <fpage>2520</fpage>
        <lpage>2522</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bts480</pub-id>
        <pub-id pub-id-type="pmid">22908215</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-7">
      <label>Lampa et al. (2019)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lampa</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Dahlö</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Alvarsson</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Spjuth</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <year>2019</year>
        <article-title>SciPipe: a workflow library for agile development of complex and dynamic bioinformatics pipelines</article-title>
        <source>GigaScience</source>
        <volume>8</volume>
        <elocation-id>giz044</elocation-id>
        <pub-id pub-id-type="doi">10.1093/gigascience/giz044</pub-id>
        <pub-id pub-id-type="pmid">31029061</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-8">
      <label>Leipzig (2017)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Leipzig</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <year>2017</year>
        <article-title>A review of bioinformatic pipeline frameworks</article-title>
        <source>Briefings in Bioinformatics</source>
        <volume>18</volume>
        <fpage>530</fpage>
        <lpage>536</lpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbw020</pub-id>
        <pub-id pub-id-type="pmid">27013646</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-9">
      <label>López-Fernández et al. (2019)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>López-Fernández</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Blanco-Míguez</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Fdez-Riverola</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Sánchez</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Lourenço</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <year>2019</year>
        <article-title>DEWE: a novel tool for executing differential expression RNA-Seq workflows in biomedical research</article-title>
        <source>Computers in Biology and Medicine</source>
        <volume>107</volume>
        <fpage>197</fpage>
        <lpage>205</lpage>
        <pub-id pub-id-type="doi">10.1016/j.compbiomed.2019.02.021</pub-id>
        <pub-id pub-id-type="pmid">30849608</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-10">
      <label>López-Fernández et al. (2020)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>López-Fernández</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Duque</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Vázquez</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Fdez-Riverola</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Reboiro-Jato</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Vieira</surname>
            <given-names>CP</given-names>
          </name>
          <name>
            <surname>Vieira</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Fdez-Riverola</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Rocha</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Mohamad</surname>
            <given-names>MS</given-names>
          </name>
          <name>
            <surname>Zaki</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Castellanos-Garzón</surname>
            <given-names>JA</given-names>
          </name>
        </person-group>
        <year>2020</year>
        <article-title>Inferring positive selection in large viral datasets</article-title>
        <conf-name>Practical applications of computational biology and bioinformatics, 13th international conference</conf-name>
        <conf-sponsor>Springer International Publishing</conf-sponsor>
        <conf-loc>Cham</conf-loc>
        <fpage>61</fpage>
        <lpage>69</lpage>
        <pub-id pub-id-type="doi">10.1007/978-3-030-23873-5_8</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-11">
      <label>López-Fernández et al. (2021)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>López-Fernández</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Vieira</surname>
            <given-names>CP</given-names>
          </name>
          <name>
            <surname>Fdez-Riverola</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Reboiro-Jato</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Vieira</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Panuccio</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Rocha</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Fdez-Riverola</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Mohamad</surname>
            <given-names>MS</given-names>
          </name>
          <name>
            <surname>Casado-Vara</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <year>2021</year>
        <article-title>Inferences on mycobacterium leprae host immune response escape and antibiotic resistance using genomic data and GenomeFastScreen</article-title>
        <conf-name>Practical applications of computational biology &amp; bioinformatics, 14th international conference (PACBB 2020). Advances in intelligent systems and computing</conf-name>
        <conf-sponsor>Springer International Publishing</conf-sponsor>
        <conf-loc>Cham</conf-loc>
        <fpage>42</fpage>
        <lpage>50</lpage>
        <pub-id pub-id-type="doi">10.1007/978-3-030-54568-0_5</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-12">
      <label>Nogueira-Rodríguez et al. (2021)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Nogueira-Rodríguez</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>López-Fernández</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Graña Castro</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Reboiro-Jato</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Glez-Peña</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Panuccio</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Rocha</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Fdez-Riverola</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Mohamad</surname>
            <given-names>MS</given-names>
          </name>
          <name>
            <surname>Casado-Vara</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <year>2021</year>
        <article-title>Compi hub: a public repository for sharing and discovering compi pipelines</article-title>
        <conf-name>Practical applications of computational biology &amp; bioinformatics, 14th international conference (PACBB 2020). Advances in intelligent systems and computing</conf-name>
        <conf-sponsor>Springer International Publishing</conf-sponsor>
        <conf-loc>Cham</conf-loc>
        <fpage>51</fpage>
        <lpage>59</lpage>
        <pub-id pub-id-type="doi">10.1007/978-3-030-54568-0_6</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-13">
      <label>Perkel (2019)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Perkel</surname>
            <given-names>JM</given-names>
          </name>
        </person-group>
        <year>2019</year>
        <article-title>Workflow systems turn raw data into scientific knowledge</article-title>
        <source>Nature</source>
        <volume>573</volume>
        <fpage>149</fpage>
        <lpage>150</lpage>
        <pub-id pub-id-type="doi">10.1038/d41586-019-02619-z</pub-id>
        <pub-id pub-id-type="pmid">31477884</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-14">
      <label>Sadedin, Pope &amp; Oshlack (2012)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sadedin</surname>
            <given-names>SP</given-names>
          </name>
          <name>
            <surname>Pope</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Oshlack</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <year>2012</year>
        <article-title>Bpipe: a tool for running and managing bioinformatics pipelines</article-title>
        <source>Bioinformatics</source>
        <volume>28</volume>
        <fpage>1525</fpage>
        <lpage>1526</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bts167</pub-id>
        <pub-id pub-id-type="pmid">22500002</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-15">
      <label>Spjuth et al. (2018)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Spjuth</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Capuccini</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Carone</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Larsson</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Schaal</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Novella</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Di Tommaso</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Notredame</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Moreno</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Khoonsari</surname>
            <given-names>PE</given-names>
          </name>
          <name>
            <surname>Herman</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kultima</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Lampa</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <year>2018</year>
        <article-title>Approaches for containerized scientific workflows in cloud environments with applications in life science</article-title>
        <source>PeerJ Preprints</source>
        <volume>6</volume>
        <elocation-id>e27141v1</elocation-id>
        <pub-id pub-id-type="doi">10.7287/peerj.preprints.27141v1</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
