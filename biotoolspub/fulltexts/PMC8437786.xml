<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_CSBJ1191 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEgr4 jpg ?>
<?FILEgr5 jpg ?>
<?FILEgr6 jpg ?>
<?FILEgr7 jpg ?>
<?FILEga1 jpg ?>
<?FILEsi1 svg ?>
<?FILEsi2 svg ?>
<?FILEsi3 svg ?>
<?FILEsi4 svg ?>
<?FILEsi5 svg ?>
<?FILEsi6 svg ?>
<?FILEsi7 svg ?>
<?FILEsi8 svg ?>
<?FILEsi9 svg ?>
<?FILEsi10 svg ?>
<?FILEsi11 svg ?>
<?FILEsi12 svg ?>
<?FILEsi13 svg ?>
<?FILEsi14 svg ?>
<?FILEsi15 svg ?>
<?FILEsi16 svg ?>
<?FILEsi17 svg ?>
<?FILEsi18 svg ?>
<?FILEsi19 svg ?>
<?FILEsi20 svg ?>
<?FILEsi21 svg ?>
<?FILEsi22 svg ?>
<?FILEsi23 svg ?>
<?FILEsi24 svg ?>
<?FILEsi25 svg ?>
<?FILEsi26 svg ?>
<?FILEsi27 svg ?>
<?FILEsi28 svg ?>
<?FILEsi29 svg ?>
<?FILEsi30 svg ?>
<?FILEsi31 svg ?>
<?FILEsi32 svg ?>
<?FILEsi33 svg ?>
<?FILEsi34 svg ?>
<?FILEsi35 svg ?>
<?FILEsi36 svg ?>
<?FILEsi37 svg ?>
<?FILEsi38 svg ?>
<?FILEsi39 svg ?>
<?FILEsi40 svg ?>
<?FILEsi41 svg ?>
<?FILEsi42 svg ?>
<?FILEsi43 svg ?>
<?FILEsi44 svg ?>
<?FILEsi45 svg ?>
<?FILEsi46 svg ?>
<?FILEsi47 svg ?>
<?FILEsi48 svg ?>
<?FILEsi49 svg ?>
<?FILEsi50 svg ?>
<?FILEsi51 svg ?>
<?FILEsi52 svg ?>
<?FILEsi53 svg ?>
<?FILEsi54 svg ?>
<?FILEsi55 svg ?>
<?FILEsi56 svg ?>
<?FILEsi57 svg ?>
<?FILEsi58 svg ?>
<?FILEsi59 svg ?>
<?FILEsi60 svg ?>
<?FILEsi61 svg ?>
<?FILEsi62 svg ?>
<?FILEsi63 svg ?>
<?FILEsi64 svg ?>
<?FILEsi65 svg ?>
<?FILEsi66 svg ?>
<?FILEsi67 svg ?>
<?FILEsi68 svg ?>
<?FILEsi271 svg ?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Comput Struct Biotechnol J</journal-id>
    <journal-id journal-id-type="iso-abbrev">Comput Struct Biotechnol J</journal-id>
    <journal-title-group>
      <journal-title>Computational and Structural Biotechnology Journal</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2001-0370</issn>
    <publisher>
      <publisher-name>Research Network of Computational and Structural Biotechnology</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8437786</article-id>
    <article-id pub-id-type="pii">S2001-0370(21)00375-5</article-id>
    <article-id pub-id-type="doi">10.1016/j.csbj.2021.08.044</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>EMCBOW-GPCR: A method for identifying G-protein coupled receptors based on word embedding and wordbooks</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au005">
        <name>
          <surname>Qiu</surname>
          <given-names>Wangren</given-names>
        </name>
        <email>qiuone@163.com</email>
        <xref rid="af005" ref-type="aff">a</xref>
        <xref rid="cor1" ref-type="corresp">⁎</xref>
      </contrib>
      <contrib contrib-type="author" id="au010">
        <name>
          <surname>Lv</surname>
          <given-names>Zhe</given-names>
        </name>
        <xref rid="af005" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author" id="au015">
        <name>
          <surname>Xiao</surname>
          <given-names>Xuan</given-names>
        </name>
        <email>jdzxiaoxuan@163.com</email>
        <xref rid="af005" ref-type="aff">a</xref>
        <xref rid="cor1" ref-type="corresp">⁎</xref>
      </contrib>
      <contrib contrib-type="author" id="au020">
        <name>
          <surname>Shao</surname>
          <given-names>Shuai</given-names>
        </name>
        <xref rid="af005" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author" id="au025">
        <name>
          <surname>Lin</surname>
          <given-names>Hao</given-names>
        </name>
        <email>hlin@uestc.edu.cn</email>
        <xref rid="af010" ref-type="aff">b</xref>
        <xref rid="cor1" ref-type="corresp">⁎</xref>
      </contrib>
      <aff id="af005"><label>a</label>School of Information Engineering, Jingdezhen Ceramic Institute, Jingdezhen, China</aff>
      <aff id="af010"><label>b</label>Center for Informational Biology, University of Electronic Science and Technology of China, Chengdu 610054, China</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>⁎</label>Corresponding authors. <email>qiuone@163.com</email><email>jdzxiaoxuan@163.com</email><email>hlin@uestc.edu.cn</email></corresp>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>31</day>
      <month>8</month>
      <year>2021</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>31</day>
      <month>8</month>
      <year>2021</year>
    </pub-date>
    <volume>19</volume>
    <fpage>4961</fpage>
    <lpage>4969</lpage>
    <history>
      <date date-type="received">
        <day>27</day>
        <month>4</month>
        <year>2021</year>
      </date>
      <date date-type="rev-recd">
        <day>7</day>
        <month>8</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>27</day>
        <month>8</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2021 The Author(s)</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).</license-p>
      </license>
    </permissions>
    <abstract abstract-type="graphical" id="ab005">
      <title>Graphical abstract</title>
      <fig id="f0040" position="anchor">
        <graphic xlink:href="ga1"/>
      </fig>
    </abstract>
    <abstract abstract-type="author-highlights" id="ab010">
      <title>Highlights</title>
      <p>
        <list list-type="simple" id="l0005">
          <list-item id="o0005">
            <label>•</label>
            <p id="p0005">An computational method was developed to identify G-protein coupled receptors.</p>
          </list-item>
          <list-item id="o0010">
            <label>•</label>
            <p id="p0010">Three word-embedding models and a bag-of-words model are used to extract original features.</p>
          </list-item>
          <list-item id="o0015">
            <label>•</label>
            <p id="p0015">A high accuracy was achieved by using fusion information.</p>
          </list-item>
          <list-item id="o0020">
            <label>•</label>
            <p id="p0020">A powerful tool was established.</p>
          </list-item>
        </list>
      </p>
    </abstract>
    <abstract id="ab015">
      <p>G Protein-Coupled Receptors (GPCRs) are one of the largest membrane protein receptor family in human, which are also important targets for many drugs. Thence, it’s of great significance to judge whether a protein is a GPCR or not. However, identifying GPCRs by experimental methods is very expensive and time-consuming. As more and more GPCR primary sequences are accumulated, it’s feasible to develop a computational model to predict GPCRs precisely and quickly. In this paper, a novel method called EMCBOW-GPCR has been proposed to improve the accuracy of identifying GPCRs based on natural language processing (NLP). For representing GPCRs, three word-embedding models and a bag-of-words model are used to extract original features. Then, the original features are thrown into a Deep-learning algorithm to extract features further and reduce the dimension. Finally, the obtained features are fed into Extreme Gradient Boosting. As shown with the results comparison, the overall prediction metrics of EMCBOW-GPCR are higher than the state of the arts. In order to be convenient for more researchers to use EMCBOW-GPCR, the method and source code have been opened in github, which are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/454170054/EMCBOW-GPCR" id="ir005">https://github.com/454170054/EMCBOW-GPCR</ext-link>, and a user-friendly web-server for EMCBOW-GPCR has been established at <ext-link ext-link-type="uri" xlink:href="http://www.jci-bioinfo.cn/emcbowgpcr" id="ir010">http://www.jci-bioinfo.cn/emcbowgpcr</ext-link>.</p>
    </abstract>
    <kwd-group id="kg005">
      <title>Keywords</title>
      <kwd>Natural language processing</kwd>
      <kwd>Word embedding</kwd>
      <kwd>Bag-of-words</kwd>
      <kwd>Extreme gradient boosting</kwd>
      <kwd>Deep-learning</kwd>
      <kwd>GPCRs</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="s0005">
    <label>1</label>
    <title>Introduction</title>
    <p id="p0025">G protein coupled receptors (GPCRs) are one of the largest family of membrane proteins in mammalian genomes which are widely distributed among the central nervous system, immune system, cardiovascular system, retina and other organs and tissues <xref rid="b0005" ref-type="bibr">[1]</xref>, <xref rid="b0010" ref-type="bibr">[2]</xref>, <xref rid="b0015" ref-type="bibr">[3]</xref>, <xref rid="b0020" ref-type="bibr">[4]</xref>. GPCRs can be divided into six classes <xref rid="b0025" ref-type="bibr">[5]</xref>: rhodopsin-like receptors, secretin-like receptors, metabo-tropic glutamate receptors, fungal mating pheromone receptors, cyclic AMP receptors and frizzled receptors. What’s more, they can regulate a wide range of physiological processes such as neurotransmission, growth, immune responses and so on <xref rid="b0030" ref-type="bibr">[6]</xref>, <xref rid="b0035" ref-type="bibr">[7]</xref>, <xref rid="b0040" ref-type="bibr">[8]</xref>, <xref rid="b0045" ref-type="bibr">[9]</xref>. Due to their structural characteristics and key role in signal transduction, GPCRs are the most important drugs target in modern drug research and development <xref rid="b0050" ref-type="bibr">[10]</xref>, <xref rid="b0055" ref-type="bibr">[11]</xref>. Therefore, identifying GPCRs accurately is very significant for drug development.</p>
    <p id="p0030">As more and more public GPCRs data is available, there are many efficient methods based on extracting features from sequence that are proposed to predict GPCRs in recent years. These methods usually consist of two parts: classification algorithm and feature extraction. The classification algorithms mainly based on statistics and machine learning methods, including Artificial Neural Network (ANN) <xref rid="b0060" ref-type="bibr">[12]</xref>, <xref rid="b0065" ref-type="bibr">[13]</xref>, Random Forest(RF) <xref rid="b0025" ref-type="bibr">[5]</xref>, <xref rid="b0070" ref-type="bibr">[14]</xref>, intimate sorting <xref rid="b0075" ref-type="bibr">[15]</xref>, K-Nearest Neighbor(KNN) <xref rid="b0080" ref-type="bibr">[16]</xref>, <xref rid="b0085" ref-type="bibr">[17]</xref>, etc. The methods of feature representation for predicting GPCRs contain amino acid composition (AAC) <xref rid="b0080" ref-type="bibr">[16]</xref>, <xref rid="b0090" ref-type="bibr">[18]</xref>, 400D <xref rid="b0025" ref-type="bibr">[5]</xref>, N-gram <xref rid="b0025" ref-type="bibr">[5]</xref>, <xref rid="b0065" ref-type="bibr">[13]</xref>, <xref rid="b0095" ref-type="bibr">[19]</xref>, SVM-Prot <xref rid="b0070" ref-type="bibr">[14]</xref>, etc. Zou <xref rid="b0070" ref-type="bibr">[14]</xref> proposed a novel method in which the GPCRs were represented by a 188D feature vectors of SVM-Prot and the synthetic minority oversampling technique (SMOTE) <xref rid="b0100" ref-type="bibr">[20]</xref>, <xref rid="b0105" ref-type="bibr">[21]</xref>, <xref rid="b0110" ref-type="bibr">[22]</xref> algorithm was used to generate some new positive samples to balance the training datasets. Finally, the prediction method adopted RF algorithm to be trained with the datasets. Recently, Yu <xref rid="b0025" ref-type="bibr">[5]</xref> used the mixed-feature extraction methods to acquire the feature vector of GPCRs. In the work, three feature engineering methods including 400D, N-Gram and parallel correlation pseudo amino acid composition (PC-PseAAC) were chosen to extract features of GPCRs, respectively. Subsequently, these three feature vectors are randomly arranged and combined to form the mixed-feature. Further, the max relevance max distance (MRMD) <xref rid="b0115" ref-type="bibr">[23]</xref> was employed to reduce the dimension of mixed-feature. According to the result, the mixed-feature concatenated by 400D and PC-PseAAC can achieve the best performance with RF algorithm. Although these methods have achieved positive results on predicting GPCRs, there is still room for improvement.</p>
    <p id="p0035">As a new research hotspot in artificial intelligence, Deep learning (DL) <xref rid="b0120" ref-type="bibr">[24]</xref>, <xref rid="b0125" ref-type="bibr">[25]</xref>, <xref rid="b0130" ref-type="bibr">[26]</xref>, <xref rid="b0135" ref-type="bibr">[27]</xref>, <xref rid="b0140" ref-type="bibr">[28]</xref> is more and more widely used in machine learning. Because of its great help to the interpretation of data such as text, image and sound, DL has achieved many positive results in speech machine translation and image recognition far beyond previous related technologies, and has been successfully applied in many fields such as bioinformatics <xref rid="b0145" ref-type="bibr">[29]</xref>, <xref rid="b0150" ref-type="bibr">[30]</xref>, computer vision <xref rid="b0155" ref-type="bibr">[31]</xref>, <xref rid="b0160" ref-type="bibr">[32]</xref>, natural language processing <xref rid="b0165" ref-type="bibr">[33]</xref>, <xref rid="b0170" ref-type="bibr">[34]</xref>, Automatic driving <xref rid="b0175" ref-type="bibr">[35]</xref>, <xref rid="b0180" ref-type="bibr">[36]</xref>and so on. In this work, we propose a novel method called EMCBOW-GPCR to predict GPCRs based on word embedding <xref rid="b0185" ref-type="bibr">[37]</xref>, <xref rid="b0190" ref-type="bibr">[38]</xref>, BOW <xref rid="b0195" ref-type="bibr">[39]</xref> and DL models. Firstly, we split the GPCRs sequences into segments of different lengths, and train the corresponding word embedding model with the split segments. Further, the every GPCR sequence is inputted into the word embedding models to get the word vectors. A BOW model was used to extract features at the same time. Secondly, the features by extracting from different methods are concatenated to form the original feature vectors. Thirdly, the original feature vectors are fed into a DL model to reduce the dimension and extract features further. Finally, the processed features are thrown into XGBoost algorithm to train a predictor. According to the results compared with other methods tested with the same data and performance measurement, our method can have a better performance.</p>
  </sec>
  <sec id="s0010">
    <label>2</label>
    <title>Datasets and methods</title>
    <sec id="s0015">
      <label>2.1</label>
      <title>Experimental datasets and performance measurement</title>
      <p id="p0040">The benchmark dataset used for evaluating the proposed method is the same as that used in literatures <xref rid="b0025" ref-type="bibr">[5]</xref>, <xref rid="b0070" ref-type="bibr">[14]</xref> and is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/454170054/EMCBOW-GPCR/blob/main/files" id="ir015">https://github.com/454170054/EMCBOW-GPCR/blob/main/files</ext-link>. The dataset sequences were download from UniProt <xref rid="b0200" ref-type="bibr">[40]</xref> database and CD-Hit <xref rid="b0205" ref-type="bibr">[41]</xref>, <xref rid="b0210" ref-type="bibr">[42]</xref> program was used to reduce the sequence homology <xref rid="b0070" ref-type="bibr">[14]</xref>. The sequence identity threshold was 0.8. The evaluation indicators used to test the performance of the methods in the work are Accuracy (Acc), Precision (Pre), Sensitivity (Sn), Specificity (Sp) and Matthews correlation coefficient (MCC) <xref rid="b0215" ref-type="bibr">[43]</xref>, <xref rid="b0220" ref-type="bibr">[44]</xref>, <xref rid="b0225" ref-type="bibr">[45]</xref>, which are listed in formula (1) explicitly.<disp-formula id="e0005"><label>(1)</label><mml:math id="M1" altimg="si10.svg"><mml:mrow><mml:mfenced open="{"><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mi mathvariant="italic">Accuracy</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mi mathvariant="italic">Precision</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TN</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">TN</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">Sensitivity</mml:mi><mml:mo>+</mml:mo><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>×</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mfenced><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfenced><mml:mrow><mml:mfenced><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mfenced><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M2" altimg="si11.svg"><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow></mml:math></inline-formula> is the number of sequences that are GPCRs in fact and predicted as GPCRs,<inline-formula><mml:math id="M3" altimg="si12.svg"><mml:mrow><mml:mi mathvariant="italic">TN</mml:mi></mml:mrow></mml:math></inline-formula> is the number of sequences that are non-GPCRs in fact and predicted as non-GPCRs, <inline-formula><mml:math id="M4" altimg="si13.svg"><mml:mrow><mml:mi mathvariant="italic">FP</mml:mi></mml:mrow></mml:math></inline-formula>is the number of sequences that are non-GPCRs predicted as GPCRs,<inline-formula><mml:math id="M5" altimg="si14.svg"><mml:mrow><mml:mi mathvariant="italic">FN</mml:mi></mml:mrow></mml:math></inline-formula>is the number of sequences that are GPCRs predicted as non-GPCRs. Further, we also apply Area Under ROC Curve (AUC) metric to evaluate the methods.</p>
    </sec>
    <sec id="s0020">
      <label>2.2</label>
      <title>Feature extraction methods</title>
      <p id="p0045">In this study, two technologies are applied to extract features from GPCRs respectively. One of them is BOW <xref rid="b0195" ref-type="bibr">[39]</xref>, <xref rid="b0230" ref-type="bibr">[46]</xref> model which has been confirmed to be powerful in extracting GPCRs features based on sequences, and the other is Word Embedding which is very popular and important concept in natural language processing (NLP). The detailed process of the two feature extraction methods is listed as follows:</p>
    </sec>
    <sec id="s0025">
      <label>2.3</label>
      <title>Word embedding</title>
      <p id="p0050">Word Embedding is a method of converting words into number vectors. The process of word embedding is to embed a high-dimensional space with all the number of words into a much lower dimensional and continuous vector space and each word or phrase is mapped to a vector in the real number field, and the result of word embedding generates a word vector which is the important technology of the task. In this paper, we trained three kinds of word embedding models and utilized them to generate corresponding feature vectors. The explicit training process is showed as follows:</p>
      <sec id="s0030">
        <label>2.3.1</label>
        <title>Step 1: Splitting GPCRs sequences into fragments and create wordbooks</title>
        <p id="p0055">In order to satisfy the input data shape of the three word-embedding models, the GPCRs sequences are broken into different length fragments which are considered as words in wordbooks. In this paper, we designed three kinds fragments, and the length <inline-formula><mml:math id="M6" altimg="si15.svg"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula> of them can be set as 2, 3 or 4, respectively, and the wordbooks were denoted as <inline-formula><mml:math id="M7" altimg="si16.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M8" altimg="si17.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M9" altimg="si18.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. For example, in order to obtain <inline-formula><mml:math id="M10" altimg="si16.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, the original sequences are broken into words whose lengths are 2, i.e., the window size is set 2 and the stride of moving window is 1. After all of this work, the words broken from each sequence would be collected, removed duplicate(s) and then form wordbook <inline-formula><mml:math id="M11" altimg="si16.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> of which the number of words is <inline-formula><mml:math id="M12" altimg="si19.svg"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula>. For example, the process of splitting GPCRs sequences into words of <inline-formula><mml:math id="M13" altimg="si16.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is shown in <xref rid="f0005" ref-type="fig">Fig. 1</xref>. The processes of creating <inline-formula><mml:math id="M14" altimg="si17.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M15" altimg="si18.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are similar to <inline-formula><mml:math id="M16" altimg="si16.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. In detail, the window sizes of <inline-formula><mml:math id="M17" altimg="si17.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M18" altimg="si18.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> should be set as 3 and 4, respectively. What’s more, the strides of moving window of <inline-formula><mml:math id="M19" altimg="si17.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M20" altimg="si18.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are equal to 1.<fig id="f0005"><label>Fig. 1</label><caption><p>The process of splitting GPCRs sequences and forming wordbook of <inline-formula><mml:math id="M21" altimg="si1.svg"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Q</mml:mi><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xlink:href="gr1"/></fig></p>
      </sec>
      <sec id="s0035">
        <label>2.3.2</label>
        <title>Step 2: Training CBOW models</title>
        <p id="p0060">There are a lot of methods in word embedding. Here, the Word2vec <xref rid="b0185" ref-type="bibr">[37]</xref> method is selected as the default word embedding method in this paper for the reason that Word2vec whose models are simple. Actually, a double layer neural networks have two widely used models to generate word vectors including continuous bag of words (CBOW) and Skip-gram. CBOW is applied to predict target words based on consecutive words before and after target word. Conversely, Skip-gram is applied to predict context words based on a word. In this work, CBOW model is chosen as the default model for word embedding. The structure of CBOW is shown in <xref rid="f0010" ref-type="fig">Fig. 2</xref>. The training of artificial neural network (ANN) <xref rid="b0095" ref-type="bibr">[19]</xref>, <xref rid="b0235" ref-type="bibr">[47]</xref>, <xref rid="b0240" ref-type="bibr">[48]</xref> usually includes two parts: forward propagation and back propagation. The forward propagation calculation of the proposed model is listed as follows:<list list-type="simple" id="l0010"><list-item id="o0025"><label>1).</label><p id="p0065">Encoding the GPCRs primary sequence with characters string. Since the original GPCRs sequences can not be directly fed into the CBOW model, a GPCR sequence can be represented with formula (2), where <inline-formula><mml:math id="M22" altimg="si20.svg"><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> is the length of the protein sequence.</p></list-item></list><disp-formula id="e0010"><label>(2)</label><mml:math id="M23" altimg="si21.svg"><mml:mrow><mml:mi>G</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>g</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:msub><mml:mi>g</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>⋯</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula><list list-type="simple" id="l0015"><list-item id="o0030"><label>2).</label><p id="p0070">Partitioning the sequence into word set.</p></list-item></list><disp-formula id="e0015"><label>(3)</label><mml:math id="M24" altimg="si22.svg"><mml:mrow><mml:mi>R</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mfenced open="{" close="}"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⋯</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⋯</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⋯</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mfenced open="{" close="}"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:msub><mml:mrow><mml:mo>,</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M25" altimg="si23.svg"><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:math></inline-formula> means the number of words in the set. Obviously, <inline-formula><mml:math id="M26" altimg="si23.svg"><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:math></inline-formula>equals <inline-formula><mml:math id="M27" altimg="si24.svg"><mml:mrow><mml:mi>L</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">-</mml:mo><mml:mi>l</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="M28" altimg="si15.svg"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula> is the length of fragments.<list list-type="simple" id="l0020"><list-item id="o0035"><label>3).</label><p id="p0075">Inputting the encoded words to CBOW and calculating the output of the hidden layer. Select the target word from <inline-formula><mml:math id="M29" altimg="si25.svg"><mml:mrow><mml:mi>R</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, mark it as <inline-formula><mml:math id="M30" altimg="si26.svg"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and choose its context <inline-formula><mml:math id="M31" altimg="si27.svg"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, i.e. <inline-formula><mml:math id="M32" altimg="si28.svg"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula> words from the upstream and <inline-formula><mml:math id="M33" altimg="si28.svg"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula> words from downstream of the target word in the protein sequences respectively. According to the corresponding wordbook created in <bold>Step 1,</bold> encoding the selected context words by using One-Hot and mark as <inline-formula><mml:math id="M34" altimg="si271.svg"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Then feed them as the input data into Input Layer of CBOW. Calculation process from input layer to hidden layer is shown in formula (4).</p></list-item></list><disp-formula id="e0020"><label>(4)</label><mml:math id="M35" altimg="si29.svg"><mml:mrow><mml:mi>h</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>s</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow><mml:mi>s</mml:mi></mml:munderover><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M36" altimg="si30.svg"><mml:mrow><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> is the weight matrix between Input Layer and Hidden Layer, whose shape is <inline-formula><mml:math id="M37" altimg="si31.svg"><mml:mrow><mml:mi>v</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="M38" altimg="si32.svg"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> is a hyperparameter.<inline-formula><mml:math id="M39" altimg="si33.svg"><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:math></inline-formula> is the output of Hidden Layer, whose shape is <inline-formula><mml:math id="M40" altimg="si34.svg"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>.<list list-type="simple" id="l0025"><list-item id="o0040"><label>4).</label><p id="p0085">Designing the object function based on the propagation process from Hidden Layer to Output Layer. The formula (5) was utilized to compute the score matrix for the wordbook.</p></list-item></list><disp-formula id="e0025"><label>(5)</label><mml:math id="M41" altimg="si35.svg"><mml:mrow><mml:mi>U</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:msup><mml:mrow><mml:mi>h</mml:mi><mml:mo>∗</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:msub><mml:mrow><mml:mo>,</mml:mo><mml:mi>u</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M42" altimg="si36.svg"><mml:mrow><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> is the weight matrix between Hidden Layer and Output Layer, whose shape is <inline-formula><mml:math id="M43" altimg="si37.svg"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula>. Then <inline-formula><mml:math id="M44" altimg="si38.svg"><mml:mrow><mml:mi mathvariant="italic">Softmax</mml:mi></mml:mrow></mml:math></inline-formula> function is used to obtain the probability distribution of output units.<disp-formula id="e0030"><label>(6)</label><mml:math id="M45" altimg="si39.svg"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msup><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>v</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M46" altimg="si40.svg"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the <inline-formula><mml:math id="M47" altimg="si41.svg"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula>th value of <inline-formula><mml:math id="M48" altimg="si42.svg"><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M49" altimg="si43.svg"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> means the <inline-formula><mml:math id="M50" altimg="si41.svg"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula>th word in wordbook.<fig id="f0010"><label>Fig. 2</label><caption><p>The structure of CBOW.</p></caption><graphic xlink:href="gr2"/></fig></p>
        <p id="p0090">To maximize the probability of the target word <inline-formula><mml:math id="M51" altimg="si26.svg"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, formula (7) was used at this step.<disp-formula id="e0035"><label>(7)</label><mml:math id="M52" altimg="si44.svg"><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msup><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>v</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msup></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="p0095">Take logarithm of formula (7) to get the objective function, and then maximize the objective function with formula (8).<disp-formula id="e0040"><label>(8)</label><mml:math id="M53" altimg="si45.svg"><mml:mrow><mml:mi mathvariant="italic">loss</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo linebreak="badbreak">-</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>v</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msup></mml:mrow></mml:math></disp-formula></p>
        <p id="p0100">However, the above process usually needs a lot of time to train the model. Here, we adopt a more efficient technology called Negative Sampling <xref rid="b0245" ref-type="bibr">[49]</xref> to accelerate the training process. By using Negative Sampling can convert the above optimization problem to a series of binary problems and speed up training better word vectors. The detail of Negative Sampling and back propagation algorithm please see to reference <xref rid="b0250" ref-type="bibr">[50]</xref>, <xref rid="b0255" ref-type="bibr">[51]</xref>. In this step, we choose the value of <inline-formula><mml:math id="M54" altimg="si32.svg"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> as 128, 256 and 512 on the basis of the number of words in relevant wordbook.</p>
      </sec>
      <sec id="s0040">
        <label>2.3.3</label>
        <title>Step 3: Extracting features with CBOW models</title>
        <p id="p0105">In <bold>Step 2,</bold> we got three kinds of word vector matrixes <inline-formula><mml:math id="M55" altimg="si30.svg"><mml:mrow><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> by training CBOW models. In this step, each sequence was converted into feature vector. The detailed process is as follows:<list list-type="simple" id="l0030"><list-item id="o0045"><label>1).</label><p id="p0110">Utilize formula (2) and (3) to process the GPCR sequence and get <inline-formula><mml:math id="M56" altimg="si46.svg"><mml:mrow><mml:mi>R</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mrow><mml:mfenced open="{" close="}"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:msub><mml:mrow><mml:mo>,</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item id="o0050"><label>2).</label><p id="p0115">According to the relevant wordbook, encode <inline-formula><mml:math id="M57" altimg="si25.svg"><mml:mrow><mml:mi>R</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> by using One-Hot and then we can get <inline-formula><mml:math id="M58" altimg="si47.svg"><mml:mrow><mml:mi mathvariant="italic">RO</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mrow><mml:mfenced open="[" close="]"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>⋮</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> Where <inline-formula><mml:math id="M59" altimg="si48.svg"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a <inline-formula><mml:math id="M60" altimg="si49.svg"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula> vector which represents the word <inline-formula><mml:math id="M61" altimg="si50.svg"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (see to formula (3)), <inline-formula><mml:math id="M62" altimg="si51.svg"><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:math></inline-formula>, and thus <inline-formula><mml:math id="M63" altimg="si52.svg"><mml:mrow><mml:mi mathvariant="italic">RO</mml:mi></mml:mrow></mml:math></inline-formula> is a <inline-formula><mml:math id="M64" altimg="si53.svg"><mml:mrow><mml:mi>B</mml:mi><mml:mo>×</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula> matrix.</p></list-item><list-item id="o0055"><label>3).</label><p id="p0120">Calculate the feature vector matrixes by using formula (9)</p></list-item></list><disp-formula id="e0045"><label>(9)</label><mml:math id="M65" altimg="si54.svg"><mml:mrow><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>R</mml:mi><mml:mi>O</mml:mi><mml:mo>∗</mml:mo><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M66" altimg="si55.svg"><mml:mrow><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> means matrix embedding features and is a <inline-formula><mml:math id="M67" altimg="si56.svg"><mml:mrow><mml:mi>B</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> matrix.<list list-type="simple" id="l0035"><list-item id="o0060"><label>4).</label><p id="p0125">Represent the GPCR sequence with formula (10).</p></list-item></list><disp-formula id="e0050"><label>(10)</label><mml:math id="M68" altimg="si57.svg"><mml:mrow><mml:mi>E</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>B</mml:mi></mml:mfrac><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M69" altimg="si58.svg"><mml:mrow><mml:mi mathvariant="italic">sum</mml:mi></mml:mrow></mml:math></inline-formula> means to sum the values along the first dimension of the matrix and <inline-formula><mml:math id="M70" altimg="si59.svg"><mml:mrow><mml:mi>E</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> means embedding features.</p>
        <p id="p0130">With the same method, we can obtain <inline-formula><mml:math id="M71" altimg="si60.svg"><mml:mrow><mml:mi>E</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M72" altimg="si61.svg"><mml:mrow><mml:mi>E</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M73" altimg="si62.svg"><mml:mrow><mml:mi>E</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, i.e. let <inline-formula><mml:math id="M74" altimg="si15.svg"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula> equals to 2, 3 or 4 for above formulas. And then the word embedding features (<inline-formula><mml:math id="M75" altimg="si63.svg"><mml:mrow><mml:mi mathvariant="italic">WEF</mml:mi></mml:mrow></mml:math></inline-formula>) of each GPCR sequence can be combined into an 896-D vector by using the formula (11).<disp-formula id="e0055"><label>(11)</label><mml:math id="M76" altimg="si64.svg"><mml:mrow><mml:mi mathvariant="italic">WEF</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>E</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow><mml:mo>⊗</mml:mo><mml:mi>E</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfenced></mml:mrow><mml:mo>⊗</mml:mo><mml:mi>E</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M77" altimg="si65.svg"><mml:mrow><mml:mo>⊕</mml:mo></mml:mrow></mml:math></inline-formula> means concatenating the two vectors.</p>
        <sec id="s0045">
          <label>2.3.3.1</label>
          <title>Bow</title>
          <p id="p0135">The brief steps of BOW features extraction from GPCRs are listed as follows:<list list-type="simple" id="l0040"><list-item id="o0065"><label>(1).</label><p id="p0140">Encoding GPCRs sequences by using AAindex <xref rid="b0260" ref-type="bibr">[52]</xref>which is a database containing more than 500 amino acid indices. The symbol ‘X’ existed in some sequences are represent with the mean of AAindex.</p></list-item><list-item id="o0070"><label>(2).</label><p id="p0145">Splitting GPCRs sequences into fragments with different sizes.</p></list-item><list-item id="o0075"><label>(3).</label><p id="p0150">Creating wordbooks and determining the number of words in each wordbook with weighted Silhouette Coefficient.</p></list-item><list-item id="o0080"><label>(4).</label><p id="p0155">Extracting 115-D BOW features denoted as <inline-formula><mml:math id="M78" altimg="si66.svg"><mml:mrow><mml:mi mathvariant="italic">BF</mml:mi></mml:mrow></mml:math></inline-formula> based on wordbooks.</p></list-item></list></p>
          <p id="p0160">The specific process of BOW features extraction can be found in reference <xref rid="b0230" ref-type="bibr">[46]</xref>.</p>
          <p id="p0165">Finally, the above feature vectors are concatenated into a 1011-D vector by using formula (12) denoted as <inline-formula><mml:math id="M79" altimg="si67.svg"><mml:mrow><mml:mi>F</mml:mi><mml:mi>_</mml:mi><mml:mi>G</mml:mi><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>.<disp-formula id="e0060"><label>(12)</label><mml:math id="M80" altimg="si68.svg"><mml:mrow><mml:mi>F</mml:mi><mml:mi>_</mml:mi><mml:mi>G</mml:mi><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mi>R</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>W</mml:mi><mml:mi>E</mml:mi><mml:mi>F</mml:mi><mml:mo>⊗</mml:mo><mml:mi>B</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:math></disp-formula></p>
        </sec>
      </sec>
    </sec>
    <sec id="s0050">
      <label>2.4</label>
      <title>Feature extraction by Deep learning</title>
      <p id="p0170">In this work, we built a simple DL model which contains three fully connected layers and two Batch Normalization (BN) <xref rid="b0265" ref-type="bibr">[53]</xref> layers to reduce the dimensions of the feature vector <inline-formula><mml:math id="M81" altimg="si67.svg"><mml:mrow><mml:mi>F</mml:mi><mml:mi>_</mml:mi><mml:mi>G</mml:mi><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>. In fact, the structure of DL model is very flexible. In order to simplify the problem, the number of neurons of each hidden layer is about half less than the previous layer. The detailed structure is shown in <xref rid="f0015" ref-type="fig">Fig. 3</xref>. What’s more, we choose Focal loss <xref rid="b0160" ref-type="bibr">[32]</xref> as the default loss function. Compared with other loss functions, Focal loss can handle the imbalance problem of datasets better <xref rid="b0160" ref-type="bibr">[32]</xref>.In this paper, the DL model is built by Tensorflow which is a very popular machine learning package. The hyperparameters of the DL model including epochs, batch size, loss function and optimizer are 20 and 32, binary cross-entropy and Adam <xref rid="b0270" ref-type="bibr">[54]</xref>. The activation function of hidden layers is chosen Leaky Relu <xref rid="b0275" ref-type="bibr">[55]</xref>. And the initial learning rate is 0.01. What’s more, we used Early Stopping method to decide when to stop training. The strategy of Early Stopping could monitor the training loss and would stop the training process if the training loss did not decrease in the next 3 epochs. The model would be trained with the training dataset. Then the 1011-D features vector <inline-formula><mml:math id="M82" altimg="si67.svg"><mml:mrow><mml:mi>F</mml:mi><mml:mi>_</mml:mi><mml:mi>G</mml:mi><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> would be input into the optimized model and the output of Layer 1 would be intercepted as the final features. From <xref rid="f0015" ref-type="fig">Fig. 3</xref>, we can see that the final features are 505-D obviously after using DL model to reduce dimension.<fig id="f0015"><label>Fig. 3</label><caption><p>The structure of deep learning model.</p></caption><graphic xlink:href="gr3"/></fig></p>
    </sec>
    <sec id="s0055">
      <label>2.5</label>
      <title>Algorithm selection</title>
      <sec id="s0060">
        <label>2.5.1</label>
        <title>Gradient boosting decision tree</title>
        <p id="p0175">Gradient Boosting Decision Tree (GBDT) is an effective machine learning algorithm in industry <xref rid="b0280" ref-type="bibr">[56]</xref> and scientific research <xref rid="b0230" ref-type="bibr">[46]</xref>, <xref rid="b0285" ref-type="bibr">[57]</xref>. In practical research and application, classification and Regression Trees (CART) <xref rid="b0290" ref-type="bibr">[58]</xref> is usually served as weak classifier for GBDT, and GBDT algorithm is trained through multiple iterations, each iteration produces a weak classifier which is trained on the basis of the residual of the previous round. The final total classifier (GBDT) is the weighted sum of the weak classifiers from each iteration of training.</p>
      </sec>
      <sec id="s0065">
        <label>2.5.2</label>
        <title>Random forest</title>
        <p id="p0180">Random Forest (RF) <xref rid="b0295" ref-type="bibr">[59]</xref> is a classical ensemble algorithm in machine learning. Because of its flexibility and generalization ability, this algorithm has been applied in many fields, such as bioinformatics, data mining and marketing management. The base learners of RF are usually CART. When a new sample is needed to be classified, each decision tree in the forest will be judged and classified separately. RF depends on a vote of their predictions to decide the final classification result.</p>
      </sec>
      <sec id="s0070">
        <label>2.5.3</label>
        <title>CatBoost</title>
        <p id="p0185">CatBoost <xref rid="b0300" ref-type="bibr">[60]</xref> is a kind of boosting algorithm based on symmetric Trees, which is universal and can be applied to a wide range of fields and various problems. Compared with other machine learning algorithms, the algorithm has three advantages. First of all, it can automatically handle categorical features. Further, CatBoost uses combined category features to make use of the relationship between features, which greatly enriches the feature dimension. Last but not least, the time of model training and predicting is very short.</p>
      </sec>
      <sec id="s0075">
        <label>2.5.4</label>
        <title>XGBoost</title>
        <p id="p0190">XGBoost <xref rid="b0305" ref-type="bibr">[61]</xref> is a well-known boosting algorithm in machine learning, which is mainly used to solve supervised learning problems. It can handle many tasks such as regression, classification and sorting and is widely used in machine learning competitions, and has achieved good results. XGBoost is generally regarded as an improvement of GBDT algorithm and can be more flexible and efficient <xref rid="b0310" ref-type="bibr">[62]</xref>, <xref rid="b0315" ref-type="bibr">[63]</xref>. The base learner of XGBoost is CART.</p>
      </sec>
    </sec>
  </sec>
  <sec id="s0080">
    <label>3</label>
    <title>Results</title>
    <sec id="s0085">
      <label>3.1</label>
      <title>Train the better CBOW models</title>
      <p id="p0195">From the mathematical derivation of CBOW, it is clear that getting a best CBOW model through training is to maximize the objective function. Here, a large value of iterations was set for the training of CBOW models and the best iterations were chosen by observing the change of objective function in the training process. In this paper, the all CBOW models were built and trained by using the software package of Python called Gensim. The code of training CBOW models is shown at https://github.com/454170054/EMCBOW-GPCR/blob/main/code/GetCBO</p>
      <p id="p0200">WFeatures.py, which contains the values of hyperparameters. Increment curves of objective functions of the three built models are shown in <xref rid="f0020" ref-type="fig">Fig. 4</xref>. From the picture, we can see when the objective functions of the models start to converge. From the left one in the picture, it is clear that when iterations of <inline-formula><mml:math id="M83" altimg="si16.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is bigger than 74, the increment of objective function is close to 0. Therefore, we choose the result after the 74th training as the default CBOW model of <inline-formula><mml:math id="M84" altimg="si16.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Similarly, the default CBOW models of <inline-formula><mml:math id="M85" altimg="si17.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M86" altimg="si17.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are the result after the 100th training and 280th training, respectively.<fig id="f0020"><label>Fig. 4</label><caption><p>Increment curves of objective functions of training CBOW.</p></caption><graphic xlink:href="gr4"/></fig></p>
    </sec>
    <sec id="s0090">
      <label>3.2</label>
      <title>Effect of different feature representations of GPCR</title>
      <p id="p0205">In this work, we used two feature extraction methods, i.e., BOW features and Word Embedding features, to represent GPCRs sequences. In this section, the performances are compared based on mentioned-above single kind of features and combined features which are generated by concatenating BOW features and Word Embedding features by 5-fold cross-validation with 20 times on benchmark dataset. In this section, CatBoost algorithm are selected as the default algorithm to build classifiers. The experimental results about AUC values of different features are shown in <xref rid="f0025" ref-type="fig">Fig. 5</xref>. From the figure, we can see that the combined features whose AUC is 0.9423 which is better than CBOW features (AUC = 0.9391) and BOW features (AUC = 0.9344). According to the results, the combined features are selected as the default features to represent GPCRs sequences.<fig id="f0025"><label>Fig. 5</label><caption><p>AUC values of CatBoost algorithm combined with different features.</p></caption><graphic xlink:href="gr5"/></fig></p>
    </sec>
    <sec id="s0095">
      <label>3.3</label>
      <title>Effect of different features generated by Deep-learning model</title>
      <p id="p0210">In this paper, a simple fully connected DL model was built with one input layer, three hidden layers, two BN layers and one output layer. Because of the model having three hidden layers and two BN layers, there are 5 kinds of features generated by the DL model. In this section, we evaluate the effectiveness of above features by 5-fold cross-validation with 20 times on benchmark dataset and the results are shown in <xref rid="f0030" ref-type="fig">Fig. 6</xref>. The details about the DL model are exhibited at <ext-link ext-link-type="uri" xlink:href="https://github.com/454170054/EMCBOW-GPCR/blob/main/code/GetResults.py" id="ir025">https://github.com/454170054/EMCBOW-GPCR/blob/main/code/GetResults.py</ext-link>. CatBoost is chosen as the default algorithm to build predictive model to get AUC values in this section. From the figure, we can find that the AUC value of original features is the smallest. This result can demonstrate the fact that the processed features by DL have better performance. What’s more, the features generated by Layer 1 of DL model achieved the biggest value of AUC so that it was chosen as the default features.<fig id="f0030"><label>Fig. 6</label><caption><p>AUC values of CatBoost algorithm combined with features generated by different layers.</p></caption><graphic xlink:href="gr6"/></fig></p>
    </sec>
    <sec id="s0100">
      <label>3.4</label>
      <title>Determination of the optimal algorithm</title>
      <p id="p0215">Up to now, there are many effective algorithms developed in the field of machine learning. Ensemble learning often performs better than the best single algorithm since it would construct a certain number of weak classifiers and then classifies a new sample by taking a (weighted) vote of their predictions. The all learning algorithms mentioned in section <bold>3</bold> belong to ensemble learning, in which the random forest is part of Bagging method and the rest is part of Boosting method <xref rid="b0320" ref-type="bibr">[64]</xref>. In this section, the above four algorithms are tested on the benchmark dataset by 5-fold cross-validation with 20 times and the performance of them is listed in <xref rid="t0005" ref-type="table">Table 1</xref> and the input features used there is the features generated by Layer 1 of the DL model. As shown in the table, the performance of DL model is worse than other algorithms which are using the features generated by DL model. What’s more, The RF algorithm has the largest Sp but having the smallest Sn and CatBoost algorithm has the best Sn. Furthermore, the Acc, Mcc and AUC value of XGBoost is the highest. Therefore, XGBoost algorithm is selected as the final algorithm to build classifier in this work. The hyperparameters used for the classifiers are included in <xref rid="t0010" ref-type="table">Table 2</xref> and the software packages can be found in <ext-link ext-link-type="uri" xlink:href="https://github.com/454170054/EMCBOW-GPCR/blob/main/requirements.txt" id="ir030">https://github.com/454170054/EMCBOW-GPCR/blob/main/requirements.txt</ext-link>.<table-wrap position="float" id="t0005"><label>Table 1</label><caption><p>Performance of different algorithms.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Algorithm</th><th>Acc</th><th>Sp</th><th>Sn</th><th>MCC</th><th>AUC</th></tr></thead><tbody><tr><td>CatBoost</td><td>92.87<inline-formula><mml:math id="M87" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.14</td><td>97.06<inline-formula><mml:math id="M88" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.13</td><td><bold>75.43</bold><inline-formula><mml:math id="M89" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula><bold>0.54</bold></td><td>76.29<inline-formula><mml:math id="M90" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.48</td><td>95.25<inline-formula><mml:math id="M91" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.14</td></tr><tr><td>RF</td><td>92.85<inline-formula><mml:math id="M92" altimg="si3.svg"><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.10</mml:mn></mml:mrow></mml:math></inline-formula></td><td><bold>98.40</bold><inline-formula><mml:math id="M93" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula><bold>0.07</bold></td><td>69.78<inline-formula><mml:math id="M94" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.44</td><td>75.85<inline-formula><mml:math id="M95" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.38</td><td>95.38<inline-formula><mml:math id="M96" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.09</td></tr><tr><td>GBDT</td><td>92.61<inline-formula><mml:math id="M97" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.13</td><td>96.77<inline-formula><mml:math id="M98" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.12</td><td>75.27<inline-formula><mml:math id="M99" altimg="si4.svg"><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.52</mml:mn></mml:mrow></mml:math></inline-formula></td><td>75.48<inline-formula><mml:math id="M100" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.58</td><td>94.82<inline-formula><mml:math id="M101" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.13</td></tr><tr><td>XGBoost</td><td><bold>92.91</bold><inline-formula><mml:math id="M102" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula><bold>0.10</bold></td><td>97.34<inline-formula><mml:math id="M103" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.11</td><td>74.45<inline-formula><mml:math id="M104" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.41</td><td><bold>76.32</bold><inline-formula><mml:math id="M105" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula><bold>0.34</bold></td><td><bold>95.56</bold><inline-formula><mml:math id="M106" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula><bold>0.13</bold></td></tr><tr><td>DL</td><td>92.23<inline-formula><mml:math id="M107" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.30</td><td>97.56<inline-formula><mml:math id="M108" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.47</td><td>70.03<inline-formula><mml:math id="M109" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>2.65</td><td>73.77<inline-formula><mml:math id="M110" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>1.14</td><td>93.80<inline-formula><mml:math id="M111" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.35</td></tr></tbody></table><table-wrap-foot><fn><p><italic>Notice</italic>: digits are mean<inline-formula><mml:math id="M112" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula> std and the bold means the best values.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="t0010"><label>Table 2</label><caption><p>The hyperparameters used for the classifiers.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Algorithm</th><th>n_estimators</th><th>Learning rate</th><th>Max depth</th></tr></thead><tbody><tr><td>CatBoost</td><td>190</td><td align="char">0.4</td><td>None</td></tr><tr><td>RF</td><td>110</td><td/><td>None</td></tr><tr><td>GBDT</td><td>100</td><td align="char">0.1</td><td>3</td></tr><tr><td>XGBoost</td><td>110</td><td align="char">0.12</td><td>3</td></tr></tbody></table><table-wrap-foot><fn><p>Notice: slash means algorithm do not have the hyperparameter.</p></fn></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="s0105">
      <label>3.5</label>
      <title>Comparison of other methods</title>
      <p id="p0220">In this work, the benchmark dataset is same as the one used in <xref rid="b0025" ref-type="bibr">[5]</xref>, <xref rid="b0070" ref-type="bibr">[14]</xref>. In order to prove the effectiveness of our method, the performance is compared between the proposed method and the other state-of-the-art method. Because the data segmentation methods in the literature are different, the same segmentation strategy was test here. In the literature <xref rid="b0070" ref-type="bibr">[14]</xref>, the state-of-the-art method was tested on the benchmark dataset by 5-fold cross-validation. Further, SMOTE algorithm was used to balance the training dataset and change the positive samples from 100 percent into 300 percent. We also take the strategy to handle the benchmark dataset and test our proposed method. The results are shown in <xref rid="t0015" ref-type="table">Table 3</xref>. It is clear that all the metrics of the proposed method are better than those of Liao. In the literature <xref rid="b0025" ref-type="bibr">[5]</xref>, the negative samples in the benchmark dataset were randomly divided into 4 groups and 2495 sequences were extracted from these 4 groups, respectively. Then, the final result is an average of the four experiment by using the four negative experiments. According to the strategy, we test our proposed method and the result is shown in <xref rid="t0020" ref-type="table">Table 4</xref>. From the table, we can find that the AUC, Acc and Precision value of our method are higher than the other. The results of comparing with other methods show that the proposed method is a good method for identifying GPCRs.<table-wrap position="float" id="t0015"><label>Table 3</label><caption><p>The results of the proposed method and Liao <xref rid="b0070" ref-type="bibr">[14]</xref>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>Acc</th><th>Sp</th><th>Sn</th><th>MCC</th><th>AUC</th></tr></thead><tbody><tr><td>Proposed method</td><td><bold>92.91</bold><inline-formula><mml:math id="M113" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula><bold>0.10</bold></td><td><bold>97.34</bold><inline-formula><mml:math id="M114" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula><bold>0.11</bold></td><td><bold>74.45</bold><inline-formula><mml:math id="M115" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula><bold>0.41</bold></td><td><bold>76.32</bold><inline-formula><mml:math id="M116" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula><bold>0.34</bold></td><td><bold>95.56</bold><inline-formula><mml:math id="M117" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula><bold>0.13</bold></td></tr><tr><td>Liao</td><td>83.33<inline-formula><mml:math id="M118" altimg="si5.svg"><mml:mrow><mml:mo>±</mml:mo><mml:mn>7.26</mml:mn></mml:mrow></mml:math></inline-formula></td><td>97.24<inline-formula><mml:math id="M119" altimg="si6.svg"><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.87</mml:mn></mml:mrow></mml:math></inline-formula></td><td>69.42<inline-formula><mml:math id="M120" altimg="si7.svg"><mml:mrow><mml:mo>±</mml:mo><mml:mn>14.91</mml:mn></mml:mrow></mml:math></inline-formula></td><td>69.24<inline-formula><mml:math id="M121" altimg="si8.svg"><mml:mrow><mml:mo>±</mml:mo><mml:mn>12.96</mml:mn></mml:mrow></mml:math></inline-formula></td><td>92.80<inline-formula><mml:math id="M122" altimg="si9.svg"><mml:mrow><mml:mo>±</mml:mo><mml:mn>3.8</mml:mn></mml:mrow></mml:math></inline-formula></td></tr></tbody></table></table-wrap><table-wrap position="float" id="t0020"><label>Table 4</label><caption><p>The results of the proposed method and literature <xref rid="b0025" ref-type="bibr">[5]</xref>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>AUC</th><th>Acc</th><th>Precision</th></tr></thead><tbody><tr><td>Proposed method</td><td align="char"><bold>95.29</bold></td><td align="char"><bold>88.11</bold></td><td align="char"><bold>89.28</bold></td></tr><tr><td>400D + PC-PseAAC</td><td align="char">94.13</td><td align="char">86.28</td><td align="char">86.62</td></tr></tbody></table></table-wrap></p>
    </sec>
  </sec>
  <sec id="s0110">
    <label>4</label>
    <title>Conclusion</title>
    <p id="p0225">In this work, CatBoost was chosen as the initial algorithm to build classifier for the reason that the time of training CatBoost is much shorter than other mentioned algorithms. Firstly, we selected the best CBOW models by optimizing the objective function. Secondly, BOW and CBOW models were employed to extract features separately and the metrics listed in formula (1) were used to evaluate the effectiveness of different feature representations for GPCRs. According to the experiment results, the combined features by concatenating BOW and Word Embedding features were better than any single features. Therefore, we determined to choose the combined features as the default features to represent GPCRs sequences. Then, a simple DL model was built and CatBoost was employed to fit the processed features generated by different layers of DL model and generate corresponding classifier. According to the performance of different classifiers, the outputs of Layer 1 were chosen as the final features. Further, XGBoost was selected as the default algorithm because of its best performance compared with other algorithms. Finally, according to results compared with other state-of-the-art methods, the proposed method called EMCBOW-GPCR got a better performance in the problem of identifying GPCRs.</p>
  </sec>
  <sec id="s0115">
    <label>5</label>
    <title>Discussion</title>
    <p id="p0230">G protein coupled receptors (GPCRs) family is one of the largest membrane protein family in human beings, and is also an important target of many drugs. In this work, a novel method for identifying GPCRs was developed. In terms of representation GPCRs, we used two extraction methods which are BOW and Word Embedding to extract features from GPCRs sequences. Then we concatenated the above two kinds of features as the input of DL which can automatically extract better features by learning from the input features. Further, we intercepted the output of Layer 1 as the input features of XGBoost which is a fairly powerful, flexible and efficient algorithm. According to the results of compared with other methods, the proposed method called EMCBOW-GPCR has a better performance for identifying GPCRs. By the way, the hyperparameter <inline-formula><mml:math id="M123" altimg="si32.svg"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> which is used in Word Embedding is flexible and hard to determine the best value. What’s more, it may be influenced the quality of features significantly. At the same time, the DL model is also very flexible and difficult to find the best structure which contains how many layers and units in each layer. Finally, the framework of EMCBOW-GPCR can be concluded as <xref rid="f0035" ref-type="fig">Fig. 7</xref>.<fig id="f0035"><label>Fig. 7</label><caption><p>The framework of proposed method EMCBOW-GPCR.</p></caption><graphic xlink:href="gr7"/></fig></p>
  </sec>
  <sec id="s0120">
    <title>CRediT authorship contribution statement</title>
    <p id="p0235"><bold>Wangren Qiu:</bold> Conceptualization, Methodology, Software, Data curation, Writing – original draft, Software, Validation, Writing - review &amp; editing. <bold>Zhe Lv:</bold> Visualization, Investigation, Software, Validation. <bold>Xuan Xiao:</bold> Conceptualization, Methodology, Software, Supervision, Writing - review &amp; editing. <bold>Shuai Shao:</bold> Visualization, Investigation. <bold>Hao Lin:</bold> Conceptualization, Methodology, Software, Supervision, Writing - review &amp; editing.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Declaration of Competing Interest</title>
    <p id="p0240">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
  </sec>
</body>
<back>
  <ref-list id="bi005">
    <title>References</title>
    <ref id="b0005">
      <label>1</label>
      <mixed-citation publication-type="other" id="h0005">Lagerstrm MC, Schith HB. Lagerstrom, M. C. &amp; Schioth, H. B. Structural diversity of G protein-coupled receptors and significance for drug discovery. Nature Rev. Drug Discov. 7, 339-357. Nature Reviews Drug Discovery 2008;7:339–57.</mixed-citation>
    </ref>
    <ref id="b0010">
      <label>2</label>
      <element-citation publication-type="journal" id="h0010">
        <person-group person-group-type="author">
          <name>
            <surname>Jacoby</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Bouhelal</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Gerspacher</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Seuwen</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>The 7 TM G-protein-coupled receptor target family</article-title>
        <source>ChemMedChem</source>
        <volume>1</volume>
        <year>2006</year>
        <fpage>760</fpage>
        <lpage>782</lpage>
        <pub-id pub-id-type="doi">10.1002/cmdc.200600134</pub-id>
      </element-citation>
    </ref>
    <ref id="b0015">
      <label>3</label>
      <element-citation publication-type="journal" id="h0015">
        <person-group person-group-type="author">
          <name>
            <surname>Fredriksson</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Lagerström</surname>
            <given-names>M.C.</given-names>
          </name>
          <name>
            <surname>Lundin</surname>
            <given-names>L.-G.</given-names>
          </name>
          <name>
            <surname>Schiöth</surname>
            <given-names>H.B.</given-names>
          </name>
        </person-group>
        <article-title>The G-protein-coupled receptors in the human genome form five main families. Phylogenetic analysis, paralogon groups, and fingerprints</article-title>
        <source>Mol Pharmacol</source>
        <volume>63</volume>
        <issue>6</issue>
        <year>2003</year>
        <fpage>1256</fpage>
        <lpage>1272</lpage>
        <pub-id pub-id-type="doi">10.1124/mol.63.6.1256</pub-id>
        <pub-id pub-id-type="pmid">12761335</pub-id>
      </element-citation>
    </ref>
    <ref id="b0020">
      <label>4</label>
      <element-citation publication-type="journal" id="h0020">
        <person-group person-group-type="author">
          <name>
            <surname>Ramesh</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Soliman</surname>
            <given-names>M.E.</given-names>
          </name>
        </person-group>
        <article-title>G-protein coupled receptors (GPCRs): a comprehensive computational perspective</article-title>
        <source>Comb Chem High Throughput Screen</source>
        <volume>18</volume>
        <year>2015</year>
        <fpage>346</fpage>
        <lpage>364</lpage>
        <pub-id pub-id-type="doi">10.2174/1386207318666150305155545</pub-id>
        <pub-id pub-id-type="pmid">25747435</pub-id>
      </element-citation>
    </ref>
    <ref id="b0025">
      <label>5</label>
      <mixed-citation publication-type="other" id="h0025">Ao C, Gao L, Yu L. Identifying G-protein Coupled Receptors Using Mixed-Feature Extraction Methods and Machine Learning Methods. IEEE Access 2020;PP:1.</mixed-citation>
    </ref>
    <ref id="b0030">
      <label>6</label>
      <element-citation publication-type="journal" id="h0030">
        <person-group person-group-type="author">
          <name>
            <surname>Eo</surname>
            <given-names>H.-S.</given-names>
          </name>
          <name>
            <surname>Choi</surname>
            <given-names>J.P.</given-names>
          </name>
          <name>
            <surname>Noh</surname>
            <given-names>S.-J.</given-names>
          </name>
          <name>
            <surname>Hur</surname>
            <given-names>C.-G.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <article-title>A combined approach for the classification of G protein-coupled receptors and its application to detect GPCR splice variants</article-title>
        <source>Comput Biol Chem</source>
        <volume>31</volume>
        <issue>4</issue>
        <year>2007</year>
        <fpage>246</fpage>
        <lpage>256</lpage>
        <pub-id pub-id-type="pmid">17631418</pub-id>
      </element-citation>
    </ref>
    <ref id="b0035">
      <label>7</label>
      <element-citation publication-type="journal" id="h0035">
        <person-group person-group-type="author">
          <name>
            <surname>Baldwin</surname>
            <given-names>J.M.</given-names>
          </name>
        </person-group>
        <article-title>Structure and function of receptors coupled to G proteins</article-title>
        <source>Curr Opin Cell Biol</source>
        <volume>6</volume>
        <issue>2</issue>
        <year>1994</year>
        <fpage>180</fpage>
        <lpage>190</lpage>
        <pub-id pub-id-type="pmid">8024808</pub-id>
      </element-citation>
    </ref>
    <ref id="b0040">
      <label>8</label>
      <element-citation publication-type="journal" id="h0040">
        <person-group person-group-type="author">
          <name>
            <surname>Chou</surname>
            <given-names>K.-C.</given-names>
          </name>
          <name>
            <surname>Elrod</surname>
            <given-names>D.W.</given-names>
          </name>
        </person-group>
        <article-title>Bioinformatical analysis of G-protein-coupled receptors</article-title>
        <source>J Proteome Res</source>
        <volume>1</volume>
        <issue>5</issue>
        <year>2002</year>
        <fpage>429</fpage>
        <lpage>433</lpage>
        <pub-id pub-id-type="pmid">12645914</pub-id>
      </element-citation>
    </ref>
    <ref id="b0045">
      <label>9</label>
      <element-citation publication-type="journal" id="h0045">
        <person-group person-group-type="author">
          <name>
            <surname>Katritch</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Cherezov</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Stevens</surname>
            <given-names>R.C.</given-names>
          </name>
        </person-group>
        <article-title>Structure-function of the g protein-coupled receptor superfamily</article-title>
        <source>Annu Rev Pharmacol Toxicol</source>
        <volume>53</volume>
        <issue>1</issue>
        <year>2013</year>
        <fpage>531</fpage>
        <lpage>556</lpage>
        <pub-id pub-id-type="pmid">23140243</pub-id>
      </element-citation>
    </ref>
    <ref id="b0050">
      <label>10</label>
      <element-citation publication-type="journal" id="h0050">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>R.u.</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>Tools for GPCR drug discovery</article-title>
        <source>Acta Pharmacol Sin</source>
        <volume>33</volume>
        <issue>3</issue>
        <year>2012</year>
        <fpage>372</fpage>
        <lpage>384</lpage>
        <pub-id pub-id-type="doi">10.1038/aps.2011.173</pub-id>
        <pub-id pub-id-type="pmid">22266728</pub-id>
      </element-citation>
    </ref>
    <ref id="b0055">
      <label>11</label>
      <element-citation publication-type="journal" id="h0055">
        <person-group person-group-type="author">
          <name>
            <surname>Alexander</surname>
            <given-names>S.P.</given-names>
          </name>
          <name>
            <surname>Mathie</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Peters</surname>
            <given-names>J.A.</given-names>
          </name>
        </person-group>
        <article-title>Guide to receptors and channels (GRAC)</article-title>
        <source>Br J Pharmacol</source>
        <volume>164</volume>
        <issue>Suppl 1</issue>
        <year>2011</year>
        <fpage>S1</fpage>
        <lpage>324</lpage>
        <pub-id pub-id-type="doi">10.1111/j.1476-5381.2011.01649_1.x</pub-id>
        <pub-id pub-id-type="pmid">22040146</pub-id>
      </element-citation>
    </ref>
    <ref id="b0060">
      <label>12</label>
      <element-citation publication-type="journal" id="h0060">
        <person-group person-group-type="author">
          <name>
            <surname>Zia Ur</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Khan</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Identifying GPCRs and their types with Chou’s pseudo amino acid composition: an approach from multi-scale energy representation and position specific scoring matrix</article-title>
        <source>Protein Pept Lett</source>
        <volume>19</volume>
        <year>2012</year>
        <fpage>890</fpage>
        <lpage>903</lpage>
        <pub-id pub-id-type="doi">10.2174/092986612801619589</pub-id>
        <pub-id pub-id-type="pmid">22316312</pub-id>
      </element-citation>
    </ref>
    <ref id="b0065">
      <label>13</label>
      <element-citation publication-type="journal" id="h0065">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Ling</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>Q.i.</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Classification of G-protein coupled receptors based on a rich generation of convolutional neural network, N-gram transformation and multiple sequence alignments</article-title>
        <source>Amino Acids</source>
        <volume>50</volume>
        <issue>2</issue>
        <year>2018</year>
        <fpage>255</fpage>
        <lpage>266</lpage>
        <pub-id pub-id-type="doi">10.1007/s00726-017-2512-4</pub-id>
        <pub-id pub-id-type="pmid">29151135</pub-id>
      </element-citation>
    </ref>
    <ref id="b0070">
      <label>14</label>
      <element-citation publication-type="journal" id="h0070">
        <person-group person-group-type="author">
          <name>
            <surname>Liao</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Ju</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zou</surname>
            <given-names>Q.</given-names>
          </name>
        </person-group>
        <article-title>Prediction of G protein-coupled receptors with SVM-prot features and random forest</article-title>
        <source>Scientifica</source>
        <volume>2016</volume>
        <year>2016</year>
        <fpage>1</fpage>
        <lpage>10</lpage>
      </element-citation>
    </ref>
    <ref id="b0075">
      <label>15</label>
      <element-citation publication-type="journal" id="h0075">
        <person-group person-group-type="author">
          <name>
            <surname>Peng</surname>
            <given-names>Z.-L.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>J.-Y.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>An improved classification of G-protein-coupled receptors using sequence-derived features</article-title>
        <source>BMC Bioinf</source>
        <volume>11</volume>
        <issue>1</issue>
        <year>2010</year>
        <fpage>420</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-11-420</pub-id>
      </element-citation>
    </ref>
    <ref id="b0080">
      <label>16</label>
      <element-citation publication-type="journal" id="h0080">
        <person-group person-group-type="author">
          <name>
            <surname>Naveed</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Khan</surname>
            <given-names>A.U.</given-names>
          </name>
        </person-group>
        <article-title>GPCR-MPredictor: multi-level prediction of G protein-coupled receptors using genetic ensemble</article-title>
        <source>Amino Acids</source>
        <volume>42</volume>
        <year>2012</year>
        <fpage>1825</fpage>
      </element-citation>
    </ref>
    <ref id="b0085">
      <label>17</label>
      <element-citation publication-type="book" id="h0085">
        <person-group person-group-type="author">
          <name>
            <surname>Dongardive</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Abraham</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Behera</surname>
            <given-names>H.S.</given-names>
          </name>
          <name>
            <surname>Mohapatra</surname>
            <given-names>D.P.</given-names>
          </name>
        </person-group>
        <part-title>Protein sequence classification based on N-gram and K-nearest neighbor algorithm</part-title>
        <year>2016</year>
        <publisher-name>Springer India</publisher-name>
        <publisher-loc>New Delhi</publisher-loc>
        <fpage>163</fpage>
        <lpage>171</lpage>
      </element-citation>
    </ref>
    <ref id="b0090">
      <label>18</label>
      <element-citation publication-type="journal" id="h0090">
        <person-group person-group-type="author">
          <name>
            <surname>Nie</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>A novel fractal approach for predicting G-protein-coupled receptors and their subfamilies with support vector machines</article-title>
        <source>Biomed Mater Eng</source>
        <volume>26</volume>
        <issue>s1</issue>
        <year>2015</year>
        <fpage>S1829</fpage>
        <lpage>S1836</lpage>
        <pub-id pub-id-type="doi">10.3233/BME-151485</pub-id>
        <pub-id pub-id-type="pmid">26405954</pub-id>
      </element-citation>
    </ref>
    <ref id="b0095">
      <label>19</label>
      <mixed-citation publication-type="other" id="h0095">Li M, Ling C, Gao J. An efficient CNN-based classification on G-protein Coupled Receptors using TF-IDF and N-gram. 2017. doi: 10.1109/ISCC.2017.8024644.</mixed-citation>
    </ref>
    <ref id="b0100">
      <label>20</label>
      <mixed-citation publication-type="other" id="h0100">Wang X, Yu B, Ma A, Chen C, Liu B, Ma Q. Protein-protein interaction sites prediction by ensemble random forests with synthetic minority oversampling technique. Bioinformatics 2019;35:2395–402. Doi: 10.1093/bioinformatics/bty995.</mixed-citation>
    </ref>
    <ref id="b0105">
      <label>21</label>
      <element-citation publication-type="journal" id="h0105">
        <person-group person-group-type="author">
          <name>
            <surname>Blagus</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Lusa</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>SMOTE for high-dimensional class-imbalanced data</article-title>
        <source>BMC Bioinf</source>
        <volume>14</volume>
        <year>2013</year>
        <fpage>1</fpage>
        <lpage>16</lpage>
      </element-citation>
    </ref>
    <ref id="b0110">
      <label>22</label>
      <element-citation publication-type="journal" id="h0110">
        <person-group person-group-type="author">
          <name>
            <surname>Chawla</surname>
            <given-names>N.V.</given-names>
          </name>
          <name>
            <surname>Bowyer</surname>
            <given-names>K.W.</given-names>
          </name>
          <name>
            <surname>Hall</surname>
            <given-names>L.O.</given-names>
          </name>
          <name>
            <surname>Kegelmeyer</surname>
            <given-names>W.P.</given-names>
          </name>
        </person-group>
        <article-title>Synthetic minority over-sampling technique</article-title>
        <source>J Artific Intell Res</source>
        <volume>16</volume>
        <year>2002</year>
        <fpage>321</fpage>
        <lpage>357</lpage>
      </element-citation>
    </ref>
    <ref id="b0115">
      <label>23</label>
      <element-citation publication-type="journal" id="h0115">
        <person-group person-group-type="author">
          <name>
            <surname>Zou</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>A novel features ranking metric with application to scalable visual and bioinformatics data classification</article-title>
        <source>Neurocomputing</source>
        <volume>173</volume>
        <year>2016</year>
        <fpage>346</fpage>
        <lpage>354</lpage>
      </element-citation>
    </ref>
    <ref id="b0120">
      <label>24</label>
      <element-citation publication-type="journal" id="h0120">
        <person-group person-group-type="author">
          <name>
            <surname>Schmidhuber</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Deep learning in neural networks: an overview</article-title>
        <source>Neural Netw</source>
        <volume>61</volume>
        <year>2015</year>
        <fpage>85</fpage>
        <lpage>117</lpage>
        <pub-id pub-id-type="pmid">25462637</pub-id>
      </element-citation>
    </ref>
    <ref id="b0125">
      <label>25</label>
      <element-citation publication-type="journal" id="h0125">
        <person-group person-group-type="author">
          <name>
            <surname>Hinton</surname>
            <given-names>G.E.</given-names>
          </name>
        </person-group>
        <article-title>Learning multiple layers of representation</article-title>
        <source>Trends Cogn Sci</source>
        <volume>11</volume>
        <issue>10</issue>
        <year>2007</year>
        <fpage>428</fpage>
        <lpage>434</lpage>
        <pub-id pub-id-type="pmid">17921042</pub-id>
      </element-citation>
    </ref>
    <ref id="b0130">
      <label>26</label>
      <element-citation publication-type="journal" id="h0130">
        <person-group person-group-type="author">
          <name>
            <surname>Hao</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Deep learning</article-title>
        <source>Int J Seman Comput</source>
        <volume>10</volume>
        <issue>03</issue>
        <year>2016</year>
        <fpage>417</fpage>
        <lpage>439</lpage>
      </element-citation>
    </ref>
    <ref id="b0135">
      <label>27</label>
      <mixed-citation publication-type="other" id="h0135">Lv H, Dao F-Y, Guan Z-X, Yang H, Li Y-W, Lin H. Deep-Kcr: accurate detection of lysine crotonylation sites using deep learning method. Brief Bioinform 2020. 10.1093/bib/bbaa255.</mixed-citation>
    </ref>
    <ref id="b0140">
      <label>28</label>
      <mixed-citation publication-type="other" id="h0140">Wang D, Zhang Z, Jiang Y, Mao Z, Wang D, Lin H, et al. DM3Loc: multi-label mRNA subcellular localization prediction and analysis based on multi-head self-attention mechanism. Nucleic Acids Res 2021. 10.1093/nar/gkab016.</mixed-citation>
    </ref>
    <ref id="b0145">
      <label>29</label>
      <mixed-citation publication-type="other" id="h0145">Duolin, Wang, Yanchun, Liang, Dong. Capsule network for protein post-translational modification site prediction. Bioinformatics 2019;35:2386–94.</mixed-citation>
    </ref>
    <ref id="b0150">
      <label>30</label>
      <element-citation publication-type="journal" id="h0150">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Pan</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>Deep learning in bioinformatics: Introduction, application, and perspective in the big data era</article-title>
        <source>Methods</source>
        <volume>166</volume>
        <year>2019</year>
        <fpage>4</fpage>
        <lpage>21</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ymeth.2019.04.008</pub-id>
        <pub-id pub-id-type="pmid">31022451</pub-id>
      </element-citation>
    </ref>
    <ref id="b0155">
      <label>31</label>
      <element-citation publication-type="journal" id="h0155">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Deep residual learning for image recognition</article-title>
        <source>IEEE Conf Comput Vis Pattern Recogn (CVPR)</source>
        <volume>2016</volume>
        <year>2016</year>
        <fpage>770</fpage>
        <lpage>778</lpage>
        <pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id>
      </element-citation>
    </ref>
    <ref id="b0160">
      <label>32</label>
      <mixed-citation publication-type="other" id="h0160">Lin TY, Goyal P, Girshick R, He K, Dollár P. Focal Loss for Dense Object Detection. IEEE Transactions on Pattern Analysis &amp; Machine Intelligence 2017;PP:2999–3007.</mixed-citation>
    </ref>
    <ref id="b0165">
      <label>33</label>
      <mixed-citation publication-type="other" id="h0165">Lin YO, Lei H, Li XY, Wu J. Deep Learning in NLP: Methods and Applications. Dianzi Keji Daxue Xuebao/Journal of the University of Electronic Science and Technology of China 2017;46:913–9.</mixed-citation>
    </ref>
    <ref id="b0170">
      <label>34</label>
      <element-citation publication-type="journal" id="h0170">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Ye</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Xing</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Cambria</surname>
            <given-names>E.</given-names>
          </name>
        </person-group>
        <article-title>Ensemble application of convolutional and recurrent neural networks for multi-label text categorization</article-title>
        <source>Int Joint Conf Neural Netw (IJCNN)</source>
        <volume>2017</volume>
        <year>2017</year>
        <fpage>2377</fpage>
        <lpage>2383</lpage>
        <pub-id pub-id-type="doi">10.1109/IJCNN.2017.7966144</pub-id>
      </element-citation>
    </ref>
    <ref id="b0175">
      <label>35</label>
      <element-citation publication-type="journal" id="h0175">
        <person-group person-group-type="author">
          <name>
            <surname>Uçar</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Demir</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Güzeliş</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>Object recognition and detection with deep learning for autonomous driving applications</article-title>
        <source>Simulation</source>
        <volume>93</volume>
        <issue>9</issue>
        <year>2017</year>
        <fpage>759</fpage>
        <lpage>769</lpage>
        <pub-id pub-id-type="doi">10.1177/0037549717709932</pub-id>
      </element-citation>
    </ref>
    <ref id="b0180">
      <label>36</label>
      <element-citation publication-type="journal" id="h0180">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Seff</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Kornhauser</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>DeepDriving: learning affordance for direct perception in autonomous driving</article-title>
        <source>IEEE Int Conf Comput Vision (ICCV)</source>
        <volume>2015</volume>
        <year>2015</year>
        <fpage>2722</fpage>
        <lpage>2730</lpage>
        <pub-id pub-id-type="doi">10.1109/ICCV.2015.312</pub-id>
      </element-citation>
    </ref>
    <ref id="b0185">
      <label>37</label>
      <mixed-citation publication-type="other" id="h0185">Mikolov T, Corrado G, Kai C, Dean J. Efficient Estimation of Word Representations in Vector Space. Proceedings of the International Conference on Learning Representations (ICLR 2013), 2013.</mixed-citation>
    </ref>
    <ref id="b0190">
      <label>38</label>
      <element-citation publication-type="journal" id="h0190">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>K.K.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Bedbrook</surname>
            <given-names>C.N.</given-names>
          </name>
          <name>
            <surname>Arnold</surname>
            <given-names>F.H.</given-names>
          </name>
        </person-group>
        <article-title>Learned protein embeddings for machine learning</article-title>
        <source>Bioinformatics</source>
        <volume>34</volume>
        <year>2018</year>
        <fpage>4138</fpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty455</pub-id>
        <pub-id pub-id-type="pmid">29933431</pub-id>
      </element-citation>
    </ref>
    <ref id="b0195">
      <label>39</label>
      <element-citation publication-type="journal" id="h0195">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Qiu</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>Identifying GPCR-drug interaction based on wordbook learning from sequences</article-title>
        <source>BMC Bioinf</source>
        <volume>21</volume>
        <year>2020</year>
        <fpage>150</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-020-3488-8</pub-id>
      </element-citation>
    </ref>
    <ref id="b0200">
      <label>40</label>
      <element-citation publication-type="journal" id="h0200">
        <person-group person-group-type="author">
          <name>
            <surname>Boutet</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Lieberherr</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Tognolli</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Schneider</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Bansal</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Bridge</surname>
            <given-names>A.J.</given-names>
          </name>
        </person-group>
        <article-title>UniProtKB/Swiss-prot, the manually annotated section of the UniProt KnowledgeBase: how to use The entry view</article-title>
        <source>Methods Mol Biol</source>
        <volume>1374</volume>
        <year>2016</year>
        <fpage>23</fpage>
        <lpage>54</lpage>
        <pub-id pub-id-type="doi">10.1007/978-1-4939-3167-5_2</pub-id>
        <pub-id pub-id-type="pmid">26519399</pub-id>
      </element-citation>
    </ref>
    <ref id="b0205">
      <label>41</label>
      <element-citation publication-type="journal" id="h0205">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Godzik</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Cd-Hit: a fast program for Clustering and Comparing Large Sets of Protein or Nucleotide Sequences</article-title>
        <source>Bioinformatics (Oxford, England)</source>
        <volume>22</volume>
        <year>2006</year>
        <fpage>1658</fpage>
        <lpage>1659</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btl158</pub-id>
      </element-citation>
    </ref>
    <ref id="b0210">
      <label>42</label>
      <element-citation publication-type="journal" id="h0210">
        <person-group person-group-type="author">
          <name>
            <surname>Fu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Niu</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <article-title>CD-HIT: accelerated for clustering the next-generation sequencing data</article-title>
        <source>Bioinformatics</source>
        <year>2012</year>
        <fpage>3150</fpage>
        <lpage>3152</lpage>
        <pub-id pub-id-type="pmid">23060610</pub-id>
      </element-citation>
    </ref>
    <ref id="b0215">
      <label>43</label>
      <element-citation publication-type="journal" id="h0215">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>H.-D.</given-names>
          </name>
          <name>
            <surname>Zulfiqar</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Yuan</surname>
            <given-names>S.-S.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>Q.-L.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z.-Y.</given-names>
          </name>
        </person-group>
        <article-title>iBLP: an XGBoost-Based Predictor for Identifying Bioluminescent Proteins</article-title>
        <source>Comput Math Methods Med</source>
        <volume>2021</volume>
        <year>2021</year>
        <fpage>6664362</fpage>
        <pub-id pub-id-type="doi">10.1155/2021/6664362</pub-id>
        <pub-id pub-id-type="pmid">33505515</pub-id>
      </element-citation>
    </ref>
    <ref id="b0220">
      <label>44</label>
      <element-citation publication-type="journal" id="h0220">
        <person-group person-group-type="author">
          <name>
            <surname>Dao</surname>
            <given-names>F.Y.</given-names>
          </name>
          <name>
            <surname>Lv</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z.M.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>DeepYY1: a deep learning approach to identify YY1-mediated chromatin loops</article-title>
        <source>Brief Bioinform</source>
        <year>2020</year>
        <pub-id pub-id-type="doi">10.1093/bib/bbaa356</pub-id>
      </element-citation>
    </ref>
    <ref id="b0225">
      <label>45</label>
      <element-citation publication-type="journal" id="h0225">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Nie</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>iATP: a sequence based method for identifying anti-tubercular peptides</article-title>
        <source>Med Chem</source>
        <year>2019</year>
        <pub-id pub-id-type="doi">10.2174/1573406415666191002152441</pub-id>
      </element-citation>
    </ref>
    <ref id="b0230">
      <label>46</label>
      <element-citation publication-type="journal" id="h0230">
        <person-group person-group-type="author">
          <name>
            <surname>Qiu</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Lv</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Hong</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Jia</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>BOW-GBDT: a GBDT classifier combining with artificial neural network for identifying GPCR-drug interaction based on wordbook learning from sequences</article-title>
        <source>Front Cell Dev Biol</source>
        <volume>8</volume>
        <year>2020</year>
        <object-id pub-id-type="publisher-id">623858</object-id>
        <pub-id pub-id-type="doi">10.3389/fcell.2020.623858</pub-id>
      </element-citation>
    </ref>
    <ref id="b0235">
      <label>47</label>
      <element-citation publication-type="journal" id="h0235">
        <person-group person-group-type="author">
          <name>
            <surname>Rumelhart</surname>
            <given-names>D.E.</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G.E.</given-names>
          </name>
          <name>
            <surname>Williams</surname>
            <given-names>R.J.</given-names>
          </name>
        </person-group>
        <article-title>Learning representations by back propagating errors</article-title>
        <source>Nature</source>
        <volume>323</volume>
        <year>1986</year>
        <fpage>533</fpage>
        <lpage>536</lpage>
      </element-citation>
    </ref>
    <ref id="b0240">
      <label>48</label>
      <element-citation publication-type="journal" id="h0240">
        <person-group person-group-type="author">
          <name>
            <surname>Judith</surname>
            <given-names>E.D.P.D.</given-names>
          </name>
          <name>
            <surname>Deleo</surname>
            <given-names>J.M.</given-names>
          </name>
        </person-group>
        <article-title>Artificial neural networks</article-title>
        <source>Cancer</source>
        <volume>91</volume>
        <year>2001</year>
        <fpage>1615</fpage>
        <lpage>1635</lpage>
        <pub-id pub-id-type="pmid">11309760</pub-id>
      </element-citation>
    </ref>
    <ref id="b0245">
      <label>49</label>
      <mixed-citation publication-type="other" id="h0245">Mikolov T, Sutskever I, Chen K, Corrado G, Dean J. Distributed representations of words and phrases and their compositionality. Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2 2013:3111–9.</mixed-citation>
    </ref>
    <ref id="b0250">
      <label>50</label>
      <mixed-citation publication-type="other" id="h0250">Rong X. word2vec Parameter Learning Explained. Computer Science 2014.</mixed-citation>
    </ref>
    <ref id="b0255">
      <label>51</label>
      <element-citation publication-type="book" id="h0255">
        <person-group person-group-type="author">
          <name>
            <surname>Bottou</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Lechevallier</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Saporta</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <part-title>Large-scale machine learning with stochastic gradient descent</part-title>
        <year>2010</year>
        <publisher-name>Physica-Verlag HD</publisher-name>
        <publisher-loc>Heidelberg</publisher-loc>
        <fpage>177</fpage>
        <lpage>186</lpage>
      </element-citation>
    </ref>
    <ref id="b0260">
      <label>52</label>
      <element-citation publication-type="journal" id="h0260">
        <person-group person-group-type="author">
          <name>
            <surname>Kawashima</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Kanehisa</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>AAindex: amino acid index database</article-title>
        <source>Nucleic Acids Res</source>
        <volume>28</volume>
        <year>2000</year>
        <fpage>374</fpage>
        <pub-id pub-id-type="pmid">10592278</pub-id>
      </element-citation>
    </ref>
    <ref id="b0265">
      <label>53</label>
      <mixed-citation publication-type="other" id="h0265">Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. International conference on machine learning, PMLR; 2015, p. 448–56.</mixed-citation>
    </ref>
    <ref id="b0270">
      <label>54</label>
      <mixed-citation publication-type="other" id="h0270">Kingma DP, Ba J. Adam: A method for stochastic optimization. ArXiv Preprint ArXiv:14126980 2014.</mixed-citation>
    </ref>
    <ref id="b0275">
      <label>55</label>
      <mixed-citation publication-type="other" id="h0275">Maas AL, Hannun AY, Ng AY, others. Rectifier nonlinearities improve neural network acoustic models. Proc. icml, vol. 30, 2013, p. 3.</mixed-citation>
    </ref>
    <ref id="b0280">
      <label>56</label>
      <mixed-citation publication-type="other" id="h0280">He X, Pan J, Jin O, Xu T, Liu B, Xu T, et al. Practical Lessons from Predicting Clicks on Ads at Facebook. Proceedings of the Eighth International Workshop on Data Mining for Online Advertising 2014:1–9. 10.1145/2648584.2648589.</mixed-citation>
    </ref>
    <ref id="b0285">
      <label>57</label>
      <element-citation publication-type="journal" id="h0285">
        <person-group person-group-type="author">
          <name>
            <surname>Tian</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>An accurate eye pupil localization approach based on adaptive gradient boosting decision tree</article-title>
        <source>Vis Commun Image Process (VCIP)</source>
        <volume>2016</volume>
        <year>2016</year>
        <fpage>1</fpage>
        <lpage>4</lpage>
        <pub-id pub-id-type="doi">10.1109/VCIP.2016.7805483</pub-id>
      </element-citation>
    </ref>
    <ref id="b0290">
      <label>58</label>
      <mixed-citation publication-type="other" id="h0290">Friedman. Classification and Regression Trees. Wadsworth International Group; 1984.</mixed-citation>
    </ref>
    <ref id="b0295">
      <label>59</label>
      <element-citation publication-type="journal" id="h0295">
        <person-group person-group-type="author">
          <name>
            <surname>Breiman</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Random forests</article-title>
        <source>Mach Learn</source>
        <volume>45</volume>
        <year>2001</year>
        <fpage>5</fpage>
        <lpage>32</lpage>
      </element-citation>
    </ref>
    <ref id="b0300">
      <label>60</label>
      <element-citation publication-type="journal" id="h0300">
        <person-group person-group-type="author">
          <name>
            <surname>Bentéjac</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Csrg</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Martínez-Muoz</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>A comparative analysis of gradient boosting algorithms</article-title>
        <source>Artif Intell Rev</source>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>31</lpage>
      </element-citation>
    </ref>
    <ref id="b0305">
      <label>61</label>
      <mixed-citation publication-type="other" id="h0305">Chen T, Guestrin C. XGBoost: A Scalable Tree Boosting System. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 2016:785–94. 10.1145/2939672.2939785.</mixed-citation>
    </ref>
    <ref id="b0310">
      <label>62</label>
      <element-citation publication-type="journal" id="h0310">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>Risk prediction of diabetes: big data mining with fusion of multifarious physical examination indicators</article-title>
        <source>Inform Fusion</source>
        <year>2021</year>
        <pub-id pub-id-type="doi">10.1016/j.inffus.2021.02.015</pub-id>
      </element-citation>
    </ref>
    <ref id="b0315">
      <label>63</label>
      <element-citation publication-type="journal" id="h0315">
        <person-group person-group-type="author">
          <name>
            <surname>Tang</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Nie</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Kang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <article-title>mRNALocater: enhance the prediction accuracy of eukaryotic mRNA subcellular localization by using model fusion strategy</article-title>
        <source>Mol Ther</source>
        <year>2021</year>
        <pub-id pub-id-type="doi">10.1016/j.ymthe.2021.04.004</pub-id>
      </element-citation>
    </ref>
    <ref id="b0320">
      <label>64</label>
      <element-citation publication-type="journal" id="h0320">
        <person-group person-group-type="author">
          <name>
            <surname>Dietterich</surname>
            <given-names>T.G.</given-names>
          </name>
        </person-group>
        <article-title>An experimental comparison of three methods for constructing ensembles of decision trees: bagging, boosting, and randomization</article-title>
        <source>Mach Learn</source>
        <volume>40</volume>
        <year>2000</year>
        <fpage>139</fpage>
        <lpage>157</lpage>
      </element-citation>
    </ref>
  </ref-list>
  <ack id="ak005">
    <sec id="s0125">
      <title>Acknowledgments</title>
      <p id="p0245">This work was supported by the grants from the National Natural Science Foundation of China (No. 31760315, 31860312), Natural Science Foundation of Jiangxi Province, China (NO. 20202BAB202007).</p>
    </sec>
    <sec id="s0130">
      <title>Author contributions</title>
      <p id="p0250">W. Q. conceived and designed the experiments; Z. L. and S. S. performed the extraction of features, model construction, model training, and evaluation. W.Q. and Z. L. analyzed the data and implemented the classifiers. Z. L. drafted the manuscript. X. X. and H. L. supervised this project and revised the manuscript. All authors read and approved the final manuscript.</p>
    </sec>
  </ack>
</back>
<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_CSBJ1191 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEgr4 jpg ?>
<?FILEgr5 jpg ?>
<?FILEgr6 jpg ?>
<?FILEgr7 jpg ?>
<?FILEga1 jpg ?>
<?FILEsi1 svg ?>
<?FILEsi2 svg ?>
<?FILEsi3 svg ?>
<?FILEsi4 svg ?>
<?FILEsi5 svg ?>
<?FILEsi6 svg ?>
<?FILEsi7 svg ?>
<?FILEsi8 svg ?>
<?FILEsi9 svg ?>
<?FILEsi10 svg ?>
<?FILEsi11 svg ?>
<?FILEsi12 svg ?>
<?FILEsi13 svg ?>
<?FILEsi14 svg ?>
<?FILEsi15 svg ?>
<?FILEsi16 svg ?>
<?FILEsi17 svg ?>
<?FILEsi18 svg ?>
<?FILEsi19 svg ?>
<?FILEsi20 svg ?>
<?FILEsi21 svg ?>
<?FILEsi22 svg ?>
<?FILEsi23 svg ?>
<?FILEsi24 svg ?>
<?FILEsi25 svg ?>
<?FILEsi26 svg ?>
<?FILEsi27 svg ?>
<?FILEsi28 svg ?>
<?FILEsi29 svg ?>
<?FILEsi30 svg ?>
<?FILEsi31 svg ?>
<?FILEsi32 svg ?>
<?FILEsi33 svg ?>
<?FILEsi34 svg ?>
<?FILEsi35 svg ?>
<?FILEsi36 svg ?>
<?FILEsi37 svg ?>
<?FILEsi38 svg ?>
<?FILEsi39 svg ?>
<?FILEsi40 svg ?>
<?FILEsi41 svg ?>
<?FILEsi42 svg ?>
<?FILEsi43 svg ?>
<?FILEsi44 svg ?>
<?FILEsi45 svg ?>
<?FILEsi46 svg ?>
<?FILEsi47 svg ?>
<?FILEsi48 svg ?>
<?FILEsi49 svg ?>
<?FILEsi50 svg ?>
<?FILEsi51 svg ?>
<?FILEsi52 svg ?>
<?FILEsi53 svg ?>
<?FILEsi54 svg ?>
<?FILEsi55 svg ?>
<?FILEsi56 svg ?>
<?FILEsi57 svg ?>
<?FILEsi58 svg ?>
<?FILEsi59 svg ?>
<?FILEsi60 svg ?>
<?FILEsi61 svg ?>
<?FILEsi62 svg ?>
<?FILEsi63 svg ?>
<?FILEsi64 svg ?>
<?FILEsi65 svg ?>
<?FILEsi66 svg ?>
<?FILEsi67 svg ?>
<?FILEsi68 svg ?>
<?FILEsi271 svg ?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Comput Struct Biotechnol J</journal-id>
    <journal-id journal-id-type="iso-abbrev">Comput Struct Biotechnol J</journal-id>
    <journal-title-group>
      <journal-title>Computational and Structural Biotechnology Journal</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2001-0370</issn>
    <publisher>
      <publisher-name>Research Network of Computational and Structural Biotechnology</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8437786</article-id>
    <article-id pub-id-type="pii">S2001-0370(21)00375-5</article-id>
    <article-id pub-id-type="doi">10.1016/j.csbj.2021.08.044</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>EMCBOW-GPCR: A method for identifying G-protein coupled receptors based on word embedding and wordbooks</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au005">
        <name>
          <surname>Qiu</surname>
          <given-names>Wangren</given-names>
        </name>
        <email>qiuone@163.com</email>
        <xref rid="af005" ref-type="aff">a</xref>
        <xref rid="cor1" ref-type="corresp">⁎</xref>
      </contrib>
      <contrib contrib-type="author" id="au010">
        <name>
          <surname>Lv</surname>
          <given-names>Zhe</given-names>
        </name>
        <xref rid="af005" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author" id="au015">
        <name>
          <surname>Xiao</surname>
          <given-names>Xuan</given-names>
        </name>
        <email>jdzxiaoxuan@163.com</email>
        <xref rid="af005" ref-type="aff">a</xref>
        <xref rid="cor1" ref-type="corresp">⁎</xref>
      </contrib>
      <contrib contrib-type="author" id="au020">
        <name>
          <surname>Shao</surname>
          <given-names>Shuai</given-names>
        </name>
        <xref rid="af005" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author" id="au025">
        <name>
          <surname>Lin</surname>
          <given-names>Hao</given-names>
        </name>
        <email>hlin@uestc.edu.cn</email>
        <xref rid="af010" ref-type="aff">b</xref>
        <xref rid="cor1" ref-type="corresp">⁎</xref>
      </contrib>
      <aff id="af005"><label>a</label>School of Information Engineering, Jingdezhen Ceramic Institute, Jingdezhen, China</aff>
      <aff id="af010"><label>b</label>Center for Informational Biology, University of Electronic Science and Technology of China, Chengdu 610054, China</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>⁎</label>Corresponding authors. <email>qiuone@163.com</email><email>jdzxiaoxuan@163.com</email><email>hlin@uestc.edu.cn</email></corresp>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>31</day>
      <month>8</month>
      <year>2021</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>31</day>
      <month>8</month>
      <year>2021</year>
    </pub-date>
    <volume>19</volume>
    <fpage>4961</fpage>
    <lpage>4969</lpage>
    <history>
      <date date-type="received">
        <day>27</day>
        <month>4</month>
        <year>2021</year>
      </date>
      <date date-type="rev-recd">
        <day>7</day>
        <month>8</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>27</day>
        <month>8</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2021 The Author(s)</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).</license-p>
      </license>
    </permissions>
    <abstract abstract-type="graphical" id="ab005">
      <title>Graphical abstract</title>
      <fig id="f0040" position="anchor">
        <graphic xlink:href="ga1"/>
      </fig>
    </abstract>
    <abstract abstract-type="author-highlights" id="ab010">
      <title>Highlights</title>
      <p>
        <list list-type="simple" id="l0005">
          <list-item id="o0005">
            <label>•</label>
            <p id="p0005">An computational method was developed to identify G-protein coupled receptors.</p>
          </list-item>
          <list-item id="o0010">
            <label>•</label>
            <p id="p0010">Three word-embedding models and a bag-of-words model are used to extract original features.</p>
          </list-item>
          <list-item id="o0015">
            <label>•</label>
            <p id="p0015">A high accuracy was achieved by using fusion information.</p>
          </list-item>
          <list-item id="o0020">
            <label>•</label>
            <p id="p0020">A powerful tool was established.</p>
          </list-item>
        </list>
      </p>
    </abstract>
    <abstract id="ab015">
      <p>G Protein-Coupled Receptors (GPCRs) are one of the largest membrane protein receptor family in human, which are also important targets for many drugs. Thence, it’s of great significance to judge whether a protein is a GPCR or not. However, identifying GPCRs by experimental methods is very expensive and time-consuming. As more and more GPCR primary sequences are accumulated, it’s feasible to develop a computational model to predict GPCRs precisely and quickly. In this paper, a novel method called EMCBOW-GPCR has been proposed to improve the accuracy of identifying GPCRs based on natural language processing (NLP). For representing GPCRs, three word-embedding models and a bag-of-words model are used to extract original features. Then, the original features are thrown into a Deep-learning algorithm to extract features further and reduce the dimension. Finally, the obtained features are fed into Extreme Gradient Boosting. As shown with the results comparison, the overall prediction metrics of EMCBOW-GPCR are higher than the state of the arts. In order to be convenient for more researchers to use EMCBOW-GPCR, the method and source code have been opened in github, which are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/454170054/EMCBOW-GPCR" id="ir005">https://github.com/454170054/EMCBOW-GPCR</ext-link>, and a user-friendly web-server for EMCBOW-GPCR has been established at <ext-link ext-link-type="uri" xlink:href="http://www.jci-bioinfo.cn/emcbowgpcr" id="ir010">http://www.jci-bioinfo.cn/emcbowgpcr</ext-link>.</p>
    </abstract>
    <kwd-group id="kg005">
      <title>Keywords</title>
      <kwd>Natural language processing</kwd>
      <kwd>Word embedding</kwd>
      <kwd>Bag-of-words</kwd>
      <kwd>Extreme gradient boosting</kwd>
      <kwd>Deep-learning</kwd>
      <kwd>GPCRs</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="s0005">
    <label>1</label>
    <title>Introduction</title>
    <p id="p0025">G protein coupled receptors (GPCRs) are one of the largest family of membrane proteins in mammalian genomes which are widely distributed among the central nervous system, immune system, cardiovascular system, retina and other organs and tissues <xref rid="b0005" ref-type="bibr">[1]</xref>, <xref rid="b0010" ref-type="bibr">[2]</xref>, <xref rid="b0015" ref-type="bibr">[3]</xref>, <xref rid="b0020" ref-type="bibr">[4]</xref>. GPCRs can be divided into six classes <xref rid="b0025" ref-type="bibr">[5]</xref>: rhodopsin-like receptors, secretin-like receptors, metabo-tropic glutamate receptors, fungal mating pheromone receptors, cyclic AMP receptors and frizzled receptors. What’s more, they can regulate a wide range of physiological processes such as neurotransmission, growth, immune responses and so on <xref rid="b0030" ref-type="bibr">[6]</xref>, <xref rid="b0035" ref-type="bibr">[7]</xref>, <xref rid="b0040" ref-type="bibr">[8]</xref>, <xref rid="b0045" ref-type="bibr">[9]</xref>. Due to their structural characteristics and key role in signal transduction, GPCRs are the most important drugs target in modern drug research and development <xref rid="b0050" ref-type="bibr">[10]</xref>, <xref rid="b0055" ref-type="bibr">[11]</xref>. Therefore, identifying GPCRs accurately is very significant for drug development.</p>
    <p id="p0030">As more and more public GPCRs data is available, there are many efficient methods based on extracting features from sequence that are proposed to predict GPCRs in recent years. These methods usually consist of two parts: classification algorithm and feature extraction. The classification algorithms mainly based on statistics and machine learning methods, including Artificial Neural Network (ANN) <xref rid="b0060" ref-type="bibr">[12]</xref>, <xref rid="b0065" ref-type="bibr">[13]</xref>, Random Forest(RF) <xref rid="b0025" ref-type="bibr">[5]</xref>, <xref rid="b0070" ref-type="bibr">[14]</xref>, intimate sorting <xref rid="b0075" ref-type="bibr">[15]</xref>, K-Nearest Neighbor(KNN) <xref rid="b0080" ref-type="bibr">[16]</xref>, <xref rid="b0085" ref-type="bibr">[17]</xref>, etc. The methods of feature representation for predicting GPCRs contain amino acid composition (AAC) <xref rid="b0080" ref-type="bibr">[16]</xref>, <xref rid="b0090" ref-type="bibr">[18]</xref>, 400D <xref rid="b0025" ref-type="bibr">[5]</xref>, N-gram <xref rid="b0025" ref-type="bibr">[5]</xref>, <xref rid="b0065" ref-type="bibr">[13]</xref>, <xref rid="b0095" ref-type="bibr">[19]</xref>, SVM-Prot <xref rid="b0070" ref-type="bibr">[14]</xref>, etc. Zou <xref rid="b0070" ref-type="bibr">[14]</xref> proposed a novel method in which the GPCRs were represented by a 188D feature vectors of SVM-Prot and the synthetic minority oversampling technique (SMOTE) <xref rid="b0100" ref-type="bibr">[20]</xref>, <xref rid="b0105" ref-type="bibr">[21]</xref>, <xref rid="b0110" ref-type="bibr">[22]</xref> algorithm was used to generate some new positive samples to balance the training datasets. Finally, the prediction method adopted RF algorithm to be trained with the datasets. Recently, Yu <xref rid="b0025" ref-type="bibr">[5]</xref> used the mixed-feature extraction methods to acquire the feature vector of GPCRs. In the work, three feature engineering methods including 400D, N-Gram and parallel correlation pseudo amino acid composition (PC-PseAAC) were chosen to extract features of GPCRs, respectively. Subsequently, these three feature vectors are randomly arranged and combined to form the mixed-feature. Further, the max relevance max distance (MRMD) <xref rid="b0115" ref-type="bibr">[23]</xref> was employed to reduce the dimension of mixed-feature. According to the result, the mixed-feature concatenated by 400D and PC-PseAAC can achieve the best performance with RF algorithm. Although these methods have achieved positive results on predicting GPCRs, there is still room for improvement.</p>
    <p id="p0035">As a new research hotspot in artificial intelligence, Deep learning (DL) <xref rid="b0120" ref-type="bibr">[24]</xref>, <xref rid="b0125" ref-type="bibr">[25]</xref>, <xref rid="b0130" ref-type="bibr">[26]</xref>, <xref rid="b0135" ref-type="bibr">[27]</xref>, <xref rid="b0140" ref-type="bibr">[28]</xref> is more and more widely used in machine learning. Because of its great help to the interpretation of data such as text, image and sound, DL has achieved many positive results in speech machine translation and image recognition far beyond previous related technologies, and has been successfully applied in many fields such as bioinformatics <xref rid="b0145" ref-type="bibr">[29]</xref>, <xref rid="b0150" ref-type="bibr">[30]</xref>, computer vision <xref rid="b0155" ref-type="bibr">[31]</xref>, <xref rid="b0160" ref-type="bibr">[32]</xref>, natural language processing <xref rid="b0165" ref-type="bibr">[33]</xref>, <xref rid="b0170" ref-type="bibr">[34]</xref>, Automatic driving <xref rid="b0175" ref-type="bibr">[35]</xref>, <xref rid="b0180" ref-type="bibr">[36]</xref>and so on. In this work, we propose a novel method called EMCBOW-GPCR to predict GPCRs based on word embedding <xref rid="b0185" ref-type="bibr">[37]</xref>, <xref rid="b0190" ref-type="bibr">[38]</xref>, BOW <xref rid="b0195" ref-type="bibr">[39]</xref> and DL models. Firstly, we split the GPCRs sequences into segments of different lengths, and train the corresponding word embedding model with the split segments. Further, the every GPCR sequence is inputted into the word embedding models to get the word vectors. A BOW model was used to extract features at the same time. Secondly, the features by extracting from different methods are concatenated to form the original feature vectors. Thirdly, the original feature vectors are fed into a DL model to reduce the dimension and extract features further. Finally, the processed features are thrown into XGBoost algorithm to train a predictor. According to the results compared with other methods tested with the same data and performance measurement, our method can have a better performance.</p>
  </sec>
  <sec id="s0010">
    <label>2</label>
    <title>Datasets and methods</title>
    <sec id="s0015">
      <label>2.1</label>
      <title>Experimental datasets and performance measurement</title>
      <p id="p0040">The benchmark dataset used for evaluating the proposed method is the same as that used in literatures <xref rid="b0025" ref-type="bibr">[5]</xref>, <xref rid="b0070" ref-type="bibr">[14]</xref> and is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/454170054/EMCBOW-GPCR/blob/main/files" id="ir015">https://github.com/454170054/EMCBOW-GPCR/blob/main/files</ext-link>. The dataset sequences were download from UniProt <xref rid="b0200" ref-type="bibr">[40]</xref> database and CD-Hit <xref rid="b0205" ref-type="bibr">[41]</xref>, <xref rid="b0210" ref-type="bibr">[42]</xref> program was used to reduce the sequence homology <xref rid="b0070" ref-type="bibr">[14]</xref>. The sequence identity threshold was 0.8. The evaluation indicators used to test the performance of the methods in the work are Accuracy (Acc), Precision (Pre), Sensitivity (Sn), Specificity (Sp) and Matthews correlation coefficient (MCC) <xref rid="b0215" ref-type="bibr">[43]</xref>, <xref rid="b0220" ref-type="bibr">[44]</xref>, <xref rid="b0225" ref-type="bibr">[45]</xref>, which are listed in formula (1) explicitly.<disp-formula id="e0005"><label>(1)</label><mml:math id="M1" altimg="si10.svg"><mml:mrow><mml:mfenced open="{"><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mi mathvariant="italic">Accuracy</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mi mathvariant="italic">Precision</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TN</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">TN</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">Sensitivity</mml:mi><mml:mo>+</mml:mo><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>×</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mfenced><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfenced><mml:mrow><mml:mfenced><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mfenced><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M2" altimg="si11.svg"><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow></mml:math></inline-formula> is the number of sequences that are GPCRs in fact and predicted as GPCRs,<inline-formula><mml:math id="M3" altimg="si12.svg"><mml:mrow><mml:mi mathvariant="italic">TN</mml:mi></mml:mrow></mml:math></inline-formula> is the number of sequences that are non-GPCRs in fact and predicted as non-GPCRs, <inline-formula><mml:math id="M4" altimg="si13.svg"><mml:mrow><mml:mi mathvariant="italic">FP</mml:mi></mml:mrow></mml:math></inline-formula>is the number of sequences that are non-GPCRs predicted as GPCRs,<inline-formula><mml:math id="M5" altimg="si14.svg"><mml:mrow><mml:mi mathvariant="italic">FN</mml:mi></mml:mrow></mml:math></inline-formula>is the number of sequences that are GPCRs predicted as non-GPCRs. Further, we also apply Area Under ROC Curve (AUC) metric to evaluate the methods.</p>
    </sec>
    <sec id="s0020">
      <label>2.2</label>
      <title>Feature extraction methods</title>
      <p id="p0045">In this study, two technologies are applied to extract features from GPCRs respectively. One of them is BOW <xref rid="b0195" ref-type="bibr">[39]</xref>, <xref rid="b0230" ref-type="bibr">[46]</xref> model which has been confirmed to be powerful in extracting GPCRs features based on sequences, and the other is Word Embedding which is very popular and important concept in natural language processing (NLP). The detailed process of the two feature extraction methods is listed as follows:</p>
    </sec>
    <sec id="s0025">
      <label>2.3</label>
      <title>Word embedding</title>
      <p id="p0050">Word Embedding is a method of converting words into number vectors. The process of word embedding is to embed a high-dimensional space with all the number of words into a much lower dimensional and continuous vector space and each word or phrase is mapped to a vector in the real number field, and the result of word embedding generates a word vector which is the important technology of the task. In this paper, we trained three kinds of word embedding models and utilized them to generate corresponding feature vectors. The explicit training process is showed as follows:</p>
      <sec id="s0030">
        <label>2.3.1</label>
        <title>Step 1: Splitting GPCRs sequences into fragments and create wordbooks</title>
        <p id="p0055">In order to satisfy the input data shape of the three word-embedding models, the GPCRs sequences are broken into different length fragments which are considered as words in wordbooks. In this paper, we designed three kinds fragments, and the length <inline-formula><mml:math id="M6" altimg="si15.svg"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula> of them can be set as 2, 3 or 4, respectively, and the wordbooks were denoted as <inline-formula><mml:math id="M7" altimg="si16.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M8" altimg="si17.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M9" altimg="si18.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. For example, in order to obtain <inline-formula><mml:math id="M10" altimg="si16.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, the original sequences are broken into words whose lengths are 2, i.e., the window size is set 2 and the stride of moving window is 1. After all of this work, the words broken from each sequence would be collected, removed duplicate(s) and then form wordbook <inline-formula><mml:math id="M11" altimg="si16.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> of which the number of words is <inline-formula><mml:math id="M12" altimg="si19.svg"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula>. For example, the process of splitting GPCRs sequences into words of <inline-formula><mml:math id="M13" altimg="si16.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is shown in <xref rid="f0005" ref-type="fig">Fig. 1</xref>. The processes of creating <inline-formula><mml:math id="M14" altimg="si17.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M15" altimg="si18.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are similar to <inline-formula><mml:math id="M16" altimg="si16.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. In detail, the window sizes of <inline-formula><mml:math id="M17" altimg="si17.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M18" altimg="si18.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> should be set as 3 and 4, respectively. What’s more, the strides of moving window of <inline-formula><mml:math id="M19" altimg="si17.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M20" altimg="si18.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are equal to 1.<fig id="f0005"><label>Fig. 1</label><caption><p>The process of splitting GPCRs sequences and forming wordbook of <inline-formula><mml:math id="M21" altimg="si1.svg"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Q</mml:mi><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xlink:href="gr1"/></fig></p>
      </sec>
      <sec id="s0035">
        <label>2.3.2</label>
        <title>Step 2: Training CBOW models</title>
        <p id="p0060">There are a lot of methods in word embedding. Here, the Word2vec <xref rid="b0185" ref-type="bibr">[37]</xref> method is selected as the default word embedding method in this paper for the reason that Word2vec whose models are simple. Actually, a double layer neural networks have two widely used models to generate word vectors including continuous bag of words (CBOW) and Skip-gram. CBOW is applied to predict target words based on consecutive words before and after target word. Conversely, Skip-gram is applied to predict context words based on a word. In this work, CBOW model is chosen as the default model for word embedding. The structure of CBOW is shown in <xref rid="f0010" ref-type="fig">Fig. 2</xref>. The training of artificial neural network (ANN) <xref rid="b0095" ref-type="bibr">[19]</xref>, <xref rid="b0235" ref-type="bibr">[47]</xref>, <xref rid="b0240" ref-type="bibr">[48]</xref> usually includes two parts: forward propagation and back propagation. The forward propagation calculation of the proposed model is listed as follows:<list list-type="simple" id="l0010"><list-item id="o0025"><label>1).</label><p id="p0065">Encoding the GPCRs primary sequence with characters string. Since the original GPCRs sequences can not be directly fed into the CBOW model, a GPCR sequence can be represented with formula (2), where <inline-formula><mml:math id="M22" altimg="si20.svg"><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> is the length of the protein sequence.</p></list-item></list><disp-formula id="e0010"><label>(2)</label><mml:math id="M23" altimg="si21.svg"><mml:mrow><mml:mi>G</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>g</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:msub><mml:mi>g</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>⋯</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula><list list-type="simple" id="l0015"><list-item id="o0030"><label>2).</label><p id="p0070">Partitioning the sequence into word set.</p></list-item></list><disp-formula id="e0015"><label>(3)</label><mml:math id="M24" altimg="si22.svg"><mml:mrow><mml:mi>R</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mfenced open="{" close="}"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⋯</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⋯</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⋯</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mfenced open="{" close="}"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:msub><mml:mrow><mml:mo>,</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M25" altimg="si23.svg"><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:math></inline-formula> means the number of words in the set. Obviously, <inline-formula><mml:math id="M26" altimg="si23.svg"><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:math></inline-formula>equals <inline-formula><mml:math id="M27" altimg="si24.svg"><mml:mrow><mml:mi>L</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">-</mml:mo><mml:mi>l</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="M28" altimg="si15.svg"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula> is the length of fragments.<list list-type="simple" id="l0020"><list-item id="o0035"><label>3).</label><p id="p0075">Inputting the encoded words to CBOW and calculating the output of the hidden layer. Select the target word from <inline-formula><mml:math id="M29" altimg="si25.svg"><mml:mrow><mml:mi>R</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, mark it as <inline-formula><mml:math id="M30" altimg="si26.svg"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and choose its context <inline-formula><mml:math id="M31" altimg="si27.svg"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, i.e. <inline-formula><mml:math id="M32" altimg="si28.svg"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula> words from the upstream and <inline-formula><mml:math id="M33" altimg="si28.svg"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula> words from downstream of the target word in the protein sequences respectively. According to the corresponding wordbook created in <bold>Step 1,</bold> encoding the selected context words by using One-Hot and mark as <inline-formula><mml:math id="M34" altimg="si271.svg"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Then feed them as the input data into Input Layer of CBOW. Calculation process from input layer to hidden layer is shown in formula (4).</p></list-item></list><disp-formula id="e0020"><label>(4)</label><mml:math id="M35" altimg="si29.svg"><mml:mrow><mml:mi>h</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>s</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow><mml:mi>s</mml:mi></mml:munderover><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M36" altimg="si30.svg"><mml:mrow><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> is the weight matrix between Input Layer and Hidden Layer, whose shape is <inline-formula><mml:math id="M37" altimg="si31.svg"><mml:mrow><mml:mi>v</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="M38" altimg="si32.svg"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> is a hyperparameter.<inline-formula><mml:math id="M39" altimg="si33.svg"><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:math></inline-formula> is the output of Hidden Layer, whose shape is <inline-formula><mml:math id="M40" altimg="si34.svg"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>.<list list-type="simple" id="l0025"><list-item id="o0040"><label>4).</label><p id="p0085">Designing the object function based on the propagation process from Hidden Layer to Output Layer. The formula (5) was utilized to compute the score matrix for the wordbook.</p></list-item></list><disp-formula id="e0025"><label>(5)</label><mml:math id="M41" altimg="si35.svg"><mml:mrow><mml:mi>U</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:msup><mml:mrow><mml:mi>h</mml:mi><mml:mo>∗</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:msub><mml:mrow><mml:mo>,</mml:mo><mml:mi>u</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M42" altimg="si36.svg"><mml:mrow><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> is the weight matrix between Hidden Layer and Output Layer, whose shape is <inline-formula><mml:math id="M43" altimg="si37.svg"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula>. Then <inline-formula><mml:math id="M44" altimg="si38.svg"><mml:mrow><mml:mi mathvariant="italic">Softmax</mml:mi></mml:mrow></mml:math></inline-formula> function is used to obtain the probability distribution of output units.<disp-formula id="e0030"><label>(6)</label><mml:math id="M45" altimg="si39.svg"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msup><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>v</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M46" altimg="si40.svg"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the <inline-formula><mml:math id="M47" altimg="si41.svg"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula>th value of <inline-formula><mml:math id="M48" altimg="si42.svg"><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M49" altimg="si43.svg"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> means the <inline-formula><mml:math id="M50" altimg="si41.svg"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula>th word in wordbook.<fig id="f0010"><label>Fig. 2</label><caption><p>The structure of CBOW.</p></caption><graphic xlink:href="gr2"/></fig></p>
        <p id="p0090">To maximize the probability of the target word <inline-formula><mml:math id="M51" altimg="si26.svg"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, formula (7) was used at this step.<disp-formula id="e0035"><label>(7)</label><mml:math id="M52" altimg="si44.svg"><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msup><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>v</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msup></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="p0095">Take logarithm of formula (7) to get the objective function, and then maximize the objective function with formula (8).<disp-formula id="e0040"><label>(8)</label><mml:math id="M53" altimg="si45.svg"><mml:mrow><mml:mi mathvariant="italic">loss</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo linebreak="badbreak">-</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>v</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msup></mml:mrow></mml:math></disp-formula></p>
        <p id="p0100">However, the above process usually needs a lot of time to train the model. Here, we adopt a more efficient technology called Negative Sampling <xref rid="b0245" ref-type="bibr">[49]</xref> to accelerate the training process. By using Negative Sampling can convert the above optimization problem to a series of binary problems and speed up training better word vectors. The detail of Negative Sampling and back propagation algorithm please see to reference <xref rid="b0250" ref-type="bibr">[50]</xref>, <xref rid="b0255" ref-type="bibr">[51]</xref>. In this step, we choose the value of <inline-formula><mml:math id="M54" altimg="si32.svg"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> as 128, 256 and 512 on the basis of the number of words in relevant wordbook.</p>
      </sec>
      <sec id="s0040">
        <label>2.3.3</label>
        <title>Step 3: Extracting features with CBOW models</title>
        <p id="p0105">In <bold>Step 2,</bold> we got three kinds of word vector matrixes <inline-formula><mml:math id="M55" altimg="si30.svg"><mml:mrow><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> by training CBOW models. In this step, each sequence was converted into feature vector. The detailed process is as follows:<list list-type="simple" id="l0030"><list-item id="o0045"><label>1).</label><p id="p0110">Utilize formula (2) and (3) to process the GPCR sequence and get <inline-formula><mml:math id="M56" altimg="si46.svg"><mml:mrow><mml:mi>R</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mrow><mml:mfenced open="{" close="}"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:msub><mml:mrow><mml:mo>,</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item id="o0050"><label>2).</label><p id="p0115">According to the relevant wordbook, encode <inline-formula><mml:math id="M57" altimg="si25.svg"><mml:mrow><mml:mi>R</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> by using One-Hot and then we can get <inline-formula><mml:math id="M58" altimg="si47.svg"><mml:mrow><mml:mi mathvariant="italic">RO</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mrow><mml:mfenced open="[" close="]"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>⋮</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> Where <inline-formula><mml:math id="M59" altimg="si48.svg"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a <inline-formula><mml:math id="M60" altimg="si49.svg"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula> vector which represents the word <inline-formula><mml:math id="M61" altimg="si50.svg"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (see to formula (3)), <inline-formula><mml:math id="M62" altimg="si51.svg"><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:math></inline-formula>, and thus <inline-formula><mml:math id="M63" altimg="si52.svg"><mml:mrow><mml:mi mathvariant="italic">RO</mml:mi></mml:mrow></mml:math></inline-formula> is a <inline-formula><mml:math id="M64" altimg="si53.svg"><mml:mrow><mml:mi>B</mml:mi><mml:mo>×</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula> matrix.</p></list-item><list-item id="o0055"><label>3).</label><p id="p0120">Calculate the feature vector matrixes by using formula (9)</p></list-item></list><disp-formula id="e0045"><label>(9)</label><mml:math id="M65" altimg="si54.svg"><mml:mrow><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>R</mml:mi><mml:mi>O</mml:mi><mml:mo>∗</mml:mo><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M66" altimg="si55.svg"><mml:mrow><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> means matrix embedding features and is a <inline-formula><mml:math id="M67" altimg="si56.svg"><mml:mrow><mml:mi>B</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> matrix.<list list-type="simple" id="l0035"><list-item id="o0060"><label>4).</label><p id="p0125">Represent the GPCR sequence with formula (10).</p></list-item></list><disp-formula id="e0050"><label>(10)</label><mml:math id="M68" altimg="si57.svg"><mml:mrow><mml:mi>E</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>B</mml:mi></mml:mfrac><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M69" altimg="si58.svg"><mml:mrow><mml:mi mathvariant="italic">sum</mml:mi></mml:mrow></mml:math></inline-formula> means to sum the values along the first dimension of the matrix and <inline-formula><mml:math id="M70" altimg="si59.svg"><mml:mrow><mml:mi>E</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> means embedding features.</p>
        <p id="p0130">With the same method, we can obtain <inline-formula><mml:math id="M71" altimg="si60.svg"><mml:mrow><mml:mi>E</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M72" altimg="si61.svg"><mml:mrow><mml:mi>E</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M73" altimg="si62.svg"><mml:mrow><mml:mi>E</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, i.e. let <inline-formula><mml:math id="M74" altimg="si15.svg"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula> equals to 2, 3 or 4 for above formulas. And then the word embedding features (<inline-formula><mml:math id="M75" altimg="si63.svg"><mml:mrow><mml:mi mathvariant="italic">WEF</mml:mi></mml:mrow></mml:math></inline-formula>) of each GPCR sequence can be combined into an 896-D vector by using the formula (11).<disp-formula id="e0055"><label>(11)</label><mml:math id="M76" altimg="si64.svg"><mml:mrow><mml:mi mathvariant="italic">WEF</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>E</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow><mml:mo>⊗</mml:mo><mml:mi>E</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfenced></mml:mrow><mml:mo>⊗</mml:mo><mml:mi>E</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M77" altimg="si65.svg"><mml:mrow><mml:mo>⊕</mml:mo></mml:mrow></mml:math></inline-formula> means concatenating the two vectors.</p>
        <sec id="s0045">
          <label>2.3.3.1</label>
          <title>Bow</title>
          <p id="p0135">The brief steps of BOW features extraction from GPCRs are listed as follows:<list list-type="simple" id="l0040"><list-item id="o0065"><label>(1).</label><p id="p0140">Encoding GPCRs sequences by using AAindex <xref rid="b0260" ref-type="bibr">[52]</xref>which is a database containing more than 500 amino acid indices. The symbol ‘X’ existed in some sequences are represent with the mean of AAindex.</p></list-item><list-item id="o0070"><label>(2).</label><p id="p0145">Splitting GPCRs sequences into fragments with different sizes.</p></list-item><list-item id="o0075"><label>(3).</label><p id="p0150">Creating wordbooks and determining the number of words in each wordbook with weighted Silhouette Coefficient.</p></list-item><list-item id="o0080"><label>(4).</label><p id="p0155">Extracting 115-D BOW features denoted as <inline-formula><mml:math id="M78" altimg="si66.svg"><mml:mrow><mml:mi mathvariant="italic">BF</mml:mi></mml:mrow></mml:math></inline-formula> based on wordbooks.</p></list-item></list></p>
          <p id="p0160">The specific process of BOW features extraction can be found in reference <xref rid="b0230" ref-type="bibr">[46]</xref>.</p>
          <p id="p0165">Finally, the above feature vectors are concatenated into a 1011-D vector by using formula (12) denoted as <inline-formula><mml:math id="M79" altimg="si67.svg"><mml:mrow><mml:mi>F</mml:mi><mml:mi>_</mml:mi><mml:mi>G</mml:mi><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>.<disp-formula id="e0060"><label>(12)</label><mml:math id="M80" altimg="si68.svg"><mml:mrow><mml:mi>F</mml:mi><mml:mi>_</mml:mi><mml:mi>G</mml:mi><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mi>R</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>W</mml:mi><mml:mi>E</mml:mi><mml:mi>F</mml:mi><mml:mo>⊗</mml:mo><mml:mi>B</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:math></disp-formula></p>
        </sec>
      </sec>
    </sec>
    <sec id="s0050">
      <label>2.4</label>
      <title>Feature extraction by Deep learning</title>
      <p id="p0170">In this work, we built a simple DL model which contains three fully connected layers and two Batch Normalization (BN) <xref rid="b0265" ref-type="bibr">[53]</xref> layers to reduce the dimensions of the feature vector <inline-formula><mml:math id="M81" altimg="si67.svg"><mml:mrow><mml:mi>F</mml:mi><mml:mi>_</mml:mi><mml:mi>G</mml:mi><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>. In fact, the structure of DL model is very flexible. In order to simplify the problem, the number of neurons of each hidden layer is about half less than the previous layer. The detailed structure is shown in <xref rid="f0015" ref-type="fig">Fig. 3</xref>. What’s more, we choose Focal loss <xref rid="b0160" ref-type="bibr">[32]</xref> as the default loss function. Compared with other loss functions, Focal loss can handle the imbalance problem of datasets better <xref rid="b0160" ref-type="bibr">[32]</xref>.In this paper, the DL model is built by Tensorflow which is a very popular machine learning package. The hyperparameters of the DL model including epochs, batch size, loss function and optimizer are 20 and 32, binary cross-entropy and Adam <xref rid="b0270" ref-type="bibr">[54]</xref>. The activation function of hidden layers is chosen Leaky Relu <xref rid="b0275" ref-type="bibr">[55]</xref>. And the initial learning rate is 0.01. What’s more, we used Early Stopping method to decide when to stop training. The strategy of Early Stopping could monitor the training loss and would stop the training process if the training loss did not decrease in the next 3 epochs. The model would be trained with the training dataset. Then the 1011-D features vector <inline-formula><mml:math id="M82" altimg="si67.svg"><mml:mrow><mml:mi>F</mml:mi><mml:mi>_</mml:mi><mml:mi>G</mml:mi><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> would be input into the optimized model and the output of Layer 1 would be intercepted as the final features. From <xref rid="f0015" ref-type="fig">Fig. 3</xref>, we can see that the final features are 505-D obviously after using DL model to reduce dimension.<fig id="f0015"><label>Fig. 3</label><caption><p>The structure of deep learning model.</p></caption><graphic xlink:href="gr3"/></fig></p>
    </sec>
    <sec id="s0055">
      <label>2.5</label>
      <title>Algorithm selection</title>
      <sec id="s0060">
        <label>2.5.1</label>
        <title>Gradient boosting decision tree</title>
        <p id="p0175">Gradient Boosting Decision Tree (GBDT) is an effective machine learning algorithm in industry <xref rid="b0280" ref-type="bibr">[56]</xref> and scientific research <xref rid="b0230" ref-type="bibr">[46]</xref>, <xref rid="b0285" ref-type="bibr">[57]</xref>. In practical research and application, classification and Regression Trees (CART) <xref rid="b0290" ref-type="bibr">[58]</xref> is usually served as weak classifier for GBDT, and GBDT algorithm is trained through multiple iterations, each iteration produces a weak classifier which is trained on the basis of the residual of the previous round. The final total classifier (GBDT) is the weighted sum of the weak classifiers from each iteration of training.</p>
      </sec>
      <sec id="s0065">
        <label>2.5.2</label>
        <title>Random forest</title>
        <p id="p0180">Random Forest (RF) <xref rid="b0295" ref-type="bibr">[59]</xref> is a classical ensemble algorithm in machine learning. Because of its flexibility and generalization ability, this algorithm has been applied in many fields, such as bioinformatics, data mining and marketing management. The base learners of RF are usually CART. When a new sample is needed to be classified, each decision tree in the forest will be judged and classified separately. RF depends on a vote of their predictions to decide the final classification result.</p>
      </sec>
      <sec id="s0070">
        <label>2.5.3</label>
        <title>CatBoost</title>
        <p id="p0185">CatBoost <xref rid="b0300" ref-type="bibr">[60]</xref> is a kind of boosting algorithm based on symmetric Trees, which is universal and can be applied to a wide range of fields and various problems. Compared with other machine learning algorithms, the algorithm has three advantages. First of all, it can automatically handle categorical features. Further, CatBoost uses combined category features to make use of the relationship between features, which greatly enriches the feature dimension. Last but not least, the time of model training and predicting is very short.</p>
      </sec>
      <sec id="s0075">
        <label>2.5.4</label>
        <title>XGBoost</title>
        <p id="p0190">XGBoost <xref rid="b0305" ref-type="bibr">[61]</xref> is a well-known boosting algorithm in machine learning, which is mainly used to solve supervised learning problems. It can handle many tasks such as regression, classification and sorting and is widely used in machine learning competitions, and has achieved good results. XGBoost is generally regarded as an improvement of GBDT algorithm and can be more flexible and efficient <xref rid="b0310" ref-type="bibr">[62]</xref>, <xref rid="b0315" ref-type="bibr">[63]</xref>. The base learner of XGBoost is CART.</p>
      </sec>
    </sec>
  </sec>
  <sec id="s0080">
    <label>3</label>
    <title>Results</title>
    <sec id="s0085">
      <label>3.1</label>
      <title>Train the better CBOW models</title>
      <p id="p0195">From the mathematical derivation of CBOW, it is clear that getting a best CBOW model through training is to maximize the objective function. Here, a large value of iterations was set for the training of CBOW models and the best iterations were chosen by observing the change of objective function in the training process. In this paper, the all CBOW models were built and trained by using the software package of Python called Gensim. The code of training CBOW models is shown at https://github.com/454170054/EMCBOW-GPCR/blob/main/code/GetCBO</p>
      <p id="p0200">WFeatures.py, which contains the values of hyperparameters. Increment curves of objective functions of the three built models are shown in <xref rid="f0020" ref-type="fig">Fig. 4</xref>. From the picture, we can see when the objective functions of the models start to converge. From the left one in the picture, it is clear that when iterations of <inline-formula><mml:math id="M83" altimg="si16.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is bigger than 74, the increment of objective function is close to 0. Therefore, we choose the result after the 74th training as the default CBOW model of <inline-formula><mml:math id="M84" altimg="si16.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Similarly, the default CBOW models of <inline-formula><mml:math id="M85" altimg="si17.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M86" altimg="si17.svg"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are the result after the 100th training and 280th training, respectively.<fig id="f0020"><label>Fig. 4</label><caption><p>Increment curves of objective functions of training CBOW.</p></caption><graphic xlink:href="gr4"/></fig></p>
    </sec>
    <sec id="s0090">
      <label>3.2</label>
      <title>Effect of different feature representations of GPCR</title>
      <p id="p0205">In this work, we used two feature extraction methods, i.e., BOW features and Word Embedding features, to represent GPCRs sequences. In this section, the performances are compared based on mentioned-above single kind of features and combined features which are generated by concatenating BOW features and Word Embedding features by 5-fold cross-validation with 20 times on benchmark dataset. In this section, CatBoost algorithm are selected as the default algorithm to build classifiers. The experimental results about AUC values of different features are shown in <xref rid="f0025" ref-type="fig">Fig. 5</xref>. From the figure, we can see that the combined features whose AUC is 0.9423 which is better than CBOW features (AUC = 0.9391) and BOW features (AUC = 0.9344). According to the results, the combined features are selected as the default features to represent GPCRs sequences.<fig id="f0025"><label>Fig. 5</label><caption><p>AUC values of CatBoost algorithm combined with different features.</p></caption><graphic xlink:href="gr5"/></fig></p>
    </sec>
    <sec id="s0095">
      <label>3.3</label>
      <title>Effect of different features generated by Deep-learning model</title>
      <p id="p0210">In this paper, a simple fully connected DL model was built with one input layer, three hidden layers, two BN layers and one output layer. Because of the model having three hidden layers and two BN layers, there are 5 kinds of features generated by the DL model. In this section, we evaluate the effectiveness of above features by 5-fold cross-validation with 20 times on benchmark dataset and the results are shown in <xref rid="f0030" ref-type="fig">Fig. 6</xref>. The details about the DL model are exhibited at <ext-link ext-link-type="uri" xlink:href="https://github.com/454170054/EMCBOW-GPCR/blob/main/code/GetResults.py" id="ir025">https://github.com/454170054/EMCBOW-GPCR/blob/main/code/GetResults.py</ext-link>. CatBoost is chosen as the default algorithm to build predictive model to get AUC values in this section. From the figure, we can find that the AUC value of original features is the smallest. This result can demonstrate the fact that the processed features by DL have better performance. What’s more, the features generated by Layer 1 of DL model achieved the biggest value of AUC so that it was chosen as the default features.<fig id="f0030"><label>Fig. 6</label><caption><p>AUC values of CatBoost algorithm combined with features generated by different layers.</p></caption><graphic xlink:href="gr6"/></fig></p>
    </sec>
    <sec id="s0100">
      <label>3.4</label>
      <title>Determination of the optimal algorithm</title>
      <p id="p0215">Up to now, there are many effective algorithms developed in the field of machine learning. Ensemble learning often performs better than the best single algorithm since it would construct a certain number of weak classifiers and then classifies a new sample by taking a (weighted) vote of their predictions. The all learning algorithms mentioned in section <bold>3</bold> belong to ensemble learning, in which the random forest is part of Bagging method and the rest is part of Boosting method <xref rid="b0320" ref-type="bibr">[64]</xref>. In this section, the above four algorithms are tested on the benchmark dataset by 5-fold cross-validation with 20 times and the performance of them is listed in <xref rid="t0005" ref-type="table">Table 1</xref> and the input features used there is the features generated by Layer 1 of the DL model. As shown in the table, the performance of DL model is worse than other algorithms which are using the features generated by DL model. What’s more, The RF algorithm has the largest Sp but having the smallest Sn and CatBoost algorithm has the best Sn. Furthermore, the Acc, Mcc and AUC value of XGBoost is the highest. Therefore, XGBoost algorithm is selected as the final algorithm to build classifier in this work. The hyperparameters used for the classifiers are included in <xref rid="t0010" ref-type="table">Table 2</xref> and the software packages can be found in <ext-link ext-link-type="uri" xlink:href="https://github.com/454170054/EMCBOW-GPCR/blob/main/requirements.txt" id="ir030">https://github.com/454170054/EMCBOW-GPCR/blob/main/requirements.txt</ext-link>.<table-wrap position="float" id="t0005"><label>Table 1</label><caption><p>Performance of different algorithms.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Algorithm</th><th>Acc</th><th>Sp</th><th>Sn</th><th>MCC</th><th>AUC</th></tr></thead><tbody><tr><td>CatBoost</td><td>92.87<inline-formula><mml:math id="M87" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.14</td><td>97.06<inline-formula><mml:math id="M88" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.13</td><td><bold>75.43</bold><inline-formula><mml:math id="M89" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula><bold>0.54</bold></td><td>76.29<inline-formula><mml:math id="M90" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.48</td><td>95.25<inline-formula><mml:math id="M91" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.14</td></tr><tr><td>RF</td><td>92.85<inline-formula><mml:math id="M92" altimg="si3.svg"><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.10</mml:mn></mml:mrow></mml:math></inline-formula></td><td><bold>98.40</bold><inline-formula><mml:math id="M93" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula><bold>0.07</bold></td><td>69.78<inline-formula><mml:math id="M94" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.44</td><td>75.85<inline-formula><mml:math id="M95" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.38</td><td>95.38<inline-formula><mml:math id="M96" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.09</td></tr><tr><td>GBDT</td><td>92.61<inline-formula><mml:math id="M97" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.13</td><td>96.77<inline-formula><mml:math id="M98" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.12</td><td>75.27<inline-formula><mml:math id="M99" altimg="si4.svg"><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.52</mml:mn></mml:mrow></mml:math></inline-formula></td><td>75.48<inline-formula><mml:math id="M100" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.58</td><td>94.82<inline-formula><mml:math id="M101" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.13</td></tr><tr><td>XGBoost</td><td><bold>92.91</bold><inline-formula><mml:math id="M102" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula><bold>0.10</bold></td><td>97.34<inline-formula><mml:math id="M103" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.11</td><td>74.45<inline-formula><mml:math id="M104" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.41</td><td><bold>76.32</bold><inline-formula><mml:math id="M105" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula><bold>0.34</bold></td><td><bold>95.56</bold><inline-formula><mml:math id="M106" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula><bold>0.13</bold></td></tr><tr><td>DL</td><td>92.23<inline-formula><mml:math id="M107" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.30</td><td>97.56<inline-formula><mml:math id="M108" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.47</td><td>70.03<inline-formula><mml:math id="M109" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>2.65</td><td>73.77<inline-formula><mml:math id="M110" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>1.14</td><td>93.80<inline-formula><mml:math id="M111" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.35</td></tr></tbody></table><table-wrap-foot><fn><p><italic>Notice</italic>: digits are mean<inline-formula><mml:math id="M112" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula> std and the bold means the best values.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="t0010"><label>Table 2</label><caption><p>The hyperparameters used for the classifiers.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Algorithm</th><th>n_estimators</th><th>Learning rate</th><th>Max depth</th></tr></thead><tbody><tr><td>CatBoost</td><td>190</td><td align="char">0.4</td><td>None</td></tr><tr><td>RF</td><td>110</td><td/><td>None</td></tr><tr><td>GBDT</td><td>100</td><td align="char">0.1</td><td>3</td></tr><tr><td>XGBoost</td><td>110</td><td align="char">0.12</td><td>3</td></tr></tbody></table><table-wrap-foot><fn><p>Notice: slash means algorithm do not have the hyperparameter.</p></fn></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="s0105">
      <label>3.5</label>
      <title>Comparison of other methods</title>
      <p id="p0220">In this work, the benchmark dataset is same as the one used in <xref rid="b0025" ref-type="bibr">[5]</xref>, <xref rid="b0070" ref-type="bibr">[14]</xref>. In order to prove the effectiveness of our method, the performance is compared between the proposed method and the other state-of-the-art method. Because the data segmentation methods in the literature are different, the same segmentation strategy was test here. In the literature <xref rid="b0070" ref-type="bibr">[14]</xref>, the state-of-the-art method was tested on the benchmark dataset by 5-fold cross-validation. Further, SMOTE algorithm was used to balance the training dataset and change the positive samples from 100 percent into 300 percent. We also take the strategy to handle the benchmark dataset and test our proposed method. The results are shown in <xref rid="t0015" ref-type="table">Table 3</xref>. It is clear that all the metrics of the proposed method are better than those of Liao. In the literature <xref rid="b0025" ref-type="bibr">[5]</xref>, the negative samples in the benchmark dataset were randomly divided into 4 groups and 2495 sequences were extracted from these 4 groups, respectively. Then, the final result is an average of the four experiment by using the four negative experiments. According to the strategy, we test our proposed method and the result is shown in <xref rid="t0020" ref-type="table">Table 4</xref>. From the table, we can find that the AUC, Acc and Precision value of our method are higher than the other. The results of comparing with other methods show that the proposed method is a good method for identifying GPCRs.<table-wrap position="float" id="t0015"><label>Table 3</label><caption><p>The results of the proposed method and Liao <xref rid="b0070" ref-type="bibr">[14]</xref>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>Acc</th><th>Sp</th><th>Sn</th><th>MCC</th><th>AUC</th></tr></thead><tbody><tr><td>Proposed method</td><td><bold>92.91</bold><inline-formula><mml:math id="M113" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula><bold>0.10</bold></td><td><bold>97.34</bold><inline-formula><mml:math id="M114" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula><bold>0.11</bold></td><td><bold>74.45</bold><inline-formula><mml:math id="M115" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula><bold>0.41</bold></td><td><bold>76.32</bold><inline-formula><mml:math id="M116" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula><bold>0.34</bold></td><td><bold>95.56</bold><inline-formula><mml:math id="M117" altimg="si2.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula><bold>0.13</bold></td></tr><tr><td>Liao</td><td>83.33<inline-formula><mml:math id="M118" altimg="si5.svg"><mml:mrow><mml:mo>±</mml:mo><mml:mn>7.26</mml:mn></mml:mrow></mml:math></inline-formula></td><td>97.24<inline-formula><mml:math id="M119" altimg="si6.svg"><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.87</mml:mn></mml:mrow></mml:math></inline-formula></td><td>69.42<inline-formula><mml:math id="M120" altimg="si7.svg"><mml:mrow><mml:mo>±</mml:mo><mml:mn>14.91</mml:mn></mml:mrow></mml:math></inline-formula></td><td>69.24<inline-formula><mml:math id="M121" altimg="si8.svg"><mml:mrow><mml:mo>±</mml:mo><mml:mn>12.96</mml:mn></mml:mrow></mml:math></inline-formula></td><td>92.80<inline-formula><mml:math id="M122" altimg="si9.svg"><mml:mrow><mml:mo>±</mml:mo><mml:mn>3.8</mml:mn></mml:mrow></mml:math></inline-formula></td></tr></tbody></table></table-wrap><table-wrap position="float" id="t0020"><label>Table 4</label><caption><p>The results of the proposed method and literature <xref rid="b0025" ref-type="bibr">[5]</xref>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>AUC</th><th>Acc</th><th>Precision</th></tr></thead><tbody><tr><td>Proposed method</td><td align="char"><bold>95.29</bold></td><td align="char"><bold>88.11</bold></td><td align="char"><bold>89.28</bold></td></tr><tr><td>400D + PC-PseAAC</td><td align="char">94.13</td><td align="char">86.28</td><td align="char">86.62</td></tr></tbody></table></table-wrap></p>
    </sec>
  </sec>
  <sec id="s0110">
    <label>4</label>
    <title>Conclusion</title>
    <p id="p0225">In this work, CatBoost was chosen as the initial algorithm to build classifier for the reason that the time of training CatBoost is much shorter than other mentioned algorithms. Firstly, we selected the best CBOW models by optimizing the objective function. Secondly, BOW and CBOW models were employed to extract features separately and the metrics listed in formula (1) were used to evaluate the effectiveness of different feature representations for GPCRs. According to the experiment results, the combined features by concatenating BOW and Word Embedding features were better than any single features. Therefore, we determined to choose the combined features as the default features to represent GPCRs sequences. Then, a simple DL model was built and CatBoost was employed to fit the processed features generated by different layers of DL model and generate corresponding classifier. According to the performance of different classifiers, the outputs of Layer 1 were chosen as the final features. Further, XGBoost was selected as the default algorithm because of its best performance compared with other algorithms. Finally, according to results compared with other state-of-the-art methods, the proposed method called EMCBOW-GPCR got a better performance in the problem of identifying GPCRs.</p>
  </sec>
  <sec id="s0115">
    <label>5</label>
    <title>Discussion</title>
    <p id="p0230">G protein coupled receptors (GPCRs) family is one of the largest membrane protein family in human beings, and is also an important target of many drugs. In this work, a novel method for identifying GPCRs was developed. In terms of representation GPCRs, we used two extraction methods which are BOW and Word Embedding to extract features from GPCRs sequences. Then we concatenated the above two kinds of features as the input of DL which can automatically extract better features by learning from the input features. Further, we intercepted the output of Layer 1 as the input features of XGBoost which is a fairly powerful, flexible and efficient algorithm. According to the results of compared with other methods, the proposed method called EMCBOW-GPCR has a better performance for identifying GPCRs. By the way, the hyperparameter <inline-formula><mml:math id="M123" altimg="si32.svg"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> which is used in Word Embedding is flexible and hard to determine the best value. What’s more, it may be influenced the quality of features significantly. At the same time, the DL model is also very flexible and difficult to find the best structure which contains how many layers and units in each layer. Finally, the framework of EMCBOW-GPCR can be concluded as <xref rid="f0035" ref-type="fig">Fig. 7</xref>.<fig id="f0035"><label>Fig. 7</label><caption><p>The framework of proposed method EMCBOW-GPCR.</p></caption><graphic xlink:href="gr7"/></fig></p>
  </sec>
  <sec id="s0120">
    <title>CRediT authorship contribution statement</title>
    <p id="p0235"><bold>Wangren Qiu:</bold> Conceptualization, Methodology, Software, Data curation, Writing – original draft, Software, Validation, Writing - review &amp; editing. <bold>Zhe Lv:</bold> Visualization, Investigation, Software, Validation. <bold>Xuan Xiao:</bold> Conceptualization, Methodology, Software, Supervision, Writing - review &amp; editing. <bold>Shuai Shao:</bold> Visualization, Investigation. <bold>Hao Lin:</bold> Conceptualization, Methodology, Software, Supervision, Writing - review &amp; editing.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Declaration of Competing Interest</title>
    <p id="p0240">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
  </sec>
</body>
<back>
  <ref-list id="bi005">
    <title>References</title>
    <ref id="b0005">
      <label>1</label>
      <mixed-citation publication-type="other" id="h0005">Lagerstrm MC, Schith HB. Lagerstrom, M. C. &amp; Schioth, H. B. Structural diversity of G protein-coupled receptors and significance for drug discovery. Nature Rev. Drug Discov. 7, 339-357. Nature Reviews Drug Discovery 2008;7:339–57.</mixed-citation>
    </ref>
    <ref id="b0010">
      <label>2</label>
      <element-citation publication-type="journal" id="h0010">
        <person-group person-group-type="author">
          <name>
            <surname>Jacoby</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Bouhelal</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Gerspacher</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Seuwen</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>The 7 TM G-protein-coupled receptor target family</article-title>
        <source>ChemMedChem</source>
        <volume>1</volume>
        <year>2006</year>
        <fpage>760</fpage>
        <lpage>782</lpage>
        <pub-id pub-id-type="doi">10.1002/cmdc.200600134</pub-id>
      </element-citation>
    </ref>
    <ref id="b0015">
      <label>3</label>
      <element-citation publication-type="journal" id="h0015">
        <person-group person-group-type="author">
          <name>
            <surname>Fredriksson</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Lagerström</surname>
            <given-names>M.C.</given-names>
          </name>
          <name>
            <surname>Lundin</surname>
            <given-names>L.-G.</given-names>
          </name>
          <name>
            <surname>Schiöth</surname>
            <given-names>H.B.</given-names>
          </name>
        </person-group>
        <article-title>The G-protein-coupled receptors in the human genome form five main families. Phylogenetic analysis, paralogon groups, and fingerprints</article-title>
        <source>Mol Pharmacol</source>
        <volume>63</volume>
        <issue>6</issue>
        <year>2003</year>
        <fpage>1256</fpage>
        <lpage>1272</lpage>
        <pub-id pub-id-type="doi">10.1124/mol.63.6.1256</pub-id>
        <pub-id pub-id-type="pmid">12761335</pub-id>
      </element-citation>
    </ref>
    <ref id="b0020">
      <label>4</label>
      <element-citation publication-type="journal" id="h0020">
        <person-group person-group-type="author">
          <name>
            <surname>Ramesh</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Soliman</surname>
            <given-names>M.E.</given-names>
          </name>
        </person-group>
        <article-title>G-protein coupled receptors (GPCRs): a comprehensive computational perspective</article-title>
        <source>Comb Chem High Throughput Screen</source>
        <volume>18</volume>
        <year>2015</year>
        <fpage>346</fpage>
        <lpage>364</lpage>
        <pub-id pub-id-type="doi">10.2174/1386207318666150305155545</pub-id>
        <pub-id pub-id-type="pmid">25747435</pub-id>
      </element-citation>
    </ref>
    <ref id="b0025">
      <label>5</label>
      <mixed-citation publication-type="other" id="h0025">Ao C, Gao L, Yu L. Identifying G-protein Coupled Receptors Using Mixed-Feature Extraction Methods and Machine Learning Methods. IEEE Access 2020;PP:1.</mixed-citation>
    </ref>
    <ref id="b0030">
      <label>6</label>
      <element-citation publication-type="journal" id="h0030">
        <person-group person-group-type="author">
          <name>
            <surname>Eo</surname>
            <given-names>H.-S.</given-names>
          </name>
          <name>
            <surname>Choi</surname>
            <given-names>J.P.</given-names>
          </name>
          <name>
            <surname>Noh</surname>
            <given-names>S.-J.</given-names>
          </name>
          <name>
            <surname>Hur</surname>
            <given-names>C.-G.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <article-title>A combined approach for the classification of G protein-coupled receptors and its application to detect GPCR splice variants</article-title>
        <source>Comput Biol Chem</source>
        <volume>31</volume>
        <issue>4</issue>
        <year>2007</year>
        <fpage>246</fpage>
        <lpage>256</lpage>
        <pub-id pub-id-type="pmid">17631418</pub-id>
      </element-citation>
    </ref>
    <ref id="b0035">
      <label>7</label>
      <element-citation publication-type="journal" id="h0035">
        <person-group person-group-type="author">
          <name>
            <surname>Baldwin</surname>
            <given-names>J.M.</given-names>
          </name>
        </person-group>
        <article-title>Structure and function of receptors coupled to G proteins</article-title>
        <source>Curr Opin Cell Biol</source>
        <volume>6</volume>
        <issue>2</issue>
        <year>1994</year>
        <fpage>180</fpage>
        <lpage>190</lpage>
        <pub-id pub-id-type="pmid">8024808</pub-id>
      </element-citation>
    </ref>
    <ref id="b0040">
      <label>8</label>
      <element-citation publication-type="journal" id="h0040">
        <person-group person-group-type="author">
          <name>
            <surname>Chou</surname>
            <given-names>K.-C.</given-names>
          </name>
          <name>
            <surname>Elrod</surname>
            <given-names>D.W.</given-names>
          </name>
        </person-group>
        <article-title>Bioinformatical analysis of G-protein-coupled receptors</article-title>
        <source>J Proteome Res</source>
        <volume>1</volume>
        <issue>5</issue>
        <year>2002</year>
        <fpage>429</fpage>
        <lpage>433</lpage>
        <pub-id pub-id-type="pmid">12645914</pub-id>
      </element-citation>
    </ref>
    <ref id="b0045">
      <label>9</label>
      <element-citation publication-type="journal" id="h0045">
        <person-group person-group-type="author">
          <name>
            <surname>Katritch</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Cherezov</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Stevens</surname>
            <given-names>R.C.</given-names>
          </name>
        </person-group>
        <article-title>Structure-function of the g protein-coupled receptor superfamily</article-title>
        <source>Annu Rev Pharmacol Toxicol</source>
        <volume>53</volume>
        <issue>1</issue>
        <year>2013</year>
        <fpage>531</fpage>
        <lpage>556</lpage>
        <pub-id pub-id-type="pmid">23140243</pub-id>
      </element-citation>
    </ref>
    <ref id="b0050">
      <label>10</label>
      <element-citation publication-type="journal" id="h0050">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>R.u.</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>Tools for GPCR drug discovery</article-title>
        <source>Acta Pharmacol Sin</source>
        <volume>33</volume>
        <issue>3</issue>
        <year>2012</year>
        <fpage>372</fpage>
        <lpage>384</lpage>
        <pub-id pub-id-type="doi">10.1038/aps.2011.173</pub-id>
        <pub-id pub-id-type="pmid">22266728</pub-id>
      </element-citation>
    </ref>
    <ref id="b0055">
      <label>11</label>
      <element-citation publication-type="journal" id="h0055">
        <person-group person-group-type="author">
          <name>
            <surname>Alexander</surname>
            <given-names>S.P.</given-names>
          </name>
          <name>
            <surname>Mathie</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Peters</surname>
            <given-names>J.A.</given-names>
          </name>
        </person-group>
        <article-title>Guide to receptors and channels (GRAC)</article-title>
        <source>Br J Pharmacol</source>
        <volume>164</volume>
        <issue>Suppl 1</issue>
        <year>2011</year>
        <fpage>S1</fpage>
        <lpage>324</lpage>
        <pub-id pub-id-type="doi">10.1111/j.1476-5381.2011.01649_1.x</pub-id>
        <pub-id pub-id-type="pmid">22040146</pub-id>
      </element-citation>
    </ref>
    <ref id="b0060">
      <label>12</label>
      <element-citation publication-type="journal" id="h0060">
        <person-group person-group-type="author">
          <name>
            <surname>Zia Ur</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Khan</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Identifying GPCRs and their types with Chou’s pseudo amino acid composition: an approach from multi-scale energy representation and position specific scoring matrix</article-title>
        <source>Protein Pept Lett</source>
        <volume>19</volume>
        <year>2012</year>
        <fpage>890</fpage>
        <lpage>903</lpage>
        <pub-id pub-id-type="doi">10.2174/092986612801619589</pub-id>
        <pub-id pub-id-type="pmid">22316312</pub-id>
      </element-citation>
    </ref>
    <ref id="b0065">
      <label>13</label>
      <element-citation publication-type="journal" id="h0065">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Ling</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>Q.i.</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Classification of G-protein coupled receptors based on a rich generation of convolutional neural network, N-gram transformation and multiple sequence alignments</article-title>
        <source>Amino Acids</source>
        <volume>50</volume>
        <issue>2</issue>
        <year>2018</year>
        <fpage>255</fpage>
        <lpage>266</lpage>
        <pub-id pub-id-type="doi">10.1007/s00726-017-2512-4</pub-id>
        <pub-id pub-id-type="pmid">29151135</pub-id>
      </element-citation>
    </ref>
    <ref id="b0070">
      <label>14</label>
      <element-citation publication-type="journal" id="h0070">
        <person-group person-group-type="author">
          <name>
            <surname>Liao</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Ju</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zou</surname>
            <given-names>Q.</given-names>
          </name>
        </person-group>
        <article-title>Prediction of G protein-coupled receptors with SVM-prot features and random forest</article-title>
        <source>Scientifica</source>
        <volume>2016</volume>
        <year>2016</year>
        <fpage>1</fpage>
        <lpage>10</lpage>
      </element-citation>
    </ref>
    <ref id="b0075">
      <label>15</label>
      <element-citation publication-type="journal" id="h0075">
        <person-group person-group-type="author">
          <name>
            <surname>Peng</surname>
            <given-names>Z.-L.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>J.-Y.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>An improved classification of G-protein-coupled receptors using sequence-derived features</article-title>
        <source>BMC Bioinf</source>
        <volume>11</volume>
        <issue>1</issue>
        <year>2010</year>
        <fpage>420</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-11-420</pub-id>
      </element-citation>
    </ref>
    <ref id="b0080">
      <label>16</label>
      <element-citation publication-type="journal" id="h0080">
        <person-group person-group-type="author">
          <name>
            <surname>Naveed</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Khan</surname>
            <given-names>A.U.</given-names>
          </name>
        </person-group>
        <article-title>GPCR-MPredictor: multi-level prediction of G protein-coupled receptors using genetic ensemble</article-title>
        <source>Amino Acids</source>
        <volume>42</volume>
        <year>2012</year>
        <fpage>1825</fpage>
      </element-citation>
    </ref>
    <ref id="b0085">
      <label>17</label>
      <element-citation publication-type="book" id="h0085">
        <person-group person-group-type="author">
          <name>
            <surname>Dongardive</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Abraham</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Behera</surname>
            <given-names>H.S.</given-names>
          </name>
          <name>
            <surname>Mohapatra</surname>
            <given-names>D.P.</given-names>
          </name>
        </person-group>
        <part-title>Protein sequence classification based on N-gram and K-nearest neighbor algorithm</part-title>
        <year>2016</year>
        <publisher-name>Springer India</publisher-name>
        <publisher-loc>New Delhi</publisher-loc>
        <fpage>163</fpage>
        <lpage>171</lpage>
      </element-citation>
    </ref>
    <ref id="b0090">
      <label>18</label>
      <element-citation publication-type="journal" id="h0090">
        <person-group person-group-type="author">
          <name>
            <surname>Nie</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>A novel fractal approach for predicting G-protein-coupled receptors and their subfamilies with support vector machines</article-title>
        <source>Biomed Mater Eng</source>
        <volume>26</volume>
        <issue>s1</issue>
        <year>2015</year>
        <fpage>S1829</fpage>
        <lpage>S1836</lpage>
        <pub-id pub-id-type="doi">10.3233/BME-151485</pub-id>
        <pub-id pub-id-type="pmid">26405954</pub-id>
      </element-citation>
    </ref>
    <ref id="b0095">
      <label>19</label>
      <mixed-citation publication-type="other" id="h0095">Li M, Ling C, Gao J. An efficient CNN-based classification on G-protein Coupled Receptors using TF-IDF and N-gram. 2017. doi: 10.1109/ISCC.2017.8024644.</mixed-citation>
    </ref>
    <ref id="b0100">
      <label>20</label>
      <mixed-citation publication-type="other" id="h0100">Wang X, Yu B, Ma A, Chen C, Liu B, Ma Q. Protein-protein interaction sites prediction by ensemble random forests with synthetic minority oversampling technique. Bioinformatics 2019;35:2395–402. Doi: 10.1093/bioinformatics/bty995.</mixed-citation>
    </ref>
    <ref id="b0105">
      <label>21</label>
      <element-citation publication-type="journal" id="h0105">
        <person-group person-group-type="author">
          <name>
            <surname>Blagus</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Lusa</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>SMOTE for high-dimensional class-imbalanced data</article-title>
        <source>BMC Bioinf</source>
        <volume>14</volume>
        <year>2013</year>
        <fpage>1</fpage>
        <lpage>16</lpage>
      </element-citation>
    </ref>
    <ref id="b0110">
      <label>22</label>
      <element-citation publication-type="journal" id="h0110">
        <person-group person-group-type="author">
          <name>
            <surname>Chawla</surname>
            <given-names>N.V.</given-names>
          </name>
          <name>
            <surname>Bowyer</surname>
            <given-names>K.W.</given-names>
          </name>
          <name>
            <surname>Hall</surname>
            <given-names>L.O.</given-names>
          </name>
          <name>
            <surname>Kegelmeyer</surname>
            <given-names>W.P.</given-names>
          </name>
        </person-group>
        <article-title>Synthetic minority over-sampling technique</article-title>
        <source>J Artific Intell Res</source>
        <volume>16</volume>
        <year>2002</year>
        <fpage>321</fpage>
        <lpage>357</lpage>
      </element-citation>
    </ref>
    <ref id="b0115">
      <label>23</label>
      <element-citation publication-type="journal" id="h0115">
        <person-group person-group-type="author">
          <name>
            <surname>Zou</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>A novel features ranking metric with application to scalable visual and bioinformatics data classification</article-title>
        <source>Neurocomputing</source>
        <volume>173</volume>
        <year>2016</year>
        <fpage>346</fpage>
        <lpage>354</lpage>
      </element-citation>
    </ref>
    <ref id="b0120">
      <label>24</label>
      <element-citation publication-type="journal" id="h0120">
        <person-group person-group-type="author">
          <name>
            <surname>Schmidhuber</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Deep learning in neural networks: an overview</article-title>
        <source>Neural Netw</source>
        <volume>61</volume>
        <year>2015</year>
        <fpage>85</fpage>
        <lpage>117</lpage>
        <pub-id pub-id-type="pmid">25462637</pub-id>
      </element-citation>
    </ref>
    <ref id="b0125">
      <label>25</label>
      <element-citation publication-type="journal" id="h0125">
        <person-group person-group-type="author">
          <name>
            <surname>Hinton</surname>
            <given-names>G.E.</given-names>
          </name>
        </person-group>
        <article-title>Learning multiple layers of representation</article-title>
        <source>Trends Cogn Sci</source>
        <volume>11</volume>
        <issue>10</issue>
        <year>2007</year>
        <fpage>428</fpage>
        <lpage>434</lpage>
        <pub-id pub-id-type="pmid">17921042</pub-id>
      </element-citation>
    </ref>
    <ref id="b0130">
      <label>26</label>
      <element-citation publication-type="journal" id="h0130">
        <person-group person-group-type="author">
          <name>
            <surname>Hao</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Deep learning</article-title>
        <source>Int J Seman Comput</source>
        <volume>10</volume>
        <issue>03</issue>
        <year>2016</year>
        <fpage>417</fpage>
        <lpage>439</lpage>
      </element-citation>
    </ref>
    <ref id="b0135">
      <label>27</label>
      <mixed-citation publication-type="other" id="h0135">Lv H, Dao F-Y, Guan Z-X, Yang H, Li Y-W, Lin H. Deep-Kcr: accurate detection of lysine crotonylation sites using deep learning method. Brief Bioinform 2020. 10.1093/bib/bbaa255.</mixed-citation>
    </ref>
    <ref id="b0140">
      <label>28</label>
      <mixed-citation publication-type="other" id="h0140">Wang D, Zhang Z, Jiang Y, Mao Z, Wang D, Lin H, et al. DM3Loc: multi-label mRNA subcellular localization prediction and analysis based on multi-head self-attention mechanism. Nucleic Acids Res 2021. 10.1093/nar/gkab016.</mixed-citation>
    </ref>
    <ref id="b0145">
      <label>29</label>
      <mixed-citation publication-type="other" id="h0145">Duolin, Wang, Yanchun, Liang, Dong. Capsule network for protein post-translational modification site prediction. Bioinformatics 2019;35:2386–94.</mixed-citation>
    </ref>
    <ref id="b0150">
      <label>30</label>
      <element-citation publication-type="journal" id="h0150">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Pan</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>Deep learning in bioinformatics: Introduction, application, and perspective in the big data era</article-title>
        <source>Methods</source>
        <volume>166</volume>
        <year>2019</year>
        <fpage>4</fpage>
        <lpage>21</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ymeth.2019.04.008</pub-id>
        <pub-id pub-id-type="pmid">31022451</pub-id>
      </element-citation>
    </ref>
    <ref id="b0155">
      <label>31</label>
      <element-citation publication-type="journal" id="h0155">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Deep residual learning for image recognition</article-title>
        <source>IEEE Conf Comput Vis Pattern Recogn (CVPR)</source>
        <volume>2016</volume>
        <year>2016</year>
        <fpage>770</fpage>
        <lpage>778</lpage>
        <pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id>
      </element-citation>
    </ref>
    <ref id="b0160">
      <label>32</label>
      <mixed-citation publication-type="other" id="h0160">Lin TY, Goyal P, Girshick R, He K, Dollár P. Focal Loss for Dense Object Detection. IEEE Transactions on Pattern Analysis &amp; Machine Intelligence 2017;PP:2999–3007.</mixed-citation>
    </ref>
    <ref id="b0165">
      <label>33</label>
      <mixed-citation publication-type="other" id="h0165">Lin YO, Lei H, Li XY, Wu J. Deep Learning in NLP: Methods and Applications. Dianzi Keji Daxue Xuebao/Journal of the University of Electronic Science and Technology of China 2017;46:913–9.</mixed-citation>
    </ref>
    <ref id="b0170">
      <label>34</label>
      <element-citation publication-type="journal" id="h0170">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Ye</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Xing</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Cambria</surname>
            <given-names>E.</given-names>
          </name>
        </person-group>
        <article-title>Ensemble application of convolutional and recurrent neural networks for multi-label text categorization</article-title>
        <source>Int Joint Conf Neural Netw (IJCNN)</source>
        <volume>2017</volume>
        <year>2017</year>
        <fpage>2377</fpage>
        <lpage>2383</lpage>
        <pub-id pub-id-type="doi">10.1109/IJCNN.2017.7966144</pub-id>
      </element-citation>
    </ref>
    <ref id="b0175">
      <label>35</label>
      <element-citation publication-type="journal" id="h0175">
        <person-group person-group-type="author">
          <name>
            <surname>Uçar</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Demir</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Güzeliş</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>Object recognition and detection with deep learning for autonomous driving applications</article-title>
        <source>Simulation</source>
        <volume>93</volume>
        <issue>9</issue>
        <year>2017</year>
        <fpage>759</fpage>
        <lpage>769</lpage>
        <pub-id pub-id-type="doi">10.1177/0037549717709932</pub-id>
      </element-citation>
    </ref>
    <ref id="b0180">
      <label>36</label>
      <element-citation publication-type="journal" id="h0180">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Seff</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Kornhauser</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>DeepDriving: learning affordance for direct perception in autonomous driving</article-title>
        <source>IEEE Int Conf Comput Vision (ICCV)</source>
        <volume>2015</volume>
        <year>2015</year>
        <fpage>2722</fpage>
        <lpage>2730</lpage>
        <pub-id pub-id-type="doi">10.1109/ICCV.2015.312</pub-id>
      </element-citation>
    </ref>
    <ref id="b0185">
      <label>37</label>
      <mixed-citation publication-type="other" id="h0185">Mikolov T, Corrado G, Kai C, Dean J. Efficient Estimation of Word Representations in Vector Space. Proceedings of the International Conference on Learning Representations (ICLR 2013), 2013.</mixed-citation>
    </ref>
    <ref id="b0190">
      <label>38</label>
      <element-citation publication-type="journal" id="h0190">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>K.K.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Bedbrook</surname>
            <given-names>C.N.</given-names>
          </name>
          <name>
            <surname>Arnold</surname>
            <given-names>F.H.</given-names>
          </name>
        </person-group>
        <article-title>Learned protein embeddings for machine learning</article-title>
        <source>Bioinformatics</source>
        <volume>34</volume>
        <year>2018</year>
        <fpage>4138</fpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty455</pub-id>
        <pub-id pub-id-type="pmid">29933431</pub-id>
      </element-citation>
    </ref>
    <ref id="b0195">
      <label>39</label>
      <element-citation publication-type="journal" id="h0195">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Qiu</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>Identifying GPCR-drug interaction based on wordbook learning from sequences</article-title>
        <source>BMC Bioinf</source>
        <volume>21</volume>
        <year>2020</year>
        <fpage>150</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-020-3488-8</pub-id>
      </element-citation>
    </ref>
    <ref id="b0200">
      <label>40</label>
      <element-citation publication-type="journal" id="h0200">
        <person-group person-group-type="author">
          <name>
            <surname>Boutet</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Lieberherr</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Tognolli</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Schneider</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Bansal</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Bridge</surname>
            <given-names>A.J.</given-names>
          </name>
        </person-group>
        <article-title>UniProtKB/Swiss-prot, the manually annotated section of the UniProt KnowledgeBase: how to use The entry view</article-title>
        <source>Methods Mol Biol</source>
        <volume>1374</volume>
        <year>2016</year>
        <fpage>23</fpage>
        <lpage>54</lpage>
        <pub-id pub-id-type="doi">10.1007/978-1-4939-3167-5_2</pub-id>
        <pub-id pub-id-type="pmid">26519399</pub-id>
      </element-citation>
    </ref>
    <ref id="b0205">
      <label>41</label>
      <element-citation publication-type="journal" id="h0205">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Godzik</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Cd-Hit: a fast program for Clustering and Comparing Large Sets of Protein or Nucleotide Sequences</article-title>
        <source>Bioinformatics (Oxford, England)</source>
        <volume>22</volume>
        <year>2006</year>
        <fpage>1658</fpage>
        <lpage>1659</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btl158</pub-id>
      </element-citation>
    </ref>
    <ref id="b0210">
      <label>42</label>
      <element-citation publication-type="journal" id="h0210">
        <person-group person-group-type="author">
          <name>
            <surname>Fu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Niu</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <article-title>CD-HIT: accelerated for clustering the next-generation sequencing data</article-title>
        <source>Bioinformatics</source>
        <year>2012</year>
        <fpage>3150</fpage>
        <lpage>3152</lpage>
        <pub-id pub-id-type="pmid">23060610</pub-id>
      </element-citation>
    </ref>
    <ref id="b0215">
      <label>43</label>
      <element-citation publication-type="journal" id="h0215">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>H.-D.</given-names>
          </name>
          <name>
            <surname>Zulfiqar</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Yuan</surname>
            <given-names>S.-S.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>Q.-L.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z.-Y.</given-names>
          </name>
        </person-group>
        <article-title>iBLP: an XGBoost-Based Predictor for Identifying Bioluminescent Proteins</article-title>
        <source>Comput Math Methods Med</source>
        <volume>2021</volume>
        <year>2021</year>
        <fpage>6664362</fpage>
        <pub-id pub-id-type="doi">10.1155/2021/6664362</pub-id>
        <pub-id pub-id-type="pmid">33505515</pub-id>
      </element-citation>
    </ref>
    <ref id="b0220">
      <label>44</label>
      <element-citation publication-type="journal" id="h0220">
        <person-group person-group-type="author">
          <name>
            <surname>Dao</surname>
            <given-names>F.Y.</given-names>
          </name>
          <name>
            <surname>Lv</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z.M.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>DeepYY1: a deep learning approach to identify YY1-mediated chromatin loops</article-title>
        <source>Brief Bioinform</source>
        <year>2020</year>
        <pub-id pub-id-type="doi">10.1093/bib/bbaa356</pub-id>
      </element-citation>
    </ref>
    <ref id="b0225">
      <label>45</label>
      <element-citation publication-type="journal" id="h0225">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Nie</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>iATP: a sequence based method for identifying anti-tubercular peptides</article-title>
        <source>Med Chem</source>
        <year>2019</year>
        <pub-id pub-id-type="doi">10.2174/1573406415666191002152441</pub-id>
      </element-citation>
    </ref>
    <ref id="b0230">
      <label>46</label>
      <element-citation publication-type="journal" id="h0230">
        <person-group person-group-type="author">
          <name>
            <surname>Qiu</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Lv</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Hong</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Jia</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>BOW-GBDT: a GBDT classifier combining with artificial neural network for identifying GPCR-drug interaction based on wordbook learning from sequences</article-title>
        <source>Front Cell Dev Biol</source>
        <volume>8</volume>
        <year>2020</year>
        <object-id pub-id-type="publisher-id">623858</object-id>
        <pub-id pub-id-type="doi">10.3389/fcell.2020.623858</pub-id>
      </element-citation>
    </ref>
    <ref id="b0235">
      <label>47</label>
      <element-citation publication-type="journal" id="h0235">
        <person-group person-group-type="author">
          <name>
            <surname>Rumelhart</surname>
            <given-names>D.E.</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G.E.</given-names>
          </name>
          <name>
            <surname>Williams</surname>
            <given-names>R.J.</given-names>
          </name>
        </person-group>
        <article-title>Learning representations by back propagating errors</article-title>
        <source>Nature</source>
        <volume>323</volume>
        <year>1986</year>
        <fpage>533</fpage>
        <lpage>536</lpage>
      </element-citation>
    </ref>
    <ref id="b0240">
      <label>48</label>
      <element-citation publication-type="journal" id="h0240">
        <person-group person-group-type="author">
          <name>
            <surname>Judith</surname>
            <given-names>E.D.P.D.</given-names>
          </name>
          <name>
            <surname>Deleo</surname>
            <given-names>J.M.</given-names>
          </name>
        </person-group>
        <article-title>Artificial neural networks</article-title>
        <source>Cancer</source>
        <volume>91</volume>
        <year>2001</year>
        <fpage>1615</fpage>
        <lpage>1635</lpage>
        <pub-id pub-id-type="pmid">11309760</pub-id>
      </element-citation>
    </ref>
    <ref id="b0245">
      <label>49</label>
      <mixed-citation publication-type="other" id="h0245">Mikolov T, Sutskever I, Chen K, Corrado G, Dean J. Distributed representations of words and phrases and their compositionality. Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2 2013:3111–9.</mixed-citation>
    </ref>
    <ref id="b0250">
      <label>50</label>
      <mixed-citation publication-type="other" id="h0250">Rong X. word2vec Parameter Learning Explained. Computer Science 2014.</mixed-citation>
    </ref>
    <ref id="b0255">
      <label>51</label>
      <element-citation publication-type="book" id="h0255">
        <person-group person-group-type="author">
          <name>
            <surname>Bottou</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Lechevallier</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Saporta</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <part-title>Large-scale machine learning with stochastic gradient descent</part-title>
        <year>2010</year>
        <publisher-name>Physica-Verlag HD</publisher-name>
        <publisher-loc>Heidelberg</publisher-loc>
        <fpage>177</fpage>
        <lpage>186</lpage>
      </element-citation>
    </ref>
    <ref id="b0260">
      <label>52</label>
      <element-citation publication-type="journal" id="h0260">
        <person-group person-group-type="author">
          <name>
            <surname>Kawashima</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Kanehisa</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>AAindex: amino acid index database</article-title>
        <source>Nucleic Acids Res</source>
        <volume>28</volume>
        <year>2000</year>
        <fpage>374</fpage>
        <pub-id pub-id-type="pmid">10592278</pub-id>
      </element-citation>
    </ref>
    <ref id="b0265">
      <label>53</label>
      <mixed-citation publication-type="other" id="h0265">Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. International conference on machine learning, PMLR; 2015, p. 448–56.</mixed-citation>
    </ref>
    <ref id="b0270">
      <label>54</label>
      <mixed-citation publication-type="other" id="h0270">Kingma DP, Ba J. Adam: A method for stochastic optimization. ArXiv Preprint ArXiv:14126980 2014.</mixed-citation>
    </ref>
    <ref id="b0275">
      <label>55</label>
      <mixed-citation publication-type="other" id="h0275">Maas AL, Hannun AY, Ng AY, others. Rectifier nonlinearities improve neural network acoustic models. Proc. icml, vol. 30, 2013, p. 3.</mixed-citation>
    </ref>
    <ref id="b0280">
      <label>56</label>
      <mixed-citation publication-type="other" id="h0280">He X, Pan J, Jin O, Xu T, Liu B, Xu T, et al. Practical Lessons from Predicting Clicks on Ads at Facebook. Proceedings of the Eighth International Workshop on Data Mining for Online Advertising 2014:1–9. 10.1145/2648584.2648589.</mixed-citation>
    </ref>
    <ref id="b0285">
      <label>57</label>
      <element-citation publication-type="journal" id="h0285">
        <person-group person-group-type="author">
          <name>
            <surname>Tian</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>An accurate eye pupil localization approach based on adaptive gradient boosting decision tree</article-title>
        <source>Vis Commun Image Process (VCIP)</source>
        <volume>2016</volume>
        <year>2016</year>
        <fpage>1</fpage>
        <lpage>4</lpage>
        <pub-id pub-id-type="doi">10.1109/VCIP.2016.7805483</pub-id>
      </element-citation>
    </ref>
    <ref id="b0290">
      <label>58</label>
      <mixed-citation publication-type="other" id="h0290">Friedman. Classification and Regression Trees. Wadsworth International Group; 1984.</mixed-citation>
    </ref>
    <ref id="b0295">
      <label>59</label>
      <element-citation publication-type="journal" id="h0295">
        <person-group person-group-type="author">
          <name>
            <surname>Breiman</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Random forests</article-title>
        <source>Mach Learn</source>
        <volume>45</volume>
        <year>2001</year>
        <fpage>5</fpage>
        <lpage>32</lpage>
      </element-citation>
    </ref>
    <ref id="b0300">
      <label>60</label>
      <element-citation publication-type="journal" id="h0300">
        <person-group person-group-type="author">
          <name>
            <surname>Bentéjac</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Csrg</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Martínez-Muoz</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>A comparative analysis of gradient boosting algorithms</article-title>
        <source>Artif Intell Rev</source>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>31</lpage>
      </element-citation>
    </ref>
    <ref id="b0305">
      <label>61</label>
      <mixed-citation publication-type="other" id="h0305">Chen T, Guestrin C. XGBoost: A Scalable Tree Boosting System. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 2016:785–94. 10.1145/2939672.2939785.</mixed-citation>
    </ref>
    <ref id="b0310">
      <label>62</label>
      <element-citation publication-type="journal" id="h0310">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>Risk prediction of diabetes: big data mining with fusion of multifarious physical examination indicators</article-title>
        <source>Inform Fusion</source>
        <year>2021</year>
        <pub-id pub-id-type="doi">10.1016/j.inffus.2021.02.015</pub-id>
      </element-citation>
    </ref>
    <ref id="b0315">
      <label>63</label>
      <element-citation publication-type="journal" id="h0315">
        <person-group person-group-type="author">
          <name>
            <surname>Tang</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Nie</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Kang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <article-title>mRNALocater: enhance the prediction accuracy of eukaryotic mRNA subcellular localization by using model fusion strategy</article-title>
        <source>Mol Ther</source>
        <year>2021</year>
        <pub-id pub-id-type="doi">10.1016/j.ymthe.2021.04.004</pub-id>
      </element-citation>
    </ref>
    <ref id="b0320">
      <label>64</label>
      <element-citation publication-type="journal" id="h0320">
        <person-group person-group-type="author">
          <name>
            <surname>Dietterich</surname>
            <given-names>T.G.</given-names>
          </name>
        </person-group>
        <article-title>An experimental comparison of three methods for constructing ensembles of decision trees: bagging, boosting, and randomization</article-title>
        <source>Mach Learn</source>
        <volume>40</volume>
        <year>2000</year>
        <fpage>139</fpage>
        <lpage>157</lpage>
      </element-citation>
    </ref>
  </ref-list>
  <ack id="ak005">
    <sec id="s0125">
      <title>Acknowledgments</title>
      <p id="p0245">This work was supported by the grants from the National Natural Science Foundation of China (No. 31760315, 31860312), Natural Science Foundation of Jiangxi Province, China (NO. 20202BAB202007).</p>
    </sec>
    <sec id="s0130">
      <title>Author contributions</title>
      <p id="p0250">W. Q. conceived and designed the experiments; Z. L. and S. S. performed the extraction of features, model construction, model training, and evaluation. W.Q. and Z. L. analyzed the data and implemented the classifiers. Z. L. drafted the manuscript. X. X. and H. L. supervised this project and revised the manuscript. All authors read and approved the final manuscript.</p>
    </sec>
  </ack>
</back>
