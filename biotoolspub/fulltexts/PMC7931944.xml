<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Big Data</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Big Data</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Big Data</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Big Data</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2624-909X</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7931944</article-id>
    <article-id pub-id-type="doi">10.3389/fdata.2020.00012</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Big Data</subject>
        <subj-group>
          <subject>Technology and Code</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>FoodKG: A Tool to Enrich Knowledge Graphs Using Machine Learning Techniques</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Gharibi</surname>
          <given-names>Mohamed</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="corresp" rid="c001">
          <sup>*</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/788330/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zachariah</surname>
          <given-names>Arun</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/788197/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Rao</surname>
          <given-names>Praveen</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <xref ref-type="aff" rid="aff3">
          <sup>3</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/194579/overview"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Department of Computer Science and Electrical Engineering, University of Missouri-Kansas City</institution>, <addr-line>Kansas City, MO</addr-line>, <country>United States</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Department of Electrical Engineering and Computer Science, University of Missouri-Columbia</institution>, <addr-line>Columbia, MO</addr-line>, <country>United States</country></aff>
    <aff id="aff3"><sup>3</sup><institution>Department of Health Management and Informatics, University of Missouri-Columbia</institution>, <addr-line>Columbia, MO</addr-line>, <country>United States</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Naoki Abe, IBM Research, United States</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Luca Maria Aiello, Nokia, United Kingdom; Amr Magdy, University of California, Riverside, United States</p>
      </fn>
      <corresp id="c001">*Correspondence: Mohamed Gharibi <email>mggvf@mail.umkc.edu</email></corresp>
      <fn fn-type="other" id="fn001">
        <p>This article was submitted to Data Mining and Management, a section of the journal Frontiers in Big Data</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>29</day>
      <month>4</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2020</year>
    </pub-date>
    <volume>3</volume>
    <elocation-id>12</elocation-id>
    <history>
      <date date-type="received">
        <day>25</day>
        <month>3</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>11</day>
        <month>3</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2020 Gharibi, Zachariah and Rao.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Gharibi, Zachariah and Rao</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>While there exist a plethora of datasets on the Internet related to Food, Energy, and Water (FEW), there is a real lack of reliable methods and tools that can consume these resources. This hinders the development of novel decision-making applications utilizing knowledge graphs. In this paper, we introduce a novel software tool, called FoodKG, that enriches FEW knowledge graphs using advanced machine learning techniques. Our overarching goal is to improve decision-making and knowledge discovery as well as to provide improved search results for data scientists in the FEW domains. Given an input knowledge graph (constructed on raw FEW datasets), FoodKG enriches it with semantically related triples, relations, and images based on the original dataset terms and classes. FoodKG employs an existing graph embedding technique trained on a controlled vocabulary called AGROVOC, which is published by the Food and Agriculture Organization of the United Nations. AGROVOC includes terms and classes in the agriculture and food domains. As a result, FoodKG can enhance knowledge graphs with semantic similarity scores and relations between different classes, classify the existing entities, and allow FEW experts and researchers to use scientific terms for describing FEW concepts. The resulting model obtained after training on AGROVOC was evaluated against the state-of-the-art word embedding and knowledge graph embedding models that were trained on the same dataset. We observed that this model outperformed its competitors based on the Spearman Correlation Coefficient score.</p>
    </abstract>
    <kwd-group>
      <kwd>machine learning</kwd>
      <kwd>graph embeddings</kwd>
      <kwd>knowledge graphs</kwd>
      <kwd>AGROVOC</kwd>
      <kwd>semantic similarity</kwd>
    </kwd-group>
    <counts>
      <fig-count count="9"/>
      <table-count count="6"/>
      <equation-count count="1"/>
      <ref-count count="48"/>
      <page-count count="12"/>
      <word-count count="6588"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>1. Introduction</title>
    <p>Food, energy, and water are the critical resources for sustaining human life on Earth. Currently, there are a plethora of datasets on the Internet related to FEW resources. However, there is still a lack of reliable tools that can consume these resources and provide decision-making capabilities (Rao et al., <xref rid="B32" ref-type="bibr">2016</xref>). Moreover, FEW data exists on the Internet in different formats with different file extensions, such as CSV, XML, and JSON, and this makes it a challenge for users to join, query, and perform other tasks (Knoblock and Szekely, <xref rid="B18" ref-type="bibr">2015</xref>). Generally, such data types are not consumable in the world of Linked Open Data (LOD), and neither are they ready to be processed by different deep learning networks (Meester, <xref rid="B21" ref-type="bibr">2018</xref>). Recently, in September 2018, Google announced its “Google Dataset Search”, which is a search engine that includes graphs and Linked Data. Google Dataset Search is a giant leap in the Semantic Web domain, but the challenge is the lack of published knowledge graphs, especially in the FEW systems area (Gharibi et al., <xref rid="B12" ref-type="bibr">2018</xref>).</p>
    <p>Knowledge graphs, including Freebase (Bollacker et al., <xref rid="B3" ref-type="bibr">2008</xref>), DBpedia, (Auer et al., <xref rid="B1" ref-type="bibr">2007</xref>), and YAGO (Suchanek et al., <xref rid="B40" ref-type="bibr">2007</xref>), have been commonly used in Semantic Web technologies, Linked Open Data, and cloud computing (Dubey et al., <xref rid="B8" ref-type="bibr">2018</xref>) due to their semantic properties. In recent years, many free and commercial knowledge graphs have been constructed from semi-structured repositories like Wikipedia or harvested from the Web. In both cases, the results are large global knowledge graphs that have a trade-off between completeness and correctness (Hixon et al., <xref rid="B15" ref-type="bibr">2015</xref>). Recently, different refinement methods have been proposed to utilize the knowledge in these graphs to make them more useful in domain-specific areas by adding the missing knowledge, identifying error pieces, and extracting useful information for users (Paulheim, <xref rid="B29" ref-type="bibr">2017</xref>). Furthermore, knowledge extraction methods used in most of the knowledge graphs are based on binary facts (Ernst et al., <xref rid="B9" ref-type="bibr">2018</xref>). These binary facts represent the relations between two entities, which limit their deep reasoning ability when there are multiple entities, especially in domain-specific areas like FEW (Vashishth et al., <xref rid="B44" ref-type="bibr">2018</xref>).</p>
    <p>The lack of reliable knowledge graphs serving FEW resources has motivated us to build our tool, FoodKG, which uses domain-specific graph embeddings to help in the decision-making process, improving knowledge discovery, simplifying access, and providing better search results. FoodKG enriches FEW datasets by adding additional knowledge and images based on the semantic similarities (Varelas et al., <xref rid="B43" ref-type="bibr">2005</xref>) between entities within the same context. To achieve these tasks, FoodKG employs a recent graph embedding technique based on self-clustering called GEMSEC (Rozemberczki et al., <xref rid="B33" ref-type="bibr">2019</xref>), which was retrained on the AGROVOC (Caracciolo et al., <xref rid="B4" ref-type="bibr">2013</xref>) dataset. AGROVOC is a collection of vocabularies that covers all areas of interest to the Food and Agriculture Organization of the United Nations, including food, nutrition, agriculture, fisheries, forestry, and the environment. The retrained model, AGROVEC, is a domain-specific graph embedding model that enables FoodKG to enhance knowledge graphs with the semantic similarity scores between different terms and concepts. In addition, FoodKG also allows users to query knowledge graphs using SPARQL through a friendly user interface.</p>
    <p>In this paper, we have proposed a tool called FoodKG that refines and enriches FEW resources to utilize the knowledge in FEW graphs in order to make them more useful for researchers, experts, and domain users. Our work makes several key contributions:</p>
    <list list-type="bullet">
      <list-item>
        <p>FoodKG is a novel software tool that aims to enrich and enhance FEW graphs using multiple features. Adding a context to the provided triples is one of the first features that allows querying the graphs more easily and providing better input for deep learning models.</p>
      </list-item>
      <list-item>
        <p>FoodKG provides different Natural Language Processing (NLP) techniques, such as POS tagging, chunking, and Stanford Parser, to extract the meaningful subjects, unify the repeated concepts, and link related entities together (Klein and Manning, <xref rid="B17" ref-type="bibr">2003</xref>; Chen and Manning, <xref rid="B5" ref-type="bibr">2014</xref>; Manning et al., <xref rid="B19" ref-type="bibr">2014</xref>).</p>
      </list-item>
      <list-item>
        <p>FoodKG employs the Specialization Tensor Model (STM) (Glavaš and Vulicć, <xref rid="B13" ref-type="bibr">2018</xref>) to predict the newly added relations within the graph.</p>
      </list-item>
      <list-item>
        <p>We adopted WordNet (Miller, <xref rid="B24" ref-type="bibr">1995</xref>) to return all the offsets for the provided subjects in order to parse the related images from ImageNet (Russakovsky et al., <xref rid="B34" ref-type="bibr">2015</xref>). These images will be added to the graph in the form of Universal Resource Locator (URL) as related and pure images.</p>
      </list-item>
      <list-item>
        <p>FoodKG utilizes the GEMSEC (Rozemberczki et al., <xref rid="B33" ref-type="bibr">2019</xref>) model that was retrained on AGROVOC with fine-tuning to produce AGROVEC to provide the semantic similarity scores between the similar and linked concepts. AGROVEC was compared with word embeddings and knowledge graph embedding models trained on the same dataset. By virtue of being trained on domain-specific graph data, AGROVEC achieved a superior performance to its competitors in terms of the Spearman Correlation Coefficient score.</p>
      </list-item>
    </list>
    <p>Our results showed that AGROVEC provides more accurate and reliable results than the other embeddings in different scenarios: category classification, semantic similarity, and scientific concepts.</p>
    <p>We have aimed at making FoodKG one of the best tools for data scientists and researchers in the FEW domains to develop next-generation applications using the concept of knowledge graphs and machine learning techniques. The rest of the paper is organized such that section 2 discusses recent related work; section 3 presents the design details of FoodKG; section 4 discusses the implementation and performance evaluation of FoodKG; and section 5 provides our conclusion.</p>
  </sec>
  <sec id="s2">
    <title>2. Literature Review</title>
    <p>FoodKG is a unique software in terms of type and purpose. There are no other systems or tools that have the same features. Our main work falls under graph embedding techniques. Embedded vectors learn the distributional semantics of words and are used in different applications such as Named Entity Recognition (NER), question answering, document classification, information retrieval, and other machine learning applications (Nadeau and Sekine, <xref rid="B27" ref-type="bibr">2007</xref>). The embedded vectors mainly rely on calculating the angle between pairs of words to check the semantic similarity and perform other word analogy tasks, such as the common example <italic>king - queen = man - woman</italic>. The two main methods for learning word vectors are matrix factorization methods, such as Latent Semantic Analysis (LSA) (Deerwester et al., <xref rid="B7" ref-type="bibr">1990</xref>), and Local Context Window (LCW) methods, such as skip-gram (Word2vec) (Mikolov et al., <xref rid="B23" ref-type="bibr">2013b</xref>). Matrix factorization is the method that generates the low-dimensional word representation in order to capture the statistical information about a corpus by decomposing large matrices after utilizing low-rank approximations. In LSA, each row corresponds to a word or a concept, whereas columns correspond to a different document in the corpus. However, while methods like LSA leverage statistical information, they do relatively poor in the word analogy task, indicating a sub-optimal vector space structure. The second method aids in making predictions within a local context window, such as the Continuous Bag-of-Words (CBOW) model (Mikolov et al., <xref rid="B22" ref-type="bibr">2013a</xref>). CBOW architecture relies on predicting the focus word from the context words. Skip-gram is the method of predicting all the context words one by one from a single given focus word. Few techniques have been proposed, such as hierarchical softmax, to optimize such predictions by building a binary tree of all the words then predict the path to a specific node. Recently, Pennington et al. (<xref rid="B30" ref-type="bibr">2014</xref>) shed light on GloVe, which is an unsupervised learning algorithm for generating embeddings by aggregating global word–word co-occurrence matrix counts where it tabulates the number of times word <italic>j</italic> appears in the context of the word <italic>i</italic>. FastText is another embedding model created by the Facebook AI Research (FAIR) group for efficient learning of word representations and sentence classification (Bojanowski et al., <xref rid="B2" ref-type="bibr">2017</xref>). FastText considers each word as a combination of <italic>n</italic>-grams of characters where <italic>n</italic> could range from 1 to the length of the word. Therefore, fastText has some advantages over Word2vec and GloVe, such as finding a vector representation for the rare words that may not appear in Word2vec and GloVe. <italic>n</italic>-gram embeddings tend to perform better on smaller datasets.</p>
    <p>A knowledge graph embedding is a type of embedding in which the input is a knowledge graph that leverages the use of relations between the vertices. We consider Holographic Embeddings of Knowledge Graphs (HolE) to be the state-of-art knowledge graph embedding model (Nickel et al., <xref rid="B28" ref-type="bibr">2016</xref>). When the input dataset is a graph instead of a text corpus we apply different embedding algorithms, such as LINE (Tang et al., <xref rid="B41" ref-type="bibr">2015</xref>), Node2ec (Grover and Leskovec, <xref rid="B14" ref-type="bibr">2016</xref>), M-NMF (Wang et al., <xref rid="B46" ref-type="bibr">2017</xref>), and DANMF (Ye et al., <xref rid="B47" ref-type="bibr">2018</xref>). DeepWalk is one of the common models for graph embedding (Perozzi et al., <xref rid="B31" ref-type="bibr">2014</xref>). DeepWalk leverages modeling and deep learning for learning latent representations of vertices in a graph by analyzing and applying random walks. Random walk in a graph is equivalent to predicting a word in a sentence. In graphs, however, the sequence of nodes that frequently appear together are considered to be the sentence within a specific window size. This technique also uses skip-gram to minimize the negative log-likelihood for the observed neighborhood samples. GEMSEC is another graph embedding algorithm that learns nodes clustering while computing the embeddings, whereas the other models do not utilize clustering. It relies on sequence-based embedding with clustering to cluster the embedded nodes simultaneously. The algorithm places the nodes in abstract feature space to minimize the negative log-likelihood of the preserved neighborhood nodes with clustering the nodes into a specific number of clusters. Graph embeddings hold the semantics between the concepts in a better way than word embeddings, and that is the reason behind using a graph embedding model to utilize graph semantics in FoodKG.</p>
  </sec>
  <sec sec-type="methods" id="s3">
    <title>3. Methodology</title>
    <p>We presented a domain-specific tool, FoodKG, that solves the problem of repeated, unused, and missing concepts in knowledge graphs and enriches the existing knowledge by adding semantically related domain-specific entities, relations, images, and semantic similarities values between the entities. We utilized AGROVEC, a graph embedding model to calculate the semantic similarity between two entities, get most similar entities, and classify entities under a set of predefined classes. AGROVEC adds the semantic similarity scores by calculating the cosine similarity for the given vectors. The triple that holds the semantic score will be encoded as a blank node where the subject is the hash of the original triple; the relation will remain the same, and the object is the actual semantic score.</p>
    <p>FoodKG will parse and process all the subjects and objects within the provided knowledge graph. For each subject, a request will be made to WordNet to fetch its offset number. WordNet is a lexical database for the English language where it groups its words into sets of synonyms called synsets with their corresponding IDs (offsets). FoodKG requires these offset numbers to obtain the related images from ImageNet since the existing images on ImageNet are organized and classified based on the WordNet offsets. ImageNet is one of the largest image repositories on the Internet, and it contains images for almost all-known classes (Chen et al., <xref rid="B6" ref-type="bibr">2018</xref>). These images will be added to the provided graph in the form of triples where the subject is the original word, the predicate will be represented by “#ImgURLs,” and the object is a Web link URL that contains the images returned from ImageNet. <xref ref-type="fig" rid="F1">Figure 1</xref> depicts the FoodKG system architecture.</p>
    <fig id="F1" position="float">
      <label>Figure 1</label>
      <caption>
        <p>FoodKG system architecture.</p>
      </caption>
      <graphic xlink:href="fdata-03-00012-g0001"/>
    </fig>
    <sec>
      <title>3.1. AGROVEC</title>
      <p>AGROVEC is a domain-specific embedding model that uses GEMSEC, a graph embedding algorithm. It was retrained and fine-tuned on AGROVOC, to produce a domain-specific embedding model. The embedding visualization (using TSNE; van der Maaten and Hinton, <xref rid="B42" ref-type="bibr">2008</xref>) for our clustered embeddings is depicted in <xref ref-type="fig" rid="F2">Figure 2</xref>. AGROVEC has the advantage of clustering compared to other models. ARGOVEC was trained with a 300-dimension vector and clustered the dataset into 10 clusters. The Gamma value used was 0.01. The number of random walks was six with windows size six. We started with the default initial learning rate of 0.001. AGROVEC was trained using the AGROVOC dataset that contains over 6 Million triples to construct the embedding.</p>
      <fig id="F2" position="float">
        <label>Figure 2</label>
        <caption>
          <p>AGROVEC embeddings visualization using TSNE for the words: Food, Energy, and Water with their top 20 nearest neighbors based on AGROVEC model. The figure shows how AGROVEC cluster similar concepts together properly.</p>
        </caption>
        <graphic xlink:href="fdata-03-00012-g0002"/>
      </fig>
    </sec>
    <sec>
      <title>3.2. Entity Extraction</title>
      <p>FoodKG provides several features; entity extraction is one of the most important features. Users can start by uploading their graphs to FoodKG. Most of the provided graphs contain the same repeated concepts and terms that were named differently (e.g., id, ID, _id, id_num, etc.) where all of them represent the same entity, and other terms use abbreviations, numbers, or short-forms (acronyms) (Shen et al., <xref rid="B37" ref-type="bibr">2015</xref>). Similar entities with different names create many repetitions and make it a challenge for different graphs to merge, search, and ingest in machine learning or linked data. To overcome this issue, we ran NLP techniques, such as POS tagging, chunking, and Stanford Parser, over all the provided subjects to extract the meaningful classes and terms that can be used in the next stage. For example, the following subjects “CHEESE,COTTAGE,S-,W/FRU” and “BUTTER,PDR,1.5OZ,PREP,W/1/1.HYD,” will be represented in FoodKG as “Cheese Cottage” and “Butter,” respectively. Users have the option of whether to provide the context of their graphs or leave it empty.</p>
    </sec>
    <sec>
      <title>3.3. Text Classification</title>
      <p>Nowadays, there are many different models to classify a given text to a set of tags or classes, such as the Long Short Term Memory (LSTM) network (Sachan et al., <xref rid="B35" ref-type="bibr">2019</xref>). Nevertheless, text classification is still a challenge when it comes to classifying a single word without a context: “apple” has a broad context, for example, and the word could refer to many different things other than apple the fruit. Therefore, few solutions have been proposed, such as referring to fruits with small letters “apple” and capital letters for brand names like “Apple” the corporation. However, such a technique does not seem to be working well in large-scale contexts. Besides, this technique does not work in all domains since domain-specific graphs may not include all different contexts for a given word. Therefore, for each domain-specific area, there should be a knowledge graph that researchers and scientists can use in their experiments. At this point, our tool, FoodKG, becomes helpful to build and enrich such knowledge graphs and classify words to a specific class. FoodKG uses AGROVEC to help in providing the context for such scenarios. We use a simple yet effective technique with the help of ConceptNet API to accomplish this task (Speer et al., <xref rid="B39" ref-type="bibr">2017</xref>). The idea is to start with a set of predefined classes; for example, let us consider only two classes for now, such as “fruits” and “animals.” After running these classes on ConceptNet, we store all returned top related concepts with the relation “type_of.” Here is an example of the returned words for “fruit”: <italic>pineapple, mango, grapes, plums, berry, etc</italic>.. The returned words for “animals” are <italic>lion, fish, dolphin, fox, pet, deer, etc</italic>.. We get only the top 10 instances from each category to limit the time complexity of our algorithm. Then, using AGROVEC embeddings, we calculate the semantic similarity score between the given word and all the other words from each class and return the highest average between them (<xref rid="TA1" ref-type="table">Algorithm 1</xref>). Based on the highest average score, we choose the category of the given word. This technique proved to be the most reliable technique when it comes to classifying a category using word embeddings. The algorithm time complexity is O(N), where N is the number of classes that we started with. As an example, AGROVEC predicted the class “Food” for the concept “brown_rice,” “Energy” class for the concept “radiation,” and “Water” for the concept “hail.”</p>
      <table-wrap id="TA1" position="float">
        <label>Algorithm 1</label>
        <caption>
          <p>Text Classification using AGROVEC and ConceptNet</p>
        </caption>
        <table frame="hsides" rules="groups">
          <tbody>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">Input:  <italic>Target, Cat</italic> = {<italic>A</italic><sub>1</sub> = {<italic>word</italic><sub>1</sub>, …, <italic>word</italic><sub>10</sub>}, …, <italic>A</italic><sub><italic>N</italic></sub> = {<italic>word</italic><sub>1</sub>, …, <italic>word</italic><sub>10</sub>}}</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">Output:  The predicted class for the <italic>Target</italic></td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1"> </td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1"> 1: function <sc>Loop</sc>(<italic>Cat</italic>[], <italic>Target</italic>)</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1"> 2:    <italic>prediction</italic> ← <italic>nil</italic></td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1"> 3:    <italic>highestAvg</italic> ← 0</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1"> 4:    <italic>N</italic> ← <italic>length</italic>(<italic>Cat</italic>)</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1"> 5:    for <italic>i</italic>←1 to <italic>N</italic>
<bold>do</bold></td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1"> 6:      <italic>total, Avg</italic> ← 0</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1"> 7:      <italic>K</italic> ← <italic>length</italic>({<italic>A</italic><sub><italic>i</italic></sub>})</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1"> 8:      for <italic>j</italic>←1 to <italic>K</italic>
<bold>do</bold></td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1"> 9:         <italic>total</italic> ← <italic>total</italic> + <italic>cosineSimilarity</italic>(<italic>Target, A</italic><sub><italic>i</italic></sub>[<italic>j</italic>])</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">10:      end <bold>for</bold></td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">11:      <italic>Avg</italic> ← <italic>total</italic>/<italic>K</italic></td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">12:      if <italic>Avg</italic> &gt; <italic>highestAvg</italic>
<bold>then</bold></td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">13:         <italic>highestAvg</italic> ← <italic>Avg</italic></td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">14:         <italic>prediction</italic> ← <italic>A</italic><sub><italic>i</italic></sub></td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">15:      end <bold>if</bold></td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">16:    end <bold>for</bold></td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">17:    return <italic>prediction</italic></td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">18:  end <bold>function</bold></td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec>
      <title>3.4. Semantic Similarity</title>
      <p>Measurement of the semantic similarity between two terms or concepts has gained much interest due to its importance in different applications, such as intelligent graphs, knowledge retrieval systems, and similarity between Web documents (Iosif and Potamianos, <xref rid="B16" ref-type="bibr">2010</xref>). Several semantic similarity measures have been developed and used based on this purpose (Martinez-Gil, <xref rid="B20" ref-type="bibr">2014</xref>). In this paper, we adopted the cosine similarity measurement to measure the similarity between two vectors. FoodKG uses the semantic similarity measure between different subjects in a given graph. The semantic similarity scores will be attached as blank nodes to the original triple where the subject is the hashed blank node ID, the relation is “#semantic_similarity,” and the object the similarity score. These similarity scores can be used in different recommendation systems, question answering, or in future NLP models. FoodKG relies on the AGROVEC embedding model to generate the similarity scores. <xref rid="T1" ref-type="table">Table 1</xref> shows the semantic similarity scores generated by AGROVEC and other models. This example was taken from the AGROVOC dataset to show how the AGROVEC model ranks these pairs in a better way than the other models. <xref rid="T2" ref-type="table">Table 2</xref> shows the top five related words for “Food,” <xref rid="T3" ref-type="table">Table 3</xref> shows the top five related words for “Energy,” and <xref rid="T4" ref-type="table">Table 4</xref> shows the top five related words for “Water.”</p>
      <table-wrap id="T1" position="float">
        <label>Table 1</label>
        <caption>
          <p>An example on how each model ranks the objects when the subject is “wheat” AGROVEC ranks the semantic similarity scores accurately from closest to furthest from the subject.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Object</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>AGROVEC</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>HolE</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>GloVe</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Word2vec</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>fastText</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Wheat_flour</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.757</td>
              <td valign="top" align="center" rowspan="1" colspan="1">−0.199</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.295</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.948</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.992</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Barley</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.523</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.868</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.421</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.741</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.976</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Grapes</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.116</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.851</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.802</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.930</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.885</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Tuna_oil</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.046</td>
              <td valign="top" align="center" rowspan="1" colspan="1">−0.769</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.376</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.524</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.940</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Building_components</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.016</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.923</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.397</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.466</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.883</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <table-wrap id="T2" position="float">
        <label>Table 2</label>
        <caption>
          <p>Top 5 related words for the concept “Foods.”</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Model</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Top 5 related words</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">AGROVEC</td>
              <td valign="top" align="left" rowspan="1" colspan="1">traditional_foods, soups, raw_foods, value_added_product, cooking_fats</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">HolE</td>
              <td valign="top" align="left" rowspan="1" colspan="1">controls, sterilizing, consumer_expenditure, Andean_Group, structural_crops</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">GloVe</td>
              <td valign="top" align="left" rowspan="1" colspan="1">meat, animal_meals, milk, water, seaweeds</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Word2vec</td>
              <td valign="top" align="left" rowspan="1" colspan="1">cocoa_beans, hides_and_skins, eggs, oilseed_protein, soyfoods</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">fastText</td>
              <td valign="top" align="left" rowspan="1" colspan="1">pet_foods, raw_foods, seafoods, soyfoods, skin_producing_animals</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <table-wrap id="T3" position="float">
        <label>Table 3</label>
        <caption>
          <p>Top 5 related words for the concept “Energy.”</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Model</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Top 5 related words</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="bottom" align="left" rowspan="1" colspan="1">AGROVEC</td>
              <td valign="top" align="center" rowspan="1" colspan="1">nuclear_energy,energy_for_agriculture,energy_expenditure,<break/> animal_power,renewable_energy</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">HolE</td>
              <td valign="top" align="left" rowspan="1" colspan="1">stored_products_pests, plant_breeding, age, formulations, sewage</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">GloVe</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Ericales,carbohydrates,Sphingidae,Orobanchaceae,fungal_spores</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Word2vec</td>
              <td valign="top" align="left" rowspan="1" colspan="1">stray_voltage_effects, irrigation_canals, libraries, agencies, CMS bioenergy,computer_science,wood_energy,cytogenetics</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">fastText</td>
              <td valign="top" align="center" rowspan="1" colspan="1">Cytogenetics</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <table-wrap id="T4" position="float">
        <label>Table 4</label>
        <caption>
          <p>Top 5 related words for the concept “Water.”</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Model</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Top 5 related words</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">AGROVEC</td>
              <td valign="top" align="left" rowspan="1" colspan="1">hydrosorption, chlorinated_water, water_statistics, body_water, virtual_water</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">HolE</td>
              <td valign="top" align="center" rowspan="1" colspan="1">isObjectOfActivity,dissolved_oxygen,economic_competition,<break/> state,international_cooperation</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">GloVe</td>
              <td valign="top" align="left" rowspan="1" colspan="1">seaweeds, meat, perishable_products, phosphorus, drugs</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Word2vec</td>
              <td valign="top" align="left" rowspan="1" colspan="1">quarters, meat_byproducts, captivity, magnetic_water, plant_parts</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">fastText</td>
              <td valign="top" align="left" rowspan="1" colspan="1">heaters, bound_water, low_water, esters, high_water</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec>
      <title>3.5. Scientific Terms</title>
      <p>Researchers and data experts often use domain-specific terms and concepts that may not be commonly used. For instance, these terms <italic>Triticum, Malus, and Fragaria</italic> are the scientific names for <italic>wheat, apples, and strawberries</italic>, respectively. However, such names may not exist in global word or knowledge graph embedding models. As for FEW, these terms can be found in our embedding model since it was trained on AGROVOC terms. This allows data experts to use similar scientific names and other related terms under the food domain while using FoodKG. <xref rid="T5" ref-type="table">Table 5</xref> shows an example of top related concepts for FEW that do not exist in global embeddings.</p>
      <table-wrap id="T5" position="float">
        <label>Table 5</label>
        <caption>
          <p>Few examples for the most used concepts in FEW domain that do not appear in global embeddings.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Food</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Energy</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Water</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">cocoa_products</td>
              <td valign="top" align="left" rowspan="1" colspan="1">energy_balance</td>
              <td valign="top" align="left" rowspan="1" colspan="1">water_activity</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">brown_rice</td>
              <td valign="top" align="left" rowspan="1" colspan="1">energy_generation</td>
              <td valign="top" align="left" rowspan="1" colspan="1">water_extraction</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">gluten_free_bread</td>
              <td valign="top" align="left" rowspan="1" colspan="1">energy_consumption</td>
              <td valign="top" align="left" rowspan="1" colspan="1">water_availability</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">skim_milk</td>
              <td valign="top" align="left" rowspan="1" colspan="1">energy_value</td>
              <td valign="top" align="left" rowspan="1" colspan="1">water_quality</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">emmental_cheese</td>
              <td valign="top" align="left" rowspan="1" colspan="1">energy_resources</td>
              <td valign="top" align="left" rowspan="1" colspan="1">water_statistics</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec>
      <title>3.6. Relationship Prediction</title>
      <p>Word embeddings are well-known in the world of NLP due to their powerful way of capturing the relatedness between different concepts. However, capturing the lexico-semantic relationship between two words (i.e., the predicate of a triple) is a critical challenge for many NLP applications and models. Few techniques have been developed previously that proposed modifying the original word embeddings to include specific relations while training the corpora (Faruqui et al., <xref rid="B10" ref-type="bibr">2015</xref>; Mrkšić et al., <xref rid="B25" ref-type="bibr">2017</xref>; Vulić and Mrkšić, <xref rid="B45" ref-type="bibr">2018</xref>). These approaches have used the post-processing trained embeddings to check the concepts that move closer together or further apart toward a specific relation. While these algorithms were able to predict specific relations like synonyms and antonyms, predicting, and discriminating between multiple relations is still a challenge. To overcome this challenge, we used transfer learning using the state-of-art STM model, that outperforms previous state-of-art models on CogALex and WordNet datasets, and the AGROVOC dataset to predict domain-specific relations between two concepts. The newly derived model aims particularly at classifying relations between different subjects in the food, agriculture, energy, and water domains.</p>
    </sec>
  </sec>
  <sec id="s4">
    <title>4. Evaluation</title>
    <p>In this section, we report the evaluation of AGROVEC and compare it with other word and knowledge graph embedding techniques: GloVe, fastText, Word2vec, and HolE.</p>
    <sec>
      <title>4.1. Evaluation Technique</title>
      <p>We employed the Spearman rank correlation coefficients (Spearman's rho; Myers and Sirois, <xref rid="B26" ref-type="bibr">2004</xref>) in order to evaluate the embedding models. Spearman's rho is a non-parametric measure for assessing the similarity score between two variables. We applied Spearman's rho between the predicted cosine similarity using the embeddings and the ground truth, which is known as the relatedness task (Schnabel et al., <xref rid="B36" ref-type="bibr">2015</xref>). When the ranks are unique, the Spearman correlation coefficient can be computed using the formula:</p>
      <disp-formula id="E1">
        <label>(1)</label>
        <mml:math id="M1">
          <mml:mtable class="eqnarray" columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>R</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>s</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mn>1</mml:mn>
                <mml:mo>-</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mn>6</mml:mn>
                    <mml:mstyle displaystyle="true">
                      <mml:munderover accentunder="false" accent="false">
                        <mml:mrow>
                          <mml:mo>∑</mml:mo>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mo>=</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>n</mml:mi>
                        </mml:mrow>
                      </mml:munderover>
                    </mml:mstyle>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi>D</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mn>2</mml:mn>
                      </mml:mrow>
                    </mml:msubsup>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>n</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:msup>
                          <mml:mrow>
                            <mml:mi>n</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mn>2</mml:mn>
                          </mml:mrow>
                        </mml:msup>
                        <mml:mo>-</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                </mml:mfrac>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>where <italic>D</italic><sub><italic>i</italic></sub> is difference between the two ranks of each observation and <italic>n</italic> is the total number of observations.</p>
      <fig id="d39e1047" position="float">
        <label>Listing 1</label>
        <caption>
          <p>SPARQL query to extract the English triples</p>
        </caption>
        <graphic xlink:href="fdata-03-00012-g0009"/>
      </fig>
    </sec>
    <sec>
      <title>4.2. Dataset Description</title>
      <p>AGROVOC is a collection of vocabularies that covers all areas of interest to the Food and Agriculture Organization of the United Nations, including food, nutrition, agriculture, fisheries, forestry, and the environment. It comprises of 32,000 concepts, in over 20 languages, where each concept is represented using a unique id. For instance, the subject “<ext-link ext-link-type="uri" xlink:href="http://aims.fao.org/aos/agrovoc/c_12332">http://aims.fao.org/aos/agrovoc/c_12332</ext-link>” corresponds to “maize.” We used the SPARQL query in listing 1 to extract English triples.</p>
    </sec>
    <sec>
      <title>4.3. Benchmark Description</title>
      <p>While there exist well-known word-embedding benchmark datasets, such as WordSim-353 (Finkelstein et al., <xref rid="B11" ref-type="bibr">2002</xref>), for evaluating the semantic similarity measures, these cannot be employed for domain-specific embeddings since many concepts related to FEW are not considered in public benchmarks. Constructing a domain-specific benchmark is a challenge considering the need for domain experts. Therefore, we leverage ConceptNet to construct a benchmark dataset for evaluating the models. ConceptNet originated from the crowd-sourcing project Open Mind Common Sense, which was launched in 1999 at the MIT Media Lab. ConceptNet used to be a home-grown crowd-sourced project with the purpose of improving the state of computational knowledge and human knowledge. However, currently, the data is generated from many trusted resources such as WordNet, DBPedia, Wiktionary (Zesch et al., <xref rid="B48" ref-type="bibr">2008</xref>), OpenCyc (Smywiński-Pohl, <xref rid="B38" ref-type="bibr">2012</xref>), and others. We split AGROVOC dataset based on its 126 unique relations to depict how each model performs against the different relations and to study the impact of the number of hops between the concepts in the embedding. For each subject and object, we looked up the weights returned from ConceptNet and considered them to be the ground truth.</p>
    </sec>
    <sec>
      <title>4.4. Results</title>
      <p>We evaluated two recent graph embedding models, namely DeepWalk and GEMSEC, trained on AGROVOC data, to analyze their performance on the FEW domains. <xref rid="T6" ref-type="table">Table 6</xref> reports the average Spearman correlation coefficient scores for DeepWalk and GEMSEC. The higher score attained by GEMSEC motivated us to use GEMSEC for constructing AGROVEC.</p>
      <table-wrap id="T6" position="float">
        <label>Table 6</label>
        <caption>
          <p>Different graph embedding techniques with their Spearman Correlation score.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Model description</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Spearman Correlation</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">DeepWalk (Perozzi et al., <xref rid="B31" ref-type="bibr">2014</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.068</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">GEMSEC (Rozemberczki et al., <xref rid="B33" ref-type="bibr">2019</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.101</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>We evaluated AGROVEC against HolE, GloVe, Word2vec, and fastText, where all of the models were retrained using their default parameters on the AGROVOC dataset except for the number of dimensions. The number of dimensions used for all models was 300, with the minimum count set as 1 to include all the concepts and relations. <xref ref-type="fig" rid="F3">Figure 3</xref> shows the average Spearman correlation coefficient scores for all the models evaluated on 126 unique relations. <xref ref-type="fig" rid="F4">Figure 4</xref> shows the Spearman correlation coefficient scores while limiting the minimum number of word pairs in each relation to 5, 10, and 25 in order to check the model's performance across the different number of word pairs. The results show that AGROVEC, based on GEMSEC trained and fine-tuned on AGROVOC, outperforms all other models by a significant margin when predicting FEW domain similarity scores. <xref ref-type="fig" rid="F2">Figure 2</xref> shows an example of the AGROVEC embedding using TSNE for the domains Food, Energy, and Water with the top 20 related terms. This shows how these domains with their top related terms are properly clustered. However, <xref ref-type="fig" rid="F5">Figures 5</xref>–<xref ref-type="fig" rid="F8">8</xref> visualize how Hole, GloVe, Word2vec, and fastText cluster Food, Energy, and Water domains with their top 20 related terms. From <xref ref-type="fig" rid="F2">Figures 2</xref>, <xref ref-type="fig" rid="F5">5</xref>–<xref ref-type="fig" rid="F8">8</xref> we observe that AGROVEC achieves better clustering, with the terms of the same domain being placed closer. This was because AGROVEC uses GEMSEC which uses self clustering.</p>
      <fig id="F3" position="float">
        <label>Figure 3</label>
        <caption>
          <p>Spearman correlation coefficient ranking scores compared against ConceptNet. This figure shows how AGROVEC scored highest scores, which means its ranking is the closest for ConceptNet ranking (all models trained on AGROVOC dataset and tested against the same benchmark).</p>
        </caption>
        <graphic xlink:href="fdata-03-00012-g0003"/>
      </fig>
      <fig id="F4" position="float">
        <label>Figure 4</label>
        <caption>
          <p>Spearman correlation coefficient scores when evaluated on all triples and relations with minimum number of word pairs in each relation being 5, 10, and 25.</p>
        </caption>
        <graphic xlink:href="fdata-03-00012-g0004"/>
      </fig>
      <fig id="F5" position="float">
        <label>Figure 5</label>
        <caption>
          <p>HolE embeddings visualization using TSNE for the words: Food, Energy, and Water with their top 20 nearest neighbors based on HolE model.</p>
        </caption>
        <graphic xlink:href="fdata-03-00012-g0005"/>
      </fig>
      <fig id="F6" position="float">
        <label>Figure 6</label>
        <caption>
          <p>GloVe embeddings visualization using TSNE for the words: Food, Energy, and Water with their top 20 nearest neighbors based on GloVe model.</p>
        </caption>
        <graphic xlink:href="fdata-03-00012-g0006"/>
      </fig>
      <fig id="F7" position="float">
        <label>Figure 7</label>
        <caption>
          <p>Word2vec embeddings visualization using TSNE for the words: Food, Energy, and Water with their top 20 nearest neighbors based on Word2vec model.</p>
        </caption>
        <graphic xlink:href="fdata-03-00012-g0007"/>
      </fig>
      <fig id="F8" position="float">
        <label>Figure 8</label>
        <caption>
          <p>FastText embeddings visualization using TSNE for the words: Food, Energy, and Water with their top 20 nearest neighbors based on fastText model.</p>
        </caption>
        <graphic xlink:href="fdata-03-00012-g0008"/>
      </fig>
      <p>We also compared the top five related terms for food, energy, and water, as detailed in <xref rid="T2" ref-type="table">Tables 2</xref>–<xref rid="T4" ref-type="table">4</xref>, respectively. While AGROVEC, which uses GEMSEC trained and fine-tuned on AGROVOC, was able to fetch appropriate concepts related to the provided terms, the other models struggled despite being trained on the same dataset using their default parameters without fine-tuning.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="s5">
    <title>5. Conclusion</title>
    <p>In this paper, we presented FoodKG, a novel software tool to enrich knowledge graphs constructed on FEW datasets by adding semantically related knowledge, semantic similarity scores, and images using advanced machine learning techniques. FoodKG relies on AGROVEC, which was constructed using GEMSEC but retrained and fine-tuned on the AGROVOC dataset. Since AGROVEC was trained on a controlled vocabulary, it provides more accurate results than global vectors in the food and agriculture domains for category classification and semantic similarity of scientific concepts. The STM model, retrained on the AGROVOC dataset, is used for the prediction of semantic relations between graph entities and classes. The output produced by FoodKG can be queried using a SPARQL engine through a friendly user interface. We evaluated AGROVEC using the Spearman Correlation Coefficient algorithm, and the results show that our model outperforms the other models trained on the same graph dataset.</p>
  </sec>
  <sec sec-type="data-availability" id="s6">
    <title>Data Availability Statement</title>
    <p>The datasets analyzed for this study can be found in the [AIMS (AGROVOC)] <ext-link ext-link-type="uri" xlink:href="http://aims.fao.org/vest-registry/vocabularies/agrovoc">http://aims.fao.org/vest-registry/vocabularies/agrovoc</ext-link>. FoodKG code can be found at this Github repository <ext-link ext-link-type="uri" xlink:href="https://github.com/Gharibim/FoodKG">https://github.com/Gharibim/FoodKG</ext-link>.</p>
  </sec>
  <sec id="s7">
    <title>Author Contributions</title>
    <p>All authors have been involved in the design, development, and evaluation of the software. They have also been involved in writing different parts of the paper.</p>
  </sec>
  <sec id="s8">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
</body>
<back>
  <ack>
    <p>We are thankful to the reviewers for their excellent comments and suggestions. This work was supported in part by the National Science Foundation under grant No. 1747751. Part of this work was done when the second and third authors were at the University of Missouri-Kansas City.</p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Auer</surname><given-names>S.</given-names></name><name><surname>Bizer</surname><given-names>C.</given-names></name><name><surname>Kobilarov</surname><given-names>G.</given-names></name><name><surname>Lehmann</surname><given-names>J.</given-names></name><name><surname>Cyganiak</surname><given-names>R.</given-names></name><name><surname>Ives</surname><given-names>Z.</given-names></name></person-group> (<year>2007</year>). <article-title>DBpedia: a nucleus for a web of open data</article-title>, in <source>The Semantic Web. ISWC 2007, ASWC 2007</source>, <volume>Vol. 4825</volume>, eds <person-group person-group-type="editor"><name><surname>Aberer</surname><given-names>K.</given-names></name><name><surname>Choi</surname><given-names>K.-S.</given-names></name><name><surname>Noy</surname><given-names>N.</given-names></name><name><surname>Allemang</surname><given-names>D.</given-names></name><name><surname>Lee</surname><given-names>K.-I.</given-names></name><name><surname>Nixon</surname><given-names>L.</given-names></name><name><surname>Golbeck</surname><given-names>J.</given-names></name><name><surname>Mika</surname><given-names>P.</given-names></name><name><surname>Maynard</surname><given-names>D.</given-names></name><name><surname>Mizoguchi</surname><given-names>R.</given-names></name><name><surname>Schreiber</surname><given-names>G.</given-names></name><name><surname>Cudré-Mauroux</surname><given-names>P.</given-names></name></person-group> (<publisher-loc>Berlin; Heidelberg</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>722</fpage>–<lpage>735</lpage>.</mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bojanowski</surname><given-names>P.</given-names></name><name><surname>Grave</surname><given-names>E.</given-names></name><name><surname>Joulin</surname><given-names>A.</given-names></name><name><surname>Mikolov</surname><given-names>T.</given-names></name></person-group> (<year>2017</year>). <article-title>Enriching word vectors with subword information</article-title>. <source>Trans. Assoc. Comput. Linguist.</source>
<volume>5</volume>, <fpage>135</fpage>–<lpage>146</lpage>. <pub-id pub-id-type="doi">10.1162/tacl_a_00051</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bollacker</surname><given-names>K.</given-names></name><name><surname>Evans</surname><given-names>C.</given-names></name><name><surname>Paritosh</surname><given-names>P.</given-names></name><name><surname>Sturge</surname><given-names>T.</given-names></name><name><surname>Taylor</surname><given-names>J.</given-names></name></person-group> (<year>2008</year>). <article-title>Freebase: a collaboratively created graph database for structuring human knowledge</article-title>, in <source>Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data</source> (<publisher-loc>ACM</publisher-loc>), <fpage>1247</fpage>–<lpage>1250</lpage>.</mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caracciolo</surname><given-names>C.</given-names></name><name><surname>Stellato</surname><given-names>A.</given-names></name><name><surname>Morshed</surname><given-names>A.</given-names></name><name><surname>Johannsen</surname><given-names>G.</given-names></name><name><surname>Rajbhandari</surname><given-names>S.</given-names></name><name><surname>Jaques</surname><given-names>Y.</given-names></name><etal/></person-group>. (<year>2013</year>). <article-title>The AGROVOC linked dataset</article-title>. <source>Semant. Web</source><volume>4</volume>, <fpage>341</fpage>–<lpage>348</lpage>. <pub-id pub-id-type="doi">10.3233/SW-130106</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>D.</given-names></name><name><surname>Manning</surname><given-names>C.</given-names></name></person-group> (<year>2014</year>). <article-title>A fast and accurate dependency parser using neural networks</article-title>, in <source>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</source>, <fpage>740</fpage>–<lpage>750</lpage>.</mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>H.</given-names></name><name><surname>Trouve</surname><given-names>A.</given-names></name><name><surname>Murakami</surname><given-names>K.</given-names></name><name><surname>Fukuda</surname><given-names>A.</given-names></name></person-group> (<year>2018</year>). <article-title>A concise conversion model for improving the RDF expression of conceptnet knowledge base</article-title>, in <source>Artificial Intelligence and Robotics</source>, eds X. Xu and H. Lu (<publisher-loc>Kitakyushu</publisher-loc>: <publisher-name>Springer Verlag</publisher-name>), <fpage>213</fpage>–<lpage>221</lpage>.</mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deerwester</surname><given-names>S.</given-names></name><name><surname>Dumais</surname><given-names>S. T.</given-names></name><name><surname>Furnas</surname><given-names>G. W.</given-names></name><name><surname>Landauer</surname><given-names>T. K.</given-names></name><name><surname>Harshman</surname><given-names>R.</given-names></name></person-group> (<year>1990</year>). <article-title>Indexing by latent semantic analysis</article-title>. <source>J. Am. Soc. Inform. Sci.</source>
<volume>41</volume>, <fpage>391</fpage>–<lpage>407</lpage>. <pub-id pub-id-type="doi">10.1002/(SICI)1097-4571(199009)41:6&lt;391::AID-ASI1&gt;3.0.CO;2-9</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dubey</surname><given-names>M.</given-names></name><name><surname>Banerjee</surname><given-names>D.</given-names></name><name><surname>Chaudhuri</surname><given-names>D.</given-names></name><name><surname>Lehmann</surname><given-names>J.</given-names></name></person-group> (<year>2018</year>). <article-title>EARL: joint entity and relation linking for question answering over knowledge graphs</article-title>, in <source>The Semantic Web - ISWC 2018</source> (<publisher-loc>Monterey, CA</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>), <fpage>108</fpage>–<lpage>126</lpage>.</mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ernst</surname><given-names>P.</given-names></name><name><surname>Siu</surname><given-names>A.</given-names></name><name><surname>Weikum</surname><given-names>G.</given-names></name></person-group> (<year>2018</year>). <article-title>Highlife: higher-arity fact harvesting</article-title>, in <source>Proceedings of the 2018 World Wide Web Conference on World Wide Web</source> (<publisher-loc>Lyon</publisher-loc>), <fpage>1013</fpage>–<lpage>1022</lpage>.</mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Faruqui</surname><given-names>M.</given-names></name><name><surname>Dodge</surname><given-names>J.</given-names></name><name><surname>Jauhar</surname><given-names>S. K.</given-names></name><name><surname>Dyer</surname><given-names>C.</given-names></name><name><surname>Hovy</surname><given-names>E.</given-names></name><name><surname>Smith</surname><given-names>N. A.</given-names></name></person-group> (<year>2015</year>). <article-title>Retrofitting word vectors to semantic lexicons</article-title>, in <source>Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</source> (<publisher-loc>Denver, CO</publisher-loc>: <publisher-name>Association for Computational Linguistics</publisher-name>), <fpage>1606</fpage>–<lpage>1615</lpage>.</mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finkelstein</surname><given-names>L.</given-names></name><name><surname>Gabrilovich</surname><given-names>E.</given-names></name><name><surname>Matias</surname><given-names>Y.</given-names></name><name><surname>Rivlin</surname><given-names>E.</given-names></name><name><surname>Solan</surname><given-names>Z.</given-names></name><name><surname>Wolfman</surname><given-names>G.</given-names></name><etal/></person-group>. (<year>2002</year>). <article-title>Placing search in context: the concept revisited</article-title>. <source>ACM Trans. Inform. Syst.</source><volume>20</volume>, <fpage>116</fpage>–<lpage>131</lpage>. <pub-id pub-id-type="doi">10.1145/503104.503110</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gharibi</surname><given-names>M.</given-names></name><name><surname>Rao</surname><given-names>P.</given-names></name><name><surname>Alrasheed</surname><given-names>N.</given-names></name></person-group> (<year>2018</year>). <article-title>RichRDF: a tool for enriching food, energy, and water datasets with semantically related facts and images</article-title>, in <source>International Semantic Web Conference (P&amp;D/Industry/BlueSky)</source> (<publisher-loc>Monterey, CA</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>).</mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Glavaš</surname><given-names>G.</given-names></name><name><surname>Vulicć</surname><given-names>I.</given-names></name></person-group> (<year>2018</year>). <article-title>Discriminating between lexico-semantic relations with the specialization tensor model</article-title>, in <source>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol. 2 (Short Papers)</source> (<publisher-loc>New Orleans, LA</publisher-loc>: <publisher-name>Association for Computational Linguistics</publisher-name>), <fpage>181</fpage>–<lpage>187</lpage>.</mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Grover</surname><given-names>A.</given-names></name><name><surname>Leskovec</surname><given-names>J.</given-names></name></person-group> (<year>2016</year>). <article-title>node2vec: Scalable feature learning for networks</article-title>, in <source>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</source> (<publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>ACM</publisher-name>), <fpage>855</fpage>–<lpage>864</lpage>.</mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hixon</surname><given-names>B.</given-names></name><name><surname>Clark</surname><given-names>P.</given-names></name><name><surname>Hajishirzi</surname><given-names>H.</given-names></name></person-group> (<year>2015</year>). <article-title>Learning knowledge graphs for question answering through conversational dialog</article-title>, in <source>Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</source> (<publisher-loc>Denver, CO</publisher-loc>: <publisher-name>Association for Computational Linguistics</publisher-name>), <fpage>851</fpage>–<lpage>861</lpage>.</mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iosif</surname><given-names>E.</given-names></name><name><surname>Potamianos</surname><given-names>A.</given-names></name></person-group> (<year>2010</year>). <article-title>Unsupervised semantic similarity computation between terms using web documents</article-title>. <source>IEEE Trans. Knowl. Data Eng.</source>
<volume>22</volume>, <fpage>1637</fpage>–<lpage>1647</lpage>. <pub-id pub-id-type="doi">10.1109/TKDE.2009.193</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Klein</surname><given-names>D.</given-names></name><name><surname>Manning</surname><given-names>C. D.</given-names></name></person-group> (<year>2003</year>). <article-title>Accurate unlexicalized parsing</article-title>, in <source>Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, Vol. 1</source> (<publisher-loc>Sapporo</publisher-loc>: <publisher-name>Association for Computational Linguistics</publisher-name>), <fpage>423</fpage>–<lpage>430</lpage>.</mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knoblock</surname><given-names>C. A.</given-names></name><name><surname>Szekely</surname><given-names>P.</given-names></name></person-group> (<year>2015</year>). <article-title>Exploiting semantics for big data integration</article-title>. <source>AI Mag.</source>
<volume>36</volume>, <fpage>25</fpage>–<lpage>39</lpage>. <pub-id pub-id-type="doi">10.1609/aimag.v36i1.2565</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Manning</surname><given-names>C.</given-names></name><name><surname>Surdeanu</surname><given-names>M.</given-names></name><name><surname>Bauer</surname><given-names>J.</given-names></name><name><surname>Finkel</surname><given-names>J.</given-names></name><name><surname>Bethard</surname><given-names>S.</given-names></name><name><surname>McClosky</surname><given-names>D.</given-names></name></person-group> (<year>2014</year>). <article-title>The stanford corenlp natural language processing toolkit</article-title>, in <source>Proceedings of 52nd Annual Meeting of the Association For Computational Linguistics: System Demonstrations</source> (<publisher-loc>Baltimore, MD</publisher-loc>), <fpage>55</fpage>–<lpage>60</lpage>.</mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martinez-Gil</surname><given-names>J.</given-names></name></person-group> (<year>2014</year>). <article-title>An overview of textual semantic similarity measures based on web intelligence</article-title>. <source>Artif. Intell. Rev.</source>
<volume>42</volume>, <fpage>935</fpage>–<lpage>943</lpage>. <pub-id pub-id-type="doi">10.1007/s10462-012-9349-8</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Meester</surname><given-names>B. D.</given-names></name></person-group> (<year>2018</year>). <article-title>High quality schema and data transformations for linked data generation</article-title>, in <source>Proceedings of the Doctoral Consortium, Part of CAiSEs</source> (<publisher-loc>Tallinn</publisher-loc>), <fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mikolov</surname><given-names>T.</given-names></name><name><surname>Chen</surname><given-names>K.</given-names></name><name><surname>Corrado</surname><given-names>G. S.</given-names></name><name><surname>Dean</surname><given-names>J.</given-names></name></person-group> (<year>2013a</year>). <article-title>Efficient estimation of word representations in vector space</article-title>. <source>arXiv:1301.3781</source>.</mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mikolov</surname><given-names>T.</given-names></name><name><surname>Sutskever</surname><given-names>I.</given-names></name><name><surname>Chen</surname><given-names>K.</given-names></name><name><surname>Corrado</surname><given-names>G.</given-names></name><name><surname>Dean</surname><given-names>J.</given-names></name></person-group> (<year>2013b</year>). <article-title>Distributed representations of words and phrases and their compositionality</article-title>, in <source>Proceedings of the 26th International Conference on Neural Information Processing Systems - Vol. 2</source> (<publisher-loc>Curran Associates Inc.</publisher-loc>), <fpage>3111</fpage>–<lpage>3119</lpage>.</mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>G. A.</given-names></name></person-group> (<year>1995</year>). <article-title>WordNet: a lexical database for english</article-title>. <source>Commun. ACM</source>
<volume>38</volume>, <fpage>39</fpage>–<lpage>41</lpage>. <pub-id pub-id-type="doi">10.1145/219717.219748</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mrkšić</surname><given-names>N.</given-names></name><name><surname>Vulić</surname><given-names>I.</given-names></name><name><surname>Séaghdha</surname><given-names>D. Ó.</given-names></name><name><surname>Leviant</surname><given-names>I.</given-names></name><name><surname>Reichart</surname><given-names>R.</given-names></name><name><surname>Gašić</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>Semantic specialization of distributional word vector spaces using monolingual and cross-lingual constraints</article-title>. <source>Trans. Assoc. Comput. Linguist.</source><volume>5</volume>, <fpage>309</fpage>–<lpage>324</lpage>. <pub-id pub-id-type="doi">10.1162/tacl_a_00063</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Myers</surname><given-names>L.</given-names></name><name><surname>Sirois</surname><given-names>M. J.</given-names></name></person-group> (<year>2004</year>). <article-title>Spearman correlation coefficients, differences between</article-title>. <source>Encyclop. Stat. Sci.</source> 12. <pub-id pub-id-type="doi">10.1002/0471667196.ess5050</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nadeau</surname><given-names>D.</given-names></name><name><surname>Sekine</surname><given-names>S.</given-names></name></person-group> (<year>2007</year>). <article-title>A survey of named entity recognition and classification</article-title>. <source>Lingvist. Invest.</source>
<volume>30</volume>, <fpage>3</fpage>–<lpage>26</lpage>. <pub-id pub-id-type="doi">10.1075/li.30.1.03nad</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Nickel</surname><given-names>M.</given-names></name><name><surname>Rosasco</surname><given-names>L.</given-names></name><name><surname>Poggio</surname><given-names>T.</given-names></name></person-group> (<year>2016</year>). <article-title>Holographic embeddings of knowledge graphs</article-title>, in <source>Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</source> (<publisher-loc>Phoenix, AZ</publisher-loc>: <publisher-name>AAAI Press</publisher-name>), <fpage>1955</fpage>–<lpage>1961</lpage>.</mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paulheim</surname><given-names>H.</given-names></name></person-group> (<year>2017</year>). <article-title>Knowledge graph refinement: a survey of approaches and evaluation methods</article-title>. <source>Semant. Web</source>
<volume>8</volume>, <fpage>489</fpage>–<lpage>508</lpage>. <pub-id pub-id-type="doi">10.3233/SW-160218</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pennington</surname><given-names>J.</given-names></name><name><surname>Socher</surname><given-names>R.</given-names></name><name><surname>Manning</surname><given-names>C. D.</given-names></name></person-group> (<year>2014</year>). <article-title>GloVe: global vectors for word representation</article-title>, in <source>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</source> (<publisher-loc>Doha</publisher-loc>: <publisher-name>Association for Computational Linguistics</publisher-name>), <fpage>1532</fpage>–<lpage>1543</lpage>.</mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Perozzi</surname><given-names>B.</given-names></name><name><surname>Al-Rfou</surname><given-names>R.</given-names></name><name><surname>Skiena</surname><given-names>S.</given-names></name></person-group> (<year>2014</year>). <article-title>Deepwalk: online learning of social representations</article-title>, in <source>Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</source> (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>ACM</publisher-name>), <fpage>701</fpage>–<lpage>710</lpage>.</mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>P.</given-names></name><name><surname>Katib</surname><given-names>A.</given-names></name><name><surname>Barron</surname><given-names>D. E. L.</given-names></name></person-group> (<year>2016</year>). <article-title>A knowledge ecosystem for the food, energy, and water system</article-title>. <source>arXiv:1609.05359</source>.</mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rozemberczki</surname><given-names>B.</given-names></name><name><surname>Davies</surname><given-names>R.</given-names></name><name><surname>Sarkar</surname><given-names>R.</given-names></name><name><surname>Sutton</surname><given-names>C.</given-names></name></person-group> (<year>2019</year>). <article-title>GEMSEC: graph embedding with self clustering</article-title>, in <source>Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2019</source> (<publisher-loc>Vancouver, BC</publisher-loc>: <publisher-name>ACM</publisher-name>), <fpage>65</fpage>–<lpage>72</lpage>.</mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russakovsky</surname><given-names>O.</given-names></name><name><surname>Deng</surname><given-names>J.</given-names></name><name><surname>Su</surname><given-names>H.</given-names></name><name><surname>Krause</surname><given-names>J.</given-names></name><name><surname>Satheesh</surname><given-names>S.</given-names></name><name><surname>Ma</surname><given-names>S.</given-names></name><etal/></person-group>. (<year>2015</year>). <article-title>ImageNet large scale visual recognition challenge</article-title>. <source>Int. J. Comput. Vis.</source><volume>115</volume>, <fpage>211</fpage>–<lpage>252</lpage>. <pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sachan</surname><given-names>D.</given-names></name><name><surname>Zaheer</surname><given-names>M.</given-names></name><name><surname>Salakhutdinov</surname><given-names>R.</given-names></name></person-group> (<year>2019</year>). <article-title>Revisiting Lstm networks for semi-supervised text classification via mixed objective function</article-title>. <source>Proc. AAAI Conf. Artif. Intell.</source>
<volume>33</volume>, <fpage>6940</fpage>–<lpage>6948</lpage>. <pub-id pub-id-type="doi">10.1609/aaai.v33i01.33016940</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schnabel</surname><given-names>T.</given-names></name><name><surname>Labutov</surname><given-names>I.</given-names></name><name><surname>Mimno</surname><given-names>D.</given-names></name><name><surname>Joachims</surname><given-names>T.</given-names></name></person-group> (<year>2015</year>). <article-title>Evaluation methods for unsupervised word embeddings</article-title>, in <source>Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</source> (<publisher-loc>Lisbon</publisher-loc>: <publisher-name>Association for Computational Linguistics</publisher-name>), <fpage>298</fpage>–<lpage>307</lpage>.</mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>W.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name><name><surname>Han</surname><given-names>J.</given-names></name></person-group> (<year>2015</year>). <article-title>Entity linking with a knowledge base: issues, techniques, and solutions</article-title>. <source>IEEE Trans. Knowl. Data Eng.</source>
<volume>27</volume>, <fpage>443</fpage>–<lpage>460</lpage>. <pub-id pub-id-type="doi">10.1109/TKDE.2014.2327028</pub-id></mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Smywiński-Pohl</surname><given-names>A.</given-names></name></person-group> (<year>2012</year>). <article-title>Classifying the Wikipedia articles into the opencyc taxonomy</article-title>, in <source>WoLE@ ISWC</source> (<publisher-loc>Boston, MA</publisher-loc>), <fpage>5</fpage>–<lpage>16</lpage>.</mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Speer</surname><given-names>R.</given-names></name><name><surname>Chin</surname><given-names>J.</given-names></name><name><surname>Havasi</surname><given-names>C.</given-names></name></person-group> (<year>2017</year>). <article-title>Conceptnet 5.5: an open multilingual graph of general knowledge</article-title>, in <source>Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</source> (<publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>AAAI Press</publisher-name>), <fpage>4444</fpage>–<lpage>4451</lpage>.</mixed-citation>
    </ref>
    <ref id="B40">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Suchanek</surname><given-names>F. M.</given-names></name><name><surname>Kasneci</surname><given-names>G.</given-names></name><name><surname>Weikum</surname><given-names>G.</given-names></name></person-group> (<year>2007</year>). <article-title>Yago: A core of semantic knowledge</article-title>, in <source>Proceedings of the 16th International Conference on World Wide Web</source> (<publisher-loc>ACM</publisher-loc>), <fpage>697</fpage>–<lpage>706</lpage>.</mixed-citation>
    </ref>
    <ref id="B41">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>J.</given-names></name><name><surname>Qu</surname><given-names>M.</given-names></name><name><surname>Wang</surname><given-names>M.</given-names></name><name><surname>Zhang</surname><given-names>M.</given-names></name><name><surname>Yan</surname><given-names>J.</given-names></name><name><surname>Mei</surname><given-names>Q.</given-names></name></person-group> (<year>2015</year>). <article-title>LINE: Large-scale information network embedding</article-title>, in <source>Proceedings of the 24th International Conference on World Wide Web</source> (<publisher-loc>Florence</publisher-loc>: <publisher-name>International World Wide Web Conferences Steering Committee</publisher-name>), <fpage>1067</fpage>–<lpage>1077</lpage>.</mixed-citation>
    </ref>
    <ref id="B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Maaten</surname><given-names>L.</given-names></name><name><surname>Hinton</surname><given-names>G.</given-names></name></person-group> (<year>2008</year>). <article-title>Visualizing data using t-SNE</article-title>. <source>J. Mach. Learn. Res.</source>
<volume>9</volume>, <fpage>2579</fpage>–<lpage>2605</lpage>.</mixed-citation>
    </ref>
    <ref id="B43">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Varelas</surname><given-names>G.</given-names></name><name><surname>Voutsakis</surname><given-names>E.</given-names></name><name><surname>Raftopoulou</surname><given-names>P.</given-names></name><name><surname>Petrakis</surname><given-names>E. G.</given-names></name><name><surname>Milios</surname><given-names>E. E.</given-names></name></person-group> (<year>2005</year>). <article-title>Semantic similarity methods in wordnet and their application to information retrieval on the web</article-title>, in <source>Proceedings of the 7th Annual ACM International Workshop on Web Information and Data Management</source> (<publisher-loc>Bremen</publisher-loc>: <publisher-name>ACM</publisher-name>), <fpage>10</fpage>–<lpage>16</lpage>.</mixed-citation>
    </ref>
    <ref id="B44">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Vashishth</surname><given-names>S.</given-names></name><name><surname>Jain</surname><given-names>P.</given-names></name><name><surname>Talukdar</surname><given-names>P.</given-names></name></person-group> (<year>2018</year>). <article-title>CESI: Canonicalizing open knowledge bases using embeddings and side information</article-title>, in <source>Proceedings of the 2018 World Wide Web Conference on World Wide Web</source> (<publisher-loc>Lyon</publisher-loc>: <publisher-name>International World Wide Web Conferences Steering Committee</publisher-name>), <fpage>1317</fpage>–<lpage>1327</lpage>.</mixed-citation>
    </ref>
    <ref id="B45">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Vulić</surname><given-names>I.</given-names></name><name><surname>Mrkšić</surname><given-names>N.</given-names></name></person-group> (<year>2018</year>). <article-title>Specialising word vectors for lexical entailment</article-title>, in <source>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol. 1 (Long Papers)</source> (<publisher-loc>New Orleans, LA</publisher-loc>: <publisher-name>Association for Computational Linguistics</publisher-name>), <fpage>1134</fpage>–<lpage>1145</lpage>.</mixed-citation>
    </ref>
    <ref id="B46">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X.</given-names></name><name><surname>Cui</surname><given-names>P.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name><name><surname>Pei</surname><given-names>J.</given-names></name><name><surname>Zhu</surname><given-names>W.</given-names></name><name><surname>Yang</surname><given-names>S.</given-names></name></person-group> (<year>2017</year>). <article-title>Community preserving network embedding</article-title>, in <source>Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</source> (<publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>AAAI Press</publisher-name>), <fpage>203</fpage>–<lpage>209</lpage>.</mixed-citation>
    </ref>
    <ref id="B47">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ye</surname><given-names>F.</given-names></name><name><surname>Chen</surname><given-names>C.</given-names></name><name><surname>Zheng</surname><given-names>Z.</given-names></name></person-group> (<year>2018</year>). <article-title>Deep autoencoder-like nonnegative matrix factorization for community detection</article-title>, in <source>Proceedings of the 27th ACM International Conference on Information and Knowledge Management</source> (<publisher-loc>Torino</publisher-loc>: <publisher-name>ACM</publisher-name>), <fpage>1393</fpage>–<lpage>1402</lpage>.</mixed-citation>
    </ref>
    <ref id="B48">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zesch</surname><given-names>T.</given-names></name><name><surname>Müller</surname><given-names>C.</given-names></name><name><surname>Gurevych</surname><given-names>I.</given-names></name></person-group> (<year>2008</year>). <article-title>Extracting lexical semantic knowledge from wikipedia and wiktionary</article-title>, in <source>Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC'08)</source> (<publisher-loc>Marrakech</publisher-loc>: <publisher-name>European Language Resources Association (ELRA</publisher-name>)), <fpage>1646</fpage>–<lpage>1652</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
