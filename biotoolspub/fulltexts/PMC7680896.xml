<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Neuroinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Neuroinform</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Neuroinform.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Neuroinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1662-5196</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7680896</article-id>
    <article-id pub-id-type="doi">10.3389/fninf.2020.572068</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Neuroscience</subject>
        <subj-group>
          <subject>Original Research</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Rxnat: An Open-Source R Package for XNAT-Based Repositories</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Gherman</surname>
          <given-names>Adrian</given-names>
        </name>
        <xref ref-type="corresp" rid="c001">
          <sup>*</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/1001551/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Muschelli</surname>
          <given-names>John</given-names>
        </name>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/55607/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Caffo</surname>
          <given-names>Brian</given-names>
        </name>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/8315/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Crainiceanu</surname>
          <given-names>Ciprian</given-names>
        </name>
      </contrib>
    </contrib-group>
    <aff><institution>Department of Biostatistics, Johns Hopkins Bloomberg School of Public Health</institution>, <addr-line>Baltimore, MD</addr-line>, <country>United States</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Daniel Marcus, Washington University in St. Louis, United States</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Christian Haselgrove, University of Massachusetts Medical School, United States; Hakim Christiaan Achterberg, Erasmus University Medical Center, Netherlands</p>
      </fn>
      <corresp id="c001">*Correspondence: Adrian Gherman <email>adig@jhu.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>09</day>
      <month>11</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2020</year>
    </pub-date>
    <volume>14</volume>
    <elocation-id>572068</elocation-id>
    <history>
      <date date-type="received">
        <day>12</day>
        <month>6</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>07</day>
        <month>10</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright Â© 2020 Gherman, Muschelli, Caffo and Crainiceanu.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Gherman, Muschelli, Caffo and Crainiceanu</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>The extensible neuroimaging archive toolkit (XNAT) is a common platform for storing and distributing neuroimaging data and is used by many key repositories of public neuroimaging data. Some examples include the Neuroimaging Informatics Tools and Resources Clearinghouse (NITRC, <ext-link ext-link-type="uri" xlink:href="https://nitrc.org/">https://nitrc.org/</ext-link>), the ConnectomeDB for the Human Connectome Project (<ext-link ext-link-type="uri" xlink:href="https://db.humanconnectome.org/">https://db.humanconnectome.org/</ext-link>), and XNAT Central (<ext-link ext-link-type="uri" xlink:href="https://central.xnat.org/">https://central.xnat.org/</ext-link>). We introduce Rxnat (<ext-link ext-link-type="uri" xlink:href="https://github.com/adigherman/Rxnat">https://github.com/adigherman/Rxnat</ext-link>), an open-source R package designed to interact with any XNAT-based repository. The program has similar capabilities with PyXNAT and XNATpy, which were developed for Python users. Rxnat was developed to address the increased popularity of R among neuroimaging researchers. The Rxnat package can query multiple XNAT repositories and download all or a specific subset of images for further processing. This provides a lingua franca for the large community of R analysts to interface with multiple XNAT-based publicly available neuroimaging repositories. The potential of Rxnat is illustrated using an example of neuroimaging data normalization from two neuroimaging repositories, NITRC and HCP.</p>
    </abstract>
    <kwd-group>
      <kwd>XNAT</kwd>
      <kwd>R</kwd>
      <kwd>neuroimaging</kwd>
      <kwd>nitrc</kwd>
      <kwd>connectome</kwd>
      <kwd>MRI</kwd>
      <kwd>normalization</kwd>
      <kwd>neuroconductor</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn001">Johns Hopkins Bloomberg School of Public Health<named-content content-type="fundref-id">10.13039/100008309</named-content></funding-source>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="3"/>
      <table-count count="1"/>
      <equation-count count="0"/>
      <ref-count count="36"/>
      <page-count count="12"/>
      <word-count count="6847"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>1. Introduction</title>
    <p>Medical imaging research is constantly evolving, which led to a dramatic increase in the availability, scale, and number of publicly-available image datasets. These datasets are heterogeneous, which raises substantial challenges for central organization and harmonization. Indeed, even downloading of relevant data from a number of different repositories is challenging because it requires multiple manual steps, which render the process essentially irreproducible. Moreover, repetitive downloading of datasets with different characteristics substantially increases the effort of the potential users.</p>
    <p>Various imaging data hosting platforms have recently emerged, with the most prominent being the eXtensible Neuroimaging Archive Toolkit, XNAT (<ext-link ext-link-type="uri" xlink:href="https://www.xnat.org">https://www.xnat.org</ext-link>) (Marcus et al., <xref rid="B11" ref-type="bibr">2007</xref>). XNAT is an open source imaging framework for neuroimaging informatics, which is structured as a database and contains procedures for storing, downloading, and querying the imaging data, as well as managing user level access permissions. Notably, XNAT has become a standard for the database backbone for dissemination of large public imaging repositories. Some examples of large image repositories that rely XNAT framework are (for more examples see <ext-link ext-link-type="uri" xlink:href="https://www.xnat.org/about/xnat-implementations.php">https://www.xnat.org/about/xnat-implementations.php</ext-link>):</p>
    <list list-type="bullet">
      <list-item>
        <p><bold>NITRC</bold> (<ext-link ext-link-type="uri" xlink:href="https://nitrc.org/">https://nitrc.org/</ext-link>). Neuroimaging Informatics Tools and Resources Clearinghouse is currently a free one-stop-shop collaborative resource for researchers who need neuroimaging analysis software, publicly available data sets, or computing power (Kennedy et al., <xref rid="B9" ref-type="bibr">2016</xref>).</p>
      </list-item>
      <list-item>
        <p><bold>ConnectomeDB</bold> (<ext-link ext-link-type="uri" xlink:href="https://db.humanconnectome.org/">https://db.humanconnectome.org/</ext-link>). The Human Connectome Project (HCP) is designed to construct a complete map of the structural and functional neural connections <italic>in vivo</italic> within and across individuals (Van Essen et al., <xref rid="B27" ref-type="bibr">2013</xref>).</p>
      </list-item>
      <list-item>
        <p><bold>XNAT Central</bold> (<ext-link ext-link-type="uri" xlink:href="https://central.xnat.org/">https://central.xnat.org/</ext-link>). XNAT Central is a database for sharing neuroimaging and related data with select collaborators or the general community (Herrick et al., <xref rid="B7" ref-type="bibr">2016</xref>).</p>
      </list-item>
    </list>
    <p>Most end users interact with XNAT via a graphical user interface (GUI), which enables researchers to download images. This interaction is manual and requires multiple point and click actions that substantially slow down image processing pipelines. Using a GUI can be difficult and time intensive, especially if a large number of images is required or multiple XNAT servers need to be queried. Thus, a software-based approach is necessary to eliminate the friction induced by the manual interaction with XNAT. Given the increased complexity, size, and number of projects that use XNAT, using software to interact with XNAT-based repositories can save time, improve workflow efficiency, and increase analytic reproducibility. Indeed, having public software that describes the data downloading process is more transparent and less prone to errors than describing the inclusion/exclusion criteria. Moreover, software can easily be adapted to extract different datasets, which substantially increases analytic efficiency.</p>
    <p>We developed the package Rxnat (<ext-link ext-link-type="uri" xlink:href="https://github.com/adigherman/Rxnat">https://github.com/adigherman/Rxnat</ext-link>) to interface with XNAT databases in the R programming language (R Core Team, <xref rid="B18" ref-type="bibr">2017</xref>). The package (1) authenticates the user, (2) queries and extracts information, and (3) downloads images and other data from these databases. The package enables users to navigate heterogeneous neuroimaging datasets using a unified syntax based on standard R data structures. We demonstrate the utility of Rxnat by describing the associated code and including an example of combining images from multiple sources in an analysis. The program has similar capabilities with PyXNAT (Schwartz et al., <xref rid="B20" ref-type="bibr">2012</xref>) and XNATpy (Achterberg, <xref rid="B1" ref-type="bibr">2015</xref>), which were developed for Python users. Rxnat is thus designed for a different analytic community than PyXNAT/XNATpy. It also interacts with the various analytic R packages in Neuroconductor (<ext-link ext-link-type="uri" xlink:href="https://neuroconductor.org/">https://neuroconductor.org/</ext-link>) (Muschelli et al., <xref rid="B16" ref-type="bibr">2018</xref>) and popular data manipulation packages such as <bold>dplyr</bold> (Wickham et al., <xref rid="B32" ref-type="bibr">2019</xref>). Using Rxnat allows to query specific subgroups of data from multiple XNAT-based image repositories, download results data in a standard format, and perform joint analyses. Thus, Rxnat provides the infrastructure to conduct complete analyses on one platform, R.</p>
  </sec>
  <sec sec-type="materials and methods" id="s2">
    <title>2. Materials and Methods</title>
    <p>Setting up an XNAT instance requires a server and specialized database expertise. We will not cover setting up XNAT servers here; instead, we will focus on how to interact with such servers. For more information of setting up an XNAT server, see <ext-link ext-link-type="uri" xlink:href="https://www.xnat.org/">https://www.xnat.org/</ext-link>.</p>
    <sec>
      <title>2.1. R as a Complete Analytic Platform</title>
      <p>Rxnat is an essential component of an analytic platform that starts with downloading public image data, pre-processing and analyzing it on one open source platform: R (R Core Team, <xref rid="B18" ref-type="bibr">2017</xref>). An advantage of R is that it is designed specifically for data analysis and has benefited from a large community development effort. While R is not the main programming language for image analysis, its community and ecosystem are developing rapidly (Tabelow and Whitcher, <xref rid="B23" ref-type="bibr">2011</xref>).</p>
      <p>Some of the advantages of having an integrated R platform are that: (1) it offers all of the primary components of modern interpreters and is at the forefront of data science software development; and (2) R provides support for object-oriented and functional programming.</p>
      <p>The computational time associated with neuroimaging software is of primary concern due to the large size of the imaging files and databases. Many imaging software tools are based on C++, which makes them fast and efficient. In contrast, R is a high level interpreted language that historically has been viewed as a slower alternative. However, R offers several avenues for utilizing compiled code. Most notably, the <bold>Rcpp</bold> package facilitates wrapping C++ code and libraries, which can create or adapt powerful and fast imaging operations. Also, R can be used as an interface and pipelining language, calling installed imaging packages from the command line, such as the <bold>fslr</bold> (Muschelli et al., <xref rid="B17" ref-type="bibr">2015</xref>) package that calls the FMRIB Software Library (<monospace>FSL</monospace>) (Jenkinson et al., <xref rid="B8" ref-type="bibr">2012</xref>). This interface of imaging packages is similar to the <bold>Nipype</bold> module in Python (Gorgolewski et al., <xref rid="B6" ref-type="bibr">2011</xref>). Both R and Python pipelining are extremely useful to combine traditional pre- and post-processing analytic steps into unified analytic pipelines.</p>
      <p>Recent efforts in data science have pushed R into the vanguard of conceptual thinking and implementation in this area. These efforts includes plotting, interactive graphics, reproducible research, data management and manipulation, dissemination and app development (Xie, <xref rid="B33" ref-type="bibr">2014</xref>; Tustison et al., <xref rid="B26" ref-type="bibr">2015</xref>; Wickham, <xref rid="B30" ref-type="bibr">2016</xref>; Sievert, <xref rid="B22" ref-type="bibr">2020</xref>). Specifically in neuroimaging data science, recent packages have increased R's capacity for static and interactive display of neuroimaging data and its analysis (Mowinckel and Vidal-PiÃ±eiro, <xref rid="B12" ref-type="bibr">2019</xref>; Fisher, <xref rid="B3" ref-type="bibr">2020</xref>; Muschelli and Gherman, <xref rid="B15" ref-type="bibr">2020</xref>).</p>
      <p>R provides a powerful package management system, which allows for a series of checks to be performed to ensure operability. The <bold>testthat</bold>, <bold>RUnit</bold> and other packages provide unit testing procedures for stability (Wickham, <xref rid="B29" ref-type="bibr">2011</xref>; Burger et al., <xref rid="B2" ref-type="bibr">2018</xref>). The comprehensive R archive network (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/">https://cran.r-project.org/</ext-link>) is primary source of general R packages. The R package management system has also allowed for the development of domain-specific package repositories that inspire collaboration and dissemination within more tightly coupled scientific domains. Perhaps the largest such example is Bioconductor, which focuses on computational biology (Gentleman et al., <xref rid="B5" ref-type="bibr">2004</xref>). Another effort is Neuroconductor, which is a domain specific R repository for image analysis (Muschelli et al., <xref rid="B16" ref-type="bibr">2018</xref>). Rxnat is a component package of Neuroconductor and was developed as a core utility by the Neuroconductor developers.</p>
      <p>For interaction with XNAT servers, R provides a number of options. The <bold>httr</bold> package provides an interface in R using the standard HTTP methods and verbs for interacting with RESTful APIs, similar to the <bold>requests</bold> module in Python (Wickham, <xref rid="B31" ref-type="bibr">2019</xref>). The <bold>RCurl</bold> and <bold>crul</bold> packages provide additional functionality, with the added benefit of using the popular curl-specific syntax (Temple Lang, <xref rid="B24" ref-type="bibr">2020</xref>). Thus, the packages <bold>httr</bold>, <bold>RCurl</bold>, and <bold>crul</bold> allow powerful queries and calls to XNAT servers.</p>
      <p>The introduction of the <bold>dplyr</bold> (Wickham et al., <xref rid="B32" ref-type="bibr">2019</xref>) R package allowed for more intuitive data manipulations steps within R. These steps include subsetting data on rows (<monospace>filter</monospace>) or columns (<monospace>select</monospace>), summarizing data (<monospace>summarize</monospace>), creating frequency tables (<monospace>count</monospace>), among others, all in a unified framework and data type (a <monospace>data.frame</monospace>). <bold>Rxnat</bold> allows users to query databases and return <monospace>data.frame</monospace>s. Thus, users who are familiar with <bold>dplyr</bold> commands can use Rxnat directly, without the need to learn the XNAT-specific querying syntax.</p>
    </sec>
    <sec>
      <title>2.2. XNAT</title>
      <p>XNAT is an open-source imaging informatics software platform dedicated to managing and distributing neuroimaging data. The under-laying framework uses XML for both database back-end operation as well as the front-end web interface. The primary set of XNAT features handle several core tasks:</p>
      <list list-type="bullet">
        <list-item>
          <p><bold>Organize and Share Data</bold> - Data stored in XNAT is associated with user defined projects and allows giving access to users on a project-by-project basis.</p>
        </list-item>
        <list-item>
          <p><bold>View and Download Data</bold> - XNAT includes an online image viewer that supports a number of common neuroimaging formats, including DICOM and Analyze.</p>
        </list-item>
        <list-item>
          <p><bold>Upload Data</bold> - XNAT offers a variety of methods to upload data including image and metadata importing directly from scanners, customized upload forms, and ZIP enabled uploaders.</p>
        </list-item>
        <list-item>
          <p><bold>Securing Access to Data</bold> - Quality control procedures provide secure ways to access the data as well as control its accessibility by fellow researchers and by the general public.</p>
        </list-item>
        <list-item>
          <p><bold>Search and Explore Data Sets</bold> - XNAT provides a web interface that allows users to store, retrieve, navigate, and query the imaging data.</p>
        </list-item>
        <list-item>
          <p><bold>Process Data</bold> - XNAT includes a powerful pipeline engine that allows the programming of complex workflows with multiple levels of automation.</p>
        </list-item>
      </list>
      <p>The Rxnat package is focused on query and data download as these two operations are most commonly used by brain imaging researchers looking to access XNAT imaging data. Here we focus on the download capabilities of Rxnat, though future releases will expand its upload capabilities.</p>
    </sec>
    <sec>
      <title>2.3. Integration With Neuroimaging in R</title>
      <sec>
        <title>2.3.1. Architecture and Design</title>
        <p>The Rxnat package relies on the R packages <bold>httr</bold>, <bold>RCurl</bold>, and <bold>crul</bold> to interact with an XNAT server. The XNAT REST API structure has a collection of resources that give access to both files (e.g., brain images) and metadata. Although the resources can be accessed through the XNAT search engine individually, running complex queries on aggregated data and downloading a certain subset of images/metadata is not possible natively (Marcus et al., <xref rid="B11" ref-type="bibr">2007</xref>). Using the Rxnat package functionality, a researcher will be able to combine multiple XNAT datasets, filter the aggregated data based on multiple criteria and download the results/images to be used in analysis pipelines.</p>
      </sec>
    </sec>
  </sec>
  <sec id="s3">
    <title>3. Examples</title>
    <sec>
      <title>3.1. Extracting Demographic Information From Multiple Image Repositories</title>
      <p>We show how to use the Rxnat package to select study participants aged 26â40 from both NITRC and HCP image repositories and download their magnetic resonance imaging (MRI) scans. Images are further processed in R using inhomogeneity correction, brain extraction, and tissue-class segmentation. Results of this processing are shown for one study participant for illustration purposes. Intensity normalization is then applied to each image and intensities distributions are compared before and after intensity normalization. The complete R code for this example can be found here: <ext-link ext-link-type="uri" xlink:href="https://raw.githubusercontent.com/adigherman/Rxnat/master/paper_code.R">https://raw.githubusercontent.com/adigherman/Rxnat/master/paper_code.R</ext-link>.</p>
      <sec>
        <title>3.1.1. Connect to NITRC and HCP</title>
        <p>The first step is to connect to each image repository using the <monospace>xnat_connect</monospace> function. Authentication is done using usernames and passwords. These values can be included in the command as <monospace>username</monospace> and <monospace>password</monospace> pairs. Alternatively, specifying <monospace>xnat_name</monospace> indicates which environment variables to use for authentication. For example, with the <monospace>nitrc</monospace> object below, as <monospace>xnat_name</monospace> is set to <monospace>âNITRCâ</monospace>, Rxnat will look in environment variables <monospace>NITRC_RXNAT_USER</monospace> and <monospace>NITRC_RXNAT_PASS</monospace> for authentication. This allows the code to be shared without revealing credentials and authentication.</p>
        <fig id="d39e503" position="float">
          <graphic xlink:href="fninf-14-572068-g0004"/>
        </fig>
        <p>The result of the <monospace>xnat_connect</monospace> function is an object of class <monospace>RXNATConnection</monospace>. We decided to use objects to store return query data as this will substantially reduce the time for subsequent operations and would not require to re-query the XNAT server. The object will not store images or other large data, but will facilitate the creation of subsequent Rxnat calls to download the data. The methods are classified into two main categories:</p>
        <list list-type="order">
          <list-item>
            <p>Internal methods required for keeping track of certain connection parameters as well as being able to perform internal operations/sanity checks on an <monospace>RXNATConnection</monospace> object.
<list list-type="bullet"><list-item><p><monospace>base_url</monospace> - returns the connection base URL and is used to create fully fledged URLs to download one or multiple images/resources.
<fig id="d39e526" position="float"><graphic xlink:href="fninf-14-572068-g0005"/></fig></p></list-item><list-item><p><monospace>xnat_name</monospace> - prints the name of the XNAT this objects is connected to and is used internally to create the name of the system environment variable that holds the username and password for authentication (in case the credentials are not passed as arguments when initializing an XNAT connection).
<fig id="d39e533" position="float"><graphic xlink:href="fninf-14-572068-g0006"/></fig></p></list-item><list-item><p><monospace>jsid</monospace> - outputs the JSESSIONID - a session identifier for the connection. This unique temporary identifier is used as an authentication token for all calls to the XNAT server.
<fig id="d39e540" position="float"><graphic xlink:href="fninf-14-572068-g0007"/></fig></p></list-item><list-item><p><monospace>close</monospace> - clears the JSESSIONID variable and closes the XNAT connection
<fig id="d39e547" position="float"><graphic xlink:href="fninf-14-572068-g0008"/></fig></p></list-item><list-item><p><monospace>is.connected</monospace> - checks the connection status signaling if a new connection needs to be established.
<fig id="d39e554" position="float"><graphic xlink:href="fninf-14-572068-g0009"/></fig></p></list-item></list></p>
          </list-item>
          <list-item>
            <p>Usability functions are the external facing Rxnat functions that allows users to interact with an XNAT repository by querying, filtering and downloading data.
<list list-type="bullet"><list-item><p><monospace>projects</monospace> - returns a tibble listing all XNAT projects that are accessible based on the supplied user credentials. This could potentially be only a subset of all projects hosted on
<fig id="d39e565" position="float"><graphic xlink:href="fninf-14-572068-g0010"/></fig></p><p>the XNAT server as privileges sometimes are granted on a per project basis.</p></list-item><list-item><p><monospace>subjects</monospace> - outputs a tibble listing all available subjects from all accessible projects. The listing also includes basic clinical information.
<fig id="d39e574" position="float"><graphic xlink:href="fninf-14-572068-g0011"/></fig></p></list-item><list-item><p><monospace>experiments</monospace> - returns a tibble listing all of the available experiments. The list is retrieved based on the provided user credentials and only lists the experiments associated with studies that are accessible.
<fig id="d39e581" position="float"><graphic xlink:href="fninf-14-572068-g0012"/></fig></p></list-item><list-item><p><monospace>get_xnat_experiment_resources</monospace> - returns a tibble with all the resources (files) associated with a particular experiment. The listing will provide information on the type of resource, size information as well as a URI (Uniform Resource identifier) to allow direct downloads of selected resources.
<fig id="d39e588" position="float"><graphic xlink:href="fninf-14-572068-g0013"/></fig></p></list-item><list-item><p><monospace>download_file</monospace> - downloads a single resource file. As an example, we will use the <inline-formula><mml:math id="M1"><mml:mrow><mml:mstyle mathvariant="bold" mathcolor="#5c6d8a"><mml:mtext>resources</mml:mtext></mml:mstyle></mml:mrow></mml:math></inline-formula> tibble from the previous method example to download the NIfTI MP-RAGE RAW image.
<fig id="d39e602" position="float"><graphic xlink:href="fninf-14-572068-g0014"/></fig></p></list-item><list-item><p><monospace>download_dir</monospace> - downloads multiple resource files at the same time
<fig id="d39e609" position="float"><graphic xlink:href="fninf-14-572068-g0015"/></fig></p></list-item><list-item><p><monospace>scans</monospace> - is used to filter the XNAT repository data and return a subset of subjects that match specific criteria. As an example we will query all NITRC projects for all subjects aged 26 that have at least one T2 type image.
<fig id="d39e616" position="float"><graphic xlink:href="fninf-14-572068-g0016"/></fig></p></list-item></list></p>
          </list-item>
        </list>
        <p>The amount of information contained in an XNAT server can be massive, so it is important to try to optimize the query/download time. An object of class <monospace>RXNATConnection</monospace> stores the projects, study participants, experiments, and scans query results internally, which substantially speeds up subsequent query operations. Thus, the initial connection call to an XNAT resource collects and stores some of the quickly retrievable data in the return object. This information is then used in every subsequent call without re-querying the XNAT server.</p>
      </sec>
      <sec>
        <title>3.1.2. Query and Download Subject Data</title>
        <p>Next, we query the NITRC and HCP repositories and acquire all patient data with the <monospace>subjects</monospace> method. We will then aggregate the data into one <monospace>data.frame</monospace> by appending the data by row:</p>
        <fig id="d39e634" position="float">
          <graphic xlink:href="fninf-14-572068-g0017"/>
        </fig>
        <p>Data seems to have information on certain demographic variables. However, a number of these variables are missing or may not be captured for this project:</p>
        <fig id="d39e638" position="float">
          <graphic xlink:href="fninf-14-572068-g0018"/>
        </fig>
        <p>When creating subsets of data based on querying individual variables different rules may be necessary, especially when aggregating multiple studies or projects. Also, some demographic variables are returned from XNAT using that query (such as age), that are not encoded in the subject-level data. Thus, using a combination of both queries and aggregate data may be necessary. This may be surprising, but in longitudinal studies some variables change over time (e.g., age, BMI, smoking status) whereas others (e.g., handedness) do not. Thus, the time-varying variables may be stored at the scan-level while the time-invariant variables may be stored at the subject-level.</p>
      </sec>
      <sec>
        <title>3.1.3. Extract Experiment Data</title>
        <p>For the purpose of this example, we focus on T1-weighted images, which are widely used for brain image segmentation. To retrieve the list of all experiments that have an associated T1 image, we use the <monospace>scans</monospace> function. NITRC stores T1 images under the T1 label, while HCP stores them under the T1w label. The data set is augmented to include the data source and image sequence, which is especially useful when combining multi-sequence data.</p>
        <fig id="d39e650" position="float">
          <graphic xlink:href="fninf-14-572068-g0019"/>
        </fig>
        <p>The <monospace>nitrc_T1_scan_resources</monospace> and <monospace>hcp_T1_scan_resources</monospace> data frames contain information associated with T1-weighted images available in the NITRC and HCP repositories, respectively. This information can be combined and then used to select the sample of interest. However, some studies may have different data encoding procedures. For example, some studies may collect age whereas other studies may collect the date of birth and visit date instead. Therefore, it is important to harmonize the common fields before starting the manipulation of the joined data.</p>
      </sec>
      <sec>
        <title>3.1.4. Filter Results to Select a Subgroup</title>
        <p>The next step is to aggregate the NITRC and HCP T1 resources/image information, join them with the subjects information, and filter for study participants with age between 26 and 40. Below <monospace>T1_resources</monospace> data frame combines the information about T1-weighted images with the subject-level data contained in the <monospace>all_subjects</monospace> data frame. The data frame <monospace>age_26_to_40_group</monospace> now contains all the necessary information to extract the specific subsample of interest.</p>
        <fig id="d39e674" position="float">
          <graphic xlink:href="fninf-14-572068-g0020"/>
        </fig>
        <p>Using these data it is now easy to produce (<xref rid="T1" ref-type="table">Table 1</xref>), which provides the gender distributions for the NITRC and HCP populations of 26- to 40-year-olds. In this age group NITRC has a higher percentage of male participants (56%) compared to HCP (34%). In addition to demographic and clinical information, scanning parameters may be wildly different across studies, and correction or adjustment of these differences may be necessary for analysis.</p>
        <table-wrap id="T1" position="float">
          <label>Table 1</label>
          <caption>
            <p>Gender distribution in the NITRC and HCP repositories for study participants between 26 and 40 years of age.</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th valign="top" align="left" rowspan="1" colspan="1">
                  <bold>XNAT server</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>Total subjects</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>Male</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>Female</bold>
                </th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td valign="top" align="left" colspan="4" rowspan="1">
                  <bold>Gender distribution age 26â40</bold>
                </td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">NITRC</td>
                <td valign="top" align="center" rowspan="1" colspan="1">132</td>
                <td valign="top" align="center" rowspan="1" colspan="1">75 (56%)</td>
                <td valign="top" align="center" rowspan="1" colspan="1">60 (44 %)</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">HCP</td>
                <td valign="top" align="center" rowspan="1" colspan="1">410</td>
                <td valign="top" align="center" rowspan="1" colspan="1">138 (34%)</td>
                <td valign="top" align="center" rowspan="1" colspan="1">272 (66%)</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
      <sec>
        <title>3.1.5. Image Processing Pipeline</title>
        <p><xref ref-type="supplementary-material" rid="SM1">Supplementary Figure 1</xref> displays the T1-weighted image from one of the study participants. The image still contains much of the mouth, neck, and brain stem, which need to be removed before applying standard imaging processing tools, such as brain segmentation. We illustrate our processing steps on this image, but the process is applied to the entire data set.</p>
        <p>The processing pipeline shown in <xref ref-type="fig" rid="F1">Figure 1</xref> will read and reorient the original T1 image, perform registration-based neck removal, brain extraction using a form of multi-atlas label fusion (MALF) (Wang et al., <xref rid="B28" ref-type="bibr">2012</xref>) brain segmentation and tissue class segmentation using the FAST algorithm (Zhang et al., <xref rid="B35" ref-type="bibr">2001</xref>).</p>
        <fig id="F1" position="float">
          <label>Figure 1</label>
          <caption>
            <p>Image processing pipeline: neck removal, inhomogeneity correction, skull stripping via registration and label fusion, tissue class segmentation, and intensity normalization across studies.</p>
          </caption>
          <graphic xlink:href="fninf-14-572068-g0001"/>
        </fig>
        <p>We now provide the major step of the software implementation. The first step is to download a sample T1-weighted image using the <monospace>download_dir</monospace> function. For this example we download an image from NITRC. The <monospace>experiment_ID</monospace> is a NITRC experiment identifier for a study participant between 26 and 40 years of age. The result of the download is a compressed directory, which is decompressed in a temporary location to access the files.</p>
        <fig id="d39e758" position="float">
          <graphic xlink:href="fninf-14-572068-g0021"/>
        </fig>
        <p>The <monospace>T1_image</monospace> object is a vector of files names from the temporary download folder and we will read in the 1st element which is the T1 weighted NIfTI file name.</p>
        <p>NB: The rest of this section describes an image processing and analysis pipeline of T1 weighted structural MRI, but it is not Rxnat specific. Please feel free to skip to the section 3.1.6 if desired.</p>
        <p>To ensure that all images have the same orientation, we use the <monospace>readrpi</monospace> function from the <monospace>fslr</monospace> package, which uses FSL to read and reorient a T1-weighted image.</p>
        <fig id="d39e775" position="float">
          <graphic xlink:href="fninf-14-572068-g0022"/>
        </fig>
        <p>The neck removal step is implemented using the <monospace>remove_neck</monospace> function from the <monospace>extrantsr</monospace> (Muschelli, <xref rid="B13" ref-type="bibr">2019</xref>) package. The empty image dimensions (including the neck slices) can be dropped by using the function <monospace>dropEmptyImageDimensions</monospace> from the <monospace>neurobase</monospace> (Muschelli, <xref rid="B14" ref-type="bibr">2020</xref>) package (<xref ref-type="fig" rid="F2">Figure 2A</xref>).</p>
        <fig id="d39e802" position="float">
          <graphic xlink:href="fninf-14-572068-g0023"/>
        </fig>
        <fig id="F2" position="float">
          <label>Figure 2</label>
          <caption>
            <p>Pipeline image processing. <bold>(A)</bold> T1-weighted image after bias-field correction and neck removal. <bold>(B)</bold> Brain mask (red) estimated using multi-atlas label fusion. <bold>(C)</bold> Brain image plotted next to a three-class tissue segmentation in white matter (WM, color-labeled white), gray matter (GM, color labeled gray), and cerebrospinal fluid (CSF, color labeled black).</p>
          </caption>
          <graphic xlink:href="fninf-14-572068-g0002"/>
        </fig>
        <p>Many MRIs contain a bias field, which is a low frequency, smooth, non-biological signal introduced by magnetic inhomogeneities. To correct the bias field signal we use the <monospace>bias_correct</monospace> function from the <monospace>extrantsr</monospace> package, which uses the N4 inhomogeneity correction (Tustison et al., <xref rid="B25" ref-type="bibr">2010</xref>).</p>
        <fig id="d39e831" position="float">
          <graphic xlink:href="fninf-14-572068-g0024"/>
        </fig>
        <p>Once images are bias field corrected, we apply brain extraction using a form of multi-atlas label fusion (MALF) (Wang et al., <xref rid="B28" ref-type="bibr">2012</xref>). MALF uses a collection of previously labeled brain images (atlases), aligns the T1-weighted image to each atlas, and obtains a labeled T1-weighted image for each registration. These labels are then combined, or fused, into one labeled map of the processed T1-weighted image. This approach is implemented using the <monospace>malf</monospace> function from the <monospace>malf.templates</monospace> package, which includes the templates from the 2012 MICCAI Multi-Atlas Labeling Challenge (Landman et al., <xref rid="B10" ref-type="bibr">2012</xref>). <xref ref-type="fig" rid="F2">Figure 2B</xref> displays the T1 image overlaid with the estimated brain mask using this approach indicating that the brain tissue is well estimated and extra-cranial areas are excluded.</p>
        <fig id="d39e850" position="float">
          <graphic xlink:href="fninf-14-572068-g0025"/>
        </fig>
        <p>All these steps an be done with fewer lines of code using the <monospace>preprocess_mri_within</monospace> function from the <monospace>extrantsr</monospace> package. This function performs N4 bias correction, image registration (if multi-sequence data is given), skull stripping (estimating the brain mask if one is not supplied), and brain mask application to the registered images. These steps have already been applied one-by-one, save for applying the brain mask, but this wrapper is useful for doing all the steps with one function, especially in cases where you are using multi-sequence data where registration is required.</p>
        <fig id="d39e860" position="float">
          <graphic xlink:href="fninf-14-572068-g0026"/>
        </fig>
        <p>Intensity normalization is an important component of image analysis, especially when results of image processing depend on voxel intensities or when one is interested in combining intensity information across individuals. For example, when images are thresholded for low values, we may want that threshold to have the same interpretation across scans. To achieve that we need to apply one of the many methods for intensity normalization; see Reinhold et al. (<xref rid="B19" ref-type="bibr">2019</xref>) for a discussion.</p>
        <p>We will use the WhiteStripe intensity normalization method introduced by Shinohara et al. (<xref rid="B21" ref-type="bibr">2014</xref>). This approach, estimates a small area in the tail of the T1-weighted image intensity distribution, labeled the âwhite stripe,â as it generally corresponds to white matter voxels. The mean and standard deviation (SD) of the voxel intensities in this area is calculated, and the image is z-scored by this mean/SD. WhiteStripe intensity normalization can be implemented using the <monospace>whitestripe</monospace> and <monospace>whitestripe_norm</monospace> functions from the <monospace>WhiteStripe</monospace> package.</p>
        <fig id="d39e881" position="float">
          <graphic xlink:href="fninf-14-572068-g0027"/>
        </fig>
        <p>After WhiteStripe normalization, the intensities of the brain image are interpreted as standard deviations from the mean of the normal-appearing white matter (NAWM). We would like to compare the effects of intensity normalization within tissues classes. To do that, one needs to perform tissue class segmentation on each image. This is implemented here using the <monospace>FAST</monospace> function from FSL. <monospace>FAST</monospace> segments a 3D brain image into different tissue types (GMâGray Matter, WMâWhite Matter, CSFâCerebrospinal Fluid). As our images are already N4 corrected, we will use the <monospace>fast_nobias</monospace> from the <monospace>fslr</monospace> package, which assumes that the bias field was removed. <xref ref-type="fig" rid="F2">Figure 2C</xref> displays the results of this segmentation.</p>
        <fig id="d39e901" position="float">
          <graphic xlink:href="fninf-14-572068-g0028"/>
        </fig>
        <p><xref ref-type="fig" rid="F3">Figure 3</xref> displays the tissue intensity densities in raw (first row) vs. WhiteStripe intensity normalized images (second row). The distribution of intensities for each study participant and tissue type is represented by one density (line) by tissue type: Cerebrospinal Fluid (CSF, left panels), Gray Matter (GM, middle panels), White Matter (WM, right panels). The density color coding corresponds to the different repositories: blue for NITRC and red for HCP. The densities of raw intensities can be clearly separated by repository, indicating that the raw units have a fundamentally different interpretation in the two studies, even when separated by tissue classes. These substantial differences are likely due to different scanning protocols, scanner types, and scanner manufacturers. Within each study there is a higher degree of overlap of MRI voxel intensity distributions, which is likely due to the study-specific scanning protocols and machines. The WhiteStripe-normalized data (second row in <xref ref-type="fig" rid="F3">Figure 3</xref>) indicates that the CSF and WM intensity distributions across all subjects and both studies are similar (see second row, right panels). For GM the overlap of distributions is much improved compared to the raw data. However, there is still separation between the studies, which may require additional normalization using, for example, RAVEL (Fortin et al., <xref rid="B4" ref-type="bibr">2017</xref>).</p>
        <fig id="F3" position="float">
          <label>Figure 3</label>
          <caption>
            <p>Tissue intensity densities in raw (first row) vs. WhiteStripe intensity normalized images (second row). The distribution of intensities for each study participant and tissue type is represented by one density (line) by tissue type: Cerebrospinal Fluid (CSF, <bold>left</bold>), Gray Matter (GM, <bold>middle</bold>), White Matter (WM, <bold>right</bold>). The density color coding corresponds to the different repositories: blue for NITRC and red for HCP.</p>
          </caption>
          <graphic xlink:href="fninf-14-572068-g0003"/>
        </fig>
      </sec>
      <sec>
        <title>3.1.6. Easier With an Interface</title>
        <p>The above image processing pipeline shows an analysis of data from multiple sources. Moreover, we stress that researchers should be careful with data aggregation as harmonization is almost surely required. While Rxnat was not the explicit focus, the full process, without a programmatic interface the process would be navigating and filtering based on the NITRC and HCP graphical interfaces. As the number of data sources grow, even if they are supported by XNAT, the probability of these interfaces being the same goes down drastically. Rxnat allows us to perform this data aggregation with reproducible scripts where the same interface works with multiple data sources, and demographic and clinical data can be analyzed and explored without downloading all individual data. To reiterate, the above analysis and data aggregation is possible without Rxnat, but is more difficult and not scripted.</p>
      </sec>
    </sec>
  </sec>
  <sec id="s4">
    <title>4. Discussion and Conclusion</title>
    <p>Large public data repositories have become focal points in neuroimaging research. Such resources can be used to: (1) conduct meta analyses and synthesis across studies (Yarkoni et al., <xref rid="B34" ref-type="bibr">2011</xref>); and (2) provide priors for data fusion with smaller local studies. Moreover, many repositories contain information about data replicability and reproducibility (Zuo et al., <xref rid="B36" ref-type="bibr">2014</xref>). Thus, they can be used to optimize processing pipelines and understand inter-subject and inter-site replicability.</p>
    <p>Public image databases often provide direct and relevant scientific information, particularly on establishing norms. Further, public databases are core resources for methodology development in neuroimaging. With easy access to these databases, a wider variety of researchers can access, use, and test emerging methodological approaches. Improving access to such data creates easier on ramps to quantitative neuroimaging research.</p>
    <p>As XNAT is the key format for image repositories, having Rxnat, an intuitive R interface and querying system native to R, can substantially accelerate this process. Rxnat can be used to assist existing researchers working in R and neuroimaging and lower the entry to computational neuroimaging in R.</p>
    <p>The Rxnat package substantially simplifies querying and downloading images from XNAT repositories for researchers familiar with R. Rxnat complements PyXNAT/XNATpy and lowers the computational barrier for a large number of analysts using R. Given the ever increasing number of large imaging data sets, the importance of packages, such as Rxnat, is likely to increase. Indeed, the most important achievement of Rxnat is that it allows the incorporation of the XNAT data into image processing pipelines. This circumvents the need to use Web GUIs to manually download the data. Moreover, because the downloading process is now scripted, the approach substantially improves the reproducibility of a study by having explicit lines of code for inclusion/exclusion criteria as well as direct calls to publicly available image repositories.</p>
    <p>For large scale studies, having a local image repository on a cluster is key for being able to generate results in a fast and secure way. Through the use of automated tasks, such as Unix cron jobs or the task scheduler for Windows systems, Rxnat can synchronize image repository with the upstream data and prepare it for <italic>ad-hoc</italic> analyses.</p>
    <p>As part of the Neuroconductor project, Rxnat is an important upstream component of the processing and analysis of neuroimaging data. Indeed, Rxnat provides the connection with XNAT repositories, while Neuroconductor provides various interfaces to popular imaging software packages such as FSL, AFNI (<ext-link ext-link-type="uri" xlink:href="https://afni.nimh.nih.gov/">https://afni.nimh.nih.gov/</ext-link>), MRICloud (<ext-link ext-link-type="uri" xlink:href="https://mricloud.org/">https://mricloud.org/</ext-link>), and ANTs (<ext-link ext-link-type="uri" xlink:href="http://stnava.github.io/ANTs/">http://stnava.github.io/ANTs/</ext-link>). As working with multiple R software packages software that are constantly changing can be difficult, we simplify the process by providing a Docker image for Neuroconductor (<ext-link ext-link-type="uri" xlink:href="https://neuroconductor.org/docker-release">https://neuroconductor.org/docker-release</ext-link>). Thus, we aim to provide data access tools as well as analysis pipelines, ready for users. Coupled with rapidly developing tools for reproducible research in R, Neuroconductor and its packages, including Rxnat, are becoming a viable option for end-to-end analyses of neuroimaging data using modern best practices.</p>
  </sec>
  <sec sec-type="data-availability" id="s5">
    <title>Data Availability Statement</title>
    <p>Publicly available datasets were analyzed in this study. This data can be found here: <ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org">https://www.nitrc.org</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://db.humanconnectome.org">https://db.humanconnectome.org</ext-link>.</p>
  </sec>
  <sec id="s6">
    <title>Ethics Statement</title>
    <p>Ethical review and approval was not required for the study on human participants in accordance with the local legislation and institutional requirements. The patients/participants provided their written informed consent to participate in this study.</p>
  </sec>
  <sec id="s7">
    <title>Author Contributions</title>
    <p>AG and JM wrote 90% of the manuscript and put together the example pipeline. AG was responsible for submitting the analysis to the cluster and aggregating the results. BC and CC oversaw the project, wrote the additional 10%, polished the manuscript, and managed the project. All authors contributed to the article and approved the submitted version.</p>
  </sec>
  <sec id="s8">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn fn-type="financial-disclosure">
      <p><bold>Funding.</bold> This work has been supported by the Department of Biostatistics at the Johns Hopkins Bloomberg School of Public Health, R01NS060910 grant from the National Institute of Neurological Disorders and Stroke (NINDS), and R01EB012547 from the National Institute of Biomedical Imaging and Bioengineering (NIBIB) at the National Institutes of Health (NIH).</p>
    </fn>
  </fn-group>
  <sec sec-type="supplementary-material" id="s9">
    <title>Supplementary Material</title>
    <p>The Supplementary Material for this article can be found online at: <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/articles/10.3389/fninf.2020.572068/full#supplementary-material">https://www.frontiersin.org/articles/10.3389/fninf.2020.572068/full#supplementary-material</ext-link></p>
    <supplementary-material content-type="local-data" id="SM1">
      <media xlink:href="Data_Sheet_1.PDF">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SM2">
      <media xlink:href="Data_Sheet_2.ZIP">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Achterberg</surname><given-names>H.</given-names></name></person-group> (<year>2015</year>). <source>XNATpy</source>. Python Module version 0.3.24.</mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burger</surname><given-names>M.</given-names></name><name><surname>Juenemann</surname><given-names>K.</given-names></name><name><surname>Koenig</surname><given-names>T.</given-names></name></person-group> (<year>2018</year>). <source>RUnit: R Unit Test Framework</source>. R package version 0.4.32.</mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fisher</surname><given-names>A.</given-names></name></person-group> (<year>2020</year>). <source>ggBrain: ggplot Brain Images</source>. R package version 0.1.2.</mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fortin</surname><given-names>J.-P.</given-names></name><name><surname>Parker</surname><given-names>D.</given-names></name><name><surname>TunÃ§</surname><given-names>B.</given-names></name><name><surname>Watanabe</surname><given-names>T.</given-names></name><name><surname>Elliott</surname><given-names>M. A.</given-names></name><name><surname>Ruparel</surname><given-names>K.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>Harmonization of multi-site diffusion tensor imaging data</article-title>. <source>Neuroimage</source><volume>161</volume>, <fpage>149</fpage>â<lpage>170</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.08.047</pub-id><?supplied-pmid 28826946?><pub-id pub-id-type="pmid">28826946</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gentleman</surname><given-names>R. C.</given-names></name><name><surname>Carey</surname><given-names>V. J.</given-names></name><name><surname>Bates</surname><given-names>D. M.</given-names></name><name><surname>Bolstad</surname><given-names>B.</given-names></name><name><surname>Dettling</surname><given-names>M.</given-names></name><name><surname>Dudoit</surname><given-names>S.</given-names></name><etal/></person-group>. (<year>2004</year>). <article-title>Bioconductor: open software development for computational biology and bioinformatics</article-title>. <source>Genome Biol</source>. <volume>5</volume>:<fpage>R80</fpage>. <pub-id pub-id-type="doi">10.1186/gb-2004-5-10-r80</pub-id><?supplied-pmid 15461798?><pub-id pub-id-type="pmid">15461798</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>K.</given-names></name><name><surname>Burns</surname><given-names>C. D.</given-names></name><name><surname>Madison</surname><given-names>C.</given-names></name><name><surname>Clark</surname><given-names>D.</given-names></name><name><surname>Halchenko</surname><given-names>Y. O.</given-names></name><name><surname>Waskom</surname><given-names>M. L.</given-names></name><etal/></person-group>. (<year>2011</year>). <article-title>Nipype: a flexible, lightweight and extensible neuroimaging data processing framework in Python</article-title>. <source>Front. Neuroinform</source>. <volume>5</volume>:<fpage>13</fpage>. <pub-id pub-id-type="doi">10.3389/fninf.2011.00013</pub-id><?supplied-pmid 21897815?><pub-id pub-id-type="pmid">21897815</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herrick</surname><given-names>R.</given-names></name><name><surname>Horton</surname><given-names>W.</given-names></name><name><surname>Olsen</surname><given-names>T.</given-names></name><name><surname>McKay</surname><given-names>M.</given-names></name><name><surname>Archie</surname><given-names>K. A.</given-names></name><name><surname>Marcus</surname><given-names>D. S.</given-names></name></person-group> (<year>2016</year>). <article-title>XNAT central: open sourcing imaging research data</article-title>. <source>Neuroimage</source>
<volume>124</volume>(<issue>Pt B</issue>), <fpage>1093</fpage>â<lpage>1096</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.06.076</pub-id><?supplied-pmid 26143202?><pub-id pub-id-type="pmid">26143202</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>M.</given-names></name><name><surname>Beckmann</surname><given-names>C. F.</given-names></name><name><surname>Behrens</surname><given-names>T. E. J.</given-names></name><name><surname>Woolrich</surname><given-names>M. W.</given-names></name><name><surname>Smith</surname><given-names>S. M.</given-names></name></person-group> (<year>2012</year>). <article-title>FSL</article-title>. <source>Neuroimage</source>
<volume>62</volume>, <fpage>782</fpage>â<lpage>790</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.09.015</pub-id><?supplied-pmid 21979382?><pub-id pub-id-type="pmid">21979382</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kennedy</surname><given-names>D. N.</given-names></name><name><surname>Haselgrove</surname><given-names>C.</given-names></name><name><surname>Riehl</surname><given-names>J.</given-names></name><name><surname>Preuss</surname><given-names>N.</given-names></name><name><surname>Buccigrossi</surname><given-names>R.</given-names></name></person-group> (<year>2016</year>). <article-title>The NITRC image repository</article-title>. <source>Neuroimage</source>
<volume>124</volume>, <fpage>1069</fpage>â<lpage>1073</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.05.074</pub-id><?supplied-pmid 26044860?><pub-id pub-id-type="pmid">26044860</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landman</surname><given-names>B. A.</given-names></name><name><surname>Ribbens</surname><given-names>A.</given-names></name><name><surname>Lucas</surname><given-names>B.</given-names></name><name><surname>Davatzikos</surname><given-names>C.</given-names></name><name><surname>Avants</surname><given-names>B.</given-names></name><name><surname>Ledig</surname><given-names>C.</given-names></name><etal/></person-group> (<year>2012</year>). <source>MICCAI 2012 Workshop on Multi-Atlas Labeling</source>. CreateSpace Independent Publishing Platform.</mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marcus</surname><given-names>D. S.</given-names></name><name><surname>Olsen</surname><given-names>T. R.</given-names></name><name><surname>Ramaratnam</surname><given-names>M.</given-names></name><name><surname>Buckner</surname><given-names>R. L.</given-names></name></person-group> (<year>2007</year>). <article-title>The Extensible Neuroimaging Archive Toolkit: an informatics platform for managing, exploring, and sharing neuroimaging data</article-title>. <source>Neuroinformatics</source>
<volume>5</volume>, <fpage>11</fpage>â<lpage>34</lpage>. <pub-id pub-id-type="doi">10.1385/NI:5:1:11</pub-id><?supplied-pmid 17426351?><pub-id pub-id-type="pmid">17426351</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mowinckel</surname><given-names>A. M.</given-names></name><name><surname>PiÃ±eiro</surname><given-names>D.</given-names></name></person-group> (<year>2019</year>). <article-title>Visualisation of brain statistics with R-packages ggseg and ggseg3d</article-title>. <source>arXiv [Preprint]. arXiv:1912.08200</source>.</mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muschelli</surname><given-names>J.</given-names></name></person-group> (<year>2019</year>). <source>extrantsr: Extra Functions to Build on the âANTsRâ Package</source>. R package version 3.9.8.1.</mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muschelli</surname><given-names>J.</given-names></name></person-group> (<year>2020</year>). <source>neurobase</source>: â<italic>Neuroconductorâ Base Package with Helper Functions for âniftiâ Objects</italic>. R package version 1.29.0.</mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muschelli</surname><given-names>J.</given-names></name><name><surname>Gherman</surname><given-names>A.</given-names></name></person-group> (<year>2020</year>). <source>papayaWidget: Embed an âPapayaâ Image Viewer</source>. R package version 0.7.1.</mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muschelli</surname><given-names>J.</given-names></name><name><surname>Gherman</surname><given-names>A.</given-names></name><name><surname>Fortin</surname><given-names>J. P.</given-names></name><name><surname>Avants</surname><given-names>B.</given-names></name><name><surname>Whitcher</surname><given-names>B.</given-names></name><name><surname>Clayden</surname><given-names>J. D.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>Neuroconductor: an R platform for medical imaging analysis</article-title>. <source>Biostatistics</source>
<volume>20</volume>, <fpage>218</fpage>â<lpage>239</lpage>. <pub-id pub-id-type="doi">10.1093/biostatistics/kxx068</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muschelli</surname><given-names>J.</given-names></name><name><surname>Sweeney</surname><given-names>E.</given-names></name><name><surname>Lindquist</surname><given-names>M.</given-names></name><name><surname>Crainiceanu</surname><given-names>C.</given-names></name></person-group> (<year>2015</year>). <article-title>fslr: Connecting the fsl software with R</article-title>. <source>R J</source>. <volume>7</volume>, <fpage>163</fpage>â<lpage>175</lpage>. <pub-id pub-id-type="doi">10.32614/RJ-2015-013</pub-id><?supplied-pmid 27330830?><pub-id pub-id-type="pmid">27330830</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><collab>R Core Team</collab></person-group> (<year>2017</year>). <source>R: A Language and Environment for Statistical Computing</source>. <publisher-loc>Vienna</publisher-loc>: <publisher-name>R Foundation for Statistical Computing</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Reinhold</surname><given-names>J. C.</given-names></name><name><surname>Dewey</surname><given-names>B. E.</given-names></name><name><surname>Carass</surname><given-names>A.</given-names></name><name><surname>Prince</surname><given-names>J. L.</given-names></name></person-group> (<year>2019</year>). <article-title>âEvaluating the impact of intensity normalization on MR image synthesis,â</article-title> in <source>Medical Imaging 2019: Image Processing</source> (<publisher-loc>International Society for Optics and Photonics</publisher-loc>). <pub-id pub-id-type="doi">10.1117/12.2513089</pub-id><?supplied-pmid 31551645?></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>Y.</given-names></name><name><surname>Barbot</surname><given-names>A.</given-names></name><name><surname>Thyreau</surname><given-names>B.</given-names></name><name><surname>Frouin</surname><given-names>V.</given-names></name><name><surname>Varoquaux</surname><given-names>G.</given-names></name><name><surname>Siram</surname><given-names>A.</given-names></name><etal/></person-group>. (<year>2012</year>). <article-title>PyXNAT: XNAT in Python</article-title>. <source>Front. Neuroinform</source>. <volume>6</volume>:<fpage>12</fpage>. <pub-id pub-id-type="doi">10.3389/fninf.2012.00012</pub-id><?supplied-pmid 22654752?><pub-id pub-id-type="pmid">22654752</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shinohara</surname><given-names>R. T.</given-names></name><name><surname>Sweeney</surname><given-names>E. M.</given-names></name><name><surname>Goldsmith</surname><given-names>J.</given-names></name><name><surname>Shiee</surname><given-names>N.</given-names></name><name><surname>Mateen</surname><given-names>F. J.</given-names></name><name><surname>Calabresi</surname><given-names>P. A.</given-names></name><etal/></person-group>. (<year>2014</year>). <article-title>Statistical normalization techniques for magnetic resonance imaging</article-title>. <source>Neuroimage Clin</source>. <volume>6</volume>, <fpage>9</fpage>â<lpage>19</lpage>. <pub-id pub-id-type="doi">10.1016/j.nicl.2014.08.008</pub-id><?supplied-pmid 25379412?><pub-id pub-id-type="pmid">25379412</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sievert</surname><given-names>C.</given-names></name></person-group> (<year>2020</year>). <source>Interactive Web-Based Data Visualization with R, Plotly, and Shiny</source>. Chapman and Hall/CRC. <pub-id pub-id-type="doi">10.1201/9780429447273</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tabelow</surname><given-names>K.</given-names></name><name><surname>Whitcher</surname><given-names>B.</given-names></name></person-group> (<year>2011</year>). <article-title>Special volume on magnetic resonance imaging in r</article-title>. <source>J. Stat. Softw</source>. <volume>44</volume>, <fpage>1</fpage>â<lpage>6</lpage>. <pub-id pub-id-type="doi">10.18637/jss.v044.i01</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Temple Lang</surname><given-names>D.</given-names></name></person-group> (<year>2020</year>). <source>RCurl: General Network (HTTP/FTP/...) Client Interface for R</source>. R package version 1.98-1.1.</mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tustison</surname><given-names>N. J.</given-names></name><name><surname>Avants</surname><given-names>B. B.</given-names></name><name><surname>Cook</surname><given-names>P. A.</given-names></name><name><surname>Zheng</surname><given-names>Y.</given-names></name><name><surname>Egan</surname><given-names>A.</given-names></name><name><surname>Yushkevich</surname><given-names>P. A.</given-names></name><etal/></person-group>. (<year>2010</year>). <article-title>N4ITK: improved N3 bias correction</article-title>. <source>IEEE Trans. Med. Imaging</source><volume>29</volume>, <fpage>1310</fpage>â<lpage>1320</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2010.2046908</pub-id><?supplied-pmid 20378467?><pub-id pub-id-type="pmid">20378467</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tustison</surname><given-names>N. J.</given-names></name><name><surname>Shrinidhi</surname><given-names>K.</given-names></name><name><surname>Wintermark</surname><given-names>M.</given-names></name><name><surname>Durst</surname><given-names>C. R.</given-names></name><name><surname>Kandel</surname><given-names>B. M.</given-names></name><name><surname>Gee</surname><given-names>J. C.</given-names></name><etal/></person-group>. (<year>2015</year>). <article-title>Optimal symmetric multimodal templates and concatenated random forests for supervised brain tumor segmentation (simplified) with ANTsR</article-title>. <source>Neuroinformatics</source><volume>13</volume>, <fpage>209</fpage>â<lpage>225</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-014-9245-2</pub-id><?supplied-pmid 25433513?><pub-id pub-id-type="pmid">25433513</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Essen</surname><given-names>D. C.</given-names></name><name><surname>Smith</surname><given-names>S. M.</given-names></name><name><surname>Barch</surname><given-names>D. M.</given-names></name><name><surname>Behrens</surname><given-names>T. E.</given-names></name><name><surname>Yacoub</surname><given-names>E.</given-names></name><name><surname>Ugurbil</surname><given-names>K.</given-names></name><etal/></person-group> (<year>2013</year>). <article-title>The WU-Minn human connectome project: an overview</article-title>. <source>Neuroimage</source>
<volume>80</volume>, <fpage>62</fpage>â<lpage>79</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.05.041</pub-id><pub-id pub-id-type="pmid">23684880</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>H.</given-names></name><name><surname>Suh</surname><given-names>J. W.</given-names></name><name><surname>Das</surname><given-names>S. R.</given-names></name><name><surname>Pluta</surname><given-names>J. B.</given-names></name><name><surname>Craige</surname><given-names>C.</given-names></name><name><surname>Yushkevich</surname><given-names>P. A.</given-names></name></person-group> (<year>2012</year>). <article-title>Multi-atlas segmentation with joint label fusion</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>. <volume>35</volume>, <fpage>611</fpage>â<lpage>623</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2012.143</pub-id><?supplied-pmid 22732662?><pub-id pub-id-type="pmid">22732662</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wickham</surname><given-names>H.</given-names></name></person-group> (<year>2011</year>). <article-title>testthat: get started with testing</article-title>. <source>R J</source>. <volume>3</volume>, <fpage>5</fpage>â<lpage>10</lpage>. <pub-id pub-id-type="doi">10.32614/RJ-2011-002</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wickham</surname><given-names>H.</given-names></name></person-group> (<year>2016</year>). <source>ggplot2: Elegant Graphics for Data Analysis</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>
<pub-id pub-id-type="doi">10.1007/978-3-319-24277-4_9</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wickham</surname><given-names>H.</given-names></name></person-group> (<year>2019</year>). <source>httr: Tools for Working with URLs and HTTP</source>. R package version 1.4.1.</mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wickham</surname><given-names>H.</given-names></name><name><surname>Francois</surname><given-names>R.</given-names></name><name><surname>Henry</surname><given-names>L.</given-names></name><name><surname>Muller</surname><given-names>K.</given-names></name></person-group> (<year>2019</year>). <source>dplyr: A Grammar of Data Manipulation</source>. R package version 0.8.3.</mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>Y.</given-names></name></person-group> (<year>2014</year>). <article-title>âknitr: a comprehensive tool for reproducible research in R,â</article-title> in <source>Implementing Reproducible Computational Research</source>, eds V. Stodden, F. Leisch, and R. D. Peng (Chapman and Hall/CRC).</mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarkoni</surname><given-names>T.</given-names></name><name><surname>Poldrack</surname><given-names>R. A.</given-names></name><name><surname>Nichols</surname><given-names>T. E.</given-names></name><name><surname>Van Essen</surname><given-names>D. C.</given-names></name><name><surname>Wager</surname><given-names>T. D.</given-names></name></person-group> (<year>2011</year>). <article-title>Large-scale automated synthesis of human functional neuroimaging data</article-title>. <source>Nat. Methods</source>
<volume>8</volume>:<fpage>665</fpage>. <pub-id pub-id-type="doi">10.1038/nmeth.1635</pub-id><?supplied-pmid 21706013?><pub-id pub-id-type="pmid">21706013</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Brady</surname><given-names>M.</given-names></name><name><surname>Smith</surname><given-names>S.</given-names></name></person-group> (<year>2001</year>). <article-title>Segmentation of brain MR images through a hidden Markov random field model and the expectation-maximization algorithm</article-title>. <source>IEEE Trans. Med. Imaging</source>
<volume>20</volume>, <fpage>45</fpage>â<lpage>57</lpage>. <pub-id pub-id-type="doi">10.1109/42.906424</pub-id><?supplied-pmid 11293691?><pub-id pub-id-type="pmid">11293691</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zuo</surname><given-names>X.-N.</given-names></name><name><surname>Anderson</surname><given-names>J. S.</given-names></name><name><surname>Bellec</surname><given-names>P.</given-names></name><name><surname>Birn</surname><given-names>R. M.</given-names></name><name><surname>Biswal</surname><given-names>B. B.</given-names></name><name><surname>Blautzik</surname><given-names>J.</given-names></name><etal/></person-group>. (<year>2014</year>). <article-title>An open science resource for establishing reliability and reproducibility in functional connectomics</article-title>. <source>Sci. Data</source><volume>1</volume>:<fpage>140049</fpage>.<?supplied-pmid 25977800?><pub-id pub-id-type="pmid">25977800</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
