<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9235472</article-id>
    <article-id pub-id-type="pmid">35758801</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btac222</article-id>
    <article-id pub-id-type="publisher-id">btac222</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>ISCB/Ismb 2022</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Systems Biology and Networks</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>MLGL-MP: a Multi-Label Graph Learning framework enhanced by pathway interdependence for Metabolic Pathway prediction</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Du</surname>
          <given-names>Bing-Xue</given-names>
        </name>
        <aff><institution>School of Life Sciences, Northwestern Polytechnical University</institution>, Xi’an 710072, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhao</surname>
          <given-names>Peng-Cheng</given-names>
        </name>
        <aff><institution>School of Life Sciences, Northwestern Polytechnical University</institution>, Xi’an 710072, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhu</surname>
          <given-names>Bei</given-names>
        </name>
        <aff><institution>School of Life Sciences, Northwestern Polytechnical University</institution>, Xi’an 710072, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yiu</surname>
          <given-names>Siu-Ming</given-names>
        </name>
        <aff><institution>Department of Computer Science, The University of Hong Kong</institution>, <country country="HK">Hong Kong 999077</country>, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Nyamabo</surname>
          <given-names>Arnold K</given-names>
        </name>
        <aff><institution>School of Computer Science, Northwestern Polytechnical University</institution>, Xi’an 710072, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Yu</surname>
          <given-names>Hui</given-names>
        </name>
        <aff><institution>School of Computer Science, Northwestern Polytechnical University</institution>, Xi’an 710072, <country country="CN">China</country></aff>
        <xref rid="btac222-cor1" ref-type="corresp"/>
        <!--huiyu@nwpu.edu.cn-->
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-2303-273X</contrib-id>
        <name>
          <surname>Shi</surname>
          <given-names>Jian-Yu</given-names>
        </name>
        <aff><institution>School of Life Sciences, Northwestern Polytechnical University</institution>, Xi’an 710072, <country country="CN">China</country></aff>
        <xref rid="btac222-cor1" ref-type="corresp"/>
        <!--jianyushi@nwpu.edu.cn-->
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btac222-cor1">To whom correspondence should be addressed. E-mail: <email>jianyushi@nwpu.edu.cn</email> or <email>huiyu@nwpu.edu.cn</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>7</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2022-06-27">
      <day>27</day>
      <month>6</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>27</day>
      <month>6</month>
      <year>2022</year>
    </pub-date>
    <volume>38</volume>
    <issue>Suppl 1</issue>
    <issue-title>ISCB ISMB 2022 Proceedings</issue-title>
    <fpage>i325</fpage>
    <lpage>i332</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2022. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btac222.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>During lead compound optimization, it is crucial to identify pathways where a drug-like compound is metabolized. Recently, machine learning-based methods have achieved inspiring progress to predict potential metabolic pathways for drug-like compounds. However, they neglect the knowledge that metabolic pathways are dependent on each other. Moreover, they are inadequate to elucidate why compounds participate in specific pathways.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>To address these issues, we propose a novel Multi-Label Graph Learning framework of Metabolic Pathway prediction boosted by pathway interdependence, called <bold>MLGL-MP</bold>, which contains a compound encoder, a pathway encoder and a multi-label predictor. The compound encoder learns compound embedding representations by graph neural networks. After constructing a pathway dependence graph by re-trained word embeddings and pathway co-occurrences, the pathway encoder learns pathway embeddings by graph convolutional networks. Moreover, after adapting the compound embedding space into the pathway embedding space, the multi-label predictor measures the proximity of two spaces to discriminate which pathways a compound participates in. The comparison with state-of-the-art methods on KEGG pathways demonstrates the superiority of our MLGL-MP. Also, the ablation studies reveal how its three components contribute to the model, including the pathway dependence, the adapter between compound embeddings and pathway embeddings, as well as the pre-training strategy. Furthermore, a case study illustrates the interpretability of MLGL-MP by indicating crucial substructures in a compound, which are significantly associated with the attending metabolic pathways. It is anticipated that this work can boost metabolic pathway predictions in drug discovery.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The code and data underlying this article are freely available at <ext-link xlink:href="https://github.com/dubingxue/MLGL-MP" ext-link-type="uri">https://github.com/dubingxue/MLGL-MP</ext-link>.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Nature Science Foundation of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>61872297</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Shaanxi Provincial Key Research &amp; Development Program, China</institution>
          </institution-wrap>
        </funding-source>
        <award-id>2020KW-063</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="8"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Enzymes catalyze drug or drug-like compounds into their metabolites, which differ significantly from these compounds themselves (<xref rid="btac222-B32" ref-type="bibr">Zhang and Tang, 2018</xref>). As a complex biotransformation, a compound metabolic pathway contains a set of interlocking enzymatic reactions (<xref rid="btac222-B12" ref-type="bibr">Jia <italic toggle="yes">et al.</italic>, 2020a</xref>). In drug discovery, it matters to identify what metabolic pathways a compound attends in the stage of lead compound optimization (<xref rid="btac222-B1" ref-type="bibr">Baranwal <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac222-B4" ref-type="bibr">Cho <italic toggle="yes">et al.</italic>, 2010</xref>; <xref rid="btac222-B25" ref-type="bibr">Sankar <italic toggle="yes">et al.</italic>, 2017</xref>). More importantly, there is a crucial need in drug design to understand why compounds attend specific metabolic pathways. However, since a compound (e.g. beta-Alanine) would attend one or more pathways (<xref rid="btac222-F1" ref-type="fig">Fig. 1</xref>), biological assays are always costly and time-consuming to identify pathways among a vast set of pathway combinations. In recent years, computational methods, especially machine learning-based methods, are promising to predict possible metabolic pathways rapidly for given compounds (<xref rid="btac222-B1" ref-type="bibr">Baranwal <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac222-B33" ref-type="bibr">Zhu <italic toggle="yes">et al.</italic>, 2021</xref>).</p>
    <fig position="float" id="btac222-F1">
      <label>Fig. 1.</label>
      <caption>
        <p>Illustration of compounds and their metabolic pathways. Labels <italic toggle="yes">l<sub>0</sub></italic>∼<italic toggle="yes">l<sub>10</sub></italic> represent different types of metabolic pathways. For example, beta-Alanine is metabolized by five pathways, labeled as <italic toggle="yes">l<sub>0</sub></italic>, <italic toggle="yes">l<sub>3</sub></italic>, <italic toggle="yes">l<sub>5</sub></italic>, <italic toggle="yes">l<sub>7</sub></italic> and <italic toggle="yes">l<sub>9</sub></italic>. Among them, pathway <italic toggle="yes">l<sub>7</sub></italic> is shared by Pentulose-5-phosphate and beta-Alanine while pathway <italic toggle="yes">l<sub>5</sub></italic> is shared by Phosphoenolpyruvate acid and beta-Alanine. Especially, pathway <italic toggle="yes">l<sub>0</sub></italic> is commonly shared by beta-Alanine, Pentulose-5-phosphate and Phosphoenolpyruvate acid. The list of metabolic pathway names can be found in <xref rid="btac222-T1" ref-type="table">Table 1</xref></p>
      </caption>
      <graphic xlink:href="btac222f1" position="float"/>
    </fig>
    <p>Former machine learning-based methods can be roughly categorized into network-based and classification-based. Network-based methods generally construct certain interaction networks and leverage network propagation algorithms to infer potential pathways for compounds. For example, <xref rid="btac222-B11" ref-type="bibr">Hu <italic toggle="yes">et al.</italic> (2011)</xref> constructed a network of chemical–chemical interactions (CCIs) to predict the association of a query compound to 11 kinds of metabolic pathways. As its extension, <xref rid="btac222-B7" ref-type="bibr">Gao <italic toggle="yes">et al.</italic> (2012)</xref> integrated three networks, involving CCIs, protein–protein interactions (PPIs) and chemical–protein interactions (CPIs) to predict metabolic pathways. Very recently, <xref rid="btac222-B33" ref-type="bibr">Zhu <italic toggle="yes">et al.</italic> (2021)</xref> proposed a heterogeneous network involving chemicals, enzymes, CCI, CPI and PPI information, where nodes are chemicals or enzymes, and edges are interactions between nodes. However, the major limitation of network-based methods cannot process the compounds which are isolated in the network.</p>
    <p>Classification-based methods leverage the technique of multi-label learning to infer potential metabolic pathways. Usually, there are three kinds of multi-label learning strategies. The first technique converts a multi-label classification into one or more binary classifications. For example, taking SVMs the binary classifier, <xref rid="btac222-B5" ref-type="bibr">Fang and Chen (2017)</xref> set compound-pathway pairs as samples where positive samples are validated pairs and negative samples are unknown pairs. <xref rid="btac222-B9" ref-type="bibr">Guo <italic toggle="yes">et al.</italic> (2018)</xref> first construct seven compound association networks from KEGG and STITCH. Then, they apply Random Walk with Restart to generate network embeddings, which further were fused as the final compound features. Last, they train a set of pathway-specific binary SVMs. As an extension of <xref rid="btac222-B9" ref-type="bibr">Guo <italic toggle="yes">et al.</italic> (2018)</xref>, <xref rid="btac222-B12" ref-type="bibr">Jia <italic toggle="yes">et al.</italic> (2020)</xref> recently use random forest (RF) as basic classifiers for accommodating more sub-types of pathways. However, these methods ignore the relationship between labels and aggregate the imbalance between positive and negative samples by generating many negative samples.</p>
    <p>The second sort uses a random k-label sets (RAKEL) (<xref rid="btac222-B26" ref-type="bibr">Tsoumakas <italic toggle="yes">et al.</italic>, 2007</xref>) to treat each kind of label combinations as a new label. Thus, it turns the multi-label classification into a multi-class classification. Compared to the previous binary classification, it provides the co-occurrence of multiple labels. For example, iMPT-FRAKEL (<xref rid="btac222-B13" ref-type="bibr">Jia <italic toggle="yes">et al.</italic>, 2020</xref>) encodes compounds by fingerprints and leverages a random k-label sets algorithm to tackle the multi-label classification by SVMs and RF. However, these methods also face the imbalance issue where new combined labels usually account for few samples.</p>
    <p>The last one directly performs a direct multi-label classification by deep learning. For example, <xref rid="btac222-B1" ref-type="bibr">Baranwal <italic toggle="yes">et al.</italic> (2020)</xref> and <xref rid="btac222-B30" ref-type="bibr">Yang <italic toggle="yes">et al.</italic> (2020)</xref> utilized graph neural networks (GNNs) [i.e. graph convolutional network (GCN) and GATs, respectively] to extract compound features based on 2D compound graphs and concatenated them with additional features derived from fingerprints (e.g. MACCS) and molecular properties (e.g. the number of aromatic rings, molecular weight and log <italic toggle="yes">P</italic>) as the final features. However, these methods ignore the dependence between pathways.</p>
    <p>In summary, although existing methods have achieved inspiring performance in metabolic pathway prediction, they neglect the dependence between pathways (i.e. pathway crosstalk in terms of biology). For example, both Lipid Metabolism (LM) and Carbohydrate Metabolism (CM) are affected by Metabolism of Cofactors and Vitamins (CVM). Deficiencies of vitamin B1, folic acid and vitamins B6 and B12 (in CVM) lead to a significant increment of lipid deposits in the aorta (in LM) (<xref rid="btac222-B14" ref-type="bibr">Kalyesubula <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac222-B18" ref-type="bibr">McNeil <italic toggle="yes">et al.</italic>, 2012</xref>). In addition, The B group vitamins help convert carbohydrates into energy in CM (<xref rid="btac222-B2" ref-type="bibr">Calderón-Ospina and Nava-Mesa, 2020</xref>). Moreover, such a pathway interdependence is asymmetric due to upstream/downstream relationships between pathways (<xref rid="btac222-B29" ref-type="bibr">Yan <italic toggle="yes">et al.</italic>, 2020</xref>). Therefore, the characterization of asymmetric interdependences among pathways would enhance the prediction task in the context that a compound participates in uncertain numbers of metabolic pathways.</p>
    <p>Furthermore, existing computational methods are inadequate to interpret why a compound attends a specific metabolic pathway (<xref rid="btac222-B1" ref-type="bibr">Baranwal <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac222-B30" ref-type="bibr">Yang <italic toggle="yes">et al.</italic>, 2020</xref>). In fact, metabolic pathways are usually related to the presence of certain chemical substructures. For example, amino and carboxylic substructures play an important role in binding to enzymes in Amino Acid Metabolism (<xref rid="btac222-B17" ref-type="bibr">Lopez and Mohiuddin, 2021</xref>). Therefore, the capture of crucial substructures (possibly revealing functional groups), will help reveal the mechanism of a compound metabolized by enzymes.</p>
    <p>To address the above issues (pathway interdependence and interpretability), we develop a Multi-Label Graph Learning framework enhanced by pathway interdependence for Metabolic Pathway prediction (MLGL-MP). This end-to-end framework contains a compound encoder, a pathway encoder and a multi-label predictor. The compound encoder learns compound embedding representations based on molecular graphs, while the pathway encoder learns pathway interdependence embeddings. The multi-label prediction discriminates which pathways a compound attends based on two kinds of embeddings. Overall, the main contributions of our MLGL-MP are as follows.
</p>
    <list list-type="order">
      <list-item>
        <p>It provides an interpretable manner to indicate crucial compound substructures which are significantly associated with metabolic pathways.</p>
      </list-item>
      <list-item>
        <p>By capturing the pathway interdependence, it significantly improves the characterization of the relevance between compounds and their metabolic pathways.</p>
      </list-item>
      <list-item>
        <p>It proposes a direct multi-pathway prediction approach by measuring the proximity between compounds and metabolic pathways in a common embedding space.</p>
      </list-item>
    </list>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Problem formulation and model construction</title>
      <p>Given <italic toggle="yes">m</italic> compounds <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> and a list<inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mi mathvariant="normal"> </mml:mi></mml:math></inline-formula>of metabolic pathways <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>. Suppose that a compound <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is assigned with a set of attending pathways <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⊆</mml:mo><mml:mi>T</mml:mi></mml:math></inline-formula>. The task is to predict the pathway set <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> of a newly coming compound <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, where <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>⊆</mml:mo><mml:mi>T</mml:mi></mml:math></inline-formula>. The prediction can be modeled as a problem of multi-label learning, which learns a function mapping <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mi mathvariant="script">F</mml:mi><mml:mo>:</mml:mo><mml:mi>M</mml:mi><mml:mo>→</mml:mo><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>. For this task, we design a multi-label graph learning framework, which contains a compound encoder, a pathway encoder and a multi-label predictor (shown in <xref rid="btac222-F2" ref-type="fig">Fig. 2</xref>).</p>
      <fig position="float" id="btac222-F2">
        <label>Fig. 2.</label>
        <caption>
          <p>The overall framework of MLGL-MP for multi-label metabolic pathway prediction. It is an end-to-end learning model, which contains a compound encoder, a pathway encoder and a multi-label predictor. The compound encoder generates compound embeddings based on molecular graphs by the composite of a GAT and a GCN. The pathway encoder generates pathway embeddings by two-layer GCNs on a pathway dependence graph, where nodes are pathways, node features are obtained by a pre-training strategy and edges are asymmetric pathway dependences. The multi-label predictor directly discriminates the metabolic pathways of given compounds by the proximity of pathway embeddings and compound embeddings</p>
        </caption>
        <graphic xlink:href="btac222f2" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>2.2 Compound encoder</title>
      <p>The compound encoder, adopting a two-layer GNN architecture, learns compound embedding representations by utilizing molecule graphs. Its first layer is a GAT (<xref rid="btac222-B27" ref-type="bibr">Veličković <italic toggle="yes">et al.</italic>, 2017</xref>), which capture the importance of chemical bonds to pathways. The second layer is a GCN (<xref rid="btac222-B15" ref-type="bibr">Kipf and Welling, 2017</xref>), which further extracts atom features by aggregating neighboring atom features and is followed by a global pooling layer (<xref rid="btac222-B20" ref-type="bibr">Nguyen <italic toggle="yes">et al.</italic>, 2021</xref>) to generate compound embeddings.</p>
      <p>According to chemical structure, each compound <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mi>c</mml:mi></mml:math></inline-formula> is represented as a molecule graph <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>, where <italic toggle="yes">V</italic> is the set of <italic toggle="yes">N</italic> atoms and <italic toggle="yes">E</italic> is a set of chemical bonds. Let <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mi mathvariant="bold">A</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> (<italic toggle="yes">N=|V|</italic>) be its adjacency matrix, in which <italic toggle="yes">a<sub>ij</sub> </italic>= 1 indicates the occurring bond between atom <italic toggle="yes">i</italic> and atom <italic toggle="yes">j</italic>, and <italic toggle="yes">a<sub>ij</sub></italic> = 0 indicates no bond. Here, each node <italic toggle="yes">v<sub>i</sub></italic> (atom) is initially represented by a <italic toggle="yes">q</italic>-dimensional binary feature vector <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>. As suggested in <xref rid="btac222-B20" ref-type="bibr">Nguyen <italic toggle="yes">et al.</italic> (2021)</xref>, the initial node features typically include the atom symbol, the number of adjacent atoms, the number of adjacent hydrogens, the implicit value of the atom and the atom occurrence in an aromatic structure.</p>
      <p>First of all, for each atom <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in the molecular graph <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mi>G</mml:mi></mml:math></inline-formula> of compound <italic toggle="yes">c</italic>, the GAT layer updates its features by aggregating the features of its neighboring atoms. The aggregation is implemented by a shared self-attention operation <italic toggle="yes">a</italic>: <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:msup><mml:mo>→</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:math></inline-formula>, which defines the importance of its neighboring atom <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to atom <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as follows:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">W</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math></disp-formula></p>
      <p>Here, the learnable weight matrix <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mi mathvariant="bold">W</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>×</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> accounts for a linear transformation from input features into higher-level features, where <italic toggle="yes">s</italic> is the dimension of updated atom features. Then, the updated features <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">'</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> of atom <italic toggle="yes">v<sub>i</sub></italic> can be defined by
<disp-formula id="E2"><label>(2)</label><mml:math id="M2" display="block" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">'</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>σ</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mo> </mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal"> </mml:mi></mml:math></inline-formula>= softmax<sub><italic toggle="yes">j</italic></sub> (<inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mi mathvariant="normal"> </mml:mi></mml:math></inline-formula>is the normalized attention coefficient and <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mo>σ</mml:mo><mml:mo>(</mml:mo><mml:mo>·</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula> is a non-linear activation function (i.e. ReLU). The normalization of attention coefficients makes themselves comparable among different nodes.</p>
      <p>Furthermore, a multihead attention is adopted to enhance the expression of the attention layer. Specifically, <italic toggle="yes">K</italic> independent attention mechanisms are performed in parallel and their features are concatenated as the updated features <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> of atom <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as follows:
<disp-formula id="E3"><label>(3)</label><mml:math id="M3" display="block" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mi>K</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mo>∥</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>σ</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mo> </mml:mo></mml:mrow></mml:mrow><mml:msubsup><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>K</mml:mi><mml:mi>s</mml:mi></mml:math></inline-formula> is the dimension of updated atom features, <italic toggle="yes">K</italic> is the number of attention heads, and <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mo>∥</mml:mo></mml:math></inline-formula> is the concatenation operation of vectors. It is remarkable that the average of <inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:math></inline-formula> accounts for the importance of the chemical bond between atom <italic toggle="yes">v<sub>i</sub></italic> and atom <italic toggle="yes">v<sub>j</sub></italic>.</p>
      <p>After that, the GCN layer following the GAT layer further updates atom features by emphasizing the topology of molecule graph. According to the propagation rules of GCN, the updated features <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>}</mml:mo></mml:math></inline-formula> of atoms of compound <italic toggle="yes">c</italic> is determined by the following matrix form:
<disp-formula id="E4"><label>(4)</label><mml:math id="M4" display="block" overflow="scroll"><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>σ</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>K</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is the GAT-based feature matrix stacked by <inline-formula id="IE31"><mml:math id="IM31" display="inline" overflow="scroll"><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>}</mml:mo></mml:math></inline-formula>, <inline-formula id="IE32"><mml:math id="IM32" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is the GCN-updating feature matrix stacked by the updated atom feature vector <inline-formula id="IE33"><mml:math id="IM33" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, <inline-formula id="IE34"><mml:math id="IM34" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>∈</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>s</mml:mi><mml:mo>×</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is the weight matrix in the GCN layer, <inline-formula id="IE35"><mml:math id="IM35" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula id="IE36"><mml:math id="IM36" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the identity matrix, <bold>D</bold>  <inline-formula id="IE37"><mml:math id="IM37" display="inline" overflow="scroll"><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is the degree matrix, in which diagonal elements are the degrees of each vertex and <inline-formula id="IE38"><mml:math id="IM38" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">D</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">ii</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo> </mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">ij</mml:mi><mml:mi mathvariant="normal"> </mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p>
      <p>Once the atom embedding vectors <inline-formula id="IE39"><mml:math id="IM39" display="inline" overflow="scroll"><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:math></inline-formula> of compound <italic toggle="yes">c</italic> are obtained, a readout operation finally turns them into the embedding vector of the compound. In the readout, both a global Max-pooling and a global Mean-pooling are performed in parallel. Their resulting embeddings, <inline-formula id="IE40"><mml:math id="IM40" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">argmax</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mi mathvariant="normal"> </mml:mi><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE41"><mml:math id="IM41" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">mean</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">mean</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, are concatenated as the final embedding <inline-formula id="IE42"><mml:math id="IM42" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> of compound <italic toggle="yes">c</italic>.</p>
    </sec>
    <sec>
      <title>2.3 Pathway encoder</title>
      <p>The metabolic pathway encoder, containing a two-layer GCN architecture, learns pathway embedding representations by constructing a pathway dependence graph, which characterizes asymmetric pathway interdependence. In such a graph, nodes are pathways <inline-formula id="IE43"><mml:math id="IM43" display="inline" overflow="scroll"><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> and directed edges are their asymmetric dependences. A compound <inline-formula id="IE44"><mml:math id="IM44" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is assigned with a set of attending pathways <inline-formula id="IE45"><mml:math id="IM45" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⊆</mml:mo><mml:mi>T</mml:mi></mml:math></inline-formula>.</p>
      <p>Inspired by <xref rid="btac222-B3" ref-type="bibr">Chen <italic toggle="yes">et al.</italic> (2019)</xref>, the construction of the graph includes two phases, node initialization and edge building. The node initialization assigns initial node features. However, they cannot be directly learned in an end-to-end manner due to the small number of pathways (i.e. 11 pathways in the benchmark dataset). To address this issue, a pre-training strategy is adopted to obtain initial pathway embeddings. Considering that pathway names are semantic, we apply GloVe, a word representation tool, to implement the pre-training (<xref rid="btac222-B22" ref-type="bibr">Pennington <italic toggle="yes">et al.</italic>, 2014</xref>). For the pathway set, GloVe generates initial pathway embeddings <inline-formula id="IE46"><mml:math id="IM46" display="inline" overflow="scroll"><mml:mo>{</mml:mo><mml:mi mathvariant="bold">q</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msup><mml:mo>}</mml:mo></mml:math></inline-formula> by learning word embeddings of semantic pathway names based on the Common Crawl dataset (<ext-link xlink:href="https://nlp.stanford.edu/projects/glove/" ext-link-type="uri">https://nlp.stanford.edu/projects/glove/</ext-link>).</p>
      <p>The edge building contains three steps to capture the pathway interdependence. First, the pathway co-occurrence is calculated based on the dataset in hand. Define the co-occurrence matrix as <inline-formula id="IE47"><mml:math id="IM47" display="inline" overflow="scroll"><mml:mi mathvariant="bold">U</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>×</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, where <italic toggle="yes">u<sub>i,j</sub></italic> is the pairwise co-occurrence counts between pathway <italic toggle="yes">t<sub>i</sub></italic> and <italic toggle="yes">t<sub>j</sub></italic>, <italic toggle="yes">u<sub>i,j</sub></italic> = <italic toggle="yes">u<sub>j,i</sub></italic> and <italic toggle="yes">C</italic> denotes the number of metabolic pathway types.</p>
      <p>Then, a probability matrix <bold>P</bold> is calculated by the co-occurrence matrix <inline-formula id="IE48"><mml:math id="IM48" display="inline" overflow="scroll"><mml:mi mathvariant="bold">U</mml:mi></mml:math></inline-formula>. Define <italic toggle="yes">p<sub>ij</sub></italic> = <italic toggle="yes">p</italic> (<italic toggle="yes">t<sub>j</sub></italic>|<italic toggle="yes">t<sub>i</sub></italic>) as the probability of <italic toggle="yes">t<sub>j</sub></italic> occurring when given <italic toggle="yes">t<sub>i</sub></italic> occurring, and <inline-formula id="IE50"><mml:math id="IM49" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal"> </mml:mi></mml:math></inline-formula>as the total occurrence count of <italic toggle="yes">t<sub>i</sub></italic>. The probability matrix <italic toggle="yes">p<sub>ij</sub></italic> can be calculated by = <italic toggle="yes">u<sub>i,j</sub></italic>/<italic toggle="yes">N<sub>i</sub></italic> . Note that <italic toggle="yes">P<sub>ij</sub></italic> is usually not equal to <italic toggle="yes">P<sub>ji</sub></italic>.</p>
      <p>As <xref rid="btac222-B3" ref-type="bibr">Chen <italic toggle="yes">et al.</italic> (2019)</xref> suggested, a binarization of <bold>P</bold> is performed to address the possible long-tail distribution of co-occurrence patterns, where a few {<italic toggle="yes">P<sub>ij</sub></italic>}are significantly greater than others. Formally, the conditional probability matrix <bold>P</bold> is turned to a binary matrix<inline-formula id="IE52"><mml:math id="IM50" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi mathvariant="normal"> </mml:mi><mml:mi mathvariant="bold">M</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>}</mml:mo><mml:mo>∈</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>×</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> by a hard threshold <inline-formula id="IE53"><mml:math id="IM51" display="inline" overflow="scroll"><mml:mo>τ</mml:mo></mml:math></inline-formula> as:
<disp-formula id="E5"><label>(5)</label><mml:math id="M5" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mo> </mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mo>τ</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mo> </mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mo>τ</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math></disp-formula></p>
      <p>This binarization removes trivial edges, which could be noise.</p>
      <p>Last, to enhance the node distinguishability during the aggregation of neighboring nodes, an extra re-weighting scheme is utilized (<xref rid="btac222-B16" ref-type="bibr">Li <italic toggle="yes">et al.</italic>, 2018</xref>) as follows:
<disp-formula id="E6"><label>(6)</label><mml:math id="M6" display="block" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mi mathvariant="normal">*</mml:mi><mml:mi mathvariant="bold">M</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math></disp-formula>where <bold>1</bold>  <inline-formula id="IE54"><mml:math id="IM52" display="inline" overflow="scroll"><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula id="IE55"><mml:math id="IM53" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is the re-weighted dependence matrix.</p>
      <p>Once the pathway interdependence graph is constructed, two layers of GCNs are used to learn pathway embeddings. Denote <inline-formula id="IE56"><mml:math id="IM54" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>as the initial feature vector of a given pathway <inline-formula id="IE57"><mml:math id="IM55" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula id="IE58"><mml:math id="IM56" display="inline" overflow="scroll"><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>×</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> as the initial pathway feature matrix stacked by <inline-formula id="IE59"><mml:math id="IM57" display="inline" overflow="scroll"><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>}</mml:mo></mml:math></inline-formula>. Passing through a GCN layer, <inline-formula id="IE60"><mml:math id="IM58" display="inline" overflow="scroll"><mml:mi mathvariant="bold">Q</mml:mi></mml:math></inline-formula> is updated by the following propagation rule:
<disp-formula id="E7"><label>(7)</label><mml:math id="M7" display="block" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>σ</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mi mathvariant="normal"> </mml:mi><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>0,1</mml:mn><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula id="IE61"><mml:math id="IM59" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>α</mml:mo><mml:mi mathvariant="normal">*</mml:mi><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula id="IE62"><mml:math id="IM60" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>×</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is its degree matrix where <inline-formula id="IE63"><mml:math id="IM61" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="normal"> </mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">ii</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo> </mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>, <inline-formula id="IE64"><mml:math id="IM62" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>×</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal"> </mml:mi></mml:math></inline-formula>is the embedding matrix in the <inline-formula id="IE65"><mml:math id="IM63" display="inline" overflow="scroll"><mml:mi mathvariant="italic">lth</mml:mi><mml:mo> </mml:mo></mml:math></inline-formula>layer, <inline-formula id="IE66"><mml:math id="IM64" display="inline" overflow="scroll"><mml:mo>α</mml:mo><mml:mo>∈</mml:mo><mml:mo>[</mml:mo><mml:mn>0,1</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula> is a trade-off coefficient, which determines how neighboring nodes are emphasized in the convolutional aggregation.</p>
      <p>In addition, <inline-formula id="IE67"><mml:math id="IM65" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the identity matrix, <inline-formula id="IE68"><mml:math id="IM66" display="inline" overflow="scroll"><mml:mo>σ</mml:mo><mml:mo>(</mml:mo><mml:mo>·</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula> denotes an LeakyReLU activation function and <inline-formula id="IE69"><mml:math id="IM67" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> is a transformation weight matrix to be learned. Specifically, the input pathway feature matrix <inline-formula id="IE70"><mml:math id="IM68" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi mathvariant="bold">Q</mml:mi></mml:math></inline-formula>, while the output of the second layer <inline-formula id="IE71"><mml:math id="IM69" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> is just the pathway embedding feature matrix, denoted as <inline-formula id="IE72"><mml:math id="IM70" display="inline" overflow="scroll"><mml:mi mathvariant="bold">O</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>×</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, where <italic toggle="yes">B</italic> is the dimension of pathway embedding features. The dimension can be same as that of compound embedding features for the further purpose of measuring the proximity between pathways and compounds.</p>
    </sec>
    <sec>
      <title>2.4 Multi-label predictor</title>
      <p>After obtaining compound embeddings and pathway embeddings, we can directly perform discriminate the pathway set, in which a compound could attend. Inspired by <xref rid="btac222-B3" ref-type="bibr">Chen <italic toggle="yes">et al.</italic> (2019)</xref>, we measure the proximities between a given compound and a list of pathways as follows:
<disp-formula id="E8"><label>(8)</label><mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="bold">O</mml:mi><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula id="IE73"><mml:math id="IM71" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> is the embedding vector of a compound <italic toggle="yes">c</italic> and <inline-formula id="IE74"><mml:math id="IM72" display="inline" overflow="scroll"><mml:mi mathvariant="bold">O</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>×</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is the pathway embedding feature matrix, of which each row denotes the embedding of pathway <italic toggle="yes">j</italic>. The proximity <inline-formula id="IE75"><mml:math id="IM73" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the predicting score of the given compound attending in the <italic toggle="yes">i</italic>-th pathway among the pathway list <inline-formula id="IE76"><mml:math id="IM74" display="inline" overflow="scroll"><mml:mi>T</mml:mi></mml:math></inline-formula>.</p>
      <p>However, such a direct proximity measure would be senseless since the compound embedding space and the pathway embedding space are of different vector spaces. To address this issue, we design an adapter to map the compound embedding space into the pathway embedding space. The adapter can be implemented by a dense neural network (DNN) containing an input layer, a hidden layer, and an output layer. Thus, the final compound representation feature is defined as <inline-formula id="IE77"><mml:math id="IM75" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">DNN</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo>)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>.</p>
      <p>Last, the multi-label classification loss (<xref rid="btac222-B10" ref-type="bibr">He <italic toggle="yes">et al.</italic>, 2021</xref>) is used when training MLGL-MP. It is defined as follows:
<disp-formula id="E9"><label>(9)</label><mml:math id="M9" display="block" overflow="scroll"><mml:mi mathvariant="italic">Loss</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mfrac><mml:mi mathvariant="normal">*</mml:mi><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">*</mml:mi><mml:mi mathvariant="italic">log</mml:mi><mml:mo>⁡</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:msup><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mi mathvariant="normal">*</mml:mi><mml:mi mathvariant="italic">log</mml:mi><mml:mo>⁡</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula id="IE78"><mml:math id="IM76" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mn>1</mml:mn></mml:math></inline-formula>} are a true label indicating whether or not the compound participates in pathway <italic toggle="yes">i</italic>, and <inline-formula id="IE79"><mml:math id="IM77" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the corresponding confidence score output by MLGL-MP.</p>
    </sec>
    <sec>
      <title>2.5 Evaluation metrics</title>
      <p>To evaluate the performance of multi-label learning models, we follow the conventional settings in <xref rid="btac222-B1" ref-type="bibr">Baranwal <italic toggle="yes">et al.</italic> (2020)</xref> and <xref rid="btac222-B30" ref-type="bibr">Yang <italic toggle="yes">et al.</italic> (2020)</xref>, which use Accuracy, Precision, Recall and F1_score as the performance metrics. The greater value these metrics are, the better performance the model achieves.</p>
      <p>Furthermore, we use four additional indicators designated for multi-label learning (<xref rid="btac222-B21" ref-type="bibr">Paniri <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac222-B31" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic>, 2019</xref>), including Hamming Loss (HL), Ranking Loss (RL), Coverage and One Error (OE). HL provides an assessment how many times a pair of sample label is misclassified. RL provides an assessment about the fraction of reversely ordered label pairs. Coverage provides an assessment how far the list of ranked labels goes down to cover all the truth labels of samples on average. OE provides an assessment about the fraction of samples whose top-ranked label is not in the set of proper labels. For HL, RL, Coverage and OE, the smaller the values, the better the performance.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Experiments</title>
    <sec>
      <title>3.1 Dataset</title>
      <p>The experimental dataset was taken from <xref rid="btac222-B1" ref-type="bibr">Baranwal <italic toggle="yes">et al.</italic> (2020)</xref>, which includes 6669 compounds and their metabolic pathway entries. The dataset was originally collected from KEGG Pathway (<ext-link xlink:href="https://www.genome.jp/kegg/pathway.html" ext-link-type="uri">https://www.genome.jp/kegg/pathway.html</ext-link>), which contains 11 types of metabolic pathways, including (i) Carbohydrate metabolism; (ii) Energy metabolism; (iii) Lipid metabolism; (iv) Nucleotide metabolism; (v) Amino acid metabolism; (vi) Metabolism of other amino acids; (vii) Glycan biosynthesis and metabolism; (viii) Metabolism of cofactors and vitamins; (ix) Metabolism of terpenoids and polyketides; (x) Biosynthesis of other secondary metabolites; and (xi) Xenobiotics biodegradation and metabolism.</p>
      <p>As the compound encoder of our MLGL-MP requires, we removed 21 compounds that cannot be converted to molecular graphs. We finally constructed a dataset containing 6648 compounds and their 11435 metabolic pathway entries. Among them, 4898 compounds attend only one metabolic pathway while the remaining 1750 compounds attend multiple metabolic pathways. Specifically, 38 out of 1750 compounds attend all the 11 metabolic pathways. The metabolic pathway dataset is summarized in <xref rid="btac222-T1" ref-type="table">Table 1</xref> and in <xref rid="btac222-F3" ref-type="fig">Fig. 3</xref>. More details can be found at <ext-link xlink:href="https://github.com/dubingxue/MLGL-MP" ext-link-type="uri">https://github.com/dubingxue/MLGL-MP</ext-link>.</p>
      <fig position="float" id="btac222-F3">
        <label>Fig. 3.</label>
        <caption>
          <p>Compound distribution on multiple metabolic pathways</p>
        </caption>
        <graphic xlink:href="btac222f3" position="float"/>
      </fig>
      <table-wrap position="float" id="btac222-T1">
        <label>Table 1.</label>
        <caption>
          <p>Statistics of metabolic pathway dataset</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Type</th>
              <th align="left" rowspan="1" colspan="1">Metabolic pathway types</th>
              <th rowspan="1" colspan="1">Involving compounds</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">l<sub>0</sub></italic>
              </td>
              <td rowspan="1" colspan="1">Carbohydrate metabolism</td>
              <td rowspan="1" colspan="1">1126</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">l<sub>1</sub></italic>
              </td>
              <td rowspan="1" colspan="1">Energy metabolism</td>
              <td rowspan="1" colspan="1">750</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">l<sub>2</sub></italic>
              </td>
              <td rowspan="1" colspan="1">Lipid metabolism</td>
              <td rowspan="1" colspan="1">1066</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">l<sub>3</sub></italic>
              </td>
              <td rowspan="1" colspan="1">Nucleotide metabolism</td>
              <td rowspan="1" colspan="1">342</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">l<sub>4</sub></italic>
              </td>
              <td rowspan="1" colspan="1">Amino acid metabolism</td>
              <td rowspan="1" colspan="1">1440</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">l<sub>5</sub></italic>
              </td>
              <td rowspan="1" colspan="1">Metabolism of other amino acids</td>
              <td rowspan="1" colspan="1">597</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">l<sub>6</sub></italic>
              </td>
              <td rowspan="1" colspan="1">Glycan biosynthesis and metabolism</td>
              <td rowspan="1" colspan="1">325</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">l<sub>7</sub></italic>
              </td>
              <td rowspan="1" colspan="1">Metabolism of cofactors and vitamins</td>
              <td rowspan="1" colspan="1">948</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">l<sub>8</sub></italic>
              </td>
              <td rowspan="1" colspan="1">Metabolism of terpenoids and polyketides</td>
              <td rowspan="1" colspan="1">1483</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">l<sub>9</sub></italic>
              </td>
              <td rowspan="1" colspan="1">Biosynthesis of other secondary metabolites</td>
              <td rowspan="1" colspan="1">1906</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">l<sub>10</sub></italic>
              </td>
              <td rowspan="1" colspan="1">Xenobiotics biodegradation and metabolism</td>
              <td rowspan="1" colspan="1">1452</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec>
      <title>3.2 Parameter setting</title>
      <p>In the compound encoder, each node of input compound graph was initially represented by a 78-dimensional (78-d) binary atom feature vector, as suggested in <xref rid="btac222-B20" ref-type="bibr">Nguyen <italic toggle="yes">et al.</italic> (2021)</xref>. In brief, the initial representation of a compound contains five groups of atom indicators, including the atom symbol (44-d), the number of adjacent atoms (11-d), the number of adjacent hydrogens (11-d), the implicit value of the atom (11-d) and the atom occurrence in an aromatic structure (1-d). More details can be found in DeepChem (<xref rid="btac222-B23" ref-type="bibr">Ramsundar, 2018</xref>). Moreover, its GAT layer adopted 10 heads of attention layers, of which the outputting atom dimension was also set as 78 to capture the importance of chemical bonds. Similarly, its GCN layer kept the same dimension (780) of outputting atom features as that in the GAT layer. Finally, through the Global Max-pooling and the Global Mean-pooling on atoms, each compound was represented a 1560-dimensional embedding vector.</p>
      <p>For the pathway encoder, the pre-training of 11 metabolic pathways was implemented by the GloVe algorithm (<xref rid="btac222-B22" ref-type="bibr">Pennington <italic toggle="yes">et al.</italic>, 2014</xref>), which represent their names in 300-dimensional embedding vectors based on the Common Crawl dataset containing 840 billion tokens and 2.2 million vocab. Both the two layers of GCNs in the pathway encoder were represented 1024-dimensional node embeddings. Finally, each pathway was represented as a 1024-dimensional vector.</p>
      <p>In the multi-label predictor, the adapter was implemented by a DNN, of which the input layer, the hidden layer and the output layer contain 1560, 1500 and 1024 neurons, respectively.</p>
      <p>After setting up the architecture of MLGL-MP, we investigated how its hyperparameters, including the threshold <inline-formula id="IE80"><mml:math id="IM78" display="inline" overflow="scroll"><mml:mo>τ</mml:mo></mml:math></inline-formula> in <xref rid="E5" ref-type="disp-formula">Equation (5)</xref> and the coefficient <inline-formula id="IE81"><mml:math id="IM79" display="inline" overflow="scroll"><mml:mo>α</mml:mo></mml:math></inline-formula> in <xref rid="E7" ref-type="disp-formula">Equation (7)</xref> influence the metabolic pathway prediction. For the pathway dependence matrix, we tuned the value of <inline-formula id="IE82"><mml:math id="IM80" display="inline" overflow="scroll"><mml:mo>τ</mml:mo></mml:math></inline-formula> for the list of {0.1, 0.2, 0.3…,0.9}. We discarded two non-convergence cases where <inline-formula id="IE83"><mml:math id="IM81" display="inline" overflow="scroll"><mml:mo>τ</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> indicates no edge removed and <inline-formula id="IE84"><mml:math id="IM82" display="inline" overflow="scroll"><mml:mo>τ</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> generates a zero-dependence matrix. Furthermore, we set <inline-formula id="IE85"><mml:math id="IM83" display="inline" overflow="scroll"><mml:mo>α</mml:mo></mml:math></inline-formula> in a set of {0.1, 0.2, 0.3…,0.9, 1.0}. Similarly, we discarded the case of <inline-formula id="IE86"><mml:math id="IM84" display="inline" overflow="scroll"><mml:mo>α</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>, which makes the dependence matrix as an identity matrix. The Accuracy metric was adopted to evaluate the investigation generated by the grid research on <inline-formula id="IE87"><mml:math id="IM85" display="inline" overflow="scroll"><mml:mo>τ</mml:mo></mml:math></inline-formula> and <inline-formula id="IE88"><mml:math id="IM86" display="inline" overflow="scroll"><mml:mo>α</mml:mo></mml:math></inline-formula>. The results show that the pair of <inline-formula id="IE89"><mml:math id="IM87" display="inline" overflow="scroll"><mml:mo>τ</mml:mo></mml:math></inline-formula> = 0.5 and <inline-formula id="IE90"><mml:math id="IM88" display="inline" overflow="scroll"><mml:mo>α</mml:mo><mml:mo> </mml:mo></mml:math></inline-formula> =0.3 accounts for the best performance of MLGL-MP (<xref rid="btac222-F4" ref-type="fig">Fig. 4</xref>). Also, we tuned the learning rate from the list {0.1, 0.01, 0.001, 0.0005, 0.0001}, where 0.0005 accounts for the best performance.</p>
      <fig position="float" id="btac222-F4">
        <label>Fig. 4.</label>
        <caption>
          <p>Grid search expanded by <inline-formula id="IE91"><mml:math id="IM89" display="inline" overflow="scroll"><mml:mo>τ</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi mathvariant="normal">and</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>α</mml:mo></mml:math></inline-formula> in terms of accuracy</p>
        </caption>
        <graphic xlink:href="btac222f4" position="float"/>
      </fig>
      <p>In addition to these fine-tuned hyper-parameters, we empirically set the number of epochs as 200, set the batch size as 256 and selected Adam as the optimizer. Similarly, ReLU and LeakyReLU with default parameters were used as the activation functions in the compound encoder and the pathway encoder, respectively.</p>
      <p>All the remaining experiments were run under the optimal values of these parameters.</p>
    </sec>
    <sec>
      <title>3.3 Comparisons with baselines</title>
      <p>We assessed the performance of MLGL-MP by the comparison with two state-of-the-art shallow learning methods, (i.e. RF and XGBoost), as well as two state-of-the-art deep learning methods, including a GCN-based method (<xref rid="btac222-B1" ref-type="bibr">Baranwal <italic toggle="yes">et al.</italic>, 2020</xref>) and a GAT-based method (<xref rid="btac222-B30" ref-type="bibr">Yang <italic toggle="yes">et al.</italic>, 2020</xref>). They are briefly summarized as follows.
</p>
      <list list-type="bullet">
        <list-item>
          <p>RF-based model: Since RF was originally designed for multi-class classification but not multi-label classification, we implemented RF in the exact same way as that in <xref rid="btac222-B1" ref-type="bibr">Baranwal <italic toggle="yes">et al.</italic> (2020)</xref>. Eleven RF classifiers were trained separately to recognize each pathway type with the parameter setting where the maximum depth of the tree is 60 and the number of decision trees is 300.</p>
        </list-item>
        <list-item>
          <p>XGBoost-based model: The implementation of XGBoost is similar to that of RF. Eleven XGBoost classifiers were trained separately with a parameter setting where the maximum depth of the tree is 30 and the number of decision trees is 300.</p>
        </list-item>
        <list-item>
          <p>The GCN-based model (<xref rid="btac222-B1" ref-type="bibr">Baranwal <italic toggle="yes">et al.</italic>, 2020</xref>): it proposes a compound subgraph representation learning based on GCNs and combined the learned subgraph embeddings as local features with global features (diverse molecular properties, MACCS fingerprints, adjacency matrix, etc.) to a feedforward neural network. We used the same parameters as those in the original paper.</p>
        </list-item>
        <list-item>
          <p>The GAT-based model (<xref rid="btac222-B30" ref-type="bibr">Yang <italic toggle="yes">et al.</italic>, 2020</xref>): it adopts GATs to obtain compound subgraph representation as local features and used the same global features same to the GCN-based model (<xref rid="btac222-B1" ref-type="bibr">Baranwal <italic toggle="yes">et al.</italic>, 2020</xref>). We used the default values of parameters as those in the original paper.</p>
        </list-item>
      </list>
      <p>For a fair comparison, we utilized 10-fold cross-validation (10-CV) for all the methods and measured their performance by two groups of metrics (<xref rid="btac222-T2" ref-type="table">Table 2</xref>). The first group contains the average Accuracy, Precision, Recall and F1_score. The greater, the better. The second group contains HL, Coverage, OE and RL, which are designated metrics for multi-label learning. The smaller, the better. The results show that the deep learning-based methods (the GCN-based, the GAT-based and our MLGL-MP) outperform the shallow learning-based methods (the RF-based and the XGBoost-based) overall. Moreover, it reveals that MLGL-MP achieves the best performance with significant improvements over all the metrics, compared with the GCN-based model and the GAT-based model. Therefore, the comparison demonstrates the superiority of our MLGL-MP.</p>
      <table-wrap position="float" id="btac222-T2">
        <label>Table 2.</label>
        <caption>
          <p>Performance evaluation on the KEGG dataset of multi-label metabolic pathway prediction</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Method</th>
              <th rowspan="1" colspan="1">Accuracy (%)</th>
              <th rowspan="1" colspan="1">Precision (%)</th>
              <th rowspan="1" colspan="1">Recall (%)</th>
              <th rowspan="1" colspan="1">F1_score (%)</th>
              <th rowspan="1" colspan="1">HL</th>
              <th rowspan="1" colspan="1">Coverage</th>
              <th rowspan="1" colspan="1">OE</th>
              <th rowspan="1" colspan="1">RL</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">RF</td>
              <td rowspan="1" colspan="1">97.59 ± 0.19</td>
              <td rowspan="1" colspan="1">83.58 ± 0.84</td>
              <td rowspan="1" colspan="1">83.54 ± 0.79</td>
              <td rowspan="1" colspan="1">83.56 ± 0.81</td>
              <td rowspan="1" colspan="1">0.024 ± 0.002</td>
              <td rowspan="1" colspan="1">1.809 ± 0.069</td>
              <td rowspan="1" colspan="1">0.156 ± 0.008</td>
              <td rowspan="1" colspan="1">0.167 ± 0.008</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">XGBoost</td>
              <td rowspan="1" colspan="1">98.04 ± 0.18</td>
              <td rowspan="1" colspan="1">89.66 ± 0.58</td>
              <td rowspan="1" colspan="1">90.49 ± 0.85</td>
              <td rowspan="1" colspan="1">90.07 ± 0.64</td>
              <td rowspan="1" colspan="1">0.020 ± 0.002</td>
              <td rowspan="1" colspan="1">1.447 ± 0.087</td>
              <td rowspan="1" colspan="1">0.099 ± 0.005</td>
              <td rowspan="1" colspan="1">0.100 ± 0.008</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GCN-based</td>
              <td rowspan="1" colspan="1">97.53 ± 0.41</td>
              <td rowspan="1" colspan="1">91.37 ± 1.20</td>
              <td rowspan="1" colspan="1">93.22 ± 1.60</td>
              <td rowspan="1" colspan="1">92.28 ± 1.30</td>
              <td rowspan="1" colspan="1">0.025 ± 0.004</td>
              <td rowspan="1" colspan="1">1.033 ± 1.140</td>
              <td rowspan="1" colspan="1">0.100 ± 0.153</td>
              <td rowspan="1" colspan="1">0.040 ± 0.082</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GAT-based</td>
              <td rowspan="1" colspan="1">97.57 ± 0.18</td>
              <td rowspan="1" colspan="1">92.71 ± 0.64</td>
              <td rowspan="1" colspan="1">92.04 ± 0.87</td>
              <td rowspan="1" colspan="1">92.53 ± 0.39</td>
              <td rowspan="1" colspan="1">0.024 ± 0.002</td>
              <td rowspan="1" colspan="1">0.830 ± 0.318</td>
              <td rowspan="1" colspan="1">0.064 ± 0.082</td>
              <td rowspan="1" colspan="1">0.024 ± 0.028</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>MLGL-MP</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>98.64 ± 0.47</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>95.26 ± 2.25</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>94.21 ± 1.94</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>94.73 ± 1.89</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.014 ± 0.005</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.559 ± 0.113</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.050 ± 0.019</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.011 ± 0.003</bold>
              </td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec>
      <title>3.4 Ablation studies</title>
      <p>In this section, we investigated why MLGL-MP can achieve inspiring prediction by ablation studies. We made three variants of MLGL-MP, of which the first removes the pathway encoder (denoted as w/o PE), the second one lacks the adapter in the multi-label predictor (denoted as w/o AP), the third (denoted as MLGL-MP-r) alters the pre-trained node feature vectors in the pathway dependence graph to randomly initialized Gaussian vectors (<xref rid="btac222-F5" ref-type="fig">Fig. 5</xref>).</p>
      <fig position="float" id="btac222-F5">
        <label>Fig. 5.</label>
        <caption>
          <p>Ablation comparison. Compared with MLGL-MP, w/o PE removes the pathway encoder, w/o AP lacks the adapter in the multi-label predictor, MLGL-MP-r denotes alters the pre-trained node feature vectors in the pathway dependence graph to randomly initialized Gaussian vectors. The left panel indicates the performance by Accuracy, Precision, Recall and F1-score. The middle one indicates the performance with regard to Coverage. The right panel indicates the comparison in terms of Hamming Loss (HL), One Error (OE) and Ranking Loss (RL)</p>
        </caption>
        <graphic xlink:href="btac222f5" position="float"/>
      </fig>
      <p>MLGL-MP significantly outperforms w/o PE over all the evaluation metrics. In detail, compared with w/o PE, MLGL-MP improves the Accuracy by 2.70%, the Precision by 5.88%, the Recall by 15.86% and the F1-score by 11.27% due to its pathway encoder. Meanwhile, MLGL-MP reduces the HL by 0.027, the Coverage by 0.212, the OE by 0.076 and the RL by over 0.018. The result indicates that the designated pathway embeddings improve metabolic pathway prediction greatly because it captures the pathway interdependence.</p>
      <p>Moreover, the comparison with w/o AP shows a similar improvement over all the metrics. In detail, MLGL-MP improves the accuracy by 1.01%, the precision by 4.43%, the recall by 3.16% and the F1-score by 3.84%. Again, it shows a better performance of multi-label learning with reducing the HL by 0.009, the Coverage by 0.046, the OE by 0.019 and the RL by 0.003. This result shows that the adapter in the predictor improves the prediction significantly by aligning compound embeddings with pathway embeddings.</p>
      <p>In addition, the comparison shows that the version with pathway pre-training (MLGL-MP) is better than that with pathway random initialization (MLGL-MP-r) over all the evaluation metrics. Thus, the pre-training strategy can improve the prediction.</p>
      <p>In general, the pathway encoder, the adapter and the pre-training strategy play indispensable roles in predicting multi-label metabolic pathways.</p>
    </sec>
    <sec>
      <title>3.5 Case study: interpretability of MLGL-MP</title>
      <p>Although deep learning is known as a black-box model, it is essential to understand how the model makes a prediction and whether the model can guide lead compound optimization in drug discovery. MLGL-MP leverages the GAT layer in its compound encoder to access why a compound participates in a specific pathway. Since the attention weights learned in the GAT layer can reflect the importance of chemical bonds in compounds, we can reveal the association between compounds’ substructures and their metabolic pathway.</p>
      <p>For example, Energy Metabolism and Amino Acid Metabolism are two important pathways in organisms (<xref rid="btac222-B24" ref-type="bibr">Rui, 2014</xref>; <xref rid="btac222-B28" ref-type="bibr">Vettore <italic toggle="yes">et al.</italic>, 2020</xref>). The former maintains the regular activity of metabolic enzymes (<xref rid="btac222-B6" ref-type="bibr">Foo <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac222-B19" ref-type="bibr">Motohashi and Akaike, 2019</xref>) while the latter are an essential process in cells (<xref rid="btac222-B17" ref-type="bibr">Lopez and Mohiuddin, 2021</xref>). Thus, we selected them as examples to illustrate the interpretability of MLGL-MP (<xref rid="btac222-F6" ref-type="fig">Fig. 6</xref>).</p>
      <fig position="float" id="btac222-F6">
        <label>Fig. 6.</label>
        <caption>
          <p>Visualization of compound substructure importance. (<bold>A</bold>) Energy metabolism; (<bold>B</bold>) Amino acid metabolisms.</p>
        </caption>
        <graphic xlink:href="btac222f6" position="float"/>
      </fig>
      <p>Overall, the visualized attention weights show that most carbon (C)-based chemical bonds constructing compound backbones usually have small attention values. More importantly, the visualization reveals that crucial substructures having high attentions are pathway-specific. We went deeper into the case of Energy Metabolism (<xref rid="btac222-F6" ref-type="fig">Fig. 6A</xref>), where Sulfur (S) and Phosphorus (P)-based chemical bonds in Energy Metabolism have higher attention values and are highlighted in red. The result is consistent with the early knowledge that <italic toggle="yes">Sulfur metabolism</italic> and <italic toggle="yes">Oxidative phosphorylation</italic> occur in the pathway of Energy Metabolism (<xref rid="btac222-B8" ref-type="bibr">Gibson and Skett, 2013</xref>). Meanwhile, recent works also provided more pieces of evidence (<xref rid="btac222-B6" ref-type="bibr">Foo <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac222-B19" ref-type="bibr">Motohashi and Akaike, 2019</xref>). For example, the unique role of <italic toggle="yes">sulfur</italic> in organisms is mainly related to redox reactions and its functions include cell protection and energy metabolism (<xref rid="btac222-B19" ref-type="bibr">Motohashi and Akaike, 2019</xref>). <italic toggle="yes">Oxidative phosphorylation</italic> has garnered increasing interest in energy metabolism as a new target space (e.g. the mycobacterial druggable target) (<xref rid="btac222-B6" ref-type="bibr">Foo <italic toggle="yes">et al.</italic>, 2020</xref>). Moreover, the case of Amino Acid Metabolism (<xref rid="btac222-F6" ref-type="fig">Fig. 6B</xref>) shows that the substructures of amino and carboxylic have greater attention weights. The result is validated by the work (<xref rid="btac222-B17" ref-type="bibr">Lopez and Mohiuddin, 2021</xref>), which indicates both <italic toggle="yes">amino</italic> (−NH2) and <italic toggle="yes">carboxylic acid</italic> (−COOH) functional groups play an important role in Amino Acid Metabolism.</p>
      <p>In summary, MLGL-MP is an interpretable model, which can indicate compound substructures significantly associated with metabolic pathways. It would help reveal why a compound participates in a specific pathway.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Conclusion</title>
    <p>In this paper, we have proposed an MLGL-MP, which contains a compound encoder, a pathway encoder and a multi-label predictor. This end-to-end framework can address two existing issues, including inadequate characterization of pathway dependences and interpretable prediction.</p>
    <p>The comparison with popular shallow learning models and deep learning models demonstrates the superiority of MLGL-MP. Moreover, the ablation studies as well as the case study validate its contributions. First, it provides an interpretable manner to indicate crucial compound substructures which are significantly associated with metabolic pathways by molecular graph attention embedding. Secondly, by capturing the pathway interdependence, it significantly improves the characterization of the relevance between compounds and their metabolic pathways. Thirdly, by measuring the proximity between compounds and metabolic pathways in a common embedding space, it proposes a direct multipathway prediction approach without extra label strategy. In summary, we believe that our study provides new insights into label dependence representation learning for other multi-label classification problems (e.g. drug toxicity prediction) in drug discovery.</p>
    <p>Moreover, though the GAT can interpret the importance of drug substructures to metabolic pathways in some sense, other parts (i.e. the pathway encoder and the adapter) in the model are of the black box. In the coming future, it is anticipated that interpretable techniques derived from image processing (e.g. visualization of hidden layers, nearest neighbors and GAN) can be utilized to achieve better interpretability in predicting metabolic pathways for compounds.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work has been supported by the National Nature Science Foundation of China [Grant number 61872297) and Shaanxi Provincial Key Research &amp; Development Program, China (Grant number 2020KW-063).</p>
    <p><italic toggle="yes">Conflict of Interest</italic>: We declare that we have no conflict of interest.</p>
  </sec>
</body>
<back>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btac222-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baranwal</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>A deep learning architecture for metabolic pathway prediction</article-title>. <source>Bioinformatics</source>, <volume>36</volume>, <fpage>2547</fpage>–<lpage>2553</lpage>.<pub-id pub-id-type="pmid">31879763</pub-id></mixed-citation>
    </ref>
    <ref id="btac222-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Calderón-Ospina</surname><given-names>C.A.</given-names></string-name>, <string-name><surname>Nava-Mesa</surname><given-names>M.O.</given-names></string-name></person-group> (<year>2020</year>) <article-title>B vitamins in the nervous system: current knowledge of the biochemical modes of action and synergies of thiamine, pyridoxine, and cobalamin</article-title>. <source>CNS Neurosci. Ther</source>., <volume>26</volume>, <fpage>5</fpage>–<lpage>13</lpage>.<pub-id pub-id-type="pmid">31490017</pub-id></mixed-citation>
    </ref>
    <ref id="btac222-B3">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>Z.M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <part-title>Multi-label image recognition with graph convolutional networks</part-title>. In: <source>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition</source>, pp. <fpage>5172</fpage>–<lpage>5181</lpage>. <publisher-name>IEEE Computer Soc</publisher-name>, <publisher-loc>Los Alamitos</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btac222-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cho</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2010</year>) <article-title>Prediction of novel synthetic pathways for the production of desired chemicals</article-title>. <source>BMC Syst. Biol</source>., <volume>4</volume>, <fpage>35</fpage>.<pub-id pub-id-type="pmid">20346180</pub-id></mixed-citation>
    </ref>
    <ref id="btac222-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fang</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>L.</given-names></string-name></person-group> (<year>2017</year>) <article-title>A binary classifier for prediction of the types of metabolic pathway of chemicals</article-title>. <source>Comb. Chem. High Throughput Screen</source>., <volume>20</volume>, <fpage>140</fpage>–<lpage>146</lpage>.<pub-id pub-id-type="pmid">27981902</pub-id></mixed-citation>
    </ref>
    <ref id="btac222-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Foo</surname><given-names>C.S.-Y.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>Oxidative phosphorylation—an update on a new, essential target space for drug discovery in <italic toggle="yes">Mycobacterium tuberculosis</italic></article-title>. <source>Appl. Sci</source>., <volume>10</volume>, <fpage>2339</fpage>.</mixed-citation>
    </ref>
    <ref id="btac222-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gao</surname><given-names>Y.-F.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2012</year>) <article-title>Predicting metabolic pathways of small molecules and enzymes based on interaction information of chemicals and proteins</article-title>. <source>PLoS One</source>, <volume>7</volume>, <fpage>e45944</fpage>.<pub-id pub-id-type="pmid">23029334</pub-id></mixed-citation>
    </ref>
    <ref id="btac222-B8">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Gibson</surname><given-names>G.G.</given-names></string-name>, <string-name><surname>Skett</surname><given-names>P.</given-names></string-name></person-group> (<year>2013</year>) <source>Introduction to Drug Metabolism</source>. <publisher-name>Springer, Boston, MA</publisher-name>. https://doi.org/10.1007/978-1-4899-6844-9.</mixed-citation>
    </ref>
    <ref id="btac222-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guo</surname><given-names>Z.H.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <article-title>A network integration method for deciphering the types of metabolic pathway of chemicals with heterogeneous information</article-title>. <source>Comb. Chem. High Throughput Screen</source>., <volume>21</volume>, <fpage>670</fpage>–<lpage>680</lpage>.<pub-id pub-id-type="pmid">30520371</pub-id></mixed-citation>
    </ref>
    <ref id="btac222-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>He</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) <article-title>Multi-label ocular disease classification with a dense correlation deep neural network</article-title>. <source>Biomed. Signal Process. Control</source>, <volume>63</volume>, <fpage>102167</fpage>.</mixed-citation>
    </ref>
    <ref id="btac222-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hu</surname><given-names>L.-L.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2011</year>) <article-title>Predicting biological functions of compounds based on chemical-chemical interactions</article-title>. <source>PLoS One</source>, <volume>6</volume>, <fpage>e29491</fpage>.<pub-id pub-id-type="pmid">22220213</pub-id></mixed-citation>
    </ref>
    <ref id="btac222-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jia</surname><given-names>Y.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020a</year>) <article-title>iMPT-FRAKEL: a simple multi-label web-server that only uses fingerprints to identify which metabolic pathway types compounds can participate in</article-title>. <source>Open Bioinf. J</source>., <volume>13</volume>, <fpage>83</fpage>–<lpage>91</lpage>.</mixed-citation>
    </ref>
    <ref id="btac222-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jia</surname><given-names>Y.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020b</year>) <article-title>Similarity-based machine learning model for predicting the metabolic pathways of compounds</article-title>. <source>IEEE Access</source>, <volume>8</volume>, <fpage>130687</fpage>–<lpage>130696</lpage>.</mixed-citation>
    </ref>
    <ref id="btac222-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kalyesubula</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) <article-title>High-dose vitamin B1 therapy prevents the development of experimental fatty liver driven by overnutrition</article-title>. <source>Dis. Models Mech</source>., <volume>14</volume>, <fpage>dmm048355</fpage>.</mixed-citation>
    </ref>
    <ref id="btac222-B15">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kipf</surname><given-names>T.</given-names></string-name>, <string-name><surname>Welling</surname><given-names>M.</given-names></string-name></person-group> (<year>2017</year>) Semi-supervised classification with graph convolutional networks. ArXiv. https://doi.org/10.48550/arXiv.1609.02907.</mixed-citation>
    </ref>
    <ref id="btac222-B16">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>Q.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) Deeper insights into graph convolutional networks for semi-supervised learning. In: <italic toggle="yes">Thirty-Second AAAI Conference on Artificial Intelligence</italic>. AAAI Press, New Orleans, Louisiana USA.</mixed-citation>
    </ref>
    <ref id="btac222-B17">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Lopez</surname><given-names>M.J.</given-names></string-name>, <string-name><surname>Mohiuddin</surname><given-names>S.S.</given-names></string-name></person-group> (<year>2021</year>) <part-title>Biochemistry, essential amino acids</part-title>. In: <source>StatPearls</source>. <publisher-name>StatPearls Publishing, Treasure Island, FL.</publisher-name></mixed-citation>
    </ref>
    <ref id="btac222-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McNeil</surname><given-names>C.J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2012</year>) <article-title>Nutritional B vitamin deficiency disrupts lipid metabolism causing accumulation of proatherogenic lipoproteins in the aorta adventitia of ApoE null mice</article-title>. <source>Mol. Nutr. Food Res</source>., <volume>56</volume>, <fpage>1122</fpage>–<lpage>1130</lpage>.<pub-id pub-id-type="pmid">22610982</pub-id></mixed-citation>
    </ref>
    <ref id="btac222-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Motohashi</surname><given-names>H.</given-names></string-name>, <string-name><surname>Akaike</surname><given-names>T.</given-names></string-name></person-group> (<year>2019</year>) <article-title>Sulfur-utilizing cytoprotection and energy metabolism</article-title>. <source>Curr. Opin. Physiol</source>., <volume>9</volume>, <fpage>1</fpage>–<lpage>8</lpage>.</mixed-citation>
    </ref>
    <ref id="btac222-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nguyen</surname><given-names>T.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) <article-title>GraphDTA: predicting drug–target binding affinity with graph neural networks</article-title>. <source>Bioinformatics</source>, <volume>37</volume>, <fpage>1140</fpage>–<lpage>1147</lpage>.<pub-id pub-id-type="pmid">33119053</pub-id></mixed-citation>
    </ref>
    <ref id="btac222-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Paniri</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>MLACO: a multi-label feature selection algorithm based on ant colony optimization</article-title>. <source>Knowledge-Based Syst</source>., <volume>192</volume>, <fpage>105285</fpage>.</mixed-citation>
    </ref>
    <ref id="btac222-B22">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Pennington</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2014</year>) Glove: global vectors for word representation. In: <italic toggle="yes">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</italic>, pp. <fpage>1532</fpage>–<lpage>1543</lpage>.</mixed-citation>
    </ref>
    <ref id="btac222-B23">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Ramsundar</surname><given-names>B.</given-names></string-name></person-group> (<year>2018</year>) <source>Molecular Machine Learning with DeepChem</source><part-title>.</part-title>  <publisher-name>Stanford University</publisher-name>, <publisher-loc>Ann Arbor, MI</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btac222-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rui</surname><given-names>L.</given-names></string-name></person-group> (<year>2014</year>) <article-title>Energy metabolism in the liver</article-title>. <source>Compr. Physiol</source>., <volume>4</volume>, <fpage>177</fpage>–<lpage>197</lpage>.<pub-id pub-id-type="pmid">24692138</pub-id></mixed-citation>
    </ref>
    <ref id="btac222-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sankar</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>Predicting novel metabolic pathways through subgraph mining</article-title>. <source>Bioinformatics</source>, <volume>33</volume>, <fpage>3955</fpage>–<lpage>3963</lpage>.<pub-id pub-id-type="pmid">28961716</pub-id></mixed-citation>
    </ref>
    <ref id="btac222-B26">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Tsoumakas</surname><given-names>G.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2007</year>) <part-title>Random k-Labelsets: an ensemble method for multilabel classification</part-title>. In: <person-group person-group-type="editor"><string-name><surname>Kok</surname><given-names>J.N.</given-names></string-name></person-group>  <etal>et al</etal> (ed.) <source>Machine Learning: ECML 2007</source>. <publisher-name>Springer Berlin Heidelberg</publisher-name>, <publisher-loc>Berlin, Heidelberg</publisher-loc>, pp. <fpage>406</fpage>–<lpage>417</lpage>.</mixed-citation>
    </ref>
    <ref id="btac222-B27">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Veličković</surname><given-names>P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) Graph attention networks. arXiv preprint arXiv:1710.10903.</mixed-citation>
    </ref>
    <ref id="btac222-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vettore</surname><given-names>L.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>New aspects of amino acid metabolism in cancer</article-title>. <source>Br. J. Cancer</source>, <volume>122</volume>, <fpage>150</fpage>–<lpage>156</lpage>.<pub-id pub-id-type="pmid">31819187</pub-id></mixed-citation>
    </ref>
    <ref id="btac222-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yan</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>Analysing the Meta-interaction between pathways by gene set topological impact analysis</article-title>. <source>BMC Genomics</source>, <volume>21</volume>, <fpage>748</fpage>.<pub-id pub-id-type="pmid">33109101</pub-id></mixed-citation>
    </ref>
    <ref id="btac222-B30">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Yang</surname><given-names>Z.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) Multi-class metabolic pathway prediction by graph attention-based deep learning method. In: <italic toggle="yes">2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</italic>. IEEE, pp. <fpage>126</fpage>–<lpage>131</lpage>.</mixed-citation>
    </ref>
    <ref id="btac222-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Manifold regularized discriminative feature selection for multi-label learning</article-title>. <source>Pattern Recognit</source>., <volume>95</volume>, <fpage>136</fpage>–<lpage>150</lpage>.</mixed-citation>
    </ref>
    <ref id="btac222-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Tang</surname><given-names>W.</given-names></string-name></person-group> (<year>2018</year>) <article-title>Drug metabolism in drug discovery and development</article-title>. <source>Acta Pharm. Sin. B</source>, <volume>8</volume>, <fpage>721</fpage>–<lpage>732</lpage>.<pub-id pub-id-type="pmid">30245961</pub-id></mixed-citation>
    </ref>
    <ref id="btac222-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhu</surname><given-names>Y.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) <article-title>iMPTCE-Hnetwork: a multilabel classifier for identifying metabolic pathway types of chemicals and enzymes with a heterogeneous network</article-title>. <source>Comput. Math. Methods Med</source>., <volume>2021</volume>, <fpage>6683051</fpage>.<pub-id pub-id-type="pmid">33488764</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
