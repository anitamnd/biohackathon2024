<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PeerJ</journal-id>
    <journal-id journal-id-type="iso-abbrev">PeerJ</journal-id>
    <journal-id journal-id-type="publisher-id">peerj</journal-id>
    <journal-title-group>
      <journal-title>PeerJ</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2167-8359</issn>
    <publisher>
      <publisher-name>PeerJ Inc.</publisher-name>
      <publisher-loc>San Diego, USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9107302</article-id>
    <article-id pub-id-type="publisher-id">13163</article-id>
    <article-id pub-id-type="doi">10.7717/peerj.13163</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Bioinformatics</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Drugs and Devices</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Computational Science</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Data Mining and Machine Learning</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DeepNC: a framework for drug-target interaction prediction with graph neural networks</article-title>
    </title-group>
    <contrib-group>
      <contrib id="author-1" contrib-type="author">
        <name>
          <surname>Tran</surname>
          <given-names>Huu Ngoc Tran</given-names>
        </name>
        <xref rid="aff-1" ref-type="aff">1</xref>
      </contrib>
      <contrib id="author-2" contrib-type="author" corresp="yes">
        <name>
          <surname>Thomas</surname>
          <given-names>J. Joshua</given-names>
        </name>
        <email>jjoshua@kdupg.edu.my</email>
        <xref rid="aff-1" ref-type="aff">1</xref>
      </contrib>
      <contrib id="author-3" contrib-type="author">
        <name>
          <surname>Ahamed Hassain Malim</surname>
          <given-names>Nurul Hashimah</given-names>
        </name>
        <xref rid="aff-2" ref-type="aff">2</xref>
      </contrib>
      <aff id="aff-1"><label>1</label><institution>Department of Computing, UOW Malaysia, KDU Penang University College</institution>, <city>George Town</city>, <state>Penang</state>, <country>Malaysia</country></aff>
      <aff id="aff-2"><label>2</label><institution>School of Computer Sciences, Universiti Sains Malaysia</institution>, <city>George Town</city>, <state>Penang</state>, <country>Malaysia</country></aff>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Orlov</surname>
          <given-names>Yuriy</given-names>
        </name>
      </contrib>
    </contrib-group>
    <pub-date pub-type="epub" date-type="pub" iso-8601-date="2022-05-11">
      <day>11</day>
      <month>5</month>
      <year iso-8601-date="2022">2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>10</volume>
    <elocation-id>e13163</elocation-id>
    <history>
      <date date-type="received" iso-8601-date="2021-10-25">
        <day>25</day>
        <month>10</month>
        <year iso-8601-date="2021">2021</year>
      </date>
      <date date-type="accepted" iso-8601-date="2022-03-03">
        <day>3</day>
        <month>3</month>
        <year iso-8601-date="2022">2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>©2022 Tran et al.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Tran et al.</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, reproduction and adaptation in any medium and for any purpose provided that it is properly attributed. For attribution, the original author(s), title, publication source (PeerJ) and either DOI or URL of the article must be cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="https://peerj.com/articles/13163"/>
    <abstract>
      <p>The exploration of drug-target interactions (DTI) is an essential stage in the drug development pipeline. Thanks to the assistance of computational models, notably in the deep learning approach, scientists have been able to shorten the time spent on this stage. Widely practiced deep learning algorithms such as convolutional neural networks and recurrent neural networks are commonly employed in DTI prediction projects. However, they can hardly utilize the natural graph structure of molecular inputs. For that reason, a graph neural network (GNN) is an applicable choice for learning the chemical and structural characteristics of molecules when it represents molecular compounds as graphs and learns the compound features from those graphs. In an effort to construct an advanced deep learning-based model for DTI prediction, we propose Deep Neural Computation (DeepNC), which is a framework utilizing three GNN algorithms: Generalized Aggregation Networks (GENConv), Graph Convolutional Networks (GCNConv), and Hypergraph Convolution-Hypergraph Attention (HypergraphConv). In short, our framework learns the features of drugs and targets by the layers of GNN and 1-D convolution network, respectively. Then, representations of the drugs and targets are fed into fully-connected layers to predict the binding affinity values. The models of DeepNC were evaluated on two benchmarked datasets (Davis, Kiba) and one independently proposed dataset (Allergy) to confirm that they are suitable for predicting the binding affinity of drugs and targets. Moreover, compared to the results of baseline methods that worked on the same problem, DeepNC proves to improve the performance in terms of mean square error and concordance index.</p>
    </abstract>
    <kwd-group kwd-group-type="author">
      <kwd>Drug-target interaction</kwd>
      <kwd>Binding affinity</kwd>
      <kwd>Drug discovery</kwd>
      <kwd>Deep learning</kwd>
      <kwd>Graph neural networks</kwd>
      <kwd>Cheminformatics</kwd>
    </kwd-group>
    <funding-group>
      <award-group id="fund-1">
        <funding-source> Ministry of Higher Education Malaysia</funding-source>
        <award-id> FRGS/1/2019/ICT02/KDUPG/02/1</award-id>
      </award-group>
      <funding-statement>This work is supported by the Fundamental Research Grant Scheme (FRGS) of the Ministry of Higher Education Malaysia under the grant project number FRGS/1/2019/ICT02/KDUPG/02/1. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
  </article-meta>
</front>
<body>
  <sec sec-type="intro">
    <title>Introduction</title>
    <sec>
      <title>Cheminformatics in drug discovery</title>
      <p>The great increasing of data in the field of chemistry over the last few decades has been witnessed, as well as the data science approaches created to analyze it. In this uprising, machine learning is still the most significant tool for analyzing and understanding chemical data (<xref rid="ref-25" ref-type="bibr">Lo et al., 2018</xref>). Machine learning is a branch of artificial intelligence that targets at developing and using computational models to learn from data and to also generate more new data. Machine learning, particularly deep learning, is becoming progressively more important in operations that deal with large amounts of data, such as drug discovery.</p>
      <p>Drug discovery is the work carried out to find the functions of bioactive compounds for the development of novel drugs, <italic toggle="yes">i.e.</italic>, searching for the candidates for new medicines (<xref rid="ref-38" ref-type="bibr">Tran et al., 2021</xref>). Traditionally, drug discovery can be considered an iterative screening process. First of all, the biochemical target is chosen; it should be known that target is a biomolecule of an organism that can be bound by a molecular structure and has its function modulated (<xref rid="ref-32" ref-type="bibr">Rifaioglu et al., 2019</xref>). The next step is to find, through chemical experiments, the compounds which interact with chosen target; these compounds are then called hits. A hit is pretty far from having an approved novel medical drug, however, it can be considered potential to be a drug candidate. Various wet-lab experiments will be performed to achieve validated hits—the hits with acceptable robustness, and then the leads—the subset of validated hits selected by parameters such as patentability and synthesizability. Many more examinations will be carried out to see which lead has physical and chemical properties such that it can be used in practice, resulting in having a smaller set called optimized leads. Finally, optimized leads will go through toxicity tests to become clinical candidates (<xref rid="ref-5" ref-type="bibr">Chan et al., 2019</xref>).</p>
      <p>Considering the daunting tasks and large costs of the drug discovery process, computational applications become potential to be assisting tools in many stages of the process. This purpose is one of the motivations for the creation and development of cheminformatics field.</p>
      <p>Cheminformatics is the domain where scientists adopt computer science techniques and tools to solve the problems in chemistry. It is a crucial research topic in drug discovery and drug development, since it focuses on challenges like chemical information analysis, as well as molecular data exploration. In the progressive cheminformatics, machine learning and deep learning are widely applied to method huge chemical knowledge and to get novel drugs. However, there is still an existing challenge which is that classical machine learning algorithms require data of a rigid format, <italic toggle="yes">e.g.</italic>, vectors of fixed length, which is not enough to describe chemical structures. The amount of research that requires deep learning techniques to learn important structural information of chemical data are increasing. Thus, algorithms particularly designed to handle graph data are earning more and more attention. Such algorithms are presently known as graph neural networks (GNN).</p>
      <p>The rest of this article briefly covers the works of GNN from the beginning days until the up-to-date researches and significant applicable research works in cheminformatics. The overall architecture of the proposed model using GNN contains detailed information from the experimental work: data collecting and processing, selection of evaluation metrics, the results of training an discussion. The article ends with the concluding remarks.</p>
    </sec>
    <sec>
      <title>Research contributions</title>
      <p>In this study, we introduce a deep learning-based framework for DTI prediction, which we call DeepNC. We utilized three GNN algorithms: Generalized Aggregation Networks, Graph Convolutional Networks, and Hypergraph Convolution-Hypergraph Attention, which were originally proposed in the research works by <xref rid="ref-22" ref-type="bibr">Li et al. (2020)</xref>, <xref rid="ref-20" ref-type="bibr">Kipf &amp; Welling (2017)</xref>, and <xref rid="ref-2" ref-type="bibr">Bai, Zhang &amp; Torr (2021)</xref>, respectively. These algorithms play an important part for the whole model in the stage of learning the graph-based input data and creating graph-based representation of that data.</p>
      <p>The study also presents an independent drug-target binding affinities dataset which is called the allergy dataset. The purpose of building the allergy dataset is to create a novel DTI collection that can be put in computational DTI prediction models, serving the aim of searching for new potential allergy drugs. In our scope of research, we defined allergy drugs as the approved drugs that are able to treat allergic reactions in humans; each allergy drug has one or more targets which are known as allergy drug targets. The dataset contains these allergy drug targets and the compounds that have appropriate binding affinity values towards these targets. The construction of this independent dataset will be further explained in ‘Datasets” section.</p>
    </sec>
  </sec>
  <sec>
    <title>Preliminaries</title>
    <sec>
      <title>Graph neural networks</title>
      <p>Graphs are a form of knowledge whose structure contains a group of nodes, which represent a group of objects, and a group of edges linking them, which symbolize the objects’ relationship. In recent times, as a result of robust communicative power of graphs, analyses of data science processing graphs with machine learning are abundant and being more developed and progressed, <italic toggle="yes">i.e.</italic>, graphs may be used as representations in various areas such as social networks (<xref rid="ref-20" ref-type="bibr">Kipf &amp; Welling, 2017</xref>; <xref rid="ref-16" ref-type="bibr">Hamilton, Ying &amp; Leskovec, 2017</xref>), physical systems (<xref rid="ref-34" ref-type="bibr">Sanchez-Gonzalez et al., 2018</xref>; <xref rid="ref-3" ref-type="bibr">Battaglia et al., 2016</xref>), protein-protein interaction networks (<xref rid="ref-12" ref-type="bibr">Fout et al., 2017</xref>), knowledge graphs (<xref rid="ref-15" ref-type="bibr">Hamaguchi et al., 2017</xref>), and plenty of different research areas (<xref rid="ref-7" ref-type="bibr">Dai et al., 2017</xref>).</p>
      <p>One of the most basic model of graph neural network was introduced by <xref rid="ref-35" ref-type="bibr">Scarselli et al. (2009)</xref>. The framework suggests that the propagation functions be applied repeatedly until the node’s representations reach a stable state. A node <italic toggle="yes">v</italic> in the graph is characterized by its state embedding <italic toggle="yes">h</italic><sub><italic toggle="yes">v</italic></sub> ∈ ℝ<sup><italic toggle="yes">D</italic></sup> and related nodes; respectively, an edge <italic toggle="yes">e</italic> is associated with its edge features <italic toggle="yes">h</italic><sub><italic toggle="yes">e</italic></sub> ∈ ℝ<sup><italic toggle="yes">C</italic></sup>. This GNN fundamental function is to learn and to create an embedding <italic toggle="yes">h</italic><sub><italic toggle="yes">v</italic></sub> for the state of the node <italic toggle="yes">v</italic>. The embedding is a vector of <italic toggle="yes">b</italic>-dimension that contains information of node <italic toggle="yes">v</italic>’s neighborhood, and is next applied to calculate and construct an output embedding <italic toggle="yes">o</italic><sub><italic toggle="yes">v</italic></sub>.</p>
    </sec>
    <sec>
      <title>Graph convolution network</title>
      <p>The architecture of Graph Convolution Network (GCN) was one of the early frameworks introduced by Kipf &amp; Welling in the research work <xref rid="ref-20" ref-type="bibr">Kipf &amp; Welling (2017)</xref>. This graph convolution network was constructed based on the fundamental rule that says the features of each node in a message-passing layer are updated by aggregating the features of its neighbours. Kipf proposed a layer-wise propagation rule as that can be applied on a graph convolutional network of many layers. The rule is written as follow: <disp-formula id="eqn-1"><label>(1)</label><alternatives><graphic xlink:href="peerj-10-13163-e001.jpg" position="float"/><tex-math id="tex-eqn-1">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}{H}^{(l+1)}=\sigma ({\widetilde {D}}^{- \frac{1}{2} }\widetilde {A}{\widetilde {D}}^{- \frac{1}{2} }{H}^{ \left( l \right) }{W}^{ \left( l \right) })\end{eqnarray*}\end{document}</tex-math><mml:math id="mml-eqn-1" overflow="scroll"><mml:mstyle displaystyle="true"><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mfenced></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mo> ˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo> ˜</mml:mo></mml:mrow></mml:mover><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mo> ˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mfenced></mml:mrow></mml:mstyle></mml:math></alternatives></disp-formula>knowing that <italic toggle="yes">H</italic><sup>(</sup><sup>0</sup>
<sup>)</sup> = <italic toggle="yes">X</italic>, <italic toggle="yes">W</italic><sup>(<italic toggle="yes">l</italic>)</sup> represents a layer-specific trainable weight matrix, <italic toggle="yes">σ</italic>(.) is an activation function, <italic toggle="yes">H</italic><sup>(<italic toggle="yes">l</italic>)</sup> ∈ ℝ<sup><italic toggle="yes">n</italic>×<italic toggle="yes">d</italic></sup> is the activation matrix in the <italic toggle="yes">l</italic><sup><italic toggle="yes">th</italic></sup> layer.</p>
      <p>For an <italic toggle="yes">n</italic>-vertice graph, its adjacency matrix is denoted as <italic toggle="yes">A</italic> ∈ℝ<sup>n×n</sup> where: <disp-formula id="eqn-2"><label>(2)</label><alternatives><graphic xlink:href="peerj-10-13163-e002.jpg" position="float"/><tex-math id="tex-eqn-2">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}{A}_{ij}= \left\{ \begin{array}{@{}l@{}} \displaystyle 1 \mathrm{if} {e}_{ij}\in E\\ \displaystyle 0 \mathrm{if} {e}_{ij}\not \in E \end{array} \right. \end{eqnarray*}\end{document}</tex-math><mml:math id="mml-eqn-2" overflow="scroll"><mml:mstyle displaystyle="true"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mn>1</mml:mn><mml:mspace width="2.0pt"/><mml:mi mathvariant="normal">if</mml:mi><mml:mspace width="2.0pt"/><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi>E</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn><mml:mspace width="2.0pt"/><mml:mi mathvariant="normal">if</mml:mi><mml:mspace width="2.0pt"/><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁄</mml:mo><mml:mo>∈</mml:mo><mml:mi>E</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mstyle></mml:math></alternatives></disp-formula>and degree matrix is denoted as <italic toggle="yes">D</italic> ∈ ℝ<sup>n×n</sup> where: <disp-formula id="eqn-3"><label>(3)</label><alternatives><graphic xlink:href="peerj-10-13163-e003.jpg" position="float"/><tex-math id="tex-eqn-3">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}{D}_{ii}=d({v}_{i})\end{eqnarray*}\end{document}</tex-math><mml:math id="mml-eqn-3" overflow="scroll"><mml:mstyle displaystyle="true"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mfenced></mml:mrow></mml:mstyle></mml:math></alternatives></disp-formula>then <italic toggle="yes">Ã</italic> is computed as <italic toggle="yes">Ã</italic> = <italic toggle="yes">A</italic> + <italic toggle="yes">I</italic><sub><italic toggle="yes">N</italic></sub> with <italic toggle="yes">I</italic><sub><italic toggle="yes">N</italic></sub> is the identity matrix and <inline-formula><alternatives><inline-graphic xlink:href="peerj-10-13163-i001.jpg"/><tex-math id="tex-ieqn-25">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\tilde {D}}_{ii}={\sum }_{j}{\tilde {A}}_{ij}$\end{document}</tex-math><mml:math id="mml-ieqn-25" overflow="scroll"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mo> ~</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo> ~</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>.</p>
      <p>The definition of the GCN is demonstrated as: <disp-formula id="eqn-4"><label>(4)</label><alternatives><graphic xlink:href="peerj-10-13163-e004.jpg" position="float"/><tex-math id="tex-eqn-4">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}Z={\widetilde {D}}^{- \frac{1}{2} }\widetilde {A}{\widetilde {D}}^{- \frac{1}{2} }X\Theta \end{eqnarray*}\end{document}</tex-math><mml:math id="mml-eqn-4" overflow="scroll"><mml:mstyle displaystyle="true"><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mo> ˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo> ˜</mml:mo></mml:mrow></mml:mover><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mo> ˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mi>X</mml:mi><mml:mi>Θ</mml:mi></mml:mstyle></mml:math></alternatives></disp-formula>where Θ ∈ℝ<sup><italic toggle="yes">C</italic>×<italic toggle="yes">F</italic></sup> is a matrix of filter parameters with <italic toggle="yes">C</italic> input channels (<italic toggle="yes">i.e.</italic>, every node will have a C-dimensional feature vector) and <italic toggle="yes">F</italic> filters for the feature.</p>
      <p>The concept of GCN can be visualized in <xref rid="fig-1" ref-type="fig">Fig. 1</xref> in which a graph representation is fed into convolutional networks to learn the graph output at the <italic toggle="yes">l</italic>-th layer.</p>
      <fig position="float" id="fig-1">
        <object-id pub-id-type="doi">10.7717/peerj.13163/fig-1</object-id>
        <label>Figure 1</label>
        <caption>
          <title>Illustration of a multi-layer graph convolutional network.</title>
        </caption>
        <graphic xlink:href="peerj-10-13163-g001" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>Generalized aggregation graph network</title>
      <p>For the purpose of training deeper GCN based on the basic GCN, <xref rid="ref-22" ref-type="bibr">Li et al. (2020)</xref> created a simple <italic toggle="yes">message-passing</italic>-based GCN that meets the message-passing requirements. Firstly, at the <italic toggle="yes">l</italic>-th layer, we consider that <inline-formula><alternatives><inline-graphic xlink:href="peerj-10-13163-i002.jpg"/><tex-math id="tex-ieqn-29">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${m}_{v}^{ \left( l \right) }\in {\mathbb{R}}^{D}$\end{document}</tex-math><mml:math id="mml-ieqn-29" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> is node <italic toggle="yes">v</italic>’s aggregated message, <inline-formula><alternatives><inline-graphic xlink:href="peerj-10-13163-i003.jpg"/><tex-math id="tex-ieqn-30">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${m}_{vu}^{ \left( l \right) }\in {\mathbb{R}}^{D}$\end{document}</tex-math><mml:math id="mml-ieqn-30" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> is an individual message for each neighbor <italic toggle="yes">u</italic> ∈ <italic toggle="yes">N</italic> (<italic toggle="yes">v</italic>) of node <italic toggle="yes">v</italic>. The neighbor’s message, node <italic toggle="yes">v</italic>’s aggregated message and its features are updated as following equations: <disp-formula id="eqn-5"><label>(5)</label><alternatives><graphic xlink:href="peerj-10-13163-e005.jpg" position="float"/><tex-math id="tex-eqn-5">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}{m}_{vu}^{ \left( l \right) }&amp; ={\rho }^{ \left( l \right) }({h}_{v}^{ \left( l \right) },{h}_{u}^{ \left( l \right) },{h}_{{e}_{vu}}^{ \left( l \right) })\end{eqnarray*}\end{document}</tex-math><mml:math id="mml-eqn-5" overflow="scroll"><mml:mstyle displaystyle="true"><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mfenced></mml:mrow></mml:mstyle></mml:math></alternatives></disp-formula>
<disp-formula id="eqn-6"><label>(6)</label><alternatives><graphic xlink:href="peerj-10-13163-e006.jpg" position="float"/><tex-math id="tex-eqn-6">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}{m}_{u}^{ \left( l \right) }&amp; ={\zeta }^{ \left( l \right) }({m}_{vu}^{ \left( l \right) })\end{eqnarray*}\end{document}</tex-math><mml:math id="mml-eqn-6" overflow="scroll"><mml:mstyle displaystyle="true"><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>ζ</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mfenced></mml:mrow></mml:mstyle></mml:math></alternatives></disp-formula>
<disp-formula id="eqn-7"><label>(7)</label><alternatives><graphic xlink:href="peerj-10-13163-e007.jpg" position="float"/><tex-math id="tex-eqn-7">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}{h}_{v}^{(l+1)}&amp; ={\phi }^{ \left( l \right) }({h}_{v}^{ \left( l \right) },{m}_{v}^{ \left( l \right) })\end{eqnarray*}\end{document}</tex-math><mml:math id="mml-eqn-7" overflow="scroll"><mml:mstyle displaystyle="true"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mfenced></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mfenced></mml:mrow></mml:mstyle></mml:math></alternatives></disp-formula>in which <italic toggle="yes">ρ</italic><sup>(<italic toggle="yes">l</italic>)</sup>, <italic toggle="yes">ζ</italic><sup>(<italic toggle="yes">l</italic>)</sup>, and <italic toggle="yes">ϕ</italic><sup>(<italic toggle="yes">l</italic>)</sup> are learnable or differentiable functions for respectively message constructions, message aggregation, and node update at the <italic toggle="yes">l</italic>-th layer.</p>
      <p>Li expanded <xref rid="eqn-12" ref-type="disp-formula">Eq. (12)</xref> by defining the message construction <italic toggle="yes">ρ</italic><sup>(<italic toggle="yes">l</italic>)</sup> as follow: <disp-formula id="eqn-8"><label>(8)</label><alternatives><graphic xlink:href="peerj-10-13163-e008.jpg" position="float"/><tex-math id="tex-eqn-8">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}{m}_{vu}^{ \left( l \right) }={\rho }^{ \left( l \right) }({h}_{v}^{ \left( l \right) },{h}_{u}^{ \left( l \right) },{h}_{{e}_{vu}}^{ \left( l \right) })=\text{ReLU}({h}_{u}^{ \left( l \right) }+\varphi ({h}_{{e}_{vu}}^{ \left( l \right) })\cdot {h}_{{e}_{vu}}^{ \left( l \right) })+\end{eqnarray*}\end{document}</tex-math><mml:math id="mml-eqn-8" overflow="scroll"><mml:mstyle displaystyle="true"><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mfenced></mml:mrow><mml:mo>=</mml:mo><mml:mtext>ReLU</mml:mtext><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>φ</mml:mi><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mfenced></mml:mrow><mml:mo>⋅</mml:mo><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mfenced></mml:mrow><mml:mo>+</mml:mo><mml:mi>ɛ</mml:mi></mml:mstyle></mml:math></alternatives></disp-formula>where ReLU is a rectified linear unit that outputs values to be greater or equal to 0; <italic toggle="yes">φ</italic>(.) is an indicator function giving 1 when there are edge features, and 0 for otherwise; <italic toggle="yes">ɛ</italic> is a small positive constant chosen as 10<sup>−7</sup>.</p>
      <p>The message aggregation <italic toggle="yes">ζ</italic><sup>(<italic toggle="yes">l</italic>)</sup> function was proposed to be either SoftMax Aggregation or PowerMean Aggregation. In order to construct these two functions, Li also suggested a concept called Generalized Message Aggregation Function: a generalized message aggregation function <italic toggle="yes">ζ</italic><sub><italic toggle="yes">x</italic></sub>(.) is defined as that is parameterized by a continuous variable <italic toggle="yes">x</italic> to produce a group of permutation invariant set functions. From this definition, they continued to propose the Generalized Mean-Max Aggregation: <italic toggle="yes">ζ</italic><sub><italic toggle="yes">x</italic></sub>(.) is a generalized mean-max aggregation function if a pair of <italic toggle="yes">x</italic> = {x<sub>1</sub>, x<sub>2</sub>} exists such that for any message, <italic toggle="yes">lim</italic><sub><italic toggle="yes">x</italic>→<italic toggle="yes">x</italic><sub>1</sub></sub><italic toggle="yes">ζ</italic><sub><italic toggle="yes">x</italic></sub>(⋅) = <italic toggle="yes">Mean</italic>(⋅) and <inline-formula><alternatives><inline-graphic xlink:href="peerj-10-13163-i004.jpg"/><tex-math id="tex-ieqn-47">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$li{m}_{x\rightarrow {x}_{2}}{\zeta }_{x} \left( \cdot \right) =Max(\cdot )$\end{document}</tex-math><mml:math id="mml-ieqn-47" overflow="scroll"><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>→</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>ζ</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mo>⋅</mml:mo></mml:mfenced></mml:mrow></mml:math></alternatives></inline-formula>. Given any message set <italic toggle="yes">m</italic><sub><italic toggle="yes">vu</italic></sub> ∈ℝ<sup><italic toggle="yes">D</italic></sup>, SoftMax Aggregation and PowerMean Aggregation are generalized functions respectively defined as: <disp-formula id="eqn-9"><label>(9)</label><alternatives><graphic xlink:href="peerj-10-13163-e009.jpg" position="float"/><tex-math id="tex-eqn-9">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}SoftMax\text{_}Ag{g}_{\beta }(\cdot )&amp; =\sum _{u\in N(v)} \frac{\exp \nolimits (\beta {m}_{vu})}{\sum _{i\in N(v)}\exp \nolimits (\beta {m}_{vi})} \end{eqnarray*}\end{document}</tex-math><mml:math id="mml-eqn-9" overflow="scroll"><mml:mstyle displaystyle="true"><mml:mi>S</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mtext>_</mml:mtext><mml:mi>A</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mo>⋅</mml:mo></mml:mfenced></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo mathsize="big" movablelimits="false"> ∑</mml:mo></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>∈</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>v</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mo class="qopname"> exp</mml:mo><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>β</mml:mi><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mfenced></mml:mrow></mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mo mathsize="big" movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>v</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:munder><mml:mo class="qopname"> exp</mml:mo><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>β</mml:mi><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mfenced></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:math></alternatives></disp-formula>
<disp-formula id="eqn-10"><label>(10)</label><alternatives><graphic xlink:href="peerj-10-13163-e010.jpg" position="float"/><tex-math id="tex-eqn-10">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}PowerMean\text{_}Ag{g}_{p}(\cdot )&amp; ={ \left( \frac{1}{ \left\vert N(v) \right\vert } \sum _{u\in N(v)}{m}_{vu}^{p} \right) }^{ \frac{1}{p} }\end{eqnarray*}\end{document}</tex-math><mml:math id="mml-eqn-10" overflow="scroll"><mml:mstyle displaystyle="true"><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>M</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mtext>_</mml:mtext><mml:mi>A</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mo>⋅</mml:mo></mml:mfenced></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mfenced separators="" open="|" close="|"><mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>v</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:munder><mml:mrow><mml:mo mathsize="big" movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>∈</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>v</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:munder><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:mstyle></mml:math></alternatives></disp-formula>where <italic toggle="yes">β</italic> is a continuous variable called inverse temperature, and <italic toggle="yes">p</italic> is a non-zero continuous variable denoting <italic toggle="yes">p</italic>-th power.</p>
      <p>Finally, in the phase of node update, Li applied a message normalization layer to the node update function, hence the function <xref rid="eqn-14" ref-type="disp-formula">Eq. (14)</xref> became: <disp-formula id="eqn-11"><label>(11)</label><alternatives><graphic xlink:href="peerj-10-13163-e011.jpg" position="float"/><tex-math id="tex-eqn-11">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}{h}_{v}^{(l+1)}={\phi }^{ \left( l \right) }({h}_{v}^{ \left( l \right) },{m}_{v}^{ \left( l \right) })=MLP \left( {h}_{v}^{ \left( l \right) }+s\cdot { \left\| {h}_{v}^{ \left( l \right) } \right\| }_{2}\cdot \frac{{m}_{v}^{ \left( l \right) }}{{ \left\| {m}_{v}^{ \left( l \right) } \right\| }_{2}} \right) \end{eqnarray*}\end{document}</tex-math><mml:math id="mml-eqn-11" overflow="scroll"><mml:mstyle displaystyle="true"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mfenced></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mfenced></mml:mrow><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:mi>P</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mfenced separators="" open="∥" close="∥"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mfenced separators="" open="∥" close="∥"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mstyle></mml:math></alternatives></disp-formula>where <italic toggle="yes">MLP</italic> (⋅) is a multi-layer perceptron, and s is a learnable scaling factor. In practice, s is set to be a learnable scalar with an initialized value of 1.</p>
    </sec>
    <sec>
      <title>Hypergraph convolution and hypergraph attention</title>
      <p>Most existing studies on GNNs consider the graphs as simple graphs, <italic toggle="yes">i.e.</italic>, each edge in a graph only connects two nodes. To describe more complicated graph structure in practical applications, the concept of hypergraph, the case where one edge can link more than two nodes (vertices), has been further studied. In this case, we can consider <bold><italic toggle="yes">G</italic></bold> = (<bold><italic toggle="yes">V</italic></bold>, <bold><italic toggle="yes">E</italic></bold>) as a hypergraph of <italic toggle="yes">n</italic> nodes and <italic toggle="yes">m</italic> hyper-edges. Each hyper-edge <italic toggle="yes">e</italic> ∈ <bold><italic toggle="yes">E</italic></bold> is presented by a positive weight value <italic toggle="yes">W</italic><sub><italic toggle="yes">ee</italic></sub> and all the weights build up a diagonal matrix <italic toggle="yes">W</italic> ∈ℝ<sup><italic toggle="yes">m</italic>×<italic toggle="yes">m</italic></sup>. While an adjacency matrix <italic toggle="yes">A</italic>, as defined by (9), is used to represent a simple graph, it is an incidence matrix <italic toggle="yes">H</italic> ∈ℝ<sup><italic toggle="yes">n</italic>×<italic toggle="yes">m</italic></sup> that is employed to represent the hypergraph <bold><italic toggle="yes">G</italic></bold>: the element <italic toggle="yes">H</italic><sub><italic toggle="yes">ie</italic></sub> of <italic toggle="yes">H</italic> is 1 when the hyper-edge <italic toggle="yes">e</italic> has a link to node <italic toggle="yes">v</italic><sub><italic toggle="yes">i</italic></sub>, otherwise it is 0.</p>
      <p><xref rid="fig-2" ref-type="fig">Figure 2</xref> depicts the visual distinction between a simple graph and a hypergraph, which are (A) and (C), respectively. Each edge of (A), shown by a line, simply connects two nodes in a basic graph. Each hyperedge of (C), marked by a colored ellipse in a hypergraph, connects more than two vertices. Matrix (B) and (D) are respectively the representation form of the simple graph (A) and the hypergraph (C).</p>
      <p>In terms of methodology, hypergraph convolution approximates each hyperedge of the hypergraph with a set of pairwise edges connecting the hyperedge’s nodes, and the learning issue has been treated as a graph-learning problem on the approximation.</p>
      <fig position="float" id="fig-2">
        <object-id pub-id-type="doi">10.7717/peerj.13163/fig-2</object-id>
        <label>Figure 2</label>
        <caption>
          <title>Depiction of a simple graph and a hypergraph.</title>
          <p>(A) A simple graph of six nodes and six edges (B) The adjacency matrix of the simple graph (A) (C) A hypergraph of seven nodes and three hyper-edges (D) The incidence matrix of the hypergraph (C).</p>
        </caption>
        <graphic xlink:href="peerj-10-13163-g002" position="float"/>
      </fig>
      <p>To define a convolution in a hypergraph, <xref rid="ref-2" ref-type="bibr">Bai, Zhang &amp; Torr (2021)</xref> assumed that more propagation functions should be applied on the nodes linked by a common hyperedge, and the hyperedges with larger weights deserve more confidence in such a propagation. The proposed the hypergraph convolution is demonstrated as: <disp-formula id="eqn-12"><label>(12)</label><alternatives><graphic xlink:href="peerj-10-13163-e012.jpg" position="float"/><tex-math id="tex-eqn-12">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}{x}_{i}^{(l+1)}=\sigma \left( \sum _{j=1}^{n}\sum _{i=1}^{m}{H}_{ie}{H}_{je}{W}_{ee}{x}_{j}^{ \left( l \right) }P \right) \end{eqnarray*}\end{document}</tex-math><mml:math id="mml-eqn-12" overflow="scroll"><mml:mstyle displaystyle="true"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mfenced></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:munderover><mml:mrow><mml:mo mathsize="big" movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mrow><mml:mo mathsize="big" movablelimits="false"> ∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mi>P</mml:mi></mml:mrow></mml:mfenced></mml:mstyle></mml:math></alternatives></disp-formula>where <inline-formula><alternatives><inline-graphic xlink:href="peerj-10-13163-i005.jpg"/><tex-math id="tex-ieqn-63">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${x}_{i}^{(l)}$\end{document}</tex-math><mml:math id="mml-ieqn-63" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>l</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> is the embedding representation of node <italic toggle="yes">v</italic><sub><italic toggle="yes">i</italic></sub> in the <italic toggle="yes">l</italic>-th layer, <italic toggle="yes">σ</italic> is a non-linear function which can be, for example, eLU (<xref rid="ref-6" ref-type="bibr">Clevert, Unterthiner &amp; Hochreiter, 2016</xref>) and LeakyReLU (<xref rid="ref-26" ref-type="bibr">Maas, Hannun &amp; Ng, 2013</xref>), and <italic toggle="yes">P</italic> is the weight matrix between <italic toggle="yes">l</italic>-th and (<italic toggle="yes">l+</italic> 1)-th layer. <xref rid="eqn-12" ref-type="disp-formula">Equation (12)</xref> can also be described in a matrix form as: <disp-formula id="eqn-13"><label>(13)</label><alternatives><graphic xlink:href="peerj-10-13163-e013.jpg" position="float"/><tex-math id="tex-eqn-13">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}{X}^{(l+1)}=\sigma ({HWH}^{\mathrm{T}}{X}^{(l)}P).\end{eqnarray*}\end{document}</tex-math><mml:math id="mml-eqn-13" overflow="scroll"><mml:mstyle displaystyle="true"><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mfenced></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>l</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:msup><mml:mi>P</mml:mi></mml:mfenced></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:math></alternatives></disp-formula>Bai also stated that stacking multiple layers like the operator <xref rid="eqn-20" ref-type="disp-formula">Eq. (20)</xref> could result in numerical instabilities and a possible risk of vanishing gradients in a neural networks. Hence, a symmetric normalization was imposed on <xref rid="eqn-19" ref-type="disp-formula">Eq. (19)</xref> and made it become: <disp-formula id="eqn-14"><label>(14)</label><alternatives><graphic xlink:href="peerj-10-13163-e014.jpg" position="float"/><tex-math id="tex-eqn-14">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}{X}^{(l+1)}=\sigma ({D}^{- \frac{1}{2} }HW{B}^{-1}{H}^{\mathrm{T}}{D}^{- \frac{1}{2} }{X}^{ \left( l \right) }P)\end{eqnarray*}\end{document}</tex-math><mml:math id="mml-eqn-14" overflow="scroll"><mml:mstyle displaystyle="true"><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mfenced></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mi>H</mml:mi><mml:mi>W</mml:mi><mml:msup><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mi>P</mml:mi></mml:mfenced></mml:mrow></mml:mstyle></mml:math></alternatives></disp-formula>where <italic toggle="yes">D</italic> ∈ℝ<sup><italic toggle="yes">n</italic>×<italic toggle="yes">n</italic></sup> and <italic toggle="yes">B</italic> ∈ℝ<sup><italic toggle="yes">m</italic>×<italic toggle="yes">m</italic></sup> are respectively the degree matrix and hyperedge matrix of hypergraph <italic toggle="yes">G</italic>, defined as: <disp-formula id="eqn-15"><label>(15)</label><alternatives><graphic xlink:href="peerj-10-13163-e015.jpg" position="float"/><tex-math id="tex-eqn-15">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}{D}_{ii}&amp; =\sum _{e=1}^{m}{W}_{ee}{H}_{ie}\end{eqnarray*}\end{document}</tex-math><mml:math id="mml-eqn-15" overflow="scroll"><mml:mstyle displaystyle="true"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo mathsize="big" movablelimits="false"> ∑</mml:mo></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></alternatives></disp-formula>
<disp-formula id="eqn-16"><label>(16)</label><alternatives><graphic xlink:href="peerj-10-13163-e016.jpg" position="float"/><tex-math id="tex-eqn-16">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}{B}_{ee}&amp; =\sum _{i=1}^{n}{H}_{ie}.\end{eqnarray*}\end{document}</tex-math><mml:math id="mml-eqn-16" overflow="scroll"><mml:mstyle displaystyle="true"><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo mathsize="big" movablelimits="false"> ∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:math></alternatives></disp-formula>
</p>
      <p>According to <xref rid="ref-40" ref-type="bibr">Velickovic et al. (2018)</xref> and <xref rid="ref-21" ref-type="bibr">Lee et al. (2019)</xref>, hypergraph convolution also owns a sort of attentional mechanism. However, such mechanism is not able to be learned nor trained in a graph structure (represented by the incidence matrix H). The purpose of attention in hypergraph is to learn a dynamic incidence matrix, followed by a dynamic transition matrix that can better reveal the intrinsic relationship between the nodes. <xref rid="ref-2" ref-type="bibr">Bai, Zhang &amp; Torr (2021)</xref> suggested to solve this issue by applying an attention learning module on the matrix <italic toggle="yes">H</italic>. The overall demonstration of hypergraph convolution—hypergraph attention is displayed in <xref rid="fig-3" ref-type="fig">Fig. 3</xref> as follow.</p>
      <fig position="float" id="fig-3">
        <object-id pub-id-type="doi">10.7717/peerj.13163/fig-3</object-id>
        <label>Figure 3</label>
        <caption>
          <title>Schematic depiction of hypergraph convolution for a hypergraph of seven nodes and three hyper-edges.</title>
        </caption>
        <graphic xlink:href="peerj-10-13163-g003" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>Related works</title>
      <p>As presented in the first section, drug discovery is an essential purpose of the development of cheminformatics field. Specifically, significant tasks in cheminformatics that are now strongly supported by computational algorithms, especially deep learning, can include activity prediction, de novo generation, ADMET prediction, interaction prediction, and binding affinity prediction.</p>
      <p>Activity prediction refers to a type of classification in which we study to see whether a specific drug has activity against one or more specific targets. A remarkable work in activity prediction using deep learning was presented by <xref rid="ref-41" ref-type="bibr">Wallach, Dzamba &amp; Heifets (2015)</xref>. In this research, Wallach used a deep CNNs model that could learn structure-based molecular data and succeeded in predicting active molecules against their chosen targets.</p>
      <p>Differently, de novo generation basically aims at generating novel molecules. Leading in adopting GNNs for de novo generation, <xref rid="ref-9" ref-type="bibr">De Cao &amp; Kipf (2018)</xref> introduced a graph learning model which was originally a GANs (Generative Adversarial Networks) framework operating on graph data. The model was proved to be able to generate diverse and novel molecules. For the same objective, <xref rid="ref-4" ref-type="bibr">Bresson &amp; Laurent (2019)</xref> took advantage of graph auto encoder: they made GCNs layers to learn the representations of molecular data, next imposed them in a latent space, and then learned to reconstruct them back to graph data. Moreover, the graph generative model by <xref rid="ref-24" ref-type="bibr">Lim et al. (2019)</xref> successfully learned the scaffolds of molecules and constructed unseen molecules during learning.</p>
      <p>Meanwhile, ADMET prediction is a drug discovery approach that can comprise of two path of resolution: predictive modeling, and generative modeling. In the scope of cheminformatics research, ADMET stands for <italic toggle="yes">absorption, distribution, metabolism, elimination, and toxicity</italic>, which are fundamental and crucial properties of drug candidates for their efficacy and safety in practical use. For ADMET prediction, firstly, we decide the molecular properties that we would like the drug to have. In predictive modeling, we aim at searching among existing compounds to find the ones that possess such properties. Suggested deep learning model by <xref rid="ref-10" ref-type="bibr">Feinberg et al. (2020)</xref> using graph convolutional neural networks had shown that GNN could learn from adjacency matrix and feature matrix of data to predict ADMET properties with higher accuracy compared to random forests and deep neural networks. Inversely, in generative modeling, computational models are made to generate molecules whose properties are matching the expected properties. A notable work in this problem was the research proposed by <xref rid="ref-36" ref-type="bibr">Segler et al. (2018)</xref>.</p>
      <p>Similar to activity prediction, ligand-protein interaction prediction is also a classification problem, but it takes the information of ligand and protein simultaneously. One of early researches in this approach is suggested by <xref rid="ref-14" ref-type="bibr">Gonczarek et al. (2018)</xref>. Their deep learning architecture was utilized to learn structure-based data of molecules and proteins, and to predict molecule-protein interaction. Next, the end-to-end model which combined GNN and CNN introduced by <xref rid="ref-39" ref-type="bibr">Tsubaki, Tomii &amp; Sese (2019)</xref> had proved to earn better evaluation results than various existing interaction prediction methods, such as a k-nearest neighbor, random forest, logistic regression, and support vector machine. In their model, GNN was applied to learn the ligands’ molecular fingerprints, and CNN was for proteins learning.</p>
      <p>Lastly, binding affinity prediction is quite comparable to ligand-protein interaction prediction. This, however, is a regression problem that provides real values reflecting the <italic toggle="yes">relationship</italic> between ligands and proteins. It can be said that ligand-protein interaction prediction and binding affinity prediction are relatable because the affinity values can tell how <italic toggle="yes">strong</italic> the ligand-protein interaction is. In our article, binding affinity is alternatively referred as drug-target interaction (DTI). Current most studied learning methods for DTI prediction can be categorized into two approaches: supervised learning-based methods, for example: the study of <xref rid="ref-43" ref-type="bibr">Yamanishi et al. (2008)</xref>, and semi-supervised learning-based methods, for which the research of <xref rid="ref-31" ref-type="bibr">Peng et al. (2017)</xref> is a remarkable work. One noticeable DTI prediction outcome came from the research of <xref rid="ref-19" ref-type="bibr">Karimi et al. (2019)</xref>: in their DeepAffinity model, firstly, they engineered associate degree end-to-end deep learning model combining repeated neural networks (RNN) and convolutional neural networks (CNN) for learning representations of compounds and supermolecule targets, and for the prediction of compound-protein affinity; secondly, they expanded the model by using graph CNN, which is a basic form of GCN, to learn 2-D representations of compounds from SMILES strings. The latter model with GCN did beat the former one which was unified RNN-CNN when comparing results, and importantly it has shown that graph data and graph neural networks are potential for designing deep learning models and affinity prediction. Separately, other remarkable researches for DTI prediction using GNNs can include proposed work PADME (<xref rid="ref-11" ref-type="bibr">Feng et al., 2018</xref>) and GraphDTA (<xref rid="ref-27" ref-type="bibr">Nguyen et al., 2021</xref>). Experimental results from GraphDTA have shown superior performance of GNNs models over DeepDTA (<xref rid="ref-28" ref-type="bibr">Öztürk, Özgür &amp; Ozkirimli, 2018</xref>) and WideDTA (<xref rid="ref-29" ref-type="bibr">Öztürk, Ozkirimli &amp; Özgür, 2019</xref>) model, which are also well-known baseline <italic toggle="yes">non-GNNs</italic> cheminformatics researches that put the fundamental design for deep learning DTI prediction. The common characteristic of these four studies (PADME, DeepDTA, WideDTA, GraphDTA) is that they are all data-driven methods, which means their models are built to automatically learn the features of the inputs (<xref rid="ref-1" ref-type="bibr">Abbasi et al., 2020</xref>). Beside above frameworks where drugs and targets features are learned independently and simultaneously, research work of <xref rid="ref-23" ref-type="bibr">Li et al. (2021)</xref> suggested a new approach when they treated each pair of drug-target as one unit and learn the representation for each pair by GCN; the affinity values are predicted by a following deep neural network.</p>
      <fig position="float" id="fig-4">
        <object-id pub-id-type="doi">10.7717/peerj.13163/fig-4</object-id>
        <label>Figure 4</label>
        <caption>
          <title>Diagram of proposed DeepNC architecture.</title>
        </caption>
        <graphic xlink:href="peerj-10-13163-g004" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="materials|methods">
    <title>Materials &amp; Methods</title>
    <sec>
      <title>Design of DeepNC</title>
      <p>In this section, we are going to explain our proposed model DeepNC whose primary components are visualized in detail in <xref rid="fig-4" ref-type="fig">Fig. 4</xref>. Our model comprises of three main blocks: the first block (1)’s job is to translate the input data into the format that can be fed into the convolutional layers. For drugs, the input data is in form of SMILES strings and these strings will be converted into graphs containing information of the drug compounds features; meanwhile, the input targets which are initially in form of ASCII strings called target sequence will be embedded into vector representation for the next 1-D convolutional layers. Next, the second block (2) contains convolutional layers that will learn the drugs and targets features: GNNs layers learn drugs features to create drugs graph-based representation and at the same time, 1-D convolutional layers learn targets features to generate targets representation. In the third block (3), the representations are concatenated and fed into fully connected layers to calculate the binding affinity values. Details of each block will be discussed in the following sub-sections.</p>
    </sec>
    <sec>
      <title>Representation of drug</title>
      <p>To connect chemistry language with computing language, we employ SMILES, which represents molecular compounds as ASCII characters strings, to be the format of input drugs data. SMILES stands for Simplified Molecular-input Line-entry System—a system that denotes chemical compounds as line notation: molecules are described in alphabets and characters.</p>
      <p>The SMILES strings of drug compounds will be converted into molecular graphs which contains important features of drugs and which is the data format that can be fed into GNNs layers. Each molecular graph has to have these information: the number of atoms (of the compound), atom features and edge index. In graph-learning language, edges represent the bonds between atoms and the atoms are alternatively called nodes. Atom features of a molecule is a set of features describing a node in the graph. In this study, we used five classes of information to demonstrate atom features: the atom symbol (symbols that are present in the SMILES strings), the number of adjacent atoms, the number of adjacent hydrogen, the implicit valence of the atom and whether the atom is in aromatic structure. In order to create molecular graphs from SMILES for learning tasks, we employed RDKit (<xref rid="ref-33" ref-type="bibr">RDKit, 2022</xref>) and PyTorch (<xref rid="ref-30" ref-type="bibr">Paszke et al., 2017</xref>).</p>
    </sec>
    <sec>
      <title>Representation of target</title>
      <p>In studied drug-target datasets, each target is demonstrated in a protein sequence. Such sequence is an ASCII string representing amino acids and is obtained from UniProt database (<xref rid="ref-18" ref-type="bibr">Jain et al., 2009</xref>) using the target’s gene name. Specifically, there are 20 amino acids in nature which contribute to create a protein.</p>
    </sec>
    <sec>
      <title>Drug-target interaction</title>
      <p>Interaction between a drug and a target can be recognized by the value of binding affinity. Binding affinity is defined as a measurement that can be used to estimate the strength of the interaction between a single biomolecule and its binding partner, which is also termed as ligand. It can be quantified, providing information on whether or not molecules are interacting as well as assigning a value to the affinity. Typically, when measuring binding affinity, researchers are interested in several parameters, but mostly in the unit of measurement called the dissociation constant (K<sub>d</sub>), which defines the likelihood that an interaction between two molecules will break (<xref rid="ref-13" ref-type="bibr">Gilson, 2010</xref>).</p>
    </sec>
    <sec>
      <title>Graph convolutional layers</title>
      <p>The proposed DeepNC framework includes two variants namely GEN and HGC-GCN, as shown in the diagram in <xref rid="fig-3" ref-type="fig">Fig. 3</xref>. The variant GEN contains three GENConv layers and one global add pooling layer, while the variant HGC-GCN contains 2 HypergraphConv layers, one GCNConv layer and one global max pooling. These layers are used to produce graph-based representation of input drugs.</p>
      <p>GENConv is a generalized graph convolution layer adopted from the research work (<xref rid="ref-22" ref-type="bibr">Li et al., 2020</xref>) which has earlier been mentioned in Generalized Aggregation Graph Network. From formula <xref rid="eqn-7" ref-type="disp-formula">Eqs. (7)</xref> and <xref rid="eqn-11" ref-type="disp-formula">(11)</xref>, we simplified the message construction function as: <disp-formula id="eqn-17"><label>(17)</label><alternatives><graphic xlink:href="peerj-10-13163-e017.jpg" position="float"/><tex-math id="tex-eqn-17">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}{x}_{i}^{{^{\prime}}}=MLP({x}_{i}+AGG\{ (RELU({x}_{j}+{e}_{ij})+{|}j\in N(i)\} ))\end{eqnarray*}\end{document}</tex-math><mml:math id="mml-eqn-17" overflow="scroll"><mml:mstyle displaystyle="true"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mi>G</mml:mi><mml:mi>G</mml:mi><mml:mrow><mml:mfenced separators="" open="{" close=""/><mml:mrow><mml:mfenced separators="" open="(" close=""/><mml:mi>R</mml:mi><mml:mi>E</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mfenced></mml:mrow><mml:mo>+</mml:mo><mml:mi>ɛ</mml:mi><mml:mo>|</mml:mo><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>i</mml:mi></mml:mfenced></mml:mrow><mml:mfenced separators="" open="" close="}"/></mml:mrow><mml:mfenced separators="" open="" close=")"/></mml:mrow></mml:mfenced></mml:mrow></mml:mstyle></mml:math></alternatives></disp-formula>where <italic toggle="yes">MLP</italic> (⋅) is a multi-layer perceptron and the aggregation scheme to use is <italic toggle="yes">softmax_sg</italic>. The global add pooling layer in GEN returns batch-wise graph-level-outputs by adding node features across the node dimension. The GENConv layer can be depicted by <xref rid="fig-5" ref-type="fig">Fig. 5</xref>.</p>
      <fig position="float" id="fig-5">
        <object-id pub-id-type="doi">10.7717/peerj.13163/fig-5</object-id>
        <label>Figure 5</label>
        <caption>
          <title>Diagram of a GENConv layer.</title>
        </caption>
        <graphic xlink:href="peerj-10-13163-g005" position="float"/>
      </fig>
      <table-wrap position="float" id="table-1">
        <object-id pub-id-type="doi">10.7717/peerj.13163/table-1</object-id>
        <label>Table 1</label>
        <caption>
          <title>Summary of studied datasets.</title>
        </caption>
        <alternatives>
          <graphic xlink:href="peerj-10-13163-g012" position="float"/>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">
                  <bold>Datasets</bold>
                </th>
                <th rowspan="1" colspan="1">
                  <bold>Number of targets</bold>
                </th>
                <th rowspan="1" colspan="1">
                  <bold>Number of compounds</bold>
                </th>
                <th rowspan="1" colspan="1">
                  <bold>Number of interaction pairs</bold>
                </th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">Davis</td>
                <td rowspan="1" colspan="1">442</td>
                <td rowspan="1" colspan="1">68</td>
                <td rowspan="1" colspan="1">30056</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Kiba</td>
                <td rowspan="1" colspan="1">229</td>
                <td rowspan="1" colspan="1">2111</td>
                <td rowspan="1" colspan="1">118254</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Allergy</td>
                <td rowspan="1" colspan="1">35</td>
                <td rowspan="1" colspan="1">286</td>
                <td rowspan="1" colspan="1">372</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>HypergraphConv is a hypergraph convolutional operator adopted from the research work (<xref rid="ref-2" ref-type="bibr">Bai, Zhang &amp; Torr, 2021</xref>) which has been explained in Hypergraph Convolution and Hypergraph Attention. The operator is simplified from <xref rid="eqn-13" ref-type="disp-formula">Eq. (13)</xref> and presented as: <disp-formula id="eqn-18"><label>(18)</label><alternatives><graphic xlink:href="peerj-10-13163-e018.jpg" position="float"/><tex-math id="tex-eqn-18">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}{X}^{{^{\prime}}}={D}^{-1}HW{B}^{-1}{H}^{T}X\Theta \end{eqnarray*}\end{document}</tex-math><mml:math id="mml-eqn-18" overflow="scroll"><mml:mstyle displaystyle="true"><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>H</mml:mi><mml:mi>W</mml:mi><mml:msup><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>X</mml:mi><mml:mi>Θ</mml:mi></mml:mstyle></mml:math></alternatives></disp-formula>where <italic toggle="yes">H</italic> ∈{0, 1}<sup><italic toggle="yes">N</italic>×<italic toggle="yes">M</italic></sup> is the incidence matrix, W ∈ℝ<sup><italic toggle="yes">M</italic></sup> is the diagonal hyperedge weight matrix, and and <italic toggle="yes">D</italic>, <italic toggle="yes">B</italic> are the corresponding degree and hyperedge matrices. An attention will be added to this layer. Illustration of a HypergraphConv layer has been shown in <xref rid="fig-3" ref-type="fig">Fig. 3</xref>.</p>
      <p>GCNConv is a graph convolutional layer extracted from the research (<xref rid="ref-20" ref-type="bibr">Kipf &amp; Welling, 2017</xref>). The operator of the layer is written as: <disp-formula id="eqn-19"><label>(19)</label><alternatives><graphic xlink:href="peerj-10-13163-e019.jpg" position="float"/><tex-math id="tex-eqn-19">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}{X}^{{^{\prime}}}={\hat {D}}^{- \frac{1}{2} }\hat {A}{\hat {D}}^{- \frac{1}{2} }X\Theta \end{eqnarray*}\end{document}</tex-math><mml:math id="mml-eqn-19" overflow="scroll"><mml:mstyle displaystyle="true"><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mo> ˆ</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo> ˆ</mml:mo></mml:mrow></mml:mover><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mo> ˆ</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mi>X</mml:mi><mml:mi>Θ</mml:mi></mml:mstyle></mml:math></alternatives></disp-formula>
</p>
    </sec>
    <sec>
      <title>Datasets</title>
      <p>Our proposed model is evaluated on two different datasets: Davis and Kiba. Numbers of compounds and targets of each dataset are noted in <xref rid="table-1" ref-type="table">Table 1</xref>. Davis is a Kinase dataset that was introduced by <xref rid="ref-8" ref-type="bibr">Davis et al. (2011)</xref>, containing 25,046 pairs of drug-target having binding affinities measured as K<sub>d</sub> with values ranging from 5.0 to 10.8 (nM). Meanwhile, Kiba dataset was contributed by <xref rid="ref-37" ref-type="bibr">Tang et al. (2014)</xref>, and it has binding affinities measured as Kiba score with values ranging from 0.0 to 17.2. The SMILES strings of compounds from both datasets were originally obtained from the PubChem compound database (<xref rid="ref-42" ref-type="bibr">Wang et al., 2017</xref>) based on their PubChem CIDs and their protein sequences were extracted from the UniProt protein database.</p>
      <p>Beside two above baseline datasets, we are building a potential dataset called ‘allergy drugs’ dataset and also involving it in training and evaluating DeepNC. This dataset are formed by firstly investigating and collecting the drugs which are used to treat allergic reactions, which are referred as ‘allergy drugs’ in this research, and their respective targets; the list of allergy drugs and their targets is achieved from DrugBank; secondly, finding and aggregating the ligands that have interactions with the allergy drug targets and noting down together with their K<sub>d</sub> values. Specifically, chosen ligand-target pairs have K<sub>d</sub> ranging from 1.0 to 15.0 (nM). SMILES strings of compounds and K<sub>d</sub> values were extracted from BindingDB database and the target sequences were extracted from the UniProt protein database based on accession numbers. Motivated by the aim of searching for medication solutions for allergies treatment, this proposed dataset is our effort in contributing in the research and discovery of allergy drugs. Summarized information of this independent dataset is also noted in <xref rid="table-1" ref-type="table">Table 1</xref>.</p>
    </sec>
    <sec>
      <title>Evaluation metrics</title>
      <p>Two metrics that are used to evaluate the model performance are mean square error (MSE) and concordance index (CI). MSE reflects the difference between the predicted values and the expected (actual) values. During training, a learning model attempts to reduce the gap between the actual value and the prediction. We used MSE as the loss function because we are working on a regression problem, where <italic toggle="yes">P</italic> is the prediction vector and <italic toggle="yes">Y</italic> is the vector of actual <italic toggle="yes">n</italic> outputs. The number <italic toggle="yes">n</italic> denotes the sample size. MSE is determined using the following formula: <disp-formula id="eqn-20"><label>(20)</label><alternatives><graphic xlink:href="peerj-10-13163-e020.jpg" position="float"/><tex-math id="tex-eqn-20">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}MSE= \frac{1}{n} \sum _{i=1}^{n}({P}_{i}-{Y}_{i})^{2}\end{eqnarray*}\end{document}</tex-math><mml:math id="mml-eqn-20" overflow="scroll"><mml:mstyle displaystyle="true"><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mrow><mml:mo mathsize="big" movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mfenced></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></alternatives></disp-formula>
</p>
      <p>In order to state whether the order of a predicted value of two random drug-target pairs is identical to the order of the true value, we use the Concordance Index (CI). The calculation of CI is in accordance with <xref rid="eqn-21" ref-type="disp-formula">(21)</xref>
<disp-formula id="eqn-21"><label>(21)</label><alternatives><graphic xlink:href="peerj-10-13163-e021.jpg" position="float"/><tex-math id="tex-eqn-21">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}CI= \frac{1}{Z} \sum _{{\delta }_{i}\gt {\delta }_{j}}h({b}_{i}-{b}_{j})\end{eqnarray*}\end{document}</tex-math><mml:math id="mml-eqn-21" overflow="scroll"><mml:mstyle displaystyle="true"><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mfrac><mml:munder><mml:mrow><mml:mo mathsize="big" movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mi>h</mml:mi><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mfenced></mml:mrow></mml:mstyle></mml:math></alternatives></disp-formula>where <italic toggle="yes">b</italic><sub><italic toggle="yes">i</italic></sub> is the predicted value for the larger affinity <italic toggle="yes">δ</italic><sub><italic toggle="yes">i</italic></sub>, <italic toggle="yes">b</italic><sub><italic toggle="yes">j</italic></sub> is the predicted value for the smaller affinity <italic toggle="yes">δ</italic><sub><italic toggle="yes">i</italic></sub>, <italic toggle="yes">Z</italic> is a normalization constant, <italic toggle="yes">h</italic> (<italic toggle="yes">x</italic>) is the step function presented as: <disp-formula id="eqn-22"><label>(22)</label><alternatives><graphic xlink:href="peerj-10-13163-e022.jpg" position="float"/><tex-math id="tex-eqn-22">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}h(x)= \left\{ \begin{array}{@{}l@{}} \displaystyle 1 \mathrm{if} x\gt 0\\ \displaystyle 0.5 \mathrm{if} x=0\\ \displaystyle 0 \mathrm{if} x\lt 0 \end{array} \right. \end{eqnarray*}\end{document}</tex-math><mml:math id="mml-eqn-22" overflow="scroll"><mml:mstyle displaystyle="true"><mml:mi>h</mml:mi><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>x</mml:mi></mml:mfenced></mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mn>1</mml:mn><mml:mspace width="2.0pt"/><mml:mi mathvariant="normal">if</mml:mi><mml:mspace width="2.0pt"/><mml:mi>x</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>5</mml:mn><mml:mspace width="2.0pt"/><mml:mi mathvariant="normal">if</mml:mi><mml:mspace width="2.0pt"/><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn><mml:mspace width="2.0pt"/><mml:mi mathvariant="normal">if</mml:mi><mml:mspace width="2.0pt"/><mml:mi>x</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mstyle></mml:math></alternatives></disp-formula>
</p>
      <table-wrap position="float" id="table-2">
        <object-id pub-id-type="doi">10.7717/peerj.13163/table-2</object-id>
        <label>Table 2</label>
        <caption>
          <title>Parameters setting for DeepNC models.</title>
        </caption>
        <alternatives>
          <graphic xlink:href="peerj-10-13163-g013" position="float"/>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col span="1"/>
              <col span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">
                  <bold>
                    <italic toggle="yes">Parameters</italic>
                  </bold>
                </th>
                <th rowspan="1" colspan="1">
                  <bold>
                    <italic toggle="yes">Settings</italic>
                  </bold>
                </th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">Learning rate</td>
                <td rowspan="1" colspan="1">0.0005</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Batch size</td>
                <td rowspan="1" colspan="1">256</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Epoch</td>
                <td rowspan="1" colspan="1">1000</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Optimizer</td>
                <td rowspan="1" colspan="1">Adam</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Graph convolutional layers in GEN</td>
                <td rowspan="1" colspan="1">3</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Graph convolutional layers in HGC-GCN</td>
                <td rowspan="1" colspan="1">3</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <fig position="float" id="fig-6">
        <object-id pub-id-type="doi">10.7717/peerj.13163/fig-6</object-id>
        <label>Figure 6</label>
        <caption>
          <title>Performance of DeepNC and GraphDTA training models on the Davis dataset.</title>
          <p>(A) MSE Values, (B) CI Values.</p>
        </caption>
        <graphic xlink:href="peerj-10-13163-g006" position="float"/>
      </fig>
      <fig position="float" id="fig-7">
        <object-id pub-id-type="doi">10.7717/peerj.13163/fig-7</object-id>
        <label>Figure 7</label>
        <caption>
          <title>Performance of DeepNC and GraphDTA training models on the Kiba dataset.</title>
          <p>(A) MSE Values (B) CI Values.</p>
        </caption>
        <graphic xlink:href="peerj-10-13163-g007" position="float"/>
      </fig>
      <fig position="float" id="fig-8">
        <object-id pub-id-type="doi">10.7717/peerj.13163/fig-8</object-id>
        <label>Figure 8</label>
        <caption>
          <title>Performance of DeepNC and GraphDTA training models on the Allergy dataset.</title>
          <p>(A) MSE Values (B) CI Values.</p>
        </caption>
        <graphic xlink:href="peerj-10-13163-g008" position="float"/>
      </fig>
      <fig position="float" id="fig-9">
        <object-id pub-id-type="doi">10.7717/peerj.13163/fig-9</object-id>
        <label>Figure 9</label>
        <caption>
          <title>(A–D) DeepNC models’ performance on the Davis dataset.</title>
        </caption>
        <graphic xlink:href="peerj-10-13163-g009" position="float"/>
      </fig>
      <fig position="float" id="fig-10">
        <object-id pub-id-type="doi">10.7717/peerj.13163/fig-10</object-id>
        <label>Figure 10</label>
        <caption>
          <title>(A–D) DeepNC models’ performance on the Kiba dataset.</title>
        </caption>
        <graphic xlink:href="peerj-10-13163-g010" position="float"/>
      </fig>
      <fig position="float" id="fig-11">
        <object-id pub-id-type="doi">10.7717/peerj.13163/fig-11</object-id>
        <label>Figure 11</label>
        <caption>
          <title>(A–D) DeepNC models’ performance on the Allergy dataset.</title>
        </caption>
        <graphic xlink:href="peerj-10-13163-g011" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>Training settings</title>
      <p>Hyperparameters applied on the GNN variants used in the training of DeepNC are summarized in <xref rid="table-2" ref-type="table">Table 2</xref>. The learning has performed with 1,000 epochs, and the network’s weights were updated using a mini-batch of size of 512. To train the networks, Adam have employed as the optimization algorithm. With the default learning rate of 0.0005. <xref rid="table-2" ref-type="table">Table 2</xref> summarizes the settings for the model trainings.</p>
    </sec>
  </sec>
  <sec sec-type="results">
    <title>Results</title>
    <sec>
      <title>Models’ training progress charts</title>
      <p>We utilized the MSE and CI values to assess the performance of proposed models in DeepNC and to compare it with the current state-of-the-art methods Simboost (<xref rid="ref-17" ref-type="bibr">He et al., 2017</xref>), DeepDTA (<xref rid="ref-27" ref-type="bibr">Nguyen et al., 2021</xref>) and GraphDTA (<xref rid="ref-28" ref-type="bibr">Öztürk, Özgür &amp; Ozkirimli, 2018</xref>), which we chose as baselines. As mentioned in Related works, GraphDTA has improved the performance of models from DeepDTA and WideDTA by replacing the CNN layers for drug representation with GNN layers. Accordingly, our research is not only meant to outperform non-GNN methods (SimBoost, DeepDTA) but is aiming at enhancing current GNN models (GraphDTA) as well.</p>
      <p><xref rid="fig-6" ref-type="fig">Figsures 6</xref>–<xref rid="fig-8" ref-type="fig">8</xref> display the values of MSE and CI of each epoch being trained by DeepNC (GEN, HGC-GCN) and GraphDTA (GCN, GAT, GIN, GAT-GCN), observed on three datasets.</p>
      <p>On each figure with 2 charts, we notice that MSE and CI values of GEN and HGC-GCN in general are better than those values of the other four models, <italic toggle="yes">i.e.</italic>, the MSE is smaller (when MSE line is lower) and CI is larger (when CI line is higher).</p>
      <p>To compare the training of DeepNC models with and without validation, the results are presented in <xref rid="fig-9" ref-type="fig">Figs. 9</xref>–<xref rid="fig-11" ref-type="fig">11</xref>. It should be noted that trainings without validation are conducted on the <italic toggle="yes">train</italic> set of each dataset and then the models are used to predict on the <italic toggle="yes">test</italic> set. Meanwhile, trainings with validation starts with models being trained on 80% of the <italic toggle="yes">train</italic> set and after that the models are used to prediction on the rest 20%, which is referred as the <italic toggle="yes">valid</italic> set. For training with validation by each model, the <italic toggle="yes">valid</italic> set is randomly split from the <italic toggle="yes">train</italic> set when we run the Python program.</p>
    </sec>
    <sec>
      <title>Evaluation results</title>
      <p>Experimental results of DeepNC and the baseline methods are remarked in <xref rid="table-3" ref-type="table">Tables 3</xref>–<xref rid="table-7" ref-type="table">7</xref>. We compare our models to SimBoost (<xref rid="ref-17" ref-type="bibr">He et al., 2017</xref>) (non-deep learning method), DeepDTA (<xref rid="ref-28" ref-type="bibr">Öztürk, Özgür &amp; Ozkirimli, 2018</xref>) (non-GNN deep learning) and GraphDTA (<xref rid="ref-27" ref-type="bibr">Nguyen et al., 2021</xref>) (deep learning with GNN architecture). It should be noted that results are extracted from the models’ training on the test sets. For Davis dataset, training results are noted in <xref rid="table-3" ref-type="table">Tables 3</xref> and <xref rid="table-4" ref-type="table">4</xref>. <xref rid="table-5" ref-type="table">Tables 5</xref> and <xref rid="table-6" ref-type="table">6</xref> contain results for training with Kiba dataset.</p>
      <p><xref rid="table-3" ref-type="table">Tables 3</xref> and <xref rid="table-5" ref-type="table">5</xref> show MSE and CI values of models being trained on Davis and Kiba dataset. Meanwhile, <xref rid="table-4" ref-type="table">Tables 4</xref> and <xref rid="table-6" ref-type="table">6</xref> report the values of <inline-formula><alternatives><inline-graphic xlink:href="peerj-10-13163-i006.jpg"/><tex-math id="tex-ieqn-93">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${r}_{m}^{2}$\end{document}</tex-math><mml:math id="mml-ieqn-93" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> which evaluate the external predictive performance of QSAR models, in which <inline-formula><alternatives><inline-graphic xlink:href="peerj-10-13163-i007.jpg"/><tex-math id="tex-ieqn-94">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${r}_{m}^{2}$\end{document}</tex-math><mml:math id="mml-ieqn-94" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> &gt;0.5 for the test set means that the models are determined to be acceptable.</p>
      <p>For the independent dataset Allergy, we only reported training results by MSE and CI values in <xref rid="table-7" ref-type="table">Table 7</xref>.</p>
      <p>For all datasets, noted results show that GEN and HGC-GCN have smaller MSE values, and larger CI values than those of the benchmark models. In terms of MSE, the results suggest that model GEN of DeepNC performed better than SimBoost, DeepDTA (<italic toggle="yes">p</italic>-value of 0.008 for both) and GraphDTA (<italic toggle="yes">p</italic>-value of 0.002); and model HGC-GCN of DeepNC performed better than SimBoost (<italic toggle="yes">p</italic>-value of 0.004), DeepDTA (<italic toggle="yes">p</italic>-value of 0.016) and GraphDTA (<italic toggle="yes">p</italic>-value of 0.002) on Davis dataset. Similarly, on Kiba dataset, the performance results suggest that model GEN of DeepNC predicted better than SimBoost (<italic toggle="yes">p</italic>-value of 0.002), DeepDTA (<italic toggle="yes">p</italic>-value of 0.004) and GraphDTA (<italic toggle="yes">p</italic>-value of 0.0001); and model HGC-GCN performed better than SimBoost (<italic toggle="yes">p</italic>-value of 0.062), DeepDTA ( <italic toggle="yes">p</italic>-value of 0.098) and GraphDTA (<italic toggle="yes">p</italic>-value of 0.016).</p>
    </sec>
  </sec>
  <sec sec-type="discussion">
    <title>Discussion</title>
    <p>In this work, we propose a framework method for predicting drug-target binding affinity, called DeepNC which represents drugs as graphs. Using deep convolution networks on GNN algorithms show that DeepNC can predict the affinity of drugs-targets better than not only non-GNN deep learning methods such as SimBoost and DeepDTA, but also GNN method (GraphDTA) and shown significant improvements over thoses methods. DeepNC perform consistently well across two separate benchmark databases in MSE, CI performance measures. <xref rid="table-3" ref-type="table">Table 3</xref> grants the performance on method, target representation, drug representation in MSE and CI for various approaches to predict the Davis dataset. The best MSE value from the baseline methods is 0.261 by DeepDTA, for both drugs and proteins are represented as 1D strings. Training on the same dataset by DeepNC, for MSE values, the model GEN has gained 0.233 and HGC-GCN gained 0.243, which has improved ±0.028 of MSE and improved ±0.009 of CI when compared to DeepDTA.</p>
    <p>In <xref rid="table-5" ref-type="table">Table 5</xref>, we observed results of the larger Kiba dataset. The baseline method of GraphDTA’s GCN has the MSE of 0.185 and CI of 0.862. The proposed GEN from DeepNC has outperformed by values of MSE and CI which are 0.133 and 0.897 respectively. Those results given by HGC-GCN are 0.172 and 0.872. Hence, we noticed that DeepNC has provided better result of MSE by ±0.048 and CI by ±0.035.</p>
    <p>Beside the testing on benchmark datasets, we have experimented with the Allergy dataset. Here we consider Graph attention networks (GAT), Graph Isomorphism (GIN), GAT-GCN, and GCN from baseline GraphDTA with GCN shows the best MSE (9.312) and the best CI (0.693). From Table 7, our proposed DeepNC, as compared with GraphDTA, has shown improvement of GEN by MSE of 9.095 and CI of 0.699, and improvement of HGC-GCN by MSE of 9.915 and CI of 0.722. Briefly, DeepNC has improved ±0.217 for MSE and ±0.029 for CI.</p>
    <p>From the results, it is suggested that representing molecules as graphs improves the performance considerably and with the combination of GEN and HGC-GCN of the framework confirm deep learning models for graphs are appropriate for drug-target interaction prediction problem.</p>
    <table-wrap position="float" id="table-3">
      <object-id pub-id-type="doi">10.7717/peerj.13163/table-3</object-id>
      <label>Table 3</label>
      <caption>
        <title>MSE and CI values of models’ training on the Davis dataset.</title>
      </caption>
      <alternatives>
        <graphic xlink:href="peerj-10-13163-g014" position="float"/>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col span="1"/>
            <col span="1"/>
            <col span="1"/>
            <col span="1"/>
            <col span="1"/>
            <col span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Method</th>
              <th rowspan="1" colspan="1">Model</th>
              <th rowspan="1" colspan="1">Drugs rep. (learning method)<xref rid="table-3fn1" ref-type="table-fn"><sup>a</sup></xref></th>
              <th rowspan="1" colspan="1">Targets rep. (learning method)<xref rid="table-3fn1" ref-type="table-fn"><sup>a</sup></xref></th>
              <th rowspan="1" colspan="1">MSE</th>
              <th rowspan="1" colspan="1">CI</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="center" colspan="2" rowspan="1">SimBoost</td>
              <td rowspan="1" colspan="1">PubChem Sim</td>
              <td rowspan="1" colspan="1">S-W</td>
              <td rowspan="1" colspan="1">0.282</td>
              <td rowspan="1" colspan="1">0.872</td>
            </tr>
            <tr>
              <td align="center" colspan="2" rowspan="1">DeepDTA</td>
              <td rowspan="1" colspan="1">SMILES (CNN)</td>
              <td rowspan="1" colspan="1">S-W</td>
              <td rowspan="1" colspan="1">0.420</td>
              <td rowspan="1" colspan="1">0.886</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">SMILES (CNN)</td>
              <td rowspan="1" colspan="1">Target sequence (CNN)</td>
              <td rowspan="1" colspan="1">0.261</td>
              <td rowspan="1" colspan="1">0.878</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">GCN</td>
              <td rowspan="1" colspan="1">Graph (GCN)</td>
              <td rowspan="1" colspan="1">Target sequence (CNN)</td>
              <td rowspan="1" colspan="1">0.302</td>
              <td rowspan="1" colspan="1">0.859</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GraphDTA</td>
              <td rowspan="1" colspan="1">GAT</td>
              <td rowspan="1" colspan="1">Graph (GAT)</td>
              <td rowspan="1" colspan="1">Target sequence (CNN)</td>
              <td rowspan="1" colspan="1">0.295</td>
              <td rowspan="1" colspan="1">0.865</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">GIN</td>
              <td rowspan="1" colspan="1">Graph (GIN)</td>
              <td rowspan="1" colspan="1">Target sequence (CNN)</td>
              <td rowspan="1" colspan="1">0.308</td>
              <td rowspan="1" colspan="1">0.860</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">GAT_GCN</td>
              <td rowspan="1" colspan="1">Graph (combined GAT-GCN)</td>
              <td rowspan="1" colspan="1">Target sequence (CNN)</td>
              <td rowspan="1" colspan="1">0.286</td>
              <td rowspan="1" colspan="1">0.870</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DeepNC</td>
              <td rowspan="1" colspan="1">GEN</td>
              <td rowspan="1" colspan="1">Graph (GEN)</td>
              <td rowspan="1" colspan="1">Target sequence (CNN)</td>
              <td rowspan="1" colspan="1">0.233</td>
              <td rowspan="1" colspan="1">0.887</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">HGC_GCN</td>
              <td rowspan="1" colspan="1">Graph (HGC-GCN)</td>
              <td rowspan="1" colspan="1">Target sequence (CNN)</td>
              <td rowspan="1" colspan="1">0.243</td>
              <td rowspan="1" colspan="1">0.881</td>
            </tr>
          </tbody>
        </table>
      </alternatives>
      <table-wrap-foot>
        <fn id="table-3fn">
          <p>
            <bold>Notes.</bold>
          </p>
        </fn>
        <fn id="table-3fn1">
          <label>a</label>
          <p>The method of learning drug/target features are given in parenthesis.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
  </sec>
  <sec>
    <title>Conclusion</title>
    <p>Graph-based learning neural network models are worth studying in terms of generating molecular graphs, because the direct use of graphs has many advantages that character string representation does not have is first, and most importantly, each molecular subgraph is interpretable. DeepNC, a new GNN molecular design framework, was described and utilized to explore novel graph-based topologies for molecular generation in this study. Deep Neural Computing (DeepNC), a new deep learning-based framework for DTI prediction that uses three GNN algorithms, is our suggested framework. Here, DeepNC demonstrated the molecular graph context tailored for drug target interaction models, where three different GNNs have investigated: Generalized Aggregation Networks (GEN), Graph Convolutional Networks (GCN), and Hypergraph Convolution-Hypergraph Attention (HGC). Hypergraphs provide a flexible and natural modeling tool to model such complex molecule structure. HypergraphConv estimates each hyperedge of the hypergraph by a customary of pair of edges connecting the vertices of the hyperedge and gives the learning problem as a graph-learning problem. The DeepNC outperforms all other models in terms of both speed and quality of generated prediction structures. Attention with graph neural network able to expand an additional flexible model and will applied to a variety of applications at the same time as hypergraph convolution and hypergraph.</p>
    <table-wrap position="float" id="table-4">
      <object-id pub-id-type="doi">10.7717/peerj.13163/table-4</object-id>
      <label>Table 4</label>
      <caption>
        <title>The average r<sup>2</sup> scores of models’ training on the Davis dataset.</title>
      </caption>
      <alternatives>
        <graphic xlink:href="peerj-10-13163-g015" position="float"/>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col span="1"/>
            <col span="1"/>
            <col span="1"/>
            <col span="1"/>
            <col span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Method</th>
              <th rowspan="1" colspan="1">Model</th>
              <th rowspan="1" colspan="1">Drugs rep. (learning method)<xref rid="table-4fn1" ref-type="table-fn"><sup>a</sup></xref></th>
              <th rowspan="1" colspan="1">Targets rep. (learning method)<xref rid="table-4fn1" ref-type="table-fn"><sup>a</sup></xref></th>
              <th rowspan="1" colspan="1">
                <inline-formula>
                  <alternatives>
                    <inline-graphic xlink:href="peerj-10-13163-i008.jpg"/>
                    <tex-math id="tex-ieqn-113">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${r}_{m}^{2}$\end{document}</tex-math>
                    <mml:math id="mml-ieqn-113" overflow="scroll">
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>r</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>m</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>2</mml:mn>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:math>
                  </alternatives>
                </inline-formula>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="center" colspan="2" rowspan="1">SimBoost</td>
              <td rowspan="1" colspan="1">PubChem Sim</td>
              <td rowspan="1" colspan="1">S-W</td>
              <td rowspan="1" colspan="1">0.644</td>
            </tr>
            <tr>
              <td align="center" colspan="2" rowspan="1">DeepDTA</td>
              <td rowspan="1" colspan="1">SMILES (CNN)</td>
              <td rowspan="1" colspan="1">Target sequence (CNN)</td>
              <td rowspan="1" colspan="1">0.630</td>
            </tr>
            <tr>
              <td rowspan="2" colspan="1">DeepNC</td>
              <td rowspan="1" colspan="1">GEN</td>
              <td rowspan="1" colspan="1">Graph (GEN)</td>
              <td rowspan="1" colspan="1">Target sequence (CNN)</td>
              <td rowspan="1" colspan="1">0.653</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">HGC_GCN</td>
              <td rowspan="1" colspan="1">Graph (HGC-GCN)</td>
              <td rowspan="1" colspan="1">Target sequence (CNN)</td>
              <td rowspan="1" colspan="1">0.686</td>
            </tr>
          </tbody>
        </table>
      </alternatives>
      <table-wrap-foot>
        <fn id="table-4fn">
          <p>
            <bold>Notes.</bold>
          </p>
        </fn>
        <fn id="table-4fn1">
          <label>a</label>
          <p>The method of learning drug/target features are given in parenthesis.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <table-wrap position="float" id="table-5">
      <object-id pub-id-type="doi">10.7717/peerj.13163/table-5</object-id>
      <label>Table 5</label>
      <caption>
        <title>MSE and CI values of models’ training on the Kiba dataset.</title>
      </caption>
      <alternatives>
        <graphic xlink:href="peerj-10-13163-g016" position="float"/>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col span="1"/>
            <col span="1"/>
            <col span="1"/>
            <col span="1"/>
            <col span="1"/>
            <col span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Method</th>
              <th rowspan="1" colspan="1">Model</th>
              <th rowspan="1" colspan="1">Drugs rep. (learning method)<xref rid="table-5fn1" ref-type="table-fn"><sup>a</sup></xref></th>
              <th rowspan="1" colspan="1">Targets rep. (learning method)<xref rid="table-5fn1" ref-type="table-fn"><sup>a</sup></xref></th>
              <th rowspan="1" colspan="1">MSE</th>
              <th rowspan="1" colspan="1">CI</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="center" colspan="2" rowspan="1">SimBoost</td>
              <td rowspan="1" colspan="1">PubChem Sim</td>
              <td rowspan="1" colspan="1">S-W</td>
              <td rowspan="1" colspan="1">0.222</td>
              <td rowspan="1" colspan="1">0.836</td>
            </tr>
            <tr>
              <td align="center" colspan="2" rowspan="1">DeepDTA</td>
              <td rowspan="1" colspan="1">SMILES (CNN)</td>
              <td rowspan="1" colspan="1">S-W</td>
              <td rowspan="1" colspan="1">0.204</td>
              <td rowspan="1" colspan="1">0.854</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">SMILES (CNN)</td>
              <td rowspan="1" colspan="1">Target sequence (CNN)</td>
              <td rowspan="1" colspan="1">0.194</td>
              <td rowspan="1" colspan="1">0.863</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">GCN</td>
              <td rowspan="1" colspan="1">Graph (GCN)</td>
              <td rowspan="1" colspan="1">Target sequence (CNN)</td>
              <td rowspan="1" colspan="1">0.185</td>
              <td rowspan="1" colspan="1">0.862</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GraphDTA</td>
              <td rowspan="1" colspan="1">GAT</td>
              <td rowspan="1" colspan="1">Graph (GAT)</td>
              <td rowspan="1" colspan="1">Target sequence (CNN)</td>
              <td rowspan="1" colspan="1">0.223</td>
              <td rowspan="1" colspan="1">0.834</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">GIN</td>
              <td rowspan="1" colspan="1">Graph (GIN)</td>
              <td rowspan="1" colspan="1">Target sequence (CNN)</td>
              <td rowspan="1" colspan="1">0.186</td>
              <td rowspan="1" colspan="1">0.852</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">GAT_GCN</td>
              <td rowspan="1" colspan="1">Graph (combined GAT-GCN)</td>
              <td rowspan="1" colspan="1">Target sequence (CNN)</td>
              <td rowspan="1" colspan="1">0.253</td>
              <td rowspan="1" colspan="1">0.824</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DeepNC</td>
              <td rowspan="1" colspan="1">GEN</td>
              <td rowspan="1" colspan="1">Graph (GEN)</td>
              <td rowspan="1" colspan="1">Target sequence (CNN)</td>
              <td rowspan="1" colspan="1">0.133</td>
              <td rowspan="1" colspan="1">0.897</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">HGC_GCN</td>
              <td rowspan="1" colspan="1">Graph (HGC-GCN)</td>
              <td rowspan="1" colspan="1">Target sequence (CNN)</td>
              <td rowspan="1" colspan="1">0.172</td>
              <td rowspan="1" colspan="1">0.872</td>
            </tr>
          </tbody>
        </table>
      </alternatives>
      <table-wrap-foot>
        <fn id="table-5fn">
          <p>
            <bold>Notes.</bold>
          </p>
        </fn>
        <fn id="table-5fn1">
          <label>a</label>
          <p>The method of learning drug/target features are given in parenthesis.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <table-wrap position="float" id="table-6">
      <object-id pub-id-type="doi">10.7717/peerj.13163/table-6</object-id>
      <label>Table 6</label>
      <caption>
        <title>The average scores of models’ training on the Kiba dataset.</title>
      </caption>
      <alternatives>
        <graphic xlink:href="peerj-10-13163-g017" position="float"/>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col span="1"/>
            <col span="1"/>
            <col span="1"/>
            <col span="1"/>
            <col span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Method</th>
              <th rowspan="1" colspan="1">Model</th>
              <th rowspan="1" colspan="1">Drugs rep. (learning method)<xref rid="table-6fn1" ref-type="table-fn"><sup>a</sup></xref></th>
              <th rowspan="1" colspan="1">Targets rep. (learning method)<xref rid="table-6fn1" ref-type="table-fn"><sup>a</sup></xref></th>
              <th rowspan="1" colspan="1">
                <inline-formula>
                  <alternatives>
                    <inline-graphic xlink:href="peerj-10-13163-i009.jpg"/>
                    <tex-math id="tex-ieqn-114">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${r}_{m}^{2}$\end{document}</tex-math>
                    <mml:math id="mml-ieqn-114" overflow="scroll">
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>r</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>m</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>2</mml:mn>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:math>
                  </alternatives>
                </inline-formula>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="center" colspan="2" rowspan="1">SimBoost</td>
              <td rowspan="1" colspan="1">PubChem Sim</td>
              <td rowspan="1" colspan="1">S-W</td>
              <td rowspan="1" colspan="1">0.629</td>
            </tr>
            <tr>
              <td align="center" colspan="2" rowspan="1">DeepDTA</td>
              <td rowspan="1" colspan="1">SMILES (CNN)</td>
              <td rowspan="1" colspan="1">Target sequence (CNN)</td>
              <td rowspan="1" colspan="1">0.673</td>
            </tr>
            <tr>
              <td rowspan="2" colspan="1">DeepNC</td>
              <td rowspan="1" colspan="1">GEN</td>
              <td rowspan="1" colspan="1">Graph (GEN)</td>
              <td rowspan="1" colspan="1">Target sequence (CNN)</td>
              <td rowspan="1" colspan="1">0.695</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">HGC_GCN</td>
              <td rowspan="1" colspan="1">Graph (HGC-GCN)</td>
              <td rowspan="1" colspan="1">Target sequence (CNN)</td>
              <td rowspan="1" colspan="1">0.624</td>
            </tr>
          </tbody>
        </table>
      </alternatives>
      <table-wrap-foot>
        <fn id="table-6fn">
          <p>
            <bold>Notes.</bold>
          </p>
        </fn>
        <fn id="table-6fn1">
          <label>a</label>
          <p>The method of learning drug/target features are given in parenthesis.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <table-wrap position="float" id="table-7">
      <object-id pub-id-type="doi">10.7717/peerj.13163/table-7</object-id>
      <label>Table 7</label>
      <caption>
        <title>MSE and CI values of models’ training on the Allergy dataset.</title>
      </caption>
      <alternatives>
        <graphic xlink:href="peerj-10-13163-g018" position="float"/>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col span="1"/>
            <col span="1"/>
            <col span="1"/>
            <col span="1"/>
            <col span="1"/>
            <col span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Method</th>
              <th rowspan="1" colspan="1">Model</th>
              <th rowspan="1" colspan="1">Drugs rep. (learning method)<xref rid="table-7fn1" ref-type="table-fn"><sup>a</sup></xref></th>
              <th rowspan="1" colspan="1">Targets rep. (learning method)<xref rid="table-7fn1" ref-type="table-fn"><sup>a</sup></xref></th>
              <th rowspan="1" colspan="1">MSE</th>
              <th rowspan="1" colspan="1">CI</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="4" colspan="1">GraphDTA</td>
              <td rowspan="1" colspan="1">GCN</td>
              <td rowspan="1" colspan="1">Graph (GCN)</td>
              <td rowspan="1" colspan="1">Target sequence (CNN)</td>
              <td rowspan="1" colspan="1">9.312</td>
              <td rowspan="1" colspan="1">0.693</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GAT</td>
              <td rowspan="1" colspan="1">Graph (GAT)</td>
              <td rowspan="1" colspan="1">Target sequence (CNN)</td>
              <td rowspan="1" colspan="1">11.200</td>
              <td rowspan="1" colspan="1">0.661</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GIN</td>
              <td rowspan="1" colspan="1">Graph (GIN)</td>
              <td rowspan="1" colspan="1">Target sequence (CNN)</td>
              <td rowspan="1" colspan="1">12.158</td>
              <td rowspan="1" colspan="1">0.659</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GAT_GCN</td>
              <td rowspan="1" colspan="1">Graph (combined GAT-GCN)</td>
              <td rowspan="1" colspan="1">Target sequence (CNN)</td>
              <td rowspan="1" colspan="1">9.951</td>
              <td rowspan="1" colspan="1">0.683</td>
            </tr>
            <tr>
              <td rowspan="2" colspan="1">DeepNC</td>
              <td rowspan="1" colspan="1">GEN</td>
              <td rowspan="1" colspan="1">Graph (GEN)</td>
              <td rowspan="1" colspan="1">Target sequence (CNN)</td>
              <td rowspan="1" colspan="1">9.095</td>
              <td rowspan="1" colspan="1">0.699</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">HGC_GCN</td>
              <td rowspan="1" colspan="1">Graph (HGC-GCN)</td>
              <td rowspan="1" colspan="1">Target sequence (CNN)</td>
              <td rowspan="1" colspan="1">9.159</td>
              <td rowspan="1" colspan="1">0.722</td>
            </tr>
          </tbody>
        </table>
      </alternatives>
      <table-wrap-foot>
        <fn id="table-7fn">
          <p>
            <bold>Notes.</bold>
          </p>
        </fn>
        <fn id="table-7fn1">
          <label>a</label>
          <p>The method of learning drug/target features are given in parenthesis.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
  </sec>
  <sec sec-type="supplementary-material" id="supplemental-information">
    <title>Supplemental Information</title>
    <supplementary-material id="supp-1" position="float" content-type="local-data">
      <object-id pub-id-type="doi">10.7717/peerj.13163/supp-1</object-id>
      <label>Supplemental Information 1</label>
      <caption>
        <title>Supplemental Material</title>
      </caption>
      <media xlink:href="peerj-10-13163-s001.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="supp-2" position="float" content-type="local-data">
      <object-id pub-id-type="doi">10.7717/peerj.13163/supp-2</object-id>
      <label>Supplemental Information 2</label>
      <caption>
        <title>The distribution of binding affinity values of three datasets</title>
      </caption>
      <media xlink:href="peerj-10-13163-s002.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack>
    <p>This study is part of Huu Ngoc Tran Tran’s Masters in Computer Science work.</p>
  </ack>
  <sec sec-type="additional-information">
    <title>Additional Information and Declarations</title>
    <fn-group content-type="competing-interests">
      <title>Competing Interests</title>
      <fn id="conflict-1" fn-type="COI-statement">
        <p>The authors declare there are no competing interests.</p>
      </fn>
    </fn-group>
    <fn-group content-type="author-contributions">
      <title>Author Contributions</title>
      <fn id="contribution-1" fn-type="con">
        <p><xref rid="author-1" ref-type="contrib">Huu Ngoc Tran Tran</xref> conceived and designed the experiments, performed the experiments, analyzed the data, prepared figures and/or tables, authored or reviewed drafts of the paper, and approved the final draft.</p>
      </fn>
      <fn id="contribution-2" fn-type="con">
        <p><xref rid="author-2" ref-type="contrib">J. Joshua Thomas</xref> conceived and designed the experiments, performed the experiments, analyzed the data, prepared figures and/or tables, authored or reviewed drafts of the paper, and approved the final draft.</p>
      </fn>
      <fn id="contribution-3" fn-type="con">
        <p><xref rid="author-3" ref-type="contrib">Nurul Hashimah Ahamed Hassain Malim</xref> conceived and designed the experiments, performed the experiments, analyzed the data, prepared figures and/or tables, authored or reviewed drafts of the paper, and approved the final draft.</p>
      </fn>
    </fn-group>
    <fn-group content-type="other">
      <title>Data Availability</title>
      <fn id="addinfo-1">
        <p>The following information was supplied regarding data availability:</p>
        <p>The data and models are available at GitHub: <ext-link xlink:href="https://github.com/thntran/DeepNC" ext-link-type="uri">https://github.com/thntran/DeepNC</ext-link>.</p>
      </fn>
    </fn-group>
  </sec>
  <ref-list content-type="authoryear">
    <title>References</title>
    <ref id="ref-1">
      <label>Abbasi et al. (2020)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Abbasi</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Razzaghi</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Poso</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ghanbari-Ara</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Masoudi-Nejad</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2020">2020</year>
        <article-title>Deep learning in drug target interaction prediction: current and future perspectives</article-title>
        <source>Current Medicinal Chemistry</source>
        <volume>28</volume>
        <issue>11</issue>
        <fpage>2100</fpage>
        <lpage>2113</lpage>
        <pub-id pub-id-type="doi">10.2174/0929867327666200907141016</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-2">
      <label>Bai, Zhang &amp; Torr (2021)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bai</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Torr</surname>
            <given-names>PHS</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2021">2021</year>
        <article-title>Hypergraph convolution and hypergraph attention</article-title>
        <source>Pattern Recognition</source>
        <volume>110</volume>
        <elocation-id>107637</elocation-id>
        <pub-id pub-id-type="doi">10.1016/j.patcog.2020.107637</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-3">
      <label>Battaglia et al. (2016)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Battaglia</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Pascanu</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lai</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Rezende</surname>
            <given-names>DJ</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2016">2016</year>
        <article-title>Interaction networks for learning about objects, relations and physics</article-title>
        <source>Advances in Neural Information Processing Systems</source>
        <volume>29</volume>
        <fpage>4502</fpage>
        <lpage>4510</lpage>
      </element-citation>
    </ref>
    <ref id="ref-4">
      <label>Bresson &amp; Laurent (2019)</label>
      <element-citation publication-type="working-paper">
        <person-group person-group-type="author">
          <name>
            <surname>Bresson</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Laurent</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2019">2019</year>
        <article-title>A two-step graph convolutional decoder for molecule generation</article-title>
        <pub-id pub-id-type="arxiv">1906.03412</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-5">
      <label>Chan et al. (2019)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chan</surname>
            <given-names>CHS</given-names>
          </name>
          <name>
            <surname>Shan</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Dahoun</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Vogel</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Yuan</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2019">2019</year>
        <article-title>Advancing drug discovery via artificial intelligence</article-title>
        <source>Trends in Pharmacological Sciences</source>
        <volume>40</volume>
        <issue>8</issue>
        <fpage>592</fpage>
        <lpage>604</lpage>
        <pub-id pub-id-type="doi">10.1016/j.tips.2019.06.004</pub-id>
        <pub-id pub-id-type="pmid">31320117</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-6">
      <label>Clevert, Unterthiner &amp; Hochreiter (2016)</label>
      <element-citation publication-type="workingpaper">
        <person-group person-group-type="author">
          <name>
            <surname>Clevert</surname>
            <given-names>D-A</given-names>
          </name>
          <name>
            <surname>Unterthiner</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Hochreiter</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2016">2016</year>
        <article-title>Fast and accurate deep network learning by exponential linear units (ELUs). ICLR</article-title>
        <pub-id pub-id-type="arxiv">1511.07289</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-7">
      <label>Dai et al. (2017)</label>
      <element-citation publication-type="working-paper">
        <person-group person-group-type="author">
          <name>
            <surname>Dai</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Khalil</surname>
            <given-names>EB</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Dilkina</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2017">2017</year>
        <article-title>Learning combinatorial optimization algorithms over graphs</article-title>
        <pub-id pub-id-type="arxiv">1704.01665</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-8">
      <label>Davis et al. (2011)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Davis</surname>
            <given-names>MI</given-names>
          </name>
          <name>
            <surname>Hunt</surname>
            <given-names>JP</given-names>
          </name>
          <name>
            <surname>Herrgard</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ciceri</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Wodicka</surname>
            <given-names>LM</given-names>
          </name>
          <name>
            <surname>Pallares</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Hocker</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Treiber</surname>
            <given-names>DK</given-names>
          </name>
          <name>
            <surname>Zarrinkar</surname>
            <given-names>PP</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2011">2011</year>
        <article-title>Comprehensive analysis of kinase inhibitor selectivity</article-title>
        <source>Nature Biotechnology</source>
        <volume>29</volume>
        <fpage>1046</fpage>
        <lpage>1051</lpage>
        <pub-id pub-id-type="doi">10.1038/nbt.1990</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-9">
      <label>De Cao &amp; Kipf (2018)</label>
      <element-citation publication-type="working-paper">
        <person-group person-group-type="author">
          <name>
            <surname>De Cao</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Kipf</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2018">2018</year>
        <article-title>MolGAN: An implicit generative model for small molecular graphs</article-title>
        <pub-id pub-id-type="arxiv">1805.11973</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-10">
      <label>Feinberg et al. (2020)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Feinberg</surname>
            <given-names>EN</given-names>
          </name>
          <name>
            <surname>Joshi</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Pande</surname>
            <given-names>VS</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>AC</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2020">2020</year>
        <article-title>Improvement in ADMET prediction with multitask deep featurization</article-title>
        <source>Journal of Medicinal Chemistry</source>
        <volume>63</volume>
        <issue>16</issue>
        <fpage>8835</fpage>
        <lpage>8848</lpage>
        <pub-id pub-id-type="doi">10.1021/acs.jmedchem.9b02187</pub-id>
        <pub-id pub-id-type="pmid">32286824</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-11">
      <label>Feng et al. (2018)</label>
      <element-citation publication-type="working-paper">
        <person-group person-group-type="author">
          <name>
            <surname>Feng</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Dueva</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Cherkasov</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ester</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2018">2018</year>
        <article-title>PADME: a deep learning-based framework for drug-target interaction prediction</article-title>
        <fpage>1</fpage>
        <lpage>29</lpage>
        <pub-id pub-id-type="arxiv">1807.09741</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-12">
      <label>Fout et al. (2017)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Fout</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Byrd</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Shariat</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Ben-Hur</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2017">2017</year>
        <article-title>Protein interface prediction using graph convolutional networks</article-title>
        <conf-name>NIPS 2017</conf-name>
        <fpage>6530</fpage>
        <lpage>6539</lpage>
      </element-citation>
    </ref>
    <ref id="ref-13">
      <label>Gilson et al. (2016)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gilson</surname>
            <given-names>MK</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Baitaluk</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Nicola</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Hwang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Chong</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2016">2016</year>
        <article-title>BindingDB in 2015: a public database for medicinal chemistry, computational chemistry and systems pharmacology</article-title>
        <source>Nucleic Acids Research</source>
        <volume>44</volume>
        <issue>D1</issue>
        <fpage>D1045</fpage>
        <lpage>D1053</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkv1072</pub-id>
        <pub-id pub-id-type="pmid">26481362</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-14">
      <label>Gonczarek et al. (2018)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gonczarek</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Tomczak</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Zaręba</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kaczmar</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Dąbrowski</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Walczak</surname>
            <given-names>MJ</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2018">2018</year>
        <article-title>Interaction prediction in structure-based virtual screening using deep learning</article-title>
        <source>Computers in Biology and Medicine</source>
        <volume>100</volume>
        <fpage>253</fpage>
        <lpage>258</lpage>
        <pub-id pub-id-type="doi">10.1016/j.compbiomed.2017.09.007</pub-id>
        <pub-id pub-id-type="pmid">28941550</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-15">
      <label>Hamaguchi et al. (2017)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Hamaguchi</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Oiwa</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Shimbo</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Matsumoto</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2017">2017</year>
        <article-title>Knowledge transfer for out-of-knowledge-base entities: a graph neural network approach</article-title>
        <conf-name>IJCAI 2017</conf-name>
        <fpage>1802</fpage>
        <lpage>1808</lpage>
      </element-citation>
    </ref>
    <ref id="ref-16">
      <label>Hamilton, Ying &amp; Leskovec (2017)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Hamilton</surname>
            <given-names>WL</given-names>
          </name>
          <name>
            <surname>Ying</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Leskovec</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2017">2017</year>
        <article-title>Inductive representation learning on large graphs</article-title>
        <conf-name>NIPS 2017</conf-name>
        <fpage>1024</fpage>
        <lpage>1034</lpage>
      </element-citation>
    </ref>
    <ref id="ref-17">
      <label>He et al. (2017)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Heidemeyer</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ban</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Cherkasov</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ester</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2017">2017</year>
        <article-title>SimBoost: a read-across approach for predicting drug-target binding affinities using gradient boosting machines</article-title>
        <source>Journal of Cheminformatics</source>
        <volume>9</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>14</lpage>
        <pub-id pub-id-type="doi">10.1186/s13321-017-0209-z</pub-id>
        <pub-id pub-id-type="pmid">28316652</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-18">
      <label>Jain et al. (2009)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jain</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Bairock</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Duvaud</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Phan</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Redaschi</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Suzek</surname>
            <given-names>BE</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>McGarvey</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Gasteiger</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2009">2009</year>
        <article-title>Infrastructure for the life sciences: design and implementation of the UniProt website</article-title>
        <source>BMC Bioinformatics</source>
        <volume>10</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>19</lpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-10-136</pub-id>
        <pub-id pub-id-type="pmid">19118496</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-19">
      <label>Karimi et al. (2019)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Karimi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2019">2019</year>
        <article-title>DeepAffinity: Interpretable deep learning of compound-protein affinity through unified recurrent and convolutional neural networks</article-title>
        <source>Bioinformatics</source>
        <volume>35</volume>
        <issue>18</issue>
        <fpage>3329</fpage>
        <lpage>3338</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btz111</pub-id>
        <pub-id pub-id-type="pmid">30768156</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-20">
      <label>Kipf &amp; Welling (2017)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Kipf</surname>
            <given-names>TN</given-names>
          </name>
          <name>
            <surname>Welling</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2017">2017</year>
        <article-title>Semi-supervised classification with graph convolutional networks</article-title>
        <conf-name>5th international conference on learning representations, ILCR 2017 - conference track proceedings</conf-name>
        <fpage>1</fpage>
        <lpage>14</lpage>
        <uri xlink:href="https://arxiv.org/abs/1609.02907">https://arxiv.org/abs/1609.02907</uri>
      </element-citation>
    </ref>
    <ref id="ref-21">
      <label>Lee et al. (2019)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>JB</given-names>
          </name>
          <name>
            <surname>Rossi</surname>
            <given-names>RA</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ahmed</surname>
            <given-names>NK</given-names>
          </name>
          <name>
            <surname>Koh</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2019">2019</year>
        <article-title>Attention models in graphs: a survey</article-title>
        <volume>13</volume>
        <issue>6</issue>
        <conf-name>TKDD</conf-name>
        <fpage>1</fpage>
        <lpage>25</lpage>
      </element-citation>
    </ref>
    <ref id="ref-22">
      <label>Li et al. (2020)</label>
      <element-citation publication-type="working-paper">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Xiong</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Thabet</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ghanem</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2020">2020</year>
        <article-title>DeeperGCN: all you need to train deeper GCNs</article-title>
        <pub-id pub-id-type="arxiv">2006.07739</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-23">
      <label>Li et al. (2021)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Qiao</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2021">2021</year>
        <article-title>Drug–target interaction predication via multi-channel graph neural networks</article-title>
        <source>Briefings in Bioinformatics</source>
        <volume>23</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbab346</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-24">
      <label>Lim et al. (2019)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lim</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hwang</surname>
            <given-names>SY</given-names>
          </name>
          <name>
            <surname>Moon</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>WY</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2019">2019</year>
        <article-title>Scaffold-based molecular design with a graph generative model</article-title>
        <source>Chemical Science</source>
        <volume>11</volume>
        <issue>4</issue>
        <fpage>1153</fpage>
        <lpage>1164</lpage>
        <pub-id pub-id-type="doi">10.1039/c9sc04503a</pub-id>
        <pub-id pub-id-type="pmid">34084372</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-25">
      <label>Lo et al. (2018)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lo</surname>
            <given-names>YC</given-names>
          </name>
          <name>
            <surname>Rensi</surname>
            <given-names>SE</given-names>
          </name>
          <name>
            <surname>Torng</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Altman</surname>
            <given-names>RB</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2018">2018</year>
        <article-title>Machine learning in chemoinformatics and drug discovery</article-title>
        <source>Drug Discovery Today</source>
        <volume>23</volume>
        <issue>8</issue>
        <fpage>1538</fpage>
        <lpage>1546</lpage>
        <pub-id pub-id-type="doi">10.1016/j.drudis.2018.05.010</pub-id>
        <pub-id pub-id-type="pmid">29750902</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-26">
      <label>Maas, Hannun &amp; Ng (2013)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Maas</surname>
            <given-names>AL</given-names>
          </name>
          <name>
            <surname>Hannun</surname>
            <given-names>AY</given-names>
          </name>
          <name>
            <surname>Ng</surname>
            <given-names>AY</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2013">2013</year>
        <article-title>Rectifier nonlinearities improve neural net work acoustic models</article-title>
        <conf-name>ICML</conf-name>
      </element-citation>
    </ref>
    <ref id="ref-27">
      <label>Nguyen et al. (2021)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nguyen</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Le</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Quinn</surname>
            <given-names>TP</given-names>
          </name>
          <name>
            <surname>Nguyen</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Le</surname>
            <given-names>TD</given-names>
          </name>
          <name>
            <surname>Venkatesh</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2021">2021</year>
        <article-title>GraphDTA: Predicting drug target binding affinity with graph neural networks</article-title>
        <source>Bioinformatics</source>
        <volume>37</volume>
        <issue>8</issue>
        <fpage>1140</fpage>
        <lpage>1147</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btaa921</pub-id>
        <pub-id pub-id-type="pmid">33119053</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-28">
      <label>Öztürk, Özgür &amp; Ozkirimli (2018)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Öztürk</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Özgür</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ozkirimli</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2018">2018</year>
        <article-title>DeepDTA: deep drug-target binding affinity prediction</article-title>
        <source>Bioinformatics</source>
        <volume>34</volume>
        <issue>17</issue>
        <fpage>i821</fpage>
        <lpage>i829</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty593</pub-id>
        <pub-id pub-id-type="pmid">30423097</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-29">
      <label>Öztürk, Ozkirimli &amp; Özgür (2019)</label>
      <element-citation publication-type="working-paper">
        <person-group person-group-type="author">
          <name>
            <surname>Öztürk</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Ozkirimli</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Özgür</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2019">2019</year>
        <article-title>WideDTA: prediction of drug-target binding affinity</article-title>
        <pub-id pub-id-type="arxiv">1902.04166</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-30">
      <label>Paszke et al. (2017)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Paszke</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gross</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Chintala</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Chanan</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>DeVito</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Desmaison</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Antiga</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Lerer</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2017">2017</year>
        <article-title>Automatic differentiation in PyTorch</article-title>
        <conf-name>NIPS-W</conf-name>
      </element-citation>
    </ref>
    <ref id="ref-31">
      <label>Peng et al. (2017)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Peng</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Liao</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2017">2017</year>
        <article-title>Predicting drug–target interactions with multi-information fusion</article-title>
        <source>IEEE Journal of Biomedical and Health Informatics</source>
        <volume>21</volume>
        <issue>2</issue>
        <fpage>561</fpage>
        <lpage>572</lpage>
        <pub-id pub-id-type="doi">10.1109/JBHI.2015.2513200</pub-id>
        <pub-id pub-id-type="pmid">26731781</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-32">
      <label>Rifaioglu et al. (2019)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rifaioglu</surname>
            <given-names>AS</given-names>
          </name>
          <name>
            <surname>Atas</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Cetin-Atalay</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Atalay</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Doǧan</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2019">2019</year>
        <article-title>Recent applications of deep learning and machine intelligence on in silico drug discovery: methods, tools and databases</article-title>
        <source>Briefings in Bioinformatics</source>
        <volume>20</volume>
        <issue>5</issue>
        <fpage>1878</fpage>
        <lpage>1912</lpage>
        <pub-id pub-id-type="doi">10.1093/bib/bby061</pub-id>
        <pub-id pub-id-type="pmid">30084866</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-33">
      <label>RDKit (2022)</label>
      <element-citation publication-type="other">
        <person-group person-group-type="author">
          <collab>RDKit</collab>
        </person-group>
        <article-title>RDKit: open-source cheminformatics</article-title>
        <uri xlink:href="https://www.rdkit.org">https://www.rdkit.org</uri>
        <year iso-8601-date="2022">2022</year>
      </element-citation>
    </ref>
    <ref id="ref-34">
      <label>Sanchez-Gonzalez et al. (2018)</label>
      <element-citation publication-type="working-paper">
        <person-group person-group-type="author">
          <name>
            <surname>Sanchez-Gonzalez</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Heess</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Springenberg</surname>
            <given-names>JT</given-names>
          </name>
          <name>
            <surname>Merel</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Riedmiller</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Hadsell</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Battaglia</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2018">2018</year>
        <article-title>Graph networks as learnable physics engines for inference and control</article-title>
        <pub-id pub-id-type="arxiv">1806.01242</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-35">
      <label>Scarselli et al. (2009)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Scarselli</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Gori</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Tsoi</surname>
            <given-names>AC</given-names>
          </name>
          <name>
            <surname>Hagenbuchner</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Monfardini</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2009">2009</year>
        <article-title>The graph neural network model</article-title>
        <source>IEEE Transactions on Neural Networks</source>
        <volume>20</volume>
        <issue>1</issue>
        <fpage>61</fpage>
        <lpage>80</lpage>
        <pub-id pub-id-type="doi">10.1109/TNN.2008.2005605</pub-id>
        <pub-id pub-id-type="pmid">19068426</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-36">
      <label>Segler et al. (2018)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Segler</surname>
            <given-names>HS</given-names>
          </name>
          <name>
            <surname>Kogej</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tyrchan</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Waller</surname>
            <given-names>MP</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2018">2018</year>
        <article-title>Generating focused molecule libraries for drug discovery with recurrent neural networks</article-title>
        <source>ACS Central Science</source>
        <volume>4</volume>
        <issue>1</issue>
        <fpage>120</fpage>
        <lpage>131</lpage>
        <pub-id pub-id-type="doi">10.1021/acscentsci.7b00512</pub-id>
        <pub-id pub-id-type="pmid">29392184</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-37">
      <label>Tang et al. (2014)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Szwajda</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Shakyawar</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Hintsanen</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Wennerberg</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Aittokallio</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2014">2014</year>
        <article-title>Making sense of large-scale kinase inhibitor bioactivity data sets: a comparative and integrative analysis</article-title>
        <source>Journal of Chemical Information and Modeling</source>
        <volume>54</volume>
        <fpage>735</fpage>
        <lpage>743</lpage>
        <pub-id pub-id-type="doi">10.1021/ci400709d</pub-id>
        <pub-id pub-id-type="pmid">24521231</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-38">
      <label>Tran et al. (2021)</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Tran</surname>
            <given-names>HNT</given-names>
          </name>
          <name>
            <surname>Joshua Thomas</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Malim</surname>
            <given-names>NHAH</given-names>
          </name>
          <name>
            <surname>Ali</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Huynh</surname>
            <given-names>SB</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Vasant</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Zelinka</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Weber</surname>
            <given-names>GW</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2021">2021</year>
        <article-title>Graph neural networks in cheminformatics</article-title>
        <source>Intelligent computing and optimization. ICO 2020</source>
        <series>Advances in intelligent systems and computing</series>
        <volume>vol. 1324</volume>
        <publisher-name>Springer</publisher-name>
        <publisher-loc>Cham</publisher-loc>
        <pub-id pub-id-type="doi">10.1007/978-3-030-68154-8_71</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-39">
      <label>Tsubaki, Tomii &amp; Sese (2019)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tsubaki</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Tomii</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Sese</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2019">2019</year>
        <article-title>Compound-protein interaction prediction with end-to-end learning of neural networks for graphs and sequences</article-title>
        <source>Bioinformatics</source>
        <volume>35</volume>
        <issue>2</issue>
        <fpage>309</fpage>
        <lpage>318</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty535</pub-id>
        <pub-id pub-id-type="pmid">29982330</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-40">
      <label>Velickovic et al. (2018)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Velickovic</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Cucurull</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Casanova</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Romero</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Lio</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2018">2018</year>
        <article-title>Graph at tention networks</article-title>
        <conf-name>ICLR</conf-name>
      </element-citation>
    </ref>
    <ref id="ref-41">
      <label>Wallach, Dzamba &amp; Heifets (2015)</label>
      <element-citation publication-type="working-paper">
        <person-group person-group-type="author">
          <name>
            <surname>Wallach</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Dzamba</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Heifets</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2015">2015</year>
        <article-title>AtomNet: a deep convolutional neural network for bioactivity prediction in structure-based drug discovery</article-title>
        <fpage>1</fpage>
        <lpage>11</lpage>
        <pub-id pub-id-type="arxiv">1510.02855</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-42">
      <label>Wang et al (2017)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Bryant</surname>
            <given-names>SH</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Gindulyte</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Shoemaker</surname>
            <given-names>BA</given-names>
          </name>
          <name>
            <surname>Thiessen</surname>
            <given-names>PA</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2017">2017</year>
        <article-title>PubChem BioAssay: 2017 update</article-title>
        <source>Nucleic Acids Research</source>
        <volume>45</volume>
        <issue>D1</issue>
        <fpage>D955</fpage>
        <lpage>D963</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkw1118</pub-id>
        <pub-id pub-id-type="pmid">27899599</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-43">
      <label>Yamanishi et al. (2008)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yamanishi</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Araki</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Gutteridge</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Honda</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Kanehisa</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2008">2008</year>
        <article-title>Prediction of drug-target interaction networks from the integration of chemical and genomic spaces</article-title>
        <source>Bioinformatics</source>
        <volume>24</volume>
        <issue>13</issue>
        <fpage>232</fpage>
        <lpage>240</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btn162</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
