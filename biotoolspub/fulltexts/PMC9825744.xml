<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9825744</article-id>
    <article-id pub-id-type="pmid">36409016</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btac744</article-id>
    <article-id pub-id-type="publisher-id">btac744</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Paper</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Data and Text Mining</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DxFormer: a decoupled automatic diagnostic system based on decoder–encoder transformer with dense symptom representations</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-9431-9247</contrib-id>
        <name>
          <surname>Chen</surname>
          <given-names>Wei</given-names>
        </name>
        <aff><institution>School of Data Science, Fudan University</institution>, Shanghai 200433, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhong</surname>
          <given-names>Cheng</given-names>
        </name>
        <aff><institution>School of Data Science, Fudan University</institution>, Shanghai 200433, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Peng</surname>
          <given-names>Jiajie</given-names>
        </name>
        <aff><institution>Research Institute of Automatic and Complex Systems, Fudan University</institution>, Shanghai 200433, <country country="CN">China</country></aff>
        <aff><institution>School of Computer Science, Northwestern Polytechnical University</institution>, Xi’an 710000, <country country="CN">China</country></aff>
        <xref rid="btac744-cor1" ref-type="corresp"/>
        <!--jiajiepeng@nwpu.edu.cn-->
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Wei</surname>
          <given-names>Zhongyu</given-names>
        </name>
        <aff><institution>School of Data Science, Fudan University</institution>, Shanghai 200433, <country country="CN">China</country></aff>
        <aff><institution>Research Institute of Automatic and Complex Systems, Fudan University</institution>, Shanghai 200433, <country country="CN">China</country></aff>
        <xref rid="btac744-cor1" ref-type="corresp"/>
        <!--zywei@fudan.edu.cn-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Wren</surname>
          <given-names>Jonathan</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btac744-cor1">To whom correspondence should be addressed. <email>jiajiepeng@nwpu.edu.cn</email> or <email>zywei@fudan.edu.cn</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>1</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2022-11-21">
      <day>21</day>
      <month>11</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>21</day>
      <month>11</month>
      <year>2022</year>
    </pub-date>
    <volume>39</volume>
    <issue>1</issue>
    <elocation-id>btac744</elocation-id>
    <history>
      <date date-type="received">
        <day>03</day>
        <month>5</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>15</day>
        <month>10</month>
        <year>2022</year>
      </date>
      <date date-type="editorial-decision">
        <day>10</day>
        <month>11</month>
        <year>2022</year>
      </date>
      <date date-type="corrected-typeset">
        <day>12</day>
        <month>12</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btac744.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Symptom-based automatic diagnostic system queries the patient’s potential symptoms through continuous interaction with the patient and makes predictions about possible diseases. A few studies use reinforcement learning (RL) to learn the optimal policy from the joint action space of symptoms and diseases. However, existing RL (or Non-RL) methods focus on disease diagnosis while ignoring the importance of symptom inquiry. Although these systems have achieved considerable diagnostic accuracy, they are still far below its performance upper bound due to few turns of interaction with patients and insufficient performance of symptom inquiry. To address this problem, we propose a new automatic diagnostic framework called DxFormer, which decouples symptom inquiry and disease diagnosis, so that these two modules can be independently optimized. The transition from symptom inquiry to disease diagnosis is parametrically determined by the <italic toggle="yes">stopping criteria</italic>. In DxFormer, we treat each symptom as a token, and formalize the symptom inquiry and disease diagnosis to a language generation model and a sequence classification model, respectively. We use the inverted version of Transformer, i.e. the decoder–encoder structure, to learn the representation of symptoms by jointly optimizing the reinforce reward and cross-entropy loss.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We conduct experiments on three real-world medical dialogue datasets, and the experimental results verify the feasibility of increasing diagnostic accuracy by improving symptom recall. Our model overcomes the shortcomings of previous RL-based methods. By decoupling symptom query from the process of diagnosis, DxFormer greatly improves the symptom recall and achieves the state-of-the-art diagnostic accuracy.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>Both code and data are available at <ext-link xlink:href="https://github.com/lemuria-wchen/DxFormer" ext-link-type="uri">https://github.com/lemuria-wchen/DxFormer</ext-link>.</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Natural Science Foundation of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>71991471</award-id>
        <award-id>6217020551</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Science and Technology Commission of Shanghai Municipality</institution>
            <institution-id institution-id-type="DOI">10.13039/501100003399</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>20dz1200600</award-id>
        <award-id>21QA1400600</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Zhejiang Lab</institution>
          </institution-wrap>
        </funding-source>
        <award-id>2019KD0AD01</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="7"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>The combination of the internet and healthcare has excellent benefits and far-reaching positive effects in improving service efficiency and promoting social equity. Automated disease diagnosis is one of the rising needs in this new healthcare model, the goal of which is to simulate the actual diagnostic process of doctors.</p>
    <p>The process of actual disease diagnosis can be considered as a sequence of queries and answers. Doctors choose relevant questions to ask the patient to have a better understanding of the patient’s physical condition (<xref rid="btac744-B8" ref-type="bibr">Janisch <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac744-B15" ref-type="bibr">Martinez <italic toggle="yes">et al.</italic>, 2020</xref>). In symptom-based automatic diagnostic system, the agent has two types of actions: one is to inquiry about a symptom, and another is to predict a disease (<xref rid="btac744-B17" ref-type="bibr">Peng <italic toggle="yes">et al.</italic>, 2018</xref>), i.e. select one of the elements from a fixed set of symptoms or diseases. The process includes several turns of <italic toggle="yes">symptom inquiry</italic> and a final turn of <italic toggle="yes">disease diagnosis</italic>.</p>
    <p>Recent years have witnessed an emerging trend of research on the task of automatic diagnosis. Considering its interactive nature, most researchers explore to model the problem by reinforcement learning (RL) (<xref rid="btac744-B23" ref-type="bibr">Wei <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btac744-B26" ref-type="bibr">Xu <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac744-B29" ref-type="bibr">Zhong <italic toggle="yes">et al.</italic>, 2022</xref>). The RL setting of most studies is that, the agent chooses an action from the action space of all symptoms and diseases at each turn of interaction, and correct symptom inquiries and disease diagnoses are positively rewarded, then the policy can be learned by maximizing the expected cumulative reward.</p>
    <p>Although RL-based approaches have made progresses for developing automatic diagnostic agents, most of them focus on disease diagnosis and lack of exploration on symptom modeling, resulting in poor symptom recall. As shown in <xref rid="btac744-F1" ref-type="fig">Figure 1</xref>, the symptom recall of most systems is within 30%. In these systems, the agent often only asks the patient for one or two symptoms and rushes to make a diagnosis. In practice, however, an average of 7–8 symptoms are mentioned in each conversation. This results in insufficient symptom features collected by the agent, which affects the performance of the disease diagnosis.</p>
    <fig position="float" id="btac744-F1">
      <label>Fig. 1.</label>
      <caption>
        <p>Symptom Recall (%) versus Disease Accuracy (%) of previous methods including Flat-DQN, REFUEL, KR-DQN and GAMP on three datasets on three datasets Dxy, MZ-4 and MZ-10</p>
      </caption>
      <graphic xlink:href="btac744f1" position="float"/>
    </fig>
    <p>To alleviate the above problems, we propose DxFormer, a decoupled automatic diagnostic framework, which consists of a Transformer-based decoder–encoder structure (not encoder–decoder). The decoder is for symptom inquiry, where the symptoms are treated as tokens in natural language, and the symptom inquiry is modeled as a conditional text generation task. The encoder is for disease diagnosis, where the symptoms collected in symptom inquiry are fed as the input sequence of the encoder, and the disease diagnosis is modeled as a sequence classification task. The decoder is encouraged to discover implicit symptoms, and the encoder is encouraged to make correct diagnosis, the two of which can work together in a decoupled manner and be trained simultaneously with little interference with each other. The dense representation of symptoms is learnt by optimizing the joint objectives. At runtime, the termination of symptom inquiry is determined by the stopping criterion. Specifically, the agent only switches from symptom inquiry to disease diagnosis if the confidence of the encoder on disease diagnosis reaches a certain threshold or the maximum number of turns is reached.</p>
    <p>To evaluate DxFormer, we conduct experiments on three real-world medical dialogue datasets: Dxy, MZ-4 and MZ-10. Experimental results verify that DxFormer greatly improves symptom recall compared with the previous methods, as well as the diagnostic accuracy. We also conduct additional ablation experiments to demonstrate the effectiveness of the components of DxFormer and further discuss the impact of the maximum number of turns and stopping criterion threshold on the model performance.</p>
    <p>The <italic toggle="yes">main contributions</italic> of this article can be summarized as follows: (i) we propose DxFormer, a decoupled system for automatic diagnosis based on inverted Transformer, motivated by approaching the performance upper bound of diagnostic accuracy by improving the symptom recall; (ii) we discuss the impact of the maximum number of turns and stopping criterion threshold on the model performance and suggest ways to utilize DxFormer in practice; and (iii) extensive experiments show that the proposed model achieves the new state-of-the-art (SOTA) results in all the three real-world datasets.</p>
  </sec>
  <sec>
    <title>2 Preliminary</title>
    <sec>
      <title>2.1 Formalization</title>
      <p><italic toggle="yes">MCR</italic>: In practice, real medical consultation records (MCRs) are utilized to build data-driven automatic diagnostic system (<xref rid="btac744-B23" ref-type="bibr">Wei <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btac744-B28" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic>, 2020</xref>). There are a large number of MCRs organized by disease categories that available in online medical communities, such as <italic toggle="yes">Haodafu</italic> (<ext-link xlink:href="https://www.haodf.com/" ext-link-type="uri">https://www.haodf.com/</ext-link>). Generally, each MCR consists of three parts: the patient’s self-provided report (i.e. self-report), multi-turn doctor–patient dialogue and the corresponding disease category. The self-report can be viewed as the first sentence in the dialogue.</p>
      <p><italic toggle="yes">Symptom attribute</italic>: Symptoms are widely present in actual doctor–patient conversations, they are the main topics discussed in medical dialogues and important basis for doctors to make diagnosis (<xref rid="btac744-B14" ref-type="bibr">Lin <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac744-B28" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic>, 2020</xref>). However, symptoms alone in the dialogue are less informative, additional annotations are needed to find the relationship between symptoms and patients. Generally, there are two kinds of relationships between a certain symptom and the patient: (i) <italic toggle="yes">Positive</italic> (POS): the patient does have the symptom; and (ii) <italic toggle="yes">Negative</italic> (NEG): the patient does not have the symptom. The annotator is required to find out all symptom entities mentioned in the dialogue, and identify their relationship with the patient. In this article, we refer to this relationship as the <italic toggle="yes">Attribute</italic> of the symptom.</p>
      <p><italic toggle="yes">Structured MCR</italic>: Let <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mi mathvariant="script" class="calligraphy">S</mml:mi></mml:math></inline-formula> denotes the set of all possible symptoms, <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mi mathvariant="script" class="calligraphy">D</mml:mi></mml:math></inline-formula> denotes the set of possible diseases and <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mi mathvariant="script" class="calligraphy">A</mml:mi></mml:math></inline-formula> denotes the set of possible attributes. A structured MCR can then be denoted as: <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi mathvariant="script" class="calligraphy">S</mml:mi></mml:mrow></mml:math></inline-formula> is the <italic toggle="yes">i</italic>-th symptom that appears in the dialogue (with the self-report as the first utterance), <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi mathvariant="script" class="calligraphy">A</mml:mi></mml:mrow></mml:math></inline-formula> is the corresponding attribute of <italic toggle="yes">s<sub>i</sub></italic> and <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script" class="calligraphy">D</mml:mi></mml:mrow></mml:math></inline-formula> is the disease label.</p>
      <p><italic toggle="yes">Explicit and implicit symptoms</italic>: Generally, symptoms appearing in the self-report are regarded as explicit symptoms, while the others are implicit symptoms. As a notation, we take the first <italic toggle="yes">k</italic> symptoms as explicit symptoms, denoted as <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mo> </mml:mo><mml:mtext mathvariant="italic">exp</mml:mtext><mml:mo> </mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>, and the implicit symptoms are denoted as <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">imp</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>. <italic toggle="yes">k</italic> is usually small because patients usually mention only one or two symptoms in their self-reports. Implicit symptoms are unknown during inference, thus the agent needs to find as many implicit symptoms as possible to obtain a more complete symptom profile about the patient before making a diagnosis.</p>
      <p><italic toggle="yes">Patient simulator</italic>: We denote the <italic toggle="yes">patient simulator</italic> as <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mi mathvariant="script" class="calligraphy">P</mml:mi></mml:math></inline-formula>, and <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mi mathvariant="script" class="calligraphy">P</mml:mi></mml:math></inline-formula> can be viewed as a function whose input is any symptom and whose output is the attribute of that symptom of the patient. Note that if the symptom is not among the implicit symptoms, a <italic toggle="yes">Unknown</italic> (UNK) attribute is returned.</p>
      <p><italic toggle="yes">Agent</italic>: Given a patient’s explicit symptoms <italic toggle="yes">S<sub>exp</sub></italic> and their attributes, the task of the agent is to choose a symptom from <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mi mathvariant="script" class="calligraphy">S</mml:mi></mml:math></inline-formula> to interact with patient simulator <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mi mathvariant="script" class="calligraphy">P</mml:mi></mml:math></inline-formula>, receive the feedback, choose the next symptom, and so on for several turns. The dialogue will terminate when the agent finally makes a diagnosis, i.e. selects a disease from <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mi mathvariant="script" class="calligraphy">D</mml:mi></mml:math></inline-formula>. The goal of agent is to learn a policy, which can efficiently find implicit symptoms to obtain more complete information of the patient and finally make a correct diagnosis.</p>
    </sec>
    <sec>
      <title>2.2 Accuracy bound</title>
      <p>To explain our motivation, we discuss how to evaluate automated diagnostic systems earlier in this section. There are usually two metrics to evaluate the automatic diagnostic system, i.e. symptom recall and diagnostic accuracy.</p>
      <p>For symptom recall (SX-Rec), it refers to the proportion of implicit symptoms that inquired by the agent. For more clarity, assuming that for a patient, the sequence of symptoms asked by the agent is <italic toggle="yes">S<sub>agt</sub></italic>, then
<disp-formula id="E1"><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:mtext>SX‒Rec</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∑</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">agt</mml:mtext></mml:mrow></mml:msub><mml:mo>∩</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">imp</mml:mtext></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>∑</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">imp</mml:mtext></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
      <p>SX-Rec measures the agent’s ability to find implicit symptoms, with a value between 0 and 1.</p>
      <p>For diagnostic accuracy (DX-Acc), it refers to the proportion of correct diagnosis, and also the final metric we aim to improve. Let’s further discuss the reasonable range of diagnostic accuracy. For the disease classifier, the input is the patient’s symptom features finally collected by the agent, and the symptom recall determines the integrity of the symptom features. If SX-Rec equals to 0, only explicit symptoms <italic toggle="yes">S<sub>exp</sub></italic> can be utilized as features, we call the accuracy of the system in this case the accuracy lower bound (<italic toggle="yes">Acc-LB</italic>); if SX-Rec equals to 1, both explicit symptoms and all implicit symptoms are available as features, in this way, the accuracy of the system is called the accuracy upper bound (<italic toggle="yes">Acc-UB</italic>).</p>
      <p>We present the accuracy bounds for the three datasets (Dxy, MZ-4 and MZ-10) in <xref rid="btac744-T1" ref-type="table">Table 1</xref> as reference. We train support vector machine (SVM) (<xref rid="btac744-B5" ref-type="bibr">Cortes and Vapnik, 1995</xref>) classifiers using 5-fold cross-validation on the training set and report the accuracy on the test set. Among them, Acc-LB means that only explicit symptoms are used as features, Acc-UB means that all symptoms are used, Acc-UB (P) means that explicit symptoms and positive implicit symptoms are used, and Acc-UB (N) means that explicit symptoms and negative implicit symptoms are used.</p>
      <table-wrap position="float" id="btac744-T1">
        <label>Table 1.</label>
        <caption>
          <p>The accuracy bound of SVM classifier trained on Dxy, MZ-4 and MZ-10, these classifiers differ in the features they use</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Dataset</th>
              <th rowspan="1" colspan="1">Acc-LB</th>
              <th rowspan="1" colspan="1">Acc-UB</th>
              <th rowspan="1" colspan="1">Acc-UB (P)</th>
              <th rowspan="1" colspan="1">Acc-UB (N)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Dxy</td>
              <td rowspan="1" colspan="1">0.644</td>
              <td rowspan="1" colspan="1">0.856</td>
              <td rowspan="1" colspan="1">0.808</td>
              <td rowspan="1" colspan="1">0.663</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MZ-4</td>
              <td rowspan="1" colspan="1">0.646</td>
              <td rowspan="1" colspan="1">0.757</td>
              <td rowspan="1" colspan="1">0.698</td>
              <td rowspan="1" colspan="1">0.693</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MZ-10</td>
              <td rowspan="1" colspan="1">0.501</td>
              <td rowspan="1" colspan="1">0.706</td>
              <td rowspan="1" colspan="1">0.629</td>
              <td rowspan="1" colspan="1">0.618</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>The results in <xref rid="btac744-T1" ref-type="table">Table 1</xref> illustrate that the diagnostic accuracy of previous SOTA systems is far from the Acc-UB of SVM classifier, especially for MZ-10 (<xref rid="btac744-F1" ref-type="fig">Fig. 1</xref>). It also suggests that both positive and negative implicit symptoms are useful for disease diagnosis. Besides, the results also give a non-rigorous reference for the agent, that is, the accuracy of a reasonable agent should be roughly between Acc-LB and Acc-UB. We mention ‘non-rigorous’ because the performance of diagnostic accuracy of different classifiers is different. The above discussion leads to the core motivation of this article, which is to <italic toggle="yes">improve the diagnostic accuracy via improving the symptom recall</italic>.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Methods</title>
    <sec>
      <title>3.1 Decoder for symptom inquiry</title>
      <p>For symptom inquiry, assuming that there is no limit on the number of turns, the agent can find all implicit symptoms by simply traversing the symptoms in <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mi mathvariant="script" class="calligraphy">S</mml:mi></mml:math></inline-formula>. In this case, the recall equals to 1, and the accuracy can reach the upper bound. However, the size of <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mi mathvariant="script" class="calligraphy">S</mml:mi></mml:math></inline-formula>, i.e. the action space of symptom inquiry, can be potentially large, this policy is undoubtedly inefficient. Fortunately, due to the apparent cooccurrence between symptoms (<xref rid="btac744-B29" ref-type="bibr">Zhong <italic toggle="yes">et al.</italic>, 2022</xref>), training a more efficient agent is promising.</p>
      <sec>
        <title>3.1.1 Architecture</title>
        <p>In DxFormer, we analogize the process of symptom inquiry to a language model (<xref rid="btac744-B2" ref-type="bibr">Bengio <italic toggle="yes">et al.</italic>, 2003</xref>). The symptoms are regarded as tokens, and attributes of symptoms are regarded as features of tokens, then symptom inquiry can be regarded as a text generation problem. We adopt the multi-layer Transformer (<xref rid="btac744-B18" ref-type="bibr">Radford <italic toggle="yes">et al.</italic>, 2018</xref>) decoder as the recurrent model, which applies a multi-headed self-attention operation over the historical symptom-attribute sequence followed by position-wise feedforward layers to produce an output distribution over target symptoms.</p>
      </sec>
      <sec>
        <title>3.1.2 Input representation</title>
        <p>Dense input representation is designed in this work. For each symptom, its input embedding is the sum of corresponding symptom, attribute, and position embeddings. One visual example is shown in <xref rid="btac744-F2" ref-type="fig">Figure 2</xref>. For symptom and attribute, we utilize embedding layers to map any symptom in <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mi mathvariant="script" class="calligraphy">S</mml:mi></mml:math></inline-formula> and any attribute in <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mi mathvariant="script" class="calligraphy">A</mml:mi></mml:math></inline-formula> into dense vectors with same dimension. For position embeddings, we adopt sinusoidal position encoding used in BERT (<xref rid="btac744-B6" ref-type="bibr">Devlin <italic toggle="yes">et al.</italic>, 2019</xref>). The input sequence of symptoms is the concatenation of explicit symptoms and agent’s historical asked symptoms.</p>
        <fig position="float" id="btac744-F2">
          <label>Fig. 2.</label>
          <caption>
            <p>DxFormer is composed of a decoder–encoder structure, where the decoder (a) is used for symptom inquiry, and the encoder (b) is used for disease diagnosis</p>
          </caption>
          <graphic xlink:href="btac744f2" position="float"/>
        </fig>
      </sec>
      <sec>
        <title>3.1.3 RL formalization</title>
        <p>As the decoder for symptom inquiry, our goal is to find as many implicit symptoms as possible within a given number of interactions, which is a discrete objective different from the one used in language models that maximizes the conditional likelihood. We can cast our generative model in the RL terminology as in <xref rid="btac744-B19" ref-type="bibr">Ranzato <italic toggle="yes">et al.</italic> (2015)</xref> due to the non-derivable property of the objective function. Our Transformer decoder can be viewed as the agent that interacts with patient simulator <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mi mathvariant="script" class="calligraphy">P</mml:mi></mml:math></inline-formula>, whose parameters <italic toggle="yes">θ</italic> define a policy <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>, that results in an action that predict the next possible symptom based on known symptoms.</p>
      </sec>
      <sec>
        <title>3.1.4 Reward setting</title>
        <p><italic toggle="yes">Priori reward</italic>: A particular disease is often related to a certain group of symptoms rather than all symptoms (<xref rid="btac744-B29" ref-type="bibr">Zhong <italic toggle="yes">et al.</italic>, 2022</xref>). Therefore, agent is encouraged to ask about symptoms related to the specific disease. We achieve this through the disease–symptom co-occurrence frequency matrix in training set. For each symptom in <italic toggle="yes">S<sub>agt</sub></italic>, if the frequency of co-occurrence of the symptom and the disease corresponding to the case exceeds zero, a positive reward of + 1 will be given, otherwise a negative reward of –1 will be given.</p>
        <p><italic toggle="yes">Ground reward</italic>: The agent is encouraged to inquiry implicit symptoms to increase symptoms recall. For any symptom in <italic toggle="yes">S<sub>agt</sub></italic>, if the symptom is also in <italic toggle="yes">S<sub>imp</sub></italic>, a reward of + 2.5 will be given, otherwise –0.5 reward will be given.</p>
        <p>Priori reward aims to prevent the agent from asking unrelated strange symptoms, while ground reward facilitates the discovery of implicit symptoms. We will discuss the settings of these reward parameters and their impact on the results in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref>.</p>
      </sec>
      <sec>
        <title>3.1.5 Training objective</title>
        <p>The final reward of each action in <italic toggle="yes">S<sub>agt</sub></italic> is equal to the sum of priori reward and ground reward, and the training objective of the decoder is to minimize the negative expected reward:
<disp-formula id="E2"><label>(1)</label><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="script" class="calligraphy">L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>θ</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">E</mml:mi></mml:mrow><mml:mrow><mml:mo>τ</mml:mo><mml:mo>∼</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>τ</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mrow><mml:mo>τ</mml:mo><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> is random reward sequence that obeys the conditional probability distribution parameterized by <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mrow><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>τ</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
        <p>To compute the gradient <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>∇</mml:mo></mml:mrow><mml:mo>θ</mml:mo></mml:msub><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>θ</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, we use the REINFORCE algorithm (<xref rid="btac744-B24" ref-type="bibr">Williams, 1992</xref>), which approximates the reward function using a single Monte-Carlo sample from <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> for each training example in the minibatch. Similar to traditional text generation, we initialize the parameter <italic toggle="yes">θ</italic> of the decoder by <italic toggle="yes">language model pre-training</italic> with maximum likelihood objectives.</p>
      </sec>
    </sec>
    <sec>
      <title>3.2 Encoder for disease diagnosis</title>
      <p>Upon the termination of symptom inquiry, we will extracted all positive and negative symptoms obtained by the agent. These symptoms, together with explicit symptoms, are used as features for disease diagnosis.</p>
      <sec>
        <title>3.2.1 Architecture</title>
        <p>We adopt a multi-layer Transformer encoder to encode these symptoms, and then pass through an average pooling layer and a linear layer to produce an output distribution over target diseases. As shown in <xref rid="btac744-F2" ref-type="fig">Figure 2</xref>, the Transformer encoder in our disease classifier differs from the decoder for symptom inquiry in the following three ways: (i) the encoder adopts a bidirectional transformer, while the decoder is unidirectional; (ii) position embeddings is removed from the input representation in the encoder, since intuitively the disease classifier should be insensitive with the order of symptoms; and (iii) our encoder is shallower than the decoder (<italic toggle="yes">M </italic>&lt;<italic toggle="yes"> N</italic>), the main reason is because the symptom inquiry is more complex and requires more parameters. It is worth noting that our encoder and decoder <italic toggle="yes">share</italic> the parameters of symptom embedding and attribute embedding.</p>
      </sec>
      <sec>
        <title>3.2.2 Training</title>
        <p>In DxFormer, the encoder is jointly trained with the decoder. Given the explicit symptoms of a patient, we obtain <italic toggle="yes">S<sub>sdec</sub></italic> and <italic toggle="yes">S<sub>gdec</sub></italic> by the decoder using sampling decoding and greedy decoding respectively, where <italic toggle="yes">S<sub>sdec</sub></italic> is used to compute the REINFORCE reward, and <italic toggle="yes">S<sub>gdec</sub></italic> is used to compute the cross-entropy loss between the predicted disease distribution and the real disease distribution. The final loss function equals to the sum of negative REINFORCE reward and the cross-entropy loss.</p>
      </sec>
      <sec>
        <title>3.2.3 Stopping criterion</title>
        <p>The transition from symptom inquiry to disease diagnosis is determined by the stopping criterion. During training, we specify a maximum turn <italic toggle="yes">T<sub>max</sub></italic> for symptom inquiry, and the agent’s goal is to find as many implicit symptoms as possible within <italic toggle="yes">T<sub>max</sub></italic> turns and make correct diagnosis. However, agents do not always need to inquiry symptoms <italic toggle="yes">T<sub>max</sub></italic> times. For some cases, the key symptom information has been collected, so it is unnecessary to continue to ask the patient to obtain some unknown or unimportant symptoms.</p>
        <p>We take a simple but effective method to stop the symptom inquiry early. After each turn of symptom inquiry, the currently collected symptoms are fed to the encoder to compute the probability distribution over the possible diseases. The symptom inquiry will be terminated once the probability of the predicted disease (with maximum probability) is beyond the certain threshold <italic toggle="yes">ϵ</italic>.</p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>4 Experiments</title>
    <sec>
      <title>4.1 Experimental datasets</title>
      <p>We evaluate DxFormer on three public real-world medical dialogue datasets: Dxy, MZ-4 and MZ-10, all of which consists of a number of annotated structured MCRs described in Section 2.1. Details statistics of the datasets are listed in <xref rid="btac744-T2" ref-type="table">Table 2</xref>.</p>
      <table-wrap position="float" id="btac744-T2">
        <label>Table 2.</label>
        <caption>
          <p>Data statistics of Dxy, MZ-4 and MZ-10, the values in the form ‘a/b’ in the last two columns represent the <italic toggle="yes">average</italic> and <italic toggle="yes">maximum</italic> values, respectively</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Dataset</th>
              <th rowspan="1" colspan="1"># of samples</th>
              <th rowspan="1" colspan="1"># of diseases</th>
              <th rowspan="1" colspan="1"># of symptoms</th>
              <th rowspan="1" colspan="1"># of Exp-symptoms</th>
              <th rowspan="1" colspan="1"># of Imp-symptoms</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Dxy (<xref rid="btac744-B26" ref-type="bibr">Xu <italic toggle="yes">et al.</italic>, 2019</xref>)</td>
              <td rowspan="1" colspan="1">527</td>
              <td rowspan="1" colspan="1">5</td>
              <td rowspan="1" colspan="1">41</td>
              <td rowspan="1" colspan="1">3.1/7</td>
              <td rowspan="1" colspan="1">1.7/6</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MZ-4 (<xref rid="btac744-B23" ref-type="bibr">Wei <italic toggle="yes">et al.</italic>, 2018</xref>)</td>
              <td rowspan="1" colspan="1">1733</td>
              <td rowspan="1" colspan="1">4</td>
              <td rowspan="1" colspan="1">230</td>
              <td rowspan="1" colspan="1">2.1/10</td>
              <td rowspan="1" colspan="1">5.5/21</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MZ-10</td>
              <td rowspan="1" colspan="1">4116</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">331</td>
              <td rowspan="1" colspan="1">1.7/12</td>
              <td rowspan="1" colspan="1">6.6/25</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <p><italic toggle="yes">Note</italic>: Exp is short for Explicit and Imp is short for Implicit.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p><italic toggle="yes">MZ-4</italic> (<xref rid="btac744-B23" ref-type="bibr">Wei <italic toggle="yes">et al.</italic>, 2018</xref>): The first human-labeled dataset collected from the pediatric department of Baidu Muzhi Doctor (<ext-link xlink:href="https://muzhi.baidu.com/" ext-link-type="uri">https://muzhi.baidu.com/</ext-link>) to evaluate automatic diagnostic system. MZ-4 includes four diagnosed diseases: children’s bronchitis, children’s functional dyspepsia, infantile diarrhea infection and upper respiratory infection.</p>
      <p><italic toggle="yes">Dxy</italic> (<xref rid="btac744-B26" ref-type="bibr">Xu <italic toggle="yes">et al.</italic>, 2019</xref>): An annotated medical dialog dataset collected from Dingxiang Doctor (<ext-link xlink:href="https://dxy.com/" ext-link-type="uri">https://dxy.com/</ext-link>), a prevalent Chinese online healthcare website. Dxy includes five diagnosed diseases: allergic rhinitis, upper respiratory infection, pneumonia, children hand-foot-mouth disease and pediatric diarrhea.</p>
      <p><italic toggle="yes">MZ-10</italic>: A dataset with multi-level annotations expanded from MZ-4 to include 10 diseases, including typical diseases of digestive system, respiratory system and endocrine system. MZ-10 also contains more symptoms. The MZ-10 are annotated by medical students. Each dialogue is annotated twice, and the kappa coefficient of symptom labels is 92.71%, which represents a high consistency between the two annotations.</p>
    </sec>
    <sec>
      <title>4.2 Baselines</title>
      <p>We compare DxFormer with some SOTA models for automatic diagnosis that use different techniques, including RL, generative adversarial network and variational autoencoder.</p>
      <p><italic toggle="yes">DQN</italic> (<xref rid="btac744-B23" ref-type="bibr">Wei <italic toggle="yes">et al.</italic>, 2018</xref>): An agent based on the Deep Q Network (DQN) algorithm that adopts the joint action space of symptoms and diseases, where positive reward is given to the agent at the end of a success diagnosis.</p>
      <p><italic toggle="yes">REFUEL</italic> (<xref rid="btac744-B17" ref-type="bibr">Peng <italic toggle="yes">et al.</italic>, 2018</xref>): A policy-based RL method with reward shaping and feature rebuilding, where a branch to reconstruct the symptom vector is utilized to guide the policy gradient.</p>
      <p><italic toggle="yes">KR-DQN</italic> (<xref rid="btac744-B26" ref-type="bibr">Xu <italic toggle="yes">et al.</italic>, 2019</xref>): An improved RL method based on DQN that integrates relational refinement branches and knowledge-routed graphs to strengthen the relation between diseases and symptoms.</p>
      <p><italic toggle="yes">GAMP</italic> (<xref rid="btac744-B25" ref-type="bibr">Xia <italic toggle="yes">et al.</italic>, 2020</xref>): A GAN-based policy gradient network. GAMP uses the GAN network to avoid generating randomized trials of symptom, and add mutual information to encourage the model to select the most discriminative symptoms.</p>
      <p><italic toggle="yes">BSODA</italic> (<xref rid="btac744-B7" ref-type="bibr">He <italic toggle="yes">et al.</italic>, 2022</xref>): An non-RL bipartite framework that uses an information-theoretic reward to collect symptoms, and a multimodal variational autoencoder model is used for disease prediction with a two-step sampling strategy.</p>
      <p>We use the open source implementation (<ext-link xlink:href="https://github.com/Guardianzc/DISCOpen-MedBox-DialoDiagnosis" ext-link-type="uri">https://github.com/Guardianzc/DISCOpen-MedBox-DialoDiagnosis</ext-link>) for DQN, REFUEL, KR-DQN and GAMP, since none of these papers provide official codes, and symptom recall is also not reported in most papers.</p>
    </sec>
    <sec>
      <title>4.3 Model configuration</title>
      <p>DxFormer is composed of a 4-layer decoder and a 1-layer encoder. For Dxy, the embedding and hidden size is set to 128, and feed-forward size is set to 256. For MZ-4 and MZ-10, the embedding and hidden size is set to 512, and feed-forward size is set to 1024. We use Adam optimizer (<xref rid="btac744-B11" ref-type="bibr">Kingma and Ba, 2014</xref>) with a learning rate of <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mrow><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> for maximum-likelihood pre-training, and learning rate of <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> for RL training. All our experiments are performed on 4 Nvidia Tesla V100 32G GPUs. Following the conventional setting, all baseline models as well as our DxFormer specify the maximum number of turns for symptom inquiry to 10 during inference, the threshold <italic toggle="yes">ϵ</italic> is set to 0.99. It is worth noting that during training, the maximum number of turns of DxFormer is set to 40.</p>
    </sec>
    <sec>
      <title>4.4 Main findings</title>
      <p>In <xref rid="btac744-T3" ref-type="table">Table 3</xref>, we report the performance of DxFormer as well as the baseline models. DxFormer shows impressive performance. On the three real-world datasets, DxFormer can find about 45–52% of implicit symptoms in 6–8 turns, and reach about 64–84% of diagnostic accuracy. Compared to best baseline models, DxFormer greatly improves the symptom recall, with an absolute improvement of nearly 12–27%. Besides, the diagnostic accuracy also surpasses all previous models. In particular, on MZ-10, DxFormer improves the accuracy by about 14 absolute percentages over the best baseline.</p>
      <table-wrap position="float" id="btac744-T3">
        <label>Table 3.</label>
        <caption>
          <p>Experimental results of DxFormer on Dxy, MZ-4 and MZ-10 dataset</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Model</th>
              <th colspan="3" rowspan="1">Dxy<hr/></th>
              <th colspan="3" rowspan="1">MZ-4<hr/></th>
              <th colspan="3" rowspan="1">MZ-10<hr/></th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1">SX-Rec</th>
              <th rowspan="1" colspan="1">DX-Acc</th>
              <th rowspan="1" colspan="1"># Turns</th>
              <th rowspan="1" colspan="1">SX-Rec</th>
              <th rowspan="1" colspan="1">DX-Acc</th>
              <th rowspan="1" colspan="1"># Turns</th>
              <th rowspan="1" colspan="1">SX-Rec</th>
              <th rowspan="1" colspan="1">DX-Acc</th>
              <th rowspan="1" colspan="1"># Turns</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>Baselines</bold>
              </td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Flat-DQN (<xref rid="btac744-B23" ref-type="bibr">Wei <italic toggle="yes">et al.</italic>, 2018</xref>)</td>
              <td rowspan="1" colspan="1">0.110</td>
              <td rowspan="1" colspan="1">0.731</td>
              <td rowspan="1" colspan="1">1.96</td>
              <td rowspan="1" colspan="1">0.062</td>
              <td rowspan="1" colspan="1">0.681</td>
              <td rowspan="1" colspan="1">1.27</td>
              <td rowspan="1" colspan="1">0.047</td>
              <td rowspan="1" colspan="1">0.408</td>
              <td rowspan="1" colspan="1">9.75</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">REFUEL (<xref rid="btac744-B17" ref-type="bibr">Peng <italic toggle="yes">et al.</italic>, 2018</xref>)</td>
              <td rowspan="1" colspan="1">0.186</td>
              <td rowspan="1" colspan="1">0.721</td>
              <td rowspan="1" colspan="1">3.11</td>
              <td rowspan="1" colspan="1">0.215</td>
              <td rowspan="1" colspan="1">0.716</td>
              <td rowspan="1" colspan="1">5.01</td>
              <td rowspan="1" colspan="1">0.262</td>
              <td rowspan="1" colspan="1">0.505</td>
              <td rowspan="1" colspan="1">5.50</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">KR-DQN (<xref rid="btac744-B26" ref-type="bibr">Xu <italic toggle="yes">et al.</italic>, 2019</xref>)</td>
              <td rowspan="1" colspan="1">0.399</td>
              <td rowspan="1" colspan="1">0.740</td>
              <td rowspan="1" colspan="1">5.65</td>
              <td rowspan="1" colspan="1">0.177</td>
              <td rowspan="1" colspan="1">0.678</td>
              <td rowspan="1" colspan="1">4.61</td>
              <td rowspan="1" colspan="1">0.279</td>
              <td rowspan="1" colspan="1">0.485</td>
              <td rowspan="1" colspan="1">5.95</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GAMP (<xref rid="btac744-B25" ref-type="bibr">Xia <italic toggle="yes">et al.</italic>, 2020</xref>)</td>
              <td rowspan="1" colspan="1">0.268</td>
              <td rowspan="1" colspan="1">0.731</td>
              <td rowspan="1" colspan="1">2.84</td>
              <td rowspan="1" colspan="1">0.107</td>
              <td rowspan="1" colspan="1">0.644</td>
              <td rowspan="1" colspan="1">2.93</td>
              <td rowspan="1" colspan="1">0.067</td>
              <td rowspan="1" colspan="1">0.500</td>
              <td rowspan="1" colspan="1">1.78</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">BSODA (<xref rid="btac744-B7" ref-type="bibr">He <italic toggle="yes">et al.</italic>, 2022</xref>)</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">0.802</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">0.731</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DxFormer (ours)</td>
              <td rowspan="1" colspan="1">
                <bold>0.506</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.817</bold>
              </td>
              <td rowspan="1" colspan="1">6.32</td>
              <td rowspan="1" colspan="1">
                <bold>0.479</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.743</bold>
              </td>
              <td rowspan="1" colspan="1">8.74</td>
              <td rowspan="1" colspan="1">
                <bold>0.449</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.633</bold>
              </td>
              <td rowspan="1" colspan="1">7.58</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>Ablation studies</bold>
              </td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DxFormer-Sparse</td>
              <td rowspan="1" colspan="1">0.456</td>
              <td rowspan="1" colspan="1">0.808</td>
              <td rowspan="1" colspan="1">7.19</td>
              <td rowspan="1" colspan="1">0.396</td>
              <td rowspan="1" colspan="1">0.722</td>
              <td rowspan="1" colspan="1">9.37</td>
              <td rowspan="1" colspan="1">0.365</td>
              <td rowspan="1" colspan="1">0.619</td>
              <td rowspan="1" colspan="1">8.24</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DxFormer-SVM</td>
              <td rowspan="1" colspan="1">0.519</td>
              <td rowspan="1" colspan="1">0.789</td>
              <td rowspan="1" colspan="1">6.32</td>
              <td rowspan="1" colspan="1">0.486</td>
              <td rowspan="1" colspan="1">0.718</td>
              <td rowspan="1" colspan="1">8.74</td>
              <td rowspan="1" colspan="1">0.451</td>
              <td rowspan="1" colspan="1">0.603</td>
              <td rowspan="1" colspan="1">7.58</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn2">
            <p>The boldface values in the table are significantly better than the best baseline values at the 5% significance level (we repeated the experiments for 10 times and conduct one-sample t-test to compute the significance).</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>Considering the performance of these baseline models on the MZ-10 dataset, the diagnostic accuracy is only on par with the lower bound of the SVM classifier (see <xref rid="btac744-T1" ref-type="table">Table 1</xref>), which suggests that these systems approximately degenerate into disease classifiers that are very weak at finding implicit symptoms. This illustrates the advantages of DxFormer over other systems when faced with more diseases and symptoms.</p>
      <p>It is worth noting that in DxFormer, agent interacts with patient simulator more rounds (# Turns) than most systems. It is not surprising since enough interaction turns are the guarantee of high symptom recall. As we mentioned in the introduction, the number of interactions is not controllable in traditional RL-based methods. It is difficult for us to compare the model performance within the same number of turns. Nevertheless, from a clinical point of view, one would expect that asking 6–9 symptoms on average to improve diagnostic accuracy is acceptable.</p>
    </sec>
    <sec>
      <title>4.5 Effect of max number of turns</title>
      <p>In <xref rid="btac744-F3" ref-type="fig">Figure 3</xref>, we visualize the effect of max number of turns, i.e. <italic toggle="yes">T<sub>max</sub></italic>, on model performance on the MZ-4 and MZ-10 datasets. It can be seen that both symptom recall and diagnostic accuracy increase with the increase of the maximum number of allowed interactions. This confirms the core motivation of this article, that <italic toggle="yes">it is feasible to improve the diagnostic accuracy by increasing the symptom recall</italic>. It can also be found that when symptom recall exceeds about 70%, diagnostic accuracy gradually converges to the upper bound, which is higher than the upper bound of SVM classifier. On the MZ-4 and MZ-10 datasets, when the agent is allowed to interact with the patient more than 20 times, the diagnostic accuracy can be as high as about 78% and 70%, respectively, which is much higher than the current SOTA results.</p>
      <fig position="float" id="btac744-F3">
        <label>Fig. 3.</label>
        <caption>
          <p>The effect of max turns on the Symptom Recall and Diagnostic Accuracy on MZ-4 (a) and MZ-10 (b) dataset</p>
        </caption>
        <graphic xlink:href="btac744f3" position="float"/>
      </fig>
      <p>Besides, we also create a strong rule-based agent for symptom inquiry, i.e. based on the symptom co-occurrence probability matrix, to find the symptom with the highest correlation with the collected symptom set each time. The yellow line in <xref rid="btac744-F3" ref-type="fig">Figure 3</xref> shows the results of the method. It can be seen that rule-based agent performs worse than DxFormer, especially on the MZ-10 dataset. When the number of symptoms increases (from MZ-4 to MZ-10), the dense representation of symptoms shows advantages, as the symptom recall of DxFormer does not decrease.</p>
    </sec>
    <sec>
      <title>4.6 Effect of stopping criterion threshold</title>
      <p>We further explore the impact of the threshold value in stopping criterion on model performance. We fix the maximum number of turns to 10 and set different thresholds to observe the model performance. The results are visualized in <xref rid="btac744-F4" ref-type="fig">Figure 4</xref>. We have two findings. First, both diagnostic accuracy and average turns decrease as the threshold value decreases from 1.0 to 0.9, this shows that, as the number of turns for symptom inquiry increases, the collected symptoms increase, and the certainty of disease classifier also increases. Secondly, symptom recall declines slowly at the beginning. This suggests that it is possible to choose an appropriate threshold that reduces the average number of turns with little loss of diagnostic accuracy.</p>
      <fig position="float" id="btac744-F4">
        <label>Fig. 4.</label>
        <caption>
          <p>The effect of threshold on MZ-4 and MZ-10 dataset</p>
        </caption>
        <graphic xlink:href="btac744f4" position="float"/>
      </fig>
      <p>In DxFormer, <italic toggle="yes">T<sub>max</sub></italic> and <italic toggle="yes">ϵ</italic> together determine the balance of accuracy and efficiency. In practice, we recommend to first choose the <italic toggle="yes">T<sub>max</sub></italic> that allows diagnostic accuracy to converge to the upper bound, and then select an appropriate threshold to achieve acceptable accuracy and efficiency.</p>
    </sec>
    <sec>
      <title>4.7 Ablation studies</title>
      <p>To verify the effectiveness of each component in DxFormer, we conduct some ablation experiments. <italic toggle="yes">DxFormer-Sparse</italic> is an agent that exactly the same as DxFormer except for the input presentation. DxFormer-Sparse uses the one-hot representation of symptom and attribute and the concatenation of the one-hot vectors are fed as the input; <italic toggle="yes">DxFormer-SVM</italic> is an agent that uses the same decoder of DxFormer but replaces the encoder with an SVM classifier.</p>
      <p>From the ablation analysis results in <xref rid="btac744-T3" ref-type="table">Table 3</xref>, both DxFormer-Sparse and DxFormer-SVM perform worse than DxFormer, which shows that the dense representation of symptoms is effective. Notably, the two variants of DxFormer can still beat SOTAs, illustrating the effectiveness of our decoupled framework. In fact, in our early attempts, models like RNN-MLP, LSTM-MLP can also work well, although not as well as DxFormer. The decoder–encoder framework and stopping criterion together contributes to the excellent performance of DxFormer.</p>
    </sec>
  </sec>
  <sec>
    <title>5 Related work</title>
    <p><italic toggle="yes">Automatic disease diagnosis</italic>: Machine learning-based method plays an important role in solving medical prediction, such as depression prediction (<xref rid="btac744-B1" ref-type="bibr">Aekwarangkoon and Thanathamathee, 2022</xref>) and epidemic prediction (<xref rid="btac744-B10" ref-type="bibr">Kim, 2021</xref>). Automatic disease diagnosis is an important part of medical prediction. Deep RL (<xref rid="btac744-B16" ref-type="bibr">Mnih <italic toggle="yes">et al.</italic>, 2013</xref>; <xref rid="btac744-B21" ref-type="bibr">Silver <italic toggle="yes">et al.</italic>, 2016</xref>) has been applied for automatic diagnosis (<xref rid="btac744-B9" ref-type="bibr">Kao <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btac744-B22" ref-type="bibr">Tang <italic toggle="yes">et al.</italic>, 2016</xref>). <xref rid="btac744-B17" ref-type="bibr">Peng <italic toggle="yes">et al.</italic> (2018)</xref> proposed reward shaping and feature rebuilding method for fast disease diagnosis. However, their data used is simulated that cannot reflect the situation of the real diagnosis. For the medical dialogue system for automatic diagnosis, <xref rid="btac744-B23" ref-type="bibr">Wei <italic toggle="yes">et al.</italic> (2018)</xref> annotated the first medical dataset for dialogue system and use a Deep Q-network (DQN) to collect additional symptoms via conversation with patients. <xref rid="btac744-B26" ref-type="bibr">Xu <italic toggle="yes">et al.</italic> (2019)</xref> released another medical dataset for the dialogue system and introduce prior knowledge to improve the diagnosis accuracy. <xref rid="btac744-B29" ref-type="bibr">Zhong <italic toggle="yes">et al.</italic> (2022)</xref> propose a hierarchical RL framework based on master–worker structure for simulating real medical consultation. There are also some related studies based on non-RL framework (<xref rid="btac744-B7" ref-type="bibr">He <italic toggle="yes">et al.</italic>, 2022</xref>; <xref rid="btac744-B25" ref-type="bibr">Xia <italic toggle="yes">et al.</italic>, 2020</xref>). Diaformer (<xref rid="btac744-B3" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2022a</xref>) is a contemporary work with our DxFormer, which is similar to our ideas, but with different motivations. We do not compare the results because the number of turns is set differently.</p>
    <p><italic toggle="yes">RL-based text generation</italic>: RL is also a popular alternative in text generation, especially when the training objective is not the traditional maximum likelihood (<xref rid="btac744-B19" ref-type="bibr">Ranzato <italic toggle="yes">et al.</italic>, 2015</xref>). Use REINFORCE algorithm to maximize the BLEU of the generated sequence to solve the exposure bias problem of the traditional seq2seq model. <xref rid="btac744-B12" ref-type="bibr">Li <italic toggle="yes">et al.</italic> (2016)</xref> use policy gradient algorithm to maximize the mutual information of generated response in dialogue system. <xref rid="btac744-B20" ref-type="bibr">Rennie <italic toggle="yes">et al.</italic> (2017)</xref> propose self-critical sequence training, which utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences in image captioning.</p>
  </sec>
  <sec>
    <title>6 Conclusions, limitations and future work</title>
    <p>In this work, we propose a decoupled system for automatic diagnosis called DxFormer, which uses Transformer’s decoder and encoder for symptom query and disease diagnosis with dense symptom representations. Symptom query and disease diagnosis are formalized into conditional text generation and sequence classification. The stopping criterion controls the transition from symptom query to disease diagnosis to parametrically control the number of interaction turns. Extensive experiments show that DxFormer can greatly improve the symptom recall rate and diagnostic accuracy.</p>
    <p>Although promising results have been obtained, the system still has some limitations: (i) in this paper, the stopping criterion is simple and crude, and its reliability and scalability should be discussed; (ii) in practice, agents should be allowed to ask multiple symptoms at one time rather than a single symptom to improve efficiency; and (iii) it is not enough to use only the symptoms for diagnosis. In the actual situation, more factors need to be considered, including medical examination, past medical history, surrounding environment, etc.</p>
    <p>In future work, we hope to further explore automatic disease diagnosis based on more features. Besides, we will also try to improve the model from the perspective of biological interpretability.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btac744_Supplementary_Data</label>
      <media xlink:href="btac744_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <sec>
    <title>Funding</title>
    <p>This work was partially supported by the Natural Science Foundation of China [71991471 and 6217020551], Science and Technology Commission of Shanghai Municipality [20dz1200600 and 21QA1400600] and Zhejiang Lab [2019KD0AD01].</p>
    <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btac744-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aekwarangkoon</surname><given-names>S.</given-names></string-name>, <string-name><surname>Thanathamathee</surname><given-names>P.</given-names></string-name></person-group> (<year>2022</year>) <article-title>Associated patterns and predicting model of life trauma, depression, and suicide using ensemble machine learning</article-title>. <source>Emerg. Sci. J</source>., <volume>6</volume>, <fpage>679</fpage>–<lpage>693</lpage>.</mixed-citation>
    </ref>
    <ref id="btac744-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bengio</surname><given-names>Y.</given-names></string-name></person-group><etal>et al</etal> (<year>2003</year>) <article-title>A neural probabilistic language model</article-title>. <source>J. Mach. Learn. Res</source>., <volume>3</volume>, <fpage>1137</fpage>–<lpage>1155</lpage>.</mixed-citation>
    </ref>
    <ref id="btac744-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2022a</year>) Diaformer: Automatic diagnosis via symptoms sequence generation. In: <italic toggle="yes">Proceedings of the AAAI Conference on Artificial Intelligence</italic>, Vol. <volume>36</volume>, pp. <fpage>4432</fpage>–<lpage>4440</lpage>.</mixed-citation>
    </ref>
    <ref id="btac744-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cortes</surname><given-names>C.</given-names></string-name>, <string-name><surname>Vapnik</surname><given-names>V.</given-names></string-name></person-group> (<year>1995</year>) <article-title>Support vector machine</article-title>. <source>Mach. Learn</source>., <volume>20</volume>, <fpage>273</fpage>–<lpage>297</lpage>.</mixed-citation>
    </ref>
    <ref id="btac744-B6">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Devlin</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) Bert: Pre-training of deep bidirectional transformers for language understanding. In: <italic toggle="yes">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</italic>, Vol. 1, pp. 4171–4186.</mixed-citation>
    </ref>
    <ref id="btac744-B7">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>He</surname><given-names>W.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) BSODA: a bipartite scalable framework for online disease diagnosis. In: <italic toggle="yes">Proceedings of the ACM Web Conference 2022, Lyon, France, April 25 - 29, 2022</italic>, pp. <fpage>2511</fpage>–<lpage>2521</lpage>.</mixed-citation>
    </ref>
    <ref id="btac744-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Janisch</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>Classification with costly features as a sequential decision-making problem</article-title>. <source>Mach. Learn</source>., <volume>109</volume>, <fpage>1587</fpage>–<lpage>1615</lpage>.</mixed-citation>
    </ref>
    <ref id="btac744-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kao</surname><given-names>H.-C.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) Context-aware symptom checking for disease diagnosis using hierarchical reinforcement learning. In: <italic toggle="yes">Proceedings of the AAAI Conference on Artificial Intelligence, Brussels, Belgium, October 31-November 4, 2018</italic>, Vol. <volume>32</volume>, pp. <fpage>2305</fpage>–<lpage>2313</lpage>.</mixed-citation>
    </ref>
    <ref id="btac744-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname><given-names>M.</given-names></string-name></person-group> (<year>2021</year>) <article-title>Prediction of covid-19 confirmed cases after vaccination: based on statistical and deep learning models</article-title>. <source>Sci. Med. J</source>., <volume>3</volume>, <fpage>153</fpage>–<lpage>165</lpage>.</mixed-citation>
    </ref>
    <ref id="btac744-B11">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kingma</surname><given-names>D.P.</given-names></string-name>, <string-name><surname>Ba</surname><given-names>J.</given-names></string-name></person-group> (<year>2015</year>) Adam: A method for stochastic optimization. In: <italic toggle="yes">International Conference on Learning Representations (ICLR)</italic>.</mixed-citation>
    </ref>
    <ref id="btac744-B12">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2016</year>) Deep reinforcement learning for dialogue generation. In: <italic toggle="yes">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Austin, Texas</italic>, Association for Computational Linguistics. pp. 1192–1202.</mixed-citation>
    </ref>
    <ref id="btac744-B14">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Lin</surname><given-names>X.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) Enhancing dialogue symptom diagnosis with global attention and symptom graph. In: <italic toggle="yes">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Hong Kong, China, November 5–7, 2019</italic>, pp. <fpage>5033</fpage>–<lpage>5042</lpage>.</mixed-citation>
    </ref>
    <ref id="btac744-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Martinez</surname><given-names>C.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>Adaptive early classification of temporal sequences using deep reinforcement learning</article-title>. <source>Knowl. Based Syst</source>., <volume>190</volume>, <fpage>105290</fpage>.</mixed-citation>
    </ref>
    <ref id="btac744-B16">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Mnih</surname><given-names>V.</given-names></string-name></person-group><etal>et al</etal> (<year>2015</year>) Human-level control through deep reinforcement learning. <italic toggle="yes">Nature</italic>, <bold>518</bold>(7540), 529–533.</mixed-citation>
    </ref>
    <ref id="btac744-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peng</surname><given-names>Y.-S.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>Refuel: exploring sparse features in deep reinforcement learning for fast disease diagnosis</article-title>. <source>Adv. Neural Inf. Process. Syst</source>., <volume>31</volume>, <fpage>7322</fpage>–<lpage>7331</lpage>.</mixed-citation>
    </ref>
    <ref id="btac744-B18">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Radford</surname><given-names>A.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <italic toggle="yes">Improving Language Understanding by Generative Pre-Training</italic>.</mixed-citation>
    </ref>
    <ref id="btac744-B19">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Ranzato</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2016</year>) Sequence level training with recurrent neural networks. Publisher Copyright: © ICLR 2016: San Juan, Puerto Rico. All Rights Reserved.; <italic toggle="yes">4th International Conference on Learning Representations, ICLR 2016</italic>.</mixed-citation>
    </ref>
    <ref id="btac744-B20">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Rennie</surname><given-names>S.J.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) Self-critical sequence training for image captioning. In: <italic toggle="yes">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, Hawaii, July 22-July 25, 2017</italic>, pp. <fpage>7008</fpage>–<lpage>7024</lpage>.</mixed-citation>
    </ref>
    <ref id="btac744-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Silver</surname><given-names>D.</given-names></string-name></person-group><etal>et al</etal> (<year>2016</year>) <article-title>Mastering the game of go with deep neural networks and tree search</article-title>. <source>Nature</source>, <volume>529</volume>, <fpage>484</fpage>–<lpage>489</lpage>.<pub-id pub-id-type="pmid">26819042</pub-id></mixed-citation>
    </ref>
    <ref id="btac744-B22">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Tang</surname><given-names>K.-F.</given-names></string-name></person-group><etal>et al</etal> (<year>2016</year>) Inquire and diagnose: Neural symptom checking ensemble using deep reinforcement learning. In: <italic toggle="yes">NIPS Workshop on Deep Reinforcement Learning, Barcelona, Spain, Dec 5-10, 2016</italic>.</mixed-citation>
    </ref>
    <ref id="btac744-B23">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Wei</surname><given-names>Z.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) Task-oriented dialogue system for automatic diagnosis. In: <italic toggle="yes">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL 2018, Melbourne, Australia, July 15-20, 2018</italic>, pp. <fpage>201</fpage>–<lpage>207</lpage>.</mixed-citation>
    </ref>
    <ref id="btac744-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Williams</surname><given-names>R.J.</given-names></string-name></person-group> (<year>1992</year>) <article-title>Simple statistical gradient-following algorithms for connectionist reinforcement learning</article-title>. <source>Mach. Learn</source>., <volume>8</volume>, <fpage>229</fpage>–<lpage>256</lpage>.</mixed-citation>
    </ref>
    <ref id="btac744-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xia</surname><given-names>Y.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) Generative adversarial regularized mutual information policy gradient framework for automatic diagnosis. In: <italic toggle="yes">Proceedings of the AAAI Conference on Artificial Intelligence, Hilton New York Midtown, New York, USA, February 7-12, 2020</italic>, Vol. <volume>34</volume>, pp. <fpage>1062</fpage>–<lpage>1069</lpage>.</mixed-citation>
    </ref>
    <ref id="btac744-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xu</surname><given-names>L.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) End-to-end knowledge-routed relational dialogue system for automatic diagnosis. In: <italic toggle="yes">Proceedings of the AAAI Conference on Artificial Intelligence, Hilton Hawaiian Village, Honolulu, Hawaii, USA, January 27 – February 1, 2019</italic>, Vol. <volume>33</volume>, pp. <fpage>7346</fpage>–<lpage>7353</lpage>.</mixed-citation>
    </ref>
    <ref id="btac744-B28">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Zeng</surname><given-names>G.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) MedDialog: a large-scale medical dialogue dataset. In: <italic toggle="yes">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</italic>, pp. <fpage>9241</fpage>–<lpage>9250</lpage>.</mixed-citation>
    </ref>
    <ref id="btac744-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhong</surname><given-names>C.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>Hierarchical reinforcement learning for automatic disease diagnosis</article-title>. <source>Bioinformatics</source>, <volume>38</volume>, <fpage>3995</fpage>–<lpage>4001</lpage>.<pub-id pub-id-type="pmid">35775965</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
