<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Healthc Inform Res</journal-id>
    <journal-id journal-id-type="iso-abbrev">Healthc Inform Res</journal-id>
    <journal-id journal-id-type="publisher-id">HIR</journal-id>
    <journal-title-group>
      <journal-title>Healthcare Informatics Research</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">2093-3681</issn>
    <issn pub-type="epub">2093-369X</issn>
    <publisher>
      <publisher-name>Korean Society of Medical Informatics</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6859266</article-id>
    <article-id pub-id-type="doi">10.4258/hir.2019.25.4.344</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Tutorial</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Augmentation of Doppler Radar Data Using Generative Adversarial Network for Human Motion Analysis</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-5610-0631</contrib-id>
        <name>
          <surname>Alnujaim</surname>
          <given-names>Ibrahim</given-names>
        </name>
        <xref ref-type="aff" rid="A1"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-4067-6254</contrib-id>
        <name>
          <surname>Kim</surname>
          <given-names>Youngwook</given-names>
        </name>
        <xref ref-type="aff" rid="A1"/>
      </contrib>
    </contrib-group>
    <aff id="A1">Department of Electrical and Computer Engineering, California State University, Fresno, CA, <country>USA</country>.</aff>
    <author-notes>
      <corresp>Corresponding Author: Youngwook Kim. Department of Electrical and Computer Engineering, California State University, 2320 E. San Ramon Ave, MS EE 94 Fresno, CA 93740-8030, USA. Tel: +1-559-278-4629, <email>youngkim@csufresno.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>31</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <volume>25</volume>
    <issue>4</issue>
    <fpage>344</fpage>
    <lpage>349</lpage>
    <history>
      <date date-type="received">
        <day>16</day>
        <month>7</month>
        <year>2019</year>
      </date>
      <date date-type="rev-recd">
        <day>07</day>
        <month>10</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>21</day>
        <month>10</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2019 The Korean Society of Medical Informatics</copyright-statement>
      <copyright-year>2019</copyright-year>
      <copyright-holder>The Korean Society of Medical Informatics</copyright-holder>
      <license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <abstract>
      <sec>
        <title>Objectives</title>
        <p>Human motion analysis can be applied to the diagnosis of musculoskeletal diseases, rehabilitation therapies, fall detection, and estimation of energy expenditure. To analyze human motion with micro-Doppler signatures measured by radar, a deep learning algorithm is one of the most effective approaches. Because deep learning requires a large data set, the high cost involved in measuring large amounts of human data is an intrinsic problem. The objective of this study is to augment human motion micro-Doppler data employing generative adversarial networks (GANs) to improve the accuracy of human motion classification.</p>
      </sec>
      <sec>
        <title>Methods</title>
        <p>To test data augmentation provided by GANs, authentic data for 7 human activities were collected using micro-Doppler radar. Each motion yielded 144 data samples. Software including GPU driver, CUDA library, cuDNN library, and Anaconda were installed to train the GANs. Keras-GPU, SciPy, Pillow, OpenCV, Matplotlib, and Git were used to create an Anaconda environment. The data produced by GANs were saved every 300 epochs, and the training was stopped at 3,000 epochs. The images generated from each epoch were evaluated, and the best images were selected.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>Each data set of the micro-Doppler signatures, consisting of 144 data samples, was augmented to produce 1,472 synthesized spectrograms of 64 × 64. Using the augmented spectrograms, the deep neural network was trained, increasing the accuracy of human motion classification.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p>Data augmentation to increase the amount of training data was successfully conducted through the use of GANs. Thus, augmented micro-Doppler data can contribute to improving the accuracy of human motion recognition.</p>
      </sec>
    </abstract>
    <kwd-group>
      <kwd>Motion Perception</kwd>
      <kwd>Data Visualization</kwd>
      <kwd>Deep Learning</kwd>
      <kwd>Big Data</kwd>
      <kwd>Supervised Machine Learning</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec sec-type="intro">
    <title>I. Introduction</title>
    <p>Human motion analysis has diverse applications in medicine, healthcare, rehabilitation, game engineering, surveillance, search and rescue, and defense. Human motion analysis can be used for the diagnosis of motion-related diseases, such as cumulative trauma disorders, psychosomatic disorders, and autism spectrum disorders [<xref rid="B1" ref-type="bibr">1</xref>]. Energy expenditure can be estimated by the class of human motion [<xref rid="B2" ref-type="bibr">2</xref>]. In addition, human gait analysis is essential for the evaluation of the degree of rehabilitation. Radar offers a unique opportunity for monitoring human motion remotely. In particular, micro-Doppler signatures produced by human limb motion contain information pertaining to such motion. Because micro-Doppler signatures are represented as a spectrogram in the form of an image, human motions can be recognized through analysis of spectrogram images.</p>
    <p>Due to the advancement of deep learning, the image recognition/classification problem can be effectively addressed by the use of deep convolutional neural networks (DCNN). To train a DCNN effectively to achieve high classification accuracy requires a large amount of image data. In the case of radar data, such an effort is challenging due to a lack of historical records as well as the high costs of collecting a large data set. Therefore, it is necessary to augment the radar data set to fully explore the capability of a DCNN. Recently, generative adversarial networks (GANs) have been successfully used to address the radar data augmentation problem [<xref rid="B3" ref-type="bibr">3</xref>].</p>
    <p>A GAN is a machine learning algorithm designed to produce large amounts of synthesized data that have similar distributions to that of the original data. Owing to this capability, GANs have many applications, such as image synthesis, image de-noising, and image-to-image translation. A GAN consists of two networks, a generative network and a discriminative network, that compete against each other during the training process. The generative network generates synthesized images, and the discriminative network evaluates the generated images. During training, the cost function is defined such that the generative network decreases the classification rate of the discriminative network, while the discriminative network is trained to increase the classification accuracy. Over the course of training, each of the networks contributes to improve the appearance of generated images [<xref rid="B4" ref-type="bibr">4</xref>].</p>
    <p>This tutorial will describe the process of setting up environments for GANs through the installation of the GPU driver, cuDNN library, CUDA library, and Anaconda along with the training of GANs using a measured data set. Finally, we will apply this approach to augment a human motion data set measured by Doppler radar to investigate whether the augmented data are effective in the training of a DCNN.</p>
  </sec>
  <sec sec-type="methods">
    <title>II. Methods</title>
    <p>In this study, the data set included 7 activities that were recorded using 12 human subjects for 12 iterations; the total number of data points was 1,008. <xref ref-type="fig" rid="F1">Figure 1</xref> shows the measurement setup. The 7 activities included boxing while moving forward, boxing while standing in place, crawling, running, sitting still, walking, and walking low while holding a stick. The data were organized in a MATLAB .matfile named <italic>Seven_activity</italic>. The .mat file has a structure file named <italic>activity</italic>. The structure has three fields. The first field, <italic>name</italic>, is a string containing the activity name; the second field, <italic>human_number</italic>, is a numerical number with a datatype double; and the third field, <italic>data</italic>, has a matrix sized 600 × 140.</p>
    <p>The GAN we designed consists of two neural networks. The generative network takes an input of a noise vector and tries to produce a synthesized image, while the discriminative network tries to classify the data correctly as synthesized or real data. To train networks, an Adam optimizer is employed to reduce the loss function. The loss function is defined using the error from the discriminative network. For the activation, a sigmoid function is used. To initialize the weight and biases in the neural networks, Xavier initialization is used. The steps and processes are described in below.</p>
    <p>This tutorial was completed on Windows 7 with an i7 CPU and an NVIDIA GTX 770 GPU. Since Anaconda and GPU drivers are available on MacOS and Ubuntu, this should be easy to replicate on any OS and any NVIDIA graphic card with cuDNN support and the same software as that used in our work. Training a GAN requires significant memory space to complete the process; a GPU is preferred because of the large amount of memory available. To use the GPU, the right drivers must be installed correctly, starting with the graphic card driver and followed by the CUDA library and the cuDNN library. To download the CUDA library, go to website (<ext-link ext-link-type="uri" xlink:href="https://developer.nvidia.com/cuda-toolkit">https://developer.nvidia.com/cuda-toolkit</ext-link>), click on ‘Download now’, then choose the appropriate operating system and follow the installation instructions [<xref rid="B5" ref-type="bibr">5</xref>]. To download the cuDNN library, go to website (<ext-link ext-link-type="uri" xlink:href="https://developer.nvidia.com/rdp/form/cudnn-download-survey">https://developer.nvidia.com/rdp/form/cudnn-download-survey</ext-link>). A membership must be created to download the file. It is necessary to download the latest library with the appropriate CUDA version and then follow the installation process [<xref rid="B6" ref-type="bibr">6</xref>]. In a Windows environment, the following path variable must be added:</p>
    <disp-quote>
      <p>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\bin;</p>
    </disp-quote>
    <disp-quote>
      <p>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\libnvvp.</p>
    </disp-quote>
    <p>The procedure to set the path variable is shown in <xref ref-type="fig" rid="F2">Figure 2</xref>. To use TensorFlow, Anaconda version 4.3 with Python 2 must be installed. This Anaconda version can be found at <ext-link ext-link-type="uri" xlink:href="https://repo.continuum.io/archive/">https://repo.continuum.io/archive/</ext-link>. Download ‘Anaconda2-4.3.1-Windows-x86.exe’ for a 32-bit system or ‘Anaconda2-4.3.1-Windows-x86_64.exe’ for a 64-bit system. Then follow the installation process. After installation of Anaconda, the following path variable must be added:</p>
    <disp-quote>
      <p>C:\Program Files\Anaconda2;</p>
    </disp-quote>
    <disp-quote>
      <p>C:\Program Files\Anaconda2\Scripts;</p>
    </disp-quote>
    <disp-quote>
      <p>C:\Program Files\Anaconda2\Library\bin.</p>
    </disp-quote>
    <p>After installing Anaconda and setting the path variable, open the Anaconda Prompt and create an environment using the following command:</p>
    <disp-quote>
      <p>conda create --name env_name python = 3.5.</p>
    </disp-quote>
    <p>The name of the environment in this case is env_name. The Python version must be set to 3.5 because this is what TensorFlow uses. The environment can be activated using the following command line:</p>
    <disp-quote>
      <p>activate env_name.</p>
    </disp-quote>
    <p><xref ref-type="fig" rid="F3">Figure 3A</xref> shows a screenshot of an example for creating and activating an environment. The following packages must then be installed—Keras-GPU, SciPy, Pillow, OpenCV, Matplotlib, and Git—in the env_name environment [<xref rid="B7" ref-type="bibr">7</xref><xref rid="B8" ref-type="bibr">8</xref><xref rid="B9" ref-type="bibr">9</xref><xref rid="B10" ref-type="bibr">10</xref><xref rid="B11" ref-type="bibr">11</xref><xref rid="B12" ref-type="bibr">12</xref>]. These packages can easily be installed by typing the following command lines:</p>
    <disp-quote>
      <p>conda install -c conda-forge keras-gpu</p>
    </disp-quote>
    <disp-quote>
      <p>conda install -c anaconda scipy</p>
    </disp-quote>
    <disp-quote>
      <p>conda install -c anaconda pillow</p>
    </disp-quote>
    <disp-quote>
      <p>conda install -c conda-forge opencv</p>
    </disp-quote>
    <disp-quote>
      <p>conda install -c conda-forge matplotlib</p>
    </disp-quote>
    <disp-quote>
      <p>conda install -c anaconda git</p>
    </disp-quote>
    <p>Each line must be executed in sequence. See <xref ref-type="fig" rid="F3">Figure 3B</xref> for an example of a package installation using Anaconda Prompt.</p>
    <p>Once everything is installed, the code must be downloaded to start the training process. The recommendation is to create a folder into which the code can be saved. Next, the folder should be set as the current directory by using the command cd. The code can be downloaded by using the following command [<xref rid="B13" ref-type="bibr">13</xref>]:</p>
    <disp-quote>
      <p>git clone https://github.com/isn350/e_hir_GAN.git</p>
    </disp-quote>
    <p>Open <italic>read.py</italic> to edit the path to the data set in line:</p>
    <disp-quote>
      <p>data_file = ‘C:/Users/STUDENT/Desktop/Ibrahim/GAN_tot’</p>
    </disp-quote>
    <p>The size of the data used in this study was 600 × 140, but if the size of data used is different, this can be modified in read.py as in the following lines:</p>
    <disp-quote>
      <p>original_image_dim_x = 600</p>
    </disp-quote>
    <disp-quote>
      <p>original_image_dim_y = 140</p>
    </disp-quote>
    <p>The number of data points in the data set must be set in <italic>read.py</italic> in the following line:</p>
    <disp-quote>
      <p>if x % 144 == 0:</p>
    </disp-quote>
    <p>The function <italic>read.py</italic> has three objectives, namely, reading the data, preparing the data for GANs training, and visualization of the data. The function reads a .mat file and resizes the images to 64 × 64 to input to the GANs, while the size of the input image can be any in the .mat file. For example, the original data size in our case was 600 × 140. Once the codes are saved, to run the code, run the following command:</p>
    <disp-quote>
      <p>python GAN_train.py</p>
    </disp-quote>
    <p>Running the above command only produces a GAN image for boxing while moving forward. To change the activity, open <italic>GAN_train.py</italic> using Notepad, find the line below to change the activity, <italic>activity</italic> = ‘<italic>boxingmoving</italic>’, and change the name between single quotation marks. The code in <italic>GAN_train.py</italic> initiates the training process of GANs. In the code, the directory of the input data and output data from GANs is determined. <xref ref-type="fig" rid="F3">Figure 3C</xref> shows an example of running the code and the output lines.</p>
  </sec>
  <sec sec-type="results">
    <title>III. Results</title>
    <p>After visual inspection, augmented images were produced at 2,700 epochs. Images of the original data are shown in <xref ref-type="fig" rid="F4">Figure 4</xref>, and <xref ref-type="fig" rid="F5">Figure 5</xref> presents the augmented images. As seen in <xref ref-type="fig" rid="F4">Figures 4</xref> and <xref ref-type="fig" rid="F5">5</xref>, the synthesized images from the GANs show a similar distribution to that of the original images. With the combination of original data and synthesized data, the DCNN is designed and trained. The number of layers of the DCNN structure is selected heuristically until the classification accuracy becomes saturated. The DCNN we designed has 6 layers including 3 convolutional layers and 3 fully connected layers. The numbers of filters in the convolutional layers are 16, 32, and 64, while the numbers of nodes in the fully connected layers are 124, 124 and 7. In the convolutional layer, batch normalization, a rectified linear unit, and max pooling are employed. The convolutional filter size is 2 × 2. We have only considered the motion classification accuracy of original data because the classification of synthesized data is meaningless even though they are used in the training process. The results reveal that the use of GANs can improve the recognition of human motion from 90% to 94% when the same DCNN structure is used.</p>
  </sec>
  <sec sec-type="discussion">
    <title>IV. Discussion</title>
    <p>This paper presented the overall process of preparing an environment for GANs and training them. In particular, we have presented an example of augmenting micro-Doppler radar data of human motion measured by Doppler radar. Owing to the augmented data set, deeper neural networks can be constructed and effectively trained, resulting in better classification accuracy. This preliminary research on the automatic recognition of human motion has the potential to contribute to diverse applications in healthcare and rehabilitation, such as human gait analysis or energy expenditure estimation.</p>
    <p>It should be noted that the current use of GANs presents challenges as it is an emerging and advancing technology. First, no standard currently exists to evaluate the quality of GANs outputs. The number of epochs should be determined by visual inspection, which can be subjective. Therefore, it is not easy to quantify the success of GANs training. Second, GANs occasionally have a mode-collapsing issue that limits the production of outputs with diverse characteristics. In addition, improperly trained GANs produce only very similar images. These issues should be addressed in the future to enable the wider use of GANs in radar image processing.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn fn-type="COI-statement">
      <p><bold>Conflict of Interest:</bold> No potential conflict of interest relevant to this article was reported.</p>
    </fn>
  </fn-group>
  <ref-list>
    <ref id="B1">
      <label>1</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Taylor</surname>
            <given-names>JL</given-names>
          </name>
          <name>
            <surname>Gotham</surname>
            <given-names>KO</given-names>
          </name>
        </person-group>
        <article-title>Cumulative life events, traumatic experiences, and psychiatric symptomatology in transition-aged youth with autism spectrum disorder</article-title>
        <source>J Neurodev Disord</source>
        <year>2016</year>
        <volume>8</volume>
        <fpage>28</fpage>
        <pub-id pub-id-type="pmid">27468315</pub-id>
      </element-citation>
    </ref>
    <ref id="B2">
      <label>2</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dongwoo</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>HC</given-names>
          </name>
        </person-group>
        <article-title>Activity energy expenditure assessment system based on activity classification using multi-site triaxial accelerometers</article-title>
        <source>Conf Proc IEEE Eng Med Biol Soc</source>
        <year>2007</year>
        <volume>2007</volume>
        <fpage>2285</fpage>
        <lpage>2287</lpage>
        <pub-id pub-id-type="pmid">18002447</pub-id>
      </element-citation>
    </ref>
    <ref id="B3">
      <label>3</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Alnujaim</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Oh</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Generative adversarial networks for classification of micro-doppler signatures of human activity</article-title>
        <source>IEEE Geosci Remote Sens Lett</source>
        <year>2019</year>
        <pub-id pub-id-type="doi">10.1109/LGRS.2019.2919770</pub-id>
        <comment>[Epub]</comment>
      </element-citation>
    </ref>
    <ref id="B4">
      <label>4</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Goodfellow</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Pouget-Abadie</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Mirza</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Warde-Farley</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Ozair</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Generative adversarial nets</article-title>
        <source>Adv Neural Inf Process Syst</source>
        <year>2014</year>
        <volume>27</volume>
        <fpage>2672</fpage>
        <lpage>2680</lpage>
      </element-citation>
    </ref>
    <ref id="B5">
      <label>5</label>
      <element-citation publication-type="book">
        <source>NVIDIA CUDA Toolkit [Internet]</source>
        <publisher-loc>Santa Clara (CA)</publisher-loc>
        <publisher-name>NVIDIA Corp.</publisher-name>
        <year>2019</year>
        <date-in-citation content-type="access-date">cited at 2019 June 18</date-in-citation>
        <comment>Available from: <ext-link ext-link-type="uri" xlink:href="https://developer.nvidia.com/cuda-toolkit">https://developer.nvidia.com/cuda-toolkit</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="B6">
      <label>6</label>
      <element-citation publication-type="book">
        <source>NVIDIA cuDNN [Internet]</source>
        <publisher-loc>Santa Clara (CA)</publisher-loc>
        <publisher-name>NVIDIA Corp.</publisher-name>
        <year>2019</year>
        <date-in-citation content-type="access-date">cited at 2019 June 18</date-in-citation>
        <comment>Available from: <ext-link ext-link-type="uri" xlink:href="https://developer.nvidia.com/cudnn">https://developer.nvidia.com/cudnn</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="B7">
      <label>7</label>
      <element-citation publication-type="book">
        <source>Anaconda Keras-GPU [Internet]</source>
        <publisher-loc>Austin (TX)</publisher-loc>
        <publisher-name>Anaconda Inc.</publisher-name>
        <year>2019</year>
        <date-in-citation content-type="access-date">cited at 2019 June 18</date-in-citation>
        <comment>Available from: <ext-link ext-link-type="uri" xlink:href="https://anaconda.org/anaconda/keras-gpu">https://anaconda.org/anaconda/keras-gpu</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="B8">
      <label>8</label>
      <element-citation publication-type="book">
        <source>Anaconda SciPy [Internet]</source>
        <publisher-loc>Austin (TX)</publisher-loc>
        <publisher-name>Anaconda Inc.</publisher-name>
        <year>2019</year>
        <date-in-citation content-type="access-date">cited at 2019 June 18</date-in-citation>
        <comment>Available from: <ext-link ext-link-type="uri" xlink:href="https://anaconda.org/anaconda/scipy">https://anaconda.org/anaconda/scipy</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="B9">
      <label>9</label>
      <element-citation publication-type="book">
        <source>Anaconda Pillow [Internet]</source>
        <publisher-loc>Austin (TX)</publisher-loc>
        <publisher-name>Anaconda Inc.</publisher-name>
        <year>2019</year>
        <date-in-citation content-type="access-date">cited at 2019 June 18</date-in-citation>
        <comment>Available from: <ext-link ext-link-type="uri" xlink:href="https://anaconda.org/conda-forge/pillow">https://anaconda.org/conda-forge/pillow</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="B10">
      <label>10</label>
      <element-citation publication-type="book">
        <source>Anaconda OpenCv [Internet]</source>
        <publisher-loc>Austin (TX)</publisher-loc>
        <publisher-name>Anaconda Inc.</publisher-name>
        <year>2019</year>
        <date-in-citation content-type="access-date">cited at 2019 June 18</date-in-citation>
        <comment>Available from: <ext-link ext-link-type="uri" xlink:href="https://anaconda.org/conda-forge/opencv">https://anaconda.org/conda-forge/opencv</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="B11">
      <label>11</label>
      <element-citation publication-type="book">
        <source>Anaconda matplotlib [Internet]</source>
        <publisher-loc>Austin (TX)</publisher-loc>
        <publisher-name>Anaconda Inc.</publisher-name>
        <year>2019</year>
        <date-in-citation content-type="access-date">cited at 2019 June 18</date-in-citation>
        <comment>Available from: <ext-link ext-link-type="uri" xlink:href="https://anaconda.org/conda-forge/matplotlib">https://anaconda.org/conda-forge/matplotlib</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="B12">
      <label>12</label>
      <element-citation publication-type="book">
        <source>Anaconda git [Internet]</source>
        <publisher-loc>Austin (TX)</publisher-loc>
        <publisher-name>Anaconda Inc.</publisher-name>
        <year>2019</year>
        <date-in-citation content-type="access-date">cited at 2019 June 18</date-in-citation>
        <comment>Available from: <ext-link ext-link-type="uri" xlink:href="https://anaconda.org/anaconda/git">https://anaconda.org/anaconda/git</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="B13">
      <label>13</label>
      <element-citation publication-type="book">
        <source>e-hir GAN Tutorial [Internet]</source>
        <publisher-loc>[place unknown]</publisher-loc>
        <publisher-name>github.com</publisher-name>
        <year>c2019</year>
        <date-in-citation content-type="access-date">cited at 2019 June 18</date-in-citation>
        <comment>Available from: <ext-link ext-link-type="uri" xlink:href="https://github.com/isn350/e_hir_GAN">https://github.com/isn350/e_hir_GAN</ext-link></comment>
      </element-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig id="F1" orientation="portrait" position="float">
    <label>Figure 1</label>
    <caption>
      <title>Setup for the seven human motion measurement using Doppler radar.</title>
    </caption>
    <graphic xlink:href="hir-25-344-g001"/>
  </fig>
  <fig id="F2" orientation="portrait" position="float">
    <label>Figure 2</label>
    <caption>
      <title>Procedure for setting up a path: (A) open Windows Explorer then choose Properties, (B) open advanced system setting, (C) setting up environment variables, and (D) select path then click edit.</title>
    </caption>
    <graphic xlink:href="hir-25-344-g002"/>
  </fig>
  <fig id="F3" orientation="portrait" position="float">
    <label>Figure 3</label>
    <caption>
      <title>Anaconda environment and training process: (A) creating and activating an Anaconda environment, (B) installing a package in Anaconda, and (C) running a training session.</title>
    </caption>
    <graphic xlink:href="hir-25-344-g003"/>
  </fig>
  <fig id="F4" orientation="portrait" position="float">
    <label>Figure 4</label>
    <caption>
      <title>Original micro-Doppler image of the following seven activities: (A) boxing while moving forward, (B) boxing while standing in place, (C) crawling, (D) running, (E) sitting still, (F) walking, and (G) walking hunched over while holding a stick.</title>
    </caption>
    <graphic xlink:href="hir-25-344-g004"/>
  </fig>
  <fig id="F5" orientation="portrait" position="float">
    <label>Figure 5</label>
    <caption>
      <title>Augmented micro-Doppler image using generative adversarial networks: (A) boxing while moving forward, (B) boxing while standing in place, (C) crawling, (D) running, (E) sitting still, (F) walking, and (G) walking hunched over while holding a stick.</title>
    </caption>
    <graphic xlink:href="hir-25-344-g005"/>
  </fig>
</floats-group>
