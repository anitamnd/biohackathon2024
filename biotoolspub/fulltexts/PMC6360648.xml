<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Genomics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Genomics</journal-id>
    <journal-title-group>
      <journal-title>BMC Genomics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2164</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6360648</article-id>
    <article-id pub-id-type="pmid">30712510</article-id>
    <article-id pub-id-type="publisher-id">5370</article-id>
    <article-id pub-id-type="doi">10.1186/s12864-018-5370-x</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Gene2vec: distributed representation of genes based on co-expression</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Du</surname>
          <given-names>Jingcheng</given-names>
        </name>
        <address>
          <email>jingcheng.du@uth.tmc.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Jia</surname>
          <given-names>Peilin</given-names>
        </name>
        <address>
          <email>peilin.jia@uth.tmc.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Dai</surname>
          <given-names>Yulin</given-names>
        </name>
        <address>
          <email>yulin.dai@uth.tmc.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Tao</surname>
          <given-names>Cui</given-names>
        </name>
        <address>
          <email>cui.tao@uth.tmc.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Zhao</surname>
          <given-names>Zhongming</given-names>
        </name>
        <address>
          <email>zhongming.zhao@uth.tmc.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes" equal-contrib="yes">
        <name>
          <surname>Zhi</surname>
          <given-names>Degui</given-names>
        </name>
        <address>
          <email>degui.zhi@uth.tmc.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9206 2401</institution-id><institution-id institution-id-type="GRID">grid.267308.8</institution-id><institution>The University of Texas School of Biomedical Informatics, </institution></institution-wrap>Houston, TX 77030 USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>4</day>
      <month>2</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>4</day>
      <month>2</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>20</volume>
    <issue>Suppl 1</issue>
    <issue-sponsor>Publication of this supplement has not been supported by sponsorship. Information about the source of funding for publication charges can be found in the individual articles. The articles have undergone the journal's standard peer review process for supplements. The Supplement Editors declare that they have no competing interests.</issue-sponsor>
    <elocation-id>82</elocation-id>
    <permissions>
      <copyright-statement>© The Author(s). 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Existing functional description of genes are categorical, discrete, and mostly through manual process. In this work, we explore the idea of gene embedding, distributed representation of genes, in the spirit of word embedding.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">From a pure data-driven fashion, we trained a 200-dimension vector representation of all human genes, using gene co-expression patterns in 984 data sets from the GEO databases. These vectors capture functional relatedness of genes in terms of recovering known pathways - the average inner product (similarity) of genes within a pathway is 1.52X greater than that of random genes. Using t-SNE, we produced a gene co-expression map that shows local concentrations of tissue specific genes. We also illustrated the usefulness of the embedded gene vectors, laden with rich information on gene co-expression patterns, in tasks such as gene-gene interaction prediction.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">We proposed a machine learning method that utilizes transcriptome-wide gene co-expression to generate a distributed representation of genes. We further demonstrated the utility of our distribution by predicting gene-gene interaction based solely on gene names. The distributed representation of genes could be useful for more bioinformatics applications.</p>
      </sec>
      <sec>
        <title>Electronic supplementary material</title>
        <p>The online version of this article (10.1186/s12864-018-5370-x) contains supplementary material, which is available to authorized users.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Distributed representation</kwd>
      <kwd>Gene2Vec</kwd>
      <kwd>Gene co-expression</kwd>
      <kwd>Embedding</kwd>
      <kwd>Word2vec</kwd>
      <kwd>Gene-gene interaction</kwd>
    </kwd-group>
    <conference xlink:href="http://icibm2018.zhaobioinfo.org/">
      <conf-name>The International Conference on Intelligent Biology and Medicine (ICIBM) 2018</conf-name>
      <conf-acronym>ICIBM 2018</conf-acronym>
      <conf-loc>Los Angeles, CA, USA</conf-loc>
      <conf-date>10-12 June 2018</conf-date>
    </conference>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par13">Genes, discrete segments of the genome that are transcribed, are basic building blocks of molecular biological systems. Although almost all transcripts in the human genome have been identified, functional annotation of genes is still a challenging task. Most existing annotation efforts organize genes into functional categories, e.g., pathways, or represent their relationship into networks. Pathways and networks crystallize biological knowledge and are convenient qualitative conceptualization of gene functions. Yet the exact functions of a gene are often more subtle and elusive to be expressed in qualitative terms.</p>
    <p id="Par14">The challenge of creating a quantitative semantic representation of discrete units of a complex system is not unique to gene systems. For a long time, creating a quantitative representation of words had been challenging for linguistic modeling. Hinton proposed the pioneering idea of ‘learning distributed representations of words’ [<xref ref-type="bibr" rid="CR1">1</xref>], i.e., representing the semantics of a word by mapping them to vectors in a high-dimension space. However, Hinton’s idea did not lead to real implementation in mainstream natural language processing (NLP) research, until recently. The word2vec model achieved success in NLP modeling [<xref ref-type="bibr" rid="CR2">2</xref>]. This process of distributed representation is often called neural embedding because the embedding function is often expressed by a neural network with a large number of parameters. This success of word2vec inspires us to investigate the possibility to represent gene functions via neural embedding.</p>
    <p id="Par15">In this study, we aim to represent genes as vectors in a high-dimension space, i.e., a gene embedding. In the word2vec model [<xref ref-type="bibr" rid="CR2">2</xref>], a word embedding is trained by maximizing the probability of word co-occurrences in context, i.e., only a few words apart in a same sentence. Analogously, we defined the context of a gene by the other genes that co-expressed with it. We derive an embedding such that the probability of the context of a gene is maximized. While it is possible to train a gene embedding using the standard NLP word embedding by using a biomedical corpus, such as the PubMed abstracts, published literature is incomplete and biased towards genes that are well-studied. Therefore, we intended to adopt a purely data-driven fashion.</p>
    <p id="Par16">Using co-expression patterns of all human genes in 984 whole transcriptome human gene expression data sets from Gene Expression Omnibus (GEO), we learned a gene embedding using a neural network. We show that our embedding grouped related genes in clusters. Moreover, we demonstrate the usefulness of the learned gene embedding to downstream tasks in the problem of prediction of gene-gene interaction.</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <sec id="Sec3">
      <title>Data collection</title>
      <sec id="Sec4">
        <title>Overview</title>
        <p id="Par17">We chose to use GEO data with rationale from both biological and technical aspects. In cellular systems, the mRNA expression levels represent activities of genes with fine resolutions. Over the past 10–20 years, GEO deposits the majority of microarray-based gene expression data in various conditions. Although the recent development of RNA-sequencing has generated transcriptomic data with advantages in both accuracy and scales than array-based data, the large cohort of GEO data provides features that are more suitable for our work. GEO data have been curated for over 10 years and hence, the measurement covers a wide range of cell and tissue types, cellular conditions, disease status, and developmental stages. As our ultimate goal is to build a gene co-expression map that could be used for inferences in various conditions, we collected GEO data for our task. In addition, we chose one single platform to reduce technical variability and required the organism to be <italic>Homo sapiens</italic>.</p>
      </sec>
      <sec id="Sec5">
        <title>Gene expression</title>
        <p id="Par18">We used the keywords “expression and human” to search in GEO on 12/24/2017 and retrieved all GSE sets that were conducted using the platform Affymetrix Human Genome U133 Plus 2.0 Array (GPL570). We required each dataset to have ≥30 samples. The downloaded gene expression intensity data were log transformed and quantile-normalized. For genes with multiple probe sets, we chose the probe set with the largest variance across all samples. Gene co-expression was measured using Pearson Correlation Coefficient (PCC) for each data set. In each data set, gene pairs with the PCC ≥ 0.9 were selected for following analysis. Selected gene pairs from all data sets were merged and serve as training data. We did not distinguish biological conditions.</p>
      </sec>
      <sec id="Sec6">
        <title>Gene types on chip</title>
        <p id="Par19">The U133 array is one of the most widely utilized platform to measure human gene expression. The chip has 54,675 probe sets for 24,442 genes. The number of probe sets per gene ranged between 1 and 15, with more than half of genes (52.08%) have one probe sets. Among these genes, 21,960 (89.85%) genes could be mapped to the current version of NCBI Entrez gene annotation. These mappable genes include 18,055 protein-coding genes, 2660 ncRNA, 730 pseudo genes, 132 snoRNA, and 383 other types of genes. Particularly for ncRNAs, there are 202 microRNA genes. Gene set enrichment analysis was conducted using Fisher’s Exact Test.</p>
      </sec>
      <sec id="Sec7">
        <title>Gene-gene interaction dataset</title>
        <p id="Par20">We followed previous work [<xref ref-type="bibr" rid="CR3">3</xref>] to build datasets for gene-gene interaction based on shared Gene Ontology (GO) annotations. GO annotation was obtained using the R (× 64 3.4.3) package “org.Hs.eg.db” (version 3.5.0). GO structure file in the obo format was downloaded from [<xref ref-type="bibr" rid="CR4">4</xref>]. All genes were mapped to NCBI Entrez Gene [<xref ref-type="bibr" rid="CR5">5</xref>] (downloaded on 11/6/2017). We defined gene pairs that shared GO annotations as the positive set of functional association. To this end, we chose the GO category “Biological Process” with experimental evidence: IDA (inferred from direct assay), IMP (inferred from mutant phenotype), IPI (inferred from protein interaction), IGI (inferred from genetic interaction), and TAS (traceable author statement). To minimize generalized annotation, we excluded the highly over-represented GO terms including (1) “signal transduction” (GO:0007165); (2) three phosphorylation terms: “protein amino acid phosphorylation” (GO:0006468), “protein amino acid autophosphorylation” (GO:0046777), and “protein amino acid dephosphorylation” (GO:0006470); and (3) all terms at the first three levels of GO hierarchy (assuming the root term of biological process, “GO:0008150”, is level 0). This lead to a total of 270,704 pairs involving 5369 genes. To build the negative data set, we obtained all gene-pairs that did not share any GO term or their children GO terms. This resulted a total of 40,879,714 gene pairs involved in 12,521 (64.85% of 19,307) human genes, serving as the set in which pairs of genes are not functionally associated.</p>
      </sec>
      <sec id="Sec8">
        <title>Tissue-specific genes</title>
        <p id="Par21">GTEx data (version 6) [<xref ref-type="bibr" rid="CR6">6</xref>] was used to estimate the tissue-specific expression pattern of genes in 27 tissues, each with ≥30 samples. For each gene, a z-score was calculated to measure its tissue specificity by comparing the average gene expression of the gene across all tissues [<xref ref-type="bibr" rid="CR7">7</xref>].</p>
      </sec>
      <sec id="Sec9">
        <title>Functional gene sets</title>
        <p id="Par22">We use clusteredness of MSigDB pathways (v5.1) [<xref ref-type="bibr" rid="CR8">8</xref>] as the target function for hyper-parameter tuning for gene embedding training. Specifically, we used the category c2 including curated pathways from various online resources such as KEGG [<xref ref-type="bibr" rid="CR9">9</xref>], Biocarta [<xref ref-type="bibr" rid="CR10">10</xref>], and Reactome [<xref ref-type="bibr" rid="CR11">11</xref>]. A total of 4726 pathways were downloaded.</p>
      </sec>
    </sec>
    <sec id="Sec10">
      <title>Concept embedding of genes</title>
      <p id="Par23">Distributed representation of word, or neural word embedding, was a recent breakthrough in NLP research based on deep learning. The goal of word embedding is to derive a linear mapping, i.e., embedding, from the discrete space of individual words to a continuous Euclidean space such that similar words will be mapped to points in close vicinity in the embedding space. The direct benefit of word embedding is that such representation of individual words, vectors in continuous space, becomes differentiable and thus amenable for back-propagation-based neural network modeling. Meanwhile, a nice surprising result is that embedded space admits basic geometry. E.g., the KING - QUEEN ≈ MAN - WOMAN.</p>
      <p id="Par24">Inspired by the success of word embedding, we intend to produce an embedding of genes, also a discrete conceptual unit, such that similar genes are mapped to similar vectors. While for genes we do not have a natural equivalent concept of sentence in natural languages, we will use the notion of co-expression. This is analogy of the concept of co-occurrence in natural languages.</p>
      <p id="Par25">For neural embedding, a neural network is designed that maximizes an objective function, often in a form of likelihood, such as the probability of a word given its context. The most commonly used architectures are skip-gram and continuous bag-of-words (CBOW) that discussed in the word2vec approach [<xref ref-type="bibr" rid="CR2">2</xref>]. In both architectures, a two-layer neural network is constructed to predict word co-occurrence, or the co-occurrence of a word and its surrounding words, or context. In CBOW, the input is the context and the output is the word; in skip-gram, the input is the word and the output is the context. For both architectures, input and output are connected through a middle projection layer. Note that neither neural network would offer satisfactory predictions for most of the words. But the real goal of word embedding is to learn a distributional representation, i.e., the parameters of the embedding mapping from the input to the middle projection layer. A simple fully connected linear layer was used for the embedding mapping. For CBOW, the embedded vectors of all words in the context are averaged and thus provide a uniform size vector for the next layer. The second layer for both architecture is a linear layer with a soft-max. A cross-entropy loss is minimized.</p>
      <p id="Par26">In gene embedding, we are using the genes who are co-expressed with the gene of interest as its context. Since the number of co-expressed genes may vary, the size of the context may vary as well. For simplicity, in this work, we extract all pairs of co-expressed genes and maximize the probability of one given the other for each pair. This is equivalent to the skip-gram model. Since we are optimizing the total probability of all edges in a co-expression network, our approach can also be viewed as a graph embedding [<xref ref-type="bibr" rid="CR12">12</xref>].</p>
      <p id="Par27">More formally, the input of the training problem is a list of gene pairs that are highly co-expressed, <italic>T</italic> = {(<italic>g</italic><sub><italic>i</italic>1</sub>, <italic>g</italic><sub><italic>i</italic>2</sub>)}, we will train an embedding network. The input of the network is a one-hot encoded vector for gene <italic>g</italic><sub><italic>i</italic></sub> ∈ <italic>R</italic><sup><italic>d</italic></sup>, where <italic>d</italic> is the number of genes and the elements of <italic>g</italic><sub><italic>i</italic></sub> are all 0 expert <italic>g</italic><sub><italic>i</italic></sub>[<italic>x</italic><sub><italic>i</italic></sub>] = 1, where <italic>x</italic><sub><italic>i</italic></sub> is the dimension corresponds to the gene <italic>g</italic><sub><italic>i</italic></sub>. The output of the network is a vector of dimension <italic>v</italic><sub><italic>i</italic></sub> ∈ <italic>R</italic><sup><italic>k</italic></sup>, the embedding dimension. The parameter of the network is a matrix <italic>W</italic> ∈ <italic>R</italic><sup><italic>d</italic> × <italic>k</italic></sup> such that <italic>v</italic><sub><italic>i</italic></sub> = <italic>Wg</italic><sub><italic>i</italic></sub>. If we define the probability<disp-formula id="Equa"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mathit{\Pr}\left({g}_i|{g}_j\right)=\frac{\mathit{\exp}\left({v}_i^T{v}_j\right)}{\sum_{j^{\prime }}\mathit{\exp}\left({v}_i^T{v}_{j^{,}}\right)} $$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mo mathvariant="italic">Pr</mml:mo><mml:mfenced close=")" open="(" separators="|"><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo mathvariant="italic">exp</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:msup><mml:mi>j</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:msub><mml:mo mathvariant="italic">exp</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>v</mml:mi><mml:msup><mml:mi>j</mml:mi><mml:mo>,</mml:mo></mml:msup></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="12864_2018_5370_Article_Equa.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par28">The loss function that is to be minimized is the negative likelihood <inline-formula id="IEq1"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ -{\sum}_{\left({g}_{i1},{g}_{i2}\right)\in T}\mathit{\Pr}\left({g}_i|{g}_j\right) $$\end{document}</tex-math><mml:math id="M4" display="inline"><mml:mo>−</mml:mo><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mfenced close=")" open="(" separators=","><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mfenced><mml:mo>∈</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo mathvariant="italic">Pr</mml:mo><mml:mfenced close=")" open="(" separators="|"><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mfenced></mml:math><inline-graphic xlink:href="12864_2018_5370_Article_IEq1.gif"/></alternatives></inline-formula>. It can be shown that this complex loss function for this single layer network is equivalent to a two-layer network with shared weight matrices of <italic>W</italic>and <italic>W</italic><sup><italic>T</italic></sup>, and the loss function as the standard cross-entropy after softmax (see, Fig. <xref rid="Fig1" ref-type="fig">1</xref>).<fig id="Fig1"><label>Fig. 1</label><caption><p>The Skip-Gram architecture was used for training for gene embedding. This is the modified architecture which is equivalent to the original word2vec, adopted from this blog [<xref ref-type="bibr" rid="CR22">22</xref>]</p></caption><graphic xlink:href="12864_2018_5370_Fig1_HTML" id="MO1"/></fig></p>
    </sec>
    <sec id="Sec11">
      <title>Training of embedding</title>
      <p id="Par29">We took all the gene pairs that have a PCC equal to or larger than 0.9 as the input. This is a choice due to limited computational resources. We shuffled the gene pairs in each dataset on every iteration to avoid the impact caused by the order of gene pairs in the datasets. The embedding was trained on all genes with a minimum frequency at 5. As number of iterations and dimensionality of the embedding are considered as two major hyper-parameters parameters for word embedding [<xref ref-type="bibr" rid="CR13">13</xref>], in order to generate “best” gene embedding, we did a preliminary parameters tuning and performed a grid search to find best parameters. The search ranges for number of iterations and embedding dimension are set at 1 to 10 and 50, 100, 200, and 300 respectively. We used the word2vec function implemented in the gensim library [<xref ref-type="bibr" rid="CR14">14</xref>] to generate gene embedding. Other parameters were set as default.</p>
      <p id="Par30">Since our goal is to obtain a gene embedding that reflects the functional relationships among genes, we selected the set of hyper-parameters that maximizes the clusteredness of genes within functional pathways. We optimized the following target function:<disp-formula id="Equb"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mathrm{c} lusteredness=\frac{\frac{1}{\left|Q\right|}{\sum}_{P\in Q}\frac{1}{\# gene\ pairs\ in\ P}{\sum}_{g_i,{g}_j\in P}\left({v}_i^T{v}_j\right)}{\frac{1}{\# gene\ pairs\ in\ {Q}^{\prime }}{\sum}_{g_i,{g}_j\in {Q}^{\prime }}\left({v}_i^T{v}_j\right)}, $$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mi mathvariant="normal">c</mml:mi><mml:mtext mathvariant="italic">lusteredness</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mfenced close="|" open="|"><mml:mi>Q</mml:mi></mml:mfenced></mml:mfrac><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>∈</mml:mo><mml:mi>Q</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>#</mml:mo><mml:mtext mathvariant="italic">gene pairs in</mml:mtext><mml:mspace width="0.25em"/><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="("><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>#</mml:mo><mml:mtext mathvariant="italic">gene pairs in</mml:mtext><mml:mspace width="0.25em"/><mml:msup><mml:mi>Q</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mfrac><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>Q</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mfenced close=")" open="("><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="12864_2018_5370_Article_Equb.gif" position="anchor"/></alternatives></disp-formula>where <italic>Q</italic> is the set of pathways in MSigDB, and <italic>Q</italic><sup>,</sup> is a set of random gene pairs. Due to the limitation of computation power, we selected all the pathways from the MSigDB with the number of genes equal or fewer than 50. In total, 6729 pathways were selected as <italic>Q</italic>. We randomly selected 1000 genes from gene embedding and generated all possible unique gene pairs (499,500 in total) as <italic>Q</italic><sup>′</sup>.</p>
    </sec>
    <sec id="Sec12">
      <title>Visualization by t-SNE</title>
      <p id="Par31">A common way to visualize high-dimensional datasets is to map the datasets into 2D or 3D array. <italic>t</italic>-Distributed Stochastic Neighbor Embedding (t-SNE) is a machine learning algorithm for dimensionality reduction, which optimizes for neighborhood preserving and thus particularly well suited for the visualization of high-dimensional datasets [<xref ref-type="bibr" rid="CR15">15</xref>]. Visualizations produced by t-SNE have been found significantly better than those produced by the other techniques [<xref ref-type="bibr" rid="CR15">15</xref>].</p>
      <p id="Par32">In order to speed up the t-SNE on the high-dimensional gene embedding, we first reduced the dimension to 50 using principal component analysis (PCA) and then applied a multicore modification of Barnes-Hut t-SNE by L. Van der Maaten [<xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR17">17</xref>]. The perplexity was set at 30 and the learning rate was set at 200. To get stable t-SNE results, we set the number of iterations at 100,000.</p>
    </sec>
    <sec id="Sec13">
      <title>Prediction of gene-gene interaction</title>
      <p id="Par33">To investigate the usefulness of the trained gene embedding for downstream tasks, we applied the embedding to the problem of gene-gene interaction prediction. The goal is, given a pair of genes, we design a gene-gene interaction predictor neural network (GGIPNN) to predict if they will be together in any of the annotated pathway [<xref ref-type="bibr" rid="CR3">3</xref>].</p>
      <p id="Par34">The architecture of GGIPNN can be seen in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. We first convert the genes in each gene pair to one-hot vectors and then map the one-hot vectors to gene embedding vectors using a shared embedding matrix. Then, the two gene embedding vectors will be concatenated together and be fed to a fully connected layer with a dimension at 100. The output will be fed to another fully connected layer with a dimension at 100. The output of the second fully connected layer will then be fed to a second (third) fully connected layer with a dimension at 10. The output will be then fed to a softmax (same as sigmoid as this is a binary classification) layer. We compute the cross entropy of the softmax function output and then compute the mean of elements across results as the loss. We choose ReLU (Rectified Linear Units) as the activation function. To avoid overfitting, we apply dropout on both the first and second fully connected layers. The dropout out rates are set at 0.5.<fig id="Fig2"><label>Fig. 2</label><caption><p>The architecture of gene-gene interaction predictor neural network (GGIPNN)</p></caption><graphic xlink:href="12864_2018_5370_Fig2_HTML" id="MO2"/></fig></p>
      <p id="Par35">Area Under Curve (AUC) is computed to measure the performance of the prediction. We compared the AUC score of our pre-trained gene embedding and embedding which is randomly initialized. We also investigated the impact of trainable embedding layer (fine-tuning during the training) versus non-trainable embedding layer (fixed during the training) on the prediction. This model was implemented in TensorFlow.</p>
      <p id="Par36">We took all the pairs from HumanNet.v1.benchmark [<xref ref-type="bibr" rid="CR3">3</xref>] as the positive pairs. This led to a total of 270,704 pairs involving 5369 genes. To build the negative data set, we obtained all gene-pairs that did not share any GO term or their children GO terms. This resulted in a total of 40,879,714 gene-pairs involved in 12,521 (64.85% of 19,307) human genes, serving as the set in which pairs of genes are not functionally associated. To avoid the impact of the imbalanced labels distribution, we randomly selected negative pairs with the equal number of the positive pairs. We then split all the unique genes into training, validation and testing sets with an proportion of 7: 1: 2. The pairs that the both two genes belong to training set are used as training; the pairs that the both two genes belong to validation set are used as validation; the pairs that the both two genes belong to testing set are used as testing. By doing so, we avoid the possibility that the neural network “memorizes” the likelihood of genes to be interacting with any other genes. In total, the training dataset has 263,016 pairs (involving 8832 genes), while the validation and testing dataset have 5568 pairs (1173 genes) and 21,448 pairs (2467 genes) respectively.</p>
    </sec>
  </sec>
  <sec id="Sec14">
    <title>Results</title>
    <sec id="Sec15">
      <title>Parameter tuning results by clusteredness</title>
      <p id="Par37">The parameter tuning results can be seen in Table <xref rid="Tab1" ref-type="table">1</xref>. As we can observe, the dimension of 200 at iteration 9 produced best gene embedding using clusteredness as the target function (1.521). As result, we chose this embedding for all following analyses.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Hyperparameter tuning using clusteredness as target function</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Dimension</th><th colspan="10">Number of Iterations</th></tr><tr><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th></tr></thead><tbody><tr><td>50</td><td>1.428</td><td>1.444</td><td>1.467</td><td>1.470</td><td><bold>1.487</bold></td><td>1.465</td><td>1.473</td><td>1.479</td><td>1.475</td><td>1.462</td></tr><tr><td>100</td><td>1.415</td><td>1.467</td><td>1.488</td><td>1.491</td><td>1.498</td><td>1.501</td><td><bold>1.519</bold></td><td>1.486</td><td>1.480</td><td>1.490</td></tr><tr><td>200</td><td>1.403</td><td>1.463</td><td>1.491</td><td>1.498</td><td>1.495</td><td>1.482</td><td>1.470</td><td>1.488</td><td><bold>1.521</bold></td><td>1.509</td></tr><tr><td>300</td><td>1.392</td><td>1.443</td><td>1.472</td><td>1.473</td><td>1.473</td><td>1.509</td><td>1.474</td><td><bold>1.513</bold></td><td>1.479</td><td>1.480</td></tr></tbody></table><table-wrap-foot><p>Bold number denotes the largest number in that row</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec16">
      <title>Gene embedding groups similar genes into spatial clusters</title>
      <p id="Par38">Using the first and second components from the t-SNE representation, we produced a gene co-expression map, based on which we explored the distribution of all human genes from our results (Fig. <xref rid="Fig3" ref-type="fig">3</xref>). A direct visualization of the gene distribution revealed that the majority of genes formed one single cloud while several isolated groups of genes scattered around. We extracted these gene islands and found they were mainly non-protein-coding genes. Island 2 was significantly populated with the snoRNA genes (pink dots, <italic>p</italic> = 1.07 × 10<sup>− 72</sup>, Fisher’s Exact Test). Island 4, located to the very right of the plot, mainly contains human cDNA/PAC clone genes. microRNA genes (cyan dots) were mainly distributed in island 2 (<italic>p</italic> = 3.99 × 10<sup>− 19</sup>), island 4 (<italic>p</italic> = 3.51 × 10<sup>− 73</sup>), and island 5 (<italic>p</italic> = 2.64 × 10<sup>− 41</sup>). A group of ncRNAs which start with “LOC” and are often uncharacterized split the whole distribution into the left panel and the right panel (red dots, Fig. <xref rid="Fig3" ref-type="fig">3</xref>). In the left panel, we observed a cluster of open reading frames (yellow dots, Fig. <xref rid="Fig3" ref-type="fig">3</xref>) in the human genome.<fig id="Fig3"><label>Fig. 3</label><caption><p>Gene co-expression map generated from embedding reveals clusters of functionally related genes. F1 and F2 are the first and the second dimensions of t-SNE. Red: LOC non-coding genes; cyan: microRNA; pink: small nucleolar RNA (snoRNA); yellow: undercharacterized ORFs</p></caption><graphic xlink:href="12864_2018_5370_Fig3_HTML" id="MO3"/></fig></p>
    </sec>
    <sec id="Sec17">
      <title>Tissue specific genes form spatial patterns in gene embedding</title>
      <p id="Par39">We mapped genes with z-scores representing their tissue-specific expression onto the gene co-expression map. We observed clear clusters in several tissues such as blood, skin, spleen, and lung (Fig. <xref rid="Fig4" ref-type="fig">4</xref> and Additional file <xref rid="MOESM1" ref-type="media">1</xref>). Genes with high tissue specificity in blood highlighted two distant clusters. This is likely because that blood samples are relatively more widely used in gene expression studies and blood-specific genes and their relationships are thus better represented in our map. Tissues that are biologically relevant showed similar patterns. For example, tissues of female reproductive systems presented graded and similar patterns, including breast, ovary, and uterus. In these tissues, genes located in the bottom part of the map in general showed increased tissue specificity, compared to genes located on the top part of the map (Fig. <xref rid="Fig4" ref-type="fig">4</xref> and Additional file <xref rid="MOESM1" ref-type="media">1</xref>). Interestingly, we found a group of ribosomal genes (~ 50) that were highly expressed in ovary and formed a small cluster in our map. In addition, cognition and neurology related tissues, such as brain (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figure S5), nerve (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figure S14), and pituitary (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figure S17), presented quite diverse patterns. Nerve and pituitary are more similar to each other, with a wide range of genes showing moderate tissue-specificity distributed across the whole map. In contrast, active genes in brain, which are mainly distributed on the top part of the map, are much smaller in numbers but showed much stronger tissue-specificity (red dots, Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figure S5). Notably, all tissues except the blood are expected to be under-represented in the GEO data we used because tissue samples are difficult to obtain for human.<fig id="Fig4"><label>Fig. 4</label><caption><p>Embedding reveals clusters of genes with tissue-specificity. Blood and spleen have clear patterns of tissue-specific genes. Reproductive system (e.g., ovary) also showed distinguished genes. Genes not available in GTEx data were colored grey</p></caption><graphic xlink:href="12864_2018_5370_Fig4_HTML" id="MO4"/></fig></p>
    </sec>
    <sec id="Sec18">
      <title>Prediction of gene-gene interaction using embedded vectors</title>
      <p id="Par40">The performances of GGIPNN with embedding matrix are presented in Fig. <xref rid="Fig5" ref-type="fig">5</xref>. Using gene embedding matrix derived from GEO but do not make them trainable, we achieved an AUC of 0.720 over the test set, in which there are no gene overlapping with the training set nor the validation set. The AUC score is lower, 0.664, for the GGIPNN with gene embedding matrix derived from GEO as initial weights but trainable. This is understandable as the gene embedding matrix for the genes in the training set was updated and leaving the gene embedding matrix in the test set “out of sync” with that for the training set, i.e., overfitting. As expected, the GGIPNN with both untrainable and trainable random embedding matrix have AUC scores (0.505 and 0.493) close to random (0.5).<fig id="Fig5"><label>Fig. 5</label><caption><p>ROC curves for gene-gene interaction predictor neural networks</p></caption><graphic xlink:href="12864_2018_5370_Fig5_HTML" id="MO5"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec19">
    <title>Discussion</title>
    <p id="Par41">In this work, we explored the idea of distributed representation of genes using their co-expression. Purely trained from their co-expression patterns in GEO, except using MSigDB as hyper-parameter tuning, the trained embedding matrix captures functional relationships among genes. In the t-SNE generated gene co-expression map of the embedding matrix, tight clusters of non-coding genes are formed, while broader clusters corresponding to tissue specific genes are also visible.</p>
    <p id="Par42">The usefulness of gene embedding is beyond simply a nice visualization. Using the gene embedding as the basic layer for a multi-layer neural network, we can predict the gene-gene interaction with an AUC of 0.720. This is an intriguing result because the only input to the predictor is the names of the two genes. Therefore, the distributed representation of the genes, i.e., their embeddings, are laden with rich semantic information about their function.</p>
    <p id="Par43">The concept of concept embedding is not new to molecular biology. Works had been done to geometrical embedding gene co-expression networks into 2-D planar networks [<xref ref-type="bibr" rid="CR18">18</xref>]. Recently, in the spirit of embedding everything, the work of bioVectors have been developed to embedding kmers in biological sequences into distributed representation [<xref ref-type="bibr" rid="CR19">19</xref>]. Yang et al. leveraged the Doc2vec model to learn embedded representations of protein sequences [<xref ref-type="bibr" rid="CR20">20</xref>]. Similarly, a project named ‘Gene2vec’ is available embedding gene sequences [<xref ref-type="bibr" rid="CR21">21</xref>]. However, to the best of our knowledge, our work is the first to directly embed genes into distributed representations based on their natural context - their expression and co-expression.</p>
    <p id="Par44">In this work, we are using the gene co-expression as the definition of “context” for gene embedding. However, it is possible to extend the current work to include other definitions of context for genes. For example, co-occurrence of genes across species, gene-gene and protein-protein interactions from experiments, and co-occurrences of genes in literature, all can be a source of information to define context.</p>
    <p id="Par45">The distributed representation of genes can enable new applications. E.g., as illustrated in the Results, with continuous representation of genes, it is possible to direct feed gene as inputs to neural networks, and can be useful for any prediction tasks with gene names as input.</p>
    <p id="Par46">A limitation of current approach is the lack of higher order semantics. In word embedding a surprising result was that the direction of the embedding space can be interpreted. For example, the vector representations of the words King, Queen, Man, and Woman formed a parallelogon. This higher order of semantics from NLP modeling may be due to that the concepts between these words were connected by certain relationships, which is reflected by the occurrences of these words being connected by certain verbs. To achieve this level of semantic embedding of genes, future works modeling more information about genes are warranted.</p>
  </sec>
  <sec id="Sec20">
    <title>Conclusions</title>
    <p id="Par47">We proposed a machine learning method that utilizes transcriptome-wide gene co-expression to generate a distributed representation of genes. We further demonstrated the utility of our distribution by predicting gene-gene interaction based solely on gene names. We believe that this distributed representation of genes could be useful for more bioinformatics applications.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Additional file</title>
    <sec id="Sec21">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="12864_2018_5370_MOESM1_ESM.docx">
            <label>Additional file 1:</label>
            <caption>
              <p>Supplementary figures (Figure S1 to Figure S27). (DOCX 13027 kb)</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>AUC</term>
        <def>
          <p id="Par4">Area Under Curve</p>
        </def>
      </def-item>
      <def-item>
        <term>CBOW</term>
        <def>
          <p id="Par5">Continuous bag-of-words</p>
        </def>
      </def-item>
      <def-item>
        <term>GEO</term>
        <def>
          <p id="Par6">Gene Expression Omnibus</p>
        </def>
      </def-item>
      <def-item>
        <term>GGIPNN</term>
        <def>
          <p id="Par7">Gene-gene interaction predictor neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>NLP</term>
        <def>
          <p id="Par8">Natural language processing</p>
        </def>
      </def-item>
      <def-item>
        <term>PCA</term>
        <def>
          <p id="Par9">Principal component analysis</p>
        </def>
      </def-item>
      <def-item>
        <term>PCC</term>
        <def>
          <p id="Par10">Pearson Correlation Coefficient</p>
        </def>
      </def-item>
      <def-item>
        <term>ReLU</term>
        <def>
          <p id="Par11">Rectified Linear Units</p>
        </def>
      </def-item>
      <def-item>
        <term>t-SNE</term>
        <def>
          <p id="Par12">t-Distributed Stochastic Neighbor Embedding</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <ack>
    <p>The content is solely the responsibility of the authors and does not necessarily represent the official views of the Cancer Prevention and Research Institute of Texas. We thank the anonymous reviewers for their careful reading of our manuscript and their many insightful comments.</p>
    <sec id="FPar1">
      <title>Funding</title>
      <p id="Par48">Research was partially supported by the Cancer Prevention Research Institute of Texas (CPRIT) Training Grant #RP160015. This article has not received sponsorship for publication.</p>
    </sec>
    <sec id="FPar2" sec-type="data-availability">
      <title>Availability of data and materials</title>
      <p id="Par49">The codes for gene2vec training, visualization, and gene-gene interaction prediction are available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/jingcheng-du/Gene2vec">https://github.com/jingcheng-du/Gene2vec</ext-link>). We also released our pre-trained gene2vec on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/jingcheng-du/Gene2vec/blob/master/pre_trained_emb/gene2vec_dim_200_iter_9.txt">https://github.com/jingcheng-du/Gene2vec/blob/master/pre_trained_emb/gene2vec_dim_200_iter_9.txt</ext-link>).</p>
    </sec>
    <sec id="FPar3">
      <title>About this supplement</title>
      <p id="Par50">This article has been published as part of <italic>BMC Genomics Volume 20 Supplement 1, 2019: Selected articles from the International Conference on Intelligent Biology and Medicine (ICIBM) 2018: genomics.</italic> The full contents of the supplement are available online at <ext-link ext-link-type="uri" xlink:href="https://bmcgenomics.biomedcentral.com/articles/supplements/volume-20-supplement-1">https://bmcgenomics.biomedcentral.com/articles/supplements/volume-20-supplement-1</ext-link>.</p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>JD, PJ and DZ designed the study, performed the experiments and drafted the manuscript. YD, and CT assisted to the study design. ZZ and DZ supervised the study. All of the authors have read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="COI-statement">
    <sec id="FPar4">
      <title>Ethics approval and consent to participate</title>
      <p>Not applicable</p>
    </sec>
    <sec id="FPar5">
      <title>Consent for publication</title>
      <p>Not applicable</p>
    </sec>
    <sec id="FPar6">
      <title>Competing interests</title>
      <p>The authors declare that they have no competing interests.</p>
    </sec>
    <sec id="FPar7">
      <title>Publisher’s Note</title>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </sec>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hinton</surname>
            <given-names>GE</given-names>
          </name>
        </person-group>
        <article-title>Learning distributed representations of concepts</article-title>
        <source>Proc Eighth Annu Conf Cogn Sci Soc</source>
        <year>1986</year>
        <volume>1</volume>
        <fpage>12</fpage>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Mikolov</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Corrado</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Dean</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <source>Efficient estimation of word representations in vector space. arXiv Prepr. arXiv1301.3781</source>
        <year>2013</year>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Blom</surname>
            <given-names>UM</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>PI</given-names>
          </name>
          <name>
            <surname>Shim</surname>
            <given-names>JE</given-names>
          </name>
          <name>
            <surname>Marcotte</surname>
            <given-names>EM</given-names>
          </name>
        </person-group>
        <article-title>Prioritizing candidate disease genes by network-based boosting of genome-wide association data</article-title>
        <source>Genome Res</source>
        <year>2011</year>
        <volume>21</volume>
        <fpage>1109</fpage>
        <lpage>1121</lpage>
        <pub-id pub-id-type="doi">10.1101/gr.118992.110</pub-id>
        <pub-id pub-id-type="pmid">21536720</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <mixed-citation publication-type="other">Gene Ontology [Internet]. Available from: <ext-link ext-link-type="uri" xlink:href="http://geneontology.org/ontology/go.obo">http://geneontology.org/ontology/go.obo</ext-link>. [cited 2018 Feb 14]</mixed-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Maglott</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Ostell</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Pruitt</surname>
            <given-names>KD</given-names>
          </name>
          <name>
            <surname>Tatusova</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Entrez gene: gene-centered information at NCBI</article-title>
        <source>Nucleic Acids Res</source>
        <year>2005</year>
        <volume>33</volume>
        <fpage>D54</fpage>
        <lpage>D58</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gki031</pub-id>
        <pub-id pub-id-type="pmid">15608257</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lonsdale</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Thomas</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Salvatore</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Phillips</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lo</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Shad</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The genotype-tissue expression (GTEx) project</article-title>
        <source>Nat Genet</source>
        <year>2013</year>
        <volume>45</volume>
        <fpage>580</fpage>
        <pub-id pub-id-type="doi">10.1038/ng.2653</pub-id>
        <pub-id pub-id-type="pmid">23715323</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Jia</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Cox</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Denny</surname>
            <given-names>JC</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>An integrative functional genomics framework for effective identification of novel regulatory variants in genome--phenome studies</article-title>
        <source>Genome Med</source>
        <year>2018</year>
        <volume>10</volume>
        <fpage>7</fpage>
        <pub-id pub-id-type="doi">10.1186/s13073-018-0513-x</pub-id>
        <pub-id pub-id-type="pmid">29378629</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Subramanian</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Tamayo</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Mootha</surname>
            <given-names>VK</given-names>
          </name>
          <name>
            <surname>Mukherjee</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ebert</surname>
            <given-names>BL</given-names>
          </name>
          <name>
            <surname>Gillette</surname>
            <given-names>MA</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles</article-title>
        <source>Proc Natl Acad Sci</source>
        <year>2005</year>
        <volume>102</volume>
        <fpage>15545</fpage>
        <lpage>15550</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.0506580102</pub-id>
        <pub-id pub-id-type="pmid">16199517</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kanehisa</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Goto</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>KEGG: Kyoto encyclopedia of genes and genomes</article-title>
        <source>Nucleic Acids Res</source>
        <year>2000</year>
        <volume>28</volume>
        <fpage>27</fpage>
        <lpage>30</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/28.1.27</pub-id>
        <pub-id pub-id-type="pmid">10592173</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">BioCarta Pathways [Internet]. Available from: <ext-link ext-link-type="uri" xlink:href="http://cgap.nci.nih.gov/Pathways/BioCarta_Pathways">http://cgap.nci.nih.gov/Pathways/BioCarta_Pathways</ext-link>. [cited 2018 Feb 14]</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Reactome [Internet]. Available from: <ext-link ext-link-type="uri" xlink:href="https://reactome.org/">https://reactome.org/</ext-link>. [cited 2018 Feb 14]</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Cai H, Zheng VW, Chang K. A comprehensive survey of graph embedding: problems, techniques and applications. IEEE Trans Knowl Data Eng. 2018;30(9):1616–37.</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lai</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>How to generate a good word embedding</article-title>
        <source>IEEE Intell Syst</source>
        <year>2016</year>
        <volume>31</volume>
        <fpage>5</fpage>
        <lpage>14</lpage>
        <pub-id pub-id-type="doi">10.1109/MIS.2016.45</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Rehurek</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Sojka</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <source>Software framework for topic modelling with large corpora. Proc. Lr. 2010 Work. New Challenges NLP Fram</source>
        <year>2010</year>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>van der Maaten</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Visualizing data using t-SNE</article-title>
        <source>J Mach Learn Res</source>
        <year>2008</year>
        <volume>9</volume>
        <fpage>2579</fpage>
        <lpage>2605</lpage>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Ulyanov</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <source>Multicore-TSNE. GitHub Repos. GitHub</source>
        <year>2016</year>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Van Der Maaten</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Accelerating t-SNE using tree-based algorithms</article-title>
        <source>J Mach Learn Res</source>
        <year>2014</year>
        <volume>15</volume>
        <fpage>3221</fpage>
        <lpage>3245</lpage>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Song</surname>
            <given-names>W-M</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Multiscale embedded gene co-expression network analysis</article-title>
        <source>PLoS Comput Biol</source>
        <year>2015</year>
        <volume>11</volume>
        <fpage>e1004574</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1004574</pub-id>
        <pub-id pub-id-type="pmid">26618778</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Asgari</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Mofrad</surname>
            <given-names>MRK</given-names>
          </name>
        </person-group>
        <article-title>Continuous distributed representation of biological sequences for deep proteomics and genomics</article-title>
        <source>PLoS One</source>
        <year>2015</year>
        <volume>10</volume>
        <fpage>e0141287</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0141287</pub-id>
        <pub-id pub-id-type="pmid">26555596</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>KK</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Bedbrook</surname>
            <given-names>CN</given-names>
          </name>
          <name>
            <surname>Arnold</surname>
            <given-names>FH</given-names>
          </name>
        </person-group>
        <article-title>Learned protein embeddings for machine learning</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>1</volume>
        <fpage>7</fpage>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Vector space representation of genetic data [Internet]. Available from: <ext-link ext-link-type="uri" xlink:href="https://github.com/davidcox143/Gene2vec">https://github.com/davidcox143/Gene2vec</ext-link>. [cited 2018 Feb 14]</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Word2Vec Tutorial - The Skip-Gram Model [Internet]. Available from: <ext-link ext-link-type="uri" xlink:href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model">http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model</ext-link>. [cited 2018 Feb 14]</mixed-citation>
    </ref>
  </ref-list>
</back>
