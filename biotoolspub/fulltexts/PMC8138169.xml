<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Genet</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Genet</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Genet.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Genetics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1664-8021</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8138169</article-id>
    <article-id pub-id-type="doi">10.3389/fgene.2021.655287</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Genetics</subject>
        <subj-group>
          <subject>Original Research</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>TrainSel: An R Package for Selection of Training Populations</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Akdemir</surname>
          <given-names>Deniz</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="corresp" rid="c001">
          <sup>*</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/376578/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Rio</surname>
          <given-names>Simon</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/1231523/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Isidro y Sánchez</surname>
          <given-names>Julio</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <xref ref-type="corresp" rid="c002">
          <sup>*</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/377092/overview"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Agriculture &amp; Food Science Centre, Animal and Crop Science Division, University College Dublin</institution>, <addr-line>Dublin</addr-line>, <country>Ireland</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Centro de Biotecnologia y Genómica de Plantas (CBGP, UPM-INIA), Instituto Nacional de Investigación y Tecnologia Agraria y Alimentaria (INIA), Universidad Politécnica de Madrid (UPM)</institution>, <addr-line>Madrid</addr-line>, <country>Spain</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Diego Jarquin, University of Nebraska-Lincoln, United States</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Roberto Fritsche-Neto, International Rice Research Institute (IRRI), Philippines; Luc L. Janss, Aarhus University, Denmark</p>
      </fn>
      <corresp id="c001">*Correspondence: Deniz Akdemir <email>deniz.akdemir.work@gmail.com</email></corresp>
      <corresp id="c002">Julio Isidro y Sánchez <email>j.isidro@upm.es</email></corresp>
      <fn fn-type="other" id="fn001">
        <p>This article was submitted to Statistical Genetics and Methodology, a section of the journal Frontiers in Genetics</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>07</day>
      <month>5</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>12</volume>
    <elocation-id>655287</elocation-id>
    <history>
      <date date-type="received">
        <day>18</day>
        <month>1</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>31</day>
        <month>3</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2021 Akdemir, Rio and Isidro y Sánchez.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Akdemir, Rio and Isidro y Sánchez</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>A major barrier to the wider use of supervised learning in emerging applications, such as genomic selection, is the lack of sufficient and representative labeled data to train prediction models. The amount and quality of labeled training data in many applications is usually limited and therefore careful selection of the training examples to be labeled can be useful for improving the accuracies in predictive learning tasks. In this paper, we present an R package, TrainSel, which provides flexible, efficient, and easy-to-use tools that can be used for the selection of training populations (STP). We illustrate its use, performance, and potentials in four different supervised learning applications within and outside of the plant breeding area.</p>
    </abstract>
    <kwd-group>
      <kwd>training optimization</kwd>
      <kwd>machine learning</kwd>
      <kwd>genomic selection</kwd>
      <kwd>genomic prediction</kwd>
      <kwd>image classification</kwd>
      <kwd>multi-objective optimization</kwd>
      <kwd>mixed models</kwd>
    </kwd-group>
    <counts>
      <fig-count count="5"/>
      <table-count count="0"/>
      <equation-count count="4"/>
      <ref-count count="50"/>
      <page-count count="12"/>
      <word-count count="8201"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>1. Introduction</title>
    <p>Genomic selection (GS) uses supervised learning for predicting genetic values of phenotyped and un-phenotyped individuals by using genomewide molecular markers (Meuwissen et al., <xref rid="B37" ref-type="bibr">2001</xref>). Genomic prediction (GP) models are built using a training data, i.e., genomic and phenotypic data for a set of individuals. Unfortunately, phenotyping of plants is an expensive and time-consuming process due to factors such as reliance on human input and budget time and resource constraints. Therefore, the most important current bottleneck in application of GS in plant breeding programs is phenotyping. Selection of training populations (STP) in this context refers to identification of a set of training individuals to be phenotyped.</p>
    <p>While the usefulness of optimal training set (TRS) in GS is clearly supported by the literature (Rincent et al., <xref rid="B45" ref-type="bibr">2012</xref>; Akdemir et al., <xref rid="B5" ref-type="bibr">2015</xref>; Isidro et al., <xref rid="B24" ref-type="bibr">2015</xref>; Lorenz and Smith, <xref rid="B33" ref-type="bibr">2015</xref>; He et al., <xref rid="B21" ref-type="bibr">2016</xref>; Cericola et al., <xref rid="B9" ref-type="bibr">2017</xref>; Neyhart et al., <xref rid="B39" ref-type="bibr">2017</xref>; Norman et al., <xref rid="B41" ref-type="bibr">2018</xref>; Akdemir and Isidro-Sánchez, <xref rid="B3" ref-type="bibr">2019</xref>; Guo et al., <xref rid="B19" ref-type="bibr">2019</xref>; Mangin et al., <xref rid="B34" ref-type="bibr">2019</xref>; de Bem Oliveira et al., <xref rid="B11" ref-type="bibr">2020</xref>; Olatoye et al., <xref rid="B42" ref-type="bibr">2020</xref>; Yu et al., <xref rid="B50" ref-type="bibr">2020</xref>; Kadam et al., <xref rid="B27" ref-type="bibr">2021</xref>), the flexible and efficient software tools for implementing them have been limited. Indeed, only a few software tools such as STPGA (Akdemir, <xref rid="B1" ref-type="bibr">2017</xref>) and TSDFGS (Ou and Liao, <xref rid="B43" ref-type="bibr">2019</xref>) are available for public use. The TSDFGS is an R package that focuses on optimization of the TRS by a genetic algorithm (GA) and can be used for STP based on three built-in design criteria. Similarly, STPGA is an R package that uses a modified GA for solving subset selection problems but also allows users to chose from many predefined or user-defined criteria. Here, we designed a TrainSel package that provides many more options, for example, the ability to select multiple sets from multiple candidate sets, specification of whether or not the resulting set needs to be ordered, or the power to perform multi-objective optimization. In addition, TrainSel can be used for searching for solutions to variety of TRS and experimental design problems, such as randomized complete block design, lattice design, etc. TrainSel uses GA in conjunction with simulated annealing (SA) steps, and functions are written in C++ using Rcpp (Eddelbuettel et al., <xref rid="B12" ref-type="bibr">2011</xref>), and therefore, improves performance and is more efficient compared to both of the above alternatives.</p>
    <p>In addition, the TrainSel package was designed to be applied not just for genomic assisted breeding situations, it can also be utilized for STP in general supervised learning problems. Supervised learning refers to the exercise of building predictive models that allow us to predict the states of certain output variables (referred as labels) based on certain input variables. To build supervised learning models we make use of a training dataset that includes observations of both the input variables and the labels, and generally, the larger and more representative the training dataset, the greater is the statistical power for supervised learning. We use the term label throughout this article to refer to the output variables that we are trying to predict. In genomic selection, labeling a genotype refers to measurement of phenotypic values for that genotype in one or more environments.</p>
    <p>In this paper, we demonstrated the usage of the TrainSel R package for STP on genomic assisted breeding applications, but also included other applications to illustrate that STP may also be worthwhile for other supervised learning tasks, such as image classification.</p>
  </sec>
  <sec sec-type="materials and methods" id="s2">
    <title>2. Materials and Methods</title>
    <sec>
      <title>2.1. Populations for Selection of Training Population (STP)</title>
      <p>During STP, we will encounter different types of populations. The target population (Akdemir and Isidro-Sánchez, <xref rid="B3" ref-type="bibr">2019</xref>) is the population that the researcher is interested in, i.e., the population we want to make inferences about. The study population is the population that is accessible to the researcher. The candidate set (CS) is a countably finite representative subset of the study population, similarly, the test set (TS) is a countably finite representative subset of the target population. We assume that we either have an idea about the topology (referring to the initial data available on CS and TS before doing the experiment) of the union of the CS and TS, or that it is relatively easy to obtain this information. Finally, the initial information about the topology of the CS and TS is used to identify a subset of the CS as the training set (TRS) for measuring the labels and additional features. These populations and the default supervised learning paradigm is illustrated in <xref ref-type="fig" rid="F1">Figure 1</xref>.</p>
      <fig id="F1" position="float">
        <label>Figure 1</label>
        <caption>
          <p>Populations in STP and their use. The study population is the population that is accessible to the researcher. The candidate set (CS) is a countably finite representative subset of the SP, similarly, the test set (TS) is a countably finite representative subset of the target population. The initial information about the topology of the CS and TS (<inline-formula><mml:math id="M1"><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="M2"><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>) is used to identify a subset of the CS as the training set (TRS) for measuring the labels (phenotypic values in GS) (<italic>Y</italic><sub><italic>Train</italic></sub>) and other related features (<italic>X</italic><sub><italic>Train</italic></sub>) (for instance, environmental covariates). The training data for TRS is used to build supervised learning models which are then used to make inferences and predictions.</p>
        </caption>
        <graphic xlink:href="fgene-12-655287-g0001"/>
      </fig>
    </sec>
    <sec>
      <title>2.2. Optimization Algorithm in TrainSel</title>
      <p>Selection of training population involves the selection of a subset from a set of candidates and therefore is a combinatorial problem. These problems are typically exponential in terms of computational complexity and may require exploring all possible solutions. Nevertheless, many modern publications point to the effectiveness of applying metaheuristics in obtaining “good” answers to combinatorial optimization problems.</p>
      <p>TrainSel uses a combination of GA (Holland, <xref rid="B23" ref-type="bibr">1992</xref>) and simulated annealing (SA) algorithm (Haines, <xref rid="B20" ref-type="bibr">1987</xref>) for solving combinatorial optimization problems. Genetic algorithm uses techniques inspired by natural evolution such as inheritance, mutation, selection, and crossover to generate better solutions through iterations (Holland, <xref rid="B23" ref-type="bibr">1992</xref>). Simulated annealing moves between solutions using a perturbation and acceptance scheme. At each iteration, a new solution is generated by perturbing the current solution, and this new solution is accepted if it improves the optimization criterion. If the perturbed solution is inferior to the current solution the new solution is accepted based on an acceptance probability that is inversely proportional to the distance of the new solution to the current solution and the current temperature of the system (Haines, <xref rid="B20" ref-type="bibr">1987</xref>). Temperature parameter varies during the iterations of the SA algorithm and usually is a decreasing function of the iteration number. Acceptance of inferior solutions during the SA iterations allows the algorithm to explore more of the possible space of solutions.</p>
      <p>Algorithms such as GA and SA outperform other traditional methods in many applications, as they are flexible and easy to implement (no mathematical analysis is needed when considering a large, complex, non-smooth, poorly-understood optimization problem). There is no proof of convergence for either GA or SA, however, they are effective on a large range of classic optimization problems, and more specifically, have proved to be effective for approximating globally optimal solutions to many combinatorial optimization problems (Glover and Kochenberger, <xref rid="B17" ref-type="bibr">2006</xref>; Fischetti and Lodi, <xref rid="B15" ref-type="bibr">2010</xref>).</p>
      <p>Algorithm 1 describes the main steps of the sample selection algorithm for the single optimization criteria problems. A similar algorithm is used when optimizing more than one criteria. The main difference is that the elite solutions of a population are defined as the non-dominated solutions of the current population.</p>
      <table-wrap id="d39e362" position="float">
        <label>Algorithm 1</label>
        <caption>
          <p>Combinatorial optimization algorithm in TrainSel</p>
        </caption>
        <table frame="hsides" rules="groups">
          <tbody>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">  1:  <italic>t</italic> = 0.</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">  2:  Initialization—Create an initial population of solutions of desired size, <italic>S</italic><sub><italic>t</italic></sub>. Parameters: npop</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">  3:  <bold>repeat</bold></td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">  4:      <italic>t</italic> = <italic>t</italic> + 1.</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">  5:      <italic>S</italic><sub><italic>t</italic></sub> = ∅.</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">  6:      Selection—Identify the best solutions in <italic>S</italic><sub><italic>t</italic>−1</sub> by the ordering of criterion values. Let the best solutions be <italic>s</italic><sub><italic>t</italic></sub>. Parameters: nelite</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">  7:      SA—Improve elements of <italic>s</italic><sub><italic>t</italic></sub> with simulated annealing algorithm. Parameters: niterSANN, stepSANN</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">  8:      Elitism—Put <italic>s</italic><sub><italic>t</italic></sub> in <italic>S</italic><sub><italic>t</italic></sub>,</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">  9:      <bold>repeat</bold></td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">10:          Crossover—Randomly pick two solutions in <italic>S</italic><sub><italic>t</italic></sub>. Obtain a recombination of these two solutions.</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">11:          Mutation—Mutate the solution from the above step with a certain mutation probability and intensity. Parameters: mutprob, mutintensity</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">12:          Insert this solution into <italic>S</italic><sub><italic>t</italic></sub>.</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">13:       <bold>until</bold>
<italic>S</italic><sub><italic>t</italic></sub> has <italic>N</italic><sub><italic>pop</italic></sub> solutions.</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">14:  <bold>until</bold> Convergence: the achievement of the maximum number of iterations or non-improvement for a prescribed number of iterations. Parameters: niterations, miniterbefstop, tolconv <bold>return</bold> Best Solution.</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>The parameters of the selection algorithm in TrainSel are: “npop” which is the size of the genetic algorithm population, “nelite” which is the number of elite solutions selected in each iteration, “niterations” which is the maximum number of iterations for the genetic algorithm, “miniterbefstop” is the minimum number of iterations of “no change” before the algorithm is deemed converged, “tolconv” which is the tolerance for determining “no change” in the criteria values, “niterSANN” which is the number of iterations for the SA algorithm, “stepSANN” which controls the speed of cooling of the SA algorithm. Each of these parameters comes with default settings, most of which do not need to be changed by the user for small to medium-sized optimization problems. For larger problems increasing “niterations” and “niterbefstop” parameters will usually suffice. We have done some experimentation with the default settings of the remaining parameters (and with relatively large values for “niterations” and “minitbefstop”) algorithm in several problems with different complexities where the true solution was known. The results from these convergence experiments are provided in <xref ref-type="supplementary-material" rid="SM1">Supplementary Figure 1</xref>. The user can use these figures to guess initial estimates for these two parameters for their problems. After the run of the algorithm, the best way to decide if the algorithm has worked is by checking the flattening of the objective function values during the final iterations.</p>
      <p>In most applications of STP, the ordering of selected samples in the TRS will not be important and therefore only one instance of each individual is required for TRS sample; we refer to this case as an unordered set (UOS). In certain cases, the order of the sample will be important but again only one instance of each individual is required, we refer to this case as ordered set (OS). The cases where we allow more than one instance of each individual is referred to as unordered multiset (UOMS) and ordered multiset (OMS). TrainSel allows users to specify which of these types of sets the optimization problem falls into. An application of the use of finding optimal ordered sets is the design of a blocked experiment where we care about the design of the experiment, i.e., the assignment of individuals to different blocks, in addition to selecting which individuals to include in the study.</p>
      <p>The search algorithm in TrainSel is not guaranteed to find globally optimal solutions, i.e., the solutions obtained by any run of TrainSel may be sub-optimal, and different solutions can be obtained given different starting conditions and optimization parameters. Another layer of safety can be obtained if the algorithm is started from multiple initial conditions, and the best of all the runs is selected as the final solution.</p>
      <p>Numerous other algorithms have been proposed for the optimal subset selection problem, many of them are heuristic exchange type algorithms (Fedorov, <xref rid="B13" ref-type="bibr">1972</xref>; Mitchell, <xref rid="B38" ref-type="bibr">1974</xref>; Nguyen and Miller, <xref rid="B40" ref-type="bibr">1992</xref>; Rincent et al., <xref rid="B45" ref-type="bibr">2012</xref>; Isidro et al., <xref rid="B24" ref-type="bibr">2015</xref>). In exchange type algorithms, new solutions are obtained by adding a sample unit and removing another at a time (some exchange algorithms might allow the exchange of more than one samples at once), these algorithms are greedy and are only proven to find the best subset for a certain type of design criteria.</p>
    </sec>
    <sec>
      <title>2.3. Design Criteria</title>
      <p>Selection of training populations is an optimal experimental design problem, and the work on the optimal experimental designs has a long and rich history (Smith, <xref rid="B48" ref-type="bibr">1918</xref>; Kiefer, <xref rid="B28" ref-type="bibr">1959</xref>; Fisher, <xref rid="B16" ref-type="bibr">1960</xref>; Fedorov, <xref rid="B13" ref-type="bibr">1972</xref>; Atkinson and Donev, <xref rid="B7" ref-type="bibr">1992</xref>; Pukelsheim and Rosenberger, <xref rid="B44" ref-type="bibr">1993</xref>; Fedorov and Hackl, <xref rid="B14" ref-type="bibr">2012</xref>; Silvey, <xref rid="B47" ref-type="bibr">2013</xref>) and it is not a surprise that many different design criteria have been proposed. These criteria can be categorized into three major groups:</p>
      <list list-type="bullet">
        <list-item>
          <p>Parametric design criteria which assume that the experimenter has specified a model before the training data is obtained. These criteria depend on a scalar function of the information matrix for the model parameters that give some indication about the sampling variances and covariances of the estimated quantities by the model. The estimated quantity might be some function of the model parameters or predictions from the model for target individuals. There are many designs obtained by optimizing such criteria are referred to as <italic>A</italic>−, <italic>D</italic>−, <italic>E</italic>−, <italic>G</italic>−, etc… optimal designs (Kiefer et al., <xref rid="B29" ref-type="bibr">1985</xref>). Bayesian design criteria use priors on the parameters of the models to evaluate the utility of designs.</p>
        </list-item>
        <list-item>
          <p>Nonparametric designs include criteria that are based on distance or similarity measures. For example, the maximin-distance design is a space-filling design that chooses a training population such that the minimum distance among the TRS is maximized (Johnson et al., <xref rid="B26" ref-type="bibr">1990</xref>). Another such design is the minimax design (Johnson et al., <xref rid="B26" ref-type="bibr">1990</xref>) where the training population is such that the maximum of the minimum distances from the training population to the rest of the CS or the TS is minimized. Space-filling designs aim to cover the experimental region with as few gaps or holes as possible. Unlike the parametric design criteria, minimax distance presumes no underlying model and, in turn, is suitable for situations where the model is unknown.</p>
        </list-item>
        <list-item>
          <p>Multiple design. The choice of an appropriate criterion requires knowledge about the model and what is required from the model. Multiple model optimal experimental design and compound optimization criteria try to overcome the choice issue by combining more than one criteria into one via some type of averaging. Alternatively, we can compare different designs using more than one criteria based on the dominance concept and use multi-objective optimization methods to decide on a certain design from out a set of Pareto optimal designs (Markowitz, <xref rid="B35" ref-type="bibr">1952</xref>, <xref rid="B36" ref-type="bibr">1968</xref>; Akdemir and Sánchez, <xref rid="B4" ref-type="bibr">2016</xref>; Akdemir et al., <xref rid="B2" ref-type="bibr">2019</xref>).</p>
        </list-item>
      </list>
      <p>TrainSel allows users to use optimization criteria by letting them write their optimization functions and therefore can be used to search designs based on all of the above categories. Given the multitude of design criteria, this flexibility is one key advantage of TrainSel to its alternatives such as STPGA or TSDFGS.</p>
      <sec>
        <title>2.3.1. Built in Criterion: CDmin</title>
        <p>The STP involves the selection of TS from CS using optimization criteria. TrainSel is supplemented with a predefined design criterion CDmin which is related to the CDmean criteria in Laloë (<xref rid="B30" ref-type="bibr">1993</xref>), Laloë and Phocas (<xref rid="B31" ref-type="bibr">2003</xref>), Rincent et al. (<xref rid="B45" ref-type="bibr">2012</xref>). The main reason for implementing this design criterion as the only built-in design criterion is due to our specific interest in applying TrainSel to the design of single and multi-environmental GP experiments.</p>
        <p>The built-in criterion CDmin depends on the linear mixed models. The linear mixed-effects model for a <italic>n</italic>-dimensional response variable <italic>y</italic>, <italic>n</italic> × <italic>p</italic> design matrix of fixed effects, <italic>n</italic> × <italic>q</italic> design matrix of random effects is defined as:</p>
        <disp-formula id="E1">
          <mml:math id="M3">
            <mml:mrow>
              <mml:mi>y</mml:mi>
              <mml:mo>=</mml:mo>
              <mml:mi>X</mml:mi>
              <mml:mi>β</mml:mi>
              <mml:mo>+</mml:mo>
              <mml:mi>Z</mml:mi>
              <mml:mi>u</mml:mi>
              <mml:mo>+</mml:mo>
              <mml:mi>ε</mml:mi>
              <mml:mo>;</mml:mo>
            </mml:mrow>
          </mml:math>
        </disp-formula>
        <p>where ε ~ <italic>N</italic><sub><italic>n</italic></sub>(0, <italic>R</italic>) is independent of <italic>u</italic> ~ <italic>N</italic><sub><italic>q</italic></sub>(0;<italic>G</italic>), <italic>β</italic> ∈ <italic>ℛ</italic><sup><italic>p</italic></sup>, <italic>G</italic> is a <italic>q</italic> × <italic>q</italic> covariance matrix and <italic>R</italic> is a <italic>n</italic> × <italic>n</italic> covariance matrix. The assumptions of the linear mixed-effects model imply <italic>E</italic>(<italic>y</italic>|<italic>X</italic>; <italic>Z</italic>) = <italic>X<italic>β</italic></italic>, <inline-formula><mml:math id="M4"><mml:mi>y</mml:mi><mml:mo>~</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mi>β</mml:mi><mml:mo>;</mml:mo><mml:mi>Z</mml:mi><mml:mi>G</mml:mi><mml:msup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mi>β</mml:mi><mml:mo>;</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> with <italic>V</italic> defined as <italic>V</italic> = <italic>ZGZ</italic>′ + <italic>R</italic>. For this model, the coefficient of determination matrix (Laloë, <xref rid="B30" ref-type="bibr">1993</xref>; Laloë and Phocas, <xref rid="B31" ref-type="bibr">2003</xref>; Rincent et al., <xref rid="B45" ref-type="bibr">2012</xref>) of <inline-formula><mml:math id="M5"><mml:mover accent="false"><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> for predicting <italic>u</italic> is given by</p>
        <disp-formula id="E2">
          <mml:math id="M6">
            <mml:mrow>
              <mml:mrow>
                <mml:mo stretchy="false">(</mml:mo>
                <mml:mrow>
                  <mml:mi>G</mml:mi>
                  <mml:msup>
                    <mml:mrow>
                      <mml:mi>Z</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>′</mml:mi>
                    </mml:mrow>
                  </mml:msup>
                  <mml:mi>P</mml:mi>
                  <mml:mi>Z</mml:mi>
                  <mml:mi>G</mml:mi>
                </mml:mrow>
                <mml:mo stretchy="false">)</mml:mo>
              </mml:mrow>
              <mml:mo>⊘</mml:mo>
              <mml:mi>G</mml:mi>
            </mml:mrow>
          </mml:math>
        </disp-formula>
        <p>where <italic>P</italic> = <italic>V</italic><sup> −1</sup> − <italic>V</italic><sup>−1</sup><italic>X</italic>(<italic>X</italic>′<italic>V</italic><sup>−1</sup><italic>X</italic>)<sup>−1</sup><italic>X</italic>′<italic>V</italic><sup>−1</sup> and ⊘ expresses the elementwise division. The minimum of the selected diagonal elements of this matrix is called the CDmin. The minimum of the coefficient of determination takes on values between 0 and 1, and the designs that give higher values for this criterion are preferred to designs with lower values. The CDmin criterion follows the maximin decision rule, maximizing this criterion amounts to maximizing the utility for the worst case scenario, and it is suitable for making risk averse decisions.</p>
        <p>Most authors use the mean of the selected diagonal elements of this matrix as the criterion, this is called the CDmean criterion. We have used CDmin instead of CDmean for several reasons. Firstly, the distribution of CD values along the diagonal for a given <italic>G</italic> matrix includes both the training samples and the remaining samples. The CD values that correspond to the training samples, as expected, form a different cluster (high values of CD) than the cluster of CD values corresponding to the samples that are not selected (low values of CD) and therefore we have a bimodal distribution for the CD values. Secondly, if the aim is to improve the generalization performance of the resulting model we prefer to move the lower part of this distribution to the right, i.e., the maximin decision amounts to improving the worst case CD value in this distribution which leads to the CDmin approach. Thirdly, the purpose of this article is not to compare effect of using different selection criteria but to show that TrainSel can be easily adopted to many different selection criteria.</p>
        <p>Alternatively, we could approach the bimodality by restricting the mean measure to be calculated only on the set difference of the CS and the TRS or on a predefined TS. It should be trivial to apply any of these modifications with TrainSel. We stress here that the choice among the many different optimization criteria require thorough analysis, but this is beyond the aims of this paper.</p>
        <p>We use two parameterizations of the above mixed model: In the first parameterization, we assume that <inline-formula><mml:math id="M7"><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>K</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="M8"><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>I</mml:mi></mml:math></inline-formula> where <inline-formula><mml:math id="M9"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="M10"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> are the variances of the random terms <italic>u</italic> and <italic>e</italic> correspondingly and <italic>K</italic> is a relationship matrix of the same dimension as <italic>G</italic>. In the second parameterization <italic>G</italic> = <italic>K</italic> ⊗ <italic>V</italic><sub><italic>k</italic></sub> and <italic>R</italic> = <italic>I</italic> ⊗ <italic>V</italic><sub><italic>e</italic></sub> where <italic>V</italic><sub><italic>k</italic></sub> and <italic>V</italic><sub><italic>e</italic></sub> are covariance matrices that relate to the effects in <italic>u</italic> and <italic>e</italic> using Kronecker structured covariances.</p>
        <p>The first model is useful for modeling random effects <italic>u</italic> related by a relationship matrix <italic>K</italic>. The STP for this model involves the selection of a predefined size set from the levels of the random term <italic>u</italic> that also correspond to factor levels in the rows (and columns) of <italic>K</italic> for labeling.</p>
        <p>The second model is useful for modeling factor levels that correspond to the rows (and columns) of <italic>K</italic> in several related environments. The covariance of these random effects in several environments is given by <italic>V</italic><sub><italic>k</italic></sub> and similarly, the covariance of the residual effects in these environments is given by <italic>V</italic><sub><italic>e</italic></sub>. In this case, we want to select predefined sizes of sets from the factor levels that correspond to the rows (and columns) of <italic>K</italic> to be labeled in the corresponding environments.</p>
        <p>The purpose of the <italic>X</italic> matrix in the mixed models above is to account for fixed effects. If the rows of the <italic>X</italic> matrix corresponding to the conditions in a given environment are heterogeneous, then, in addition to selecting the levels of the random effect in the TRS, we would like to arrange the training sample optimally to the conditions expressed in the rows of <italic>X</italic>. In these cases, we are looking to identify a TRS that is an ordered subset of the CS. If no <italic>X</italic> matrix is specified or if the rows of <italic>X</italic> are homogeneous within environments the order of the assignments will not matter. In this case, STP involves the selection of an unordered sample as TRS.</p>
      </sec>
    </sec>
    <sec>
      <title>2.4. Datasets and Applications</title>
      <p>In this section, we describe the datasets, simulations, and related analysis. We are testing Trainsel with four applications: The first application deals with STP for GP of hybrid performance, the second with a design of multi-environmental GS experiment. The third application deals with STP for an image recognition problem. Our final application on splines regression entails simultaneous selection of design points among a set of candidates and allocation of knots through the range of the explanatory variables.</p>
    </sec>
    <sec>
      <title>2.5. Application 1: Wheat Data for Hybrid Performance Prediction</title>
      <p>This dataset was published in Liu et al. (<xref rid="B32" ref-type="bibr">2016</xref>) and was used in a similar context in Guo et al. (<xref rid="B19" ref-type="bibr">2019</xref>). The genetic dataset included the marker data (90 k SNP array based on an Illumina Infinium genotyping platform) for 135 elite winter wheat individuals adapted to Central Europe. A total of 1, 604 F1 hybrids were generated in a factorial crossing scheme with 120 inbred individuals serving as female and 15 inbred individuals serving as male parents.</p>
      <p>All genomic data for the wheat data for hybrid performance prediction application were obtained from the Dryad Digital Repository (doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.461nc">10.5061/dryad.461nc</ext-link>). All related phenotypic data were obtained from the Digital Repository (doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5447/IPK/2016/11">10.5447/IPK/2016/11</ext-link>). Marker information for the hybrids was deduced from the parental individuals.</p>
      <p>All individuals were evaluated in up to six environments. The adjusted means over environments for each of the 1, 604 F1 hybrids for 7 traits (gluten content, kernel hardness, protein content, SDS volume, starch content, test weight, 1, 000-kernel weight) were treated as the labels for the traits.</p>
      <p>After removing the hybrids that came from parents with partial phenotypic data, we were left with 795 hybrids (full factorial crosses between 15 males and 53 females with complete phenotypic data). We have complete phenotypic data for all of these 795 hybrids in this application. Nevertheless, in practice, the evaluation of each of the hybrids involves making the cross between the corresponding parents and evaluating them in phenotypic trials, which are time-consuming and expensive. It is, therefore, desirable to reduce the costs involved in the generation and phenotypic evaluation by using a subset of all possible hybrids in the experiments and to use the data generated from these experiments for training genomic prediction models to make inferences about the phenotypic performance of untested hybrids.</p>
      <p>In this application, we examine STP for hybrid performance prediction, i.e., we would like to select a prespecified size subset (50, 75, 100, 200 hybrids) of all possible 795 hybrids for training and use the phenotypic data from the TRS to predict the performance of the remaining hybrids. The TRSs were determined either by TrainSel using the CDmin criterion or by random sampling (repeated 30 times). The remaining hybrids were used as the TS where the prediction accuracies were evaluated using the correlation or the mean squared error between the predicted genotypic values and the observed phenotypes.</p>
      <p>We only used the additive effects when calculating the CDmin criterion values through use of an additive relationship calculated from the marker scores. It is possible to include other effects such as dominance by supplementing the additive effects matrix with a dominance relationship matrix.</p>
    </sec>
    <sec>
      <title>2.6. Application 2: Wheat Data for Multi-Environmental GS Experiment Design</title>
      <p>We have obtained this dataset from <ext-link ext-link-type="uri" xlink:href="https://triticeaetoolbox.org/wheat">https://triticeaetoolbox.org/wheat</ext-link>. The genotypic data included 989 individuals genotyped for 24, 740 markers. All of these individuals had complete phenotypic data on plant height and stripe rust severity from three environmental trials. Using this data we have performed a cross-validation experiment where we explored the potential of STP for the multi-environmental design of GS experiments. We varied the number of overlapping individuals between the environments intending to see the effect on the predictive ability for the untested individuals.</p>
      <p>We start each replication of the experiment by randomly selecting 240 individuals as the CS and the remaining individuals as the TS. Given the candidate individuals, we assume would like to construct an experiment in tree environments each of which can accommodate a fixed number of individuals (20, 40, 60, 80). To see how the replication affects the maximum CDmin values we also restrict the total number of individuals in the whole experiment to multiples of 1.2, 1.5, 2, 2.5, 3 of the number of individuals in each environment. Note that, restricting the total number of individuals to a multiple of 1.2 of the number of individuals allowed in each of the environments correspond to almost total replication (we did not use a factor of 1 because this value corresponds to a different type of combinatorial problem), on the other hand, a multiple of 3 corresponds to no replication, the intermediate values allow some amount of replication. We have assumed that the covariance of genotypic values between all trials pairs were 0.7 and we have assumed that the residuals were independent within and between trials. Besides, we have assumed that the heritabilities of both experiments were the same and equal to 0.5. We repeated this experiment 15 times and for each replication, we record the maximum CDmin value obtained and we also check the accuracy of the model in the TSs by calculating the correlation of the trait values in the TS and corresponding predictions from models based on different TRSs.</p>
    </sec>
    <sec>
      <title>2.7. Application 3: MINST Datasets for Image Recognition</title>
      <p>Image classification refers to the task of predicting the kind of objects in images. To train image classification models we need labeled images as training data. In this context, the purpose of STP would be to identify a subset of images to be labeled from out of a larger set of images.</p>
      <p>In this application, we used a standard image classification data, the MINST fashion dataset, obtained using the “tf.keras.datasets” module, which consists of 28 × 28 grayscale images of 70, 000 in 10 categories. The original data is split into two parts, the training set has 60, 000 images and the test set has 10, 000 images. In both the training and test datasets, the different classes were equally represented.</p>
      <p>We performed the following experiment with this dataset: We started each replication of the experiment by identifying 1, 000 samples at random from the original training set of size 60, 000 as candidates. The number of samples from each class in the CS were arbitrarily set as 500, 450, 400, 350, 300, 250, 200, 150, 100, and 50 to assure an unbalanced CS. We chose a TRS of 100 or 200 samples out of the CS using TrainSel with the maximin distance criterion and using the distances among the 794 image features of samples in the CS. In addition, 100 random samples of sizes 100 and 200 were taken from the same CS as random TRSs. For each TRS, we recorded the entropy for the class distributions in the TRSs, the loss, and the accuracy for the predictions in the TS. We used the same 4-layer convolutional deep neural network prediction model for all the TRSs, these models were trained using the Keras R package (Allaire and Chollet, <xref rid="B6" ref-type="bibr">2018</xref>). This experiment was repeated 50 times.</p>
    </sec>
    <sec>
      <title>2.8. Application 4: STP for Splines Regression</title>
      <p>Spline regression is a commonly used regression technique for modeling nonlinear relationships between a continuous response and continuous explanatory variables. In this technique the ranges of the explanatory variables are divided into bins using points which are called knots and the response is modeled with a piecewise polynomial with a set of extra constraints (continuity, continuity of the first derivative, and continuity of the second derivative) at the knots.</p>
      <p>A commonly used form of splines, namely the natural cubic splines, uses cubic segments. The model for a natural cubic spline that relates the response <italic>y</italic> to the input variable <italic>x</italic> can be expressed as</p>
      <disp-formula id="E3">
        <mml:math id="M11">
          <mml:mrow>
            <mml:mi>y</mml:mi>
            <mml:mo>=</mml:mo>
            <mml:msub>
              <mml:mi>β</mml:mi>
              <mml:mn>0</mml:mn>
            </mml:msub>
            <mml:mo>+</mml:mo>
            <mml:msub>
              <mml:mi>β</mml:mi>
              <mml:mn>1</mml:mn>
            </mml:msub>
            <mml:mi>x</mml:mi>
            <mml:mo>+</mml:mo>
            <mml:msub>
              <mml:mi>β</mml:mi>
              <mml:mn>2</mml:mn>
            </mml:msub>
            <mml:msub>
              <mml:mrow>
                <mml:mo stretchy="false">(</mml:mo>
                <mml:mi>x</mml:mi>
                <mml:mo>−</mml:mo>
                <mml:msub>
                  <mml:mi>k</mml:mi>
                  <mml:mn>1</mml:mn>
                </mml:msub>
                <mml:mo stretchy="false">)</mml:mo>
              </mml:mrow>
              <mml:mo>+</mml:mo>
            </mml:msub>
            <mml:mo>+</mml:mo>
            <mml:msub>
              <mml:mi>β</mml:mi>
              <mml:mn>3</mml:mn>
            </mml:msub>
            <mml:msub>
              <mml:mrow>
                <mml:mo stretchy="false">(</mml:mo>
                <mml:mi>x</mml:mi>
                <mml:mo>−</mml:mo>
                <mml:msub>
                  <mml:mi>k</mml:mi>
                  <mml:mn>2</mml:mn>
                </mml:msub>
                <mml:mo stretchy="false">)</mml:mo>
              </mml:mrow>
              <mml:mo>+</mml:mo>
            </mml:msub>
            <mml:mo>+</mml:mo>
            <mml:mo>.</mml:mo>
            <mml:mo>.</mml:mo>
            <mml:mo>.</mml:mo>
            <mml:mo>+</mml:mo>
            <mml:msub>
              <mml:mi>β</mml:mi>
              <mml:mn>6</mml:mn>
            </mml:msub>
            <mml:msub>
              <mml:mrow>
                <mml:mo stretchy="false">(</mml:mo>
                <mml:mi>x</mml:mi>
                <mml:mo>−</mml:mo>
                <mml:msub>
                  <mml:mi>k</mml:mi>
                  <mml:mi>p</mml:mi>
                </mml:msub>
                <mml:mo stretchy="false">)</mml:mo>
              </mml:mrow>
              <mml:mo>+</mml:mo>
            </mml:msub>
            <mml:mo>+</mml:mo>
            <mml:msubsup>
              <mml:mi>σ</mml:mi>
              <mml:mi>ε</mml:mi>
              <mml:mn>2</mml:mn>
            </mml:msubsup>
          </mml:mrow>
        </mml:math>
      </disp-formula>
      <p>where</p>
      <disp-formula id="E4">
        <mml:math id="M12">
          <mml:mrow>
            <mml:msub>
              <mml:mrow>
                <mml:mo stretchy="false">(</mml:mo>
                <mml:mi>x</mml:mi>
                <mml:mo>−</mml:mo>
                <mml:mi>k</mml:mi>
                <mml:mo stretchy="false">)</mml:mo>
              </mml:mrow>
              <mml:mo>+</mml:mo>
            </mml:msub>
            <mml:mo>=</mml:mo>
            <mml:mrow>
              <mml:mo>{</mml:mo>
              <mml:mrow>
                <mml:mtable columnalign="left">
                  <mml:mtr columnalign="left">
                    <mml:mtd columnalign="left">
                      <mml:mrow>
                        <mml:mn>0</mml:mn>
                        <mml:mo>,</mml:mo>
                        <mml:mtext> if  </mml:mtext>
                        <mml:mi>x</mml:mi>
                        <mml:mo>&lt;</mml:mo>
                        <mml:mi>k</mml:mi>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                  <mml:mtr columnalign="left">
                    <mml:mtd columnalign="left">
                      <mml:mrow>
                        <mml:mi>x</mml:mi>
                        <mml:mo>−</mml:mo>
                        <mml:mi>k</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mtext> if  </mml:mtext>
                        <mml:mi>x</mml:mi>
                        <mml:mo>≥</mml:mo>
                        <mml:mi>k</mml:mi>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:mrow>
            </mml:mrow>
          </mml:mrow>
        </mml:math>
      </disp-formula>
      <p>and <italic>k</italic><sub>1</sub>, <italic>k</italic><sub>2</sub>, …, <italic>k</italic><sub><italic>p</italic></sub> are the knot positions that are to be specified as hyper-parameters. Due to this dependence the model matrix for this model will be written as <italic>X</italic>(<italic>k</italic>). The qubic spline is a linear model, therefore, the formula for D-optimality criteria for this model can be expressed as <italic>D</italic>(<italic>k</italic>) = |<italic>X</italic>(<italic>k</italic>)′<italic>X</italic>(<italic>k</italic>)| and its value depends on the choice of the knots. A “good” design maximizes the value of this function, i.e., we need to select the design points and also find the best knots for the selected set of design points.</p>
      <p>In this simulation exercise, we show that we can simultaneously pick a TRS of design points out of a set of candidates and set the knot positions using TrainSel, i.e., we want to select a set of <italic>x</italic> values from a set of given candidates and find values of <italic>k</italic><sub>1</sub>, <italic>k</italic><sub>2</sub>, …, <italic>k</italic><sub><italic>p</italic></sub> that maximizes <italic>D</italic>(<italic>k</italic>). Just like in other supervised learning scenarios, we assume we have no access to the values of the response apriori, their values will be observed only in the TRS and these along with the selected optimal knots will be used to fit the cubic spline model. The model will be used in the prediction of the response and the predicted response values in the CS will be compared to the true value of the response (the function value at <italic>x</italic>) by calculating mean squared errors. The results obtained by the optimization approach will be compared to the same size random sample of <italic>x</italic> selected from the CS and with the standard approach that involves placing knots at equally spaced quantiles of the range of the <italic>x</italic> values (Ruppert, <xref rid="B46" ref-type="bibr">2002</xref>) in the CS.</p>
      <p>In each replication of the experiment, we started with a 1, 000 candidate <italic>x</italic> values sampled uniformly between 0 and 1. We selected 200 (or 300) <italic>x</italic> values from these candidate values and also determine the placement of 15 knots. Following the benchmark experiments in Ruppert (<xref rid="B46" ref-type="bibr">2002</xref>) we generated our response variables from four different functions (namely logit, sine, bump, spahat functions). More details on these functions and the generation of the response values are given in the <xref ref-type="supplementary-material" rid="SM1">Supplementary Material</xref>. The mean squared error for the predictions from the optimized set with optimized knots and random TRSs with equally spaced quantile knots were compared. This experiment was replicated 30 times.</p>
    </sec>
  </sec>
  <sec id="s3">
    <title>3. Results and Discussion</title>
    <sec>
      <title>3.1. Application 1: Wheat Data for Hybrid Performance Prediction</title>
      <p>The results of the application on hybrid performance are summarized by the boxplots in <xref ref-type="fig" rid="F2">Figure 2</xref> for two traits. The results for the remaining five traits were summarized in <xref ref-type="supplementary-material" rid="SM1">Supplementary Figure 2</xref>. Preliminary analysis with the wheat data indicated that the hybrids selected as training by maximizing the CDmin criterion, provided more accurate prediction models for predicting the remaining hybrids as compared to models based on a random sample of hybrids. The relative efficiency of the optimized samples depended on the number of hybrids selected in the TRS, and also on the trait. Nevertheless, there was a clear optimized trend overall. The relative performance of the optimized TRS to random samples is minimal when the sample size were as low as 50, and it peaked for about sample size of 100, this relative efficiency decreased as the sample size increases. These results indicated that the CDmin criterion was a useful method for selecting wheat hybrids for predictive performance. In our opinion, hybrid prediction problems provide a perfect situation to exploit the STP approaches.</p>
      <fig id="F2" position="float">
        <label>Figure 2</label>
        <caption>
          <p>The correlations and the mean squared errors between the predicted and observed trait values of the hybrids in the test data. There is an advantage in using optimized training samples for this dataset. The correlations and mean squared errors between the predicted and observed trait values of the hybrids in the test sets were significantly better for the optimized samples than the correlations and mean squared errors of the predicted and observed for the random samples.</p>
        </caption>
        <graphic xlink:href="fgene-12-655287-g0002"/>
      </fig>
    </sec>
    <sec>
      <title>3.2. Application 2: Wheat Data for Multi-Environmental GS Experiment Design</title>
      <p>When designing a multi-environmental GS experiment, we would like to allocate individuals in environments so that we have a representative sample of individuals in each environment and, at the same time, have genetically similar individuals across environments. Genomic information is not utilized when designing experiments using classical methods such as randomized block design, and therefore, these designs are expected to perform worse than designs that make use of genomic information.</p>
      <p>The CDmin values of the optimal samples on the first row of <xref ref-type="fig" rid="F3">Figure 3</xref> indicate that CDmin values are maximized for intermediate amount of replication between the experiments. Since, the square root of the CD relates directly to the expected accuracy, we can use this information to decide on the size and amount of replication for a multi-environmental GS experiment.</p>
      <fig id="F3" position="float">
        <label>Figure 3</label>
        <caption>
          <p>Optimally designed multi-environmental GS experiments can boost prediction accuracies. In the first row, the CDmin values of the optimal samples show that the CDmin values are maximized for the intermediate amount of replication between the experiments. The second and third rows of figure show the attained accuracy for optimal samples and random samples for plant height and stripe rust.</p>
        </caption>
        <graphic xlink:href="fgene-12-655287-g0003"/>
      </fig>
      <p>The second and third rows of <xref ref-type="fig" rid="F3">Figure 3</xref> showed the attained accuracy for optimal samples and random samples for plant height and stripe rust. As we can see the optimal experiments had better accuracy compared to the random experiments at all experiment sizes, levels of replication and for both of the traits. The trends in the observed accuracies for both the random samples and the optimized samples followed the trends observed in the CDmin values in the first row of the <xref ref-type="fig" rid="F3">Figure 3</xref>.</p>
      <p>These results demonstrated that optimally designed multi-environmental GS experiments can boost prediction accuracies as compared to randomized block designs. We note here that designing multi-environmental experiments with a large number of candidate individuals can be computationally costly. A useful strategy in these cases involves reducing the size of the candidate set to a manageable size by selecting a optimal subset from the full candidate set using suitable design criterion and using the reduced candidate set in the design of the multi-environmental experiment.</p>
    </sec>
    <sec>
      <title>3.3. Application 3: MINST Datasets for Image Recognition</title>
      <p>The results of this experiment are summarized in <xref ref-type="fig" rid="F4">Figure 4</xref>. The TRS identified by TrainSel using the maximin distance criterion had higher entropy in their label distributions on average compared to those of random samples for both TRS sizes (<xref ref-type="fig" rid="F4">Figure 4</xref>). Entropy is a widely used measure for quantifying inhomogeneity, impurity in machine learning applications. The predictions from the models trained on the optimal TRS were on average more accurate and had lower cost as measured by sparse cross-entropy.</p>
      <fig id="F4" position="float">
        <label>Figure 4</label>
        <caption>
          <p>Images selected optimally in the TRS have higher entropy in their label distributions than of the random samples <bold>(C)</bold> and the generalization performance of the model measured by both loss <bold>(A)</bold> and accuracy <bold>(B)</bold> functions in the test dataset indicate that optimally selected samples yield better models than the ones built on random samples.</p>
        </caption>
        <graphic xlink:href="fgene-12-655287-g0004"/>
      </fig>
      <p>Note that, in this application, we have started each replication of the experiment with an unbalanced CS. Entropy is a measure of balance in the label distributions, and entropy of the label distributions in the TRSs selected at random mirrors the unbalance in the CS. In addition, optimally selected samples have higher entropy values meaning that the labels for the samples were more evenly distributed, and this resulted in models with better accuracy, i.e., the percentage of correctly classified examples were higher (<xref ref-type="fig" rid="F4">Figures 4A–C</xref>). In addition, the lower values of the loss function in the test data for optimal samples indicated that the estimates of probabilities used for the classification of observations lead to more confident decisions with more confident class probability estimates.</p>
    </sec>
    <sec>
      <title>3.4. Application 4: STP for Splines Regression</title>
      <p>The results of the splines experiment are summarized in <xref ref-type="fig" rid="F5">Figure 5</xref>. For all combinations of the number of knots, the number of TRS sizes, the optimally designed experiments where both knot placements and selected samples in the TRS were decided by optimizing the D-optimality criterion have resulted in splines models with lower mean squared error values as compared to the splines models trained on random samples with knots located at equally spaced sample quantiles. This was true for all of the four different response surfaces we have tested.</p>
      <fig id="F5" position="float">
        <label>Figure 5</label>
        <caption>
          <p>TrainSel spline application. The logarithm of the mean square errors (y-axis) splines models trained on random samples with knots (10, 15, 20, and 30) located at equally spaced sample quantiles. Optimal training size (x-axis) and knots were selected by optimizing the D-optimality criterion for the different number of knots and different sample sizes for a set of functions. At each combination of sample size and the number of knots mean squared errors are lower for the latter approach. Although, in very few cases random sample performed slightly better than the optimized samples, the general trend is in favor of the optimized approach.</p>
        </caption>
        <graphic xlink:href="fgene-12-655287-g0005"/>
      </fig>
      <p>This example used TrainSel used to optimize a mixed integer optimization problem. Mixed integer programming finds many applications in plant breeding, for instance, it can be used in optimizing sequencing resources (Gonen et al., <xref rid="B18" ref-type="bibr">2017</xref>; Cheng et al., <xref rid="B10" ref-type="bibr">2020</xref>), estimating parental combinations to balance gains and inbreeding (Brisbane and Gibson, <xref rid="B8" ref-type="bibr">1995</xref>; Jannink, <xref rid="B25" ref-type="bibr">2010</xref>; Heslot et al., <xref rid="B22" ref-type="bibr">2015</xref>), or genomic mating (Akdemir and Sánchez, <xref rid="B4" ref-type="bibr">2016</xref>).</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="s4">
    <title>4. Conclusions</title>
    <p>TrainSel provides algorithms for the optimization of mixed-integer problems. It was written with the STP problems in focus. The main use cases are given below:</p>
    <list list-type="order">
      <list-item>
        <p>Identifying a TRS from a larger CS for labeling especially when per sample cost of labeling is relatively high.</p>
      </list-item>
      <list-item>
        <p>Design of experiments based on any user-defined design criteria or with built-in mixed model-based criteria.</p>
      </list-item>
      <list-item>
        <p>Design of single or multi-environmental genomic prediction/selection experiments where the phenotyping is the major constraining factor.</p>
      </list-item>
      <list-item>
        <p>TrainSel can also be used in other combinatorial optimization problems. Some examples of such problems include max clique, independent set, vertex cover, knapsack, set covering, set partitioning, feature subset selection (for supervised and unsupervised learning), traveling salesman, job scheduling problems.</p>
      </list-item>
    </list>
    <p>The best feature of TrainSel is where we combine training set selection with a particular experimental design, and this option has not been implemented in any other STP software.</p>
    <p>Reasons for using this package are as follows:</p>
    <list list-type="order">
      <list-item>
        <p>Most of the existing STP or statistical design software (such as TSDFGS, AlgDesign; Wheeler, <xref rid="B49" ref-type="bibr">2004</xref>) will optimize only a few built-in optimization criteria. You can use TrainSel easily with your own design criteria.</p>
      </list-item>
      <list-item>
        <p>Existing STP or statistical design software (such as STPGA, TSDFGS, AlgDesign) will optimize a single criterion at a time, but TrainSel offers an additional better possibility, i.e., we can specify multiple objectives that must be optimized simultaneously.</p>
      </list-item>
      <list-item>
        <p>TrainSel uses a memetic evolutionary algorithm which in our experiments achieved better convergence than a simple genetic algorithm which was the basis for STPGA and TSDFGS.</p>
      </list-item>
      <list-item>
        <p>The ability to handle ordered or unordered samples, with or without replication, along with several numerical variables to optimize user-defined functions makes this package a flexible general optimization tool.</p>
      </list-item>
    </list>
    <p>We have illustrated with several applications that the benefits of using TrainSel in STP problems. These applications were mostly related to GP and GS, however, one of the major claims of this article is that the same techniques can be used for any supervised learning problem where labeling samples is the main bottleneck for obtaining the training data. We have exemplified this with two applications, one in image classification and another one related to spline regression.</p>
  </sec>
  <sec id="s5">
    <title>5. Implementation and Usage</title>
    <p>TrainSel is implemented in R with most of the code written in Rcpp. Sample usage is illustrated in the Supplementary and also in the help files within the package documentation. The source code and installation details are provided at <ext-link ext-link-type="uri" xlink:href="https://github.com/TheRocinante-lab/TrainSel">https://github.com/TheRocinante-lab/TrainSel</ext-link>.</p>
  </sec>
  <sec sec-type="data-availability" id="s6">
    <title>Data Availability Statement</title>
    <p>Publicly available datasets were analyzed in this study. This data can be found at: referenced in the article.</p>
  </sec>
  <sec id="s7">
    <title>Author Contributions</title>
    <p>DA: conception and design of the work, R and Rcpp programs, drafting the article, and critical revision of the article. JI: drafting the article and critical revision of the article. SR: critical revision of the article. All authors contributed to the article and approved the submitted version.</p>
  </sec>
  <sec sec-type="COI-statement" id="conf1">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. The reviewer RF-N declared a past co-authorship with one of the authors, DA, to the handling editor.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn fn-type="financial-disclosure">
      <p><bold>Funding.</bold> Results have been achieved within the framework of the first transnational joint call for research projects in the SusCrop ERA-Net Cofound on Sustainable Crop production, with funding from Department of Agriculture, Food and the Marine grant No.2017EN104. This project has also received funding from the European Union's Horizon 2020 research and innovation program under grant agreement No 818144, and also the Severo Ochoa Program for Centres of Excellence in R&amp;D. JI was supported by the Beatriz Galindo Program (BEAGAL18/00115) from the Ministerio de Educación y Formación Professional of Spain and the Severo Ochoa Program for Centres of Excellence in R&amp;D from the Agencia Estatal de Investigación of Spain, grant SEV-2016-0672 (2017-2021) to the CBGP.</p>
    </fn>
  </fn-group>
  <sec sec-type="supplementary-material" id="s8">
    <title>Supplementary Material</title>
    <p>The Supplementary Material for this article can be found online at: <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/articles/10.3389/fgene.2021.655287/full#supplementary-material">https://www.frontiersin.org/articles/10.3389/fgene.2021.655287/full#supplementary-material</ext-link></p>
    <supplementary-material content-type="local-data" id="SM1">
      <media xlink:href="Data_Sheet_1.PDF">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Akdemir</surname><given-names>D.</given-names></name></person-group> (<year>2017</year>). <source>STPGA: Selection of Training Populations by Genetic Algorithm. R package version 5.2.1)</source>. <pub-id pub-id-type="doi">10.1101/111989</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akdemir</surname><given-names>D.</given-names></name><name><surname>Beavis</surname><given-names>W.</given-names></name><name><surname>Fritsche-Neto</surname><given-names>R.</given-names></name><name><surname>Singh</surname><given-names>A. K.</given-names></name><name><surname>Isidro-Sánchez</surname><given-names>J.</given-names></name></person-group> (<year>2019</year>). <article-title>Multi-objective optimized genomic breeding strategies for sustainable food improvement</article-title>. <source>Heredity</source>
<volume>122</volume>, <fpage>672</fpage>–<lpage>683</lpage>. <pub-id pub-id-type="doi">10.1038/s41437-018-0147-1</pub-id><?supplied-pmid 30262841?><pub-id pub-id-type="pmid">30262841</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akdemir</surname><given-names>D.</given-names></name><name><surname>Isidro-Sánchez</surname><given-names>J.</given-names></name></person-group> (<year>2019</year>). <article-title>Design of training populations for selective phenotyping in genomic prediction</article-title>. <source>Sci. Rep</source>. <volume>9</volume>, <fpage>1</fpage>–<lpage>15</lpage>. <pub-id pub-id-type="doi">10.1038/s41598-018-38081-6</pub-id><?supplied-pmid 30723226?><pub-id pub-id-type="pmid">30626917</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akdemir</surname><given-names>D.</given-names></name><name><surname>Sánchez</surname><given-names>J. I.</given-names></name></person-group> (<year>2016</year>). <article-title>Efficient breeding by genomic mating</article-title>. <source>Front. Genet</source>. <volume>7</volume>:<fpage>210</fpage>. <pub-id pub-id-type="doi">10.3389/fgene.2016.00210</pub-id><?supplied-pmid 27965707?><pub-id pub-id-type="pmid">27965707</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akdemir</surname><given-names>D.</given-names></name><name><surname>Sanchez</surname><given-names>J. I.</given-names></name><name><surname>Jannink</surname><given-names>J.-L.</given-names></name></person-group> (<year>2015</year>). <article-title>Optimization of genomic selection training populations with a genetic algorithm</article-title>. <source>Genet. Sel. Evol</source>. <volume>47</volume>:<fpage>38</fpage>. <pub-id pub-id-type="doi">10.1186/s12711-015-0116-6</pub-id><?supplied-pmid 25943105?><pub-id pub-id-type="pmid">25943105</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Allaire</surname><given-names>J.</given-names></name><name><surname>Chollet</surname><given-names>F.</given-names></name></person-group> (<year>2018</year>). <source>keras: R Interface to'keras'. R Package Version 2.2. 0</source>.</mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Atkinson</surname><given-names>A.</given-names></name><name><surname>Donev</surname><given-names>A.</given-names></name></person-group> (<year>1992</year>). <source>Optimum Experimental Designs</source>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Clarendon</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brisbane</surname><given-names>J.</given-names></name><name><surname>Gibson</surname><given-names>J.</given-names></name></person-group> (<year>1995</year>). <article-title>Balancing selection response and rate of inbreeding by including genetic relationships in selection decisions</article-title>. <source>Theor. Appl. Genet</source>. <volume>91</volume>, <fpage>421</fpage>–<lpage>431</lpage>. <pub-id pub-id-type="doi">10.1007/BF00222969</pub-id><?supplied-pmid 24169831?><pub-id pub-id-type="pmid">24169831</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cericola</surname><given-names>F.</given-names></name><name><surname>Jahoor</surname><given-names>A.</given-names></name><name><surname>Orabi</surname><given-names>J.</given-names></name><name><surname>Andersen</surname><given-names>J. R.</given-names></name><name><surname>Janss</surname><given-names>L. L.</given-names></name><name><surname>Jensen</surname><given-names>J.</given-names></name></person-group> (<year>2017</year>). <article-title>Optimizing training population size and genotyping strategy for genomic prediction using association study results and pedigree information. a case of study in advanced wheat breeding lines</article-title>. <source>PLoS ONE</source>
<volume>12</volume>:<fpage>e0169606</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0169606</pub-id><?supplied-pmid 28081208?><pub-id pub-id-type="pmid">28081208</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>H.</given-names></name><name><surname>Xu</surname><given-names>K.</given-names></name><name><surname>Abraham</surname><given-names>K. J.</given-names></name></person-group> (<year>2020</year>). <article-title>Optimizing sequencing resources in genotyped livestock populations using linear programming</article-title>. <source>BioRxiv [Preprint]</source>. <pub-id pub-id-type="doi">10.1101/2020.06.29.179093</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Bem Oliveira</surname><given-names>I.</given-names></name><name><surname>Amadeu</surname><given-names>R. R.</given-names></name><name><surname>Ferr ao</surname><given-names>L. F. V.</given-names></name><name><surname>Mu noz</surname><given-names>P. R.</given-names></name></person-group> (<year>2020</year>). <article-title>Optimizing whole-genomic prediction for autotetraploid blueberry breeding</article-title>. <source>Heredity</source>
<volume>125</volume>, <fpage>437</fpage>–<lpage>448</lpage>. <pub-id pub-id-type="doi">10.1038/s41437-020-00357-x</pub-id><?supplied-pmid 33077896?><pub-id pub-id-type="pmid">33077896</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eddelbuettel</surname><given-names>D.</given-names></name><name><surname>François</surname><given-names>R.</given-names></name><name><surname>Allaire</surname><given-names>J.</given-names></name><name><surname>Ushey</surname><given-names>K.</given-names></name><name><surname>Kou</surname><given-names>Q.</given-names></name><name><surname>Russel</surname><given-names>N.</given-names></name><etal/></person-group>. (<year>2011</year>). <article-title>RCPP: seamless R and C++ integration</article-title>. <source>J. Stat. Softw</source>. <volume>40</volume>, <fpage>1</fpage>–<lpage>18</lpage>. <pub-id pub-id-type="doi">10.18637/jss.v040.i08</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fedorov</surname><given-names>V. V.</given-names></name></person-group> (<year>1972</year>). <source>Theory of Optimal Experiments</source>. <publisher-name>Elsevier</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fedorov</surname><given-names>V. V.</given-names></name><name><surname>Hackl</surname><given-names>P.</given-names></name></person-group> (<year>2012</year>). <source>Model-Oriented Design of Experiments, Vol. 125</source>. <publisher-name>Springer Science &amp; Business Media</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischetti</surname><given-names>M.</given-names></name><name><surname>Lodi</surname><given-names>A.</given-names></name></person-group> (<year>2010</year>). <article-title>Heuristic in mixed integer programming</article-title>. <source>Wiley Encyclop. Oper. Res. Manage. Sci</source>. <pub-id pub-id-type="doi">10.1002/9780470400531.eorms0376</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fisher</surname><given-names>R. A.</given-names></name></person-group> (<year>1960</year>). <source>The Design of Experiments</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Hafner</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Glover</surname><given-names>F. W.</given-names></name><name><surname>Kochenberger</surname><given-names>G. A.</given-names></name></person-group> (<year>2006</year>). <source>Handbook of Metaheuristics, Vol. 57</source>. <publisher-name>Springer Science &amp; Business Media</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gonen</surname><given-names>S.</given-names></name><name><surname>Ros-Freixedes</surname><given-names>R.</given-names></name><name><surname>Battagin</surname><given-names>M.</given-names></name><name><surname>Gorjanc</surname><given-names>G.</given-names></name><name><surname>Hickey</surname><given-names>J. M.</given-names></name></person-group> (<year>2017</year>). <article-title>A method for the allocation of sequencing resources in genotyped livestock populations</article-title>. <source>Genet. Select. Evol</source>. <volume>49</volume>:<fpage>47</fpage>. <pub-id pub-id-type="doi">10.1186/s12711-017-0322-5</pub-id><?supplied-pmid 28521728?><pub-id pub-id-type="pmid">28521728</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>T.</given-names></name><name><surname>Yu</surname><given-names>X.</given-names></name><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>Zhang</surname><given-names>H.</given-names></name><name><surname>Zhu</surname><given-names>C.</given-names></name><name><surname>Flint-Garcia</surname><given-names>S.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Optimal designs for genomic selection in hybrid crops</article-title>. <source>Mol. Plant</source><volume>12</volume>, <fpage>390</fpage>–<lpage>401</lpage>. <pub-id pub-id-type="doi">10.1016/j.molp.2018.12.022</pub-id><?supplied-pmid 30625380?><pub-id pub-id-type="pmid">30625380</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haines</surname><given-names>L. M.</given-names></name></person-group> (<year>1987</year>). <article-title>The application of the annealing algorithm to the construction of exact optimal designs for linear-regression models</article-title>. <source>Technometrics</source>
<volume>29</volume>, <fpage>439</fpage>–<lpage>447</lpage>. <pub-id pub-id-type="doi">10.1080/00401706.1987.10488272</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>S.</given-names></name><name><surname>Schulthess</surname><given-names>A. W.</given-names></name><name><surname>Mirdita</surname><given-names>V.</given-names></name><name><surname>Zhao</surname><given-names>Y.</given-names></name><name><surname>Korzun</surname><given-names>V.</given-names></name><name><surname>Bothe</surname><given-names>R.</given-names></name><etal/></person-group>. (<year>2016</year>). <article-title>Genomic selection in a commercial winter wheat population</article-title>. <source>Theor. Appl. Genet</source>. <volume>129</volume>, <fpage>641</fpage>–<lpage>651</lpage>. <pub-id pub-id-type="doi">10.1007/s00122-015-2655-1</pub-id><?supplied-pmid 26747048?><pub-id pub-id-type="pmid">26747048</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heslot</surname><given-names>N.</given-names></name><name><surname>Jannink</surname><given-names>J.-L.</given-names></name><name><surname>Sorrells</surname><given-names>M. E.</given-names></name></person-group> (<year>2015</year>). <article-title>Perspectives for genomic selection applications and research in plants</article-title>. <source>Crop Sci</source>. <volume>55</volume>, <fpage>1</fpage>–<lpage>12</lpage>. <pub-id pub-id-type="doi">10.2135/cropsci2014.03.0249</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Holland</surname><given-names>J. H.</given-names></name></person-group> (<year>1992</year>). <source>Adaptation in Natural and Artificial Systems: An Introductory Analysis With Applications to Biology, Control, and Artificial Intelligence</source>. <publisher-name>MIT Press</publisher-name>. <pub-id pub-id-type="doi">10.7551/mitpress/1090.001.0001</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isidro</surname><given-names>J.</given-names></name><name><surname>Jannink</surname><given-names>J.-L.</given-names></name><name><surname>Akdemir</surname><given-names>D.</given-names></name><name><surname>Poland</surname><given-names>J.</given-names></name><name><surname>Heslot</surname><given-names>N.</given-names></name><name><surname>Sorrells</surname><given-names>M. E.</given-names></name></person-group> (<year>2015</year>). <article-title>Training set optimization under population structure in genomic selection</article-title>. <source>Theor. Appl. Genet</source>. <volume>128</volume>, <fpage>145</fpage>–<lpage>158</lpage>. <pub-id pub-id-type="doi">10.1007/s00122-014-2418-4</pub-id><?supplied-pmid 25367380?><pub-id pub-id-type="pmid">25367380</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jannink</surname><given-names>J.-L.</given-names></name></person-group> (<year>2010</year>). <article-title>Dynamics of long-term genomic selection</article-title>. <source>Genet. Select. Evol</source>. <volume>42</volume>:<fpage>35</fpage>. <pub-id pub-id-type="doi">10.1186/1297-9686-42-35</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>M. E.</given-names></name><name><surname>Moore</surname><given-names>L. M.</given-names></name><name><surname>Ylvisaker</surname><given-names>D.</given-names></name></person-group> (<year>1990</year>). <article-title>Minimax and maximin distance designs</article-title>. <source>J. Stat. Plann. Infer</source>. <volume>26</volume>, <fpage>131</fpage>–<lpage>148</lpage>. <pub-id pub-id-type="doi">10.1016/0378-3758(90)90122-B</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kadam</surname><given-names>D. C.</given-names></name><name><surname>Rodriguez</surname><given-names>O. R.</given-names></name><name><surname>Lorenz</surname><given-names>A. J.</given-names></name></person-group> (<year>2021</year>). <article-title>Optimization of training sets for genomic prediction of early-stage single crosses in maize</article-title>. <source>Theor. Appl. Genet</source>. <volume>134</volume>, <fpage>687</fpage>–<lpage>699</lpage>. <pub-id pub-id-type="doi">10.1007/s00122-020-03722-w</pub-id><?supplied-pmid 33398385?><pub-id pub-id-type="pmid">33398385</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiefer</surname><given-names>J.</given-names></name></person-group> (<year>1959</year>). <article-title>Optimum experimental designs</article-title>. <source>J. R. Stat. Soc. Ser. B</source>
<volume>21</volume>, <fpage>272</fpage>–<lpage>319</lpage>. <pub-id pub-id-type="doi">10.1111/j.2517-6161.1959.tb00338.x</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kiefer</surname><given-names>J. C.</given-names></name><name><surname>Brown</surname><given-names>L.</given-names></name><name><surname>Olkin</surname><given-names>I.</given-names></name><name><surname>Sacks</surname><given-names>J.</given-names></name></person-group> (<year>1985</year>). <source>Jack Carl Kiefer Collected Papers: Design of Experiments</source>. <publisher-name>Springer</publisher-name>. <pub-id pub-id-type="doi">10.1007/978-1-4613-8505-9</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laloë</surname><given-names>D.</given-names></name></person-group> (<year>1993</year>). <article-title>Precision and information in linear models of genetic evaluation</article-title>. <source>Genet. Select. Evol</source>. <volume>25</volume>, <fpage>557</fpage>–<lpage>576</lpage>. <pub-id pub-id-type="doi">10.1186/1297-9686-25-6-557</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laloë</surname><given-names>D.</given-names></name><name><surname>Phocas</surname><given-names>F.</given-names></name></person-group> (<year>2003</year>). <article-title>A proposal of criteria of robustness analysis in genetic evaluation</article-title>. <source>Livest. Prod. Sci</source>. <volume>80</volume>, <fpage>241</fpage>–<lpage>256</lpage>. <pub-id pub-id-type="doi">10.1016/S0301-6226(02)00092-1</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>G.</given-names></name><name><surname>Zhao</surname><given-names>Y.</given-names></name><name><surname>Gowda</surname><given-names>M.</given-names></name><name><surname>Longin</surname><given-names>C. F. H.</given-names></name><name><surname>Reif</surname><given-names>J. C.</given-names></name><name><surname>Mette</surname><given-names>M. F.</given-names></name></person-group> (<year>2016</year>). <article-title>Predicting hybrid performances for quality traits through genomic-assisted approaches in central European wheat</article-title>. <source>PLoS ONE</source>
<volume>11</volume>:<fpage>e0158635</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0158635</pub-id><?supplied-pmid 27383841?><pub-id pub-id-type="pmid">27383841</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lorenz</surname><given-names>A. J.</given-names></name><name><surname>Smith</surname><given-names>K. P.</given-names></name></person-group> (<year>2015</year>). <article-title>Adding genetically distant individuals to training populations reduces genomic prediction accuracy in barley</article-title>. <source>Crop Sci</source>. <volume>55</volume>, <fpage>2657</fpage>–<lpage>2667</lpage>. <pub-id pub-id-type="doi">10.2135/cropsci2014.12.0827</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mangin</surname><given-names>B.</given-names></name><name><surname>Rincent</surname><given-names>R.</given-names></name><name><surname>Rabier</surname><given-names>C.-E.</given-names></name><name><surname>Moreau</surname><given-names>L.</given-names></name><name><surname>Goudemand-Dugue</surname><given-names>E.</given-names></name></person-group> (<year>2019</year>). <article-title>Training set optimization of genomic prediction by means of ethacc</article-title>. <source>PLoS ONE</source>
<volume>14</volume>:<fpage>e0205629</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0205629</pub-id><?supplied-pmid 30779753?><pub-id pub-id-type="pmid">30779753</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markowitz</surname><given-names>H.</given-names></name></person-group> (<year>1952</year>). <article-title>Portfolio selection</article-title>. <source>J. Fin</source>. <volume>7</volume>, <fpage>77</fpage>–<lpage>91</lpage>. <pub-id pub-id-type="doi">10.1111/j.1540-6261.1952.tb01525.x</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Markowitz</surname><given-names>H. M.</given-names></name></person-group> (<year>1968</year>). <source>Portfolio Selection: Efficient Diversification of Investments, Vol. 16</source>. <publisher-name>Yale University Press</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meuwissen</surname><given-names>T.</given-names></name><name><surname>Hayes</surname><given-names>B.</given-names></name><name><surname>Goddard</surname><given-names>M.</given-names></name></person-group> (<year>2001</year>). <article-title>Prediction of total genetic value using genome-wide dense marker maps</article-title>. <source>Genetics</source>
<volume>157</volume>, <fpage>1819</fpage>–<lpage>1829</lpage>. <?supplied-pmid 11290733?><pub-id pub-id-type="pmid">11290733</pub-id></mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mitchell</surname><given-names>T.</given-names></name></person-group> (<year>1974</year>). <article-title>An algorithm for the construction of “d-optimal” experimental designs</article-title>. <source>Technometrics</source>
<volume>16</volume>, <fpage>203</fpage>–<lpage>210</lpage>. <pub-id pub-id-type="doi">10.1080/00401706.1974.10489175</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neyhart</surname><given-names>J. L.</given-names></name><name><surname>Tiede</surname><given-names>T.</given-names></name><name><surname>Lorenz</surname><given-names>A. J.</given-names></name><name><surname>Smith</surname><given-names>K. P.</given-names></name></person-group> (<year>2017</year>). <article-title>Evaluating methods of updating training data in long-term genomewide selection</article-title>. <source>G3</source>
<volume>7</volume>, <fpage>1499</fpage>–<lpage>1510</lpage>. <pub-id pub-id-type="doi">10.1534/g3.117.040550</pub-id><?supplied-pmid 28315831?><pub-id pub-id-type="pmid">28315831</pub-id></mixed-citation>
    </ref>
    <ref id="B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>N.</given-names></name><name><surname>Miller</surname><given-names>A.</given-names></name></person-group> (<year>1992</year>). <article-title>A review of some exchange algorithms for constructing discrete d-optimal designs</article-title>. <source>Comput. Stat. Data Anal</source>. <volume>14</volume>, <fpage>489</fpage>–<lpage>498</lpage>. <pub-id pub-id-type="doi">10.1016/0167-9473(92)90064-M</pub-id></mixed-citation>
    </ref>
    <ref id="B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norman</surname><given-names>A.</given-names></name><name><surname>Taylor</surname><given-names>J.</given-names></name><name><surname>Edwards</surname><given-names>J.</given-names></name><name><surname>Kuchel</surname><given-names>H.</given-names></name></person-group> (<year>2018</year>). <article-title>Optimising genomic selection in wheat: effect of marker density, population size and population structure on prediction accuracy</article-title>. <source>G3</source>
<volume>8</volume>, <fpage>2889</fpage>–<lpage>2899</lpage>. <pub-id pub-id-type="doi">10.1534/g3.118.200311</pub-id><?supplied-pmid 29970398?><pub-id pub-id-type="pmid">29970398</pub-id></mixed-citation>
    </ref>
    <ref id="B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olatoye</surname><given-names>M. O.</given-names></name><name><surname>Clark</surname><given-names>L. V.</given-names></name><name><surname>Labonte</surname><given-names>N. R.</given-names></name><name><surname>Dong</surname><given-names>H.</given-names></name><name><surname>Dwiyanti</surname><given-names>M. S.</given-names></name><name><surname>Anzoua</surname><given-names>K. G.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>Training population optimization for genomic selection in miscanthus</article-title>. <source>G3</source><volume>10</volume>, <fpage>2465</fpage>–<lpage>2476</lpage>. <pub-id pub-id-type="doi">10.1534/g3.120.401402</pub-id><?supplied-pmid 32457095?><pub-id pub-id-type="pmid">32457095</pub-id></mixed-citation>
    </ref>
    <ref id="B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ou</surname><given-names>J.-H.</given-names></name><name><surname>Liao</surname><given-names>C.-T.</given-names></name></person-group> (<year>2019</year>). <article-title>Training set determination for genomic selection</article-title>. <source>Theor. Appl. Genet</source>. <volume>132</volume>, <fpage>2781</fpage>–<lpage>2792</lpage>. <pub-id pub-id-type="doi">10.1007/s00122-019-03387-0</pub-id><?supplied-pmid 31267147?><pub-id pub-id-type="pmid">31267147</pub-id></mixed-citation>
    </ref>
    <ref id="B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pukelsheim</surname><given-names>F.</given-names></name><name><surname>Rosenberger</surname><given-names>J.</given-names></name></person-group> (<year>1993</year>). <article-title>Experimental designs for model discrimination</article-title>. <source>J. Am. Stat. Assoc</source>. <volume>88</volume>, <fpage>642</fpage>–<lpage>649</lpage>. <pub-id pub-id-type="doi">10.1080/01621459.1993.10476317</pub-id></mixed-citation>
    </ref>
    <ref id="B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rincent</surname><given-names>R.</given-names></name><name><surname>Laloë</surname><given-names>D.</given-names></name><name><surname>Nicolas</surname><given-names>S.</given-names></name><name><surname>Altmann</surname><given-names>T.</given-names></name><name><surname>Brunel</surname><given-names>D.</given-names></name><name><surname>Revilla</surname><given-names>P.</given-names></name><etal/></person-group>. (<year>2012</year>). <article-title>Maximizing the reliability of genomic selection by optimizing the calibration set of reference individuals: comparison of methods in two diverse groups of maize inbreds (<italic>Zea mays</italic> L.)</article-title>. <source>Genetics</source><volume>192</volume>, <fpage>715</fpage>–<lpage>728</lpage>. <pub-id pub-id-type="doi">10.1534/genetics.112.141473</pub-id><?supplied-pmid 22865733?><pub-id pub-id-type="pmid">22865733</pub-id></mixed-citation>
    </ref>
    <ref id="B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruppert</surname><given-names>D.</given-names></name></person-group> (<year>2002</year>). <article-title>Selecting the number of knots for penalized splines</article-title>. <source>J. Comput. Graph. Stat</source>. <volume>11</volume>, <fpage>735</fpage>–<lpage>757</lpage>. <pub-id pub-id-type="doi">10.1198/106186002853</pub-id></mixed-citation>
    </ref>
    <ref id="B47">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Silvey</surname><given-names>S.</given-names></name></person-group> (<year>2013</year>). <source>Optimal Design: An Introduction to the Theory for Parameter Estimation, Vol. 1</source>. <publisher-name>Springer Science &amp; Business Media</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>K.</given-names></name></person-group> (<year>1918</year>). <article-title>On the standard deviations of adjusted and interpolated values of an observed polynomial function and its constants and the guidance they give towards a proper choice of the distribution of observations</article-title>. <source>Biometrika</source>
<volume>12</volume>, <fpage>1</fpage>–<lpage>85</lpage>. <pub-id pub-id-type="doi">10.1093/biomet/12.1-2.1</pub-id></mixed-citation>
    </ref>
    <ref id="B49">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wheeler</surname><given-names>B.</given-names></name></person-group> (<year>2004</year>). <source>Algdesign. The R Project for Statistical Computing</source>.</mixed-citation>
    </ref>
    <ref id="B50">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>X.</given-names></name><name><surname>Leiboff</surname><given-names>S.</given-names></name><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>Guo</surname><given-names>T.</given-names></name><name><surname>Ronning</surname><given-names>N.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>Genomic prediction of maize microphenotypes provides insights for optimizing selection and mining diversity</article-title>. <source>Plant Biotechnol. J</source>. <volume>18</volume>, <fpage>2456</fpage>–<lpage>2465</lpage>. <pub-id pub-id-type="doi">10.1111/pbi.13420</pub-id><?supplied-pmid 32452105?><pub-id pub-id-type="pmid">32452105</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
