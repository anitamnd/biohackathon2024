<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with OASIS Tables with MathML3 v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-archive-oasis-article1-mathml3.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jpoasis-nisons2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Med Phys</journal-id>
    <journal-id journal-id-type="iso-abbrev">Med Phys</journal-id>
    <journal-id journal-id-type="doi">10.1002/(ISSN)2473-4209</journal-id>
    <journal-id journal-id-type="publisher-id">MP</journal-id>
    <journal-title-group>
      <journal-title>Medical Physics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">0094-2405</issn>
    <issn pub-type="epub">2473-4209</issn>
    <publisher>
      <publisher-name>John Wiley and Sons Inc.</publisher-name>
      <publisher-loc>Hoboken</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6899669</article-id>
    <article-id pub-id-type="pmid">31389023</article-id>
    <article-id pub-id-type="doi">10.1002/mp.13753</article-id>
    <article-id pub-id-type="publisher-id">MP13753</article-id>
    <article-categories>
      <subj-group subj-group-type="overline">
        <subject>Technical Note</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>QUANTITATIVE IMAGING AND IMAGE PROCESSING</subject>
        <subj-group subj-group-type="heading">
          <subject>Technical Notes</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Technical Note: PYRO‐NN: Python reconstruction operators in neural networks</article-title>
      <alt-title alt-title-type="right-running-head">PYRO‐NN</alt-title>
      <alt-title alt-title-type="left-running-head">Syben <italic>et al</italic>.</alt-title>
    </title-group>
    <contrib-group>
      <contrib id="mp13753-cr-0001" contrib-type="author" corresp="yes">
        <name>
          <surname>Syben</surname>
          <given-names>Christopher</given-names>
        </name>
        <xref ref-type="aff" rid="mp13753-aff-0001">
          <sup>1</sup>
        </xref>
        <address>
          <email>christopher.syben@fau.de</email>
        </address>
      </contrib>
      <contrib id="mp13753-cr-0002" contrib-type="author">
        <name>
          <surname>Michen</surname>
          <given-names>Markus</given-names>
        </name>
        <xref ref-type="aff" rid="mp13753-aff-0001">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib id="mp13753-cr-0003" contrib-type="author">
        <name>
          <surname>Stimpel</surname>
          <given-names>Bernhard</given-names>
        </name>
        <xref ref-type="aff" rid="mp13753-aff-0001">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib id="mp13753-cr-0004" contrib-type="author">
        <name>
          <surname>Seitz</surname>
          <given-names>Stephan</given-names>
        </name>
        <xref ref-type="aff" rid="mp13753-aff-0001">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib id="mp13753-cr-0005" contrib-type="author">
        <name>
          <surname>Ploner</surname>
          <given-names>Stefan</given-names>
        </name>
        <xref ref-type="aff" rid="mp13753-aff-0001">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib id="mp13753-cr-0006" contrib-type="author">
        <name>
          <surname>Maier</surname>
          <given-names>Andreas K.</given-names>
        </name>
        <xref ref-type="aff" rid="mp13753-aff-0001">
          <sup>1</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="mp13753-aff-0001">
      <label>
        <sup>1</sup>
      </label>
      <named-content content-type="organisation-division">Pattern Recognition Lab</named-content>
      <institution>Friedich‐Alexander Universität Erlangen‐Nürnberg</institution>
      <postal-code>91058</postal-code>
      <city>Erlangen</city>
      <country country="DE">Germany</country>
    </aff>
    <author-notes>
      <corresp id="correspondenceTo"><label>*</label>Author to whom correspondence should be addressed. Electronic mail: <email>christopher.syben@fau.de</email>.</corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>27</day>
      <month>8</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <month>11</month>
      <year>2019</year>
    </pub-date>
    <volume>46</volume>
    <issue>11</issue>
    <issue-id pub-id-type="doi">10.1002/mp.v46.11</issue-id>
    <fpage>5110</fpage>
    <lpage>5115</lpage>
    <history>
      <date date-type="received">
        <day>30</day>
        <month>4</month>
        <year>2019</year>
      </date>
      <date date-type="rev-recd">
        <day>23</day>
        <month>7</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>24</day>
        <month>7</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <!--<copyright-statement content-type="issue-copyright"> Copyright &#x000a9; 2019 American Association of Physicists in Medicine <copyright-statement>-->
      <copyright-statement content-type="article-copyright">© 2019 The Authors. Medical Physics published by Wiley Periodicals, Inc. on behalf of American Association of Physicists in Medicine</copyright-statement>
      <license license-type="creativeCommonsBy">
        <license-p>This is an open access article under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link> License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="file:MP-46-5110.pdf"/>
    <abstract>
      <sec id="mp13753-sec-0001">
        <title>Purpose</title>
        <p>Recently, several attempts were conducted to transfer deep learning to medical image reconstruction. An increasingly number of publications follow the concept of embedding the computed tomography (CT) reconstruction as a known operator into a neural network. However, most of the approaches presented lack an efficient CT reconstruction framework fully integrated into deep learning environments. As a result, many approaches use workarounds for mathematically unambiguously solvable problems.</p>
      </sec>
      <sec id="mp13753-sec-0002">
        <title>Methods</title>
        <p>PYRO‐NN is a generalized framework to embed known operators into the prevalent deep learning framework Tensorflow. The current status includes state‐of‐the‐art parallel‐, fan‐, and cone‐beam projectors, and back‐projectors accelerated with CUDA provided as Tensorflow layers. On top, the framework provides a high‐level Python API to conduct FBP and iterative reconstruction experiments with data from real CT systems.</p>
      </sec>
      <sec id="mp13753-sec-0003">
        <title>Results</title>
        <p>The framework provides all necessary algorithms and tools to design end‐to‐end neural network pipelines with integrated CT reconstruction algorithms. The high‐level Python API allows a simple use of the layers as known from Tensorflow. All algorithms and tools are referenced to a scientific publication and are compared to existing non‐deep learning reconstruction frameworks. To demonstrate the capabilities of the layers, the framework comes with baseline experiments, which are described in the supplementary material. The framework is available as open‐source software under the Apache 2.0 licence at <ext-link ext-link-type="uri" xlink:href="https://github.com/csyben/PYRO-NN">https://github.com/csyben/PYRO-NN</ext-link>.</p>
      </sec>
      <sec id="mp13753-sec-0004">
        <title>Conclusions</title>
        <p>PYRO‐NN comes with the prevalent deep learning framework Tensorflow and allows to setup end‐to‐end trainable neural networks in the medical image reconstruction context. We believe that the framework will be a step toward reproducible research and give the medical physics community a toolkit to elevate medical image reconstruction with new deep learning techniques.</p>
      </sec>
    </abstract>
    <kwd-group kwd-group-type="author-generated">
      <kwd id="mp13753-kwd-0001">inverse problems</kwd>
      <kwd id="mp13753-kwd-0002">known operator learning</kwd>
      <kwd id="mp13753-kwd-0003">machine learning</kwd>
      <kwd id="mp13753-kwd-0004">open source</kwd>
      <kwd id="mp13753-kwd-0005">reconstruction</kwd>
    </kwd-group>
    <funding-group>
      <award-group id="funding-0001">
        <funding-source>
          <institution-wrap>
            <institution>European Research Council </institution>
            <institution-id institution-id-type="open-funder-registry">10.13039/100010663</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>810316</award-id>
      </award-group>
      <award-group id="funding-0002">
        <funding-source>Friedrich‐Alexander University Erlangen‐Nürnberg</funding-source>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="1"/>
      <table-count count="0"/>
      <page-count count="6"/>
      <word-count count="4390"/>
    </counts>
    <custom-meta-group>
      <custom-meta>
        <meta-name>source-schema-version-number</meta-name>
        <meta-value>2.0</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>cover-date</meta-name>
        <meta-value>November 2019</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>details-of-publishers-convertor</meta-name>
        <meta-value>Converter:WILEY_ML3GV2_TO_JATSPMC version:5.7.2 mode:remove_FC converted:05.12.2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body id="mp13753-body-0001">
  <sec id="mp13753-sec-0005">
    <label>1</label>
    <title>Introduction</title>
    <p>In recent years, major breakthroughs made deep learning increasingly prevalent in more and more fields. It revolutionizes the way of classification and regression tasks in speech and image recognition<xref rid="mp13753-bib-0001" ref-type="ref">1</xref>, <xref rid="mp13753-bib-0002" ref-type="ref">2</xref>, <xref rid="mp13753-bib-0003" ref-type="ref">3</xref> and many other areas. Even in the medical domain, where interpretability and reliability are one of the most important driving forces, deep learning has led to astonishing results.<xref rid="mp13753-bib-0004" ref-type="ref">4</xref> One of the most cited papers of recent years is the U‐net<xref rid="mp13753-bib-0005" ref-type="ref">5</xref> which outperforms classical machine learning algorithms in segmentation tasks. In the subsequent time, the U‐net architecture emerged to many more tasks, for example, artifact correction, image fusion, image‐to‐image translation, and even into the context of medical image reconstruction.<xref rid="mp13753-bib-0006" ref-type="ref">6</xref>, <xref rid="mp13753-bib-0007" ref-type="ref">7</xref>, <xref rid="mp13753-bib-0008" ref-type="ref">8</xref> However, this domain is fundamentally different from those in which the advent of deep learning began, and the question arises as to whether these learned signal reconstruction pipelines are reliable and stable enough for a critical area such as medical imaging.<xref rid="mp13753-bib-0009" ref-type="ref">9</xref> Two special issues: “<italic>Deep learning in medical imaging</italic>”<xref rid="mp13753-bib-0010" ref-type="ref">10</xref> and “<italic>Machine Learning for Image Reconstruction</italic>”<xref rid="mp13753-bib-0011" ref-type="ref">11</xref> in transactions on medical imaging (TMI) in 2016 and 2018 discuss the increasing relevance of deep learning methods in medical image reconstruction.</p>
    <p>The presented approaches can be divided into either pre‐ or post‐processing approaches or fully end‐to‐end trained methods. For the first type, the actual reconstruction pipeline is based on well known signal reconstruction algorithms omitting the end‐to‐end capability due to its complexity. For the second type of approaches, the modeling of the end‐to‐end pipeline can be realized under two different paradigms. One way is to learn the whole signal processing pipeline, an exceptionally clear representative of this paradigm is <italic>AUTOMAP</italic>.<xref rid="mp13753-bib-0012" ref-type="ref">12</xref> Directly in contrast to this is the emerging paradigm of embedding known operators.<xref rid="mp13753-bib-0013" ref-type="ref">13</xref> This preserves the end‐to‐end learning capability but includes the known operations of the reconstruction chain to preserve the credibility of the signals, reduce the error bound of the learning process and decrease the number of parameters and thus the amount of necessary training data. This paradigm gets increasingly popular, with multiple publications following the way of embedding known operators in the computed tomography (CT) context and successfully including the CT reconstruction as known operators into the network architecture to be able to benefit from the end‐to‐end training capability of deep learning.<xref rid="mp13753-bib-0014" ref-type="ref">14</xref>, <xref rid="mp13753-bib-0015" ref-type="ref">15</xref>, <xref rid="mp13753-bib-0016" ref-type="ref">16</xref>, <xref rid="mp13753-bib-0017" ref-type="ref">17</xref>, <xref rid="mp13753-bib-0018" ref-type="ref">18</xref>, <xref rid="mp13753-bib-0019" ref-type="ref">19</xref>, <xref rid="mp13753-bib-0020" ref-type="ref">20</xref>, <xref rid="mp13753-bib-0021" ref-type="ref">21</xref> However, the publications that follow this path are still less represented than those that use deep learning only as pre‐ or post‐processing. We believe that a major reason for this is the non‐trivial implementation of known operators in existing deep learning frameworks. Even publications that successfully take on this challenge often refer to their own implementations as prototypical<xref rid="mp13753-bib-0015" ref-type="ref">15</xref> or provide frameworks on abstract wrapped levels.<xref rid="mp13753-bib-0016" ref-type="ref">16</xref>, <xref rid="mp13753-bib-0022" ref-type="ref">22</xref> An efficient and publicly usable solution integrated into one of the popular deep learning frameworks, however, remains pending.</p>
    <p>To strengthen the paradigm of known operators, elaborate the research in the medical image reconstruction, and to avoid reimplementations and incompatibilities, we started to work on an open source software framework PYRO‐NN, which allow an easy way to integrate known algorithms into the deep learning framework Tensorflow.<xref rid="mp13753-bib-0023" ref-type="ref">23</xref> We provide multiple forward and backward projectors for CT implemented in CUDA based on scientific publications supported with a high‐level Python API for simple use of state‐of‐the‐art CT reconstruction, even from different setups of real CT scanners. The profound integration into Tensorflow on C++/Cuda level allows to handle occurring performance and memory issues and, additionally, allows an easy customization of the algorithms compared to a wrapper alternative like.<xref rid="mp13753-bib-0022" ref-type="ref">22</xref>, <xref rid="mp13753-bib-0024" ref-type="ref">24</xref> Furthermore, the high‐level Python API offers an easy link between deep learning and community driven frameworks. For the CT domain this allows to use a wide range of tools (e.g., filter, redundancy weights, etc.).<xref rid="mp13753-bib-0024" ref-type="ref">24</xref>, <xref rid="mp13753-bib-0025" ref-type="ref">25</xref>, <xref rid="mp13753-bib-0026" ref-type="ref">26</xref>, <xref rid="mp13753-bib-0027" ref-type="ref">27</xref>
</p>
    <p>We believe that this framework will help the community leverage the power of end‐to‐end training of machine learning algorithms directly from the data, while continuing to apply mathematically sound solutions to uniquely solvable problems.</p>
  </sec>
  <sec id="mp13753-sec-0006">
    <label>2</label>
    <title>Materials and Methods</title>
    <p>The framework concept is designed to include native C++ and CUDA based algorithms into the deep learning framework Tensorflow. In detail, PYRO‐NN provides network layers as CUDA implementations to generate parallel‐, fan‐, and cone‐beam x‐ray projections and to reconstruct them within any neural network constructed with Tensorflow. Due to the nature of the projection and reconstruction operation, we intrinsically provide the analytical gradients for all of these layers with respect to their inputs, which allows fully end‐to‐end trainable networks. Furthermore, with PYRO‐NN we provide filters and weights based on scientific publications to allow proper filtered‐backprojection (FBP) reconstructions. The PYRO‐NN API is inspired by the CONRAD<xref rid="mp13753-bib-0026" ref-type="ref">26</xref> framework to adapt the ability to reconstruct data from real clinical scanners and by using PyConrad<xref rid="mp13753-bib-0027" ref-type="ref">27</xref> many more tools and phantoms can easily be used in the deep learning context. The current state of the framework features a CT reconstruction pipeline, while the basic design allows to transfer the whole concept to other signal reconstruction domains within one framework and, therefore, points out a direction to future development and community contribution.</p>
    <sec id="mp13753-sec-0007">
      <label>2.A.</label>
      <title>Software design/rationale</title>
      <p>The development speed in the deep learning community is tremendous. Like in the research itself, the toolkits and frameworks are developing in the same speed, which often causes conflicts in interoperability of self‐developed solutions and version mismatches between different frameworks and toolkits. To ensure a robust version control, the framework is directly included into the building process of the Tensorflow sources.</p>
      <sec id="mp13753-sec-0008">
        <label>2.A.1.</label>
        <title>PYRO‐NN‐layers</title>
        <p>The known operators can be implemented as CUDA kernels with an additional C++ class following the design of the Tensorflow API for the embedding as a Tensorflow layer. Unlike other frameworks that simply wrap the implementation at the Python level, this provides the advantage of full control over device resources such as memory utilization and implementation efficiency. The separation of the operator implementation as a native CUDA kernel and the information control allows an easy extension towards other deep learning frameworks. The integration of PyTorch is planned for the future. The integration of known operators can be found under: <ext-link ext-link-type="uri" xlink:href="https://github.com/csyben/PYRO-NN-Layers">https://github.com/csyben/PYRO-NN-Layers</ext-link>.</p>
      </sec>
      <sec id="mp13753-sec-0009">
        <label>2.A.2.</label>
        <title>PYRO‐NN</title>
        <p>We provide a high‐level Python API to allow a convenient use of the known operators as normal Tensorflow layers and offers additional helper functions. The provided Python package automatically invokes the relevant algorithms to compute the gradient with respect to the input of the layer in an efficient way. The provision of the gradient is a necessity to enable a gradient flow through the entire network and, thus, allow fully end‐to‐end trainable networks with known operators. The package can be installed via <italic>pip</italic> or from: <ext-link ext-link-type="uri" xlink:href="https://github.com/csyben/PYRO-NN">https://github.com/csyben/PYRO-NN</ext-link>.</p>
        <p>All together, these rationales offer the community with a generic, version stable, framework to easily include known operators into neural networks. The source code is publicly available under the Apache 2.0 licence to be directly compatible with Tensorflow and to allow uncomplicated community contributions to existing projects. A detailed description of the software architecture and the build process can be found in the supplementary material Section <xref rid="mp13753-sec-0005" ref-type="sec">1</xref>.</p>
      </sec>
    </sec>
    <sec id="mp13753-sec-0010">
      <label>2.B.</label>
      <title>CT reconstruction in neural networks</title>
      <p>Based on the generic design of the framework, the current state provides all necessary algorithms and tools for analytical parallel‐, fan‐, and cone‐beam reconstruction. The necessary algorithms are implemented within Tensorflow as an own layer, while the respective tools, for example, filter, weights, etc., are provided on the Python level to supply a high‐level API for CT reconstruction. In the following, we introduce the mapping of the known operator to a layer for our case study of CT reconstruction, followed by a description of the provided algorithms and tools.</p>
      <sec id="mp13753-sec-0011">
        <label>2.B.1.</label>
        <title>The known operator</title>
        <p>For the task of reconstructing object information from acquired x‐ray projections, an efficient analytical method is well known and is called FBP. To embed these methods into a neural network, the whole acquisition and reconstruction procedure of a CT system needs to be described with discrete linear algebra to embed them into a neural network. The acquisition of projection data of the object can be described with <disp-formula id="mp13753-disp-0001"><label>(1)</label><mml:math id="nlm-math-1"><mml:mrow><mml:mi mathvariant="bold">Ax</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold">p</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <bold>A</bold> is the matrix describing the geometry, the so called system matrix which can be algorithmically implemented as the forward‐projection operator. The object is denoted by <bold>x</bold> and <bold>p</bold> are the acquired projections of object <bold>x</bold> under the system described by <bold>A</bold>. The reconstruction according to the FBP algorithm can be conducted using the Moore‐Penrose pseudoinverse for the system matrix <mml:math id="nlm-math-2"><mml:mrow><mml:msup><mml:mi mathvariant="bold">A</mml:mi><mml:mi>⊤</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">AA</mml:mi><mml:mi>⊤</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math> which gives: <disp-formula id="mp13753-disp-0002"><label>(2)</label><mml:math id="nlm-math-3"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold">A</mml:mi><mml:mi>⊤</mml:mi></mml:msup><mml:msup><mml:mi mathvariant="bold">F</mml:mi><mml:mi mathvariant="bold">H</mml:mi></mml:msup><mml:mrow><mml:mi mathvariant="bold">K</mml:mi><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <mml:math id="nlm-math-4"><mml:msup><mml:mi mathvariant="bold">A</mml:mi><mml:mi>⊤</mml:mi></mml:msup></mml:math> is the adjoint system matrix which can algorithmically implemented as the back‐projection operator. According to the FBP, the inverse bracket describes a filter operation, which is conducted by a multiplication with the diagonal filter matrix <bold>K</bold> in the Fourier domain. Consequently <bold>F</bold>, <mml:math id="nlm-math-5"><mml:msup><mml:mi mathvariant="bold">F</mml:mi><mml:mi mathvariant="bold">H</mml:mi></mml:msup></mml:math> is the Fourier transform and the respective adjoint, that is, inverse operation. Hence, the forward and backward model can be expressed completely as discrete linear algebra, allowing fully end‐to‐end trainable networks. As the publications from Würfl and Syben et al. show that <bold>A</bold> and <mml:math id="nlm-math-6"><mml:msup><mml:mi mathvariant="bold">A</mml:mi><mml:mi>⊤</mml:mi></mml:msup></mml:math> are their respective operators to calculate the gradient, therefore the gradient flow through these layers can be ensured.<xref rid="mp13753-bib-0017" ref-type="ref">17</xref>, <xref rid="mp13753-bib-0019" ref-type="ref">19</xref>
</p>
      </sec>
      <sec id="mp13753-sec-0012">
        <label>2.B.2.</label>
        <title>The operator as a layer</title>
        <p>From iterative reconstruction, it is known that the system matrix is usually too large to store in memory; therefore, we compute the operator on the fly using ray‐based algorithms. There are several ways for the computation. We introduce the <italic>ray‐driven forward‐projection</italic> and the <italic>voxel‐driven back‐projection</italic> algorithmically as native CUDA kernels for the integration into Tensorflow. Note that when using a ray‐driven forward‐projection algorithm to compute the result of the multiplication with <bold>A</bold>, then the voxel‐driven back‐projection algorithm is not the respective adjoint operation <mml:math id="nlm-math-7"><mml:msup><mml:mi mathvariant="bold">A</mml:mi><mml:mi>⊤</mml:mi></mml:msup></mml:math>. They are a so called an unmatched projector‐/back‐projector pair. The implications of matched projectors and shear‐warp projectors on the convergence and runtime are subject to future work and are briefly discussed in Section <xref rid="mp13753-sec-0020" ref-type="sec">3</xref>.</p>
        <p>The <italic>forward‐projection</italic> to generate projections from the input volume are implemented as CUDA kernels in a ray driven manner. For each detector pixel, a ray <mml:math id="nlm-math-8"><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math> is cast through the scene, accumulating the absorption values along the line. We provide forward projectors for two‐dimensional (2D) parallel‐ and fan‐beam geometry based on ray vectors and respective geometry parameters. Furthermore, a three‐dimensional (3D) cone‐beam forward projector based on projection matrices is implemented according to Galigekere et al.<xref rid="mp13753-bib-0028" ref-type="ref">28</xref> The CUDA kernels are parallelized over the detector pixels computing the line integral along the ray.</p>
        <p>The <italic>back‐projection</italic> operators to reconstruct simulated or real projection data are implemented as CUDA kernels in a voxel‐driven manner. For each pixel/voxel to be reconstructed, the projection of the point on all projection images is accumulated. The framework provides the respective 2D parallel‐ and fan‐beam back‐projection algorithms based on geometry parameters and ray vectors. Following the forward projection, the 3D cone‐beam back‐projection is based on projection matrices according to Scherl et al.<xref rid="mp13753-bib-0029" ref-type="ref">29</xref> Note that for runtime efficiency, the distance weighting for the cone‐beam circular trajectory geometry is included within the kernel. Currently, only circular trajectories are supported by default. Thus, we recommend adapting the projectors for special reconstruction accordingly. The back‐projection kernel is parallelized over the voxels projecting the respective position on the different detector coordinates interpolating the measured line integral.</p>
        <p>For the 3D cone‐beam case, the framework offers the possibility to choose between a texture and a kernel interpolation mode. While texture interpolation is associated with very short computing times, Tensorflow’s memory management in combination with CUDA implies that the data must be kept twice in memory. For kernel interpolation, the situation is exactly the other way around, the computations are slower but no additional memory is needed. As both options are provided the user can decide on a per application bases. Furthermore, as the 3D cone‐beam operators are based on projection matrices, calibrated matrices from real systems can be used as shown in the CONRAD framework.<xref rid="mp13753-bib-0026" ref-type="ref">26</xref>
</p>
      </sec>
    </sec>
    <sec id="mp13753-sec-0013">
      <label>2.C.</label>
      <title>High‐level python API</title>
      <p>To supply the community with an easy‐to‐use version of the described layers, we provide the necessary structure and additional tools like filters, weights, phantoms, etc., within the Python framework. In the following, the outline of the necessary structure to utilize the layers is shown, followed by a short introduction of the provided tools.</p>
      <sec id="mp13753-sec-0014">
        <label>2.C.1.</label>
        <title>Reconstruction and geometry</title>
        <p>The high‐level Python API wraps the provided reconstruction layers in Tensorflow. Thus, the framework registers the respective adjoint operation for the gradient computation automatically. All attributes necessary for the provided forward‐projection and backward‐projection layers are covered with a base geometry class and corresponding specialized derived geometry classes, for example, cone‐beam geometry class dependent on projection matrices.</p>
      </sec>
      <sec id="mp13753-sec-0015">
        <label>2.C.2.</label>
        <title>Phantoms</title>
        <p>PYRO‐NN contains a set of simple geometric objects, for example, circle, ellipsoid, sphere, and rectangles to easily create a more complex numerical phantom. Furthermore, the framework provides an analytical description of the 2D Shepp–Logan phantom<xref rid="mp13753-bib-0030" ref-type="ref">30</xref> as well as a 3D extension of it based on the CONRAD implementation.<xref rid="mp13753-bib-0026" ref-type="ref">26</xref>
</p>
      </sec>
      <sec id="mp13753-sec-0016">
        <label>2.C.3.</label>
        <title>Trajectories</title>
        <p>The trajectory describes the geometric scanner setup over the whole scan. For the 2D parallel‐ and fan‐beam cases, the trajectory is described by the central ray vector for each projection. For the 3D cone‐beam case, the trajectory is described by a set of projection matrices, which allows to use calibrated projection matrices from real scanner systems. Within the high‐level Python API, we provide basic methods to compute the respective rays or projection matrices based on a given geometry. The open‐source concept of the whole framework allows to contribute to the diversity of provided trajectories.</p>
      </sec>
      <sec id="mp13753-sec-0017">
        <label>2.C.4.</label>
        <title>Filters</title>
        <p>To allow a basic reconstruction in the context of neural networks, PYRO‐NN provides the Ramp and Ram‐Lak filter implemented according to Kak and Slaney.<xref rid="mp13753-bib-0031" ref-type="ref">31</xref> The filters can be directly assigned as weights to a multiplication layer and are a multiplication with a diagonal matrix in the Fourier domain as shown in Eq. <xref rid="mp13753-disp-0002" ref-type="disp-formula">(2)</xref>.</p>
      </sec>
      <sec id="mp13753-sec-0018">
        <label>2.C.5.</label>
        <title>Correction weights</title>
        <p>In order to support fan‐ and cone‐beam reconstructions for the short‐scan case, the framework contains geometric and redundancy correction weights implemented according to Kak and Slaney.<xref rid="mp13753-bib-0031" ref-type="ref">31</xref>
</p>
      </sec>
      <sec id="mp13753-sec-0019">
        <label>2.C.6.</label>
        <title>Network architectures</title>
        <p>Following the paradigm of precision learning,<xref rid="mp13753-bib-0013" ref-type="ref">13</xref> different network architectures can be setup or even derived as shown by Syben et al.<xref rid="mp13753-bib-0021" ref-type="ref">21</xref> We provide various examples within the framework to assist users in using the framework. The experiments in the supplementary material cover a baseline network able to reconstruct a short‐scan cone‐beam CT according to the Feldkamp–Davis–Kress (FDK) algorithm<xref rid="mp13753-bib-0032" ref-type="ref">32</xref> (Section <xref rid="mp13753-sec-0007" ref-type="sec">2.A</xref>), a reconstruction with raw‐data and projection matrices from a real system (Section <xref rid="mp13753-sec-0010" ref-type="sec">2.B</xref>), an example of learning the correct reconstruction filter discretization (Section <xref rid="mp13753-sec-0013" ref-type="sec">2.C</xref>) and a novel baseline network to perform iterative reconstruction within few lines of code (Fig. <xref rid="mp13753-fig-0001" ref-type="fig">1</xref>; Section <xref rid="mp13753-sec-0014" ref-type="sec">2.D</xref>). A detailed description of the experiments can be found in the supplementary material Section 2.A–2.D. In addition, executable experiments are made available online as a Code Ocean Capsule.<xref rid="mp13753-bib-0033" ref-type="ref">33</xref>
</p>
        <fig fig-type="Figure" xml:lang="en" id="mp13753-fig-0001" orientation="portrait" position="float">
          <label>Figure 1</label>
          <caption>
            <p> PYRO‐NN iterative reconstruction network. The training procedure solves: <mml:math id="nlm-math-9"><mml:mrow><mml:mtext>min</mml:mtext><mml:mspace width="4pt"/><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:mi mathvariant="bold">TV</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math>. The seeked reconstruction is achieved when the optimal training state is reached. [Color figure can be viewed at <ext-link ext-link-type="uri" xlink:href="http://wileyonlinelibrary.com">http://wileyonlinelibrary.com</ext-link>]</p>
          </caption>
          <graphic id="nlm-graphic-1" xlink:href="MP-46-5110-g001"/>
        </fig>
      </sec>
    </sec>
  </sec>
  <sec id="mp13753-sec-0020">
    <label>3</label>
    <title>Discussion</title>
    <p>Recently, there have been several different attempts to transfer the astonishing capability of deep learning into the field of medical image reconstruction. In order to transfer deep learning toward medical image reconstruction and at the same time address these problems, the idea of embedding known operators into the neural networks is increasingly pursued as the growing number of publications shows.</p>
    <p>In this paper, we present a framework providing the known operators for CT reconstruction and all necessary tools to conduct experiments on real scenarios. We believe that such an open‐source framework will reduce the barriers of such approaches and will elevate the research in the medical image reconstruction domain. To encourage the research, we provide baseline experiments in the supplementary material and example code within the framework, allow an starting point for own research ideas.</p>
    <p>To the best of our knowledge, PYRO‐NN is the first framework that provides CT reconstruction algorithms as native CUDA kernels within neural networks. This allows full control over the device resources in contrast to CT algorithms wrapped on Python level. We choose to implement the projector and back‐projector as a unmatched projector pair. The implications of unmatched pairs are already analyzed in the context of iterative reconstruction by Zeng et al.<xref rid="mp13753-bib-0034" ref-type="ref">34</xref> Zeng et al. concluded that unmatched‐pairs can be beneficial due to the algorithmic speedup, while the convergence of the algorithm has to be kept in mind. While we have not noticed negative effects on the training process in our experiments,<xref rid="mp13753-bib-0019" ref-type="ref">19</xref>, <xref rid="mp13753-bib-0021" ref-type="ref">21</xref> we want to investigate the implications of unmatched‐projector pairs to the training procedure in future work.</p>
    <p>As the combination of deep neural networks and CT reconstruction can, especially in the 3D case, easily exceeds the GPU’s memory, the provided algorithms allow the user a trade‐off choice between computational‐ and memory efficient implementations. Furthermore, the concept of the framework enables a problem‐specific solution, since the algorithms and the gradients can be changed by the user at any time. Additionally, as the core of PYRO‐NN is an extension of the existing Tensorflow build process, every known operator which allows the calculation of sub‐gradients can easily be modeled as a Tensorflow layer. Besides the actual CUDA implementation, there is only the need of an information control class following the Tensorflow API guidelines. Therefore, the setup allows an easy extension toward other frameworks like PyTorch as the CUDA kernel implementation of the known operator stays untouched.</p>
    <p>We provide the known operators for CT reconstruction on CUDA level with the respective necessary tools like filters and weights on Python level. Nevertheless, the framework design allows an easy extension to other fields, for example, magnetic resonance imaging (MRI) and many more. With the increasing amount of publications being supplemented by open‐source reference implementations, we believe that with help of the community PYRO‐NN can grow beyond the application on CT reconstruction.</p>
  </sec>
  <sec id="mp13753-sec-0021">
    <label>4</label>
    <title>Conclusion and Outlook</title>
    <p>PYRO‐NN is an open‐source software framework developed to elevate the use of known operators within neural networks. This allows to transfer the power of deep learning to medical image reconstruction while making use of existing knowledge about the physical principles. Currently, the framework provides state‐of‐the‐art CT reconstruction algorithms within the Tensorflow deep learning environment, supported by the necessary tools for the reconstruction pipeline. This allows to use existing CT reconstruction algorithms in combination with neural networks in an end‐to‐end trainable fashion. The generic design of the framework makes it easy to extend it to other modalities. We hope that our open‐source framework will encourage other groups to join these efforts making the framework a valuable element in the deep learning medical image reconstruction field. The main objective of the framework is to enable the community to use CT reconstruction algorithms in end‐to‐end neural networks and to elevate the research in medical image reconstruction. The software package is available under <ext-link ext-link-type="uri" xlink:href="https://github.com/csyben/PYRO-NN">https://github.com/csyben/PYRO-NN</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://github.com/csyben/PYRO-NN-Layers">https://github.com/csyben/PYRO-NN-Layers</ext-link>.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supporting information</title>
    <supplementary-material content-type="local-data" id="mp13753-sup-0001">
      <caption>
        <p><bold>Appendix S1:</bold> Supplementary Material.</p>
      </caption>
      <media xlink:href="MP-46-5110-s001.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="mp13753-sec-0022">
    <title>Acknowledgments</title>
    <p>The research leading to these results has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation program (ERC Grant No. 810316). Additional financial support for this project was granted by the Emerging Fields Initiative (EFI) of the Friedrich‐Alexander University Erlangen‐Nürnberg (FAU).</p>
  </ack>
  <ref-list content-type="cited-references" id="mp13753-bibl-0001">
    <title>References</title>
    <ref id="mp13753-bib-0001">
      <label>1</label>
      <mixed-citation publication-type="journal" id="mp13753-cit-0001"><string-name><surname>LeCun</surname><given-names>Y</given-names></string-name>, <string-name><surname>Bengio</surname><given-names>Y</given-names></string-name>, <string-name><surname>Hinton</surname><given-names>G.</given-names></string-name><article-title>Deep learning</article-title>. <source xml:lang="en">Nature</source>. <year>2015</year>;<volume>521</volume>:<fpage>436</fpage>.<pub-id pub-id-type="pmid">26017442</pub-id></mixed-citation>
    </ref>
    <ref id="mp13753-bib-0002">
      <label>2</label>
      <mixed-citation publication-type="book" id="mp13753-cit-0002"><string-name><surname>Krizhevsky</surname><given-names>A</given-names></string-name>, <string-name><surname>Sutskever</surname><given-names>I</given-names></string-name>, <string-name><surname>Hinton</surname><given-names>GE</given-names></string-name>. <chapter-title>Imagenet classification with deep convolutional neural networks</chapter-title> In: <person-group person-group-type="editor"><name name-style="western"><surname>Pereira</surname><given-names>F</given-names></name></person-group>, <person-group person-group-type="editor"><name name-style="western"><surname>Burges</surname><given-names>CJC</given-names></name></person-group>, <person-group person-group-type="editor"><name name-style="western"><surname>Bottou</surname><given-names>L</given-names></name></person-group>, <person-group person-group-type="editor"><name name-style="western"><surname>Weinberger</surname><given-names>KQ</given-names></name></person-group>, eds. <source xml:lang="en">Advances in Neural Information Processing Systems 25</source>, <publisher-loc>Red Hook, NY</publisher-loc>: <publisher-name>Curran Associates, Inc.</publisher-name>; <year>2012</year>:<fpage>1097</fpage>–<lpage>1105</lpage>.</mixed-citation>
    </ref>
    <ref id="mp13753-bib-0003">
      <label>3</label>
      <mixed-citation publication-type="miscellaneous" id="mp13753-cit-0003"><string-name><surname>Van Den Oord</surname><given-names>A</given-names></string-name>, <string-name><surname>Dieleman</surname><given-names>S</given-names></string-name>, <string-name><surname>Zen</surname><given-names>H</given-names></string-name>, et al. <article-title>Wavenet: A generative model for raw audio</article-title>. CoRR abs/1609.03499; <year>2016</year>.</mixed-citation>
    </ref>
    <ref id="mp13753-bib-0004">
      <label>4</label>
      <mixed-citation publication-type="miscellaneous" id="mp13753-cit-0004"><string-name><surname>Maier</surname><given-names>A</given-names></string-name>, <string-name><surname>Syben</surname><given-names>C</given-names></string-name>, <string-name><surname>Lasser</surname><given-names>T</given-names></string-name>, <string-name><surname>Riess</surname><given-names>C</given-names></string-name>. <article-title>A gentle introduction to deep learning in medical image processing</article-title>. arXiv preprint arXiv:1810.05401; <year>2018</year>.</mixed-citation>
    </ref>
    <ref id="mp13753-bib-0005">
      <label>5</label>
      <mixed-citation publication-type="miscellaneous" id="mp13753-cit-0005"><string-name><surname>Ronneberger</surname><given-names>O</given-names></string-name>, <string-name><surname>Fischer</surname><given-names>P</given-names></string-name>, <string-name><surname>Brox</surname><given-names>T</given-names></string-name>. <article-title>U‐net: Convolutional networks for biomedical image segmentation</article-title>. In: <italic>International Conference on Medical image computing and computer‐assisted intervention</italic> Springer; <year>2015</year>:<fpage>234</fpage>–<lpage>241</lpage>.</mixed-citation>
    </ref>
    <ref id="mp13753-bib-0006">
      <label>6</label>
      <mixed-citation publication-type="miscellaneous" id="mp13753-cit-0006"><string-name><surname>Stimpel</surname><given-names>B</given-names></string-name>, <string-name><surname>Syben</surname><given-names>C</given-names></string-name>, <string-name><surname>Würfl</surname><given-names>T</given-names></string-name>, <string-name><surname>Mentl</surname><given-names>K</given-names></string-name>, <string-name><surname>Dörfler</surname><given-names>A</given-names></string-name>, <string-name><surname>Maier</surname><given-names>A</given-names></string-name>. <article-title>MR to x‐ray projection image synthesis</article-title>. arXiv preprint arXiv:1710.07498; <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="mp13753-bib-0007">
      <label>7</label>
      <mixed-citation publication-type="journal" id="mp13753-cit-0007"><string-name><surname>Jin</surname><given-names>KH</given-names></string-name>, <string-name><surname>McCann</surname><given-names>MT</given-names></string-name>, <string-name><surname>Froustey</surname><given-names>E</given-names></string-name>, <string-name><surname>Unser</surname><given-names>M.</given-names></string-name><article-title>Deep convolutional neural network for inverse problems in imaging</article-title>. <source xml:lang="en">IEEE Trans Image Process</source>. <year>2017</year>;<volume>26</volume>:<fpage>4509</fpage>–<lpage>4522</lpage>.<pub-id pub-id-type="pmid">28641250</pub-id></mixed-citation>
    </ref>
    <ref id="mp13753-bib-0008">
      <label>8</label>
      <mixed-citation publication-type="miscellaneous" id="mp13753-cit-0008"><string-name><surname>Kofler</surname><given-names>A</given-names></string-name>, <string-name><surname>Haltmeier</surname><given-names>M</given-names></string-name>, <string-name><surname>Kolbitsch</surname><given-names>C</given-names></string-name>, <string-name><surname>Kachelrieß</surname><given-names>M</given-names></string-name>, <string-name><surname>Dewey</surname><given-names>M.</given-names></string-name><article-title>A u‐nets cascade for sparse view computed tomography</article-title>. In: <italic>International Workshop on Machine Learning for Medical Image Reconstruction</italic> Springer; <year>2018</year>:<fpage>91</fpage>–<lpage>99</lpage>.</mixed-citation>
    </ref>
    <ref id="mp13753-bib-0009">
      <label>9</label>
      <mixed-citation publication-type="miscellaneous" id="mp13753-cit-0009"><string-name><surname>Antun</surname><given-names>V</given-names></string-name>, <string-name><surname>Renna</surname><given-names>F</given-names></string-name>, <string-name><surname>Poon</surname><given-names>C</given-names></string-name>, <string-name><surname>Adcock</surname><given-names>B</given-names></string-name>, <string-name><surname>Hansen</surname><given-names>AC.</given-names></string-name><article-title>On instabilities of deep learning in image reconstruction‐does ai come at a cost?</article-title>. arXiv preprint arXiv:1902.05300; <year>2019</year>.</mixed-citation>
    </ref>
    <ref id="mp13753-bib-0010">
      <label>10</label>
      <mixed-citation publication-type="journal" id="mp13753-cit-0010"><string-name><surname>Greenspan</surname><given-names>H</given-names></string-name>, <string-name><surname>Van Ginneken</surname><given-names>B</given-names></string-name>, <string-name><surname>Summers</surname><given-names>RM.</given-names></string-name><article-title>Guest editorial deep learning in medical imaging: overview and future promise of an exciting new technique</article-title>. <source xml:lang="en">IEEE Trans Med Imaging</source>. <year>2016</year>;<volume>35</volume>:<fpage>1153</fpage>–<lpage>1159</lpage>.</mixed-citation>
    </ref>
    <ref id="mp13753-bib-0011">
      <label>11</label>
      <mixed-citation publication-type="journal" id="mp13753-cit-0011"><string-name><surname>Wang</surname><given-names>G</given-names></string-name>, <string-name><surname>Ye</surname><given-names>JC</given-names></string-name>, <string-name><surname>Mueller</surname><given-names>K</given-names></string-name>, <string-name><surname>Fessler</surname><given-names>JA.</given-names></string-name><article-title>Image reconstruction is a new frontier of machine learning</article-title>. <source xml:lang="en">IEEE Trans Med Imaging</source>. <year>2018</year>;<volume>37</volume>:<fpage>1289</fpage>–<lpage>1296</lpage>.<pub-id pub-id-type="pmid">29870359</pub-id></mixed-citation>
    </ref>
    <ref id="mp13753-bib-0012">
      <label>12</label>
      <mixed-citation publication-type="journal" id="mp13753-cit-0012"><string-name><surname>Zhu</surname><given-names>B</given-names></string-name>, <string-name><surname>Liu</surname><given-names>JZ</given-names></string-name>, <string-name><surname>Cauley</surname><given-names>SF</given-names></string-name>, <string-name><surname>Rosen</surname><given-names>BR</given-names></string-name>, <string-name><surname>Rosen</surname><given-names>MS.</given-names></string-name><article-title>Image reconstruction by domain‐transform manifold learning</article-title>. <source xml:lang="en">Nature</source>. <year>2018</year>;<volume>555</volume>:<fpage>487</fpage>–<lpage>492</lpage>.<pub-id pub-id-type="pmid">29565357</pub-id></mixed-citation>
    </ref>
    <ref id="mp13753-bib-0013">
      <label>13</label>
      <mixed-citation publication-type="miscellaneous" id="mp13753-cit-0013"><string-name><surname>Maier</surname><given-names>A</given-names></string-name>, <string-name><surname>Schebesch</surname><given-names>F</given-names></string-name>, <string-name><surname>Syben</surname><given-names>C.</given-names></string-name><article-title>Precision learning: Towards use of known operators in neural networks</article-title>. in 2018 24th ICPR, IEEE; <year>2018</year>:<fpage>183</fpage>–<lpage>188</lpage>.</mixed-citation>
    </ref>
    <ref id="mp13753-bib-0014">
      <label>14</label>
      <mixed-citation publication-type="journal" id="mp13753-cit-0014"><string-name><surname>Ye</surname><given-names>JC</given-names></string-name>, <string-name><surname>Han</surname><given-names>Y</given-names></string-name>, <string-name><surname>Cha</surname><given-names>E.</given-names></string-name><article-title>Deep convolutional framelets: a general deep learning framework for inverse problems</article-title>. <source xml:lang="en">SIAM J Imaging Sci</source>. <year>2018</year>;<volume>11</volume>:<fpage>991</fpage>–<lpage>1048</lpage>.</mixed-citation>
    </ref>
    <ref id="mp13753-bib-0015">
      <label>15</label>
      <mixed-citation publication-type="journal" id="mp13753-cit-0015"><string-name><surname>Chen</surname><given-names>H</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Chen</surname><given-names>Y</given-names></string-name>, et al. <article-title>LEARN: learned experts assessment‐based reconstruction network for sparse‐data CT</article-title>. <source xml:lang="en">IEEE Trans Med Imaging</source>. <year>2018</year>;<volume>37</volume>:<fpage>1333</fpage>–<lpage>1347</lpage>.<pub-id pub-id-type="pmid">29870363</pub-id></mixed-citation>
    </ref>
    <ref id="mp13753-bib-0016">
      <label>16</label>
      <mixed-citation publication-type="journal" id="mp13753-cit-0016"><string-name><surname>Adler</surname><given-names>J</given-names></string-name>, <string-name><surname>Öktem</surname><given-names>O.</given-names></string-name><article-title>Learned primal‐dual reconstruction</article-title>. <source xml:lang="en">IEEE Trans Med Imaging</source>. <year>2018</year>;<volume>37</volume>:<fpage>1322</fpage>–<lpage>1332</lpage>.<pub-id pub-id-type="pmid">29870362</pub-id></mixed-citation>
    </ref>
    <ref id="mp13753-bib-0017">
      <label>17</label>
      <mixed-citation publication-type="miscellaneous" id="mp13753-cit-0017"><string-name><surname>Würfl</surname><given-names>T</given-names></string-name>, <string-name><surname>Ghesu</surname><given-names>FC</given-names></string-name>, <string-name><surname>Christlein</surname><given-names>V</given-names></string-name>, <string-name><surname>Maier</surname><given-names>A.</given-names></string-name><article-title>Deep learning computed tomography</article-title>. In: <italic>International Conference on Medical Image Computing and Computer‐Assisted Intervention</italic> Springer; <year>2016</year>:<fpage>432</fpage>–<lpage>440</lpage>.</mixed-citation>
    </ref>
    <ref id="mp13753-bib-0018">
      <label>18</label>
      <mixed-citation publication-type="book" id="mp13753-cit-0018"><string-name><surname>Hammernik</surname><given-names>K</given-names></string-name>, <string-name><surname>Würfl</surname><given-names>T</given-names></string-name>, <string-name><surname>Pock</surname><given-names>T</given-names></string-name>, <string-name><surname>Maier</surname><given-names>A.</given-names></string-name><chapter-title>A deep learning architecture for limited‐angle computed tomography reconstruction</chapter-title> In: <source xml:lang="en">Bildverarbeitung für die Medizin 2017</source>, <publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer</publisher-name>; <year>2017</year>:<fpage>92</fpage>–<lpage>97</lpage>.</mixed-citation>
    </ref>
    <ref id="mp13753-bib-0019">
      <label>19</label>
      <mixed-citation publication-type="miscellaneous" id="mp13753-cit-0019"><string-name><surname>Syben</surname><given-names>C</given-names></string-name>, <string-name><surname>Stimpel</surname><given-names>B</given-names></string-name>, <string-name><surname>Breininger</surname><given-names>K</given-names></string-name>, et al. <article-title>Precision learning: reconstruction filter kernel discretization</article-title>. In Proceedings of the Fifth International Conference on Image Formation in x‐Ray Computed Tomography; <year>2018</year>:<fpage>386</fpage>–<lpage>390</lpage>.</mixed-citation>
    </ref>
    <ref id="mp13753-bib-0020">
      <label>20</label>
      <mixed-citation publication-type="journal" id="mp13753-cit-0020"><string-name><surname>Würfl</surname><given-names>T</given-names></string-name>, <string-name><surname>Hoffmann</surname><given-names>M</given-names></string-name>, <string-name><surname>Christlein</surname><given-names>V</given-names></string-name>, et al. <article-title>Deep learning computed tomography: learning projection‐domain weights from image domain in limited angle problems</article-title>. <source xml:lang="en">IEEE Trans Med Imaging</source>. <year>2018</year>;<volume>37</volume>:<fpage>1454</fpage>–<lpage>1463</lpage>.<pub-id pub-id-type="pmid">29870373</pub-id></mixed-citation>
    </ref>
    <ref id="mp13753-bib-0021">
      <label>21</label>
      <mixed-citation publication-type="miscellaneous" id="mp13753-cit-0021"><string-name><surname>Syben</surname><given-names>C</given-names></string-name>, <string-name><surname>Stimpel</surname><given-names>B</given-names></string-name>, <string-name><surname>Lommen</surname><given-names>J</given-names></string-name>, <string-name><surname>Würfl</surname><given-names>T</given-names></string-name>, <string-name><surname>Dörfler</surname><given-names>A</given-names></string-name>, <string-name><surname>Maier</surname><given-names>A.</given-names></string-name><article-title>Deriving neural network architectures using precision learning: Parallel‐to‐fan beam conversion</article-title>. In: <person-group person-group-type="editor"><name name-style="western"><surname>Bruhn</surname><given-names>A</given-names></name></person-group>, ed. Pattern Recognition, 40th German Conference; <year>2018</year>.</mixed-citation>
    </ref>
    <ref id="mp13753-bib-0022">
      <label>22</label>
      <mixed-citation publication-type="miscellaneous" id="mp13753-cit-0022"><collab collab-type="authors">Adler J</collab>
, <string-name><surname>Kohr</surname><given-names>H</given-names></string-name>, <string-name><surname>Oktem</surname><given-names>O</given-names></string-name>. <article-title>Operator discretization library (odl)</article-title>, Software 2017. Available from <ext-link ext-link-type="uri" xlink:href="https://github.com/odlgroup/odl">https://github.com/odlgroup/odl</ext-link>.</mixed-citation>
    </ref>
    <ref id="mp13753-bib-0023">
      <label>23</label>
      <mixed-citation publication-type="miscellaneous" id="mp13753-cit-0023"><string-name><surname>Abadi</surname><given-names>M</given-names></string-name>, <string-name><surname>Barham</surname><given-names>P</given-names></string-name>, <string-name><surname>Chen</surname><given-names>J</given-names></string-name>, et al. <article-title>Tensorflow: a system for large‐scale machine learning</article-title>. In: OSDI. Vol. 16; <year>2016</year>:<fpage>265</fpage>–<lpage>283</lpage>.</mixed-citation>
    </ref>
    <ref id="mp13753-bib-0024">
      <label>24</label>
      <mixed-citation publication-type="journal" id="mp13753-cit-0024"><string-name><surname>van Aarle</surname><given-names>W</given-names></string-name>, <string-name><surname>Palenstijn</surname><given-names>WJ</given-names></string-name>, <string-name><surname>Cant</surname><given-names>J</given-names></string-name>, et al. <article-title>Fast and flexible x‐ray tomography using the ASTRA toolbox</article-title>. <source xml:lang="en">Opt Express</source>. <year>2016</year>;<volume>24</volume>:<fpage>25129</fpage>–<lpage>25147</lpage>.<pub-id pub-id-type="pmid">27828452</pub-id></mixed-citation>
    </ref>
    <ref id="mp13753-bib-0025">
      <label>25</label>
      <mixed-citation publication-type="journal" id="mp13753-cit-0025"><string-name><surname>Gürsoy</surname><given-names>D</given-names></string-name>, <string-name><surname>De Carlo</surname><given-names>F</given-names></string-name>, <string-name><surname>Xiao</surname><given-names>X</given-names></string-name>, <string-name><surname>Jacobsen</surname><given-names>C.</given-names></string-name><article-title>Tomopy: a framework for the analysis of synchrotron tomographic data</article-title>. <source xml:lang="en">J Synchrotron Radiat</source>. <year>2014</year>;<volume>21</volume>:<fpage>1188</fpage>–<lpage>1193</lpage>.<pub-id pub-id-type="pmid">25178011</pub-id></mixed-citation>
    </ref>
    <ref id="mp13753-bib-0026">
      <label>26</label>
      <mixed-citation publication-type="journal" id="mp13753-cit-0026"><string-name><surname>Maier</surname><given-names>A</given-names></string-name>, <string-name><surname>Hofmann</surname><given-names>HG</given-names></string-name>, <string-name><surname>Berger</surname><given-names>M</given-names></string-name>, et al. <article-title>CONRAD–a software framework for cone–beam imaging in radiology</article-title>. <source xml:lang="en">Med Phys</source>. <year>2013</year>;<volume>40</volume>:<fpage>111914</fpage>.<pub-id pub-id-type="pmid">24320447</pub-id></mixed-citation>
    </ref>
    <ref id="mp13753-bib-0027">
      <label>27</label>
      <mixed-citation publication-type="miscellaneous" id="mp13753-cit-0027"><string-name><surname>Syben</surname><given-names>C</given-names></string-name>, <string-name><surname>Seitz</surname><given-names>S</given-names></string-name>, <string-name><surname>Maier</surname><given-names>A.</given-names></string-name><article-title>Pyconrad</article-title>. Software; <year>2017</year> Available from <ext-link ext-link-type="uri" xlink:href="https://git5.cs.fau.de/PyConrad/pyCONRAD">https://git5.cs.fau.de/PyConrad/pyCONRAD</ext-link>
</mixed-citation>
    </ref>
    <ref id="mp13753-bib-0028">
      <label>28</label>
      <mixed-citation publication-type="journal" id="mp13753-cit-0028"><string-name><surname>Galigekere</surname><given-names>RR</given-names></string-name>, <string-name><surname>Wiesent</surname><given-names>K</given-names></string-name>, <string-name><surname>Holdsworth</surname><given-names>DW.</given-names></string-name><article-title>Cone‐beam reprojection using projection‐matrices</article-title>. <source xml:lang="en">IEEE Trans Med Imaging</source>. <year>2003</year>;<volume>22</volume>:<fpage>1202</fpage>–<lpage>1214</lpage>.<pub-id pub-id-type="pmid">14552575</pub-id></mixed-citation>
    </ref>
    <ref id="mp13753-bib-0029">
      <label>29</label>
      <mixed-citation publication-type="miscellaneous" id="mp13753-cit-0029"><string-name><surname>Scherl</surname><given-names>H</given-names></string-name>, <string-name><surname>Keck</surname><given-names>B</given-names></string-name>, <string-name><surname>Kowarschik</surname><given-names>M</given-names></string-name>, <string-name><surname>Hornegger</surname><given-names>J.</given-names></string-name><article-title>Fast GPU‐based CT reconstruction using the common unified device architecture (CUDA)</article-title>. In: <italic>Nuclear Science Symposium Conference Record, 2007</italic> NSS’07. IEEE, Vol. 6. IEEE; <year>2007</year>:<fpage>4464</fpage>–<lpage>4466</lpage>.</mixed-citation>
    </ref>
    <ref id="mp13753-bib-0030">
      <label>30</label>
      <mixed-citation publication-type="journal" id="mp13753-cit-0030"><string-name><surname>Shepp</surname><given-names>LA</given-names></string-name>, <string-name><surname>Logan</surname><given-names>BF.</given-names></string-name><article-title>The fourier reconstruction of a head section</article-title>. <source xml:lang="en">IEEE Trans Nucl Sci</source>. <year>1974</year>;<volume>21</volume>:<fpage>21</fpage>–<lpage>43</lpage>.</mixed-citation>
    </ref>
    <ref id="mp13753-bib-0031">
      <label>31</label>
      <mixed-citation publication-type="book" id="mp13753-cit-0031"><string-name><surname>Kak</surname><given-names>AC</given-names></string-name>, <string-name><surname>Slaney</surname><given-names>M</given-names></string-name>. <source xml:lang="en">Principles of Computerized Tomographic Imaging</source>. <publisher-loc>New York</publisher-loc>:<publisher-name>IEEE Press</publisher-name>; <year>1988</year>.</mixed-citation>
    </ref>
    <ref id="mp13753-bib-0032">
      <label>32</label>
      <mixed-citation publication-type="journal" id="mp13753-cit-0032"><string-name><surname>Feldkamp</surname><given-names>LA</given-names></string-name>, <string-name><surname>Davis</surname><given-names>L</given-names></string-name>, <string-name><surname>Kress</surname><given-names>JW.</given-names></string-name><article-title>Practical cone‐beam algorithm</article-title>. <source xml:lang="en">JOSA A</source>. <year>1984</year>;<volume>1</volume>:<fpage>612</fpage>–<lpage>619</lpage>.</mixed-citation>
    </ref>
    <ref id="mp13753-bib-0033">
      <label>33</label>
      <mixed-citation publication-type="miscellaneous" id="mp13753-cit-0033"><string-name><surname>Syben</surname><given-names>C.</given-names></string-name><article-title>Code for technical note: Pyro‐nn: Python reconstruction operators in neural networks</article-title>. Code Ocean; <year>2019</year><pub-id pub-id-type="doi">10.24433/CO.1164752.v1</pub-id></mixed-citation>
    </ref>
    <ref id="mp13753-bib-0034">
      <label>34</label>
      <mixed-citation publication-type="journal" id="mp13753-cit-0034"><string-name><surname>Zeng</surname><given-names>G</given-names></string-name>, <string-name><surname>Gullberg</surname><given-names>GT.</given-names></string-name><article-title>Unmatched projector/backprojector pairs in an iterative reconstruction algorithm</article-title>. <source xml:lang="en">IEEE Trans Med Imaging</source>. <year>2000</year>;<volume>19</volume>:<fpage>548</fpage>–<lpage>555</lpage><pub-id pub-id-type="pmid">11021698</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
