<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD with MathML3 v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1-mathml3.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName nihms2pmcx2.xsl?>
<?ConverterInfo.Version 1?>
<?properties manuscript?>
<?origin nihpa?>
<?iso-abbr Neuroimage?>
<?submitter-system nihms?>
<?submitter-canonical-name Elsevier?>
<?submitter-canonical-id ELSEVIERAM?>
<?submitter-userid 8068823?>
<?submitter-authority myNCBI?>
<?submitter-login elsevieram?>
<?submitter-name Elsevier Author Support?>
<?domain nihpa?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">9215515</journal-id>
    <journal-id journal-id-type="pubmed-jr-id">20498</journal-id>
    <journal-id journal-id-type="nlm-ta">Neuroimage</journal-id>
    <journal-id journal-id-type="iso-abbrev">Neuroimage</journal-id>
    <journal-title-group>
      <journal-title>NeuroImage</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1053-8119</issn>
    <issn pub-type="epub">1095-9572</issn>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7784120</article-id>
    <article-id pub-id-type="pmid">32702486</article-id>
    <article-id pub-id-type="doi">10.1016/j.neuroimage.2020.117161</article-id>
    <article-id pub-id-type="manuscript">nihpa1657524</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Cortical surface registration using unsupervised learning</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Cheng</surname>
          <given-names>Jieyu</given-names>
        </name>
        <xref ref-type="aff" rid="A1">a</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Dalca</surname>
          <given-names>Adrian V.</given-names>
        </name>
        <xref ref-type="aff" rid="A1">a</xref>
        <xref ref-type="aff" rid="A2">b</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Fischl</surname>
          <given-names>Bruce</given-names>
        </name>
        <xref ref-type="aff" rid="A1">a</xref>
        <xref ref-type="aff" rid="A2">b</xref>
        <xref rid="FN1" ref-type="author-notes">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zöllei</surname>
          <given-names>Lilla</given-names>
        </name>
        <xref ref-type="aff" rid="A1">a</xref>
        <xref rid="CR1" ref-type="corresp">*</xref>
        <xref rid="FN1" ref-type="author-notes">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <collab>Alzheimer’s Disease Neuroimaging Initiative</collab>
        <xref rid="FN2" ref-type="author-notes">2</xref>
      </contrib>
    </contrib-group>
    <aff id="A1"><label>a</label>A.A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital, Harvard Medical School, USA</aff>
    <aff id="A2"><label>b</label>Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, USA</aff>
    <author-notes>
      <fn id="FN1">
        <label>1</label>
        <p id="P1">These authors share senior authorship.</p>
      </fn>
      <fn id="FN2">
        <label>2</label>
        <p id="P2">Data used in preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (<ext-link ext-link-type="uri" xlink:href="http://adni.loni.usc.edu">adni.loni.usc.edu</ext-link>). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: <ext-link ext-link-type="uri" xlink:href="http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf">http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf</ext-link>.</p>
      </fn>
      <fn fn-type="con" id="FN3">
        <p id="P3">CRediT authorship contribution statement</p>
        <p id="P4"><bold>Jieyu Cheng:</bold> Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Validation, Visualization, Writing - original draft, Writing - review &amp; editing. <bold>Adrian V. Dalca:</bold> Conceptualization, Methodology, Software, Writing - review &amp; editing. <bold>Bruce Fischl:</bold> Conceptualization, Methodology, Resources, Software, Writing - review &amp; editing. <bold>Lilla Zöllei:</bold> Conceptualization, Funding acquisition, Investigation, Methodology, Project administration, Resources, Software, Supervision, Writing - review &amp; editing.</p>
      </fn>
      <corresp id="CR1"><label>*</label>Corresponding author. <email>lzollei@nmr.mgh.harvard.edu</email> (L. Zöllei).</corresp>
    </author-notes>
    <pub-date pub-type="nihms-submitted">
      <day>30</day>
      <month>12</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>20</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <day>01</day>
      <month>11</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>05</day>
      <month>1</month>
      <year>2021</year>
    </pub-date>
    <volume>221</volume>
    <fpage>117161</fpage>
    <lpage>117161</lpage>
    <!--elocation-id from pubmed: 10.1016/j.neuroimage.2020.117161-->
    <permissions>
      <license>
        <license-p>This is an open access article under the CC BY-NC-ND license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link>).</license-p>
      </license>
    </permissions>
    <abstract id="ABS1">
      <p id="P5">Non-rigid cortical registration is an important and challenging task due to the geometric complexity of the human cortex and the high degree of inter-subject variability. A conventional solution is to use a spherical representation of surface properties and perform registration by aligning cortical folding patterns in that space. This strategy produces accurate spatial alignment, but often requires high computational cost. Recently, convolutional neural networks (CNNs) have demonstrated the potential to dramatically speed up volumetric registration. However, due to distortions introduced by projecting a sphere to a 2D plane, a direct application of recent learning-based methods to surfaces yields poor results. In this study, we present SphereMorph, a diffeomorphic registration framework for cortical surfaces using deep networks that addresses these issues. SphereMorph uses a UNet-style network associated with a spherical kernel to learn the displacement field and warps the sphere using a modified spatial transformer layer. We propose a resampling weight in computing the data fitting loss to account for distortions introduced by polar projection, and demonstrate the performance of our proposed method on two tasks, including cortical parcellation and group-wise functional area alignment. The experiments show that the proposed SphereMorph is capable of modeling the geometric registration problem in a CNN framework and demonstrate superior registration accuracy and computational efficiency. The source code of SphereMorph will be released to the public upon acceptance of this manuscript at <ext-link ext-link-type="uri" xlink:href="https://github.com/voxelmorph/spheremorph">https://github.com/voxelmorph/spheremorph</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>Cortical surface registration</kwd>
      <kwd>Deep learning</kwd>
      <kwd>Unsupervised learning</kwd>
      <kwd>SphereMorph</kwd>
      <kwd>Subject-to-atlas registration</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <label>1.</label>
    <title>Introduction</title>
    <p id="P6">Non-rigid shape registration is an important area of research in medical imaging, in particular for establishing cross-subject spatial correspondence in the cerebral cortex. This type of spatial alignment has been shown to improve the statistical power of group functional MRI (fMRI) analysis (<xref rid="R40" ref-type="bibr">Van Atteveldt et al., 2004</xref>; <xref rid="R15" ref-type="bibr">Frost and Goebel, 2012</xref>) resulting from the improved correspondence of functional areas. Due to the geometric complexity of the cortex and the large variability between individuals, cortical surface registration remains a challenging task. Inter-subject surface alignment is commonly driven by geometric features that describe measures of cortical shape (folding), such as sulcal depth or local curvature (<xref rid="R14" ref-type="bibr">Fischl et al., 1999</xref>; <xref rid="R45" ref-type="bibr">Yeo et al., 2010</xref>; <xref rid="R6" ref-type="bibr">Conroy et al., 2013</xref>; <xref rid="R37" ref-type="bibr">Tardif et al., 2015</xref>).</p>
    <p id="P7">A widely used cortical surface registration approach is to map the surface onto the unit sphere in order to perform computations in this canonical domain. Existing efforts are mainly focused on the adaptation of registration algorithms in the Euclidean space (<xref rid="R14" ref-type="bibr">Fischl et al., 1999</xref>; <xref rid="R45" ref-type="bibr">Yeo et al., 2010</xref>; <xref rid="R30" ref-type="bibr">Robinson et al., 2014</xref>). These aim to optimize a similarity metric between the target and the deformed source volumes, regularized by various energies (<xref rid="R34" ref-type="bibr">Sotiras et al., 2013</xref>). FreeSurfer (<xref rid="R14" ref-type="bibr">Fischl et al., 1999</xref>) registers an individual surface to a probabilistic atlas computed from a representative set of subjects by minimizing the squared difference between the average convexity across subjects and that of the individual, weighted by the inverse variance of the convexity across subjects. The Multimodal Surface Matching tool (<xref rid="R30" ref-type="bibr">Robinson et al., 2014</xref>) uses a similarity between the input and reference mesh features in a coarse-to-fine manner. The deformation is driven by aligning local patches around control points, i.e. vertices of a low resolution mesh, and then propagated to the high resolution input mesh via interpolation. Spherical Demons (<xref rid="R45" ref-type="bibr">Yeo et al., 2010</xref>) modifies the classical Demons method (<xref rid="R38" ref-type="bibr">Thirion, 1998</xref>) using velocity vectors tangent to the sphere. The two-step optimization of classical Demons also holds for the spherical case in which the second step handles the deformation regularization by spherical thin plate spline interpolation. To encourage desirable mathematical properties such as invertibility, diffeomorphic transforms have seen extensive methodological development, yielding state-of-the-art tools (<xref rid="R2" ref-type="bibr">Ashburner, 2007</xref>; <xref rid="R46" ref-type="bibr">Zhang et al., 2017</xref>). Unfortunately, since these methods solve an optimization problem for each image pair, they often exhibit long execution times.</p>
    <p id="P8">High computational costs have led to an increase in the popularity of supervised (<xref rid="R26" ref-type="bibr">Krebs et al., 2017</xref>; <xref rid="R33" ref-type="bibr">Sokooti et al., 2017</xref>; <xref rid="R44" ref-type="bibr">Yang et al., 2016</xref>) and unsupervised (<xref rid="R3" ref-type="bibr">Balakrishnan et al., 2019</xref>; <xref rid="R8" ref-type="bibr">Dalca et al., 2018</xref>, <xref rid="R9" ref-type="bibr">2019</xref>; <xref rid="R29" ref-type="bibr">Niethammer et al., 2019</xref>; <xref rid="R43" ref-type="bibr">Wang et al., 2015</xref>) learning-based registration algorithms. Considering the difficulty of establishing ground truth spatial correspondences, supervised methods require predictions from existing algorithms (<xref rid="R44" ref-type="bibr">Yang et al., 2016</xref>), simulations (<xref rid="R33" ref-type="bibr">Sokooti et al., 2017</xref>), or both (<xref rid="R26" ref-type="bibr">Krebs et al., 2017</xref>). In contrast, unsupervised methods make use of Spatial Transformer Networks (STN) (<xref rid="R22" ref-type="bibr">Jaderberg et al., 2015</xref>) to warp the moving image in a differentiable way, enabling end-to-end training (<xref rid="R3" ref-type="bibr">Balakrishnan et al., 2019</xref>; <xref rid="R9" ref-type="bibr">Dalca et al., 2019</xref>; <xref rid="R23" ref-type="bibr">Jason et al., 2016</xref>; <xref rid="R27" ref-type="bibr">Krebs et al., 2019</xref>; <xref rid="R10" ref-type="bibr">de Vos et al., 2019</xref>; <xref rid="R29" ref-type="bibr">Niethammer et al., 2019</xref>). Some unsupervised methods (<xref rid="R3" ref-type="bibr">Balakrishnan et al., 2019</xref>; <xref rid="R9" ref-type="bibr">Dalca et al., 2019</xref>; <xref rid="R27" ref-type="bibr">Krebs et al., 2019</xref>) model a stationary velocity field as latent variables representing deformations in a generative probabilistic model. They use a scaling and squaring layer (<xref rid="R1" ref-type="bibr">Arsigny et al., 2006</xref>) for the Lie group exponentiation of the velocity field to generate diffeomorphic transforms, thus guaranteeing topology preservation. These methods have demonstrated high quality performance in registering various types of medical images. Therefore, we build on these concepts when working with surfaces. A more complex model (<xref rid="R29" ref-type="bibr">Niethammer et al., 2019</xref>) utilizes a vector momentum-parameterized stationary velocity field (vSVF) and jointly optimizes the local regularizer parameterized by a deep network and the registration parameters of the vSVF model for location-varying regularization. In addition to velocity field registration models, sparse learning is also explored for image-template key point matching (<xref rid="R43" ref-type="bibr">Wang et al., 2015</xref>) and a subsequent interpolation to a dense deformation field via radial basis functions. However, the performance is dependent on the key point selection.</p>
    <p id="P9">Recent studies have developed geometric convolutional neural networks (CNN) (<xref rid="R35" ref-type="bibr">Su and Grauman, 2017</xref>; <xref rid="R5" ref-type="bibr">Cohen et al., 2018</xref>; <xref rid="R7" ref-type="bibr">Coors et al., 2018</xref>; <xref rid="R32" ref-type="bibr">Seong et al., 2018</xref>; <xref rid="R24" ref-type="bibr">Jiang et al., 2019</xref>; <xref rid="R47" ref-type="bibr">Zhao et al., 2019</xref>) that operate on a spherical manifold to solve classification and detection tasks. To address the distortions introduced by projecting signals to a planar image, regular convolutions with increased kernel sizes near polar regions have been utilized (<xref rid="R35" ref-type="bibr">Su and Grauman, 2017</xref>). Spherical CNNs encode rotational instead of translational equivariance into the network in Euclidean space to solve classification problems (<xref rid="R5" ref-type="bibr">Cohen et al., 2018</xref>). In SphereNet, the convolution kernel on the sphere has been approximated by encoding the vertex neighborhood information on 2D tangent planes, which enables adapting existing CNN architectures to the omnidirectional setup for object detection and classification tasks (<xref rid="R7" ref-type="bibr">Coors et al., 2018</xref>). Geometric CNNs (gCNN) (<xref rid="R32" ref-type="bibr">Seong et al., 2018</xref>) also deal with convolution and pooling operations of a CNN on a mesh surface. However, they have so far only been tested on sex classification, using cortical thickness images. A convolutional kernel discretized by an unstructured mesh was recently proposed and evaluated on spherical MNIST classification and 3D object detection tasks (<xref rid="R24" ref-type="bibr">Jiang et al., 2019</xref>). Spherical U-Net (<xref rid="R47" ref-type="bibr">Zhao et al., 2019</xref>) proposes a novel Direct Neighbor convolutional kernel based on expansion and contraction process of icosahedron and defines corresponding convolution and pooling operations. Graph convolutions (Gopinath et al., 2019a, 2019b) have been utilized for brain surface data in aligned spectral domains to learn the node-wise prediction (Gopinath et al., 2019a), e.g. cortex parcellation and global subject-wise information (Gopinath et al., 2019b), e.g. disease classification or age regression. FastSurfer (<xref rid="R20" ref-type="bibr">Henschel et al., 2020</xref>) introduces a full alternative pipeline for FreeSurfer and omits nonlinear surface-atlas registration via fast spherical mapping that quickly maps the volumetric parcellation to cortex using Laplace Eigenfunctions. Most existing work, however, focuses on the construction of spherical convolutional kernels and, to the best of our knowledge, neural networks have not yet been extended to surface registration. Compared with classification or detection tasks, besides convolution and pooling operations on spheres, a learning-based registration method should address local deformations defined on spheres. However, existing spatial transformation networks (<xref rid="R13" ref-type="bibr">Esteves et al., 2018</xref>; <xref rid="R36" ref-type="bibr">Tai et al., 2019</xref>) for spheres only address global deformations, which are not suitable for accommodating nonlinear deformation fields.</p>
    <p id="P10">In this paper, we propose a diffeomorphic framework combining a generative model for surfaces with CNNs to register individual cortical surfaces to an atlas space. This framework adapts conventional VoxelMorph for registering Euclidean images (<xref rid="R9" ref-type="bibr">Dalca et al., 2019</xref>) to spherical manifolds. In order to address the limitations of 2D planar projection, we construct a weighted neighborhood graph defined on 2D grids, which accounts for the non-uniform metric tensor of the spherical representation to encode a stationary velocity field. Considering that the 2D projection operation samples the arc-length for each latitude to the same number of points, we also take sampling distortion into account at different latitudes in the likelihood model. We quantify the performance of our framework through two applications: the generation of cortical parcellations and the alignment of functional activations. The experimental results demonstrate that our framework yields better registration accuracy to state-of-the-art classical methods at a significantly reduced computational cost, and more accurate results compared to current learning-based methods.</p>
    <p id="P11">Our contributions can be summarized as:</p>
    <list list-type="order" id="L1">
      <list-item>
        <p id="P12">We propose a learning-based framework for spherical surface registration, which provides accurate and efficient performance compared to conventional registration work;</p>
      </list-item>
      <list-item>
        <p id="P13">We derive an Maximum a Posteriori (MAP) solution for deformation fields in the spherical domain by correcting distortions from planar projection;</p>
      </list-item>
      <list-item>
        <p id="P14">Different from existing spherical networks, the registration results from our proposed method aid the cortex parcellation as well as various group analyses (i.e. local thickness, functional activation etc.) while other spherical networks only learn one or multiple preset measures;</p>
      </list-item>
      <list-item>
        <p id="P15">We explore the use of different features for functional alignment.</p>
      </list-item>
    </list>
    <p id="P16">The remainder of this paper is organized as follows. We first introduce the cortical registration problem, review the conventional VoxelMorph (<xref rid="R9" ref-type="bibr">Dalca et al., 2019</xref>) framework, and propose our method and network structure. We then describe two evaluation experiments including cortical parcellation and functional alignment, and show results for these experiments. Finally, we present our discussions and conclusions.</p>
  </sec>
  <sec id="S2">
    <label>2.</label>
    <title>Methods</title>
    <p id="P17">Numerous studies using FreeSurfer have demonstrated its efficacy in spherical-based cortical registration. We build on ideas for our surface representation from the FreeSurfer spherical registration (<xref rid="R14" ref-type="bibr">Fischl et al., 1999</xref>) and model the unsupervised learning structure for the registration field following VoxelMorph (<xref rid="R9" ref-type="bibr">Dalca et al., 2019</xref>).</p>
    <sec id="S3">
      <label>2.1.</label>
      <title>Registration problem definition</title>
      <p id="P18">In the FreeSurfer spherical registration pipeline (<xref rid="R14" ref-type="bibr">Fischl et al., 1999</xref>), surface geometry is encoded as a convexity attribute at each mesh vertex and the representation of the atlas surface is computed from a group of adult subjects. In order to register the surfaces of an individual to the atlas space, first a white matter mesh is generated and mapped to the unit sphere by minimizing metric distortion (<xref rid="R14" ref-type="bibr">Fischl et al., 1999</xref>). Next, an optimal rotational alignment is computed by global search over two rotation angles on the sphere. Then a 2D canonical warp is computed to align the subject’s convexity pattern with that of the mean pattern encoded in the atlas, by minimizing the mean squared difference, weighted by the inverse of the atlas variance. Our goal is to compute this canonical warp using a CNN framework.</p>
      <p id="P19">Let <italic>S</italic><sub><italic>x</italic></sub> be the unit sphere and <italic>I</italic><sub><italic>x</italic></sub> the corresponding scalar field over the sphere (e.g. sulcal depth or curvature) projected into 2D longitude/latitude parameterization. Let <italic>I</italic><sub><italic>a</italic></sub> be the atlas mean image as defined in FreeSurfer, and <italic>M</italic> and <italic>N</italic> be the number of image rows and columns. Let <inline-formula><mml:math display="inline" id="M3"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>…</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> be a diagonal matrix where each diagonal element denotes the variability of the corresponding feature at a particular vertex as defined in the FreeSurfer atlas variance image. The goal is to find the spatial transformation Φ : <italic>S</italic><sup>2</sup> → <italic>S</italic><sup>2</sup> given <italic>I</italic><sub><italic>x</italic></sub> and <italic>I</italic><sub><italic>a</italic></sub> that maximizes the a posteriori probability of the transform assuming certain smoothness priors on the warp.</p>
    </sec>
    <sec id="S4">
      <label>2.2.</label>
      <title>VoxelMorph</title>
      <p id="P20">We assume a diffeomorphic deformation based on a stationary velocity field <italic>v</italic>, denoted as Φ<sub><italic>v</italic></sub>, and adapt a generative probabilistic model following VoxelMorph (<xref rid="R9" ref-type="bibr">Dalca et al., 2019</xref>). VoxelMorph uses Maximum a Posteriori (MAP) estimation to obtain the most likely velocity field <italic>v</italic>* at each voxel/pixel given a pair of images. VoxelMorph models the prior probability of <italic>v</italic> as a zero-mean multivariate normal distribution, <inline-formula><mml:math display="inline" id="M4"><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where Σ<sub><italic>v</italic></sub> is the covariance matrix. An individual image can be estimated by warping the FreeSurfer atlas, thus we model the warped image <italic>I</italic><sub><italic>x</italic></sub> ∘ Φ<sub><italic>v</italic></sub> as <inline-formula><mml:math display="inline" id="M5"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi>v</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>∘</mml:mo><mml:msub><mml:mi mathvariant="normal">Φ</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where Φ<sub><italic>v</italic></sub> is the inverse transformation of the atlas warping to the individual. The aim is to maximize the posterior probability <inline-formula><mml:math display="inline" id="M6"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi>v</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mo>∫</mml:mo><mml:mi>v</mml:mi></mml:msub><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, where the marginalization over <italic>v</italic> is intractable. In this case, a variational approximation <italic>q</italic><sub><italic>ψ</italic></sub> (<italic>v</italic>|<italic>I</italic><sub><italic>x</italic></sub>; <italic>I</italic><sub><italic>a</italic></sub>) is adopted with parameters ψ, by minimizing its dissimilarity, Kullback–Leibler (KL) divergence, with the true posterior probability. For simplicity, the approximate posterior <italic>q</italic><sub><italic>ψ</italic></sub> (<italic>v</italic>|<italic>I</italic><sub><italic>x</italic></sub>; <italic>I</italic><sub><italic>a</italic></sub>) is restricted to a multivariate normal distribution <inline-formula><mml:math display="inline" id="M7"><mml:mrow><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math display="inline" id="M8"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and a diagonal <inline-formula><mml:math display="inline" id="M9"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are functions estimated with a U-Net core (<xref rid="R31" ref-type="bibr">Ronneberger et al., 2015</xref>), as shown in <xref rid="F2" ref-type="fig">Fig. 2</xref>.</p>
      <p id="P21">Using the above assumptions, maximizing the posterior probability can be approximated by minimizing the following loss:
<disp-formula id="FD1"><label>(1)</label><mml:math display="block" id="M10"><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="normal">log</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi>v</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>ψ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>‖</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <italic>E</italic><sub><italic>q</italic></sub>[ ·] operates the expectation computation given the distribution <italic>q</italic> and <italic>q</italic> is short for <italic>q</italic><sub><italic>ψ</italic></sub> (<italic>v</italic>|<italic>I</italic><sub><italic>x</italic></sub>; <italic>I</italic><sub><italic>a</italic></sub>). The first term describes the reconstruction loss and the second term is a KL divergence term, encouraging the estimated posterior probability <italic>q</italic><sub><italic>ψ</italic></sub> (<italic>v</italic>|<italic>I</italic><sub><italic>x</italic></sub>; <italic>I</italic><sub><italic>a</italic></sub>) to be close to the prior <italic>p</italic>(<italic>v</italic>). VoxelMorph encourages the smoothness of the velocity field <italic>v</italic> by setting <inline-formula><mml:math display="inline" id="M11"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">Λ</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula>, where the parameter <italic>λ</italic> controls the scale of the velocity field and <italic>L</italic> is the graph Laplacian matrix defined on the Euclidean grid. <italic>L</italic> is computed as <italic>L =</italic> (<italic>D</italic> – <italic>A</italic>), where <italic>A</italic> is the neighborhood adjacency matrix and <italic>D</italic> is the graph degree matrix. Thus, <xref rid="FD1" ref-type="disp-formula">Eq. (1)</xref> can be rewritten as:
<disp-formula id="FD2"><label>(2)</label><mml:math display="block" id="M12"><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="normal">log</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi>v</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>ψ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi mathvariant="normal">log</mml:mi><mml:mo>|</mml:mo><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msub><mml:mi>E</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>∘</mml:mo><mml:msub><mml:mi mathvariant="normal">Φ</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>∘</mml:mo><mml:msub><mml:mi mathvariant="normal">Φ</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mspace width="0.25em"/><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="normal">log</mml:mi><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Λ</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:mi mathvariant="italic">tr</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Λ</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="normal">Λ</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mspace width="1em"/><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>K</mml:mi></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>∘</mml:mo><mml:msub><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>∘</mml:mo><mml:msub><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mspace width="1em"/><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mi>D</mml:mi><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="normal">log</mml:mi><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="normal">Λ</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo></mml:math></disp-formula>
where <italic>K</italic> is the number of samples <inline-formula><mml:math display="inline" id="M13"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>˜</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:math></inline-formula> used to approximate the expectation. We use <italic>K</italic> = 1. We treat the fixed atlas <italic>I</italic><sub><italic>a</italic></sub> and the warped individual image <italic>I</italic><sub><italic>x</italic></sub> ∘ Φ<sub><italic>v</italic></sub> as <italic>MN</italic> × 1 vectors. We denote this naive application of the registration of 2D projected images as the <italic>2D VoxelMorph</italic> method and it serves as a benchmark in our experiments.</p>
      <p id="P22">Unfortunately, the 2D projection step introduces two main problems, as shown in <xref rid="F1" ref-type="fig">Fig. 1</xref>:</p>
      <list list-type="bullet" id="L2">
        <list-item>
          <p id="P23">varying level of distortions with different latitudes (distortion increases from the equator to the poles); and</p>
        </list-item>
        <list-item>
          <p id="P24">inability to represent the periodic property of θ and the geometry of the poles (an enclosed spherical surface is projected onto a rectangular image region, introducing discontinuities at the image borders).</p>
        </list-item>
      </list>
      <p id="P25">Hence, the <italic>2D VoxelMorph</italic> method over-weights the alignment for near-pole regions, yielding misalignment in most regions even compared to global rigid registration as shown in <xref rid="S10" ref-type="sec">Section 3</xref>.</p>
    </sec>
    <sec id="S5">
      <label>2.3.</label>
      <title>Proposed method: SphereMorph</title>
      <p id="P26">To address the above issues, we propose SphereMorph. We start by defining the registration problem in the spherical domain. The spherical representation of an individual’s surface is first rotated for a rigid alignment with the atlas, as in FreeSurfer. The spherical surface is parameterized by the longitude θ and latitude <italic>ϕ</italic> and sampled to an <italic>M</italic> × <italic>N</italic> two-dimensional image with a geometric or functional feature, e.g. convexity, assigned as a pixel intensity measure.</p>
      <sec id="S6">
        <label>2.3.1.</label>
        <title>Prior correction</title>
        <p id="P27">We assume that the displacement field is smooth on the sphere considering the anatomical continuity via a graph Laplacian regularizer. We define a neighbor connectivity graph <italic>G</italic><sub><italic>S</italic></sub> on the spherical manifold and represent the velocity with respect to Cartesian coordinates as a signal defined on this graph. Let <inline-formula><mml:math display="inline" id="M14"><mml:mi mathvariant="script">T</mml:mi></mml:math></inline-formula> denote the conversion from polar to Cartesian coordinates, that is <inline-formula><mml:math display="inline" id="M15"><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="script">T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">sin</mml:mi><mml:mi>ϕ</mml:mi><mml:mi mathvariant="normal">cos</mml:mi><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">sin</mml:mi><mml:mi>ϕ</mml:mi><mml:mi mathvariant="normal">sin</mml:mi><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">cos</mml:mi><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, then the geodesic velocity at each vertex <italic>Ver</italic><sub><italic>i</italic></sub>(<italic>θ</italic><sub><italic>i</italic></sub>, <italic>ϕ</italic><sub><italic>i</italic></sub>), <italic>i</italic> ∈ [1, 2,..,<italic>MN</italic>] is given by <inline-formula><mml:math display="inline" id="M16"><mml:mrow><mml:mi>v</mml:mi><mml:mo>’</mml:mo><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="script">T</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Φ</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="script">T</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
        <p id="P28">Each vertex on the projected image is considered as a node and each grid edge connecting two adjacent nodes as their edge in <italic>G</italic><sub><italic>S</italic></sub>. We connect leftmost and rightmost nodes due to the periodicity in longitude. The weight of the connection between vertices in <italic>G</italic><sub><italic>S</italic></sub> varies with location to account for the horizontal edge distance on the spherical surface which is proportional to sin <italic>ϕ</italic>. Thus, we define the weight of each grid edge connecting vertices <italic>Ver</italic><sub><italic>i</italic></sub>(<italic>θ</italic><sub><italic>i</italic></sub>, <italic>ϕi</italic>), <italic>Ver</italic><sub><italic>j</italic></sub>(<italic>θ</italic><sub><italic>j</italic></sub>, <italic>ϕj</italic>) as:
<disp-formula id="FD3"><label>(3)</label><mml:math display="block" id="M17"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mspace width="0.25em"/><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi mathvariant="normal">sin</mml:mi><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if</mml:mtext></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≠</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if</mml:mtext></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≠</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p>
        <p id="P29">We construct the corresponding neighborhood adjacency matrix <italic>A</italic><sub><italic>S</italic></sub> with entries <inline-formula><mml:math display="inline" id="M18"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and the degree matrix <italic>D</italic><sub><italic>S</italic></sub> with diagonal entries <inline-formula><mml:math display="inline" id="M19"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mo>∑</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Finally, we denote the Laplacian of this weighted graph as <italic>L</italic><sub><italic>S</italic></sub> = <italic>D</italic><sub><italic>S</italic></sub> – <italic>A</italic><sub><italic>S</italic></sub> and define the covariance of geodesic velocity as <inline-formula><mml:math display="inline" id="M20"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>’</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">Λ</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Intuitively, this formulation can be seen as increasing the regularization near the poles, where the Euclidean distance between mesh nodes is small, and decreasing the weighting near the equator where the Euclidean distances are larger.</p>
      </sec>
      <sec id="S7">
        <label>2.3.2.</label>
        <title>Distortion correction</title>
        <p id="P30">For VoxelMorph, which deals with Euclidean image registration, the sampling of grid points is equally distributed. However, a spherical parameterization leads to denser sampling grids for regions at higher latitudes as shown in <xref rid="F1" ref-type="fig">Fig. 1</xref>. Thus, we assign mesh locations from these regions lower weights in computing the data-fitting term, by introducing a diagonal matrix <italic>S</italic> ∈ <italic>R</italic><sup><italic>MN</italic>×<italic>MN</italic></sup> with each diagonal entry encoding the resampling weight <italic>S</italic><sub><italic>ii</italic></sub> = sin<italic>ϕ</italic><sub><italic>i</italic></sub> for each vertex <italic>Ver</italic><sub><italic>i</italic></sub>(<italic>θ</italic><sub><italic>i</italic></sub>, <italic>ϕ</italic><sub><italic>i</italic></sub>) and model <inline-formula><mml:math display="inline" id="M21"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi>v</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>∘</mml:mo><mml:msub><mml:mi mathvariant="normal">Φ</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The first data-fitting term in <xref rid="FD1" ref-type="disp-formula">Eq. (1)</xref> is then modified as:
<disp-formula id="FD4"><label>(4)</label><mml:math display="block" id="M22"><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="normal">log</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi>v</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>K</mml:mi></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>∘</mml:mo><mml:msub><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>∘</mml:mo><mml:msub><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      </sec>
      <sec id="S8">
        <label>2.3.3.</label>
        <title>Loss function</title>
        <p id="P31">Starting with <xref rid="FD1" ref-type="disp-formula">Eq. (1)</xref> and taking into account the spherical geometry, we arrive at the below objective function:
<disp-formula id="FD5"><label>(5)</label><mml:math display="block" id="M23"><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="normal">log</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi>v</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>ψ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>‖</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="normal">log</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi>v</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>ψ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>’</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>‖</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>’</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>K</mml:mi></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>∘</mml:mo><mml:msub><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>∘</mml:mo><mml:msub><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>’</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="normal">log</mml:mi><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>’</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>’</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="normal">Λ</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>’</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>.</mml:mo></mml:math></disp-formula>
where <inline-formula><mml:math display="inline" id="M24"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>’</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math display="inline" id="M25"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>’</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The first term in <xref rid="FD5" ref-type="disp-formula">Eq. (5)</xref> is the data-fitting term, which encourages matching surfaces after warping and the second term drives the posterior to approximate the smoothness prior defined on a spherical grid.</p>
      </sec>
      <sec id="S9">
        <label>2.3.4.</label>
        <title>Network structure</title>
        <p id="P32"><xref rid="F2" ref-type="fig">Fig. 2</xref> illustrates the individual stages task fmriof our pipeline. For a given vertex at location (<italic>θ</italic>, <italic>φ</italic>), we utilize inverse gnomonic projection, which maps points on the tangent plane to the spherical surface as in SphereNet (<xref rid="R7" ref-type="bibr">Coors et al., 2018</xref>), to obtain the corresponding locations on the projected image for the neighbor vertex on its tangent plane. We implement the convolution and pooling operations in each 3× 3 local tangent patch shown in <xref rid="F1" ref-type="fig">Fig. 1</xref> and build a UNet core (<xref rid="R31" ref-type="bibr">Ronneberger et al., 2015</xref>), which contains four downsampling and four upsampling layers. Following the sampling layer, seven scaling and squaring operators take the layer output, or velocity field, and return a diffeomorphism Φ. <italic>2D VoxelMorph</italic> uses a dense spatial transformer layer on (<italic>θ</italic>’, <italic>φ</italic>’) after displacement to retrieve the warped image while SphereMorph warps the image by computing the interpolation grids as ((<italic>θ</italic>’+2<italic>π</italic>) mod2<italic>π</italic>, <italic>φ</italic>’) for transformer layer. The model is implemented in Keras with a Tensorflow backend and the ADAM optimizer as part of the VoxelMorph package. We set the hyperparameter λ = 3× 10<sup>7</sup>, the learning rate to 1× 10<sup>−5</sup>, and trained our dense model for 200 epochs, by which point the loss had converged as shown in <xref rid="F3" ref-type="fig">Fig. 3</xref>. All experiments were conducted on the same workstation with Intel Xeon <email>X5550@2.67GHz</email> and used NVIDIA Tesla P40C for all CNN-based methods. The source code of SphereMorph will be released to the public upon acceptance of this manuscript at <ext-link ext-link-type="uri" xlink:href="https://github.com/voxelmorph/spheremorph">https://github.com/voxelmorph/spheremorph</ext-link>.</p>
      </sec>
    </sec>
  </sec>
  <sec id="S10">
    <label>3.</label>
    <title>Experimental setup</title>
    <p id="P33">To demonstrate the accuracy and efficiency of the proposed registration framework, SphereMorph, we used two sets of experiments, cortical parcellation and fMRI group analysis, on two independent test data sets.</p>
    <sec id="S11">
      <label>3.1.</label>
      <title>Data</title>
      <sec id="S12">
        <label>3.1.1.</label>
        <title>Training data set</title>
        <p id="P34">The spherical atlas from FreeSurfer served as the fixed image in our model. As training data, we used the surface convexity maps of the left hemispheres of 800 randomly selected subjects from the ADNI1 cohort that was released as part of the publicly available Alzheimer’s Disease Neuroimaging Initiative (ADNI) (<xref rid="R28" ref-type="bibr">Mueller et al., 2005</xref>) (<ext-link ext-link-type="uri" xlink:href="http://adni.loni.usc.edu/">http://adni.loni.usc.edu/</ext-link>) and was processed by FreeSurfer v5.1. The ADNI was launched in 2003 as a public-private partnership, led by Principal Investigator Michael W. Weiner, MD. The primary goal of ADNI has been to test whether serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment (MCI) and early Alzheimer’s disease (AD). The ADNI dataset consists of longitudinal T1-weighted scans from 836 subjects that are divided into four classes: elderly controls (n = 252), early mild cognitive impairment (eMCI, n = 215), late MCI (lMCI, n = 176), and AD (n = 193). The subjects were scanned on average 4.8 times (minimum: a single time; maximum: 11 times; 4013 scans in total), with a mean interval between scans equal to 286 days (minimum: 23 days, maximum: 1567 days). The mean age at baseline of the subjects was 75.1 ± 6.6 years. Since the ADNI project spans multiple sites, different scanners were used to acquire the images; further details on the acquisitions can be found at <ext-link ext-link-type="uri" xlink:href="http://adni.loni.usc.edu/data-samples/adni-data-inventory/">http://adni.loni.usc.edu/data-samples/adni-data-inventory/</ext-link>.</p>
      </sec>
      <sec id="S13">
        <label>3.1.2.</label>
        <title>Test data sets</title>
        <p id="P35">(1) We used FreeSurfer-processed MRI scans of 39 subjects from a cohort recruited by the Washington University Alzheimer’s Disease Research Center (ADRC) (<xref rid="R42" ref-type="bibr">Van Horn et al., 2001</xref>). The MRI scans were acquired on a 1.5T Vision system (Siemens, Erlangen Germany). T1-weighted magnetization-prepared rapid gradient echo (MP-RAGE) scans were obtained according to the following protocol: two sagittal acquisitions, FOV = 224, Matrix = 256 × 256, Resolution = 1 × 1 × 1.25<italic>mm</italic><sup>3</sup>, TR= 9.7 ms, TE = 4 ms, Flip angle = 10, TI = 20 ms, TD = 200 ms. Two acquisitions were averaged together to increase the contrast-to-noise ratio. For the cortical parcellation experiment, we separated the data set into two: 9 validation subjects and 30 held-out test subjects. All subjects have 34 cortical areas manually annotated (<xref rid="R11" ref-type="bibr">Desikan et al., 2006</xref>), making them ideal for evaluating registration accuracy.</p>
        <p id="P36">(2) Additionally, we used another set of 100 unrelated young and healthy subjects from the Human Connectome Project (HCP) (<xref rid="R41" ref-type="bibr">Van Essen et al., 2013</xref>) as a second test set. The HCP project used state-of-the-art fMRI hardware and acquisition parameters in a sample of highly educated, healthy subjects. For each subject, seven task fMRI sessions were collected, including working memory, gambling, motor, language, social cognition, relational processing and emotional processing, totaling 48:30 <italic>min</italic> of fMRI data. The acquisition parameters and minimal preprocessing of these data have been described extensively elsewhere (<xref rid="R17" ref-type="bibr">Glasser et al., 2013</xref>; <xref rid="R4" ref-type="bibr">Barch et al., 2013</xref>).</p>
      </sec>
    </sec>
    <sec id="S14">
      <label>3.2.</label>
      <title>Baselines</title>
      <p id="P37">We compared our proposed registration method to rigid registration on the sphere (i.e. two rotations) and four other nonlinear registration methods: a 2D version of VoxelMorph, Multimodal Surface Matching (MSM) (<xref rid="R30" ref-type="bibr">Robinson et al., 2014</xref>), Spherical Demons (SD) (<xref rid="R45" ref-type="bibr">Yeo et al., 2010</xref>), and FreeSurfer (FS) spherical registration (<xref rid="R14" ref-type="bibr">Fischl et al., 1999</xref>). We chose the sulcal depth as the input feature for all registration methods. Additionally, we also explored the usage of the curvature and T1/T2 maps together with sulcal depth in the HCP functional group analysis experiments. Additionally, we also explored the usage of the curvature and T1/T2 maps together with sulcal depth in the HCP functional group analysis experiments. We trained 2D VoxelMorph using the same training set as described above and selected the hyperparameter <italic>λ</italic> that yielded the best performance on the validation set. MSM is a surface-based registration approach that offers significant flexibility with regards to the set of features that are used to drive the spatial alignment. MSM drives the deformation via aligning local patches around control points in a multi-resolution fashion. It is implemented on CPU using a fast, multi-resolution, discrete optimization scheme, offering significant computational speed-up compared to other classical methods. We ran MSM over three resolution levels with five iterations per level. Specifically, we set the regularization parameters as (0.1, 0.2, 0.3) for the parcellation experiments. For the functional group analysis experiments, we relied on the publicly released ‘MSMSulc’ results which were generated by running MSM with high regularization parameters (10, 7.5, 7.5) and using the sulcal depth feature. Compared to the low regularization, MSM with high regularization has been demonstrated to yield lower group alignment in folding patterns but smaller area distortions (<xref rid="R30" ref-type="bibr">Robinson et al., 2014</xref>), which leads to better functional alignment after following registration steps using “myelin” maps. Spherical Demons, a fast diffeomorphic landmark-free surface registration tool implements the regularization for its objective function via iterative Gaussian smoothing. We explored a range of smoothing iteration numbers (5, 10, 15, 20) to optimize performance and used the results of 10 iterations.</p>
    </sec>
    <sec id="S15">
      <label>3.3.</label>
      <title>Evaluation</title>
      <p id="P38">To evaluate registration accuracy, we relied on the resulting spatial transformations to project the atlas parcellation back to individual scan space. For an accurate registration solution, the test subject’s cortical parcellations will resemble the manually outlined versions. In order to quantify how well they match, we computed the Dice overlap coefficient (<xref rid="R12" ref-type="bibr">Dice, 1945</xref>), the overall Mean Minimum Distance (MMD) as well as the individual MMD measures for each anatomical region. The Dice overlap coefficient, <inline-formula><mml:math display="inline" id="M26"><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>∩</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, quantifies the surface area overlap (<italic>R</italic><sub><italic>M</italic>∩<italic>A</italic></sub>) between manual (<italic>R</italic><sub><italic>M</italic></sub>) and automatic method-generated parcels (<italic>R</italic><sub><italic>A</italic></sub>) and the MMD describes the discrepancy between the parcellation boundaries: <italic>MMD</italic>(<italic>M</italic>, <italic>A</italic>)= 1/<italic>N</italic> · Σ<sub><italic>i</italic></sub><italic>d</italic>(<italic>m</italic><sub><italic>i</italic></sub>, <italic>a</italic><sub><italic>i</italic></sub>) where <italic>d</italic>(<italic>m</italic><sub><italic>i</italic></sub>, <italic>a</italic><sub><italic>i</italic></sub>) denotes the Euclidean distance between a vertex <italic>m</italic><sub><italic>i</italic></sub> on a manual boundary and its corresponding closest vertex <italic>a</italic><sub><italic>i</italic></sub> on an automatic method-generated boundary. Additionally, we also tested how consistently the various registration methods aligned anatomical features, such as convexity/sulcal depth and curvature.</p>
      <p id="P39">In the second set of experiments, we mapped all individuals within the group to the HCP’s 2 mm standard grayordinates space (<xref rid="R17" ref-type="bibr">Glasser et al., 2013</xref>), using the displacement field, then computed group maps of task-evoked activations. To evaluate the quality of the group alignment quantitatively, we computed average correlations between the group-average and the projected individual activation maps across 86 task contrasts derived from the seven fMRI tasks.</p>
    </sec>
  </sec>
  <sec id="S16">
    <label>4.</label>
    <title>Results</title>
    <sec id="S17">
      <label>4.1.</label>
      <title>Computational efficiency</title>
      <p id="P40"><xref rid="T1" ref-type="table">Table 1</xref> summarizes registration accuracy and computation time for all methods. All the compared methods take spherical cortical surfaces as input, which are generated by brain surface tessellation including fixing topological defects and inflation to the sphere from volumetric segmentation. For a brain surface with 120k-150k vertices, it takes around 0.9h and 0.25h CPU computation time for surface tessellation and inflation, respectively. The comparison of registration time illustrates that the proposed method ran approximately 20 times faster than the conventional registration method in FreeSurfer. On a CPU, the default FreeSurfer pipeline takes around 13 min to complete the spherical registration. The total computation time of our proposed framework is approximately 0.74 min, including the initial alignment, deep network deployment, and displacement field mapping. With GPU acceleration, the computation time of our method can be reduced to 0.65 min, where the deep network deployment is accomplished within a second. Compared with other registration methods including MSM and Spherical Demons (SD), CNN-based methods provide more than an order of magnitude improvement in execution speed.</p>
    </sec>
    <sec id="S18">
      <label>4.2.</label>
      <title>Cortical parcellation experiment</title>
      <sec id="S19">
        <label>4.2.1.</label>
        <title>Parcellation accuracy</title>
        <p id="P41"><xref rid="F4" ref-type="fig">Fig. 4</xref> shows representative cortical segmentation results from all the methods on two randomly selected test subjects from the ADRC dataset. The 2D VoxelMorph-estimated annotation exhibits large differences compared to the ground truth in the lateral occipital regions (marked by white arrows), while FreeSurfer and SphereMorph provide results close to the manual annotations. <xref rid="F5" ref-type="fig">Fig. 5</xref> displays the two subjects for which our proposed method yields the lowest and highest overall Dice values.</p>
        <p id="P42"><xref rid="T1" ref-type="table">Table 1</xref> provides an overview of registration accuracy by comparing manual annotations to parcellations generated by the different registration methods, including rigid alignment, 2D Voxelmorph, MSM, SD, FreeSurfer as well as our proposed method. Our proposed method and FreeSurfer achieved the highest accuracy. SphereMorph yields significantly higher overall Dice coefficients than MSM after performing a one-tail Wilcoxon rank sum test on their respective Dice coefficients (<italic>p</italic> = 0.0318). The deep learning baseline, 2D VoxelMorph, performed significantly worse. This is due to the fact that this method does not account for the distortions intrinsic to the spherical coordinate system. <xref rid="F6" ref-type="fig">Fig. 6</xref> compares parcel-wise Dice overlap coefficient values associated with the different registration methods. Our method produced higher mean Dice overlap coefficients than MSM for all structures except the Entorhinal, Paracentral and Middletemporal, and showed statistically significant improvement in regional Dice overlap coefficient compared to MSM in the Temporalpole (<italic>p</italic> = 0.0090), Parahippocampal (<italic>p</italic> = 0.0102), Transversetemporal (<italic>p</italic> = 0.0051), Caudalmiddlefrontal (<italic>p</italic> = 0.0237), Rostralmiddlefrontal (<italic>p</italic> = 0.0433) and Lingual (<italic>p</italic> = 0.0148) regions. Compared to Spherical Demons, SphereMorph produced higher mean regional Dice coefficients in 26 out of 34 regions. The regions where this was not the case are the Entorhinal, Rostralanteriorcingulate, Parahippocampal, Fusiform, Transversetemporal, Insula and Lateralorbitofrontal areas. Pairwise t-tests show that SphereMorph compared to SD generated statistically significantly higher Dice scores in the Postcentral (<italic>p</italic> = 0.0015) and Rostralmiddlefrontal (<italic>p</italic> = 0.0191) areas while SD outperformed SphereMorph in the Transversetemporal (<italic>p</italic> = 0.0245) region. In all other areas the performance difference was not statistically significant. Postcentral and Rostralmiddlefrontal regions exhibit larger standard deviations (0.12± 0.05 and 0.15 ± 0.09 respectively) as shown in the atlas while the Transversetemporal region has higher agreement in sulcal depth (atlas standard deviation 0.05 ± 0.02) across subjects. The inverse-variance weighting in SphereMorph makes it more flexible in regions with large group variance, leading to better parcellation results in these regions when compared to SD. SD uses the sum of squared differences as a similarity estimate and yields good parcellation in regions with high agreement in sulcal depth.</p>
      </sec>
      <sec id="S20">
        <label>4.2.2.</label>
        <title>Group average sulcal maps</title>
        <p id="P43">To evaluate the performance on a finer scale, we also computed the group mean sulcal depth maps after registration for the test data. The resulting mean and standard deviation maps are displayed in <xref rid="F7" ref-type="fig">Fig. 7</xref>. We assume that a better group alignment leads to a sharper group mean and smaller group variation. As expected, the group mean maps provide more detailed information for all nonlinear registration methods than rigid alignment. Moreover, all these methods show smaller standard deviations, suggesting that they provide better alignment in convexity.</p>
        <p id="P44">All nonlinear registration methods exhibited similar distributions of sulcal variations across brain regions. Specifically, the pre-central, post-central and insula regions show lower standard deviation, suggesting higher agreement of cross-subject convexity in these regions.</p>
      </sec>
      <sec id="S21">
        <label>4.2.3.</label>
        <title>Robustness analysis</title>
        <p id="P45">We investigated the impact of pole positioning on the registration accuracy for SphereMorph by projecting the sphere using 9 different north pole locations spanning <italic>θ</italic> ∈ (0, <italic>π</italic> /2, <italic>π</italic>), <italic>φ</italic> ∈ (0, <italic>π</italic> /6, <italic>π</italic> /3, <italic>π</italic> /2) and registering the corresponding 2D planar images with corresponding atlas data. We generated the deformed sphere and then re-computed all the region- and distance-based metrics. To evaluate the robustness with respect to different projection centers, we conducted analysis of variance (ANOVA) between all the computed evaluation metrics for these 9 groups. None of the ANOVA analyses (overall Dice: <italic>F</italic> = 0.2,<italic>p</italic> = 0.9842, overall MMD: <italic>F</italic> = 0.93, <italic>p</italic> = 0.4821) found any significant differences between the 9 groups of cortical parcellations, indicating that the registration accuracy is not sensitive to the arbitrary location of the poles.</p>
      </sec>
      <sec id="S22">
        <label>4.2.4.</label>
        <title>Areal distortion analysis</title>
        <p id="P46">We calculated the Jacobian map for registered surfaces as an areal distortion measure, where a value close to one indicates a small distortion. <xref rid="F8" ref-type="fig">Fig. 8</xref> compares the distortion maps for all compared registration methods, suggesting least amount of distortion introduced by SphereMorph. Additionally, we measured the percentage of vertices with a non-positive Jacobian determinant for SphereMorph in the case of a randomly selected subject and it resulted in 0.63%. While we model the deformation field as a diffeomorphism in the continuous case, discretization and numerical errors can lead to a small fraction of locations with Jacobians that are not positive definite.</p>
      </sec>
    </sec>
    <sec id="S23">
      <label>4.3.</label>
      <title>Cross-subject alignment of fMRI activation maps</title>
      <sec id="S24">
        <label>4.3.1.</label>
        <title>Group average maps</title>
        <p id="P47"><xref rid="F9" ref-type="fig">Fig. 9</xref> illustrates the quality of group-wise alignment of convexity after MSM, Spherical Demons, FreeSurfer, and our method were run on our second test data set. Our method yielded smaller variations within the group than MSM (<italic>p</italic> = 0.0013), just as in the case of the cortical parcellation experiments. For these experiments, we used default regularization parameters for FreeSurfer and optimized the regularization level of Spherical Demons via exploring the number of smoothing iterations for best parcellation performance. Regarding MSM, we relied on the HCP-distributed MSM registration results that were driven only by folding patterns with a high regularization, as suggested in (<xref rid="R30" ref-type="bibr">Robinson et al., 2014</xref>), where such settings lead to better functional alignment when followed by a registration step using “myelin” maps. This choice lead to the high group variance displayed on <xref rid="F9" ref-type="fig">Fig. 9</xref>. In order to make this comparison more fair, we also ran MSM using a combination of the T1/T2 map with the folding patterns for an evaluation of accuracy in functional alignments as explained below. We denote the MSM registration version incorporating folding patterns along with the T1/T2 map as MSM-T1/T2 and the SphereMorph version as SphereMorph-T1/T2.</p>
        <p id="P48">While we expect folding patterns to be predictive of functional areas, this relationship is variable and complex. In order to assess how well the various methods align functionally homologous regions across subjects, we computed group average functional activation maps for all 86 tasks using registration driven by sulcal depth information. <xref rid="F10" ref-type="fig">Fig. 10</xref> compares group activation results from the 100 subjects for the Gambling reward contrast. Our method improves the functional alignment across subjects over MSM, particularly in the inferiorparietal and precuneus regions, indicated by the arrows. To evaluate the group alignment performance quantitatively, we computed the average correlations between the group-average and individual activation maps after registration across 86 task contrasts derived from seven tasks. All results in <xref rid="T2" ref-type="table">Table 2</xref> were computed using registration results driven by sulcal depth maps except for MSM-T1/T2 and SphereMorph-T1/T2, where the T1/T2 map was also incorporated in order to drive the registration.</p>
        <p id="P49"><xref rid="F11" ref-type="fig">Fig. 11</xref> displays average correlations for the different registration methods. For all 86 contrasts, SphereMorph resulted in significantly higher correlation coefficients than MSM (increase of 0.043± 0.009, <italic>p</italic> = 0.028 and relative increase of 8.99 ± 2.85%, <italic>p</italic> = 0.0024).</p>
      </sec>
      <sec id="S25">
        <label>4.3.2.</label>
        <title>Alignment performance analysis using multi-modality features</title>
        <p id="P50">We evaluated the performance for the proposed SphereMorph using curvature and (separately) T1/T2 features to their respective atlases, using our sulcal depth alignment as initialization. We computed a ‘T1/T2’ atlas from three rounds of CNN registration within group. <xref rid="F12" ref-type="fig">Fig. 12</xref> compares the group agreements in task activation for only sulc and two cascade processes using respective curvature and T1/T2 maps. Using T1/T2 as input post sulcal depth-based registration significantly improves the group agreement level (<italic>p</italic> &lt; 0.01) in four MOTOR tasks, including RF, RF-AVG, neg-RF and AVG-RF, as indicated by the arrows. All four tasks are related to right finger tapping. <xref rid="F13" ref-type="fig">Fig. 13</xref> displays the group average activation for RF contrast. Using the T1/T2 map leads to a larger region with positive response in the paracentral and superiorfrontal areas (marked by arrows) compared to using sulcal depth as the only input.</p>
      </sec>
    </sec>
  </sec>
  <sec id="S26">
    <label>5.</label>
    <title>Discussions and conclusions</title>
    <p id="P51">In this paper we present a learning-based method, SphereMorph, for registering cortical surfaces and investigate its performance using two sets of experiments, by comparing it to rigid alignment, 2D VoxelMorph, MSM, Spherical Demons, and FreeSurfer. The proposed SphereMorph yields results that are comparable or superior to the state-of-the-art methods for alignment of folding patterns, cortical parcellation and functional alignment, while offering approximately a 20× computational speedup, showing the accuracy and efficiency of our method for cortical registration. Different from existing spherical networks that learn a node-wise (e.g. cortex parcellation) or subject-wise measure (e.g. gender or abnormality classification), our registration framework is re flexible and can be applied for both parcellation as well as various group analysis tasks.</p>
    <p id="P52">Compared to conventional registration methods, SphereMorph takes 2D parameterized images as input, outputs the deformation field in a 2D canonical space, then warps the sphere in the original Cartesian space. We directly address two issues associated with the 2D projection: the effects of substantial distortions introduced by the parameterization and the violation of continuity at the borders of the 2D plane (i.e. the imposition of spherical topology). To account for the distortion introduced by the parameterization, we modify the data likelihood term and construct the deformation velocity on a graph weighted by the metric tensor of the parameterization. We use the same weights as in existing work by <xref rid="R25" ref-type="bibr">Khasanova and Frossard (2017)</xref> to construct our graph as it has been shown to be capable of encoding the geometry of an omnidirectional camera in the final feature representation of an image. In the implementation of network structure, we encode the neighborhood information by leveraging a 3× 3. spherical kernel defined in SphereNet (<xref rid="R7" ref-type="bibr">Coors et al., 2018</xref>) instead of a conventional 2D kernel for the network convolution and pooling operations. Compared to the spherical kernel defined in (<xref rid="R35" ref-type="bibr">Su and Grauman, 2017</xref>), the SphereNet kernel is able to handle both the discontinuity problem and planar distortions. While other existing spherical kernels (<xref rid="R32" ref-type="bibr">Seong et al., 2018</xref>; <xref rid="R24" ref-type="bibr">Jiang et al., 2019</xref>; <xref rid="R47" ref-type="bibr">Zhao et al., 2019</xref>) defined on the 3D mesh could be used for the spherical operations in our networks, given that we model the registration problem based on the longitude-latitude representation, an additional interpolation step would need to be introduced for them, potentially introducing error.</p>
    <p id="P53">We then use a scaling-and-squaring layer to obtain the exponential map in the spherical domain and this can be efficiently implemented on the GPU, without the density of the nodes significantly impacting runtime (<xref rid="R9" ref-type="bibr">Dalca et al., 2019</xref>). In addition, we modify the spatial transformer layer to represent the periodicity of longitude in spherical coordinates. Combining these adaptations, our model shows a higher agreement with manual parcellations compared to 2D VoxelMorph, which does not account for distortions and topological changes induced by the 2D parameterization. For 2D VoxelMorph, each point on the rectangular grid contributes equally to the registration, resulting in the alignment of regions near the poles affecting the energy functional more than other regions with the same area in the Euclidean embedding space. Thus, the optimized deformation is over-fitted in these regions. Our experimental results confirm this when comparing the automatically generated parcellations to the manual ones. With an excessive weighting of the alignment of these polar regions, 2D VoxelMorph performs even worse than the initial alignment.</p>
    <p id="P54">Our proposed registration framework is applicable to any signal or feature that can be represented or sampled onto the cortical surface. However, in the cortical parcellation experiment, the ADRC dataset only has structural scans. Hence, we trained our model as well as MSM and SD using convexity values to drive the registrations. FreeSurfer originally trained its parcellation atlas using the manual annotations on the ADRC data set and yields the best parcellation performance but longest execution time among all the compared methods. Compared to FreeSurfer, which uses a line minimization optimization strategy to obtain a dense displacement field, Spherical Demons reduces the registration problem to a Gauss-Newton optimization step of a non-linear least-squares problem and a displacement field smoothing operation by simple convolution, resulting in faster computation time. We found no significant difference between our proposed SphereMorph and FreeSurfer while SphereMorph shows significant smaller group variations and higher overall Dice coefficients when compared to MSM, suggesting SphereMorph achieves better structural alignment than MSM.</p>
    <p id="P55">In addition to the accuracy of the structural alignment, when using the same feature (i.e. convexity) for registration, both qualitative and quantitative evaluation results for the task-related experiments demonstrate that the proposed method also yields higher agreement of functional regions across subjects than the current registration method in the Human Connectome Project (HCP) pipeline (<xref rid="R17" ref-type="bibr">Glasser et al., 2013</xref>). SphereMorph improves the within-group correlation coefficients significantly for all tasks compared with MSM. The group agreement in convexity is also higher after SphereMorph registration than with MSM. This implies the relationship between cortical folding patterns and boundaries of functional areas as demonstrated in previous work (<xref rid="R21" ref-type="bibr">Hinds et al., 2008</xref>). However, using folding-based features alone may not be sufficient to provide an accurate functional alignment across the entire cortex due to regions with highly variable folding patterns across subjects, as well as structure-function variability. Thus, in the functional alignment experiments, we take the sulcal depth registration as initialization and then use cortical T1/T2 maps as input of our network for a multi-step alignment. The cortical T1/T2 maps are computed based on the ratio of T1-weighted to T2-weighted images which correlates with many functionally distinct areas in individual subjects (<xref rid="R16" ref-type="bibr">Glasser and Van Essen, 2011</xref>). The combination of T1/T2 and sulcal depth shows significant improvement in group alignment of MOTOR-related tasks, indicating the correlation between T1/T2 and MOTOR-related regions. SphereMorph, just like FreeSurfer, uses an inverse variance weighting scheme to compute the data similarity term. At final evaluation and in comparison with other methods not using such a weighting cheme, SphereMorph can be at a disadvantage due to the presence of areas that are not optimized for alignment. Interestingly though, we found that SphereMorph yields better parcellation outcomes than SD in the Postcentral and Rostralmiddlefrontal areas, which are regions of high variance in the atlas, while SD performs better in the Transversetemporal region that is of lower variance. Consistent with these findings, <xref rid="T2" ref-type="table">Table 2</xref> demonstrates higher accuracy functional alignment for SphereMorph compared to SD, while SD yields smaller group variations in folding patterns. This implies that increased structural variability is a marker of reduced structure-function coupling. Thus we believe that using inverse-variance weighting helps improving the functional alignment.</p>
    <p id="P56">All experiments in this paper focus on subject-to-atlas registration. We will explore the performance of our network for inter-subject registration in the future. In addition, our proposed model can include other contrasts in addition to the convexity metrics used above to drive the registration. Existing studies have shown the improvement in group registration accuracy relying on features generated from resting-state functional MRI (rfMRI) after conventional convexity-driven registration (<xref rid="R30" ref-type="bibr">Robinson et al., 2014</xref>; <xref rid="R39" ref-type="bibr">Tong et al., 2017</xref>). In the future, we will investigate the use of these to further improve the accuracy of our technique.</p>
  </sec>
</body>
<back>
  <ack id="S27">
    <title>Acknowledgement</title>
    <p id="P57">Support for this research was provided in part by the NIH BRAIN Initiative grant U01MH117023, the Eunice Kennedy Shriver National Institute of Child Health and Human Development (NICHD) (5R21HD95338-02, 5R01HD085813-04, 5R01 HD065762-09, 5R01HD093578-03, 5R01lEB024343-03), the National Institute of Biomedical Imaging and Bioengineering (P41EB015896, 1R01EB 023281, R01EB006758, R21EB018907, R01EB019956, 5R03E B022754-02), the National Institute on Aging (1R56AG064027, 5R01AG008122, R01AG016495), the National Institute of Mental Health 5U01MH109589-04, the National Institute of Diabetes and Digestive and Kidney Diseases (1-R21-DK-108277-01), the National deep learning based neuroimaging pipeline of Neurological Disorders and Stroke (R01NS0525851, R21NS072652, R01NS070963, R01NS083534, 5U01NS086625, 5U24NS10059103, R01NS105820), and was made possible by the resources provided by Shared Instrumentation Grants 1S10RR023401, 1S10RR019307, and 1S10RR023043. Additional support was provided by the NIH Blueprint for Neuroscience Research (5U01-MH093765), part of the multi-institutional Human Connectome Project. In addition, BF has a financial interest in CorticoMetrics, a company whose medical pursuits focus on brain imaging and measurement technologies. BF’s interests were reviewed and are managed by Massachusetts General Hospital and Partners HealthCare in accordance with their conflict of interest policies. Data collection and sharing for this project was also partially funded by the Alzheimer’s Disease Neuroimaging Initiative (ADNI) (National Institutes of Health Grant U01 AG024904) and DOD ADNI (Department of Defense award number W81XWH-12-2-0012). ADNI is funded by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, and through generous contributions from the following: AbbVie, Alzheimer’s Association; Alzheimer’s Drug Discovery Foundation; Araclon Biotech; BioClinica, Inc.; Biogen; Bristol-Myers Squibb Company; CereSpir, Inc.; Cogstate; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli Lilly and Company; EuroImmun; F. Hoffmann-La Roche Ltd and its affiliated company Genentech, Inc.; Fujirebio; GE Healthcare; IXICO Ltd.; Janssen Alzheimer Immunotherapy Research &amp; Development, LLC.; Johnson &amp; Johnson Pharmaceutical Research &amp; Development LLC.; Lumosity; Lundbeck; Merck &amp; Co., Inc.; Meso Scale Diagnostics, LLC.; NeuroRx Research; Neurotrack Technologies; Novartis Pharmaceuticals Corporation; Pfizer Inc.; Piramal Imaging; Servier; Takeda Pharmaceutical Company; and Transition Therapeutics. The Canadian Institutes of Health Research is providing funds to support ADNI clinical sites in Canada. Private sector contributions are facilitated by the Foundation for the National Institutes of Health (<ext-link ext-link-type="uri" xlink:href="http://www.fnih.org/">www.fnih.org</ext-link>). The grantee organization is the Northern California Institute for Research and Education, and the study is coordinated by the Alzheimer’s Therapeutic Research Institute at the University of Southern California. ADNI data are disseminated by the Laboratory for Neuro Imaging at the University of Southern California.</p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="R1">
      <mixed-citation publication-type="book"><name><surname>Arsigny</surname><given-names>V</given-names></name>, <name><surname>Commowick</surname><given-names>O</given-names></name>, <name><surname>Pennec</surname><given-names>X</given-names></name>, <name><surname>Ayache</surname><given-names>N</given-names></name>, <year>2006</year><chapter-title>A log-Euclidean framework for statistics on diffeomorphisms</chapter-title> In: <source>MICCAI</source>. <publisher-name>Springer</publisher-name>, pp. <fpage>924</fpage>–<lpage>931</lpage>.</mixed-citation>
    </ref>
    <ref id="R2">
      <mixed-citation publication-type="journal"><name><surname>Ashburner</surname><given-names>J</given-names></name>, <year>2007</year><article-title>A fast diffeomorphic image registration algorithm</article-title>. <source>Neuroimage</source><volume>38</volume> (<issue>1</issue>), <fpage>95</fpage>–<lpage>113</lpage>.<pub-id pub-id-type="pmid">17761438</pub-id></mixed-citation>
    </ref>
    <ref id="R3">
      <mixed-citation publication-type="journal"><name><surname>Balakrishnan</surname><given-names>G</given-names></name>, <name><surname>Zhao</surname><given-names>A</given-names></name>, <name><surname>Sabuncu</surname><given-names>M</given-names></name>, <name><surname>Guttag</surname><given-names>J</given-names></name>, <name><surname>Dalca</surname><given-names>AV</given-names></name>, <year>2019</year><article-title>Voxelmorph: a learning framework for deformable medical image registration</article-title>. <source>IEEE Trans. Med. Imag</source><volume>38</volume>, <fpage>1788</fpage>–<lpage>1800</lpage>.</mixed-citation>
    </ref>
    <ref id="R4">
      <mixed-citation publication-type="journal"><name><surname>Barch</surname><given-names>DM</given-names></name>, <name><surname>Burgess</surname><given-names>GC</given-names></name>, <name><surname>Harms</surname><given-names>MP</given-names></name>, <name><surname>Petersen</surname><given-names>SE</given-names></name>, <name><surname>Schlaggar</surname><given-names>BL</given-names></name>, <name><surname>Corbetta</surname><given-names>M</given-names></name>, <name><surname>Glasser</surname><given-names>MF</given-names></name>, <name><surname>Curtiss</surname><given-names>S</given-names></name>, <name><surname>Dixit</surname><given-names>S</given-names></name>, <name><surname>Feldt</surname><given-names>C</given-names></name>, <etal/>, <year>2013</year><article-title>Function in the human connectome: task-fMRI and individual differences in behavior</article-title>. <source>Neuroimage</source><volume>80</volume>, <fpage>169</fpage>–<lpage>189</lpage>.<pub-id pub-id-type="pmid">23684877</pub-id></mixed-citation>
    </ref>
    <ref id="R5">
      <mixed-citation publication-type="confproc"><name><surname>Cohen</surname><given-names>TS</given-names></name>, <name><surname>Geiger</surname><given-names>M</given-names></name>, <name><surname>Köhler</surname><given-names>J</given-names></name>, <name><surname>Welling</surname><given-names>M</given-names></name>, <year>2018</year><source>Spherical CNNs</source>. In: <conf-name>International Conference on Learning Representations. ICLR</conf-name>).</mixed-citation>
    </ref>
    <ref id="R6">
      <mixed-citation publication-type="journal"><name><surname>Conroy</surname><given-names>BR</given-names></name>, <name><surname>Singer</surname><given-names>BD</given-names></name>, <name><surname>Guntupalli</surname><given-names>JS</given-names></name>, <name><surname>Ramadge</surname><given-names>PJ</given-names></name>, <name><surname>Haxby</surname><given-names>JV</given-names></name>, <year>2013</year><article-title>Inter-subject alignment of human cortical anatomy using functional connectivity</article-title>. <source>Neuroimage</source><volume>81</volume>, <fpage>400</fpage>–<lpage>411</lpage>.<pub-id pub-id-type="pmid">23685161</pub-id></mixed-citation>
    </ref>
    <ref id="R7">
      <mixed-citation publication-type="confproc"><name><surname>Coors</surname><given-names>B</given-names></name>, <name><surname>Paul Condurache</surname><given-names>A</given-names></name>, <name><surname>Geiger</surname><given-names>A</given-names></name>, <year>2018</year><source>SphereNet: learning spherical representations for detection and classification in omnidirectional images</source>. In: <conf-name>Proceedings of the European Conference on Computer Vision. ECCV)</conf-name>, pp. <fpage>518</fpage>–<lpage>533</lpage>.</mixed-citation>
    </ref>
    <ref id="R8">
      <mixed-citation publication-type="journal"><name><surname>Dalca</surname><given-names>AV</given-names></name>, <name><surname>Balakrishnan</surname><given-names>G</given-names></name>, <name><surname>Guttag</surname><given-names>J</given-names></name>, <name><surname>Sabuncu</surname><given-names>MR</given-names></name>, <year>2018</year><article-title>Unsupervised learning for fast probabilistic diffeomorphic registration</article-title>. <source>MICCAI</source><volume>11070</volume>, <fpage>729</fpage>–<lpage>738</lpage>.</mixed-citation>
    </ref>
    <ref id="R9">
      <mixed-citation publication-type="journal"><name><surname>Dalca</surname><given-names>AV</given-names></name>, <name><surname>Balakrishnan</surname><given-names>G</given-names></name>, <name><surname>Guttag</surname><given-names>J</given-names></name>, <name><surname>Sabuncu</surname><given-names>M</given-names></name>, <year>2019</year><article-title>Unsupervised learning of probabilistic diffeomorphic registration for images and surfaces</article-title>. <source>Med. Image Anal</source>. <volume>57</volume>, <fpage>226</fpage>–<lpage>236</lpage>.<pub-id pub-id-type="pmid">31351389</pub-id></mixed-citation>
    </ref>
    <ref id="R10">
      <mixed-citation publication-type="journal"><name><surname>de Vos</surname><given-names>BD</given-names></name>, <name><surname>Berendsen</surname><given-names>FF</given-names></name>, <name><surname>Viergever</surname><given-names>MA</given-names></name>, <name><surname>Sokooti</surname><given-names>H</given-names></name>, <name><surname>Staring</surname><given-names>M</given-names></name>, <name><surname>Išgum</surname><given-names>I</given-names></name>, <year>2019</year><article-title>A deep learning framework for unsupervised affine and deformable image registration</article-title>. <source>Med. Image Anal</source>. <volume>52</volume>, <fpage>128</fpage>–<lpage>143</lpage>.<pub-id pub-id-type="pmid">30579222</pub-id></mixed-citation>
    </ref>
    <ref id="R11">
      <mixed-citation publication-type="journal"><name><surname>Desikan</surname><given-names>RS</given-names></name>, <name><surname>Ségonne</surname><given-names>F</given-names></name>, <name><surname>Fischl</surname><given-names>B</given-names></name>, <name><surname>Quinn</surname><given-names>BT</given-names></name>, <name><surname>Dickerson</surname><given-names>BC</given-names></name>, <name><surname>Blacker</surname><given-names>D</given-names></name>, <name><surname>Buckner</surname><given-names>RL</given-names></name>, <name><surname>Dale</surname><given-names>AM</given-names></name>, <name><surname>Maguire</surname><given-names>RP</given-names></name>, <name><surname>Hyman</surname><given-names>BT</given-names></name>, <etal/>, <year>2006</year><article-title>An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest</article-title>. <source>Neuroimage</source><volume>31</volume> (<issue>3</issue>), <fpage>968</fpage>–<lpage>980</lpage>.<pub-id pub-id-type="pmid">16530430</pub-id></mixed-citation>
    </ref>
    <ref id="R12">
      <mixed-citation publication-type="journal"><name><surname>Dice</surname><given-names>LR</given-names></name>, <year>1945</year><article-title>Measures of the amount of ecologic association between species</article-title>. <source>Ecology</source><volume>26</volume> (<issue>3</issue>), <fpage>297</fpage>–<lpage>302</lpage>.</mixed-citation>
    </ref>
    <ref id="R13">
      <mixed-citation publication-type="confproc"><name><surname>Esteves</surname><given-names>C</given-names></name>, <name><surname>Allen-Blanchette</surname><given-names>C</given-names></name>, <name><surname>Zhou</surname><given-names>X</given-names></name>, <name><surname>Daniilidis</surname><given-names>K</given-names></name>, <year>2018</year><source>Polar transformer networks</source>. In: <conf-name>International Conference on Learning Representations. ICLR</conf-name>).</mixed-citation>
    </ref>
    <ref id="R14">
      <mixed-citation publication-type="journal"><name><surname>Fischl</surname><given-names>B</given-names></name>, <name><surname>Sereno</surname><given-names>MI</given-names></name>, <name><surname>Tootell</surname><given-names>RB</given-names></name>, <name><surname>Dale</surname><given-names>AM</given-names></name>, <year>1999</year><article-title>High-resolution intersubject averaging and a coordinate system for the cortical surface</article-title>. <source>Hum. Brain Mapp</source><volume>8</volume> (<issue>4</issue>), <fpage>272</fpage>–<lpage>284</lpage>.<pub-id pub-id-type="pmid">10619420</pub-id></mixed-citation>
    </ref>
    <ref id="R15">
      <mixed-citation publication-type="journal"><name><surname>Frost</surname><given-names>MA</given-names></name>, <name><surname>Goebel</surname><given-names>R</given-names></name>, <year>2012</year><article-title>Measuring structural–functional correspondence: spatial variability of specialised brain regions after macro-anatomical alignment</article-title>. <source>Neuroimage</source><volume>59</volume> (<issue>2</issue>), <fpage>1369</fpage>–<lpage>1381</lpage>.<pub-id pub-id-type="pmid">21875671</pub-id></mixed-citation>
    </ref>
    <ref id="R16">
      <mixed-citation publication-type="journal"><name><surname>Glasser</surname><given-names>MF</given-names></name>, <name><surname>Van Essen</surname><given-names>DC</given-names></name>, <year>2011</year><article-title>Mapping human cortical areas in vivo based on myelin content as revealed by T1- and T2-weighted MRI</article-title>. <source>J. Neurosci</source><volume>31</volume> (<issue>32</issue>), <fpage>11597</fpage>–<lpage>11616</lpage>.<pub-id pub-id-type="pmid">21832190</pub-id></mixed-citation>
    </ref>
    <ref id="R17">
      <mixed-citation publication-type="journal"><name><surname>Glasser</surname><given-names>MF</given-names></name>, <name><surname>Sotiropoulos</surname><given-names>SN</given-names></name>, <name><surname>Wilson</surname><given-names>JA</given-names></name>, <name><surname>Coalson</surname><given-names>TS</given-names></name>, <name><surname>Fischl</surname><given-names>B</given-names></name>, <name><surname>Andersson</surname><given-names>JL</given-names></name>, <name><surname>Xu</surname><given-names>J</given-names></name>, <name><surname>Jbabdi</surname><given-names>S</given-names></name>, <name><surname>Webster</surname><given-names>M</given-names></name>, <name><surname>Polimeni</surname><given-names>JR</given-names></name>, <etal/>, <year>2013</year><article-title>The minimal preprocessing pipelines for the Human Connectome Project</article-title>. <source>Neuroimage</source><volume>80</volume>, <fpage>105</fpage>–<lpage>124</lpage>.<pub-id pub-id-type="pmid">23668970</pub-id></mixed-citation>
    </ref>
    <ref id="R18">
      <mixed-citation publication-type="journal"><name><surname>Gopinath</surname><given-names>K</given-names></name>, <name><surname>Desrosiers</surname><given-names>C</given-names></name>, <name><surname>Lombaert</surname><given-names>H</given-names></name>, <year>2019</year><article-title>Graph convolutions on spectral embeddings for cortical surface parcellation</article-title>. <source>Med. Image Anal</source><volume>54</volume>, <fpage>297</fpage>–<lpage>305</lpage>.<pub-id pub-id-type="pmid">30974398</pub-id></mixed-citation>
    </ref>
    <ref id="R19">
      <mixed-citation publication-type="confproc"><name><surname>Gopinath</surname><given-names>K</given-names></name>, <name><surname>Desrosiers</surname><given-names>C</given-names></name>, <name><surname>Lombaert</surname><given-names>H</given-names></name>, <year>2019</year><source>Adaptive graph convolution pooling for brain surface analysis</source>. In: <conf-name>International Conference on Information Processing in Medical Imaging</conf-name><collab>Springer</collab>, pp. <fpage>86</fpage>–<lpage>98</lpage>.</mixed-citation>
    </ref>
    <ref id="R20">
      <mixed-citation publication-type="journal"><name><surname>Henschel</surname><given-names>L</given-names></name>, <name><surname>Conjeti</surname><given-names>S</given-names></name>, <name><surname>Estrada</surname><given-names>S</given-names></name>, <name><surname>Diers</surname><given-names>K</given-names></name>, <name><surname>Fischl</surname><given-names>B</given-names></name>, <name><surname>Reuter</surname><given-names>M</given-names></name>, <day>1</day><month>10</month><year>2020</year><article-title>Fastsurfer–a fast and accurate deep learning based neuroimaging pipeline</article-title>. <source>Neuroimage</source><volume>219</volume>, <fpage>117012</fpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117012</pub-id><comment>.</comment><pub-id pub-id-type="pmid">32526386</pub-id></mixed-citation>
    </ref>
    <ref id="R21">
      <mixed-citation publication-type="journal"><name><surname>Hinds</surname><given-names>OP</given-names></name>, <name><surname>Rajendran</surname><given-names>N</given-names></name>, <name><surname>Polimeni</surname><given-names>JR</given-names></name>, <name><surname>Augustinack</surname><given-names>JC</given-names></name>, <name><surname>Wiggins</surname><given-names>G</given-names></name>, <name><surname>Wald</surname><given-names>LL</given-names></name>, <name><surname>Rosas</surname><given-names>HD</given-names></name>, <name><surname>Potthast</surname><given-names>A</given-names></name>, <name><surname>Schwartz</surname><given-names>EL</given-names></name>, <name><surname>Fischl</surname><given-names>B</given-names></name>, <year>2008</year><article-title>Accurate prediction of v1 location from cortical folds in a surface coordinate system</article-title>. <source>Neuroimage</source><volume>39</volume> (<issue>4</issue>), <fpage>1585</fpage>–<lpage>1599</lpage>.<pub-id pub-id-type="pmid">18055222</pub-id></mixed-citation>
    </ref>
    <ref id="R22">
      <mixed-citation publication-type="journal"><name><surname>Jaderberg</surname><given-names>M</given-names></name>, <name><surname>Simonyan</surname><given-names>K</given-names></name>, <name><surname>Zisserman</surname><given-names>A</given-names></name>, <etal/>, <year>2015</year><article-title>Spatial transformer networks</article-title>. <source>In: Advances in Neural Information Processing Systems</source>, pp. <fpage>2017</fpage>–<lpage>2025</lpage>.</mixed-citation>
    </ref>
    <ref id="R23">
      <mixed-citation publication-type="confproc"><name><surname>Jason</surname><given-names>JY</given-names></name>, <name><surname>Harley</surname><given-names>AW</given-names></name>, <name><surname>Derpanis</surname><given-names>KG</given-names></name>, <year>2016</year><source>Back to basics: unsupervised learning of optical flow via brightness constancy and motion smoothness</source>. In: <conf-name>Proceedings of the European Conference on Computer Vision (ECCV)</conf-name><collab>Springer</collab>, pp. <fpage>3</fpage>–<lpage>10</lpage>.</mixed-citation>
    </ref>
    <ref id="R24">
      <mixed-citation publication-type="confproc"><name><surname>Jiang</surname><given-names>C</given-names></name>, <name><surname>Huang</surname><given-names>J</given-names></name>, <name><surname>Kashinath</surname><given-names>K</given-names></name>, <name><surname>Marcus</surname><given-names>P</given-names></name>, <name><surname>Niessner</surname><given-names>M</given-names></name>, <etal/>, <year>2019</year><source>Spherical CNNs on unstructured grids</source>. In: <conf-name>International Conference on Learning Representations (ICLR)</conf-name>.</mixed-citation>
    </ref>
    <ref id="R25">
      <mixed-citation publication-type="confproc"><name><surname>Khasanova</surname><given-names>R</given-names></name>, <name><surname>Frossard</surname><given-names>P</given-names></name>, <year>2017</year><source>Graph-based classification of omnidirectional images</source>. In: <conf-name>Proceedings of the IEEE International Conference on Computer Vision (ICCV)</conf-name>, pp. <fpage>869</fpage>–<lpage>878</lpage>.</mixed-citation>
    </ref>
    <ref id="R26">
      <mixed-citation publication-type="book"><name><surname>Krebs</surname><given-names>J</given-names></name>, <name><surname>Mansi</surname><given-names>T</given-names></name>, <name><surname>Delingette</surname><given-names>H</given-names></name>, <name><surname>Zhang</surname><given-names>L</given-names></name>, <name><surname>Ghesu</surname><given-names>FC</given-names></name>, <name><surname>Miao</surname><given-names>S</given-names></name>, <name><surname>Maier</surname><given-names>AK</given-names></name>, <name><surname>Ayache</surname><given-names>N</given-names></name>, <name><surname>Liao</surname><given-names>R</given-names></name>, <name><surname>Kamen</surname><given-names>A</given-names></name>, <year>2017</year><chapter-title>Robust non-rigid registration through agent-based action learning</chapter-title> In: <source>MICCAI</source>. <publisher-name>Springer</publisher-name>, pp. <fpage>344</fpage>–<lpage>352</lpage>.</mixed-citation>
    </ref>
    <ref id="R27">
      <mixed-citation publication-type="journal"><name><surname>Krebs</surname><given-names>J</given-names></name>, <name><surname>Delingette</surname><given-names>H</given-names></name>, <name><surname>Mailhé</surname><given-names>B</given-names></name>, <name><surname>Ayache</surname><given-names>N</given-names></name>, <name><surname>Mansi</surname><given-names>T</given-names></name>, <year>2019</year><article-title>Learning a probabilistic model for diffeomorphic registration</article-title>. <source>IEEE Trans. Med. Imag</source><volume>38</volume> (<issue>9</issue>), <fpage>2165</fpage>–<lpage>2176</lpage>.</mixed-citation>
    </ref>
    <ref id="R28">
      <mixed-citation publication-type="journal"><name><surname>Mueller</surname><given-names>SG</given-names></name>, <name><surname>Weiner</surname><given-names>MW</given-names></name>, <name><surname>Thal</surname><given-names>LJ</given-names></name>, <name><surname>Petersen</surname><given-names>RC</given-names></name>, <name><surname>Jack</surname><given-names>CR</given-names></name>, <name><surname>Jagust</surname><given-names>W</given-names></name>, <name><surname>Trojanowski</surname><given-names>JQ</given-names></name>, <name><surname>Toga</surname><given-names>AW</given-names></name>, <name><surname>Beckett</surname><given-names>L</given-names></name>, <year>2005</year><article-title>Ways toward an early diagnosis in Alzheimer’s disease: the Alzheimer’s disease neuroimaging initiative (ADNI)</article-title>. <source>Alzheimer’s Dementia</source><volume>1</volume> (<issue>1</issue>), <fpage>55</fpage>–<lpage>66</lpage>.</mixed-citation>
    </ref>
    <ref id="R29">
      <mixed-citation publication-type="confproc"><name><surname>Niethammer</surname><given-names>M</given-names></name>, <name><surname>Kwitt</surname><given-names>R</given-names></name>, <name><surname>Vialard</surname><given-names>F-X</given-names></name>, <year>2019</year><source>Metric learning for image registration</source>. In: <conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name>, pp. <fpage>8463</fpage>–<lpage>8472</lpage>.</mixed-citation>
    </ref>
    <ref id="R30">
      <mixed-citation publication-type="journal"><name><surname>Robinson</surname><given-names>EC</given-names></name>, <name><surname>Jbabdi</surname><given-names>S</given-names></name>, <name><surname>Glasser</surname><given-names>MF</given-names></name>, <name><surname>Andersson</surname><given-names>J</given-names></name>, <name><surname>Burgess</surname><given-names>GC</given-names></name>, <name><surname>Harms</surname><given-names>MP</given-names></name>, <name><surname>Smith</surname><given-names>SM</given-names></name>, <name><surname>Van Essen</surname><given-names>DC</given-names></name>, <name><surname>Jenkinson</surname><given-names>M</given-names></name>, <year>2014</year><article-title>MSM: a new flexible framework for multimodal surface matching</article-title>. <source>Neuroimage</source><volume>100</volume>, <fpage>414</fpage>–<lpage>426</lpage>.<pub-id pub-id-type="pmid">24939340</pub-id></mixed-citation>
    </ref>
    <ref id="R31">
      <mixed-citation publication-type="confproc"><name><surname>Ronneberger</surname><given-names>O</given-names></name>, <name><surname>Fischer</surname><given-names>P</given-names></name>, <name><surname>Brox</surname><given-names>T</given-names></name>, <year>2015</year><source>U-net: convolutional networks for biomedical image segmentation</source>. In: <conf-name>International Conference on Medical Image Computing and Computer-Assisted Intervention</conf-name><collab>Springer</collab>, pp. <fpage>234</fpage>–<lpage>241</lpage>.</mixed-citation>
    </ref>
    <ref id="R32">
      <mixed-citation publication-type="journal"><name><surname>Seong</surname><given-names>S-B</given-names></name>, <name><surname>Pae</surname><given-names>C</given-names></name>, <name><surname>Park</surname><given-names>H-J</given-names></name>, <year>2018</year><article-title>Geometric convolutional neural network for analyzing surface-based neuroimaging data</article-title>. <source>Front. Neuroinf</source>. <volume>12</volume>, <fpage>42</fpage>.</mixed-citation>
    </ref>
    <ref id="R33">
      <mixed-citation publication-type="book"><name><surname>Sokooti</surname><given-names>H</given-names></name>, <name><surname>de Vos</surname><given-names>B</given-names></name>, <name><surname>Berendsen</surname><given-names>F</given-names></name>, <name><surname>Lelieveldt</surname><given-names>BP</given-names></name>, <name><surname>Išgum</surname><given-names>I</given-names></name>, <name><surname>Staring</surname><given-names>M</given-names></name>, <year>2017</year><chapter-title>Nonrigid image registration using multi-scale 3D convolutional neural networks</chapter-title> In: <source>MICCAI</source>. <publisher-name>Springer</publisher-name>, pp. <fpage>232</fpage>–<lpage>239</lpage>.</mixed-citation>
    </ref>
    <ref id="R34">
      <mixed-citation publication-type="journal"><name><surname>Sotiras</surname><given-names>A</given-names></name>, <name><surname>Davatzikos</surname><given-names>C</given-names></name>, <name><surname>Paragios</surname><given-names>N</given-names></name>, <year>2013</year><article-title>Deformable medical image registration: a survey</article-title>. <source>IEEE Trans. Med. Imag</source>. <volume>32</volume> (<issue>7</issue>), <fpage>1153</fpage>–<lpage>1190</lpage>.</mixed-citation>
    </ref>
    <ref id="R35">
      <mixed-citation publication-type="journal"><name><surname>Su</surname><given-names>Y-C</given-names></name>, <name><surname>Grauman</surname><given-names>K</given-names></name>, <year>2017</year><article-title>Learning spherical convolution for fast features from 360 imagery</article-title>. <source>In: Advances in Neural Information Processing Systems (NIPS)</source>, pp. <fpage>529</fpage>–<lpage>539</lpage>.</mixed-citation>
    </ref>
    <ref id="R36">
      <mixed-citation publication-type="confproc"><name><surname>Tai</surname><given-names>KS</given-names></name>, <name><surname>Bailis</surname><given-names>P</given-names></name>, <name><surname>Valiant</surname><given-names>G</given-names></name>, <year>2019</year><source>Equivariant transformer networks</source>. In: <conf-name>International Conference on Machine Learning (ICML)</conf-name>.</mixed-citation>
    </ref>
    <ref id="R37">
      <mixed-citation publication-type="journal"><name><surname>Tardif</surname><given-names>CL</given-names></name>, <name><surname>Schäfer</surname><given-names>A</given-names></name>, <name><surname>Waehnert</surname><given-names>M</given-names></name>, <name><surname>Dinse</surname><given-names>J</given-names></name>, <name><surname>Turner</surname><given-names>R</given-names></name>, <name><surname>Bazin</surname><given-names>P-L</given-names></name>, <year>2015</year><article-title>Multi-contrast multi-scale surface registration for improved alignment of cortical areas</article-title>. <source>Neuroimage</source><volume>111</volume>, <fpage>107</fpage>–<lpage>122</lpage>.<pub-id pub-id-type="pmid">25676917</pub-id></mixed-citation>
    </ref>
    <ref id="R38">
      <mixed-citation publication-type="journal"><name><surname>Thirion</surname><given-names>J-P</given-names></name>, <year>1998</year><article-title>Image matching as a diffusion process: an analogy with maxwell’s demons</article-title>. <source>Med. Image Anal</source><volume>2</volume> (<issue>3</issue>), <fpage>243</fpage>–<lpage>260</lpage>.<pub-id pub-id-type="pmid">9873902</pub-id></mixed-citation>
    </ref>
    <ref id="R39">
      <mixed-citation publication-type="journal"><name><surname>Tong</surname><given-names>T</given-names></name>, <name><surname>Aganj</surname><given-names>I</given-names></name>, <name><surname>Ge</surname><given-names>T</given-names></name>, <name><surname>Polimeni</surname><given-names>JR</given-names></name>, <name><surname>Fischl</surname><given-names>B</given-names></name>, <year>2017</year><article-title>Functional density and edge maps: characterizing functional architecture in individuals and improving cross-subject registration</article-title>. <source>Neuroimage</source><volume>158</volume>, <fpage>346</fpage>–<lpage>355</lpage>.<pub-id pub-id-type="pmid">28716714</pub-id></mixed-citation>
    </ref>
    <ref id="R40">
      <mixed-citation publication-type="journal"><name><surname>Van Atteveldt</surname><given-names>N</given-names></name>, <name><surname>Formisano</surname><given-names>E</given-names></name>, <name><surname>Goebel</surname><given-names>R</given-names></name>, <name><surname>Blomert</surname><given-names>L</given-names></name>, <year>2004</year><article-title>Integration of letters and speech sounds in the human brain</article-title>. <source>Neuron</source><volume>43</volume> (<issue>2</issue>), <fpage>271</fpage>–<lpage>282</lpage>.<pub-id pub-id-type="pmid">15260962</pub-id></mixed-citation>
    </ref>
    <ref id="R41">
      <mixed-citation publication-type="journal"><name><surname>Van Essen</surname><given-names>DC</given-names></name>, <name><surname>Smith</surname><given-names>SM</given-names></name>, <name><surname>Barch</surname><given-names>DM</given-names></name>, <name><surname>Behrens</surname><given-names>TE</given-names></name>, <name><surname>Yacoub</surname><given-names>E</given-names></name>, <name><surname>Ugurbil</surname><given-names>K</given-names></name>, <name><surname>Consortium</surname><given-names>W-MH</given-names></name>, <etal/>, <year>2013</year><article-title>The Wu-Minn human connectome project: an overview</article-title>. <source>Neuroimage</source><volume>80</volume>, <fpage>62</fpage>–<lpage>79</lpage>.<pub-id pub-id-type="pmid">23684880</pub-id></mixed-citation>
    </ref>
    <ref id="R42">
      <mixed-citation publication-type="journal"><name><surname>Van Horn</surname><given-names>JD</given-names></name>, <name><surname>Grethe</surname><given-names>JS</given-names></name>, <name><surname>Kostelec</surname><given-names>P</given-names></name>, <name><surname>Woodward</surname><given-names>JB</given-names></name>, <name><surname>Aslam</surname><given-names>JA</given-names></name>, <name><surname>Rus</surname><given-names>D</given-names></name>, <name><surname>Rockmore</surname><given-names>D</given-names></name>, <name><surname>Gazzaniga</surname><given-names>MS</given-names></name>, <year>2001</year><article-title>The functional Magnetic Resonance Imaging data center (fMRIDC): the challenges and rewards of large–scale databasing of neuroimaging studies</article-title>. <source>Philos. Trans. R. Soc. Lond. B Biol. Sci</source><volume>356</volume> (<issue>1412</issue>), <fpage>1323</fpage>–<lpage>1339</lpage>.<pub-id pub-id-type="pmid">11545705</pub-id></mixed-citation>
    </ref>
    <ref id="R43">
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Q</given-names></name>, <name><surname>Kim</surname><given-names>M</given-names></name>, <name><surname>Shi</surname><given-names>Y</given-names></name>, <name><surname>Wu</surname><given-names>G</given-names></name>, <name><surname>Shen</surname><given-names>D</given-names></name>, <name><surname>Initiative</surname><given-names>ADN</given-names></name>, <etal/>, <year>2015</year><article-title>Predict brain MR image registration via sparse learning of appearance and transformation</article-title>. <source>Med. Image Anal</source><volume>20</volume> (<issue>1</issue>), <fpage>61</fpage>–<lpage>75</lpage>.<pub-id pub-id-type="pmid">25476412</pub-id></mixed-citation>
    </ref>
    <ref id="R44">
      <mixed-citation publication-type="book"><name><surname>Yang</surname><given-names>X</given-names></name>, <name><surname>Kwitt</surname><given-names>R</given-names></name>, <name><surname>Niethammer</surname><given-names>M</given-names></name>, <year>2016</year><chapter-title>Fast predictive image registration</chapter-title><source>In: Deep Learning and Data Labeling for Medical Applications</source>. <publisher-name>Springer</publisher-name>, pp. <fpage>48</fpage>–<lpage>57</lpage>.</mixed-citation>
    </ref>
    <ref id="R45">
      <mixed-citation publication-type="journal"><name><surname>Yeo</surname><given-names>BT</given-names></name>, <name><surname>Sabuncu</surname><given-names>MR</given-names></name>, <name><surname>Vercauteren</surname><given-names>T</given-names></name>, <name><surname>Ayache</surname><given-names>N</given-names></name>, <name><surname>Fischl</surname><given-names>B</given-names></name>, <name><surname>Golland</surname><given-names>P</given-names></name>, <year>2010</year><article-title>Spherical demons: fast diffeomorphic landmark-free surface registration</article-title>. <source>IEEE Trans. Med. Imag</source>. <volume>29</volume> (<issue>3</issue>), <fpage>650</fpage>–<lpage>668</lpage>.</mixed-citation>
    </ref>
    <ref id="R46">
      <mixed-citation publication-type="book"><name><surname>Zhang</surname><given-names>M</given-names></name>, <name><surname>Liao</surname><given-names>R</given-names></name>, <name><surname>Dalca</surname><given-names>AV</given-names></name>, <name><surname>Turk</surname><given-names>EA</given-names></name>, <name><surname>Luo</surname><given-names>J</given-names></name>, <name><surname>Grant</surname><given-names>PE</given-names></name>, <name><surname>Golland</surname><given-names>P</given-names></name>, <year>2017</year><chapter-title>Frequency diffeomorphisms for efficient image registration</chapter-title> In: <source>IPMI</source>. <publisher-name>Springer</publisher-name>, pp. <fpage>559</fpage>–<lpage>570</lpage>.</mixed-citation>
    </ref>
    <ref id="R47">
      <mixed-citation publication-type="confproc"><name><surname>Zhao</surname><given-names>F</given-names></name>, <name><surname>Xia</surname><given-names>S</given-names></name>, <name><surname>Wu</surname><given-names>Z</given-names></name>, <name><surname>Duan</surname><given-names>D</given-names></name>, <name><surname>Wang</surname><given-names>L</given-names></name>, <name><surname>Lin</surname><given-names>W</given-names></name>, <name><surname>Gilmore</surname><given-names>JH</given-names></name>, <name><surname>Shen</surname><given-names>D</given-names></name>, <name><surname>Li</surname><given-names>G</given-names></name>, <year>2019</year><source>Spherical u-net on cortical surfaces: methods and applications</source>. In: <conf-name>International Conference on Information Processing in Medical Imaging</conf-name><collab>Springer</collab>, pp. <fpage>855</fpage>–<lpage>866</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig id="F1" orientation="portrait" position="float">
    <label>Fig. 1.</label>
    <caption>
      <p id="P58">Illustration of a spherical mesh and its corresponding 2D rectangular grids by planar projection. <italic>ϕ</italic> and θ denote the longitude and latitude respectively. Regions with higher latitude, e.g. red dots here, have denser samplings compared to regions with lower latitude (e.g. blue dots), yielding greater distortions. The regions crossing <italic>θ</italic> = 0 (green line) become non-adjacent due to the cutting and flattening operations.</p>
    </caption>
    <graphic xlink:href="nihms-1657524-f0001"/>
  </fig>
  <fig id="F2" orientation="portrait" position="float">
    <label>Fig. 2.</label>
    <caption>
      <p id="P59">The proposed cortical surface registration workflow (SphereMorph). The input spherical representation is first rigidly aligned to the atlas using convexity patterns and then projected onto a polar coordinate system. The Spherical U-Net core takes parameterized input image <italic>I</italic><sub><italic>x</italic></sub> and atlas mean image <italic>I</italic><sub><italic>a</italic></sub> as inputs and estimates the distribution, <inline-formula><mml:math display="inline" id="M1"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math display="inline" id="M2"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, of the velocity field <italic>v</italic>, which is then sampled and integrated using scaling and squaring steps to generate the deformation field Φ<sub><italic>v</italic></sub>.</p>
    </caption>
    <graphic xlink:href="nihms-1657524-f0002"/>
  </fig>
  <fig id="F3" orientation="portrait" position="float">
    <label>Fig. 3.</label>
    <caption>
      <p id="P60">Line plot of the converging loss function over training epochs.</p>
    </caption>
    <graphic xlink:href="nihms-1657524-f0003"/>
  </fig>
  <fig id="F4" orientation="portrait" position="float">
    <label>Fig. 4.</label>
    <caption>
      <p id="P61">Cortical parcellation results for two example subjects from the ADRC dataset. For each subject, the upper rows show the cortical parcellation estimated by different registration methods and the lower rows give a closer view of the parcellation comparison in the lateral occipital region (white arrows) with manual parcellation boundary superimposed.</p>
    </caption>
    <graphic xlink:href="nihms-1657524-f0004"/>
  </fig>
  <fig id="F5" orientation="portrait" position="float">
    <label>Fig. 5.</label>
    <caption>
      <p id="P62">Cortical parcellation results for two subjects where our proposed registration method yields the lowest (top 2 rows) and the highest (bottom 2 rows) overall Dice scores. The overall Dice overlap values for each registration method are displayed below each subfigure.</p>
    </caption>
    <graphic xlink:href="nihms-1657524-f0005"/>
  </fig>
  <fig id="F6" orientation="portrait" position="float">
    <label>Fig. 6.</label>
    <caption>
      <p id="P63">Boxplots of Dice overlap coefficients for all the 34 anatomical structures computed with respect to 2D VoxelMorph, MSM, Spherical Demons, FreeSurfer spherical registration and SphereMorph.</p>
    </caption>
    <graphic xlink:href="nihms-1657524-f0006"/>
  </fig>
  <fig id="F7" orientation="portrait" position="float">
    <label>Fig. 7.</label>
    <caption>
      <p id="P64">Sulcal depth alignment generated by rigid, MSM, Spherical Demons and SphereMorph. The maps show group mean and standard deviation of convexity from 30 testing subjects.</p>
    </caption>
    <graphic xlink:href="nihms-1657524-f0007"/>
  </fig>
  <fig id="F8" orientation="portrait" position="float">
    <label>Fig. 8.</label>
    <caption>
      <p id="P65">Jacobian maps of registered surfaces, representing areal distortion measurements. Comparing outcomes for all registration solutions, SphereMorph demonstrates the lowest amount of distortion.</p>
    </caption>
    <graphic xlink:href="nihms-1657524-f0008"/>
  </fig>
  <fig id="F9" orientation="portrait" position="float">
    <label>Fig. 9.</label>
    <caption>
      <p id="P66">Spherical Demons, FreeSurfer and SphereMorph for the 100 unrelated subjects in our second test data set. The top panel shows group-average maps and the bottom panel shows group standard deviations.</p>
    </caption>
    <graphic xlink:href="nihms-1657524-f0009"/>
  </fig>
  <fig id="F10" orientation="portrait" position="float">
    <label>Fig. 10.</label>
    <caption>
      <p id="P67">Task fMRI alignment resulting from convexity-driven spatial registration using MSM, Spherical Demons, FreeSurfer and SphereMorph. The maps show group activation results from 100 subjects for the Gambling task reward contrasts (with the outline of Freesurfer-based cortical parcellations for easier interpretation). SphereMorph improves the functional alignment across subjects over MSM, particularly in the inferiorparietal and precuneus regions, indicated by the arrows.</p>
    </caption>
    <graphic xlink:href="nihms-1657524-f0010"/>
  </fig>
  <fig id="F11" orientation="portrait" position="float">
    <label>Fig. 11.</label>
    <caption>
      <p id="P68">Comparison in the correlation coefficients computed between the group-average and individual activation maps after registration across 86 task contrasts. Higher coefficients suggest better agreement in task activations across subjects.</p>
    </caption>
    <graphic xlink:href="nihms-1657524-f0011"/>
  </fig>
  <fig id="F12" orientation="portrait" position="float">
    <label>Fig. 12.</label>
    <caption>
      <p id="P69">Comparison of agreement in 86 task activations using different features. We take the ‘sulc’ and ‘curv’ atlases from FreeSurfer and compute a ‘T1/T2’ atlas from three rounds of CNN registration within group. Contrasts showing significant improvement (<italic>p</italic> &lt; 0.01) after incorporation of the T1/T2 feature are pointed out by arrows.</p>
    </caption>
    <graphic xlink:href="nihms-1657524-f0012"/>
  </fig>
  <fig id="F13" orientation="portrait" position="float">
    <label>Fig. 13.</label>
    <caption>
      <p id="P70">Task fMRI alignment driven by using different feature maps (sulcal depth and sulcal depth + T1/T2). The maps show group activation results from 100 subjects for the MOTOR task: RF contrasts, with the outline of Freesurfer-based cortical parcellations for easier interpretation. The arrows highlight the paracentral and superiorfrontal areas, where using the T1/T2 map leads to a larger region with positive response compared to using sulcal depth as the only input.</p>
    </caption>
    <graphic xlink:href="nihms-1657524-f0013"/>
  </fig>
  <table-wrap id="T1" position="float" orientation="portrait">
    <label>Table 1.</label>
    <caption>
      <p id="P71">Overview of registration accuracy and CPU run time for rigid alignment, 2D VoxelMorph, MSM, Spherical Demons, FreeSurfer spherical registration, and our proposed SphereMorph. SphereMorph performs in a comparable manner with FreeSurfer, while being roughly 20 times faster.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="top" rowspan="1" colspan="1">Method</th>
          <th align="left" valign="top" rowspan="1" colspan="1">Dice</th>
          <th align="left" valign="top" rowspan="1" colspan="1">overall MMD (<italic>mm</italic>)</th>
          <th align="left" valign="top" rowspan="1" colspan="1">Time (<italic>min</italic>)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Rigid</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.840 ±0.029</td>
          <td align="left" valign="top" rowspan="1" colspan="1">3.13±0.43</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.59±0.08</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">2D VoxelMorph</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.819± 0.027</td>
          <td align="left" valign="top" rowspan="1" colspan="1">3.47 ± 0.48</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.73 ± 0.08</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">MSM</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.872 ±0.014</td>
          <td align="left" valign="top" rowspan="1" colspan="1">2.63±0.27</td>
          <td align="left" valign="top" rowspan="1" colspan="1">9.56±1.02</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Spherical Demons</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.881±0.009</td>
          <td align="left" valign="top" rowspan="1" colspan="1">2.43±0.21</td>
          <td align="left" valign="top" rowspan="1" colspan="1">6.15±0.59</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">FreeSurfer</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.889±0.014</td>
          <td align="left" valign="top" rowspan="1" colspan="1">2.24±0.21</td>
          <td align="left" valign="top" rowspan="1" colspan="1">13.46±3.13</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">SphereMorph</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.882±0.014</td>
          <td align="left" valign="top" rowspan="1" colspan="1">2.42±0.24</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.74±0.09</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="T2" position="float" orientation="portrait">
    <label>Table 2.</label>
    <caption>
      <p id="P72">Comparison of group functional alignment for MSM, Spherical Demons, FreeSurfer spherical registration, and our proposed method SphereMorph. Correlation coefficients are calculated for the average correlation coefficients of all 86 contrasts. MSM-T1/T2 and SphereMorph-T1/T2 denote the incorporation of the T1/T2 map along with the sulcal depth feature. All other methods only rely on the sulcal depth.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="top" rowspan="1" colspan="1">Method</th>
          <th align="left" valign="top" rowspan="1" colspan="1">correlation coefficients</th>
          <th align="left" valign="top" rowspan="1" colspan="1">difference with MSM</th>
          <th align="left" valign="top" rowspan="1" colspan="1">relative difference (%)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">MSM</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0:5037±0:1256</td>
          <td align="left" valign="top" rowspan="1" colspan="1">–</td>
          <td align="left" valign="top" rowspan="1" colspan="1">–</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Spherical Demons</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0:5298± 0:1258</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0:0261 ± 0:0082</td>
          <td align="left" valign="top" rowspan="1" colspan="1">5:60± 2:35</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">FreeSurfer</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0:5372 ± 0:1259</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0:0335 ± 0:0093</td>
          <td align="left" valign="top" rowspan="1" colspan="1">7:12 ± 2:62</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">SphereMorph</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0:5460± 0:1254</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0:0424 ± 0:0090</td>
          <td align="left" valign="top" rowspan="1" colspan="1">8:99 ± 2:85</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">MSM-T1/T2</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0:5456 ± 0:1310</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0:0419 ± 0:0081</td>
          <td align="left" valign="top" rowspan="1" colspan="1">8:65 ± 1:89</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">SphereMorph-T1/T2</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0:5535 ± 0:1243</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0:0498 ± 0:0164</td>
          <td align="left" valign="top" rowspan="1" colspan="1">10:64 ± 4:46</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
