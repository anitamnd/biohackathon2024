<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 2?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">4384298</article-id>
    <article-id pub-id-type="pmid">25627334</article-id>
    <article-id pub-id-type="publisher-id">454</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-015-0454-y</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>TagDust2: a generic method to extract reads from sequencing data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Lassmann</surname>
          <given-names>Timo</given-names>
        </name>
        <address>
          <email>timolassmann@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000000094465255</institution-id><institution-id institution-id-type="GRID">grid.7597.c</institution-id><institution>RIKEN Center for Life Science Technologies (CLST), </institution><institution>RIKEN Yokohama Institute, </institution></institution-wrap>1-7-22 Suehiro-cho, Tsurumi-ku, Yokohama, 230-0045 Kanagawa Japan </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1936 7910</institution-id><institution-id institution-id-type="GRID">grid.1012.2</institution-id><institution>Telethon Kids Institute, </institution><institution>The University of Western Australia, </institution></institution-wrap>100 Roberts Road, Subiaco, Subiaco, 6008 Western Australia Australia </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>28</day>
      <month>1</month>
      <year>2015</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2015</year>
    </pub-date>
    <volume>16</volume>
    <elocation-id>24</elocation-id>
    <history>
      <date date-type="received">
        <day>11</day>
        <month>6</month>
        <year>2014</year>
      </date>
      <date date-type="accepted">
        <day>9</day>
        <month>1</month>
        <year>2015</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© Lassmann; licensee BioMed Central. 2015</copyright-statement>
      <license license-type="open-access">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0">http://creativecommons.org/licenses/by/4.0</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p>Arguably the most basic step in the analysis of next generation sequencing data (NGS) involves the extraction of mappable reads from the raw reads produced by sequencing instruments. The presence of barcodes, adaptors and artifacts subject to sequencing errors makes this step non-trivial.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>Here I present TagDust2, a generic approach utilizing a library of hidden Markov models (HMM) to accurately extract reads from a wide array of possible read architectures. TagDust2 extracts more reads of higher quality compared to other approaches. Processing of multiplexed single, paired end and libraries containing unique molecular identifiers is fully supported. Two additional post processing steps are included to exclude known contaminants and filter out low complexity sequences. Finally, TagDust2 can automatically detect the library type of sequenced data from a predefined selection.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p>Taken together TagDust2 is a feature rich, flexible and adaptive solution to go from raw to mappable NGS reads in a single step. The ability to recognize and record the contents of raw reads will help to automate and demystify the initial, and often poorly documented, steps in NGS data analysis pipelines. TagDust2 is freely available at: <ext-link ext-link-type="uri" xlink:href="http://tagdust.sourceforge.net">http://tagdust.sourceforge.net</ext-link>.</p>
      </sec>
      <sec>
        <title>Electronic supplementary material</title>
        <p>The online version of this article (doi:10.1186/s12859-015-0454-y) contains supplementary material, which is available to authorized users.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Next generation sequencing</kwd>
      <kwd>TagDust</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2015</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p>Next generation sequencing has greatly accelerated the accumulation of genomics data. Different protocols targeting the genome, epigenome and transcriptions are widely used [<xref ref-type="bibr" rid="CR1">1</xref>]. In essence, all protocols capture biological sequences of interest while adding adaptors and other sequences to facilitate cost effective sequencing. An obvious examples is the use of indices or barcodes allowing researchers to multiplex sequencing experiments [<xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR3">3</xref>]. In addition recent protocols include random oligos in the library construction to correct for PCR and other biases [<xref ref-type="bibr" rid="CR4">4</xref>]. As the length of such auxiliary sequences increases so is the chance that sequencing errors occur in these key sequences. In the best case these errors lead to some sequences being lost to the downstream analysis, but in the worse case sequences can be mixed up between samples leading to analytical noise. A compounding complication is that error rates of current sequencing instruments vary and is not obvious how to select an appropriate strategy to process the raw data.</p>
    <p>Programs have been developed to tackle individual steps in this general area including the removal of artefacts from raw NGS sequencing files [<xref ref-type="bibr" rid="CR5">5</xref>], read trimming [<xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR7">7</xref>] and read demultiplexing [<xref ref-type="bibr" rid="CR8">8</xref>-<xref ref-type="bibr" rid="CR10">10</xref>]. All programs are based on matching known strings to the sequenced reads. However none tackle the whole problem comprehensively necessitating the construction of processing pipelines [<xref ref-type="bibr" rid="CR11">11</xref>-<xref ref-type="bibr" rid="CR13">13</xref>] to obtain clean sequences for assembly or mapping to a reference sequence. Parameterization and maintenance of these pipelines for individual NGS assays is cumbersome and only exacerbated by the rapid pace of protocol development [<xref ref-type="bibr" rid="CR14">14</xref>].</p>
    <p>TagDust2 solves this problem in a generic way while guaranteeing good accuracy. The key part of finding the mappable read and matching it to a barcode is carried out by a user defined hidden Markov model (HMM). This approach has several key advantages. Firstly, a user defined architecture gives great flexibility in respect to the different sequencing assays used. Secondly, HMMs themselves are well suited to recognize sequences even in the presence of sequencing errors. Artifacts such as primer dimers will by definition not match the HMM and are therefore easily recognized and discarded (see Figure <xref rid="Fig1" ref-type="fig">1</xref>). Finally, by comparing reads to multiple HMMs it is trivial to detect the library protocol used. The latter can be used in production environments to automatically recognize and process different NGS protocols.
<fig id="Fig1"><label>Figure 1</label><caption><p><bold>Overview of the TagDust2 workflow.</bold><bold>1)</bold> A user specifies the expected read architecture as a sequence of pre-defined blocks. Here there are four of such blocks. <bold>2)</bold> A HMM is constructed by concatenating the pre-defined blocks in the order given by the -1 …command line options. For example -2 B:GTA,AAC is translated into the second (red) part of the HMM and models the presence of two mutually exclusive barcode sequences. <bold>3)</bold> Reads are scanned with the HMM and each nucleotide is labelled by the block it belongs to. In the example shown the three letter barcode GTA is recognised in the raw sequence. <bold>4)</bold> Based on the labelling of the sequence, a barcode is assigned to each read and remaining sequences are trimmed.</p></caption><graphic xlink:href="12859_2015_454_Fig1_HTML" id="MO1"/></fig>
</p>
  </sec>
  <sec id="Sec2">
    <title>Approach</title>
    <p>Let us define the read architecture as the order of adapters, barcodes and the mappable sequence itself. TagDust2 contains a library of HMM models, each designed to capture the different types of sequences one might encounter in real raw NGS reads. For example, one model recognizes partial sequences such as 5’ and 3’ adapters while another captures the presence of mutually exclusive barcode sequences used for multiplexing libraries.</p>
    <p>Each sequenced read is scored against the global HMM using the forward algorithm [<xref ref-type="bibr" rid="CR15">15</xref>] and a zero order background model. If barcodes are present in the architecture an additional background HMM of the same length as the barcode is introduced. The backward algorithm is then used to determine the total probability of the most likely barcode sequence used. All probabilities are converted into a phred scaled extraction quality analogous to the mapping quality introduced in MAQ [<xref ref-type="bibr" rid="CR16">16</xref>]. Simply put, the extraction quality reflects how certain we can be that the read matches the read architecture and a particular given barcode. To finally extract the read sequence TagDust2 employs an optimal accuracy decoder [<xref ref-type="bibr" rid="CR17">17</xref>].</p>
    <p>In transcriptome sequencing it is common to exclude ribosomal RNAs from the downstream analysis. In addition users may wish to extract known sequences such as spike-ins to perform separate analysis. For completeness TagDust2 includes the option to scan all extracted reads against a database of known contaminants and exclude them from mapping. Similarly, a low-complexity filter is included. Taken together TagDust2 performs all steps required to go from raw to mappable sequences and therefore greatly simplifies processing pipelines.</p>
  </sec>
  <sec id="Sec3">
    <title>Implementation</title>
    <p>TagDust2 is a complete departure from the original TagDust [<xref ref-type="bibr" rid="CR5">5</xref>] program. TagDust2 implements a small library of HMMs referred to as segments. Each segment contains a silent start and end state which are used to connect multiple segments. Segments are hand-designed to capture commonly occurring features in raw sequences such as the combination of barcodes, variable length sequences and so on (see the user manual for a complete list). Users can use a simple command line interface to specify the expected sequence of segments in their reads. TagDust2 automatically constructs a global HMMs from the segments and starts scoring the individual reads (see Figure <xref rid="Fig1" ref-type="fig">1</xref>). Alternatively users can create a file containing a selection of pre-defined architectures. TagDust will score reads against all architectures and determine the most appropriate match.</p>
    <p>Internally, TagDust2 uses a full profile HMMs for each segment. To emulate different segments I simply set some transition probabilities to zero. For example the HMM segment corresponding to the actual read is implemented as a profile HMM with one column and transitions directly to and from the insertion state. The most computationally demanding parts are parallelized using threads. TagDust2 is written in C, extensively documented and the source code is freely available.</p>
    <sec id="Sec4">
      <title>Sequence scoring</title>
      <p>In short read mapping the mapping quality <italic>Q</italic> reflects the confidence we have in one particular mapping location over all others [<xref ref-type="bibr" rid="CR16">16</xref>]. Analogously, TagDust2 compares the probability of each read matching to the user specified HMM to the total summed probability including a random model:
<disp-formula id="Equa"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$P = 1 - \frac{P(x|M)^{\ast} V}{P(x|M) + P(x|R)} $$
\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>|</mml:mo><mml:mi>M</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>|</mml:mo><mml:mi>M</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>|</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12859_2015_454_Equa.gif" position="anchor"/></alternatives></disp-formula> where <italic>P</italic>(<italic>x</italic>|<italic>M</italic>) is the total summed probability of a read matching the model derived by the forward algorithm and <italic>P</italic>(<italic>x</italic>|<italic>R</italic>) is the probability of the read give a random zero order Markov model. <italic>V</italic> represents the fraction of <italic>P</italic>(<italic>x</italic>|<italic>M</italic>) corresponding to the most likely barcode sequence, if present or 1 otherwise. It is defined as the most probable transition from a silent state <italic>s</italic> to the first match state <italic>m</italic> of a barcode <italic>j</italic>:
<disp-formula id="Equb"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$V = \max_{j} \left(\frac{f_{s}(i) a_{s,m_{j}} e_{m_{j}}(x+1) b_{m_{j}}(i+1)}{\sum\limits_{\pi} P(x,\pi | M)}\right) $$
\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>max</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:munder><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>π</mml:mi><mml:mo>|</mml:mo><mml:mi>M</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2015_454_Equb.gif" position="anchor"/></alternatives></disp-formula> where <italic>f</italic>
<sub><italic>s</italic></sub> is the total probability summed up over all paths <italic>π</italic> leading up to state <italic>s</italic> and <inline-formula id="IEq1"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$b_{m_{j}}$
\end{document}</tex-math><mml:math id="M6"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2015_454_IEq1.gif"/></alternatives></inline-formula> the total probability of all paths starting from <italic>m</italic>
<sub><italic>j</italic></sub>. <italic>a</italic> and <italic>e</italic> are the transition and emission probabilities respectively. The denominator is the total probability of the sequence <italic>x</italic> given the model <italic>M</italic> summed over all paths <italic>π</italic>.</p>
      <p>To further reduce possibility of one barcode being mixed up with one another TagDust2 always adds one additional HMM to the segment. The latter is of the same length as other barcodes but the emission probabilities are set to the background frequencies. This model captures raw sequences in which the barcode sequence is too ambiguous to be uniquely assigned.</p>
    </sec>
    <sec id="Sec5">
      <title>Threshold determination</title>
      <p>Selecting an appropriate threshold on <italic>P</italic> is not trivial. A stringent threshold is appropriate when many auxiliary sequences are present. However, the same threshold may yield no results when simple read architectures are given because the distributions of <italic>P</italic>(<italic>x</italic>|<italic>M</italic>) and <italic>P</italic>(<italic>x</italic>|<italic>R</italic>) will overlap. In other words thresholds should be set independently for different read architectures.</p>
      <p>To set thresholds dynamically and automatically, TagDust2 simulates, or emits, reads from the model <italic>M</italic> and the random model <italic>R</italic>. All reads are scored and the threshold is set to the value that gives the best sensitivity plus specificity.</p>
    </sec>
    <sec id="Sec6">
      <title>Optimal accuracy decoding</title>
      <p>To obtain the most probable labeling of a raw sequence, TagDust2 employs the optimal accuracy decoding algorithm as described in [<xref ref-type="bibr" rid="CR17">17</xref>]. To apply this algorithm to our problem the label probability of a nucleotide is defined by the summed posterior label probabilities of all states belonging to a particular HMM segment. A secondary dynamic programming algorithm is used to determine the path with the maximum posterior label probability, constrained by the global HMM architecture. The label probabilities are essentially used as a substitution matrix while the architecture is enforced by the equivalent of gap penalties.</p>
      <p>If fingerprints, a random sequence added to detect PCR artifacts, are present TagDust2 checks at this stage if the length after decoding matches the users input. If not the read is discarded.</p>
    </sec>
    <sec id="Sec7">
      <title>Further read filtering</title>
      <p>TagDust2 allows users to specify a fasta file containing known sequences the user wishes to exclude from mapping. For example in transcriptome sequencing one commonly wants to remove ribosomal sequences from the downstream analysis. To address this issue in a general manner TagDust2 exhaustively align all reads to the target reference sequences and discard all reads with less than a user defined number of mismatches, insertions and deletions. For efficiency I implemented the Myers bit parallel algorithm [<xref ref-type="bibr" rid="CR18">18</xref>] using SIMD instructions as well as using thread level parallelism. The number of reads matching different reference sequences are automatically recorded.</p>
    </sec>
    <sec id="Sec8">
      <title>Filtering low complexity sequences</title>
      <p>Tagdust2 implements a simplified version of the DUST module (R. Tatusov and D.J. Lipman, unpublished data) to filter out low complexity reads. The algorithm is only applied to the first sixty-four nucleotides of the reads.</p>
    </sec>
  </sec>
  <sec id="Sec9" sec-type="results">
    <title>Results</title>
    <p>To assess the performance of TagDust2 I generated large datasets by varying the number of barcodes used, their lengths and the per base error rate. In all experiments 90 thousand sequences containing the expected architecture were generated. An additional 10 thousand random sequences were added to assess the number of false positives. To obtain sets of barcodes with maximal dissimilarity I used programs described by Faircloth et al. [<xref ref-type="bibr" rid="CR19">19</xref>]. Given that the original read and sample are known the number of reads assigned to the wrong sample and the total number of extracted reads can be quantified.</p>
    <p>I compared the performance of TagDust2 to cutadapt [<xref ref-type="bibr" rid="CR7">7</xref>], the fastx-toolkit [<xref ref-type="bibr" rid="CR10">10</xref>] and Btrim [<xref ref-type="bibr" rid="CR8">8</xref>] using default parameters.</p>
    <p>The TagDust2 software distribution contains a document written using the literate programming paradigm to reproduce all the results presented here (see also Additional file <xref rid="MOESM1" ref-type="media">1</xref>).</p>
    <sec id="Sec10">
      <title>Basic de-multlexing</title>
      <p>A simple application of TagDust2 is de-multiplexing of libraries. In two separate experiments, four and six nucleotide long barcodes were appended to reads. The number of barcodes and their length was varied together with the sequencer error rate. The maximum similarity between any pair of barcodes was two when using 4nt and three when using 6nt barcodes (Figure <xref rid="Fig2" ref-type="fig">2</xref>).
<fig id="Fig2"><label>Figure 2</label><caption><p><bold>Demultiplexing of libraries with 4nt barcodes assuming different error rates.</bold> From left to right: simulations using 8, 24 or 48 different barcodes. The top panels show the recall and the bottom panels the precision. TagDust sacrifices recall for high precision.</p></caption><graphic xlink:href="12859_2015_454_Fig2_HTML" id="MO2"/></fig>
</p>
      <p>TagDust2 is more conservative at extracting reads compared to fastx when using 4nt barcodes. As the number of barcodes and error rates are increased the precision of both programs is decreasing. However, the precision of TagDust2 is consistently higher compared to fastx and far less affected by the per-base error rate.</p>
      <p>Increasing the barcode length to six nucleotides makes it much easier to unambiguously assign reads to a particular sample (Figure <xref rid="Fig3" ref-type="fig">3</xref>). Here there is no appreciable difference in recall between the two programs. As before, TagDust2 is consistently more precise compared to fastx.
<fig id="Fig3"><label>Figure 3</label><caption><p><bold>Demultiplexing of libraries with 6nt barcodes assuming different sequencer error rates.</bold> From left to right: simulations using 8, 24 or 48 different barcodes. The top panels show the recall and the bottom panels the precision.</p></caption><graphic xlink:href="12859_2015_454_Fig3_HTML" id="MO3"/></fig>
</p>
    </sec>
    <sec id="Sec11">
      <title>De-multiplexing in the presence of 5’ and 3’ adapters</title>
      <p>In a more complicated case, we add both 5’ and 3’ adapters (AGGGAGGACGATGCGG and GTGTCAGTCACTTCCAGCGG) to the simulated case from before (Figures <xref rid="Fig4" ref-type="fig">4</xref> and <xref rid="Fig5" ref-type="fig">5</xref>). TagDust2 performs favorably in these cases. The additional long sequences make it easy to differentiate between real and random sequences and hence the recall is high.
<fig id="Fig4"><label>Figure 4</label><caption><p><bold>Demultiplexing of libraries with 5‘ and 3‘ linkers and 4nt barcodes assuming different sequencer error rates.</bold> From left to right: simulations using 8, 24 or 48 different barcodes. The top panels show the recall and the bottom panels the precision.</p></caption><graphic xlink:href="12859_2015_454_Fig4_HTML" id="MO4"/></fig>
<fig id="Fig5"><label>Figure 5</label><caption><p><bold>Demultiplexing of libraries with 5‘ and 3‘ linkers and 6nt barcodes assuming different sequencer error rates.</bold> From left to right: simulations using 8, 24 or 48 different barcodes. The top panels show the recall and the bottom panels the precision.</p></caption><graphic xlink:href="12859_2015_454_Fig5_HTML" id="MO5"/></fig>
</p>
      <p>Perhaps most importantly, the syntax for running TagDust2 is virtually unchanged from before. Working with more complex architectures is simple, negating the need to write many protocol specific scripts for data processing.</p>
    </sec>
    <sec id="Sec12">
      <title>Automatic detection of architectures</title>
      <p>In parallel to running TagDust2 with a case specific command line for each of the situations above, I also ran TagDust using a pre-specified architecture file. In brief, this file contains a list of all possible architectures used in all the examples above. In all cases, TagDust2 run in this mode selected the correct architecture out of all the other options and produced identical results to those shown in Figures <xref rid="Fig2" ref-type="fig">2</xref>, <xref rid="Fig3" ref-type="fig">3</xref>, <xref rid="Fig4" ref-type="fig">4</xref> and <xref rid="Fig5" ref-type="fig">5</xref>. In other words, TagDust2 using the same options could recognize the presence or absence of adapters, distinguish between different numbers of 4nt or 6nt barcodes while being able to give good results irrespective of the sequencing error rate.</p>
    </sec>
    <sec id="Sec13">
      <title>Comparison to the Casava pipeline</title>
      <p>On Illumina sequencing instruments de-multiplexing of samples is performed automatically by the program bcl2fastq as part of the CASAVA pipeline. This program simultaneously converts the per-cycle BCL basecall files to per-read fastq files. BCL files are not readily available but fortuitously, the bcl2fastq software [<xref ref-type="bibr" rid="CR9">9</xref>] is distributed together with a validation dataset containing the raw files of one lane of multiplexed paired-end samples (RUN 110120_P20_0993_A805CKABXX). The lane contains two human samples and a PhiX control sample indexed by 6nt long sequences (ACAGTG, ACTTGA, TTAGGC). By turning off the de-multiplexing functionality in bcl2fastq I could perform the de-multiplexing separately with TagDust2. All reads were aligned to their matching reference genomes (GRCh38, NC_001422.1) using BWA-MEM [<xref ref-type="bibr" rid="CR20">20</xref>].</p>
      <p>TagDust2 was able to assign more reads to all of the three samples compared the files produced by bcl2fastq (Table <xref rid="Tab1" ref-type="table">1</xref>). Mapping the reads to their references demonstrates that the additional reads extracted by TagDust2 are usable for downstream analysis.
<table-wrap id="Tab1"><label>Table 1</label><caption><p><bold>Comparison to Illumina’s bcl2fastq</bold></p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left"><bold>Sample</bold></th><th align="left"><bold>bcl2fastq</bold></th><th align="left"><bold>TagDust2</bold></th><th align="left"><bold>Difference (TagDust2 -</bold></th></tr><tr><th align="left"/><th align="left"/><th align="left"/><th align="left"/><th align="left"><bold>bcl2fastq)</bold></th></tr></thead><tbody><tr><td align="left">Number of</td><td align="left">AR005</td><td align="left">1.472.734</td><td align="left">1.508.848</td><td align="left">36.114</td></tr><tr><td align="left">extracted reads</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left"/><td align="left">AR008</td><td align="left">1.518.208</td><td align="left">1.543.918</td><td align="left">25.710</td></tr><tr><td align="left"/><td align="left">PhiX</td><td align="left">49.104</td><td align="left">51.488</td><td align="left">2.384</td></tr><tr><td align="left">Number of</td><td align="left">AR005</td><td align="left">1.435.567</td><td align="left">1.464.037</td><td align="left">28.470</td></tr><tr><td align="left">aligned reads</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left"/><td align="left">AR008</td><td align="left">1.479.158</td><td align="left">1.498.524</td><td align="left">19.366</td></tr><tr><td align="left"/><td align="left">PhiX</td><td align="left">46.369</td><td align="left">48.296</td><td align="left">1.927</td></tr></tbody></table></table-wrap>
</p>
    </sec>
    <sec id="Sec14">
      <title>Real datasets</title>
      <p>To highlight the performance of TagDust2 on real datasets with complicated read architectures I obtained the raw reads from a single cell transcriptome dataset [<xref ref-type="bibr" rid="CR21">21</xref>] using 96 barcodes directly from the authors and a dataset containing unique molecular identifiers [<xref ref-type="bibr" rid="CR4">4</xref>] (European nucleotide archive accession numbers: ERR048988 - ERR048994).</p>
      <p>The raw reads from the single cell transcriptome dataset contain a 6 nucleotide barcode sequence followed by three guanines and then the actual read sequence. TagDust2 extracted 18.6 million additional reads compared to the original data processing pipeline used (Table <xref rid="Tab2" ref-type="table">2</xref>). After mapping to the mouse genome using Tophat2 [<xref ref-type="bibr" rid="CR22">22</xref>], 3.5 million additional reads could be aligned.
<table-wrap id="Tab2"><label>Table 2</label><caption><p><bold>Summary of single cell extracted reads</bold></p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><bold>Description</bold></th><th align="left"><bold>Original</bold></th><th align="left"><bold>Using TagDust2</bold></th></tr></thead><tbody><tr><td align="left">Total reads</td><td align="left">110.622.138 (100.00%)</td><td align="left">110.622.138 (100.00%)</td></tr><tr><td align="left">Extracted</td><td align="left">82.325.179 (74.42%)</td><td align="left">100.900.895 (91.21%)</td></tr><tr><td align="left">Mapped</td><td align="left">66.204.974 (59.85%)</td><td align="left">69.685.999 (62.99%)</td></tr></tbody></table></table-wrap>
</p>
      <p>In the second case each read contains a random 10 nucleotide unique molecular identifier (UMI). Finding the same UMI associated with reads mapping to same location is a good indicator that these reads are PCR duplicates. TagDust2 automatically recognizes UMI sequences and converts them into a unique number. To understand whether the UMIs actually help in reducing technical noise caused by PCR amplification I compared libraries amplified using either 15 or 25 PCR cycles. After collapsing reads mapping to the same region with the same UMI the sample to sample correlation could be improved (Figure <xref rid="Fig6" ref-type="fig">6</xref>). More importantly, TagDust2 was able to extract the reads using the short command line:
<fig id="Fig6"><label>Figure 6</label><caption><p><bold>Sample to sample correlation can be improved by using unique molecular identifiers.</bold> The right panel shows correlation between samples using 15 and 25 PCR cycles without using the UMI sequences. The left panel shows the same data after collapsing all reads mapping to the location and containing the same UMI. TagDust correctly identified PCR artifacts based on their UMI sequences.</p></caption><graphic xlink:href="12859_2015_454_Fig6_HTML" id="MO6"/></fig>
</p>
      <p>
        <graphic xlink:href="12859_2015_454_Figa_HTML.gif" id="MO7"/>
      </p>
      <p>
tagdust2 -1 F:NNN -2 S:T -3 F:NNNN -4 S:T -5 F:NNN -6 B:GACTT -7 S:GGGG -8 R:N
</p>
      <p>specifying that we expect a 10 nucleotide fingerprint sequence (F:) separated by thymidines (S:) followed by a 5 nucleotide barcode (B:), 4 guanosines and finally the mappable read (R:). Apart form illustrating the inherent flexibility of TagDust2 when using complicated architectures, the command line itself also documents the contents of the raw sequences. Such information completely demystifies the early steps in NGS processing pipelines.</p>
    </sec>
  </sec>
  <sec id="Sec15" sec-type="conclusion">
    <title>Conclusions</title>
    <p>The experiments on simulated and real data indicate that TagDust2 improves the initial steps in read processing in several ways. Firstly, using HMMs allows TagDust2 to effectively work datasets with a broad range of sequencing error rates. Secondly, the implementation of a library of possible read segments makes TagDust2 very accessible and useable. Finally, the automatic selection of architectures and thresholds greatly simplifies and generalizes the initial parts of data processing pipelines. In production environments TagDust2 allows users to define several read architectures and use the same pipeline for the pre-processing of diverse data types.</p>
    <p>The approach presented here allows researchers to extract reads accurately and with a performance guarantee. Together with the two additional post processing functions, TagDust2 is a one stop solution to go from raw to mappable reads. The inherent flexible design makes TagDust2 applicable to a wide spectrum of current and future datasets.</p>
  </sec>
  <sec id="Sec16">
    <title>Availability and requirements</title>
    <p><bold>Project name:</bold> TagDust<bold>Project home page:</bold>
<ext-link ext-link-type="uri" xlink:href="http://tagdust.sourceforge.net">http://tagdust.sourceforge.net</ext-link>
<bold>Operating systems:</bold> Unix/Linux<bold>Programming language:</bold> C<bold>Other requirements:</bold> NA<bold>License:</bold> GNU General Public License version 3.0 (GPLv3)</p>
  </sec>
</body>
<back>
  <app-group>
    <app id="App1">
      <sec id="Sec17">
        <title>Additional file</title>
        <p>
          <media position="anchor" xlink:href="12859_2015_454_MOESM1_ESM.pdf" id="MOESM1">
            <label>Additional file 1</label>
            <caption>
              <p>
                <bold>Code and description to reproduce the figures in the manuscript.</bold>
              </p>
            </caption>
          </media>
        </p>
      </sec>
    </app>
  </app-group>
  <fn-group>
    <fn>
      <p>
        <bold>Competing interests</bold>
      </p>
      <p>The authors declare that he has no competing interests.</p>
    </fn>
    <fn>
      <p>
        <bold>Authors’ contributions</bold>
      </p>
      <p>TL wrote the software and paper.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>This work was supported by a Research Grant from the Japanese Ministry of Education, Culture, Sports, Science and Technology (MEXT) to the RIKEN Center for Life Science Technologies.</p>
  </ack>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bernstein</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Birney</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Dunham</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Green</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Gunter</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Snyder</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>An integrated encyclopedia of dna elements in the human genome</article-title>
        <source>Nature</source>
        <year>2012</year>
        <volume>489</volume>
        <issue>7414</issue>
        <fpage>57</fpage>
        <lpage>74</lpage>
        <pub-id pub-id-type="doi">10.1038/nature11247</pub-id>
        <pub-id pub-id-type="pmid">22955616</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Craig</surname>
            <given-names>DW</given-names>
          </name>
          <name>
            <surname>Pearson</surname>
            <given-names>JV</given-names>
          </name>
          <name>
            <surname>Szelinger</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sekar</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Redman</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Corneveaux</surname>
            <given-names>JJ</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Identification of genetic variants using bar-coded multiplexed sequencing</article-title>
        <source>Nat Methods</source>
        <year>2008</year>
        <volume>5</volume>
        <issue>10</issue>
        <fpage>887</fpage>
        <lpage>93</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.1251</pub-id>
        <pub-id pub-id-type="pmid">18794863</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kircher</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Sawyer</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Meyer</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Double indexing overcomes inaccuracies in multiplex sequencing on the illumina platform</article-title>
        <source>Nucleic Acids Res</source>
        <year>2012</year>
        <volume>40</volume>
        <issue>1</issue>
        <fpage>e3</fpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkr771</pub-id>
        <pub-id pub-id-type="pmid">22021376</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kivioja</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Vähärautio</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Karlsson</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Bonke</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Enge</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Linnarsson</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Counting absolute numbers of molecules using unique molecular identifiers</article-title>
        <source>Nat Methods</source>
        <year>2012</year>
        <volume>9</volume>
        <issue>1</issue>
        <fpage>72</fpage>
        <lpage>4</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.1778</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lassmann</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Hayashizaki</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Daub</surname>
            <given-names>CO</given-names>
          </name>
        </person-group>
        <article-title>Tagdust—a program to eliminate artifacts from next generation sequencing data</article-title>
        <source>Bioinformatics</source>
        <year>2009</year>
        <volume>25</volume>
        <issue>21</issue>
        <fpage>2839</fpage>
        <lpage>40</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btp527</pub-id>
        <pub-id pub-id-type="pmid">19737799</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bolger</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Lohse</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Usadel</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Trimmomatic: a flexible trimmer for illumina sequence data</article-title>
        <source>Bioinformatics</source>
        <year>2014</year>
        <volume>30</volume>
        <issue>15</issue>
        <fpage>2114</fpage>
        <lpage>2120</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btu170</pub-id>
        <pub-id pub-id-type="pmid">24695404</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Martin</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Cutadapt removes adapter sequences from high-throughput sequencing reads</article-title>
        <source>EMBnet J</source>
        <year>2011</year>
        <volume>17</volume>
        <issue>1</issue>
        <fpage>10</fpage>
        <pub-id pub-id-type="doi">10.14806/ej.17.1.200</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kong</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Btrim: a fast, lightweight adapter and quality trimming program for next-generation sequencing technologies</article-title>
        <source>Genomics</source>
        <year>2011</year>
        <volume>98</volume>
        <issue>2</issue>
        <fpage>152</fpage>
        <lpage>3</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ygeno.2011.05.009</pub-id>
        <pub-id pub-id-type="pmid">21651976</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9</label>
      <mixed-citation publication-type="other">BCL, 2FASTQ Conversion Software 1.8.4. [<ext-link ext-link-type="uri" xlink:href="http://support.illumina.com/downloads/bcl2fastq_conversion_software_184.html">http://support.illumina.com/downloads/bcl2fastq_conversion_software_184.html</ext-link>]</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10</label>
      <mixed-citation publication-type="other">fastx-toolkit. [<ext-link ext-link-type="uri" xlink:href="http://hannonlab.cshl.edu/fastx_toolkit/">http://hannonlab.cshl.edu/fastx_toolkit/</ext-link>]</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Camerlengo</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Ozer</surname>
            <given-names>HG</given-names>
          </name>
          <name>
            <surname>Onti-Srinivasan</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>From sequencer to supercomputer: an automatic pipeline for managing and processing next generation sequencing data</article-title>
        <source>AMIA Summits Translational Sci Proc.</source>
        <year>2012</year>
        <volume>2012</volume>
        <fpage>1</fpage>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lassmann</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Hasegawa</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Daub</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Carninci</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Hayashizaki</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Moirai: a compact workflow system for cage analysis</article-title>
        <source>BMC Bioinf.</source>
        <year>2014</year>
        <volume>15</volume>
        <issue>1</issue>
        <fpage>144</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-15-144</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Giardine</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Riemer</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Hardison</surname>
            <given-names>RC</given-names>
          </name>
          <name>
            <surname>Burhans</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Elnitski</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Shah</surname>
            <given-names>P</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Galaxy: a platform for interactive large-scale genome analysis</article-title>
        <source>Genome Res.</source>
        <year>2005</year>
        <volume>15</volume>
        <issue>10</issue>
        <fpage>1451</fpage>
        <lpage>5</lpage>
        <pub-id pub-id-type="doi">10.1101/gr.4086505</pub-id>
        <pub-id pub-id-type="pmid">16169926</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14</label>
      <mixed-citation publication-type="other">Pachter L. *Seq. Bits of DNA: Reviews and commentary on computational biology. [{http://liorpachter.wordpress.com/seq/}]</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15</label>
      <mixed-citation publication-type="other">Durbin R. Biological sequence analysis: probabilistic models of proteins and nucleic acids: Cambridge university press; 1998.</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Ruan</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Durbin</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Mapping short dna sequencing reads and calling variants using mapping quality scores</article-title>
        <source>Genome Res.</source>
        <year>2008</year>
        <volume>18</volume>
        <issue>11</issue>
        <fpage>1851</fpage>
        <lpage>8</lpage>
        <pub-id pub-id-type="doi">10.1101/gr.078212.108</pub-id>
        <pub-id pub-id-type="pmid">18714091</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Käll</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Krogh</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sonnhammer</surname>
            <given-names>EL</given-names>
          </name>
        </person-group>
        <article-title>An hmm posterior decoder for sequence feature prediction that includes homology information</article-title>
        <source>Bioinformatics</source>
        <year>2005</year>
        <volume>21</volume>
        <issue>suppl 1</issue>
        <fpage>251</fpage>
        <lpage>257</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bti1014</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Myers</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>A fast bit-vector algorithm for approximate string matching based on dynamic programming</article-title>
        <source>J ACM (JACM)</source>
        <year>1999</year>
        <volume>46</volume>
        <issue>3</issue>
        <fpage>395</fpage>
        <lpage>415</lpage>
        <pub-id pub-id-type="doi">10.1145/316542.316550</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Faircloth</surname>
            <given-names>BC</given-names>
          </name>
          <name>
            <surname>Glenn</surname>
            <given-names>TC</given-names>
          </name>
        </person-group>
        <article-title>Not all sequence tags are created equal: designing and validating sequence identification tags robust to indels</article-title>
        <source>PloS one</source>
        <year>2012</year>
        <volume>7</volume>
        <issue>8</issue>
        <fpage>42543</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0042543</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20</label>
      <mixed-citation publication-type="other">Li H. Aligning sequence reads, clone sequences and assembly contigs with bwa-mem. arXiv preprint. 2013. arXiv:1303.3997.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Islam</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kjällquist</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Moliner</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Zajac</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Fan</surname>
            <given-names>J-B</given-names>
          </name>
          <name>
            <surname>Lönnerberg</surname>
            <given-names>P</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Characterization of the single-cell transcriptional landscape by highly multiplex rna-seq</article-title>
        <source>Genome Res.</source>
        <year>2011</year>
        <volume>21</volume>
        <issue>7</issue>
        <fpage>1160</fpage>
        <lpage>7</lpage>
        <pub-id pub-id-type="doi">10.1101/gr.110882.110</pub-id>
        <pub-id pub-id-type="pmid">21543516</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Pertea</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Trapnell</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Pimentel</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Kelley</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Salzberg</surname>
            <given-names>SL</given-names>
          </name>
        </person-group>
        <article-title>Tophat2: accurate alignment of transcriptomes in the presence of insertions, deletions and gene fusions</article-title>
        <source>Genome Biol.</source>
        <year>2013</year>
        <volume>14</volume>
        <issue>4</issue>
        <fpage>36</fpage>
        <pub-id pub-id-type="doi">10.1186/gb-2013-14-4-r36</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
