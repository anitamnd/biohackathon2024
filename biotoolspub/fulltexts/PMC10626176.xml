<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_GPB715 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEgr4 jpg ?>
<?FILEmmc1 docx ?>
<?FILEmmc2 docx ?>
<?FILEmmc3 docx ?>
<?FILEmmc4 docx ?>
<?FILEmmc5 docx ?>
<?FILEmmc6 docx ?>
<?FILEmmc7 docx ?>
<?FILEsi1 svg ?>
<?FILEsi2 svg ?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Genomics Proteomics Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Genomics Proteomics Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Genomics, Proteomics &amp; Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1672-0229</issn>
    <issn pub-type="epub">2210-3244</issn>
    <publisher>
      <publisher-name>Elsevier</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10626176</article-id>
    <article-id pub-id-type="pii">S1672-0229(23)00066-9</article-id>
    <article-id pub-id-type="doi">10.1016/j.gpb.2023.04.001</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Web Server</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>NetGO 3.0: Protein Language Model Improves Large-scale Functional Annotations</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au005">
        <name>
          <surname>Wang</surname>
          <given-names>Shaojun</given-names>
        </name>
        <xref rid="af005" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author" id="au010">
        <name>
          <surname>You</surname>
          <given-names>Ronghui</given-names>
        </name>
        <xref rid="af005" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author" id="au015">
        <name>
          <surname>Liu</surname>
          <given-names>Yunjia</given-names>
        </name>
        <xref rid="af010" ref-type="aff">2</xref>
      </contrib>
      <contrib contrib-type="author" id="au020">
        <name>
          <surname>Xiong</surname>
          <given-names>Yi</given-names>
        </name>
        <xref rid="af015" ref-type="aff">3</xref>
        <xref rid="af020" ref-type="aff">4</xref>
      </contrib>
      <contrib contrib-type="author" id="au025">
        <name>
          <surname>Zhu</surname>
          <given-names>Shanfeng</given-names>
        </name>
        <email>zhusf@fudan.edu.cn</email>
        <xref rid="af005" ref-type="aff">1</xref>
        <xref rid="af025" ref-type="aff">5</xref>
        <xref rid="af030" ref-type="aff">6</xref>
        <xref rid="af035" ref-type="aff">7</xref>
        <xref rid="af040" ref-type="aff">8</xref>
        <xref rid="cor1" ref-type="corresp">⁎</xref>
      </contrib>
      <aff id="af005"><label>1</label>Institute of Science and Technology for Brain-Inspired Intelligence and MOE Frontiers Center for Brain Science, Fudan University, Shanghai 200433, China</aff>
      <aff id="af010"><label>2</label>School of Life Sciences, Fudan University, Shanghai 200433, China</aff>
      <aff id="af015"><label>3</label>Department of Bioinformatics and Biostatistics, Shanghai Jiao Tong University, Shanghai 200240, China</aff>
      <aff id="af020"><label>4</label>Shanghai Artificial Intelligence Laboratory, Shanghai 200232, China</aff>
      <aff id="af025"><label>5</label>Shanghai Qi Zhi Institute, Shanghai 200030, China</aff>
      <aff id="af030"><label>6</label>MOE Key Laboratory of Computational Neuroscience and Brain-Inspired Intelligence, Fudan University, Shanghai 200433, China</aff>
      <aff id="af035"><label>7</label>Shanghai Key Laboratory of Intelligent Information Processing and Shanghai Institute of Artificial Intelligence Algorithm, Fudan University, Shanghai 200433, China</aff>
      <aff id="af040"><label>8</label>Zhangjiang Fudan International Innovation Center, Shanghai 200433, China</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>⁎</label>Corresponding author. <email>zhusf@fudan.edu.cn</email></corresp>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>17</day>
      <month>4</month>
      <year>2023</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="ppub">
      <month>4</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>17</day>
      <month>4</month>
      <year>2023</year>
    </pub-date>
    <volume>21</volume>
    <issue>2</issue>
    <fpage>349</fpage>
    <lpage>358</lpage>
    <history>
      <date date-type="received">
        <day>17</day>
        <month>5</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>24</day>
        <month>2</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>7</day>
        <month>4</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2023 Beijing Institute of Genomics</copyright-statement>
      <copyright-year>2023</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).</license-p>
      </license>
    </permissions>
    <abstract id="ab005">
      <p>As one of the state-of-the-art automated function prediction (AFP) methods, NetGO 2.0 integrates multi-source information to improve the performance. However, it mainly utilizes the proteins with experimentally supported functional annotations without leveraging valuable information from a vast number of unannotated proteins. Recently, <bold>protein language models</bold> have been proposed to learn informative representations [<italic>e.g.</italic>, Evolutionary Scale Modeling (ESM)-1b embedding] from protein sequences based on self-supervision. Here, we represented each protein by ESM-1b and used logistic regression (LR) to train a new model, LR-ESM, for AFP. The experimental results showed that LR-ESM achieved comparable performance with the best-performing component of NetGO 2.0. Therefore, by incorporating LR-ESM into NetGO 2.0, we developed NetGO 3.0 to improve the performance of AFP extensively. NetGO 3.0 is freely accessible at <ext-link ext-link-type="uri" xlink:href="https://dmiip.sjtu.edu.cn/ng3.0" id="ir005">https://dmiip.sjtu.edu.cn/ng3.0</ext-link>.</p>
    </abstract>
    <kwd-group id="kg005">
      <title>Keywords</title>
      <kwd>Protein function prediction</kwd>
      <kwd>Web service</kwd>
      <kwd>Protein language model</kwd>
      <kwd>Learning to rank</kwd>
      <kwd>Large-scale multi-label learning</kwd>
    </kwd-group>
  </article-meta>
  <notes>
    <p id="ms005">Handled by Alex Bateman</p>
  </notes>
</front>
<body>
  <sec id="s0005">
    <title>Introduction</title>
    <p id="p0005">Proteins are complex molecules that play essential roles in various biological activities. To understand the underlying mechanism of an organism as a physical system, annotating the functions of proteins is a crucial task. Gene Ontology (GO) came into being in 1998 to describe varying levels of functional information on gene/RNA/protein, which contains three domains: molecular function (MF), biological process (BP), and cellular component (CC) with over 40,000 terms <xref rid="b0005" ref-type="bibr">[1]</xref>. As of November 2022, the number of raw protein sequences is more than 230 million in Universal Protein Knowledgebase (UniProtKB), but less than 0.1% of them have experimental annotations <xref rid="b0010" ref-type="bibr">[2]</xref>. It is thus desirable to develop high-performance computational methods to achieve automated function prediction (AFP) without costly experiments <xref rid="b0015" ref-type="bibr">[3]</xref>.</p>
    <p id="p0010">AFP is a large-scale multi-label classification problem in which multiple related GO terms are assigned to a target protein. In the last few years, several high-performance web servers have been developed for AFP, such as INGA 2.0 <xref rid="b0020" ref-type="bibr">[4]</xref>, DeepGOWeb <xref rid="b0025" ref-type="bibr">[5]</xref>, MetaGO <xref rid="b0030" ref-type="bibr">[6]</xref>, and QAUST <xref rid="b0035" ref-type="bibr">[7]</xref>. Under the learning to rank (LTR) framework <xref rid="b0040" ref-type="bibr">[8]</xref>, GOLabeler <xref rid="b0045" ref-type="bibr">[9]</xref>, NetGO <xref rid="b0050" ref-type="bibr">[10]</xref>, and NetGO 2.0 <xref rid="b0055" ref-type="bibr">[11]</xref> achieved a state-of-the-art performance in the recent community-wide Critical Assessment of Functional Annotation (CAFA) <xref rid="b0015" ref-type="bibr">[3]</xref>. Specifically, NetGO 2.0 integrates protein information from different sources to encode proteins in a computer-understandable way, such as sequences, protein domains, protein–protein interaction networks, and scientific literature. However, it does not leverage valuable information from unannotated proteins (&gt; 99.9% of all known proteins).</p>
    <p id="p0015">Recently, the idea of pre-training in natural language processing <xref rid="b0060" ref-type="bibr">[12]</xref> has been extended to build protein language models using self-supervised learning on millions of sequences <xref rid="b0065" ref-type="bibr">[13]</xref>, <xref rid="b0070" ref-type="bibr">[14]</xref>, <xref rid="b0075" ref-type="bibr">[15]</xref>. Most protein language models predict the masked or next amino acid within a sequence and generate protein embeddings that can generalize across downstream tasks (more details shown in <xref rid="s0095" ref-type="sec">File S1</xref>). Some recent studies have explored protein language models for AFP <xref rid="b0080" ref-type="bibr">[16]</xref>, <xref rid="b0085" ref-type="bibr">[17]</xref>. However, they have a common limitation: less frequent GO terms (<italic>e.g.</italic>, having less than 40 annotated proteins) are excluded in the evaluation, which accounts for around 75% of total annotations in the CAFA setting. In this work, we predicted the associations between proteins and each GO term based on Evolutionary Scale Modeling (ESM)-1b embeddings, which were trained on over 250 million protein sequences <xref rid="b0065" ref-type="bibr">[13]</xref>. Our experimental results showed that the learned representations were helpful to AFP. Therefore, we developed NetGO 3.0 by incorporating ESM-1b embeddings in order to improve the performance extensively, which highlights the predictive power of the protein language model for AFP.</p>
  </sec>
  <sec id="s0010">
    <title>Method</title>
    <sec id="s0015">
      <title>Protein language models</title>
      <p id="p0020">A challenging problem is figuring out how to represent protein sequences as fixed-length vectors that capture the realistic sequence–function relationship. Traditional methods rely on a holistic understanding of protein properties. Recently, protein language models have provided a solution that interprets protein sequences as sentences and amino acids as words to extract fundamental features of a protein with rich and systematic information. Protein language models train nonlinear neural networks with an unsupervised objective on a large-scale dataset of protein sequences <xref rid="b0065" ref-type="bibr">[13]</xref>, <xref rid="b0070" ref-type="bibr">[14]</xref>, <xref rid="b0075" ref-type="bibr">[15]</xref>, <xref rid="b0105" ref-type="bibr">[21]</xref>.</p>
      <p id="p0025">Generally, protein language models apply deep learning models such as recurrent neural networks (RNN) and Transformer to achieve statistical embeddings of proteins from tremendous sequences. UniRep represented protein sequences as fixed-length vectors by long short-term memory (LSTM) with ∼ 24 million sequences <xref rid="b0075" ref-type="bibr">[15]</xref>. Task Assessing Protein Embeddings (TAPE) distilled protein properties from sequences by semi-supervised learning based on ResNet, LSTM, and Transformer, and then evaluated their performance on five biologically relevant tasks <xref rid="b0105" ref-type="bibr">[21]</xref>. Moreover, a multi-task learning framework was recently proposed to incorporate structural information (<italic>e.g.</italic>, contact maps and structural similarity prediction tasks) to enrich protein language models <xref rid="b0110" ref-type="bibr">[22]</xref>. Furthermore, researchers applied protein language models to study protein molecular function prediction <xref rid="b0085" ref-type="bibr">[17]</xref>. UDSMProt put forward a task-agnostic representation for proteins and achieved good performance on protein-level prediction tasks, namely enzyme class prediction and GO prediction <xref rid="b0080" ref-type="bibr">[16]</xref>. However, both methods should have considered less frequent GO terms.</p>
      <p id="p0030">In this study, a new component logistic regression (LR)-ESM in NetGO 3.0 was proposed to utilize ESM-1b, a 34-layer Transformer-based model trained on Universal Protein Archive (UniParc) database with 250 million protein sequences and 650 million parameters, to generate protein-level representations by average pooling across all residue-level embeddings <xref rid="b0065" ref-type="bibr">[13]</xref>.</p>
    </sec>
    <sec id="s0020">
      <title>Implementation</title>
      <p id="p0035">NetGO 2.0 integrates seven component methods, which are Naïve, BLAST-KNN, LR-3mer, LR-InterPro, Net-KNN, LR-Text, and Seq-RNN. We replaced Seq-RNN with LR-ESM in NetGO 3.0, which makes function prediction based on a protein language model. Specifically, LR-ESM utilized ESM-1b, a 34-layer Transformer-based model trained on the UniParc database with 250 million sequences <xref rid="b0065" ref-type="bibr">[13]</xref>, to generate protein embeddings and complete prediction. As ESM-1b has a limitation of protein sequence length, we kept the first 1000 amino acids for those protein sequences longer than 1024. We then used ESM-1b to encode each amino acid as an embedding of size 1280 for a target protein. To obtain the protein-level embedding, we applied the operation of average pooling on all amino acid positions, which comprehensively collects information from sequence data alone. Finally, LR-ESM utilized protein embeddings as input to train LR classifiers and estimated the association between target proteins and each GO term.</p>
    </sec>
    <sec id="s0025">
      <title>Benchmark datasets</title>
      <p id="p0040">As NetGO 2.0 collected the data following the setting of CAFA, we utilized the same benchmark dataset to evaluate the performance of NetGO 3.0 and the competing methods. <xref rid="s0095" ref-type="sec">Table S1</xref> reports the number of proteins in the benchmark dataset.</p>
      <p id="p0045">To take advantage of the latest annotation data, we collected sequences and GO terms before January 2022 from Universal Protein (UniProt) <xref rid="b0010" ref-type="bibr">[2]</xref>, Gene Ontology Annotation (GOA) <xref rid="b0115" ref-type="bibr">[23]</xref>, and GO <xref rid="b0005" ref-type="bibr">[1]</xref>. Similarly, we trained and updated our model on the new dataset by following the standard protocols of NetGO 2.0 <xref rid="b0055" ref-type="bibr">[11]</xref>. Training data are all experimental annotation data before January 2020. Validation data are all experimental no-knowledge and limited-knowledge proteins annotated from January 2020 to December 2020. Testing data are all experimental no-knowledge proteins between January 2021 and December 2021. More details for the new dataset and the definition of no-knowledge and limited-knowledge proteins are listed in <xref rid="s0095" ref-type="sec">File S1</xref> and <xref rid="s0095" ref-type="sec">Table S2</xref>.</p>
    </sec>
  </sec>
  <sec id="s0030">
    <title>Results</title>
    <p id="p0050">We compared the performance of NetGO 3.0 with the competing methods on the benchmark dataset from NetGO 2.0. The performance was evaluated by area under the precision–recall curve (AUPRC) and two standard metrics in CAFA, the maximum F1-score (<inline-formula><mml:math id="M1" altimg="si1.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) and the minimum semantic distance (<inline-formula><mml:math id="M2" altimg="si2.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>). The definitions of these three metrics are given in <xref rid="s0095" ref-type="sec">Section S1</xref> of <xref rid="s0095" ref-type="sec">File S1</xref>.</p>
    <sec id="s0035">
      <title>Performance comparison of NetGO 3.0 with its component methods and competing methods</title>
      <p id="p0055"><xref rid="t0005" ref-type="table">Table 1</xref> illustrates the test results for NetGO 3.0, NetGO 2.0, GOLabeler, DeepGOWeb, and the component methods of NetGO 3.0. Previous studies have shown that GOLabeler and NetGO 2.0 achieved top performance in CAFA3 and CAFA4, respectively <xref rid="b0045" ref-type="bibr">[9]</xref>, <xref rid="b0055" ref-type="bibr">[11]</xref>, and DeepGOWeb provided an accurate prediction for protein function by deep learning <xref rid="b0025" ref-type="bibr">[5]</xref>.<table-wrap position="float" id="t0005"><label>Table 1</label><caption><p><bold>Performance comparison of NetGO 3.0 with its components and competing methods on the test set</bold></p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="left"><bold>Method</bold></th><th colspan="3" align="center"><inline-formula><mml:math id="M3" altimg="si1.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula><hr/></th><th colspan="3" align="center"><bold>AUPRC</bold><hr/></th><th colspan="3" align="center"><inline-formula><mml:math id="M4" altimg="si2.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula><hr/></th></tr><tr><th align="left"><bold>MF</bold></th><th align="left"><bold>BP</bold></th><th align="left"><bold>CC</bold></th><th align="left"><bold>MF</bold></th><th align="left"><bold>BP</bold></th><th align="left"><bold>CC</bold></th><th align="left"><bold>MF</bold></th><th align="left"><bold>BP</bold></th><th align="left"><bold>CC</bold></th></tr></thead><tbody><tr><td>Naïve</td><td align="char">0.416</td><td align="char">0.256</td><td align="char">0.542</td><td align="char">0.276</td><td align="char">0.118</td><td align="char">0.464</td><td align="char">5.683</td><td align="char">14.497</td><td align="char">6.136</td></tr><tr><td>BLAST-KNN</td><td align="char">0.632</td><td align="char">0.312</td><td align="char">0.566</td><td align="char">0.542</td><td align="char">0.132</td><td align="char">0.405</td><td align="char">4.098</td><td align="char">14.198</td><td align="char">5.288</td></tr><tr><td>LR-3mer</td><td align="char">0.427</td><td align="char">0.258</td><td align="char">0.552</td><td align="char">0.317</td><td align="char">0.125</td><td align="char">0.478</td><td align="char">5.512</td><td align="char">14.487</td><td align="char">6.035</td></tr><tr><td>LR-InterPro</td><td align="char"><underline>0.651</underline></td><td align="char">0.325</td><td align="char"><underline>0.641</underline></td><td align="char"><underline>0.623</underline></td><td align="char">0.166</td><td align="char"><underline>0.587</underline></td><td align="char">4.055</td><td align="char">14.090</td><td align="char">5.066</td></tr><tr><td>Net-KNN</td><td align="char">0.519</td><td align="char">0.325</td><td align="char">0.596</td><td align="char">0.416</td><td align="char">0.192</td><td align="char">0.528</td><td align="char">5.298</td><td align="char">13.929</td><td align="char">5.554</td></tr><tr><td>Seq-RNN</td><td align="char">0.524</td><td align="char">0.265</td><td align="char">0.574</td><td align="char">0.424</td><td align="char">0.124</td><td align="char">0.477</td><td align="char">5.129</td><td align="char">14.465</td><td align="char">5.573</td></tr><tr><td>LR-Text</td><td align="char">0.464</td><td align="char">0.248</td><td align="char">0.479</td><td align="char">0.353</td><td align="char">0.154</td><td align="char">0.403</td><td align="char">5.362</td><td align="char">13.919</td><td align="char">5.713</td></tr><tr><td>LR-ESM</td><td align="char">0.637</td><td align="char"><underline>0.334</underline></td><td align="char">0.631</td><td align="char">0.615</td><td align="char"><underline>0.197</underline></td><td align="char">0.572</td><td align="char"><underline>3.891</underline></td><td align="char"><underline>13.612</underline></td><td align="char"><underline>5.052</underline></td></tr><tr><td>DeepGOWeb</td><td align="char">0.620</td><td align="char">0.305</td><td align="char">0.620</td><td align="char">0.521</td><td align="char">0.115</td><td align="char">0.493</td><td align="char">4.496</td><td align="char">14.772</td><td align="char">5.550</td></tr><tr><td>GOLabeler</td><td align="char">0.667</td><td align="char">0.326</td><td align="char">0.631</td><td align="char">0.647</td><td align="char">0.193</td><td align="char">0.557</td><td align="char">3.970</td><td align="char">13.558</td><td align="char">5.295</td></tr><tr><td>NetGO 2.0</td><td align="char">0.666</td><td align="char">0.366</td><td align="char">0.663</td><td align="char">0.655</td><td align="char"><bold>0.269</bold></td><td align="char">0.593</td><td align="char">4.013</td><td align="char">12.984</td><td align="char">4.756</td></tr><tr><td>NetGO 3.0</td><td align="char"><bold>0.679</bold></td><td align="char"><bold>0.378</bold></td><td align="char"><bold>0.670</bold></td><td align="char"><bold>0.672</bold></td><td align="char">0.268</td><td align="char"><bold>0.620</bold></td><td align="char"><bold>3.840</bold></td><td align="char"><bold>12.800</bold></td><td align="char"><bold>4.735</bold></td></tr></tbody></table><table-wrap-foot><fn><p><italic>Note</italic>: Naïve, BLAST-KNN, LR-3mer, LR-InterPro, Net-KNN, Seq-RNN, and LR-Text are component methods from NetGO 2.0. LR-ESM is a new component method which replaces Seq-RNN in NetGO 3.0. The underlined numbers imply the best performance for component methods. The bold numbers mean the best performance among competing methods. <inline-formula><mml:math id="M5" altimg="si1.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, the maximum F1-score; AUPRC, area under precision–recall curve; <inline-formula><mml:math id="M6" altimg="si2.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, the minimum semantic distance; MF, molecular function; BP, biological process; CC, cellular component; LR, logistic regression; KNN, k-nearest neighbors; BLAST, Basic Local Alignment Search Tool; RNN, recurrent neural networks; ESM, Evolutionary Scale Modeling; GO, Gene Ontology.</p></fn></table-wrap-foot></table-wrap></p>
      <p id="p0060">We selected Naïve, BLAST-KNN, and Seq-RNN <xref rid="b0055" ref-type="bibr">[11]</xref> from NetGO 2.0 as three baseline methods. The Naïve method annotates each pair of protein and GO term with a score that equals the probability of the term appearing in the training data. BLAST-KNN assigns a protein with GO terms based on annotations of its top BLAST hits <xref rid="b0045" ref-type="bibr">[9]</xref>. Although the first two are component methods inherited from both NetGO and NetGO2.0, Seq-RNN is a new component of NetGO2.0, which is designed to extract the deep representation of a protein sequence <xref rid="b0055" ref-type="bibr">[11]</xref>. As shown in <xref rid="f0005" ref-type="fig">Figure 1</xref> and <xref rid="t0005" ref-type="table">Table 1</xref>, LR-ESM outperformed baseline methods on all three GO domains. As a replacement for Seq-RNN, LR-ESM achieved a better performance. Specifically, in terms of <inline-formula><mml:math id="M7" altimg="si1.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, LR-ESM achieved 21.6%, 31.3%, and 7.5% improvements over Seq-RNN on MF, BP, and CC, respectively, which indicates the effectiveness of ESM-1b for AFP. Moreover, LR-ESM and LR-InterPro showed comparable performance in all three GO domains (<xref rid="t0005" ref-type="table">Table 1</xref>). Note that, in terms of <inline-formula><mml:math id="M8" altimg="si2.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, LR-ESM outperformed all other component methods and even achieved a better performance on MF than NetGO 2.0. Therefore, it is reasonable to construct a more robust model by incorporating LR-ESM into NetGO 2.0.<fig id="f0005"><label>Figure 1</label><caption><p><bold>Performance comparison on</bold><inline-formula><mml:math id="M9" altimg="si1.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula><bold>and</bold><inline-formula><mml:math id="M10" altimg="si2.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></p><p>The performance of Naïve, BLAST-KNN, Seq-RNN, LR-ESM, NetGO 2.0, and NetGO 3.0 on the benchmark dataset of NetGO 2.0 over three GO domains is shown. Higher values for <inline-formula><mml:math id="M11" altimg="si1.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and lower values for <inline-formula><mml:math id="M12" altimg="si2.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> indicate better performance over three GO domains. The error lines denote the confidence intervals (95%) calculated by bootstrapping with 100 iterations on the test set. MF, molecular function; BP, biological process; CC, cellular component; GO, Gene Ontology; LR, logistic regression; KNN, k-nearest neighbors; BLAST, Basic Local Alignment Search Tool; RNN, recurrent neural networks; ESM, Evolutionary Scale Modeling.</p></caption><graphic xlink:href="gr1"/></fig></p>
      <p id="p0065">Furthermore, we compared NetGO 3.0 with GOLabeler, DeepGOWeb, and NetGO 2.0, three high-performance methods in CAFA. As shown in <xref rid="t0005" ref-type="table">Table 1</xref>, NetGO 3.0 achieved a more superior performance than the competing methods. In terms of <inline-formula><mml:math id="M13" altimg="si1.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M14" altimg="si2.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, NetGO 3.0 achieved a better performance in all three GO domains. For example, NetGO 3.0 achieved the highest <inline-formula><mml:math id="M15" altimg="si1.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> of 0.378 in BP, which is 16.0%, 23.9%, and 3.3% improvements over GOLabeler (0.326), DeepGOWeb (0.305), and NetGO 2.0 (0.366), respectively. The results demonstrate that NetGO 3.0 can benefit from protein language models with deep dense embeddings.</p>
      <p id="p0070">To better illustrate the strength of NetGO 3.0, we drew Venn diagrams in <xref rid="f0010" ref-type="fig">Figure 2</xref> to show the overlaps and differences among the prediction results of NetGO 3.0, GOLabeler, and DeepGOWeb. There are three main findings. (1) Although each method can predict distinct GO terms, the prediction results of the three methods overlapped substantially, especially in CC. Specifically, there were 6.96 GO terms assigned to one protein on average that were predicted by all three methods in CC, which accounted for 62.5%, 70.1%, and 77.3% in the prediction results of DeepGOWeb (11.14), GOLabeler (9.84), and NetGO 3.0 (9.00), respectively. (2) DeepGOWeb predicted more GO terms but achieved lower performance than the other two methods, indicating that false-positive GO terms are common in the prediction results. For example, DeeGOWeb predicted 21.34 distinct GO terms and achieved the lowest <inline-formula><mml:math id="M16" altimg="si1.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> of 0.305 in BP, which suggests that most of its predicted GO terms are incorrect. (3) Compared with MF and CC, NetGO 3.0 and GOLabeler differred significantly in predicting GO terms in BP. In terms of BP, although the 15.42 GO terms predicted by the two methods are consistent, the numbers of distinct GO terms predicted by NetGO 3.0 and GOLabeler are 9.59 and 8.18, respectively. We note that NetGO 3.0 performed better than GOLabeler in BP in terms of <inline-formula><mml:math id="M17" altimg="si1.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, where NetGO 3.0 (0.378) achieved a 16.0% improvement compared with GOLabeler (0.326). It demonstrates that NetGO 3.0 is more accurate and can predict more true-positive terms for query proteins.<fig id="f0010"><label>Figure 2</label><caption><p><bold>The overlap and difference among the GO terms predicted by GOLabeler, DeepGOWeb, and NetGO 3.0</bold></p><p>The Venn diagrams depict the overlap and difference among the GO terms predicted by GOLabeler, DeepGOWeb, and NetGO 3.0 in MF (<bold>A</bold>), BP (<bold>B</bold>), and CC (<bold>C</bold>), respectively. Numbers in the graph represent the average number of predicted GO terms over test proteins in three methods.</p></caption><graphic xlink:href="gr2"/></fig></p>
    </sec>
    <sec id="s0040">
      <title>Performance on specific species (humans and mice)</title>
      <p id="p0075">Species-specific analyses are helpful for researchers to study a certain species. Here, we explored the performance of different AFP methods over two model species, humans and mice. <xref rid="t0010" ref-type="table">Table 2</xref> and <xref rid="t0015" ref-type="table">Table 3</xref> showed the performance of NetGO 3.0 and NetGO 2.0, as well as the components of both methods for protein function prediction in humans and mice. We observed that all methods obtained a better prediction performance on human proteins than on mouse proteins. For example, LR-InterPro, LR-ESM, and NetGO 3.0 achieved higher AUPRC values of 0.704, 0.690, and 0.730 on human proteins in MF, whereas the three methods only achieved AUPRC values of 0.609, 0.615, and 0.620 on mouse proteins. The annotation information for different species is from different databases, which may lead to the difference. Moreover, LR-ESM again achieved a similar performance as LR-InterPro in both species, which strongly demonstrates that features extracted by ESM-1b are as robust as InterProScan among many species.<table-wrap position="float" id="t0010"><label>Table 2</label><caption><p><bold>Performance comparison of NetGO 3.0 and NetGO 2.0 as well as their component methods for protein function prediction in humans</bold></p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="left"><bold>Method</bold></th><th colspan="3" align="center"><inline-formula><mml:math id="M18" altimg="si1.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula><hr/></th><th colspan="3" align="center"><bold>AUPRC</bold><hr/></th><th colspan="3" align="center"><inline-formula><mml:math id="M19" altimg="si2.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula><hr/></th></tr><tr><th align="left"><bold>MF</bold></th><th align="left"><bold>BP</bold></th><th align="left"><bold>CC</bold></th><th align="left"><bold>MF</bold></th><th align="left"><bold>BP</bold></th><th align="left"><bold>CC</bold></th><th align="left"><bold>MF</bold></th><th align="left"><bold>BP</bold></th><th align="left"><bold>CC</bold></th></tr></thead><tbody><tr><td>BLAST-KNN</td><td align="char">0.655</td><td align="char">0.370</td><td align="char">0.520</td><td align="char">0.581</td><td align="char">0.229</td><td align="char">0.339</td><td align="char">3.186</td><td align="char">13.067</td><td align="char">4.478</td></tr><tr><td>LR-InterPro</td><td align="char"><underline>0.715</underline></td><td align="char">0.373</td><td align="char">0.626</td><td align="char"><underline>0.704</underline></td><td align="char">0.307</td><td align="char">0.589</td><td align="char"><underline>3.078</underline></td><td align="char">12.732</td><td align="char">4.254</td></tr><tr><td>Net-KNN</td><td align="char">0.598</td><td align="char">0.358</td><td align="char">0.592</td><td align="char">0.565</td><td align="char">0.243</td><td align="char">0.536</td><td align="char">3.882</td><td align="char">14.304</td><td align="char">4.756</td></tr><tr><td>Seq-RNN</td><td align="char">0.596</td><td align="char">0.291</td><td align="char">0.585</td><td align="char">0.523</td><td align="char">0.184</td><td align="char">0.536</td><td align="char">3.850</td><td align="char">14.729</td><td align="char">4.401</td></tr><tr><td>LR-ESM</td><td align="char">0.711</td><td align="char"><underline>0.450</underline></td><td align="char"><underline>0.645</underline></td><td align="char">0.690</td><td align="char"><underline>0.358</underline></td><td align="char"><underline>0.664</underline></td><td align="char">3.105</td><td align="char"><underline>12.327</underline></td><td align="char"><underline>3.946</underline></td></tr><tr><td>NetGO 2.0</td><td align="char">0.715</td><td align="char">0.441</td><td align="char">0.673</td><td align="char">0.725</td><td align="char">0.401</td><td align="char">0.630</td><td align="char">3.018</td><td align="char">11.917</td><td align="char">3.566</td></tr><tr><td>NetGO 3.0</td><td align="char"><bold>0.721</bold></td><td align="char"><bold>0.481</bold></td><td align="char"><bold>0.674</bold></td><td align="char"><bold>0.730</bold></td><td align="char"><bold>0.439</bold></td><td align="char"><bold>0.670</bold></td><td align="char"><bold>2.929</bold></td><td align="char"><bold>11.451</bold></td><td align="char"><bold>3.557</bold></td></tr></tbody></table><table-wrap-foot><fn><p><italic>Note</italic>: BLAST-KNN, LR-InterPro, Net-KNN, and Seq-RNN are component methods from NetGO 2.0. LR-ESM is a new component method which replaces Seq-RNN in NetGO 3.0. The underlined numbers imply the best performance for component methods. The bold numbers mean the best performance among competing methods.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="t0015"><label>Table 3</label><caption><p><bold>Performance comparison of NetGO 3.0 and NetGO 2.0 as well as their component methods for protein function prediction in mice</bold></p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="left"><bold>Method</bold></th><th colspan="3" align="center"><inline-formula><mml:math id="M20" altimg="si1.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula><hr/></th><th colspan="3" align="center"><bold>AUPRC</bold><hr/></th><th colspan="3" align="center"><inline-formula><mml:math id="M21" altimg="si2.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula><hr/></th></tr><tr><th align="left"><bold>MF</bold></th><th align="left"><bold>BP</bold></th><th align="left"><bold>CC</bold></th><th align="left"><bold>MF</bold></th><th align="left"><bold>BP</bold></th><th align="left"><bold>CC</bold></th><th align="left"><bold>MF</bold></th><th align="left"><bold>BP</bold></th><th align="left"><bold>CC</bold></th></tr></thead><tbody><tr><td>BLAST-KNN</td><td align="char">0.616</td><td align="char">0.353</td><td align="char">0.572</td><td align="char">0.575</td><td align="char">0.147</td><td align="char">0.463</td><td align="char">5.681</td><td align="char">21.804</td><td align="char">5.931</td></tr><tr><td>LR-InterPro</td><td align="char">0.605</td><td align="char">0.344</td><td align="char"><underline>0.591</underline></td><td align="char">0.609</td><td align="char"><underline>0.211</underline></td><td align="char"><underline>0.542</underline></td><td align="char">5.732</td><td align="char">21.044</td><td align="char"><underline>5.489</underline></td></tr><tr><td>Net-KNN</td><td align="char">0.408</td><td align="char">0.341</td><td align="char">0.566</td><td align="char">0.253</td><td align="char">0.199</td><td align="char">0.502</td><td align="char">8.080</td><td align="char">21.309</td><td align="char">6.002</td></tr><tr><td>Seq-RNN</td><td align="char">0.520</td><td align="char">0.265</td><td align="char">0.537</td><td align="char">0.373</td><td align="char">0.106</td><td align="char">0.462</td><td align="char">6.787</td><td align="char">22.795</td><td align="char">5.993</td></tr><tr><td>LR-ESM</td><td align="char"><underline>0.639</underline></td><td align="char"><underline>0.352</underline></td><td align="char">0.561</td><td align="char"><underline>0.615</underline></td><td align="char">0.197</td><td align="char">0.539</td><td align="char"><underline>5.710</underline></td><td align="char"><underline>20.639</underline></td><td align="char">5.664</td></tr><tr><td>NetGO 2.0</td><td align="char"><bold>0.649</bold></td><td align="char">0.420</td><td align="char">0.617</td><td align="char">0.618</td><td align="char">0.315</td><td align="char">0.557</td><td align="char">5.683</td><td align="char">19.572</td><td align="char">5.563</td></tr><tr><td>NetGO 3.0</td><td align="char"><bold>0.649</bold></td><td align="char"><bold>0.427</bold></td><td align="char"><bold>0.620</bold></td><td align="char"><bold>0.620</bold></td><td align="char"><bold>0.316</bold></td><td align="char"><bold>0.568</bold></td><td align="char"><bold>5.583</bold></td><td align="char"><bold>19.545</bold></td><td align="char"><bold>5.034</bold></td></tr></tbody></table><table-wrap-foot><fn><p><italic>Note</italic>: BLAST-KNN, LR-InterPro, Net-KNN, and Seq-RNN are component methods from NetGO 2.0. LR-ESM is a new component method which replaces Seq-RNN in NetGO 3.0. The underlined numbers imply the best performance for component methods. The bold numbers mean the best performance among competing methods.</p></fn></table-wrap-foot></table-wrap></p>
      <p id="p0080">For human and mouse proteins, NetGO 3.0 outperformed NetGO 2.0 in all three GO domains. Specifically, NetGO 3.0 performed better than NetGO 2.0 in human BP prediction, which achieveed 9.3% and 9.5% improvements in terms of <inline-formula><mml:math id="M22" altimg="si1.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and AUPRC, respectively. Further, the results highlight the importance of source data and the effectiveness of the protein language model.</p>
    </sec>
    <sec id="s0045">
      <title>Performance comparison over groups categorized by the number of annotations per GO term</title>
      <p id="p0085">We divided GO terms in the test dataset into three groups according to the number of annotations per GO term: 10–30, 31–100, and &gt; 100. <xref rid="t0020" ref-type="table">Table 4</xref> showed the M-AUPRC computed in each group, where M-AUPRC is GO term-centric by averaging AUPRC on each GO term. LR-ESM outperformed other component methods in most cases, which indicates that ESM-1b embeddings are informative. Note that LR-ESM consistently ranked higher than LR-InterPro for three domains in the first group, especially for BP, which obtained a 47.8% improvement. It proves that protein embeddings are effective with such a vast amount of training data for AFP.<table-wrap position="float" id="t0020"><label>Table 4</label><caption><p><bold>Performance comparison over groups categorized by the number of annotations per GO term</bold></p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="left"><bold>Method</bold></th><th colspan="3" align="center"><bold>M-AUPRC in</bold><bold>MF</bold><hr/></th><th colspan="3" align="center"><bold>M-AUPRC in</bold><bold>BP</bold><hr/></th><th colspan="3" align="center"><bold>M-AUPRC in</bold><bold>CC</bold><hr/></th></tr><tr><th align="left"><bold>10–30</bold></th><th align="left"><bold>31–100</bold></th><th align="left"><bold>&gt; 100</bold></th><th align="left"><bold>10–30</bold></th><th align="left"><bold>31–100</bold></th><th align="left"><bold>&gt; 100</bold></th><th align="left"><bold>10–30</bold></th><th align="left"><bold>31–100</bold></th><th align="left"><bold>&gt; 100</bold></th></tr></thead><tbody><tr><td>BLAST-KNN</td><td align="char">0.628</td><td align="char">0.497</td><td align="char">0.614</td><td align="char">0.197</td><td align="char">0.131</td><td align="char">0.224</td><td align="char">0.265</td><td align="char">0.291</td><td align="char">0.528</td></tr><tr><td>LR-InterPro</td><td align="char">0.618</td><td align="char"><underline>0.562</underline></td><td align="char">0.634</td><td align="char">0.209</td><td align="char">0.138</td><td align="char">0.224</td><td align="char">0.231</td><td align="char">0.307</td><td align="char"><underline>0.589</underline></td></tr><tr><td>Net-KNN</td><td align="char">0.330</td><td align="char">0.253</td><td align="char">0.545</td><td align="char">0.139</td><td align="char">0.132</td><td align="char">0.222</td><td align="char">0.210</td><td align="char">0.269</td><td align="char">0.501</td></tr><tr><td>Seq-RNN</td><td align="char">0.434</td><td align="char">0.326</td><td align="char">0.525</td><td align="char">0.054</td><td align="char">0.062</td><td align="char">0.139</td><td align="char">0.152</td><td align="char">0.195</td><td align="char">0.437</td></tr><tr><td>LR-ESM</td><td align="char"><underline>0.642</underline></td><td align="char">0.516</td><td align="char"><underline>0.658</underline></td><td align="char"><underline>0.307</underline></td><td align="char"><underline>0.154</underline></td><td align="char"><underline>0.242</underline></td><td align="char"><underline>0.333</underline></td><td align="char"><underline>0.342</underline></td><td align="char">0.572</td></tr><tr><td>NetGO 2.0</td><td align="char">0.658</td><td align="char">0.569</td><td align="char">0.659</td><td align="char">0.248</td><td align="char">0.212</td><td align="char">0.329</td><td align="char">0.300</td><td align="char">0.389</td><td align="char">0.588</td></tr><tr><td>NetGO 3.0</td><td align="char"><bold>0.675</bold></td><td align="char"><bold>0.571</bold></td><td align="char"><bold>0.665</bold></td><td align="char"><bold>0.250</bold></td><td align="char"><bold>0.213</bold></td><td align="char"><bold>0.335</bold></td><td align="char"><bold>0.386</bold></td><td align="char"><bold>0.422</bold></td><td align="char"><bold>0.636</bold></td></tr></tbody></table><table-wrap-foot><fn><p><italic>Note</italic>: BLAST-KNN, LR-InterPro, Net-KNN, and Seq-RNN are component methods from NetGO 2.0. LR-ESM is a new component method which replaces Seq-RNN in NetGO 3.0. The underlined numbers imply the best performance for component methods. The bold numbers mean the best performance among competing methods.</p></fn></table-wrap-foot></table-wrap></p>
      <p id="p0090">NetGO 3.0 achieved the best results among all the methods in every group and domain except in the first group in BP, and the improvement over NetGO 2.0 was especially significant in CC. Specifically, the advances made by NetGO 3.0 were 28.7%, 8.4%, and 8.2% for the three groups, respectively. Moreover, we collected the CC terms in the second and third layers annotated with more than ten proteins in the test set. As shown in <xref rid="s0095" ref-type="sec">Figure S1</xref>, NetGO 3.0 achieved a better performance on most GO terms, which strongly suggests that ESM-1b is powerful for predicting protein functions about CC.</p>
    </sec>
    <sec id="s0050">
      <title>Performance comparison on difficult proteins</title>
      <p id="p0095">Following the CAFA setting, proteins with a BLAST identity of less than 0.6 to any protein in training data are identified as “difficult proteins” <xref rid="b0015" ref-type="bibr">[3]</xref>. In the test set, there are 66, 85, and 70 difficult proteins in MF, BP, and CC, respectively. It is evident that methods based on homology find it hard to predict the function of difficult proteins accurately. <xref rid="t0025" ref-type="table">Table 5</xref> showed the performance of different methods in dealing with difficult proteins. As mentioned above, BLAST-KNN, a method that annotates target proteins by homology proteins, ranked last in 9 experimental settings. We found that LR-InterPro and LR-ESM were the two best-performing component methods in this scenario. For example, in terms of <inline-formula><mml:math id="M23" altimg="si2.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, there is a slight difference between the two methods in three domains. LR-ESM and LR-InterPro achieved the best performance for all component methods in 6 and 3 out of 9 settings. Once again, NetGO 3.0 was proved to be the best method for predicting the function of difficult proteins.<table-wrap position="float" id="t0025"><label>Table 5</label><caption><p><bold>Performance on difficult proteins</bold></p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2"><bold>Method</bold></th><th colspan="3" align="center"><inline-formula><mml:math id="M24" altimg="si1.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula><hr/></th><th colspan="3" align="center"><bold>AUPRC</bold><hr/></th><th colspan="3" align="center"><inline-formula><mml:math id="M25" altimg="si2.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula><hr/></th></tr><tr><th><bold>MF</bold></th><th><bold>BP</bold></th><th><bold>CC</bold></th><th><bold>MF</bold></th><th><bold>BP</bold></th><th><bold>CC</bold></th><th><bold>MF</bold></th><th><bold>BP</bold></th><th><bold>CC</bold></th></tr></thead><tbody><tr><td>BLAST-KNN</td><td align="char">0.469</td><td align="char">0.261</td><td align="char">0.386</td><td align="char">0.217</td><td align="char">0.057</td><td align="char">0.206</td><td align="char">4.689</td><td align="char">13.036</td><td align="char">6.128</td></tr><tr><td>LR-InterPro</td><td align="char"><underline>0.614</underline></td><td align="char">0.308</td><td align="char">0.620</td><td align="char"><underline>0.551</underline></td><td align="char">0.156</td><td align="char">0.558</td><td align="char"><underline>4.018</underline></td><td align="char">12.217</td><td align="char">5.579</td></tr><tr><td>Net-KNN</td><td align="char">0.555</td><td align="char">0.319</td><td align="char">0.591</td><td align="char">0.308</td><td align="char">0.184</td><td align="char">0.528</td><td align="char">5.341</td><td align="char">12.564</td><td align="char">5.543</td></tr><tr><td>Seq-RNN</td><td align="char">0.486</td><td align="char">0.223</td><td align="char">0.560</td><td align="char">0.312</td><td align="char">0.097</td><td align="char">0.430</td><td align="char">5.264</td><td align="char">13.427</td><td align="char">5.946</td></tr><tr><td>LR-ESM</td><td align="char">0.598</td><td align="char"><underline>0.342</underline></td><td align="char"><underline>0.631</underline></td><td align="char">0.453</td><td align="char"><underline>0.216</underline></td><td align="char"><underline>0.594</underline></td><td align="char">4.314</td><td align="char"><underline>12.142</underline></td><td align="char"><underline>5.271</underline></td></tr><tr><td>NetGO 2.0</td><td align="char"><bold>0.645</bold></td><td align="char">0.356</td><td align="char">0.634</td><td align="char">0.596</td><td align="char">0.274</td><td align="char">0.595</td><td align="char">4.005</td><td align="char">11.644</td><td align="char">4.888</td></tr><tr><td>NetGO 3.0</td><td align="char"><bold>0.654</bold></td><td align="char"><bold>0.369</bold></td><td align="char"><bold>0.668</bold></td><td align="char"><bold>0.605</bold></td><td align="char"><bold>0.276</bold></td><td align="char"><bold>0.609</bold></td><td align="char"><bold>3.969</bold></td><td align="char"><bold>11.421</bold></td><td align="char"><bold>4.782</bold></td></tr></tbody></table><table-wrap-foot><fn><p><italic>Note</italic>: BLAST-KNN, LR-InterPro, Net-KNN, and Seq-RNN are component methods from NetGO 2.0. LR-ESM is a new component method which replaces Seq-RNN in NetGO 3.0. The underlined numbers imply the best performance for component methods. The bold numbers mean the best performance among competing methods.</p></fn></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="s0055">
      <title>Performance comparison on proteins with sequence length longer than 1000 amino acids</title>
      <p id="p0100">We performed a truncation operation for proteins longer than 1000 amino acids so that ESM-1b could generate representations for all proteins in the dataset. Focusing on the performance of each method on these long proteins helps us better understand the advantages and limitations of NetGO 3.0. There exist 21, 78, and 26 test proteins in MF, BP, and CC, respectively. <xref rid="t0030" ref-type="table">Table 6</xref> showed the prediction results of component methods, NetGO 2.0, and NetGO 3.0. We found that LR-ESM was no longer one of the best-performing component methods, which indirectly led to the worse performance of NetGO 3.0 than NetGO 2.0 in MF and BP. By comparing the performance of each method on the entire test set in <xref rid="t0005" ref-type="table">Table 1</xref>, we noticed that the performance decreased for all methods except Net-KNN. This suggests that function prediction for long proteins is a challenge.<table-wrap position="float" id="t0030"><label>Table 6</label><caption><p><bold>Performance comparison on proteins with sequence length longer than 1000 amino acids</bold></p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2"><bold>Method</bold></th><th colspan="3" align="center"><inline-formula><mml:math id="M26" altimg="si1.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula><hr/></th><th colspan="3" align="center"><bold>AUPRC</bold><hr/></th><th colspan="3" align="center"><inline-formula><mml:math id="M27" altimg="si2.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula><hr/></th></tr><tr><th><bold>MF</bold></th><th><bold>BP</bold></th><th><bold>CC</bold></th><th><bold>MF</bold></th><th><bold>BP</bold></th><th><bold>CC</bold></th><th><bold>MF</bold></th><th><bold>BP</bold></th><th><bold>CC</bold></th></tr></thead><tbody><tr><td>BLAST-KNN</td><td align="char">0.514</td><td align="char">0.272</td><td align="char">0.549</td><td align="char">0.271</td><td align="char">0.119</td><td align="char">0.465</td><td align="char">6.349</td><td align="char">15.176</td><td align="char">7.072</td></tr><tr><td>LR-InterPro</td><td align="char"><underline>0.595</underline></td><td align="char">0.312</td><td align="char"><underline>0.638</underline></td><td align="char">0.407</td><td align="char">0.111</td><td align="char"><underline>0.603</underline></td><td align="char"><underline>5.389</underline></td><td align="char"><underline>15.454</underline></td><td align="char"><underline>6.253</underline></td></tr><tr><td>Net-KNN</td><td align="char">0.515</td><td align="char"><underline>0.329</underline></td><td align="char">0.609</td><td align="char"><underline>0.455</underline></td><td align="char"><underline>0.205</underline></td><td align="char">0.598</td><td align="char">6.271</td><td align="char">14.510</td><td align="char">6.907</td></tr><tr><td>Seq-RNN</td><td align="char">0.509</td><td align="char">0.304</td><td align="char">0.587</td><td align="char">0.329</td><td align="char">0.162</td><td align="char">0.508</td><td align="char">6.103</td><td align="char">14.965</td><td align="char">6.903</td></tr><tr><td>LR-ESM</td><td align="char">0.536</td><td align="char">0.309</td><td align="char">0.586</td><td align="char">0.424</td><td align="char">0.135</td><td align="char">0.563</td><td align="char">5.855</td><td align="char">15.213</td><td align="char">6.662</td></tr><tr><td>NetGO 2.0</td><td align="char"><bold>0.587</bold></td><td align="char"><bold>0.357</bold></td><td align="char">0.625</td><td align="char"><bold>0.497</bold></td><td align="char"><bold>0.241</bold></td><td align="char">0.589</td><td align="char"><bold>5.312</bold></td><td align="char"><bold>13.824</bold></td><td align="char">6.240</td></tr><tr><td>NetGO 3.0</td><td align="char">0.577</td><td align="char">0.348</td><td align="char"><bold>0.631</bold></td><td align="char">0.485</td><td align="char">0.215</td><td align="char"><bold>0.606</bold></td><td align="char">5.452</td><td align="char">13.947</td><td align="char"><bold>5.938</bold></td></tr></tbody></table><table-wrap-foot><fn><p><italic>Note</italic>: BLAST-KNN, LR-InterPro, Net-KNN, and Seq-RNN are component methods from NetGO 2.0. LR-ESM is a new component method which replaces Seq-RNN in NetGO 3.0. The underlined numbers imply the best performance for component methods. The bold numbers mean the best performance among competing methods.</p></fn></table-wrap-foot></table-wrap></p>
      <p id="p0105">Moreover, we compared the prediction performance of NetGO 2.0 and NetGO 3.0 on several unannotated proteins Q3UZV7, F1QKQ1, and Q2HX28. The sequence lengths of these three proteins are 1028, 1356, and 1409, respectively. As shown in <xref rid="s0095" ref-type="sec">Table S3</xref>, NetGO 2.0 achieved better AUPRC on three proteins, which indicates that the truncated sequences in long proteins are important sources of information and are critical for predicting functions. This further confirms that NetGO 3.0 needs to be improved in handling long sequences, which will be important future research work.</p>
    </sec>
    <sec id="s0060">
      <title>Visualization of the predicted results</title>
      <p id="p0110">We presented more options to visualize the predicted GO terms to better illustrate prediction results. Compared with NetGO 2.0, the new web server offers a novel perspective to present the results, which can provide more relevant information about predicted GO terms. <xref rid="f0015" ref-type="fig">Figure 3</xref> showed the new result page of NetGO 3.0, which mainly includes three ways to visualize the prediction performance. Although GO terms in top layers usually achieve a higher score and rank higher, NetGO 3.0 clarifies the depth of predicted GO terms, which allows users to find specific GO terms in bottom layers. Note that the color in the result page and node size in <xref rid="f0015" ref-type="fig">Figure 3</xref>D are determined by the predicted confidence score, which can help users better understand the predicted results in an original view.<fig id="f0015"><label>Figure 3</label><caption><p><bold>Visualization of prediction results on the web server</bold></p><p><bold>A.</bold> Prediction result page of NetGO 3.0 website. “GO DAG”, “Bar plot”, and “Bubble plot” are the new interfaces to visualize the predicted GO terms. We also added a new column named “Depth” to show the depth of GO terms in GO analysis. <bold>B.</bold> The predicted GO terms and their DAGs. <bold>C.</bold> Bar plot showing the predicted GO terms and their confidence scores. <bold>D.</bold> Bubble plot showing the predicted GO terms and their depth in GO analysis. DAG, directed acyclic graph.</p></caption><graphic xlink:href="gr3"/></fig></p>
    </sec>
    <sec id="s0065">
      <title>Case study</title>
      <p id="p0115">Finally, we selected a specific protein as input and showed the results obtained by NetGO 3.0 and its competing methods. Ubiquitin-like protein 5 (UniProt ID: Q9FGZ9) is a difficult protein with low BLAST similarity to training proteins. <xref rid="s0095" ref-type="sec">Table S4</xref> showed the 18 GO terms in BP annotated to protein Q9FGZ9. <xref rid="f0020" ref-type="fig">Figure 4</xref> also depicted the directed acyclic graph (DAG) according to the relationship of 18 GO terms in GO. As shown in <xref rid="s0095" ref-type="sec">Table S4</xref>, BLAST-KNN failed to achieve a valid result because homology-based methods were not suitable for difficult protein function prediction. LR-InterPro and LR-ESM extracted features from raw amino acid sequences and obtained better results than BLAST-KNN. In the top 20 predicted GO terms, the number of true-positive samples achieved by LR-ESM was significantly larger than other methods, which predicted 14 correct function labels. NetGO and NetGO 2.0 predicted only six correct GO terms, which were not competitive compared to LR-ESM and NetGO 3.0. The reason for this phenomenon may be that the new component method, LR-ESM, is more robust for difficult proteins than other methods and is able to represent them more efficiently. With the support of the protein language model, NetGO 3.0 achieved 15 true GO terms out of 19 predicted ones, which successfully predicted the GO terms that NetGO and NetGO 2.0 failed to predict. <xref rid="f0020" ref-type="fig">Figure 4</xref> illustrated the hierarchy of correctly predicted GO terms, indicating that NetGO 3.0 is able to predict those GO terms with less information in the deeper layers. Overall, this typical example demonstrates that the high predictive performance of NetGO 3.0 is closely related to the protein language models.<fig id="f0020"><label>Figure 4</label><caption><p><bold>DAG of GO terms associated with Q9FGZ9 in BP</bold></p><p>Each GO term is attached with tags, which illustrates that the GO term is predicted correctly by corresponding methods.</p></caption><graphic xlink:href="gr4"/></fig></p>
    </sec>
  </sec>
  <sec id="s0070">
    <title>Conclusion</title>
    <p id="p0120">We have developed NetGO 3.0 to improve the performance of large-scale AFP by incorporating a new component LR-ESM, which utilizes a protein language model to generate powerful representations of proteins. Interesting future work would be integrating protein structural information into NetGO 3.0 to enhance the performance of AFP <xref rid="b0090" ref-type="bibr">[18]</xref>, <xref rid="b0095" ref-type="bibr">[19]</xref>, <xref rid="b0100" ref-type="bibr">[20]</xref>.</p>
  </sec>
  <sec sec-type="data-availability" id="s0075">
    <title>Data availability</title>
    <p id="p0125">The web server of NetGO 3.0 is freely accessible at <ext-link ext-link-type="uri" xlink:href="https://dmiip.sjtu.edu.cn/ng3.0" id="ir010">https://dmiip.sjtu.edu.cn/ng3.0</ext-link>.</p>
  </sec>
  <sec sec-type="COI-statement" id="s0080">
    <title>Competing interests</title>
    <p id="p0130">The authors have declared no competing interests.</p>
  </sec>
  <sec id="s0085">
    <title>CRediT authorship contribution statement</title>
    <p id="p0135"><bold>Shaojun Wang:</bold> Data curation, Software, Methodology, Writing – original draft. <bold>Ronghui You:</bold> Conceptualization, Methodology, Writing – review &amp; editing. <bold>Yunjia Liu:</bold> Methodology, Visualization, Writing – review &amp; editing. <bold>Yi Xiong:</bold> Resources, Writing – review &amp; editing. <bold>Shanfeng Zhu:</bold> Conceptualization, Resources, Methodology, Writing – review &amp; editing. All authors have read and approved the final manuscript.</p>
  </sec>
</body>
<back>
  <ref-list id="bi005">
    <title>References</title>
    <ref id="b0005">
      <label>1</label>
      <element-citation publication-type="journal" id="h0005">
        <person-group person-group-type="author">
          <name>
            <surname>Ashburner</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Ball</surname>
            <given-names>C.A.</given-names>
          </name>
          <name>
            <surname>Blake</surname>
            <given-names>J.A.</given-names>
          </name>
          <name>
            <surname>Botstein</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Butler</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Cherry</surname>
            <given-names>J.M.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Gene Ontology: tool for the unification of biology</article-title>
        <source>Nat Genet</source>
        <volume>25</volume>
        <year>2000</year>
        <fpage>25</fpage>
        <lpage>29</lpage>
        <pub-id pub-id-type="pmid">10802651</pub-id>
      </element-citation>
    </ref>
    <ref id="b0010">
      <label>2</label>
      <element-citation publication-type="journal" id="h0010">
        <person-group person-group-type="author">
          <collab>UniProt Consortium</collab>
        </person-group>
        <article-title>UniProt: the universal protein knowledgebase in 2021</article-title>
        <source>Nucleic Acids Res</source>
        <volume>49</volume>
        <year>2021</year>
        <fpage>D480</fpage>
        <lpage>D489</lpage>
        <pub-id pub-id-type="pmid">33237286</pub-id>
      </element-citation>
    </ref>
    <ref id="b0015">
      <label>3</label>
      <element-citation publication-type="journal" id="h0015">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Bergquist</surname>
            <given-names>T.R.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>A.J.</given-names>
          </name>
          <name>
            <surname>Kacsoh</surname>
            <given-names>B.Z.</given-names>
          </name>
          <name>
            <surname>Crocker</surname>
            <given-names>A.W.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The CAFA challenge reports improved protein function prediction and new functional annotations for hundreds of genes through experimental screens</article-title>
        <source>Genome Biol</source>
        <volume>20</volume>
        <year>2019</year>
        <fpage>244</fpage>
        <pub-id pub-id-type="pmid">31744546</pub-id>
      </element-citation>
    </ref>
    <ref id="b0020">
      <label>4</label>
      <element-citation publication-type="journal" id="h0020">
        <person-group person-group-type="author">
          <name>
            <surname>Piovesan</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Tosatto</surname>
            <given-names>S.C.</given-names>
          </name>
        </person-group>
        <article-title>INGA 2.0: improving protein function prediction for the dark proteome</article-title>
        <source>Nucleic Acids Res</source>
        <volume>47</volume>
        <year>2019</year>
        <fpage>W373</fpage>
        <lpage>W378</lpage>
        <pub-id pub-id-type="pmid">31073595</pub-id>
      </element-citation>
    </ref>
    <ref id="b0025">
      <label>5</label>
      <element-citation publication-type="journal" id="h0025">
        <person-group person-group-type="author">
          <name>
            <surname>Kulmanov</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Zhapa-Camacho</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Hoehndorf</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>DeepGOWeb: fast and accurate protein function prediction on the (Semantic) Web</article-title>
        <source>Nucleic Acids Res</source>
        <volume>49</volume>
        <year>2021</year>
        <fpage>W140</fpage>
        <lpage>W146</lpage>
        <pub-id pub-id-type="pmid">34019664</pub-id>
      </element-citation>
    </ref>
    <ref id="b0030">
      <label>6</label>
      <element-citation publication-type="journal" id="h0030">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Freddolino</surname>
            <given-names>P.L.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>MetaGO: predicting Gene Ontology of non-homologous proteins through low-resolution protein structure prediction and protein–protein network mapping</article-title>
        <source>J Mol Biol</source>
        <volume>430</volume>
        <year>2018</year>
        <fpage>2256</fpage>
        <lpage>2265</lpage>
        <pub-id pub-id-type="pmid">29534977</pub-id>
      </element-citation>
    </ref>
    <ref id="b0035">
      <label>7</label>
      <element-citation publication-type="journal" id="h0035">
        <person-group person-group-type="author">
          <name>
            <surname>Smaili</surname>
            <given-names>F.Z.</given-names>
          </name>
          <name>
            <surname>Tian</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Roy</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Alazmi</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Arold</surname>
            <given-names>S.T.</given-names>
          </name>
          <name>
            <surname>Mukherjee</surname>
            <given-names>S.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>QAUST: protein function prediction using structure similarity, protein interaction, and functional motifs</article-title>
        <source>Genomics Proteomics Bioinformatics</source>
        <volume>19</volume>
        <year>2021</year>
        <fpage>998</fpage>
        <lpage>1011</lpage>
        <pub-id pub-id-type="pmid">33631427</pub-id>
      </element-citation>
    </ref>
    <ref id="b0040">
      <label>8</label>
      <element-citation publication-type="journal" id="h0040">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>A short introduction to learning to rank</article-title>
        <source>IEICE Trans Inf Syst</source>
        <volume>94-D</volume>
        <year>2011</year>
        <fpage>1854</fpage>
        <lpage>1862</lpage>
      </element-citation>
    </ref>
    <ref id="b0045">
      <label>9</label>
      <element-citation publication-type="journal" id="h0045">
        <person-group person-group-type="author">
          <name>
            <surname>You</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Xiong</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Mamitsuka</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>GOLabeler: improving sequence-based large-scale protein function prediction by learning to rank</article-title>
        <source>Bioinformatics</source>
        <volume>34</volume>
        <year>2018</year>
        <fpage>2465</fpage>
        <lpage>2473</lpage>
        <pub-id pub-id-type="pmid">29522145</pub-id>
      </element-citation>
    </ref>
    <ref id="b0050">
      <label>10</label>
      <element-citation publication-type="journal" id="h0050">
        <person-group person-group-type="author">
          <name>
            <surname>You</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Yao</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Xiong</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Mamitsuka</surname>
            <given-names>H.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>NetGO: improving large-scale protein function prediction with massive network information</article-title>
        <source>Nucleic Acids Res</source>
        <volume>47</volume>
        <year>2019</year>
        <fpage>W379</fpage>
        <lpage>W387</lpage>
        <pub-id pub-id-type="pmid">31106361</pub-id>
      </element-citation>
    </ref>
    <ref id="b0055">
      <label>11</label>
      <element-citation publication-type="journal" id="h0055">
        <person-group person-group-type="author">
          <name>
            <surname>Yao</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>You</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Xiong</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>NetGO 2.0: improving large-scale protein function prediction with massive sequence, text, domain, family and network information</article-title>
        <source>Nucleic Acids Res</source>
        <volume>49</volume>
        <year>2021</year>
        <fpage>W469</fpage>
        <lpage>W475</lpage>
        <pub-id pub-id-type="pmid">34038555</pub-id>
      </element-citation>
    </ref>
    <ref id="b0060">
      <label>12</label>
      <mixed-citation publication-type="other" id="h0060">Devlin J, Chang M, Lee K, Toutanova K. BERT: pre-training of deep bidirectional transformers for language understanding. arXiv 2019;1810.04805.</mixed-citation>
    </ref>
    <ref id="b0065">
      <label>13</label>
      <element-citation publication-type="journal" id="h0065">
        <person-group person-group-type="author">
          <name>
            <surname>Rives</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Meier</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Sercu</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Goyal</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>J.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</article-title>
        <source>Proc Nat Acad Sci U S A</source>
        <volume>118:e2016239118</volume>
        <year>2021</year>
      </element-citation>
    </ref>
    <ref id="b0070">
      <label>14</label>
      <mixed-citation publication-type="other" id="h0070">Elnaggar A, Heinzinger M, Dallago C, Rehawi G, Wang Y, Jones L, et al. ProtTrans: towards cracking the language of life’s code through self-supervised deep learning and high performance computing. arXiv 2021;2007.06225.</mixed-citation>
    </ref>
    <ref id="b0075">
      <label>15</label>
      <element-citation publication-type="journal" id="h0075">
        <person-group person-group-type="author">
          <name>
            <surname>Alley</surname>
            <given-names>E.C.</given-names>
          </name>
          <name>
            <surname>Khimulya</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Biswas</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>AlQuraishi</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Church</surname>
            <given-names>G.M.</given-names>
          </name>
        </person-group>
        <article-title>Unified rational protein engineering with sequence-based deep representation learning</article-title>
        <source>Nat Methods</source>
        <volume>16</volume>
        <year>2019</year>
        <fpage>1315</fpage>
        <lpage>1322</lpage>
        <pub-id pub-id-type="pmid">31636460</pub-id>
      </element-citation>
    </ref>
    <ref id="b0080">
      <label>16</label>
      <element-citation publication-type="journal" id="h0080">
        <person-group person-group-type="author">
          <name>
            <surname>Strodthoff</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Wagner</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Wenzel</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Samek</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <article-title>UDSMProt: universal deep sequence models for protein classification</article-title>
        <source>Bioinformatics</source>
        <volume>36</volume>
        <year>2020</year>
        <fpage>2401</fpage>
        <lpage>2409</lpage>
        <pub-id pub-id-type="pmid">31913448</pub-id>
      </element-citation>
    </ref>
    <ref id="b0085">
      <label>17</label>
      <element-citation publication-type="journal" id="h0085">
        <person-group person-group-type="author">
          <name>
            <surname>Villegas-Morcillo</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Makrodimitris</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>van Ham</surname>
            <given-names>R.C.H.J.</given-names>
          </name>
          <name>
            <surname>Gomez</surname>
            <given-names>A.M.</given-names>
          </name>
          <name>
            <surname>Sanchez</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Reinders</surname>
            <given-names>M.J.T.</given-names>
          </name>
        </person-group>
        <article-title>Unsupervised protein embeddings outperform hand-crafted sequence and structure features at predicting molecular function</article-title>
        <source>Bioinformatics</source>
        <volume>37</volume>
        <year>2021</year>
        <fpage>162</fpage>
        <lpage>170</lpage>
        <pub-id pub-id-type="pmid">32797179</pub-id>
      </element-citation>
    </ref>
    <ref id="b0090">
      <label>18</label>
      <element-citation publication-type="journal" id="h0090">
        <person-group person-group-type="author">
          <name>
            <surname>Lai</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Accurate protein function prediction via graph attention networks with predicted structure information</article-title>
        <source>Brief Bioinform</source>
        <volume>23:bbab502</volume>
        <year>2022</year>
      </element-citation>
    </ref>
    <ref id="b0095">
      <label>19</label>
      <element-citation publication-type="journal" id="h0095">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Freddolino</surname>
            <given-names>P.L.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>COFACTOR: improved protein function prediction by combining structure, sequence and protein–protein interaction information</article-title>
        <source>Nucleic Acids Res</source>
        <volume>45</volume>
        <year>2017</year>
        <fpage>W291</fpage>
        <lpage>W299</lpage>
        <pub-id pub-id-type="pmid">28472402</pub-id>
      </element-citation>
    </ref>
    <ref id="b0100">
      <label>20</label>
      <element-citation publication-type="journal" id="h0100">
        <person-group person-group-type="author">
          <name>
            <surname>Gligorijević</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Renfrew</surname>
            <given-names>P.D.</given-names>
          </name>
          <name>
            <surname>Kosciolek</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Leman</surname>
            <given-names>J.K.</given-names>
          </name>
          <name>
            <surname>Berenberg</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Vatanen</surname>
            <given-names>T.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Structure-based protein function prediction using graph convolutional networks</article-title>
        <source>Nat Commun</source>
        <volume>12</volume>
        <year>2021</year>
        <fpage>3158</fpage>
        <pub-id pub-id-type="pmid">34039974</pub-id>
      </element-citation>
    </ref>
    <ref id="b0105">
      <label>21</label>
      <element-citation publication-type="journal" id="h0105">
        <person-group person-group-type="author">
          <name>
            <surname>Rao</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Bhattacharya</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Thomas</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Duan</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Canny</surname>
            <given-names>J.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Evaluating protein transfer learning with TAPE</article-title>
        <source>Adv Neural Inf Process Syst</source>
        <volume>32</volume>
        <year>2019</year>
        <fpage>9689</fpage>
        <lpage>9701</lpage>
        <pub-id pub-id-type="pmid">33390682</pub-id>
      </element-citation>
    </ref>
    <ref id="b0110">
      <label>22</label>
      <element-citation publication-type="journal" id="h0110">
        <person-group person-group-type="author">
          <name>
            <surname>Bepler</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Berger</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>Learning the protein language: evolution, structure, and function</article-title>
        <source>Cell Syst</source>
        <volume>12</volume>
        <year>2021</year>
        <fpage>654</fpage>
        <lpage>669</lpage>
        <pub-id pub-id-type="pmid">34139171</pub-id>
      </element-citation>
    </ref>
    <ref id="b0115">
      <label>23</label>
      <element-citation publication-type="journal" id="h0115">
        <person-group person-group-type="author">
          <name>
            <surname>Huntley</surname>
            <given-names>R.P.</given-names>
          </name>
          <name>
            <surname>Sawford</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Mutowo-Meullenet</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Shypitsyna</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Bonilla</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>M.J.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The GOA database: Gene Ontology Annotation updates for 2015</article-title>
        <source>Nucleic Acids Res</source>
        <volume>43</volume>
        <year>2015</year>
        <fpage>D1057</fpage>
        <lpage>D1063</lpage>
        <pub-id pub-id-type="pmid">25378336</pub-id>
      </element-citation>
    </ref>
  </ref-list>
  <sec id="s0095" sec-type="supplementary-material">
    <title>Supplementary material</title>
    <p id="p0150">The following are the Supplementary material to this article:<supplementary-material content-type="local-data" id="m0035"><caption><title>Supplementary File S1</title><p><bold>Supplementary information about evaluation metrics, data collection, and performance analysis of different competing models</bold></p></caption><media xlink:href="mmc1.docx"/></supplementary-material></p>
    <p id="p0155">
      <supplementary-material content-type="local-data" id="m0030">
        <caption>
          <title>Supplementary Figure S1</title>
          <p><bold>The performance over M-AUPRC in CC</bold> There are 31 GO terms in the second and third layers of the CC domain that are annotated with more than ten proteins in the test set. By checking the performance improvement of NetGO 3.0 over 2.0 on each term, the GO terms that have more than a 5% increase or decrease are shown in red or green, respectively.</p>
        </caption>
        <media xlink:href="mmc2.docx"/>
      </supplementary-material>
    </p>
    <p id="p0160">
      <supplementary-material content-type="local-data" id="m0025">
        <caption>
          <title>Supplementary Table S1</title>
          <p>
            <bold>Summary of the benchmark dataset</bold>
          </p>
        </caption>
        <media xlink:href="mmc3.docx"/>
      </supplementary-material>
    </p>
    <p id="p0165">
      <supplementary-material content-type="local-data" id="m0020">
        <caption>
          <title>Supplementary Table S2</title>
          <p>
            <bold>Summary of the new dataset</bold>
          </p>
        </caption>
        <media xlink:href="mmc4.docx"/>
      </supplementary-material>
    </p>
    <p id="p0170">
      <supplementary-material content-type="local-data" id="m0015">
        <caption>
          <title>Supplementary Table S3</title>
          <p>
            <bold>Performance on three long proteins in BP</bold>
          </p>
        </caption>
        <media xlink:href="mmc5.docx"/>
      </supplementary-material>
    </p>
    <p id="p0175">
      <supplementary-material content-type="local-data" id="m0010">
        <caption>
          <title>Supplementary Table S4</title>
          <p>
            <bold>Predicted GO terms of Q9FGZ9 in BP by NetGO 3.0 and competing methods</bold>
          </p>
        </caption>
        <media xlink:href="mmc6.docx"/>
      </supplementary-material>
    </p>
    <p id="p0180">
      <supplementary-material content-type="local-data" id="m0005">
        <caption>
          <title>Supplementary Table S5</title>
          <p>
            <bold>Performance of different competing models</bold>
          </p>
        </caption>
        <media xlink:href="mmc7.docx"/>
      </supplementary-material>
    </p>
  </sec>
  <ack id="ak005">
    <title>Acknowledgments</title>
    <p id="p0140">Shanfeng Zhu has been supported by the National Natural Science Foundation of China (Grant Nos. 61872094 and 62272105), the Shanghai Municipal Science and Technology Major Project (Grant No. 2018SHZDZX01), the ZJ Lab, and the Shanghai Research Center for Brain Science and Brain-Inspired Intelligence Technology. Shaojun Wang and Ronghui You have been supported by the 111 Project (Grant No. B18015), the Shanghai Municipal Science and Technology Major Project (Grant No. 2017SHZDZX01), and the Information Technology Facility, CAS-MPG Partner Institute for Computational Biology, Shanghai Institute for Biological Sciences, Chinese Academy of Sciences. Yi Xiong has been supported by the National Natural Science Foundation of China (Grant Nos. 61832019 and 62172274). This work is supported by Beijing Academy of Artificial Intelligence (BAAI). We are thankful to Prof. Xiaodi Huang and Ms. Sarah Replogle for English proofreading.</p>
  </ack>
  <fn-group>
    <fn id="d35e1775">
      <p id="np005">Peer review under responsibility of Beijing Institute of Genomics, Chinese Academy of Sciences / China National Center for Bioinformation and Genetics Society of China.</p>
    </fn>
    <fn id="s0090" fn-type="supplementary-material">
      <p id="p0145">Supplementary data to this article can be found online at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.gpb.2023.04.001" id="ir015">https://doi.org/10.1016/j.gpb.2023.04.001</ext-link>.</p>
    </fn>
  </fn-group>
</back>
