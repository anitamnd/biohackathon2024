<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7703753</article-id>
    <article-id pub-id-type="pmid">31702773</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btz796</article-id>
    <article-id pub-id-type="publisher-id">btz796</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Papers</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Gene Expression</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Model selection for metabolomics: predicting diagnosis of coronary artery disease using automated machine learning</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Orlenko</surname>
          <given-names>Alena</given-names>
        </name>
        <aff><institution>Department of Biostatistics, Epidemiology and Informatics, Institute for Biomedical Informatics, University of Pennsylvania</institution>, Philadelphia, PA, <country country="US">USA</country></aff>
        <xref rid="btz796-FM1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kofink</surname>
          <given-names>Daniel</given-names>
        </name>
        <aff><institution>Department of Cardiology, Division Heart and Lungs</institution>, Utrecht, The <country country="NL">Netherlands</country></aff>
        <xref rid="btz796-FM1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lyytikäinen</surname>
          <given-names>Leo-Pekka</given-names>
        </name>
        <aff><institution>Department of Cardiology, Division Heart and Lungs</institution>, Utrecht, The <country country="NL">Netherlands</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Nikus</surname>
          <given-names>Kjell</given-names>
        </name>
        <aff><institution>Department of Clinical Chemistry, Fimlab Laboratories</institution>, Tampere, <country country="FI">Finland</country></aff>
        <aff><institution>Department of Cardiology, Tampere University Hospital</institution>, Tampere, <country country="FI">Finland</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Mishra</surname>
          <given-names>Pashupati</given-names>
        </name>
        <aff><institution>Department of Clinical Chemistry, Fimlab Laboratories</institution>, Tampere, <country country="FI">Finland</country></aff>
        <aff><institution>Department of Cardiology, Tampere University Hospital</institution>, Tampere, <country country="FI">Finland</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kuukasjärvi</surname>
          <given-names>Pekka</given-names>
        </name>
        <aff><institution>Department of Cardio-Thoracic Surgery, Heart Center, Tampere University Hospital</institution>, Tampere, <country country="FI">Finland</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Karhunen</surname>
          <given-names>Pekka J</given-names>
        </name>
        <aff><institution>Department of Forensic Medicine, Fimlab Laboratories</institution>, Tampere, <country country="FI">Finland</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kähönen</surname>
          <given-names>Mika</given-names>
        </name>
        <aff><institution>Department of Clinical Physiology, Tampere University Hospital</institution>, Tampere, <country country="FI">Finland</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-7253-6048</contrib-id>
        <name>
          <surname>Laurikka</surname>
          <given-names>Jari O</given-names>
        </name>
        <aff><institution>Department of Cardio-Thoracic Surgery, Heart Center, Tampere University Hospital</institution>, Tampere, <country country="FI">Finland</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lehtimäki</surname>
          <given-names>Terho</given-names>
        </name>
        <aff><institution>Department of Clinical Chemistry, Fimlab Laboratories</institution>, Tampere, <country country="FI">Finland</country></aff>
        <aff><institution>Department of Cardiology, Tampere University Hospital</institution>, Tampere, <country country="FI">Finland</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Asselbergs</surname>
          <given-names>Folkert W</given-names>
        </name>
        <aff><institution>Department of Cardiology, Division Heart and Lungs</institution>, Utrecht, The <country country="NL">Netherlands</country></aff>
        <aff><institution>Health Data Research UK London, Institute for Health Informatics, University College London</institution>, London, <country country="GB">UK</country></aff>
        <aff><institution>Institute of Cardiovascular Science, Faculty of Population Health Sciences, University College London</institution>, London, <country country="GB">UK</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Moore</surname>
          <given-names>Jason H</given-names>
        </name>
        <aff><institution>Department of Biostatistics, Epidemiology and Informatics, Institute for Biomedical Informatics, University of Pennsylvania</institution>, Philadelphia, PA, <country country="US">USA</country></aff>
        <xref rid="btz796-cor1" ref-type="corresp"/>
        <!--jhmoore@upenn.edu-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Kelso.</surname>
          <given-names>Janet</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btz796-cor1">
        <email>jhmoore@upenn.edu</email>
      </corresp>
      <fn id="btz796-FM1">
        <p>The authors wish it to be known that, in their opinion, Alena Orlenko and Daniel Kofink should be regarded as Joint First Authors.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="collection">
      <day>15</day>
      <month>3</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2019-11-08">
      <day>08</day>
      <month>11</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>08</day>
      <month>11</month>
      <year>2019</year>
    </pub-date>
    <volume>36</volume>
    <issue>6</issue>
    <fpage>1772</fpage>
    <lpage>1778</lpage>
    <history>
      <date date-type="received">
        <day>18</day>
        <month>3</month>
        <year>2019</year>
      </date>
      <date date-type="rev-recd">
        <day>03</day>
        <month>10</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>30</day>
        <month>10</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2019. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2019</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btz796.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Selecting the optimal machine learning (ML) model for a given dataset is often challenging. Automated ML (AutoML) has emerged as a powerful tool for enabling the automatic selection of ML methods and parameter settings for the prediction of biomedical endpoints. Here, we apply the tree-based pipeline optimization tool (TPOT) to predict angiographic diagnoses of coronary artery disease (CAD). With TPOT, ML models are represented as expression trees and optimal pipelines discovered using a stochastic search method called genetic programing. We provide some guidelines for TPOT-based ML pipeline selection and optimization-based on various clinical phenotypes and high-throughput metabolic profiles in the Angiography and Genes Study (ANGES).</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We analyzed nuclear magnetic resonance-derived lipoprotein and metabolite profiles in the ANGES cohort with a goal to identify the role of non-obstructive CAD patients in CAD diagnostics. We performed a comparative analysis of TPOT-generated ML pipelines with selected ML classifiers, optimized with a grid search approach, applied to two phenotypic CAD profiles. As a result, TPOT-generated ML pipelines that outperformed grid search optimized models across multiple performance metrics including balanced accuracy and area under the precision-recall curve. With the selected models, we demonstrated that the phenotypic profile that distinguishes non-obstructive CAD patients from no CAD patients is associated with higher precision, suggesting a discrepancy in the underlying processes between these phenotypes.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>TPOT is freely available via <ext-link xlink:href="http://epistasislab.github.io/tpot/" ext-link-type="uri">http://epistasislab.github.io/tpot/</ext-link>.</p>
      </sec>
      <sec id="s4">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Institutes of Health</institution>
            <institution-id institution-id-type="DOI">10.13039/100000002</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>R01 LM010098</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>NIH</institution>
            <institution-id institution-id-type="DOI">10.13039/100000002</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="7"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Although predictive analysis in biomedical research is typically based on deriving quantitative measures of confidence through the creation and fitting of a hypothesis-specific probability model, machine learning (ML)-based algorithms offers a wide range of different techniques that focus on prediction, through pattern recognition learning, with minimal underlying assumptions about the features. ML is especially effective when features are involved in nonlinear interactions or when no strong scientific hypothesis about feature interactions is established. In clinical research, ML-derived predictive models could facilitate preliminary hypothesis generation as well as be used for biomarker discovery through the selection of informative features. The choice of the most appropriate ML algorithm is a challenging process. Indeed, dozens of ML algorithms have been implemented in various languages: e.g. Scikit-learn library for Python (<xref rid="btz796-B18" ref-type="bibr">Pedregosa <italic toggle="yes">et al.</italic>, 2011</xref>), Weka software for Java (<xref rid="btz796-B24" ref-type="bibr">Witten <italic toggle="yes">et al.</italic>, 2016</xref>) and the caret package for R (<xref rid="btz796-B12" ref-type="bibr">Kuhn <italic toggle="yes">et al.</italic>, 2008</xref>). Each classification or regression ML algorithm contains numerous parameters that need to be selected and optimized. A common approach is to perform an exhaustive search over the selected algorithm parameter set (<xref rid="btz796-B3" ref-type="bibr">Bzdok <italic toggle="yes">et al.</italic>, 2018</xref>). Additionally, uncertainty in ML model selection comes from the number of various pre-processing algorithms such as, feature selectors (group of computational algorithms providing a reduction in the feature list according to a select statistical scoring metrics e.g. variance, f-value, χ<sup>2</sup> etc.) and feature transformers [group of computational algorithms which provides transformation of the dataset with feature pre-processing (such as standardization and normalization), reduction of dimensionality of the feature set, or generation of new feature(s) from existing ones] that might be needed to enrich the data for signal. Together with ML model hyperparameter tuning, this creates numerous possible combinations to be validated on one particular dataset of interest. Automated ML (AutoML) seeks to take the guesswork out of this process by treating ML algorithms and pre-processing methods as building blocks for pipelines that are constructed and evaluated using a search algorithm.</p>
    <p>AutoML methods to date have employed multiple optimization techniques: ML algorithm hyperparameter tuning implemented in the mlr R package (<xref rid="btz796-B2" ref-type="bibr">Bischl <italic toggle="yes">et al.</italic>, 2016</xref>), full pipeline Bayesian hyperparameter optimization used in Auto-WEKA (<xref rid="btz796-B23" ref-type="bibr">Thornton <italic toggle="yes">et al.</italic>, 2013</xref>) and auto-sklearn (<xref rid="btz796-B7" ref-type="bibr">Feurer <italic toggle="yes">et al.</italic>, 2015</xref>), Bayesian optimization of pipeline operators including the choice of imputer (group of algorithms providing a replacement of missing data with substituted values), selected feature transformers, ML model and calibrator is available via AutoPrognosis (<xref rid="btz796-B1" ref-type="bibr">Alaa and van der Schaar, 2018</xref>). Our specific AutoML method of interest is the tree-based pipeline optimization tool (TPOT) that takes a higher-level approach to the optimization process by using genetic programing to find optimal ML pipelines. TPOT has been observed to automatically generate ML pipelines that match or exceed the performance of traditionally tuned supervised ML algorithms (<xref rid="btz796-B15" ref-type="bibr">Olson and Moore, 2016</xref>). In clinical applications (<xref rid="btz796-B17" ref-type="bibr">Orlenko <italic toggle="yes">et al.</italic>, 2018</xref>), TPOT has delivered promising predictive performance while remaining robust to mixed datatypes and large feature spaces, containing clinical, demographic and biomarker data. Here we use TPOT to predict angiographic diagnosis of coronary artery disease (CAD) in the Angiography and Genes Study (ANGES) using metabolomics data. In addition, we provide a guideline for TPOT-based ML pipeline selection based on various clinical phenotypes and high-throughput metabolic profiles.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 TPOT overview</title>
      <p>Here we used TPOT as our AutoML method to generate optimized ML pipelines for the ANGES dataset. Briefly, TPOT employs genetic programing (<xref rid="btz796-B11" ref-type="bibr">Koza, 1992</xref>) from the Python package DEAP (<xref rid="btz796-B8" ref-type="bibr">Fortin <italic toggle="yes">et al.</italic>, 2012</xref>) to select series of data pre-processing functions and ML classification or regression algorithms that aim to maximize the performance of the model for a dataset of interest. In addition to ML algorithm, TPOT model pipeline (<xref rid="btz796-F1" ref-type="fig">Fig. 1</xref>) may contain a diverse combinations of data transformers implemented in Scikit-learn Python library such as various types of pre-processors [Standard Scaler (SS), Min Max Scaler, Max Abs Scaler, Binarizer, Normalizer, polynomial features expansion] as well as various feature selectors [Variance Threshold, Select Percentile (SP), recursive feature elimination (RFE) etc.]. In certain cases, constructing a new feature set can be useful for extracting important information (e.g. when a selected method analyzes one feature at a time while complex feature interactions are present in the dataset). TPOT also contains several custom feature constructor implementations: zero counts (count of zero/non-zeros per sample), stacking estimator (SE) (generates predictions and class probabilities with a classifier of choice as new features), one hot encoder (transforms categorical feature into binary features) and a selection of sklearn transformer implementations: PCA, independent component analysis, non-linear transformations through kernel approximation (Nystroem, RBF Sampler). The TPOT full configuration consists of 11 classification algorithms, 14 feature transformers and 5 feature selectors (for the full list please refer to the TPOT website). To combine all of these operators, TPOT employs a tree-based structure (<xref rid="btz796-F1" ref-type="fig">Fig. 1 and</xref>  <xref rid="sup1" ref-type="supplementary-material"> SupplementaryFig. S1</xref>): every pipeline starts with one or more copies of the entire dataset at the beginning of the tree structure and proceeds with feature transformation/selection operators described above or ML algorithm. Operators then modify the original dataset and it is further passed along the tree to the next operator or in the case if there are multiple copies of the dataset it may be combined into a single set via a combination operator.</p>
      <fig position="float" id="btz796-F1">
        <label>Fig. 1.</label>
        <caption>
          <p>Example of the TPOT pipeline</p>
        </caption>
        <graphic xlink:href="btz796f1" position="float"/>
      </fig>
      <p>The automatic optimization process via genetic programing begins with the initialization of a population of pipelines by randomly generating a fixed number of tree-based pipelines that is further subjected to the evolutionary algorithm through rounds (generations) of mutation, recombination of pipeline components and selection. The fitness of the pipeline is calculated via Pareto multi-objective function that aims to maximize ML algorithm’s performance metrics of choice (accuracy, precision, recall, f1 score, <italic toggle="yes">r</italic><sup>2</sup>, mean squared error etc.) while minimizing the pipeline’s complexity (i.e. the number of data transformers/selectors in the pipeline). There are several sources of variation that changes the structure of a TPOT pipeline and facilitates the selection of the most fit for a given dataset: crossover and mutation. First, one-point crossover applied to a user-specified percentage of pipelines where two randomly selected pipelines split at random point in the tree and their contents exchanged. Subsequently, mutation is applied at a fixed user-defined frequency with changes in the form of addition, removal or substitution of the pipeline operators. Once the changes have been introduced, and their fitness effect calculated, the TPOT pipeline with the highest fitness from the current generation is selected to replace 10% of the population in the next generation. The remaining 90% of the population is selected via three-way tournament with two-way parsimony: three pipelines selected for the tournament where at first the lowest fitness one is removed and then the least complexed of the remaining two is selected to be reproduced in the next generation. Population size, generation number and mutation rate parameters could be defined by the user. Here we will be comparing TPOT-based model optimization to the more traditional exhaustive tuning of the selected ML algorithms parameters.</p>
    </sec>
    <sec>
      <title>2.2 TPOT model selection for CAD phenotype in ANGES</title>
      <sec>
        <title>2.2.1 Study population</title>
        <p>This study is based on data from the ANGES, which enrolled 1000 patients referred to coronary angiography at Tampere University Hospital (Finland) between September 2002 and July 2005 (for details see <xref rid="btz796-B13" ref-type="bibr">Mennander <italic toggle="yes">et al.</italic>, 2008</xref>). This study included 925 patients from whom coronary angiography results and serum samples for metabolic profiling were available. Patients were categorized into three groups according to the angiographic findings. Obstructive or functionally relevant CAD was defined as stenosis of ≥50% stenosis of any major coronary artery (left anterior descending, left circumflex or right coronary artery). Less than 50% coronary artery stenosis was categorized as a non-obstructive CAD. Patients were considered to have no CAD if no major coronary artery showed any sign of stenosis. ANGES approved by the Ethics Committee of Tampere University Hospital. All patients gave written informed consent, and the study conforms to the Declaration of Helsinki.</p>
      </sec>
      <sec>
        <title>2.2.2 Metabolic profiling</title>
        <p>Fasting EDTA serum samples were stored at −80°C prior to analysis. A high-throughput <sup>1</sup>H NMR metabolomics platform (<xref rid="btz796-B22" ref-type="bibr">Soininen <italic toggle="yes">et al.</italic>, 2015</xref>) was used to quantify 73 lipid and metabolic measures: 56 lipid-related measures (including concentrations of 14 lipoprotein subclasses), 8 amino acids, 4 glycolysis-related metabolites and 5 other metabolites.</p>
      </sec>
      <sec>
        <title>2.2.3 Study design</title>
        <p>In addition to 73 metabolic features, we included 27 demographic and clinical features. Baseline characteristics of the study population are listed in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref>. We processed the missing values with KNN imputation strategy using fancyimpute python package (<xref rid="btz796-B20" ref-type="bibr">Rubinsteyn <italic toggle="yes">et al.</italic>, 2016</xref>). We used TPOT in classification mode as our datasets of interest have binary phenotypes. In clinics, non-obstructive is distinguished from obstructive CAD based on the extent of the coronary stenosis to identify patients requiring revascularization, even though both CAD phenotypes represent the same pathophysiological pathway. We, therefore, performed comparisons for two different profiles: no CAD versus non-obstructive and obstructive CAD (P1) and no CAD and non-obstructive CAD versus obstructive CAD (P2). We split the datasets into training (75%) and validation (25%) sets and run all optimization approaches on the training set. We reported both training and validation sets scores in the result section.</p>
        <p>TPOT was set to run for 1000 generations or 24 h (whichever happens first) with the population size of 1000 pipelines. For each phenotypic profile TPOT-based model selection was performed with different configuration: full configuration with complete list of data operators and ML classification models (Model 1), reduced configuration with logistic regression (LR) classifier and complete list of data transformers and selectors (Model 2), reduced configuration with decision tree (DT) classifier and complete list of data transformers and selectors (Model 3), reduced configuration with random forest (RF) classifier and complete list of data transformers and selectors (Model 4). The performance of each pipeline was estimated with 10-fold cross-validation and balanced accuracy. Balanced accuracy is a metric used in imbalanced datasets in order to avoid inflated performance estimates. It evaluates the accuracy of each class and then computes an unweighted average of each class accuracy. In the case of the balanced dataset it will be equal to the accuracy score (<xref rid="btz796-B14" ref-type="bibr">Mosley, 2010</xref>). Since evolutionary computation is a stochastic process, each configuration was run 50 times with different random seeds and the pipeline with the highest balanced accuracy score was selected as the representative model. This optimization strategy has been chosen empirically. We usually recommend running TPOT for 1000 generations which can take significant amount of time in cases of a large datasets. We propose using multiple random seeds in order to compensate for any potential effects from the initialization step.</p>
        <p>We further compared TPOT-based model selection to the exhaustive grid search parameter tuning of LR classifier (Model 5), DT classifier (Model 6) and RF Classifier (Model 7). The choice of tree-based classifiers for evaluation, in addition to LR, is justified by the results of the previous benchmarking studies where tree-based methods reported superior performance in comparison to the other ML classifiers (<xref rid="btz796-B6" ref-type="bibr">Fernández-Delgado <italic toggle="yes">et al.</italic>, 2014</xref>; <xref rid="btz796-B16" ref-type="bibr">Olson <italic toggle="yes">et al.</italic>, 2018</xref>) Additionally, the grid search optimization method was applied to the ML classifiers from the TPOT-generated pipelines that reported the highest performance that were combined into a pipeline with a subset of data pre-processors. Since the penalty parameter depends on the scale of each feature, we standard-scaled (z-transformed) all features before training linear regression models with and without feature selection. We also applied standard scaling before training naïve Bayes classifiers for multivariate Bernoulli models. The naïve Bayes classifier for Bernoulli models implemented in sklearn performs binarization of the continuous features with a default threshold of 0.0. Prior standard scaling thus results in mean binarization of continuous features. We have reported different performance metrics (balanced accuracy, area under the curve (AUC), Precision-Recall curve (PRC), Precision, Recall) for all ML pipelines along with model complexity (the number of data transformation steps).</p>
        <p>The predictive power of the clinical and metabolic features was reported for the best pipelines for both phenotypic profiles. To calculate the coefficients of the predictive ability of the specific feature we used permutation feature importance (PFI) approach. Within this approach we first calculated a pipeline performance (balanced accuracy) on the unchanged dataset, and then permuted the values within a feature and calculate the performance of the pipeline on the modified dataset. The resulting difference in performances is the PFI score. This procedure was repeated 100 times for each feature and the mean taken as a final PFI score.</p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 Model selection with TPOT</title>
      <p><xref rid="btz796-T1" ref-type="table">Table 1(A)</xref> outlines the summary of the comparative analysis of model selection from the TPOT optimization process and grid search parameter tuning for P1 phenotype. As a result of TPOT optimization with full configuration pipeline Model A1 has been selected with the validation set balanced accuracy 0.77. It contains four pre-processing operators (RFE, SE with LR Classifier, SE with Multinomial Naïve Bayes Classifier, SS and Bernoulli Naïve Bayes (BNB) as ML classifier). TPOT optimization with reduced configuration of LR-only classifier (Model A2) has produced a pipeline with eight pre-processors (SE with LR Classifier, Zero Counts, Select From Model with Extra Trees Classifier, Binarizer, Normalizer and SS) and validation set balanced accuracy 0.76. Model A3 is the result of TPOT reduced configuration with DT Classifier only had a pipeline with six pre-processors [Binarizer, Select Few, SE with DT Classifier<sup>1</sup>, Normalizer<sup>1</sup>, SE with DT Classifier<sup>2</sup> and Normalizer<sup>2</sup> (here and further abbreviation<sup>1</sup> and <sup>2</sup>identify that operator was used twice within the pipeline but with a different hyperparameters setup)] with the validation set balanced accuracy 0.70. Model A4 has been selected during TPOT RF Classifier only optimization and had three pre-processors (SE with RF Classifier and Normalizer) along with grid search parameter tuning for LR, DT and RF classifiers (Models A5–A7 correspondingly) reported noticeably lower validation set balanced accuracy performance (balanced accuracy 0.61–0.69). Overall, the first two models have reported very similar performances in comparison to the remaining models for the P1; however, ultimately the best model appears to be Model A1 selected by the TPOT optimization with full configuration.</p>
      <table-wrap position="float" id="btz796-T1">
        <label>Table 1.</label>
        <caption>
          <p>Comparative analysis of the TPOT optimization of selected model with various performance metrics for P1(A) and P2(B).</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Model</th>
              <th rowspan="1" colspan="1">Balanced accuracy V/T</th>
              <th rowspan="1" colspan="1">Precision V/T</th>
              <th rowspan="1" colspan="1">Recall V/T</th>
              <th rowspan="1" colspan="1">ROC AUC V/T</th>
              <th rowspan="1" colspan="1">PRC V/T</th>
              <th rowspan="1" colspan="1">Pipeline complexity</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td colspan="7" style="#F2F2F2" rowspan="1">A. P1</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"> A1. TPOT (BNB)</td>
              <td rowspan="1" colspan="1">
                <bold>0.77/0.79</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.91/0.93</bold>
              </td>
              <td rowspan="1" colspan="1">0.77/0.79</td>
              <td rowspan="1" colspan="1">
                <bold>0.77/0.86</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.88/0.95</bold>
              </td>
              <td rowspan="1" colspan="1">5</td>
            </tr>
            <tr>
              <td style="#F2F2F2" rowspan="1" colspan="1"> A2. LR TPOT</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.76/0.79</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.90/0.93</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.79/0.80</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.76/0.86</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.87/0.95</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">9</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"> A3. DT TPOT</td>
              <td rowspan="1" colspan="1">0.70/0.74</td>
              <td rowspan="1" colspan="1">0.88/0.89</td>
              <td rowspan="1" colspan="1">0.71/0.80</td>
              <td rowspan="1" colspan="1">0.70/0.74</td>
              <td rowspan="1" colspan="1">0.85/0.87</td>
              <td rowspan="1" colspan="1">7</td>
            </tr>
            <tr>
              <td style="#F2F2F2" rowspan="1" colspan="1"> A4. RF TPOT</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.69 /0.69</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.85/0.86</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.85/0.88</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.61/0.81</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.81/0.94</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">4</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"> A5. LR GS</td>
              <td rowspan="1" colspan="1">0.68/0.72</td>
              <td rowspan="1" colspan="1">0.85/0.87</td>
              <td rowspan="1" colspan="1">0.85/0.90</td>
              <td rowspan="1" colspan="1">0.68/0.87</td>
              <td rowspan="1" colspan="1">0.83/0.95</td>
              <td rowspan="1" colspan="1">1</td>
            </tr>
            <tr>
              <td style="#F2F2F2" rowspan="1" colspan="1"> A6. DT GS</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.61/0.67</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.81/0.83</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.86/0.84</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.61/0.72</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.81/0.87</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">1</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"> A7. RF GS</td>
              <td rowspan="1" colspan="1">0.61/0.66</td>
              <td rowspan="1" colspan="1">0.81/0.84</td>
              <td rowspan="1" colspan="1">
                <bold>0.87/0.88</bold>
              </td>
              <td rowspan="1" colspan="1">0.64/0.81</td>
              <td rowspan="1" colspan="1">0.82/0.93</td>
              <td rowspan="1" colspan="1">1</td>
            </tr>
            <tr>
              <td colspan="7" style="#F2F2F2" rowspan="1">B. P2</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"> B1. TPOT (BNB)</td>
              <td rowspan="1" colspan="1">
                <bold>0.78/0.78</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.82/0.84</bold>
              </td>
              <td rowspan="1" colspan="1">0.79/0.79</td>
              <td rowspan="1" colspan="1">
                <bold>0.78/0.82</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.78/0.86</bold>
              </td>
              <td rowspan="1" colspan="1">5</td>
            </tr>
            <tr>
              <td style="#F2F2F2" rowspan="1" colspan="1"> B2. LR TPOT</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.77/0.75</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.80/0.80</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.84/0.80</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.77/0.84</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.76/0.90</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">5</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"> B3. DT TPOT</td>
              <td rowspan="1" colspan="1">0.75/0.76</td>
              <td rowspan="1" colspan="1">0.78/0.81</td>
              <td rowspan="1" colspan="1">0.84/0.81</td>
              <td rowspan="1" colspan="1">0.72/0.80</td>
              <td rowspan="1" colspan="1">0.73/0.84</td>
              <td rowspan="1" colspan="1">6</td>
            </tr>
            <tr>
              <td style="#F2F2F2" rowspan="1" colspan="1"> B4. RF TPOT</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.76/0.76</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.78/0.81</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">
                <bold>0.86/0.83</bold>
              </td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.75/0.83</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.74/0.88</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">4</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"> B5. LR GS</td>
              <td rowspan="1" colspan="1">0.73/0.74</td>
              <td rowspan="1" colspan="1">0.76/0.78</td>
              <td rowspan="1" colspan="1">0.84/0.84</td>
              <td rowspan="1" colspan="1">0.73/0.85</td>
              <td rowspan="1" colspan="1">0.73/0.89</td>
              <td rowspan="1" colspan="1">1</td>
            </tr>
            <tr>
              <td style="#F2F2F2" rowspan="1" colspan="1"> B6. DT GS</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.74/0.73</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.78/0.80</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.81/0.76</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.74/0.78</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.74/0.83</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">1</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"> B7. RF GS</td>
              <td rowspan="1" colspan="1">0.72/0.72</td>
              <td rowspan="1" colspan="1">0.77/0.78</td>
              <td rowspan="1" colspan="1">0.76/0.81</td>
              <td rowspan="1" colspan="1">0.69/0.83</td>
              <td rowspan="1" colspan="1">0.70/0.88</td>
              <td rowspan="1" colspan="1">1</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <p><bold><italic toggle="yes">Note</italic></bold>: Metrics’ score are shown for validation (V) and training (T) set. The highest score in each metrics category is marked via bold font. The best model for each phenotypic profile was selected according to the highest balanced accuracy. BNB, Bernoulli Naïve Bayes classifier; LR, logistic regression classifier; DT, decision tree classifier; RF, random forest classifier; GS, grid search optimization.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>Similarly, we have summarized models selected for P2 phenotypes via <xref rid="btz796-T1" ref-type="table">Table 1(B)</xref>. TPOT with the full configuration produced pipeline with four pre-processors (Variance Threshold, RFE, SE with LR, Max Abs Scaler) and BNB (Model B1) with the validation set balanced accuracy 0.78. TPOT with the reduced configurations have produced slightly less accurate models with validation set balanced accuracy 0.77 for Model B2 or LR Classifier also with four pre-processors (RFE<sup>1</sup>, SE with LR Classifier, SS and RFE<sup>2</sup>), balanced accuracy 0.75 for Model B3 or DT Classifier with five pre-processors (Binarizer, SE with DT Classifier, Select Fwe, Nystroem and PCA), balanced accuracy 0.76 for Model B4 or RF Classifier with three pre-processors (Binarizer, Normalizer and SP). Grid search tuning of LR, DT and RF classifiers produced balanced accuracy in the range 0.70–0.74. Overall, the best predictive performance came from the TPOT full configuration selection (Model B1).</p>
      <p>In ML binary classification analysis, current research has shifted away from simply presenting the accuracy of the model when reporting clinically relevant findings. Therefore, in this study, predictive power of the selected models was additionally evaluated via precision, recall, and threshold-based performance metrics. One of the most common metrics employed in clinical settings is precision or positive predictive values that characterizes the model’s ability to not to label negative samples as positive samples. It is often complemented by recall metrics—the ability of the model to detect all the positive samples. These metrics are single-threshold, meaning that they are defined for a single choice of decision threshold for an ML classifier and therefore are not able to describe its behavior within a range of decision criterions. This problem, however, could be resolved by reporting various receiver operator characteristic (ROC) curves (<xref rid="btz796-B19" ref-type="bibr">Provost <italic toggle="yes">et al.</italic>, 1998</xref>). The most commonly used one, ROC area AUC, illustrates the results of binary classification problem as classifier threshold is varied. It reports how the true positive rate (number of correctly classified positive samples) varies with the false positive rate (number of incorrectly classified negative samples; <xref rid="btz796-F2" ref-type="fig">Fig. 2</xref>). Models with BNB classifier optimized by TPOT reported the top ROC score for both P1 and P2. Similar to ROC AUC, PRC plots provides classifier-wide estimation; however, when dealing with imbalanced dataset PRC provides a more informative description of the classifier performance (<xref rid="btz796-B5" ref-type="bibr">Davis and Goadrich, 2006</xref>; <xref rid="btz796-B21" ref-type="bibr">Saito and Rehmsmeier, 2015</xref>). According to the PRC curves (<xref rid="btz796-F3" ref-type="fig">Fig. 3</xref>), TPOT models with BNB classifier (Models A1 and B1) have reported the top performance for both P1 and P2 similar to the other metrics. Overall, BNB classifier models optimized by TPOT have been ultimately first among four out five performance metrics reported here for both P1 and P2.</p>
      <fig position="float" id="btz796-F2">
        <label>Fig. 2.</label>
        <caption>
          <p>ROC AUC curves for selected models for P1 dataset (<bold>A</bold>) and P2 dataset (<bold>B</bold>)</p>
        </caption>
        <graphic xlink:href="btz796f2" position="float"/>
      </fig>
      <fig position="float" id="btz796-F3">
        <label>Fig. 3.</label>
        <caption>
          <p>Precision-recall curves for selected models for P1 dataset (<bold>A</bold>) and P2 dataset (<bold>B</bold>)</p>
        </caption>
        <graphic xlink:href="btz796f3" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.2 Complexity and performance tradeoff in model selection</title>
      <p>We further raised the question about the relationship between a model’s complexity and its performance with our assumption that a more complex model generally performs better than a less complex model. Indeed, for both P1 and P2 the best performing models (Models A1 and B1) had a complexity of five, where, in addition to optimized ML algorithm, four data transforming operators had been selected. To test the robustness of this model, we performed a sensitivity analysis where we consecutively removed pre-processing operators from the model pipeline to track the changes in its performance. As it can be seen from the <xref rid="btz796-T2" ref-type="table">Table 2(A)</xref>, the balanced accuracy and ROC AUC performance dramatically decrease after removal of the first two pre-processors (RFE and SE with LR Classifier). This suggests that this particular dataset may contain intricate non-linear relationships among features so high complexity combination of data transformers is needed to describe these relationships. Similar situations were observed for the model selected for P2: removal of the first three pre-processors (Variance Threshold, RFE, SE with LR) led to 10% decrease in both balanced accuracy and ROC AUC metrics [<xref rid="btz796-T2" ref-type="table">Table 2(B)</xref>]. In summary, we observed a consistent decrease across all performance metrics over the decrease in complexity confirming the general trend for increased complexity—increased performance relationship.</p>
      <table-wrap position="float" id="btz796-T2">
        <label>Table 2.</label>
        <caption>
          <p>The complexity-performance relationship for models selected by the TPOT optimization for P1(A) and P2(B)</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Model</th>
              <th rowspan="1" colspan="1">Balanced accuracy V/T</th>
              <th rowspan="1" colspan="1">Precision V/T</th>
              <th rowspan="1" colspan="1">Recall V/T</th>
              <th rowspan="1" colspan="1">ROC AUC V/T</th>
              <th rowspan="1" colspan="1">PRC V/T</th>
              <th rowspan="1" colspan="1">Pipeline complexity</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td colspan="7" style="#F2F2F2" rowspan="1">A. P1</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"> Model A1</td>
              <td rowspan="1" colspan="1">0.77/0.79</td>
              <td rowspan="1" colspan="1">0.91/0.93</td>
              <td rowspan="1" colspan="1">0.77/0.79</td>
              <td rowspan="1" colspan="1">0.77/0.86</td>
              <td rowspan="1" colspan="1">0.88/0.95</td>
              <td rowspan="1" colspan="1">5</td>
            </tr>
            <tr>
              <td style="#F2F2F2" rowspan="1" colspan="1"> Pr-1</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.74/0.77</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.90/0.92</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.77/0.79</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.74/0.84</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.86/0.94</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">4</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"> Pr-2</td>
              <td rowspan="1" colspan="1">0.67/0.73</td>
              <td rowspan="1" colspan="1">0.86/0.90</td>
              <td rowspan="1" colspan="1">0.73/0.72</td>
              <td rowspan="1" colspan="1">0.67/0.80</td>
              <td rowspan="1" colspan="1">0.83/0.92</td>
              <td rowspan="1" colspan="1">3</td>
            </tr>
            <tr>
              <td style="#F2F2F2" rowspan="1" colspan="1"> Pr-3</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.64/0.69</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.84/0.89</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.7/0.68</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.64/0.76</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.82/0.91</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">2</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"> Pr-4</td>
              <td rowspan="1" colspan="1">0.62/0.61</td>
              <td rowspan="1" colspan="1">0.81/0.81</td>
              <td rowspan="1" colspan="1">0.95/0.96</td>
              <td rowspan="1" colspan="1">0.62/0.85</td>
              <td rowspan="1" colspan="1">0.81/0.95</td>
              <td rowspan="1" colspan="1">1</td>
            </tr>
            <tr>
              <td colspan="7" style="#F2F2F2" rowspan="1">B. P2</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"> Model B1</td>
              <td rowspan="1" colspan="1">0.78/0.78</td>
              <td rowspan="1" colspan="1">0.82/0.84</td>
              <td rowspan="1" colspan="1">0.79/0.79</td>
              <td rowspan="1" colspan="1">0.78/0.82</td>
              <td rowspan="1" colspan="1">0.78/0.86</td>
              <td rowspan="1" colspan="1">5</td>
            </tr>
            <tr>
              <td style="#F2F2F2" rowspan="1" colspan="1"> Pr-1</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.78/0.76</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.82/0.82</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.81/0.79</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.78/0.81</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.78/0.86</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">4</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"> Pr-2</td>
              <td rowspan="1" colspan="1">0.74/0.76</td>
              <td rowspan="1" colspan="1">0.79/0.82</td>
              <td rowspan="1" colspan="1">0.77/0.78</td>
              <td rowspan="1" colspan="1">0.74/0.81</td>
              <td rowspan="1" colspan="1">0.74/0.86</td>
              <td rowspan="1" colspan="1">3</td>
            </tr>
            <tr>
              <td style="#F2F2F2" rowspan="1" colspan="1"> Pr-3</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.68/0.75</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.75/0.82</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.7/0.75</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.68/0.8</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.7/0.85</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">2</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"> Pr-4</td>
              <td rowspan="1" colspan="1">0.68/0.75</td>
              <td rowspan="1" colspan="1">0.75/0.82</td>
              <td rowspan="1" colspan="1">0.7/0.75</td>
              <td rowspan="1" colspan="1">0.68/0.8</td>
              <td rowspan="1" colspan="1">0.7/0.85</td>
              <td rowspan="1" colspan="1">1</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn2">
            <p><bold><italic toggle="yes">Note</italic></bold><italic toggle="yes">:</italic> Model ‘Pr-1’, ‘Pr-2’ etc. indicate the number of pre-processors removed from the original model pipeline.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>Given the results of the model selection process for P1, we concluded that a proper choice and optimization of data pre-processing operators and ML algorithms could be equally important if the goal is to obtain a maximum performance score. Therefore, to make a fair comparison of the TPOT optimization with competitive grid search-based model selection approach we have evaluated the performance of LR and BNB ML algorithms in various combinations with SS, SP and RFE pre-processors (<xref rid="btz796-T3" ref-type="table">Table 3</xref>). For P1 addition of SS operator in combination with RFE selector to the LR algorithm resulted into noticeable increase in balanced accuracy (from 0.68 to 0.73). Further addition of feature selectors, however, only added 1% increase in balanced accuracy score for both profiles.</p>
      <table-wrap position="float" id="btz796-T3">
        <label>Table 3.</label>
        <caption>
          <p>Comparative analysis of the grid search optimization of selected ML algorithms with SS, SP and RFE pre-processing operators for P1(A) and P2(B)</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Model</th>
              <th rowspan="1" colspan="1">Balanced accuracy V/T</th>
              <th rowspan="1" colspan="1">Precision V/T</th>
              <th rowspan="1" colspan="1">Recall V/T</th>
              <th rowspan="1" colspan="1">ROC AUC V/T</th>
              <th rowspan="1" colspan="1">PRC V/T</th>
              <th rowspan="1" colspan="1">Pipeline complexity</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td colspan="7" style="#F2F2F2" rowspan="1">A. P1</td>
            </tr>
            <tr>
              <td colspan="7" rowspan="1"> LR pipelines</td>
            </tr>
            <tr>
              <td style="#F2F2F2" rowspan="1" colspan="1">  LR</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.68/0.72</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.85/0.87</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.85/0.90</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.68/0.87</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.84/0.95</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">1</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">  LR +SS</td>
              <td rowspan="1" colspan="1">0.68/0.77</td>
              <td rowspan="1" colspan="1">0.86/0.92</td>
              <td rowspan="1" colspan="1">0.76/0.80</td>
              <td rowspan="1" colspan="1">0.68/0.84</td>
              <td rowspan="1" colspan="1">0.84/0.94</td>
              <td rowspan="1" colspan="1">2</td>
            </tr>
            <tr>
              <td style="#F2F2F2" rowspan="1" colspan="1">  LR + SS + SP</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.68/0.78</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.86/0.92</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.76/0.81</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.68/0.84</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.84/0.94</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">3</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">  LR + SS + RFE</td>
              <td rowspan="1" colspan="1">0.73/0.77</td>
              <td rowspan="1" colspan="1">0.88/0.91</td>
              <td rowspan="1" colspan="1">0.80/0.81</td>
              <td rowspan="1" colspan="1">0.73/0.84</td>
              <td rowspan="1" colspan="1">0.86/0.94</td>
              <td rowspan="1" colspan="1">3</td>
            </tr>
            <tr>
              <td colspan="7" style="#F2F2F2" rowspan="1"> BNB pipelines</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">  BNB</td>
              <td rowspan="1" colspan="1">0.72/0.77</td>
              <td rowspan="1" colspan="1">0.88/0.91</td>
              <td rowspan="1" colspan="1">0.76/0.78</td>
              <td rowspan="1" colspan="1">0.72/0.85</td>
              <td rowspan="1" colspan="1">0.85/0.95</td>
              <td rowspan="1" colspan="1">1</td>
            </tr>
            <tr>
              <td style="#F2F2F2" rowspan="1" colspan="1">  BNB + SS</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.66/0.72</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.85/0.89</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.73/0.73</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.66/0.79</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.83/0.92</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">2</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">  BNB + SS + SP</td>
              <td rowspan="1" colspan="1">0.71/0.76</td>
              <td rowspan="1" colspan="1">0.88/0.92</td>
              <td rowspan="1" colspan="1">0.76/0.75</td>
              <td rowspan="1" colspan="1">0.71/0.84</td>
              <td rowspan="1" colspan="1">0.85/0.95</td>
              <td rowspan="1" colspan="1">3</td>
            </tr>
            <tr>
              <td style="#F2F2F2" rowspan="1" colspan="1">  BNB + SS + RFE</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.70/0.73</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.88/0.90</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.75/0.76</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.70/0.82</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.85/0.94</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">3</td>
            </tr>
            <tr>
              <td colspan="7" rowspan="1">B. P2</td>
            </tr>
            <tr>
              <td colspan="7" style="#F2F2F2" rowspan="1"> LR pipelines</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">  LR</td>
              <td rowspan="1" colspan="1">0.73/0.74</td>
              <td rowspan="1" colspan="1">0.76/0.78</td>
              <td rowspan="1" colspan="1">0.84/0.84</td>
              <td rowspan="1" colspan="1">0.73/0.85</td>
              <td rowspan="1" colspan="1">0.73/0.89</td>
              <td rowspan="1" colspan="1">1</td>
            </tr>
            <tr>
              <td style="#F2F2F2" rowspan="1" colspan="1">  LR +SS</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.69/0.76</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.73/0.81</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.79/0.80</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.69/0.84</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.70/0.89</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">2</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">  LR + SS + SP</td>
              <td rowspan="1" colspan="1">0.72/0.76</td>
              <td rowspan="1" colspan="1">0.76/0.81</td>
              <td rowspan="1" colspan="1">0.80/0.79</td>
              <td rowspan="1" colspan="1">0.72/0.83</td>
              <td rowspan="1" colspan="1">0.73/0.89</td>
              <td rowspan="1" colspan="1">3</td>
            </tr>
            <tr>
              <td style="#F2F2F2" rowspan="1" colspan="1">  LR + SS + RFE</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.74/0.74</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.78/0.80</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.81/0.80</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.74/0.84</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.74/0.89</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">3</td>
            </tr>
            <tr>
              <td colspan="7" rowspan="1"> BNB pipelines</td>
            </tr>
            <tr>
              <td style="#F2F2F2" rowspan="1" colspan="1">  BNB</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.70/0.74</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.75/0.80</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.77/0.79</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.70/0.80</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.71/0.85</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">1</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">  BNB + SS</td>
              <td rowspan="1" colspan="1">0.63/0.66</td>
              <td rowspan="1" colspan="1">0.69/0.75</td>
              <td rowspan="1" colspan="1">0.69/0.67</td>
              <td rowspan="1" colspan="1">0.63/0.74</td>
              <td rowspan="1" colspan="1">0.66/0.81</td>
              <td rowspan="1" colspan="1">2</td>
            </tr>
            <tr>
              <td style="#F2F2F2" rowspan="1" colspan="1">  BNB + SS + SP</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.74/0.75</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.80/0.83</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.74/0.75</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.74/0.82</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">0.74/0.87</td>
              <td style="#F2F2F2" rowspan="1" colspan="1">3</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">  BNB + SS + RFE</td>
              <td rowspan="1" colspan="1">0.65/0.67</td>
              <td rowspan="1" colspan="1">0.71/0.76</td>
              <td rowspan="1" colspan="1">0.74/0.73</td>
              <td rowspan="1" colspan="1">0.65/0.77</td>
              <td rowspan="1" colspan="1">0.68/0.83</td>
              <td rowspan="1" colspan="1">3</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn3">
            <p><bold><italic toggle="yes">Note</italic></bold><italic toggle="yes">:</italic> SS, standard scaler; SP, select percentile<italic toggle="yes">;</italic> RFE, recursive feature eliminator; LR, logistic regression classifier; BNB, Bernoulli Naïve Bayes classifier.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>Addition of SS to BNB algorithm, on the contrary, reduced the model performance according to all reported performance metrics for both profiles. However, the further addition of SP feature selector to the pipeline resulted into an improvement of BNB pipeline performance according to balanced accuracy for P2 profile. This comparative analysis provides a brief insight into the importance of the correct choice and optimization of the data pre-processing operators or their combination for clinical datasets with complex relationships. Although the addition of specific pre-processors enhanced grid search optimization as compared with the ML algorithm tuning itself, TPOT agnostic optimization still provides with the best overall ML solution for both phenotypic profiles.</p>
    </sec>
    <sec>
      <title>3.3 Feature importances</title>
      <p>The TPOT selection process generated ML model pipelines that outperformed the traditional exhaustive parameter search approach for both CAD phenotypic profiles. Within clinical settings, one of the most important considerations for an ML model is interpretation via importance coefficients for features. Some models provide built-in functions such as coefficients in linear regression or feature importance scores in tree-based models. However, it is often the case that ML algorithms don’t have an implemented feature importance function or a model pipeline could be too complex to use this function. In these cases, PFI can be a great alternative metric. Here we report 30 features with the top coefficients received from the PFI analysis performed on the P1 dataset with Model A1 and P2 dataset with Model B1 (<xref rid="btz796-F4" ref-type="fig">Fig. 4A and B</xref>). The top coefficients correspond to clinical features such as sex, age, information about medication previously taken and several metabolic determinants like overall lipid content and HDL for P1 (<xref rid="btz796-F4" ref-type="fig">Fig. 4A</xref>) and various fatty acids for P2 (<xref rid="btz796-F4" ref-type="fig">Fig. 4B</xref>). Interestingly, both subsets report a rapid change in decline of the mean decrease in balanced accuracy coefficients after the top ten features, e.g. for P1 (<xref rid="btz796-F4" ref-type="fig">Fig. 4A</xref>) coefficients for the top 10 features experience a decrease from 8.8 (Prev_susp_MI) to 0.88 (Arrhythmia) while the remaining top features slowly approach coefficient of zero and with that behavior create a threshold for selection of the most important features that could be used in CAD diagnosis predictions.</p>
      <fig position="float" id="btz796-F4">
        <label>Fig. 4.</label>
        <caption>
          <p>PFI coefficients produced by Model A1 for P1 dataset with validation set balanced accuracy 0.77 (<bold>A</bold>) and by Model B1 for P2 dataset with validation set balanced accuracy 0.78 (<bold>B</bold>)</p>
        </caption>
        <graphic xlink:href="btz796f4" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec>
    <title>4 Discussion</title>
    <p>In this study, we present a comprehensive analysis of AutoML methods implemented in TPOT to predict the angiographic diagnosis of CAD, using metabolic and clinical features. We demonstrate that for the ANGES dataset, the TPOT model selection approach produces classification pipelines that outperform exhaustive grid search optimization (<xref rid="btz796-T1" ref-type="table">Tables 1</xref> and <xref rid="btz796-T3" ref-type="table">3</xref>). As reported in the summary tables (<xref rid="btz796-T1" ref-type="table">Table 1</xref>), the BNB classifier in combination with four pre-processors showed the best performance for P1, with a validation set balanced accuracy 0.77. In comparison, the grid search optimized LR yielded a balanced accuracy 0.68 which generates a 9% difference. With a balanced accuracy 0.78, BNB classifier combined with four pre-processors was the best performing model for P2, which outperformed the best grid search optimized model—DT classifier by 4%.</p>
    <p>The agnostic approach that uses minimum assumption about model selection employed by TPOT offers great potential for clinical prediction, particularly if the underlying mechanistic relationship between the different features is unknown. Here we employed two TPOT-based optimization approaches: full configuration with a complete list of pre-processors and classifiers and reduced configuration with a complete list of pre-processors and one selected classifier (LR, DT or RF). We have observed that in both instances TPOT with full set of classifiers and pre-processors had generated the model pipelines that outperform all optimization strategies (including reduced configuration TPOT and grid search with/without pre-processors). This suggests that the choice of pre-processors, and its combinations, could be as important as the choice of ML algorithm and its parameters (e.g. <xref rid="btz796-T2" ref-type="table">Table 2</xref> shows that for P1 removing first 2 pre-processors decreases balanced accuracy on 10%). That was also demonstrated via comparison of grid search optimization with selected data pre-processors (<xref rid="btz796-T3" ref-type="table">Table 3</xref>). Alternatively, to perform the agnostic model selection via the grid search approach one will need extensive computational resources. For example, to optimize an ensemble tree-based ML algorithm with five hyperparameters with 10 possible values for each, grid search will need to sample 100 000 combinations to select the best one and it would take over 2700 computing hours (with 32 GB RAM Desktop, 10 s per combination). A list of pre-processors can significantly increase the running time: adding 10 pre-processors to the space search will increase the number of combinations beyond 9.8 trillion (or over 270 billion computing hours). This would be an estimate for the pre-processors with no hyperparameters. However, many contain at least one hyperparameter to tune which would greatly increase the total number of combinations. Therefore, an agnostic approach to model selection with grid search may not be feasible with modern computational resources and thus stochastic search methods such as genetic programing may be the best option.</p>
    <p>As we mentioned earlier, the best classification performance was associated with full configuration TPOT optimization that resulted in pipelines with the BNB classification algorithm for both P1 and P2. This algorithm is rarely considered for classification task in biomedical predictive analytics and is most often used in spam detection procedures. The BNB classifier contains a binarization threshold function that could be relevant to some datasets, especially those with binary and/or discrete features. Indeed, the top features detected via PFI analysis were binary in both profiles. However, BNB classifier outperforms all the competitive algorithms only with the select pre-processors, with the most impactful ones to be RFE selector and SE with LR Classifier (<xref rid="btz796-T2" ref-type="table">Table 2</xref>). The first one uses Extra Tree Classifier estimation to reduce feature set in half, the second one generates class probabilities with the LR classifier as a synthetic feature, and therefore in addition to BNB assumptions this pipeline contains informational contribution from tree-based and linear models. Overall, making minimum assumption when building a model (and selecting feature transformers and selectors) could be very useful strategy when little is known about relationships between features and phenotype (e.g. for early hypothesis generation studies).</p>
    <p>We applied selected models to compare phenotypic profiles with a goal to clarify the role of non-obstructive CAD patients in CAD diagnostics. As a result of this comparison, the TPOT optimization produced a model pipeline with similar classification accuracy for both non-obstructive CAD/no CAD versus no CAD profile and obstructive CAD versus non-obstructive CAD/no CAD profile (BNB TPOT, 0.78) (<xref rid="btz796-T1" ref-type="table">Table 1</xref> and <xref rid="btz796-F5" ref-type="fig">Fig. 5</xref>) but closer look across all metrics identified a substantially higher precision and precision-recall curve values associated with the first profile (<xref rid="btz796-T1" ref-type="table">Table 1</xref> and <xref rid="btz796-F2" ref-type="fig">Fig. 2</xref>). Obstructive CAD is defined using arbitrary thresholds (in our study 50% stenosis) to identify patients requiring revascularization. However, non-obstructive and obstructive CAD represent the same pathophysiological pathways, complicating the identification of discriminative features. Therefore, we performed an examination of selected models with regard to the predictive power of the features via PFI procedure. As a result, a subset of top predictive clinical and metabolic features was outlined (<xref rid="btz796-F4" ref-type="fig">Fig. 4</xref>) for both phenotypic profiles. For the P1 profile a number of known risk factors (<xref rid="btz796-B9" ref-type="bibr">Hajar, 2017</xref>) and phenotypic proxies were found among top features including myocadiac infarction signs, age, sex, history of percutaneous coronary intervention (PCI) procedure, administration of nitrate and statin medications, hyperlipidemia and HDL cholesterol. The top predictive features differ between P1 and P2: in the P2 dataset metabolic features and specifically fatty acids seems to be more relevant for the prediction of CAD diagnosis (linoleic acid, total saturated fatty acid, omega-6 fatty acid and polyunsaturated fatty acid cumulative coefficient &gt;2%). These deviations could be related to non-obstructive CAD effect and the difference in the model pipeline selected for each profile. Overall, clinical information in combination with selected metabolic features (HDL, fatty acids) are potent predictors of CAD diagnosis.</p>
    <fig position="float" id="btz796-F5">
      <label>Fig. 5.</label>
      <caption>
        <p>Normalized confusion matrix for selected TPOT-optimized models for P1 dataset (<bold>A</bold>) and P2 dataset (<bold>B</bold>)</p>
      </caption>
      <graphic xlink:href="btz796f5" position="float"/>
    </fig>
  </sec>
  <sec>
    <title>5 Conclusion</title>
    <p>We demonstrated the power of agnostic model selection with the AutoML tool TPOT for CAD diagnosis prediction using clinical and metabolic data from the ANGES cohort. As a result, TPOT optimization automatically produced predictive models that outperformed grid search optimized models. We used selected models to show that phenotypic profile that distinguishes non-obstructive CAD patients from no CAD patients is associated with higher precision and subsequently have a different subset of predictive features than phenotypic profile that treats no CAD patients and non-obstructive CAD patients as the same outcome.</p>
  </sec>
  <sec>
    <title>Authors’ contribution</title>
    <p>A.O. and D.K. contributed to the conception of the project, conduct of the data analysis, and the interpretation of the results, and wrote the manuscript. F.W.A. and J.H.M. contributed to the conception of the project and the interpretation of the results and edited the manuscript for intellectual content. L.P.L., K.N., P.M., P.K., P.J.K., M.K., J.O.L. and T.L. contributed to the acquisition of the data and provided critical revisions to the manuscript.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was supported by grant [R01 LM010098] from the National Institutes of Health (USA).</p>
    <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btz796_Supplementary_Data</label>
      <media xlink:href="btz796_supplementary_data.zip">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btz796-B1">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Alaa</surname><given-names>A.M.</given-names></string-name>, <string-name><surname>van der Schaar</surname><given-names>M.</given-names></string-name></person-group> (<year>2018</year>) AutoPrognosis: automated clinical prognostic modeling via Bayesian optimization with structured Kernel learning. arXiv [cs.LG].</mixed-citation>
    </ref>
    <ref id="btz796-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bischl</surname><given-names>B.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) 
<article-title>mlr: machine learning in R</article-title>. <source>J. Mach. Learn. Res</source>., <volume>17</volume>, <fpage>5938</fpage>–<lpage>5942</lpage>.</mixed-citation>
    </ref>
    <ref id="btz796-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bzdok</surname><given-names>D.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>Points of significance: statistics versus machine learning</article-title>. <source>Nat. Methods</source>, <fpage>1</fpage>–<lpage>7</lpage>.</mixed-citation>
    </ref>
    <ref id="btz796-B5">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Davis</surname><given-names>J.</given-names></string-name>, <string-name><surname>Goadrich</surname><given-names>M.</given-names></string-name></person-group> (<year>2006</year>) The relationship between precision-recall and ROC curves. In: <italic toggle="yes">Proceedings of the 23rd International Conference on Machine Learning</italic>, ICML ’06, pp. <fpage>233</fpage>–<lpage>240</lpage>. ACM, New York, NY, USA.</mixed-citation>
    </ref>
    <ref id="btz796-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fernández-Delgado</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2014</year>) 
<article-title>Do we need hundreds of classifiers to solve real world classification problems?</article-title>  <source>J. Mach. Learn. Res</source>., <volume>15</volume>, <fpage>3133</fpage>–<lpage>3181</lpage>.</mixed-citation>
    </ref>
    <ref id="btz796-B7">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Feurer</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2015</year>) Efficient and robust automated machine learning. In: Cortes,C. <italic toggle="yes">et al</italic>. (eds.), <italic toggle="yes">Advances in Neural Information Processing Systems 28</italic>. Curran Associates, Inc., pp. <fpage>2962</fpage>–<lpage>2970</lpage>.</mixed-citation>
    </ref>
    <ref id="btz796-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fortin</surname><given-names>F.A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2012</year>) 
<article-title>DEAP: evolutionary algorithms made easy</article-title>. <source>J. Mach. Learn. Res</source>., <volume>13</volume>, <fpage>2171</fpage>–<lpage>2175</lpage>.</mixed-citation>
    </ref>
    <ref id="btz796-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hajar</surname><given-names>R.</given-names></string-name></person-group> (<year>2017</year>) 
<article-title>Risk factors for coronary artery disease: historical perspectives</article-title>. <source>Heart Views</source>, <volume>18</volume>, <fpage>109</fpage>–<lpage>114</lpage>.<pub-id pub-id-type="pmid">29184622</pub-id></mixed-citation>
    </ref>
    <ref id="btz796-B11">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Koza</surname><given-names>J.R.</given-names></string-name></person-group> (<year>1992</year>) <source>Genetic Programming: On the Programming of Computers by Means of Natural Selection.</source> 
 <publisher-name>MIT Press</publisher-name>, 
<publisher-loc>Cambridge, MA, USA</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btz796-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kuhn</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2008</year>) 
<article-title>Building predictive models in R using the caret package</article-title>. <source>J. Stat. Softw</source>., <volume>28</volume>, <fpage>1</fpage>–<lpage>26</lpage>.<pub-id pub-id-type="pmid">27774042</pub-id></mixed-citation>
    </ref>
    <ref id="btz796-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mennander</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2008</year>) 
<article-title>Diagnostic performance of plasma high sensitive C-reactive protein in detecting three-vessel coronary artery disease: modification by apolipoprotein E genotype</article-title>. <source>Scand. J. Clin. Lab. Invest</source>., <volume>68</volume>, <fpage>714</fpage>–<lpage>719</lpage>.<pub-id pub-id-type="pmid">18609102</pub-id></mixed-citation>
    </ref>
    <ref id="btz796-B14">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Mosley</surname><given-names>L.</given-names></string-name></person-group> (<year>2010</year>). A balanced approach to the multi-class imbalance problem. <italic toggle="yes">IJCV.</italic></mixed-citation>
    </ref>
    <ref id="btz796-B15">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Olson</surname><given-names>R.S.</given-names></string-name>, <string-name><surname>Moore</surname><given-names>J.H.</given-names></string-name></person-group> (<year>2016</year>) TPOT: a tree-based pipeline optimization tool for automating machine learning. In: Hutter,F. <italic toggle="yes">et al</italic>. (eds.) <italic toggle="yes">Proceedings of the Workshop on Automatic Machine Learning, Proceedings of Machine Learning Research</italic>. PMLR, New York, New York, pp. <fpage>66</fpage>–<lpage>74</lpage>.</mixed-citation>
    </ref>
    <ref id="btz796-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Olson</surname><given-names>R.S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>Data-driven advice for applying machine learning to bioinformatics problems</article-title>. <source>Pac. Symp. Biocomput</source>., <volume>23</volume>, <fpage>192</fpage>–<lpage>203</lpage>.<pub-id pub-id-type="pmid">29218881</pub-id></mixed-citation>
    </ref>
    <ref id="btz796-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Orlenko</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>Considerations for automated machine learning in clinical metabolic profiling: altered homocysteine plasma concentration associated with metformin exposure</article-title>. <source>Pac. Symp. Biocomput</source>., <volume>23</volume>, <fpage>460</fpage>–<lpage>471</lpage>.<pub-id pub-id-type="pmid">29218905</pub-id></mixed-citation>
    </ref>
    <ref id="btz796-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pedregosa</surname><given-names>F.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2011</year>) 
<article-title>Scikit-learn: machine learning in Python</article-title>. <source>J. Mach. Learn. Res</source>., <volume>12</volume>, <fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation>
    </ref>
    <ref id="btz796-B19">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Provost</surname><given-names>F.J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>1998</year>) <part-title>The case against accuracy estimation for comparing induction algorithms</part-title>. In: Shavlik,J.W. (ed.) <source>ICML</source>, Morgan Kaufmann Publishers Inc., San Francisco, CA, pp. <fpage>445</fpage>–<lpage>453</lpage>.</mixed-citation>
    </ref>
    <ref id="btz796-B20">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Rubinsteyn</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) fancyimpute: version 0.0.9. [2018-02-15].</mixed-citation>
    </ref>
    <ref id="btz796-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saito</surname><given-names>T.</given-names></string-name>, <string-name><surname>Rehmsmeier</surname><given-names>M.</given-names></string-name></person-group> (<year>2015</year>) 
<article-title>The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets</article-title>. <source>PLoS One</source>, <volume>10</volume>, <fpage>e0118432.</fpage><pub-id pub-id-type="pmid">25738806</pub-id></mixed-citation>
    </ref>
    <ref id="btz796-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Soininen</surname><given-names>P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2015</year>) 
<article-title>Quantitative serum nuclear magnetic resonance metabolomics in cardiovascular epidemiology and genetics</article-title>. <source>Circ. Cardiovasc. Genet</source>., <volume>8</volume>, <fpage>192</fpage>–<lpage>206</lpage>.<pub-id pub-id-type="pmid">25691689</pub-id></mixed-citation>
    </ref>
    <ref id="btz796-B23">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Thornton</surname><given-names>C.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2013</year>) Auto-WEKA. In: Ghani,R. <italic toggle="yes">et al</italic>. (eds.) <italic toggle="yes">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD ’13</italic>. ACM, New York, NY.</mixed-citation>
    </ref>
    <ref id="btz796-B24">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Witten</surname><given-names>I.H.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) <italic toggle="yes">Data Mining: Practical Machine Learning Tools and Techniques</italic>. Morgan Kaufmann.</mixed-citation>
    </ref>
  </ref-list>
</back>
