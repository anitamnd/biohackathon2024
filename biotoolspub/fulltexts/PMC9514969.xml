<?DTDIdentifier.IdentifierValue http://null/schema/dtds/document/fulltext/xcr/xocs-article.xsd?>
<?DTDIdentifier.IdentifierType schema?>
<?SourceDTD.DTDName xocs-article.xsd?>
<?SourceDTD.Version 1.0?>
<?ConverterInfo.XSLTName ftrr2jats.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Comput Biol Med</journal-id>
    <journal-id journal-id-type="iso-abbrev">Comput Biol Med</journal-id>
    <journal-title-group>
      <journal-title>Computers in Biology and Medicine</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">0010-4825</issn>
    <issn pub-type="epub">1879-0534</issn>
    <publisher>
      <publisher-name>Elsevier Ltd.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9514969</article-id>
    <article-id pub-id-type="pii">S0010-4825(22)00800-9</article-id>
    <article-id pub-id-type="doi">10.1016/j.compbiomed.2022.106092</article-id>
    <article-id pub-id-type="publisher-id">106092</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>SVD-CLAHE boosting and balanced loss function for Covid-19 detection from an imbalanced Chest X-Ray dataset</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au000001">
        <name>
          <surname>Roy</surname>
          <given-names>Santanu</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
        <xref rid="cor1" ref-type="corresp">⁎</xref>
      </contrib>
      <contrib contrib-type="author" id="au000002">
        <name>
          <surname>Tyagi</surname>
          <given-names>Mrinal</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">b</xref>
      </contrib>
      <contrib contrib-type="author" id="au000003">
        <name>
          <surname>Bansal</surname>
          <given-names>Vibhuti</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">b</xref>
      </contrib>
      <contrib contrib-type="author" id="au000004">
        <name>
          <surname>Jain</surname>
          <given-names>Vikas</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">c</xref>
      </contrib>
      <aff id="aff1"><label>a</label>School of Engineering and Technology, Christ (Deemed to be University), Bangalore 560074, India</aff>
      <aff id="aff2"><label>b</label>Bharati Vidyapeeth’s College of Engineering, New Delhi 110063, India</aff>
      <aff id="aff3"><label>c</label>School of Computer Science Engineering and Technology, Bennett University, Greater Noida, UP 201310, India</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>⁎</label>Corresponding author.</corresp>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>28</day>
      <month>9</month>
      <year>2022</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="ppub">
      <month>11</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>28</day>
      <month>9</month>
      <year>2022</year>
    </pub-date>
    <volume>150</volume>
    <fpage>106092</fpage>
    <lpage>106092</lpage>
    <history>
      <date date-type="received">
        <day>6</day>
        <month>3</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>15</day>
        <month>8</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>3</day>
        <month>9</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022 Elsevier Ltd. All rights reserved.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Elsevier Ltd</copyright-holder>
      <license>
        <license-p>Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.</license-p>
      </license>
    </permissions>
    <abstract id="d1e976">
      <p>Covid-19 disease has had a disastrous effect on the health of the global population, for the last two years. Automatic early detection of Covid-19 disease from Chest X-Ray (CXR) images is a very crucial step for human survival against Covid-19. In this paper, we propose a novel data-augmentation technique, called SVD-CLAHE Boosting and a novel loss function Balanced Weighted Categorical Cross Entropy (BWCCE), in order to detect Covid 19 disease efficiently from a highly class-imbalanced Chest X-Ray image dataset. Our proposed SVD-CLAHE Boosting method is comprised of both oversampling and under-sampling methods. First, a novel Singular Value Decomposition (SVD) based contrast enhancement and Contrast Limited Adaptive Histogram Equalization (CLAHE) methods are employed for oversampling the data in minor classes. Simultaneously, a Random Under Sampling (RUS) method is incorporated in major classes, so that the number of images per class will be more balanced. Thereafter, Balanced Weighted Categorical Cross Entropy (BWCCE) loss function is proposed in order to further reduce small class imbalance after SVD-CLAHE Boosting. Experimental results reveal that ResNet-50 model on the augmented dataset (by SVD-CLAHE Boosting), along with BWCCE loss function, achieved 95% F1 score, 94% accuracy, 95% recall, 96% precision and 96% AUC, which is far better than the results by other conventional Convolutional Neural Network (CNN) models like InceptionV3, DenseNet-121, Xception etc. as well as other existing models like Covid-Lite and Covid-Net. Hence, our proposed framework outperforms other existing methods for Covid-19 detection. Furthermore, the same experiment is conducted on VGG-19 model in order to check the validity of our proposed framework. Both ResNet-50 and VGG-19 model are pre-trained on the ImageNet dataset. We publicly shared our proposed augmented dataset on Kaggle website (https://www.kaggle.com/tr1gg3rtrash/balanced-augmented-covid-cxr-dataset), so that any research community can widely utilize this dataset. Our code is available on GitHub website online (https://github.com/MrinalTyagi/SVD-CLAHE-and-BWCCE).</p>
    </abstract>
    <kwd-group id="d1e982">
      <title>Keywords</title>
      <kwd>Class imbalance problem</kwd>
      <kwd>Covid-19 detection</kwd>
      <kwd>Chest X-Ray (CXR) images</kwd>
      <kwd>Data augmentation</kwd>
      <kwd>Categorical Cross Entropy (CCE)</kwd>
      <kwd>Contrast Limited Adaptive Histogram Equalization (CLAHE)</kwd>
      <kwd>Singular Value Decomposition (SVD)</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <label>1</label>
    <title>Introduction</title>
    <p id="d1e1013">Severe Acute Respiratory Syndrome Corona Virus2 (SARS-CoV2) <xref rid="b1" ref-type="bibr">[1]</xref> was declared as ‘pandemic’ by World Health Organization (WHO) in March 2020. Still now, over the last two years, more than 230 million people have been affected by this novel Corona Virus and more than 4.8 million people were declared dead <xref rid="b2" ref-type="bibr">[2]</xref> due to this virus. One of the features of this SARS-CoV2 is that it directly attacks human respiratory system and causes different forms of lung opacity, pneumonia and chest infections in human body <xref rid="b3" ref-type="bibr">[3]</xref>. Moreover, it can penalize human life, by destroying their immune systems. Covid-19 virus has become dangerous because it is contagious, which means it can be transmitted from person to person through their physical contact or breath contact <xref rid="b4" ref-type="bibr">[4]</xref>. One of the crucial steps into the battle against COVID-19 disease is to incorporate accurate, automated and easily available screening methods of infected patients, like radiology examination using chest radiography, Computed Tomography (CT) scanning and Reverse Transcriptase-Polymerase Chain Reaction (RT-PCR) <xref rid="b5" ref-type="bibr">[5]</xref> etc. Although RT-PCR is the most common golden standard screening method for Covid-19 detection, it is also a very tedious and manual process, and several researchers have already identified that RT-PCR can not detect Covid-19 with higher accuracy and sensitivity <xref rid="b6" ref-type="bibr">[6]</xref>. Thus, RT-PCR has not been considered in the field of machine learning or computer vision for automatic Covid-19 detection. Several researchers (<xref rid="b7" ref-type="bibr">[7]</xref>,<xref rid="b8" ref-type="bibr">[8]</xref>) have proposed automatic Covid-19 detection model using computer vision and deep learning model from CXR or CT scan images. In this paper, we have considered CXR images’ dataset because this is the most easily available screening method and not very costly <xref rid="b9" ref-type="bibr">[9]</xref> like CT scanning. The main objective of this research is to classify the CXR images into four classes (I) Covid 19, (II) Lung Opacity, (III) Normal and (IV) Viral Pneumonia.</p>
    <p id="d1e1054">Class imbalance is a very common problem in medical image diagnosis, since in hospitals the number of patients for different diseases varies considerably. The problem of class imbalance appears when the number of images in one class outnumbered the other classes <xref rid="b10" ref-type="bibr">[10]</xref> and consequently it may affect the final results of the classification. That means images from minor classes may be misclassified into major class which is undesirable. One of the ways to tackle this problem is to deploy Under-Sampling, in order to balance the dataset. Random Under-Sampling (RUS) <xref rid="b11" ref-type="bibr">[11]</xref> is the most frequently employed under-sampling method by researchers and it is known for its simplicity. It simply randomly chooses some images from major class in order to exclude them. However, RUS does not work efficiently for a highly-imbalanced dataset, since large number of randomly excluded images may contain significant features for the classification task. Thus, many researchers preferred over-sampling over under-sampling for resolving this class imbalance problem. In the recent advent, the most frequently employed over-sampling method for CNN is Data Augmentation technique <xref rid="b12" ref-type="bibr">[12]</xref> by zooming, cropping, rotation and flipping. SMOTE <xref rid="b13" ref-type="bibr">[13]</xref>, KNN <xref rid="b14" ref-type="bibr">[14]</xref> and GAN-Oversampling (<xref rid="b15" ref-type="bibr">[15]</xref>,<xref rid="b16" ref-type="bibr">[16]</xref>) etc. have also been widely utilized by several researchers. However, there is no guarantee that these data-augmentation methods will be feature invariant for the classification task. Indeed, this is dependent on the statistics of the dataset. SMOTE <xref rid="b13" ref-type="bibr">[13]</xref> generates synthetic images, based on random interpolation between their neighbor samples in a minor class. According to Z. Chen et al. <xref rid="b17" ref-type="bibr">[17]</xref>, SMOTE can generate a very different statistical distribution of dataset than original dataset which is not desirable. GAN-Oversampling <xref rid="b16" ref-type="bibr">[16]</xref> is a comparatively efficient method, where synthetic images are produced by Generative Adversarial Network (GAN). However, GAN-oversampling is computationally very costly than other data-augmentation techniques, moreover, larger number of images are required to perform GAN-Oversampling <xref rid="b16" ref-type="bibr">[16]</xref>. This is not feasible for small datasets. Another significant limitation of these data-augmentation methods is that they may produce very similar images (almost replicas), in large amounts in the augmented dataset. This does not improve the model performance, moreover, this may further induce overfitting in the model <xref rid="b18" ref-type="bibr">[18]</xref>. Hence, any data-augmentation method has a trade-off that, it should not produce exactly same images, as well as it should not produce very dis-similar images (otherwise that will be feature variant for final classification task). Another limitation of over-sampling is that it further increases the training times of CNN model. Several researchers (<xref rid="b11" ref-type="bibr">[11]</xref>, <xref rid="b19" ref-type="bibr">[19]</xref>) have come up with a hybrid method of employing both oversampling and under-sampling in recent times, but it does not resolve the class imbalance problem in a generalized way.</p>
    <p id="d1e1117">Boosting is another technique which generally boosts the performance of a weak classifier <xref rid="b20" ref-type="bibr">[20]</xref>. Boosting can be any method like oversampling, under-sampling <xref rid="b11" ref-type="bibr">[11]</xref>, ensemble method <xref rid="b17" ref-type="bibr">[17]</xref> etc. RUS Boosting <xref rid="b11" ref-type="bibr">[11]</xref>, Ada Boosting <xref rid="b20" ref-type="bibr">[20]</xref>, SMOTE Boosting <xref rid="b21" ref-type="bibr">[21]</xref> etc. have been widely utilized by numerous researchers. Ada-Boosting is an ensemble technique which uses variants of Nearest Neighbor Classifiers to enhance the performance of this weak classifier. However, this kind of ensemble method may increase computational complexity of the model and consequently, it may induce huge overfitting. RUS Boosting and SMOTE Boosting are further modifying Ada Boost by combining data sampling (oversampling and under-sampling) and ensemble method. J. Sun et al. <xref rid="b22" ref-type="bibr">[22]</xref> recently proposed an AdaBoost SVM Ensemble model along with SMOTE and time weighting in order to resolve class imbalance problem for financial distress prediction. All of these aforementioned Boosting methods are very time consuming and more feasible for time series data (or in NLP) than digital images. In this paper, we propose a novel SVD-CLAHE Boosting which is based on only over-sampling and under-sampling methods. We call this method as Boosting, because we have observed there is a huge boosting performance of standard CNN models, after employing proposed augmented dataset. Further it is explained in depth in Section <xref rid="sec2" ref-type="sec">2</xref>.</p>
    <p id="d1e1154">Another way to resolve this issue, is to incorporate a cost sensitive <xref rid="b23" ref-type="bibr">[23]</xref> Deep Learning model in which different kind of loss function is used in order to alleviate class imbalance problem. For example, many researchers employed Weighted Categorical Cross Entropy (WCCE) (<xref rid="b24" ref-type="bibr">[24]</xref>, <xref rid="b25" ref-type="bibr">[25]</xref>) for CNN model. The idea is to give little bit more weightage to the minor class and a little bit less weightage to the major class. However, WCCE may not work if the dataset is highly imbalanced. This is further explained in depth in the Section <xref rid="sec3.3" ref-type="sec">3.3</xref>. There are many different loss functions like focal loss <xref rid="b26" ref-type="bibr">[26]</xref>, center loss <xref rid="b27" ref-type="bibr">[27]</xref>, distribution balanced loss <xref rid="b28" ref-type="bibr">[28]</xref>, Anchor loss <xref rid="b29" ref-type="bibr">[29]</xref> proposed by several scientists who have worked in the same direction of class imbalance problem. Y. Cui et al. <xref rid="b30" ref-type="bibr">[30]</xref> found that the problem of class imbalance can be resolved just by modifying weights in loss function for different classes, rather than employing entirely different loss function. They have come up with a novel methodology of assigning weights, which are inversely proportional to the number of images to the corresponding class. However, we found that their methodology of weights assignment is not dependent on the number of classes, thus, it is not adaptive with multi-class classification task. Focal loss <xref rid="b26" ref-type="bibr">[26]</xref>, Tversky loss <xref rid="b31" ref-type="bibr">[31]</xref>, Unified focal loss <xref rid="b32" ref-type="bibr">[32]</xref> etc. are used very frequently to resolve class imbalance problem for image segmentation. However, for classification task, we didn’t find any specific loss function which efficiently works with any kind of class imbalance problem, in a generalized way.</p>
    <p id="d1e1209">The Covid 19 dataset is taken from publicly available Kaggle website (<xref rid="b33" ref-type="bibr">[33]</xref>, <xref rid="b34" ref-type="bibr">[34]</xref>) This dataset consists of four different classes. (I) Covid, (II) Lung Opacity (LO), (III) Normal, (IV) Viral Pneumonia (VP). A team of researchers from Qatar University, Doha, Qatar, and the University of Dhaka, Bangladesh have created this database of CXR images. In the first update, they released 219 COVID-19, 1341 normal and 1345 viral pneumonia CXR images. In the 2nd update, they have increased the COVID-19 class to 1200 CXR images. In the 3rd update, they have modified the database into total 3616 COVID-19 positive cases, along with 10,192 Normal, 6012 Lung Opacity and 1345 Viral Pneumonia (VP) images.</p>
    <p id="d1e1220">There are some challenges in this dataset which are as follows.</p>
    <p id="d1e1222">
      <list list-type="simple" id="d1e1224">
        <list-item id="lst1">
          <label>•</label>
          <p id="d1e1228">Clearly this can be observed that the number of images in each class, varies significantly in this dataset. Thus, conventional CNN model may not work on this dataset efficiently. This is a class imbalance problem.</p>
        </list-item>
        <list-item id="lst2">
          <label>•</label>
          <p id="d1e1233">Intra-class variance in Covid class, is comparatively higher than that of other classes, which is further discussed in section-4.2. This kind of intra-class statistical variability in the dataset further makes the classification task a lot more complicated.</p>
        </list-item>
        <list-item id="lst3">
          <label>•</label>
          <p id="d1e1238">Many of the images have poor contrast and poor background luminance, thus, it may lead to poor feature extraction in CNN model.</p>
        </list-item>
      </list>
    </p>
    <p id="d1e1240">In order to resolve these aforementioned challenges, we have proposed a novel framework in this paper. The main contributions of this paper are explained as follows:</p>
    <p id="d1e1242">
      <list list-type="simple" id="d1e1244">
        <list-item id="lst4">
          <label>1.</label>
          <p id="d1e1248">A novel data-augmentation technique, “SVD-CLAHE Boosting”, is proposed for resolving class imbalance problem from a highly imbalanced Covid 19 Chest X-Ray (CXR) dataset.</p>
        </list-item>
        <list-item id="lst5">
          <label>2.</label>
          <p id="d1e1253">A novel SVD-based contrast enhancement method, CLAHE 0.5 and CLAHE 1.0, are employed for over-sampling, whereas under-sampling is done by RUS in major classes.</p>
        </list-item>
        <list-item id="lst6">
          <label>3.</label>
          <p id="d1e1258">A novel loss function, “Balanced Weighted Categorical Cross-Entropy (BWCCE)”, is proposed for eliminating little class imbalance present after employing SVD-CLAHE Boosting.</p>
        </list-item>
        <list-item id="lst7">
          <label>4.</label>
          <p id="d1e1263">For training the CNN models, a transfer learning approach is deployed in which pre-trained weights are taken from a large ImageNet dataset.</p>
        </list-item>
        <list-item id="lst8">
          <label>5.</label>
          <p id="d1e1268">A unique framework (i.e. “SVD-CLAHE Boosting” data augmentation along with BWCCE loss function) is proposed for ResNet-50 model, which performs more efficiently than individual ResNet-50 model and other existing models.</p>
        </list-item>
        <list-item id="lst9">
          <label>6.</label>
          <p id="d1e1273">For the validity of the aforementioned framework, the same experiment is also conducted on the VGG-19 model.</p>
        </list-item>
        <list-item id="lst10">
          <label>7.</label>
          <p id="d1e1278">We have also shared the augmented dataset (of 30,033 images), generated by proposed SVD-CLAHE Boosting method, on the Kaggle site. To the best of our knowledge, this kind of augmented and balanced Covid CXR dataset was not available so far in public.</p>
        </list-item>
      </list>
    </p>
    <p id="d1e1281">The rest of the paper is organized in the following way: Section <xref rid="sec2" ref-type="sec">2</xref> presents a brief explanation of existing methods for Covid-19 detection from CXR images. Section <xref rid="sec3" ref-type="sec">3</xref> describes the entire proposed methodology, that is SVD-CLAHE Boosting and BWCCE Loss function, performed on ResNet-50 CNN model. Moreover, in Section <xref rid="sec4" ref-type="sec">4</xref>, quantitative and qualitative results of several CNN models are compared with the proposed framework. In Section <xref rid="sec5" ref-type="sec">5</xref>, we present our concluding remarks.</p>
  </sec>
  <sec id="sec2">
    <label>2</label>
    <title>Existing methods of Covid detection</title>
    <p id="d1e1309">M. Shiddhartha and Avik Santra <xref rid="b35" ref-type="bibr">[35]</xref> proposed a very light weighted CNN model, called Covid Lite model, which is based on Depth-wise Separable Convolutional Neural Network (DSCNN). The advantage of deploying DSCNN over DCNN is that, DSCNN reduces computational complexity of the model (during training) considerably, since sequential point wise convolution is incorporated. Moreover, White Balance and CLAHE image processing methods are utilized as pre-processing methods before feeding the images into DSCNN. The authors have employed a very small dataset of CXR images, having only 1823 images. Thus, their proposed Covid-Lite model is feasible for training this small dataset. L. Wang et al. <xref rid="b36" ref-type="bibr">[36]</xref> first time designed a novel CNN architecture, especially dedicated to Covid-19 detection from Chest X-Ray (CXR) images. The authors deployed a CNN architecture, called Projection Expansion Projection Extension (PEPX), based on human-machine collaborative design strategy. They incorporated a very light weighted CNN so that it can efficiently train the Covid dataset from the scratch. They called their architecture as Covid-Net. Moreover, they made a new CXR dataset Covid-x publicly available, which consists of 13,975 CXR images. Their dataset had three classes: Covid, Pneumonia and Normal. Y. Xu et al <xref rid="b37" ref-type="bibr">[37]</xref> recently employed a novel Mask Attention (MANet) based model for covid-19 detection from CXR images for five classes: Covid, Normal, Tuberculosis (TB), Bacterial Pneumonia (BP) and Viral Pneumonia (VP). Their model is comprised of two stage network. In the first stage, they segmented only the lung portions from the CXR images using a ResUnet (ResNet Backbone Unet) model. Thereafter, in the second stage, they employed several standard CNN models like ResNet-34, ResNet-50, VGG-16 and Inception-V3 etc., in order to accomplish the final classification task. They employed ResUnet model as an attention-based model <xref rid="b38" ref-type="bibr">[38]</xref>, which gives more attention to the important lung portions of the image. They have shown experimentally that accuracy of those CNN models is improved 0.5-1%, after employing MANet.</p>
    <p id="d1e1328">M. Togacar et al. <xref rid="b39" ref-type="bibr">[39]</xref> proposed a novel framework in which they have created two different augmented datasets, by using a fuzzy color-based image processing method and original dataset stacking technique. The main purpose of utilizing fuzzy color based pre-processing technique is to convert original images into the images with lesser noise and clear the foreground information. Moreover, a dataset stacking technique is incorporated to fuse the original images onto these processed images, further providing better clarity and high contrast images. They have combined two datasets from two different sources, in order to make a small dataset of 458 images. Moreover, they have employed two very lightweight CNNs, which are Mobile-Net V2 <xref rid="b40" ref-type="bibr">[40]</xref> and Squeeze-Net <xref rid="b41" ref-type="bibr">[41]</xref> for feature extraction from very small dataset. This is followed by an SVM classifier for the final classification task. Although, their method used data-augmentation, it did not work in the direction of reducing class imbalance from the dataset. Instead, they tried to improve the contrast of the original images, for better feature extraction. Recently, M. Mamalakis et al. <xref rid="b42" ref-type="bibr">[42]</xref> proposed a deep transfer learning pipeline DenResCov-19, which is based on ensemble of ResNet-50 <xref rid="b43" ref-type="bibr">[43]</xref> and DenseNet-121 models <xref rid="b44" ref-type="bibr">[44]</xref>. These models are pre-trained on ImageNet dataset. According to the author, combining both of these models further improved the model’s overall performance significantly. D. Das et al. <xref rid="b45" ref-type="bibr">[45]</xref> proposed a truncated Inception-Net model in which they modified the architecture of the traditional Inception-V3 model <xref rid="b46" ref-type="bibr">[46]</xref> a little bit, to reduce its complexity. Moreover, they deployed both Max-pooling and Global Average Pooling operation in order to reduce the dimension of the image considerably. Although they reduced the complexity of Inception-V3 considerably, their preparation of datasets was entirely a manual process. Other than these works, a lot of comparative analysis have been done in the direction of deep learning-based CNN model for Covid 19 detection <xref rid="b47" ref-type="bibr">[47]</xref>, <xref rid="b48" ref-type="bibr">[48]</xref>, <xref rid="b49" ref-type="bibr">[49]</xref>, <xref rid="b50" ref-type="bibr">[50]</xref>, <xref rid="b51" ref-type="bibr">[51]</xref>, <xref rid="b52" ref-type="bibr">[52]</xref>, <xref rid="b53" ref-type="bibr">[53]</xref>, <xref rid="b54" ref-type="bibr">[54]</xref> from CXR images and CT images. S. Nayak et al. <xref rid="b7" ref-type="bibr">[7]</xref> presented a survey paper for Covid 19 detection from CXR images, in which they implemented many standard CNN models AlexNet <xref rid="b55" ref-type="bibr">[55]</xref>, VGG-16 <xref rid="b56" ref-type="bibr">[56]</xref>, Google Net <xref rid="b57" ref-type="bibr">[57]</xref>, ResNet-34 <xref rid="b43" ref-type="bibr">[43]</xref>, ResNet-50 <xref rid="b43" ref-type="bibr">[43]</xref>, InceptionV3 <xref rid="b46" ref-type="bibr">[46]</xref> etc., for CXR Covid dataset.</p>
    <p id="d1e1398">All of the methods mentioned above did not work toward alleviating class imbalance problems from a particular dataset. Instead, we have found that most of the researchers <xref rid="b36" ref-type="bibr">[36]</xref>, <xref rid="b39" ref-type="bibr">[39]</xref>, <xref rid="b42" ref-type="bibr">[42]</xref>, <xref rid="b45" ref-type="bibr">[45]</xref>, <xref rid="b49" ref-type="bibr">[49]</xref> combined different Covid CXR datasets from different sources and manually discarded many images, in order to produce a balanced dataset. This is a wrong approach, in our perspective, because they have manually reduced the challenges of the dataset. Moreover, different datasets can have different kinds of statistics; thus, combining all the datasets into one is not feasible. A.M.Khan et al. <xref rid="b49" ref-type="bibr">[49]</xref> claimed that still now there is no balanced Covid dataset available online. To the best of our knowledge, our proposed method is the first attempt to provide an automatic deep learning framework that can efficiently work on a highly class imbalanced Covid-19 dataset. Unlike other existing research work, our proposed augmentation technique (of preparing a dataset) is purely automatic.</p>
    <p id="d1e1408">In this research, we intentionally chose this dataset with the challenges mentioned above. Many researchers already found 97–98 percentage accuracy, F1-score, precision and recall for covid-19 detection from other existing CXR datasets <xref rid="b36" ref-type="bibr">[36]</xref>, <xref rid="b58" ref-type="bibr">[58]</xref>. However, the same is not true for the employed dataset. We found conventional CNN model does not work efficiently for this employed dataset <xref rid="b33" ref-type="bibr">[33]</xref> due to the challenges as mentioned earlier. Hence, we believe that there is a huge research scope to further improve the existing models’ performance for this challengeable dataset.</p>
    <p id="d1e1418">Our research mainly focuses on resolving the class imbalance problem in a highly imbalanced and challengeable dataset. We have not found an equally challenged and imbalanced dataset for Covid-19 detection from CXR images; thus, other existing datasets are not feasible for our proposed methodology. Instead, we have prepared three more augmented datasets along with this original dataset. We tested the performance of two different CNN models on those datasets for the validity of the proposed framework.<fig id="fig1"><label>Fig. 1</label><caption><p>Block Diagram of entire proposed model (SVD-CLAHE Boosting + ResNet-50 + BWCCE).</p></caption><graphic xlink:href="gr1_lrg"/></fig></p>
  </sec>
  <sec id="sec3">
    <label>3</label>
    <title>Methodology</title>
    <p id="d1e1427">The methodology for our proposed method can be divided into three parts: I. Choosing a suitable CNN model, II. SVD-CLAHE Boosting for class imbalance problem, III. Balanced Weighted Categorical Cross-Entropy (BWCCE).</p>
    <sec id="sec3.1">
      <label>3.1</label>
      <title>Choosing suitable CNN model</title>
      <p id="d1e1434">Since the employed dataset is very challenging, which is mentioned in the previous section, our first task is to find a suitable CNN model for this dataset. We have tested various CNN models like InceptionV3 <xref rid="b46" ref-type="bibr">[46]</xref>, Xception <xref rid="b59" ref-type="bibr">[59]</xref>, DenseNet-121 <xref rid="b44" ref-type="bibr">[44]</xref>, VGG 16, VGG 19 <xref rid="b56" ref-type="bibr">[56]</xref>, ResNet-50 <xref rid="b43" ref-type="bibr">[43]</xref> etc. on this dataset, which is further presented in the results and analysis section. We observed that VGG 16, VGG19, and ResNet-50 have slightly better results than Xception, DenseNet-121, InceptionV3, etc. VGG models are also known for their simplicity which is a direct modification of Alex-Net. Because of this simplicity, their model converges faster, and consequently, they have better performances than other complicated models (Inception-V3, DenseNet-121, Xception). ResNet-50 model also performs efficiently for this dataset, despite its complicated structure. To the best of our knowledge, due to the skip connection present in the ResNet-50 model <xref rid="b43" ref-type="bibr">[43]</xref> it can alleviate the problem of vanishing gradients, which generally appear during weights updating by the back-propagation algorithm (especially for a large and complicated network like InceptionV3, Xception). Moreover, the ResNet-50 model has a higher (50) number of layers, enabling the network to make very complicated decisions. Thus, we have chosen ResNet-50 as our proposed model. Additionally, we have also employed VGG-19 to check the validity of the proposed framework (i.e., SVD-CLAHE boosting + BWCCE loss function). The entire proposed model is presented in <xref rid="fig1" ref-type="fig">Fig. 1</xref>.</p>
    </sec>
    <sec id="sec3.2">
      <label>3.2</label>
      <title>SVD-CLAHE boosting for class imbalance problem</title>
      <p id="d1e1470">Any class imbalance problem can be defined as follows: Assume the data samples in an image dataset is represented by <inline-formula><mml:math id="d1e1473" altimg="si1.svg" display="inline"><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="d1e1550" altimg="si2.svg" display="inline"><mml:mi>m</mml:mi></mml:math></inline-formula> is the total number of samples in the dataset, <inline-formula><mml:math id="d1e1555" altimg="si3.svg" display="inline"><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is <inline-formula><mml:math id="d1e1566" altimg="si4.svg" display="inline"><mml:mi>i</mml:mi></mml:math></inline-formula>th original image and <inline-formula><mml:math id="d1e1571" altimg="si5.svg" display="inline"><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is its corresponding label. The total number of classes in the dataset is <inline-formula><mml:math id="d1e1581" altimg="si6.svg" display="inline"><mml:mi>K</mml:mi></mml:math></inline-formula>. Thus, labeled data <inline-formula><mml:math id="d1e1586" altimg="si7.svg" display="inline"><mml:mrow><mml:mi>y</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mi>K</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
      <p id="d1e1612">The estimated probabilities for each class are represented by <inline-formula><mml:math id="d1e1615" altimg="si8.svg" display="inline"><mml:mrow><mml:mi>p</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mtext>[</mml:mtext><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>,….<inline-formula><mml:math id="d1e1657" altimg="si9.svg" display="inline"><mml:msup><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, where <inline-formula><mml:math id="d1e1682" altimg="si10.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="d1e1713" altimg="si11.svg" display="inline"><mml:mrow><mml:mo>∀</mml:mo><mml:mi>L</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula>; here <inline-formula><mml:math id="d1e1737" altimg="si12.svg" display="inline"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> means that the probability of a sample ‘<inline-formula><mml:math id="d1e1753" altimg="si4.svg" display="inline"><mml:mi>i</mml:mi></mml:math></inline-formula>’, will be correctly classified into the class ‘<inline-formula><mml:math id="d1e1758" altimg="si14.svg" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula>’.</p>
      <p id="d1e1765">A dataset is said to have class imbalance problem if and only if: <disp-formula id="fd1"><label>(1)</label><mml:math id="d1e1774" altimg="si15.svg" display="block"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mfrac><mml:mo linebreak="goodbreak">≪</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
</p>
      <p id="d1e1806">where <inline-formula><mml:math id="d1e1809" altimg="si16.svg" display="inline"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the number of samples in a minor class ‘<inline-formula><mml:math id="d1e1821" altimg="si17.svg" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>’, and <inline-formula><mml:math id="d1e1829" altimg="si18.svg" display="inline"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the number of samples in a major class ‘<inline-formula><mml:math id="d1e1842" altimg="si19.svg" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:math></inline-formula>’. The name ‘<inline-formula><mml:math id="d1e1850" altimg="si17.svg" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>’ is chosen for the lower class, the name ‘<inline-formula><mml:math id="d1e1858" altimg="si19.svg" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:math></inline-formula>’ is chosen to indicate the higher class, <inline-formula><mml:math id="d1e1866" altimg="si22.svg" display="inline"><mml:mrow><mml:mi>m</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:msub><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the total number of samples in the dataset.</p>
      <p id="d1e1887">In Eq. <xref rid="fd1" ref-type="disp-formula">(1)</xref>, <inline-formula><mml:math id="d1e1894" altimg="si23.svg" display="inline"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula> can also be interpreted as the probability that one sample will be correctly classified into minor class ‘<inline-formula><mml:math id="d1e1911" altimg="si17.svg" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>’. Thus, the purpose of any over-sampling method is to significantly increase this probability of classification for minor classes so that it will be comparable to the probability of classification for major classes.</p>
      <p id="d1e1918">Hence, the purpose of any over-sampling method is, <disp-formula id="fd2"><label>(2)</label><mml:math id="d1e1927" altimg="si25.svg" display="block"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">≈</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula>Here, <inline-formula><mml:math id="d1e1959" altimg="si26.svg" display="inline"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the probability of <inline-formula><mml:math id="d1e1975" altimg="si4.svg" display="inline"><mml:mi>i</mml:mi></mml:math></inline-formula>th sample, correctly classified into minor class ‘<inline-formula><mml:math id="d1e1981" altimg="si17.svg" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>’. <inline-formula><mml:math id="d1e1989" altimg="si29.svg" display="inline"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the probability of <inline-formula><mml:math id="d1e2005" altimg="si30.svg" display="inline"><mml:mi>j</mml:mi></mml:math></inline-formula>th sample, correctly classified into major class ‘<inline-formula><mml:math id="d1e2010" altimg="si19.svg" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:math></inline-formula>’.</p>
      <p id="d1e2017">The performance of ResNet-50 is fraught with the problem of a highly class imbalance of dataset, although it is working better than other models. This can be further observed in Section <xref rid="sec5" ref-type="sec">5</xref>. We have noticed a slight fluctuation in the precision and recall results for different classes; moreover, we think 90% accuracy (by ResNet-50) can be further improved for the multi-class classification task for this imbalanced dataset.</p>
      <p id="d1e2024">This paper develops a novel data-augmentation technique, based on contrast enhancement by SVD based method and CLAHE method. Our proposed data-augmentation method neither generates exactly same images, nor they generate very dis-similar images, in terms of statistics. Thus, it overcomes the trade-off of conventional data augmentation mentioned earlier in Section <xref rid="sec1" ref-type="sec">1</xref>. The idea is to generate a synthetic, pre-processed, and balanced dataset so that the model has enough images per class to learn how to distinguish various classes of images. By SVD based contrast enhancement, we produce synthetic images with slightly different statistics in terms of luminance and contrast. Whereas CLAHE 0.5 and CLAHE 1.0 further improved the clarity and contrast of the images, which can strengthen the edges during feature extraction by the CNN model. Some of the pre-processed images by CLAHE 0.5 and SVD+CLAHE 0.5 are shown in <xref rid="fig2" ref-type="fig">Fig. 2</xref>. We call our proposed oversampling and RUS under-sampling method SVD-CLAHE Boosting because it can significantly boost the performance of a standard CNN model by alleviating the class imbalance problem from the dataset.</p>
      <p id="d1e2035">The exact method of the proposed SVD-CLAHE Boosting method is further explained below in-depth in three parts: (a) SVD based contrast enhancement method, (b) CLAHE Contrast enhancement method, (c) Proposed SVD-CLAHE boosting according to intra-class variance.</p>
      <p id="d1e2038">
        <fig id="fig2">
          <label>Fig. 2</label>
          <caption>
            <p>Example of proposed Augmented Dataset (by SVD-CLAHE Boosting).</p>
          </caption>
          <graphic xlink:href="gr2_lrg"/>
        </fig>
      </p>
      <sec id="sec3.2.1">
        <label>3.2.1</label>
        <title>SVD based contrast enhancement method</title>
        <p id="d1e2046">The first step of this method is to decompose the images into two orthogonal matrices and one singular value (diagonal) matrix by Singular Value Decomposition (SVD) <xref rid="b60" ref-type="bibr">[60]</xref>, which is given in Eq. <xref rid="fd3" ref-type="disp-formula">(3)</xref>. After that, we modify only the singular matrix by multiplying it with a real constant, named as ‘ratio’. In Eq. <xref rid="fd3" ref-type="disp-formula">(3)</xref>, these two decomposed orthogonal matrices have all the essential information of the images; thus, only multiplying a real constant value with the singular value matrix does not lose any critical information, to the best of our knowledge.</p>
        <p id="d1e2060">There was already a similar method of SVD based contrast enhancement, which was proposed for satellite images <xref rid="b61" ref-type="bibr">[61]</xref>; however, by their method, the value of ratio does not give a higher value for the employed Covid image dataset. Thus, we proposed a novel SVD based contrast enhancement method that will be suitable for the CXR dataset. In our proposed method, we choose a reference image with very good contrast. We choose the value of ‘ratio’ according to the contrast difference between the reference image and the source image. The SVD contrast enhancement method is further explained below in Eqs. <xref rid="fd3" ref-type="disp-formula">(3)</xref>–<xref rid="fd8" ref-type="disp-formula">(8)</xref>.</p>
        <p id="d1e2074"><bold>Step1:</bold> Decompose <inline-formula><mml:math id="d1e2079" altimg="si32.svg" display="inline"><mml:mi>I</mml:mi></mml:math></inline-formula> into <inline-formula><mml:math id="d1e2084" altimg="si33.svg" display="inline"><mml:mi>U</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="d1e2089" altimg="si34.svg" display="inline"><mml:mi>S</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="d1e2095" altimg="si35.svg" display="inline"><mml:mi>V</mml:mi></mml:math></inline-formula>, whereas <inline-formula><mml:math id="d1e2100" altimg="si33.svg" display="inline"><mml:mi>U</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="d1e2105" altimg="si35.svg" display="inline"><mml:mi>V</mml:mi></mml:math></inline-formula> are orthogonal matrices, <inline-formula><mml:math id="d1e2110" altimg="si34.svg" display="inline"><mml:mi>S</mml:mi></mml:math></inline-formula> is the singular value matrix, which is a diagonal matrix. <disp-formula id="fd3"><label>(3)</label><mml:math id="d1e2122" altimg="si39.svg" display="block"><mml:mrow><mml:mi>I</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>U</mml:mi><mml:mi>S</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:math></disp-formula>
</p>
        <p id="d1e2134"><bold>Step2:</bold> The real constant ‘ratio’ is determined in the following way, which is presented from Eq. <xref rid="fd4" ref-type="disp-formula">(4)</xref>–<xref rid="fd7" ref-type="disp-formula">(7)</xref>. Here, <inline-formula><mml:math id="d1e2147" altimg="si40.svg" display="inline"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> means global standard deviation, and <inline-formula><mml:math id="d1e2158" altimg="si41.svg" display="inline"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> means the global mean of the image. <inline-formula><mml:math id="d1e2168" altimg="si42.svg" display="inline"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the global contrast of the target image, <inline-formula><mml:math id="d1e2182" altimg="si43.svg" display="inline"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the global contrast of the source image. <disp-formula id="fd4"><label>(4)</label><mml:math id="d1e2200" altimg="si44.svg" display="block"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
<disp-formula id="fd5"><label>(5)</label><mml:math id="d1e2264" altimg="si45.svg" display="block"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
<inline-formula><mml:math id="d1e2329" altimg="si46.svg" display="inline"><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="8.5359pt"/><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">&gt;</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>
<disp-formula id="fd6"><label>(6)</label><mml:math id="d1e2366" altimg="si47.svg" display="block"><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mn>1</mml:mn><mml:mo>.</mml:mo><mml:mn>05</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">−</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
<inline-formula><mml:math id="d1e2436" altimg="si48.svg" display="inline"><mml:mrow><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math></inline-formula>
<disp-formula id="fd7"><label>(7)</label><mml:math id="d1e2455" altimg="si49.svg" display="block"><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mn>1</mml:mn><mml:mo>.</mml:mo><mml:mn>05</mml:mn></mml:mrow></mml:math></disp-formula>
</p>
        <p id="d1e2475"><bold>Step3:</bold> Multiply ‘<inline-formula><mml:math id="d1e2480" altimg="si34.svg" display="inline"><mml:mi>S</mml:mi></mml:math></inline-formula>’ (the singular matrix) with the real constant ‘<inline-formula><mml:math id="d1e2485" altimg="si51.svg" display="inline"><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:math></inline-formula>’ given in the following Eq. <xref rid="fd8" ref-type="disp-formula">(8)</xref>. <disp-formula id="fd8"><label>(8)</label><mml:math id="d1e2510" altimg="si52.svg" display="block"><mml:mrow><mml:msup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>S</mml:mi><mml:mo linebreak="goodbreak">∗</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:math></disp-formula>
</p>
        <p id="d1e2535"><bold>Step4:</bold> Concatenate <inline-formula><mml:math id="d1e2540" altimg="si53.svg" display="inline"><mml:mrow><mml:mi>U</mml:mi><mml:msup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mi>V</mml:mi></mml:mrow></mml:math></inline-formula> into <inline-formula><mml:math id="d1e2555" altimg="si54.svg" display="inline"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, whereas, <inline-formula><mml:math id="d1e2565" altimg="si54.svg" display="inline"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the processed image.</p>
        <p id="d1e2574"><italic>Explanation</italic>: The idea is to generate some synthetic images by modifying the singular matrix in SVD space. In order to fix the constant ‘<inline-formula><mml:math id="d1e2579" altimg="si51.svg" display="inline"><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:math></inline-formula>’ (by which the singular matrix is to be multiplied), we have chosen one target image from each class that has very good contrast and luminance property. After that, we compute the global contrast, according to S. Roy et al. <xref rid="b62" ref-type="bibr">[62]</xref>, which is nothing but the ratio of global standard deviation to the global mean value for both reference image and source image. These are in Eqs. <xref rid="fd4" ref-type="disp-formula">(4)</xref>, <xref rid="fd5" ref-type="disp-formula">(5)</xref>, respectively. We fix the ratio by Eq. <xref rid="fd6" ref-type="disp-formula">(6)</xref>, which should be proportional to the contrast between the reference and source images. A similar kind of adaptive transformation is recently proposed by S. Roy et al. <xref rid="b63" ref-type="bibr">[63]</xref> for color normalization of histopathology images. However, we observe that some CXR images (<inline-formula><mml:math id="d1e2611" altimg="si57.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) can be negative. Because there is no guarantee that reference image contrast is always greater than that of any other source images in the dataset. In that case, no further contrast enhancement is required because the source image contrast is already greater than that of the reference image. Thus, the value of ‘ratio’ should be chosen 1. However, we choose the by-default value of ‘ratio’ to 1.05 instead of 1, which are given in Eqs. <xref rid="fd6" ref-type="disp-formula">(6)</xref>, <xref rid="fd7" ref-type="disp-formula">(7)</xref>. Because it will ensure that there will always be around 5 percent contrast enhancement for every image. Otherwise, the same images could be produced by the proposed SVD based contrast enhancement, which is not desirable for data augmentation, mentioned in Section <xref rid="sec1" ref-type="sec">1</xref>. Therefore, it was necessary to produce slightly different images (in terms of contrast and luminance statistics) during oversampling in a minor class, thereby choosing the default value of ratio to 1.05. Some of the images from the proposed augmented dataset are shown in <xref rid="fig2" ref-type="fig">Fig. 2</xref>.</p>
      </sec>
      <sec id="sec3.2.2">
        <label>3.2.2</label>
        <title>CLAHE Contrast enhancement method</title>
        <p id="d1e2655">Contrast Limited Adaptive Histogram Equalization (CLAHE) <xref rid="b64" ref-type="bibr">[64]</xref> is a modified version of Global Histogram Equalization (GHE) <xref rid="b65" ref-type="bibr">[65]</xref> in which histogram stretching operation is limited by</p>
        <p id="d1e2665">a maximum clipping value (e.g., 0.5, 1.0). Here HE method is employed locally, which means it is performed in every local region (or window) of the image. We have chosen this window size 8 × 8 empirically for this operation. This is to clarify that, CLAHE 0.5 means it specifies a scale of contrast clipping 0.5 (this is not a version). For example, CLAHE 0.5 will do less contrast enhancement than CLAHE 1.0. However, we have also noticed that the conventional CLAHE 2.0 method (according to open cv library it is, CLAHE 2.0) is not inevitable of data loss. It can do over-contrast enhancement for some CXR images. Consequently, it might not work for CNN models. Therefore, we have chosen CLAHE 0.5 and CLAHE 1.0 so that there will not be excess contrast enhancement and, consequently, there will be comparatively little data loss. In order to ensure that fact, we have computed the correlation coefficient between the processed image and the original image in the overall CXR dataset for different scales of CLAHE. We have found that the mean correlation co-efficient value does not go less than 0.93 for CLAHE 0.5 and does not go less than 0.90 for CLAHE 1.0. Therefore, we have empirically chosen the CLAHE 0.5 and CLAHE 1.0 contrast enhancement methods with an 8 × 8 window for data augmentation.<fig id="fig3"><label>Fig. 3</label><caption><p>The entire scheme of proposed SVD-CLAHE Boosting.</p></caption><graphic xlink:href="gr3_lrg"/></fig></p>
      </sec>
      <sec id="sec3.2.3">
        <label>3.2.3</label>
        <title>Proposed SVD-CLAHE Boosting according to intra-class variance</title>
        <p id="d1e2674">We have done a series of experiments on SVD-CLAHE boosting. <xref rid="tbl1" ref-type="table">Table 1</xref> represents a different number of images per class we have chosen for various experiments. The results of their performances can be found in more depth in Section <xref rid="sec5" ref-type="sec">5</xref>. First, we have performed conventional data-augmentation techniques <xref rid="b12" ref-type="bibr">[12]</xref> like rotation, flipping (horizontal and vertical), zooming, and cropping in minor classes (e.g., Viral Pneumonia and the Covid class) in order to do oversampling. However, we observed that incorporating such data augmentation produces worse results.</p>
        <p id="d1e2689">Because we have found that these data augmentation techniques are not exactly featured invariant for these CXR images, for example, if we flip those images horizontally, we notice a very different image is produced. This will differ the feature extraction process for the classification task. Consequently, it further increases the complexity during the training of the CNN model. Later, we generate an augmented dataset by SVD-CLAHE oversampling and RUS, where each class has an equal number of images (5355) to resolve the datasets class imbalance problem. This is shown in <xref rid="tbl1" ref-type="table">Table 1</xref>. However, we have found that producing a precisely same (or very similar) number of images per class did not resolve this class imbalance problem entirely from this dataset. However, it improved the model performance a little bit.<table-wrap position="float" id="tbl1"><label>Table 1</label><caption><p>Comparisons of various experiments with different number of images per classes.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset (training)</th><th align="left">Covid</th><th align="left">Normal</th><th align="left">LO</th><th align="left">VP</th></tr></thead><tbody><tr><td align="left">Original</td><td align="left">2923</td><td align="left">8096</td><td align="left">4831</td><td align="left">1082</td></tr><tr><td align="left">SVD-CLAHE Boosting</td><td align="left">5355</td><td align="left">5355</td><td align="left">5355</td><td align="left">5355</td></tr><tr><td align="left">(Equal images per class)</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">Proposed SVD-</td><td align="left">8769</td><td align="left">8192</td><td align="left">7662</td><td align="left">5410</td></tr><tr><td align="left">CLAHE Boosting</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr></tbody></table></table-wrap></p>
        <p id="d1e2697">In the final experiment, a dataset of 30,033 images is prepared by the proposed SVD-CLAHE Boosting method. In this method, the number of images per class is chosen based on an intra-class variance in each class. To the best of our knowledge, if in one class the intra-class variance among the images is huge (that means different images have different kinds of statistics in a class), in that case, the number of images trained to the CNN model must also be higher for the corresponding class. Because of higher statistical variance, neural networks may need a higher number of such images to converge faster. Hence, intra-class variance significantly impacts this kind of dataset for the final classification task. We have chosen ten images per class with the most similarity among all images in that class to compute the intra-class variance in the dataset. We have employed five different (expert) people in order to choose the most similar images from each class, and thereafter, we computed intra-similarity by estimating correlation co-efficient <xref rid="b66" ref-type="bibr">[66]</xref> between the chosen image and all other images in that class, which is given in Eq. <xref rid="fd9" ref-type="disp-formula">(9)</xref>. This is to clarify that, here expert people means they are experts in computer vision task and thus, understand well statistical similarity inside images. Subsequently, we take an average of all such values and subtract it from one to compute the final intra-class variance. <disp-formula id="fd9"><label>(9)</label><mml:math id="d1e2714" altimg="si58.svg" display="block"><mml:mrow><mml:mi>σ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mn>1</mml:mn><mml:mo linebreak="goodbreak">−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>N</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:munderover><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
</p>
        <p id="d1e2819">where <inline-formula><mml:math id="d1e2822" altimg="si59.svg" display="inline"><mml:mrow><mml:mi>M</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> per class, which is the chosen number of images having the most similarity per class, <inline-formula><mml:math id="d1e2832" altimg="si60.svg" display="inline"><mml:mi>N</mml:mi></mml:math></inline-formula>
<inline-formula><mml:math id="d1e2837" altimg="si61.svg" display="inline"><mml:mo>=</mml:mo></mml:math></inline-formula> a total number of images in one class, <inline-formula><mml:math id="d1e2843" altimg="si62.svg" display="inline"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the selected image (from those 10 images) in one class which are the most similar, <inline-formula><mml:math id="d1e2853" altimg="si63.svg" display="inline"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is any other image in that class, <inline-formula><mml:math id="d1e2863" altimg="si64.svg" display="inline"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula> is representing correlation coefficient between two random variables <inline-formula><mml:math id="d1e2875" altimg="si65.svg" display="inline"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="d1e2881" altimg="si66.svg" display="inline"><mml:mi>y</mml:mi></mml:math></inline-formula>. Having more intra-class similarity (structural similarity) means that it has a lesser intra-class variance. Therefore, we subtracted the computed correlation coefficient from 1 in Eq. <xref rid="fd9" ref-type="disp-formula">(9)</xref> to have a probabilistic kind of intuition to compute the final intra-class variance in each class.</p>
        <p id="d1e2889">We have found mean intra-class variance for different classes are 0.4717, 0.4428, 0.4147, and 0.2948 for Covid, Normal, Lung Opacity, and Viral Pneumonia classes, respectively, according to the Eq. <xref rid="fd9" ref-type="disp-formula">(9)</xref>. This can be further observed from <xref rid="tbl2" ref-type="table">Table 2</xref>, that the Covid class has the highest intra-class variance, which means that in the covid class the statistics among images are comparatively dis-similar than other classes.<table-wrap position="float" id="tbl2"><label>Table 2</label><caption><p>Mean Values of Intra-class similarity and Intra-class variance for different classes of Covid 19 dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Classes</th><th align="left">Covid</th><th align="left">Normal</th><th align="left">LO</th><th align="left">VP</th></tr></thead><tbody><tr><td align="left">Intra-class similarity</td><td align="left">0.5283</td><td align="left">0.5572</td><td align="left">0.5852</td><td align="left"><bold>0.7052</bold></td></tr><tr><td align="left">Intra-class variance</td><td align="left"><bold>0.4717</bold></td><td align="left">0.4428</td><td align="left">0.4147</td><td align="left">0.2958</td></tr></tbody></table></table-wrap></p>
        <p id="d1e2901">Hence, after incorporating the notion of intra-class variance, the purpose of proposed SVD-CLAHE Boosting will be slightly different than Eq. <xref rid="fd2" ref-type="disp-formula">(2)</xref>. This is given by Eq. <xref rid="fd10" ref-type="disp-formula">(10)</xref>. <disp-formula id="fd10"><label>(10)</label><mml:math id="d1e2918" altimg="si67.svg" display="block"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">∗</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">≈</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">∗</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
</p>
        <p id="d1e3018">Here <inline-formula><mml:math id="d1e3021" altimg="si26.svg" display="inline"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the probability that a sample ‘<inline-formula><mml:math id="d1e3037" altimg="si4.svg" display="inline"><mml:mi>i</mml:mi></mml:math></inline-formula>’ will be correctly classified taken from minor class ‘<inline-formula><mml:math id="d1e3042" altimg="si17.svg" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>’. <inline-formula><mml:math id="d1e3051" altimg="si29.svg" display="inline"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the probability of a sample ‘<inline-formula><mml:math id="d1e3067" altimg="si30.svg" display="inline"><mml:mi>j</mml:mi></mml:math></inline-formula>’ will be correctly classified taken from major class ‘<inline-formula><mml:math id="d1e3072" altimg="si19.svg" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:math></inline-formula>’. Indeed, we found for this SVD-CLAHE Boosting, for minor class probability <inline-formula><mml:math id="d1e3080" altimg="si26.svg" display="inline"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> multiplied by its intra-class similarity (that is, nothing but <inline-formula><mml:math id="d1e3097" altimg="si75.svg" display="inline"><mml:mrow><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula>), should be similar to that of major class. Hence, for the Viral Pneumonia (VP) class, the probability of classification multiplied by 0.7052 (from <xref rid="tbl2" ref-type="table">Table 2</xref>), should be similar to the probability of classification for the Normal class multiplied by 0.5572. This perhaps enables us to choose fewer images from the VP class than other classes. Because in the case of the VP class, this intra-class similarity among images is higher than of other classes.</p>
        <p id="d1e3119">Viral Pneumonia (VP), which has the least number (1082) of images for training, is oversampled by SVD-based contrast enhancement, CLAHE 0.5 and CLAHE 1.0. Moreover, the proposed SVD+CLAHE 0.5 and the original image sets are also included to oversample VP (minor) class 5 times larger (1082 × 5 <inline-formula><mml:math id="d1e3122" altimg="si61.svg" display="inline"><mml:mo>=</mml:mo></mml:math></inline-formula> 5410) than previous. Now, keeping this number of image combination constant for the VP class, we try to compute what should be the number of images for other classes according to the formula of intra-class variance. We have found those numbers should be 8657, 7610, and 8125 for covid, lung opacity, and normal classes, respectively. Therefore, for covid class, we have generated thrice the size of the dataset (2923 × 3 <inline-formula><mml:math id="d1e3127" altimg="si61.svg" display="inline"><mml:mo>=</mml:mo></mml:math></inline-formula> 8769), by incorporating SVD contrast enhancement method and CLAHE 0.5 method, along with original images. Moreover, for lung opacity, we employed RUS in order to exclude 1k images from that class (4831-1000 <inline-formula><mml:math id="d1e3132" altimg="si61.svg" display="inline"><mml:mo>=</mml:mo></mml:math></inline-formula> 3831). This is followed by CLAHE 0.5 method (along with original images), to get twice of rest of the data size (3831 × 2 <inline-formula><mml:math id="d1e3138" altimg="si61.svg" display="inline"><mml:mo>=</mml:mo></mml:math></inline-formula> 7662). For normal class, we have excluded 4k images by RUS, followed by CLAHE 0.5 (along with original) to twice the remaining dataset (4096 × 2 <inline-formula><mml:math id="d1e3143" altimg="si61.svg" display="inline"><mml:mo>=</mml:mo></mml:math></inline-formula> 8192). This can be observed that the new augmented dataset (of 30,033 images) has a very similar number of images in each class, compared to the required number of images, according to its intra-class variance. The entire scheme of generating this augmented dataset is presented in <xref rid="fig3" ref-type="fig">Fig. 3</xref>.</p>
      </sec>
    </sec>
    <sec id="sec3.3">
      <label>3.3</label>
      <title>Balanced Weighted Categorical Cross Entropy (BWCCE)</title>
      <p id="d1e3156">By employing SVD-CLAHE Boosting, we generated a new augmented dataset, which is more balanced than the original dataset. However, we observed that the performance of the ResNet-50 model on this augmented dataset is still a little bit imbalanced. This can be observed from its classification report for different classes and its confusion matrix. Therefore, a new loss function is proposed to alleviate a little bit of class imbalance from the model after employing SVD-CLAHE Boosting.</p>
      <p id="d1e3158">Categorical Cross-Entropy (CCE) <xref rid="b67" ref-type="bibr">[67]</xref> loss function can be expressed as the following mathematical formula. <disp-formula id="fd11"><label>(11)</label><mml:math id="d1e3171" altimg="si81.svg" display="block"><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mo linebreak="goodbreak">−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo linebreak="goodbreak">∗</mml:mo><mml:munderover><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">∗</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where, <inline-formula><mml:math id="d1e3274" altimg="si82.svg" display="inline"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the probability that the <inline-formula><mml:math id="d1e3289" altimg="si4.svg" display="inline"><mml:mi>i</mml:mi></mml:math></inline-formula>th sample belongs to class c, <inline-formula><mml:math id="d1e3294" altimg="si84.svg" display="inline"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the class weight for <inline-formula><mml:math id="d1e3308" altimg="si4.svg" display="inline"><mml:mi>i</mml:mi></mml:math></inline-formula>th sample for total <inline-formula><mml:math id="d1e3313" altimg="si6.svg" display="inline"><mml:mi>K</mml:mi></mml:math></inline-formula> no of classes and for each class <inline-formula><mml:math id="d1e3319" altimg="si84.svg" display="inline"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is same, <inline-formula><mml:math id="d1e3333" altimg="si60.svg" display="inline"><mml:mi>N</mml:mi></mml:math></inline-formula> is the total number of samples in a particular class.</p>
      <p id="d1e3337">In case of conventional CCE, <inline-formula><mml:math id="d1e3340" altimg="si84.svg" display="inline"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is by-default chosen as <disp-formula id="fd12"><label>(12)</label><mml:math id="d1e3360" altimg="si90.svg" display="block"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></disp-formula>For class imbalance problem, Weighted Categorical Cross-Entropy (WCCE) is widely employed by researchers <xref rid="b24" ref-type="bibr">[24]</xref>, <xref rid="b25" ref-type="bibr">[25]</xref>. The expression of the WCCE loss function is given in the following equation. <disp-formula id="fd13"><label>(13)</label><mml:math id="d1e3392" altimg="si91.svg" display="block"><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mo linebreak="goodbreak">−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo linebreak="goodbreak">∗</mml:mo><mml:munderover><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">∗</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
</p>
      <p id="d1e3489">Whereas <inline-formula><mml:math id="d1e3492" altimg="si92.svg" display="inline"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the weight in each class. <inline-formula><mml:math id="d1e3502" altimg="si93.svg" display="inline"><mml:mi>β</mml:mi></mml:math></inline-formula> of the minor classes must be inversely proportional to the number of images per class and <inline-formula><mml:math id="d1e3507" altimg="si93.svg" display="inline"><mml:mi>β</mml:mi></mml:math></inline-formula> of the major classes keep remain the same, that is 1. However, the number of images in minor class may differ a lot from the same of major class. In that case, <inline-formula><mml:math id="d1e3513" altimg="si93.svg" display="inline"><mml:mi>β</mml:mi></mml:math></inline-formula> of the minor class can be a very higher value. Consequently, the CNN model will be more biased to a particular (minor) class, and it will further push the accuracy and precision of the minor class closer to 1. However, the accuracy and precision of other classes will fluctuate considerably in that case, which is not desirable. This is further demonstrated in Section <xref rid="sec4" ref-type="sec">4</xref>.</p>
      <p id="d1e3522">In order to resolve the problem mentioned above, we propose a novel BWCCE loss function in which bias weights <inline-formula><mml:math id="d1e3525" altimg="si93.svg" display="inline"><mml:mi>β</mml:mi></mml:math></inline-formula> are assigned inversely proportional to the number of images in class. However, unlike WCCE, bias weights of BWCCE are assigned based on the probability notion. The following equation can express the proposed BWCCE loss function. <disp-formula id="fd14"><label>(14)</label><mml:math id="d1e3536" altimg="si97.svg" display="block"><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>W</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mo linebreak="goodbreak">−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo linebreak="goodbreak">∗</mml:mo><mml:munderover><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">∗</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where, the bias weight in each class, <inline-formula><mml:math id="d1e3639" altimg="si92.svg" display="inline"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is given by the following equation. <disp-formula id="fd15"><label>(15)</label><mml:math id="d1e3656" altimg="si99.svg" display="block"><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where, <inline-formula><mml:math id="d1e3713" altimg="si100.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>K</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">&gt;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>
</p>
      <p id="d1e3725">Here, in Eq. <xref rid="fd15" ref-type="disp-formula">(15)</xref>, <inline-formula><mml:math id="d1e3732" altimg="si101.svg" display="inline"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the total number of samples in class c, <inline-formula><mml:math id="d1e3742" altimg="si102.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the total number of samples in the entire dataset, <inline-formula><mml:math id="d1e3761" altimg="si103.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>K</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is just a normalization factor which ensures that sum of all <inline-formula><mml:math id="d1e3775" altimg="si93.svg" display="inline"><mml:mi>β</mml:mi></mml:math></inline-formula>’s will not go beyond the value 1, <inline-formula><mml:math id="d1e3780" altimg="si6.svg" display="inline"><mml:mi>K</mml:mi></mml:math></inline-formula> is a total number of classes.</p>
      <p id="d1e3784">
        <statement id="lem1">
          <label>Lemma 1</label>
          <p id="d1e3789">
            <italic>If weight of each class in a Weighted Cross Entropy, is chosen as</italic>
            <inline-formula>
              <mml:math id="d1e3794" altimg="si106.svg" display="inline">
                <mml:mrow>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>β</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>c</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo>
                  <mml:mfrac>
                    <mml:mrow>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>K</mml:mi>
                      <mml:mo>−</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                  </mml:mfrac>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mn>1</mml:mn>
                    <mml:mo>−</mml:mo>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>n</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>c</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mo>∑</mml:mo>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>c</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>n</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>c</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                    </mml:mfrac>
                    <mml:mo>)</mml:mo>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
            <italic>, where</italic>
            <inline-formula>
              <mml:math id="d1e3853" altimg="si107.svg" display="inline">
                <mml:mrow>
                  <mml:mo>∀</mml:mo>
                  <mml:mi>c</mml:mi>
                  <mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo>
                  <mml:mn>1</mml:mn>
                  <mml:mo>,</mml:mo>
                  <mml:mn>2</mml:mn>
                  <mml:mo>,</mml:mo>
                  <mml:mo>.</mml:mo>
                  <mml:mo>.</mml:mo>
                  <mml:mi>K</mml:mi>
                </mml:mrow>
              </mml:math>
            </inline-formula>
            <italic>and</italic>
            <inline-formula>
              <mml:math id="d1e3881" altimg="si100.svg" display="inline">
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mi>K</mml:mi>
                  <mml:mo linebreak="goodbreak" linebreakstyle="after">&gt;</mml:mo>
                  <mml:mn>1</mml:mn>
                  <mml:mo>)</mml:mo>
                </mml:mrow>
              </mml:math>
            </inline-formula>
            <italic>, then sum of weights of all class will be equal to 1.</italic>
          </p>
        </statement>
      </p>
      <p id="d1e3895">That is, <disp-formula id="fd16"><label>(16)</label><mml:math id="d1e3904" altimg="si109.svg" display="block"><mml:mrow><mml:munderover><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></disp-formula>
</p>
      <p id="d1e3931">
        <statement id="d1e3932">
          <label>Proof</label>
          <p id="d1e3935">The number of classes <inline-formula><mml:math id="d1e3938" altimg="si110.svg" display="inline"><mml:mrow><mml:mo>=</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula>. We have to prove that <inline-formula><mml:math id="d1e3946" altimg="si111.svg" display="inline"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> if <inline-formula><mml:math id="d1e3975" altimg="si106.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The total number of images in entire dataset <inline-formula><mml:math id="d1e4033" altimg="si102.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> will be a same constant number, let us assume this ‘<inline-formula><mml:math id="d1e4051" altimg="si2.svg" display="inline"><mml:mi>m</mml:mi></mml:math></inline-formula>’.</p>
        </statement>
      </p>
      <p id="d1e4056">Thus, substituting the value from Eq. <xref rid="fd15" ref-type="disp-formula">(15)</xref> into Eq. <xref rid="fd16" ref-type="disp-formula">(16)</xref> we get, <disp-formula id="fd17"><label>(17)</label><mml:math id="d1e4073" altimg="si115.svg" display="block"><mml:mrow><mml:munderover><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="fd18"><label>(18)</label><mml:math id="d1e4195" altimg="si116.svg" display="block"><mml:mrow><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:munderover><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:mo linebreak="badbreak">⋯</mml:mo><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mfrac><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="fd19"><label>(19)</label><mml:math id="d1e4294" altimg="si117.svg" display="block"><mml:mrow><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:munderover><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></disp-formula>
</p>
      <p id="d1e4365">Hence, substituting the value from Eq. <xref rid="fd19" ref-type="disp-formula">(19)</xref> into Eq. <xref rid="fd18" ref-type="disp-formula">(18)</xref>, we get, <disp-formula id="fd20"><label>(20)</label><mml:math id="d1e4382" altimg="si118.svg" display="block"><mml:mrow><mml:munderover><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
</p>
      <p id="d1e4450">As <inline-formula><mml:math id="d1e4453" altimg="si119.svg" display="inline"><mml:mrow><mml:mi>K</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">≠</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, from Eq. <xref rid="fd20" ref-type="disp-formula">(20)</xref>, this is proved that sum of <inline-formula><mml:math id="d1e4467" altimg="si93.svg" display="inline"><mml:mi>β</mml:mi></mml:math></inline-formula>’s for BWCCE will be equal to 1. Hence, the proposed loss function BWCCE supports the notion of probability for assigning weights. Therefore, we have observed that the proposed loss function BWCCE produces much more balanced classification results than conventional CCE and WCCE. In every class, the results of accuracy, precision, and recall are a little more balanced (do not fluctuate too much) by the proposed BWCCE.</p>
      <p id="d1e4471">Explanation: First, let us understand why did we choose <inline-formula><mml:math id="d1e4474" altimg="si106.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Here, we assigned weights <inline-formula><mml:math id="d1e4531" altimg="si93.svg" display="inline"><mml:mi>β</mml:mi></mml:math></inline-formula> with the notion of probability. The ratio <inline-formula><mml:math id="d1e4536" altimg="si123.svg" display="inline"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></inline-formula> perhaps shows a probabilistic perspective, and thus, it is subtracted from 1, which is the total probability. For example, the Viral Pneumonia has 5410 images after the proposed SVD-CLAHE boosting out of 30,033 images. Hence, the probability that one sample from viral pneumonia will be classified correctly is <inline-formula><mml:math id="d1e4564" altimg="si124.svg" display="inline"><mml:mrow><mml:mn>5410</mml:mn><mml:mo>/</mml:mo><mml:mn>30</mml:mn><mml:mo>,</mml:mo><mml:mn>033</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>1801</mml:mn></mml:mrow></mml:math></inline-formula>. This is comparatively lesser than the average probability for four classes (i.e., 0.25). Thus, subtracting this probability value from 1 (which is the sum of total probability) and then normalizing the value gives you the intuition of how much bias weight should be chosen for viral pneumonia. That is, <inline-formula><mml:math id="d1e4586" altimg="si125.svg" display="inline"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfrac><mml:mo linebreak="goodbreak" linebreakstyle="after">∗</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>1801</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>2733</mml:mn></mml:mrow></mml:math></inline-formula> which is now a slightly higher than average weight per class i.e., 0.25. Here <inline-formula><mml:math id="d1e4622" altimg="si126.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mn>4</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>1</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>3</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is the normalization factor, because number of classes is 4. In <xref rid="lem1" ref-type="statement">Lemma 1</xref>, it is already proved that sum of <inline-formula><mml:math id="d1e4647" altimg="si93.svg" display="inline"><mml:mi>β</mml:mi></mml:math></inline-formula>’s will be equal to one, if the normalization factor is <inline-formula><mml:math id="d1e4652" altimg="si103.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>K</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>. Similarly, if you compute the <inline-formula><mml:math id="d1e4666" altimg="si93.svg" display="inline"><mml:mi>β</mml:mi></mml:math></inline-formula> for the major class, we will get a lesser weight, i.e., less than the average weight 0.25. Unlike WCCE, the weights of different classes for BWCCE are not exactly chosen inversely proportional to number of images in that class, rather it is chosen based on the notion of probability of classes. Thus, weights for different classes do not deviate too much from the average weight 0.25. Consequently, deep neural networks will not be biased too much towards one particular class for BWCCE. In the results and analysis section, this can be further observed that ResNet-50 with BWCCE loss function not only improves classification results a little bit, but also, it enables the neural network to provide a very stable validation graph, which is a significant improvement. Furthermore, we have observed that ResNet-50 model along with BWCCE loss function, converges a little bit faster, during training. This justifies the necessity of employing the proposed loss function.</p>
    </sec>
    <sec id="sec3.4">
      <label>3.4</label>
      <title>Physical interpretation of proposed methodology</title>
      <p id="d1e4675">From <xref rid="fig4" ref-type="fig">Fig. 4</xref>, the necessity of the proposed methodology can be easily visualized. <xref rid="fig4" ref-type="fig">Figs. 4</xref>(a)–<xref rid="fig4" ref-type="fig">4</xref>(d) represent the distribution of major (+) and minor (-) class based on several images. Clearly, <xref rid="fig4" ref-type="fig">Fig. 4</xref>(a) shows that there is a huge class imbalance in the original dataset, which means the number of images in major class and minor class is very different. <xref rid="fig4" ref-type="fig">Fig. 4</xref>(b) indicates the distribution of major class peak is considerably reduced after employing RUS, whereas the distribution of minor class is same. Because images are only excluded from major classes, but not from minor classes. <xref rid="fig4" ref-type="fig">Fig. 4</xref>(c) represents major and minor class distribution after employing SVD-CLAHE oversampling. Clearly peak of both major and minor classes is enhanced after incorporating SVD-CLAHE oversampling, so that the major class distribution is now comparable to the distribution of minor class. <xref rid="fig4" ref-type="fig">Fig. 4</xref>(d) represents major and minor class distribution after employing the BWCCE loss function. Clearly, the BWCCE function was deployed to alleviate a little bit of class imbalance present after SVD-CLAHE Boosting (i.e., RUS + SVD-CLAHE Over-sampling). <xref rid="fig4" ref-type="fig">Fig. 4</xref>(d) shows that both classes’ distribution becomes exactly equivalent because by the BWCCE loss function, the minor class is given more weightage (or, bias) than the major class. Thus, effectively the distribution of both classes becomes equivalent. <xref rid="fig4" ref-type="fig">Figs. 4</xref>(e)–<xref rid="fig4" ref-type="fig">4</xref>(h) represents major and minor classes cluster visualization. Increasing the number of inner circles in a class is equivalent to increasing the number of samples in that class. Increasing the size of the inner circle in a class is equivalent to increasing the weight bias in that class. In <xref rid="fig4" ref-type="fig">Fig. 4</xref>(f), this can be visualized that the number of inner circles in major class decreases after RUS, which means the number of samples is decreased from the major class by RUS. <xref rid="fig4" ref-type="fig">Fig. 4</xref>(g) indicates that the number of inner circles in both major and minor classes is considerably increased after incorporating SVD-CLAHE over-sampling. Now the size of the bigger circle for major and minor classes is kind of comparable. However, still, there was little class imbalance present after employing SVD-CLAHE Boosting. <xref rid="fig4" ref-type="fig">Fig. 4</xref>(h) represents the final cluster after employing SVD-CLAHE Boosting and BWCCE loss function. Clearly, the size of the inner circles of the minor class is increased here, which means in the BWCCE loss function, the minor class is getting more biased than the major class. Hence, finally, the bigger circle of both major and minor classes in <xref rid="fig4" ref-type="fig">Fig. 4</xref>(h) is now equivalent. This reveals that our proposed methodology makes the distribution of major class and minor class similar; hence it alleviates the class imbalance problem completely. All these diagrams of distribution and cluster are entirely imaginary. They have not been taken from any statistical plot of the dataset, and they are just employed for better visualization of the proposed methodology.<fig id="fig4"><label>Fig. 4</label><caption><p><bold>Visualization of entire proposed methodology</bold><xref rid="fig4" ref-type="fig">Fig. 4</xref>(a) presents the distribution of major and minor classes in the original dataset, based on a number of images. <xref rid="fig4" ref-type="fig">Figs. 4</xref>(b) to <xref rid="fig4" ref-type="fig">4</xref>(d) present the changes in distribution after employing the proposed methodology. <xref rid="fig4" ref-type="fig">Fig. 4</xref>(e) indicates the cluster representation of major and minor classes for the original dataset. <xref rid="fig4" ref-type="fig">Fig. 4</xref>(f) to <xref rid="fig4" ref-type="fig">Fig. 4</xref>(h) indicate the changes in cluster representation in major and minor classes after employing RUS, SVD-CLAHE Over-sampling, and BWCCE loss function, respectively. All these diagrams of distribution and cluster representation are completely imaginary and have not been taken from any statistical plot of the dataset.</p></caption><graphic xlink:href="gr4_lrg"/></fig></p>
    </sec>
  </sec>
  <sec id="sec4">
    <label>4</label>
    <title>Experimental results and analysis</title>
    <p id="d1e4743">The following points can summarize the entire experimental results and analysis section:</p>
    <p id="d1e4745">
      <list list-type="simple" id="d1e4747">
        <list-item id="lst11">
          <label>1.</label>
          <p id="d1e4751">First, experimental results of several CNN models like InceptionV3, DenseNet-121, VGG 16, VGG 19, ResNet-50 etc. are compared for this Covid CXR dataset and shown in <xref rid="tbl3" ref-type="table">Table 3</xref>. Moreover, two existing models (only for Covid detection), Covid-Lite by M.Shiddhartha et al. <xref rid="b35" ref-type="bibr">[35]</xref>, and Covid-Net by Wang et al. <xref rid="b36" ref-type="bibr">[36]</xref>, are implemented on this CXR dataset. This can be observed from <xref rid="tbl3" ref-type="table">Table 3</xref> that ResNet-50 and VGG-19 have better results than other models. The reason was already mentioned in Section <xref rid="sec4.1" ref-type="sec">4.1</xref>.</p>
        </list-item>
        <list-item id="lst12">
          <label>2.</label>
          <p id="d1e4778">We have chosen the ResNet-50 model as the proposed model. Moreover, different data-augmentation techniques are employed to prepare three more augmented datasets and the original dataset. These augmented datasets are: (I) an augmented dataset by traditional data-augmentation technique (i.e., by rotation, flipping, zooming, etc.), (II) an augmented dataset prepared by SVD-CLAHE Boosting in which a number of images per class is the same, and finally (III) the proposed augmented dataset by SVD-CLAHE Boosting, in which a number of images per class is chosen based on intra-class variance. After that, the ResNet-50 model is performed on these four datasets. Furthermore, different loss functions are incorporated in the ResNet-50 model for the proposed augmented dataset. All the results of the ResNet-50 model experiments are presented in <xref rid="tbl4" ref-type="table">Table 4</xref>.</p>
        </list-item>
        <list-item id="lst13">
          <label>3.</label>
          <p id="d1e4787">We have chosen another CNN model, VGG-19, to check the proposed framework’s validity. Thus, the same experiments are also conducted on the VGG-19 model. The results of VGG-19 are presented in <xref rid="tbl5" ref-type="table">Table 5</xref>.</p>
        </list-item>
        <list-item id="lst14">
          <label>4.</label>
          <p id="d1e4796">A comparison of training and validation graphs for both ResNet-50 and VGG-19 models are shown in <xref rid="fig5" ref-type="fig">Fig. 5</xref> and <xref rid="fig6" ref-type="fig">Fig. 6</xref>, respectively.</p>
        </list-item>
      </list>
    </p>
    <sec id="sec4.1">
      <label>4.1</label>
      <title>Training specification</title>
      <p id="d1e4811">
        <list list-type="simple" id="d1e4813">
          <list-item id="lst15">
            <label>•</label>
            <p id="d1e4817">All of the experiments have been performed using Tensorflow and Keras. Tesla P100 GPU, provided by Google Colab Pro service, is used to train the models. 25 GB RAM was also available from the service to prevent RAM crashes during the experiments.</p>
          </list-item>
          <list-item id="lst16">
            <label>•</label>
            <p id="d1e4822">Adams-optimizer is employed as the preferred choice of optimizer for all the experiments with a learning rate of <inline-formula><mml:math id="d1e4825" altimg="si130.svg" display="inline"><mml:mrow><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, beta 1 value of 0.9, beta 2 value of 0.999, and epsilon value of <inline-formula><mml:math id="d1e4840" altimg="si130.svg" display="inline"><mml:mrow><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>.</p>
          </list-item>
          <list-item id="lst17">
            <label>•</label>
            <p id="d1e4857">A batch size of 32 was used while training CNN models. All the experiments are performed with 80% training data and 20% testing data of the total dataset. This training-testing data splitting is done randomly. Out of entire training dataset, 20 percent are further chosen as a validation set randomly, to check the validity of model performance.</p>
          </list-item>
          <list-item id="lst18">
            <label>•</label>
            <p id="d1e4862">All the images are resized into 224 × 224 before feeding them into the CNN model.</p>
          </list-item>
          <list-item id="lst19">
            <label>•</label>
            <p id="d1e4867">Transfer learning approach is deployed for training all the standard CNN models. All these models are pretrained on the ImageNet dataset <xref rid="b68" ref-type="bibr">[68]</xref> (by Keras) and finetuned on CXR dataset. However, existing Covid-Lite and Covid-Net models are trained from the scratch since their pre-trained weights are not available on Keras.</p>
          </list-item>
          <list-item id="lst20">
            <label>•</label>
            <p id="d1e4876">Early stopping call-back to stop the model training (if the model starts to overfit) is employed based on the validation loss. Patience of 5 epochs is incorporated in the Early Stopping call-back, which minimizes the validation loss. That means, if the validation loss keeps increasing for 5 consecutive epochs, then the call-back will automatically stop the model training in order to avoid overfitting. Weights are restored to the last checkpoints where the model was not overfitting.</p>
          </list-item>
          <list-item id="lst21">
            <label>•</label>
            <p id="d1e4881">All the experiments are performed with the same hyperparameters and with the same aforementioned methodology during training.</p>
          </list-item>
        </list>
      </p>
    </sec>
    <sec id="sec4.2">
      <label>4.2</label>
      <title>Evaluation metrics</title>
      <p id="d1e4888">Standard ‘accuracy’ metric is used as evaluation metric to check how accurately the model is working. Besides that precision, recall as well as F1 Score are also evaluated in order to make sure the model is not affected by any class imbalance problem. The mathematical formulae for the aforementioned metrics are given below: <disp-formula id="fd21"><label>(21)</label><mml:math id="d1e4897" altimg="si132.svg" display="block"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
<disp-formula id="fd22"><label>(22)</label><mml:math id="d1e4945" altimg="si133.svg" display="block"><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
<disp-formula id="fd23"><label>(23)</label><mml:math id="d1e4988" altimg="si134.svg" display="block"><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
<disp-formula id="fd24"><label>(24)</label><mml:math id="d1e5052" altimg="si135.svg" display="block"><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo linebreak="badbreak">∗</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo linebreak="badbreak">∗</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
</p>
      <p id="d1e5143"><inline-formula><mml:math id="d1e5145" altimg="si136.svg" display="inline"><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula> stands for True Positives, i.e., instances labeled as positive and classified correctly as a positive instance. <inline-formula><mml:math id="d1e5153" altimg="si137.svg" display="inline"><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> stands for True negatives, which represent instances where the actual label is negative and is predicted likewise. <inline-formula><mml:math id="d1e5161" altimg="si138.svg" display="inline"><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula> stands for False positives where the instance belongs to the negative label but is predicted to be positive. <inline-formula><mml:math id="d1e5169" altimg="si139.svg" display="inline"><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> stands for False negatives, where instances are positive but are predicted as negative. Besides these metrics, we have also computed AUC metric which is an important metric for measuring class imbalance.</p>
    </sec>
    <sec id="sec4.3">
      <label>4.3</label>
      <title>Results analysis and discussions</title>
      <p id="d1e5181">First, various deep convolutional neural networks, namely, ReNet50, InceptionV3, DenseNet-121, VGG-16, VGG-19, Covid-Lite, proposed by M. Siddhartha et al. <xref rid="b35" ref-type="bibr">[35]</xref>, and Covid-Net by Wang et al. <xref rid="b36" ref-type="bibr">[36]</xref> are performed to the imbalance dataset. This provides us an insight into the problem by observing how this dataset responds to naive methods of Deep Learning. This can be further observed from <xref rid="tbl3" ref-type="table">Table 3</xref>. The exact training specification mentioned in Section <xref rid="sec4.1" ref-type="sec">4.1</xref> is followed for all the experiments, except the Covid-Net and Covid-Lite models.</p>
      <p id="d1e5204">For the Covid-Net model, a deep learning framework with PEPX blocks proposed by Wang et al. <xref rid="b36" ref-type="bibr">[36]</xref>, has been trained on the original dataset from scratch with 183,695,108 parameters with all of them as trainable. A batch size of 8, for 30 epochs is employed and call-back of early stopping is taken with the patience of 5. The model ran for 25 epochs to give an accuracy of 0.87 on the testing data. Although this model is trained from scratch on the original dataset, it provides good results in terms of accuracy, F1 score, precision, and recall which can be observed from <xref rid="tbl3" ref-type="table">Table 3</xref>. However, we have found that this model is still a slightly complicated model (having many no. of layers) and thus, inducing overfitting in this CXR dataset.<table-wrap position="float" id="tbl3"><label>Table 3</label><caption><p>Comparisons of Mean values of evaluation metrics for various existing models with the proposed framework on the test set.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Methodology</th><th align="left">F1 score</th><th align="left">Accuracy</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">AUC</th></tr></thead><tbody><tr><td align="left">Covid Lite</td><td align="left">0.85</td><td align="left">0.85</td><td align="left">0.84</td><td align="left">0.87</td><td align="left">0.88</td></tr><tr><td align="left">by <xref rid="b35" ref-type="bibr">[35]</xref></td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">Covid-Net</td><td align="left">0.87</td><td align="left">0.87</td><td align="left">0.88</td><td align="left">0.86</td><td align="left">0.88</td></tr><tr><td align="left">by <xref rid="b36" ref-type="bibr">[36]</xref></td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">Inception-V3</td><td align="left">0.78</td><td align="left">0.8</td><td align="left">0.77</td><td align="left">0.79</td><td align="left">0.80</td></tr><tr><td align="left">Xception</td><td align="left">0.79</td><td align="left">0.8</td><td align="left">0.81</td><td align="left">0.77</td><td align="left">0.78</td></tr><tr><td align="left">DenseNet-121</td><td align="left">0.84</td><td align="left">0.82</td><td align="left">0.82</td><td align="left">0.86</td><td align="left">0.89</td></tr><tr><td align="left">VGG-16</td><td align="left">0.88</td><td align="left">0.88</td><td align="left">0.89</td><td align="left">0.87</td><td align="left">0.90</td></tr><tr><td align="left">VGG-19</td><td align="left">0.91</td><td align="left">0.90</td><td align="left">0.92</td><td align="left">0.90</td><td align="left">0.91</td></tr><tr><td align="left">ResNet-50</td><td align="left">0.90</td><td align="left">0.89</td><td align="left">0.91</td><td align="left">0.91</td><td align="left">0.92</td></tr><tr><td colspan="6"><hr/></td></tr><tr><td align="left"><bold>VGG-19 with</bold></td><td align="left"><bold>0.95</bold></td><td align="left"><bold>0.94</bold></td><td align="left"><bold>0.96</bold></td><td align="left"><bold>0.95</bold></td><td align="left"><bold>0.96</bold></td></tr><tr><td align="left"><bold>proposed</bold></td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left"><bold>framework</bold></td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td colspan="6"><hr/></td></tr><tr><td align="left"><bold>ResNet-50 with</bold></td><td align="left"><bold>0.95</bold></td><td align="left"><bold>0.94</bold></td><td align="left"><bold>0.96</bold></td><td align="left"><bold>0.95</bold></td><td align="left"><bold>0.96</bold></td></tr><tr><td align="left"><bold>proposed</bold></td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left"><bold>framework</bold></td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr></tbody></table></table-wrap></p>
      <p id="d1e5216">For Covid-Lite model, original dataset is pre-processed by CLAHE and white balance image processing techniques, as mentioned by M.Shiddhartha et al. <xref rid="b35" ref-type="bibr">[35]</xref> in their paper. The dataset is then split into training, validation and testing sets with a ratio of 0.7, 0.1 and 0.2. Thereafter, they are fed into the Covid-Lite model to train it from scratch for 50 epochs with a batch size of 32. It was observed that training and validation parameters are highly fluctuating, and the number of trainable parameters is 1,019,396. The performance by their model is little bit poor. Due to the class imbalance problem present in the original dataset, there is huge fluctuation of results in their classification report. Their proposed model is only suitable for a balanced dataset.</p>
      <p id="d1e5222">Standard CNN models like Inception-V3, Xception, and DenseNet-121 have little bit poor results than other models due to the huge complexity of their models. ResNet50, VGG 16, and VGG 19 performed better than other models in terms of accuracy, precision, recall, and F1 score, which can be observed in <xref rid="tbl3" ref-type="table">Table 3</xref>. VGG 16 and VGG 19 are comparatively simpler models than Inceptionv3, Xception, and DenseNet-121. Therefore, those VGG models have no overfitting in their model performances. ResNet-50 is another model that shows promising results, shown in <xref rid="tbl3" ref-type="table">Table 3</xref>, despite its complicated structure. We believe that the skip connection present in the model can alleviate the problem of vanishing gradients, and it also reduces the complexity of the model a little bit.<fig id="fig5"><label>Fig. 5</label><caption><p>Comparisons of performances of several experiments on ResNet-50 Model (a) training accuracy, (b) training F1 score, (c) training loss, (d) validation accuracy, (e) validation F1 score, (f) validation loss. The experiments are employed are already labeled in the diagram, those are ResNet-50 on original dataset, ResNet-50 on augmented dataset (by SVD-CLAHE boosting) with equal no of images per class, ResNet-50 on augmented dataset (by proposed SVD-CLAHE boosting), ResNet-50+ SVD-CLAHE Boosting +WCCE, proposed method (ResNet-50+ SVD-CLAHE Boosting +BWCCE).</p></caption><graphic xlink:href="gr5_lrg"/></fig><fig id="fig6"><label>Fig. 6</label><caption><p>Comparisons of performances of several experiments on VGG-19 Model (a) training accuracy, (b) training F1 score, (c) training loss, (d) validation accuracy, (e) validation F1 score, (f) validation loss. The experiments are employed are already labeled in the diagram, those are VGG-19 on original dataset, VGG-19 on augmented dataset (by SVD-CLAHE boosting) with equal no of images per class, VGG-19 on augmented dataset (by proposed SVD-CLAHE boosting), VGG-19+ SVD-CLAHE Boosting +WCCE, proposed method (VGG-19+ SVD-CLAHE Boosting +BWCCE).</p></caption><graphic xlink:href="gr6_lrg"/></fig></p>
      <p id="d1e5236">We have chosen ResNet-50 as proposed model over VGG-19 and VGG-16, because we have observed the graphs of validation loss and validation accuracy (which can be observed in <xref rid="fig5" ref-type="fig">Fig. 5</xref> and <xref rid="fig6" ref-type="fig">Fig. 6</xref>) fluctuates more for VGG-19. Moreover, according to our perspective, due to higher number of layers (50) present in ResNet-50, it can accomplish very complicated task and consequently its performance graph of validation is more stable than that of VGG-19. However, we have chosen VGG-19 for checking the validity of the proposed framework, because it is having the second best results after ResNet-50.<table-wrap position="float" id="tbl4"><label>Table 4</label><caption><p>Ablation Study of of various experiments conducted on ResNet-50 model on the testing set.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Methodology</th><th align="left">F1-score</th><th align="left">Accuracy</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">AUC</th></tr></thead><tbody><tr><td align="left">ResNet-50 with</td><td align="left">0.90</td><td align="left">0.89</td><td align="left">0.91</td><td align="left">0.91</td><td align="left">0.943</td></tr><tr><td align="left">original dataset</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">ResNet-50 with conven-</td><td align="left">0.89</td><td align="left">0.88</td><td align="left">0.91</td><td align="left">0.87</td><td align="left">0.907</td></tr><tr><td align="left">tional data augmentation</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">(flipping,rotation)</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">ResNet-50 with SVD-</td><td align="left">0.92</td><td align="left">0.91</td><td align="left">0.91</td><td align="left">0.93</td><td align="left">0.937</td></tr><tr><td align="left">CLAHE Boosting</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">with equal no of images</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">ResNet-50 with proposed</td><td align="left">0.94</td><td align="left">0.93</td><td align="left">0.96</td><td align="left">0.93</td><td align="left">0.966</td></tr><tr><td align="left">SVD-CLAHE Boosting</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">ResNet-50 with proposed</td><td align="left">0.94</td><td align="left">0.94</td><td align="left">0.96</td><td align="left">0.93</td><td align="left">0.965</td></tr><tr><td align="left">SVD-CLAHE Boosting</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">with WCCE</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td colspan="6"><hr/></td></tr><tr><td align="left"><bold>Proposed method</bold></td><td align="left"><bold>0.95</bold></td><td align="left"><bold>0.94</bold></td><td align="left"><bold>0.96</bold></td><td align="left"><bold>0.95</bold></td><td align="left"><bold>0.967</bold></td></tr><tr><td align="left">(ResNet-50+SVD-CLAHE</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">Boosting+BWCCE)</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr></tbody></table></table-wrap></p>
      <p id="d1e5248">The following experiments are conducted with the ResNet-50 model. Important observations from all these experiments are presented below.</p>
      <p id="d1e5250">
        <list list-type="simple" id="d1e5252">
          <list-item id="lst22">
            <label>•</label>
            <p id="d1e5256">First, conventional data-augmentation techniques like rotation, flipping (horizontal and vertical), zooming, etc., are employed for making an augmented dataset using the ImageDataGenerator, available in Tensorflow. After that, the ResNet-50 model is performed on that augmented dataset. The ResNet-50 model with this conventional data augmentation produces poor results than the ResNet-50 model on the original dataset. This can be further noticed in <xref rid="tbl4" ref-type="table">Table 4</xref>. The reason we already discussed in Section <xref rid="sec4" ref-type="sec">4</xref>. According to our perspective, these data-augmentation techniques are not feature invariant for the final classification task.</p>
          </list-item>
          <list-item id="lst23">
            <label>•</label>
            <p id="d1e5270">SVD-CLAHE Boosting is performed to produce a balanced augmented dataset in which each class has an equal number of images. After that, the ResNet-50 model is performed on this augmented dataset. The performance of this data augmentation is slightly improved than the performance of ResNet-50 on the original dataset. F1 score, accuracy, and recall factor are improved by 1-2%, which can be observed in <xref rid="tbl4" ref-type="table">Table 4</xref>. However, by observing its confusion matrix, we found that little class imbalance exists.</p>
          </list-item>
          <list-item id="lst24">
            <label>•</label>
            <p id="d1e5279">Proposed SVD-CLAHE Boosting (data-augmentation) method is performed to produce a third augmented dataset. The number of images per class is chosen according to the intra-class variance, which is already explained in Section <xref rid="sec4" ref-type="sec">4</xref>. ResNet-50 model on this augmented dataset (by proposed SVD-CLAHE Boosting) has performed very effectively, as it produced accuracy, precision, recall, and F1 scores 93%, 96%, 93%, and 94% respectively, given in <xref rid="tbl4" ref-type="table">Table 4</xref>. This is an approx 4%–5% improvement from ResNet-50 with the original dataset. This is a significant improvement or boosting performance compared to other existing methods mentioned in <xref rid="tbl3" ref-type="table">Table 3</xref>. Thus, this justifies the proposed data-augmentation method and its name ‘SVD-CLAHE Boosting’.</p>
          </list-item>
          <list-item id="lst25">
            <label>•</label>
            <p id="d1e5297">Although the proposed SVD-CLAHE boosting works very effectively, we still have found a little bit of class imbalance, which is noticed by its confusion matrix and from the classification report. Therefore, the Weighted Categorical Cross-Entropy (WCCE) loss function is incorporated in the ResNet-50 model, performed on the proposed augmented dataset. We have observed a little bit of improvement (1%) in the accuracy, compared to the performance of ResNet-50 on the proposed augmented dataset, after employing WCCE. However, from <xref rid="fig5" ref-type="fig">Fig. 5</xref>, this can be visualized that WCCE further incorporates a little bit of fluctuation of validation graphs for accuracy, F1 score, and loss. This is undesirable. Overall, we have observed that WCCE pushes the accuracy and precision of the model slightly more for minor classes, however, this induces a little bit of fluctations in the performances among other classes.</p>
          </list-item>
          <list-item id="lst26">
            <label>•</label>
            <p id="d1e5306">Therefore, we develop a novel Balanced Weighted Categorical Cross-Entropy loss function (BWCCE) to alleviate the limitation mentioned earlier. This can be observed from <xref rid="tbl4" ref-type="table">Table 4</xref> that BWCCE performs slightly better than conventional CCE and WCCE for this proposed augmented dataset. ResNet-50 model (along with BWCCE loss function) on the proposed augmented (30k) dataset has achieved the accuracy, precision, recall, and F1 scores of 94%, 96%, 95%, and 95%, respectively. Overall, there is a 1% improvement in accuracy, 1% improvement in F1 score, and 2% improvement in recall than the ResNet-50 model after SVD-CLAHE Boosting. Additionally, from <xref rid="fig5" ref-type="fig">Fig. 5</xref> this can be observed that the proposed framework (that is, ResNet-50+SVD-CLAHE Boosting+BWCCE) provides the least validation loss; moreover, it provides validation accuracy and F1 score graph (shown in <xref rid="fig5" ref-type="fig">Fig. 5</xref>), more stable than that of WCCE and CCE. This justifies the effectiveness of the BWCCE loss function.</p>
          </list-item>
        </list>
      </p>
      <p id="d1e5320">Furthermore, this has also been observed from <xref rid="fig7" ref-type="fig">Fig. 7</xref>(a) that the ResNet-50 model with BWCCE loss function converges much faster (by only 9 epochs) than that of CCE (20 epochs) and WCCE (13 epochs). <xref rid="fig7" ref-type="fig">Fig. 7</xref>(b) also shows that the average time taken per epoch during training by the proposed BWCCE loss function is significantly lesser than that of other loss functions. Hence, this can be concluded that the proposed BWCCE loss function simplifies the optimization problem of a complicated CNN model; thus, it enables the CNN model to converge much faster during training.<fig id="fig7"><label>Fig. 7</label><caption><p>(a) No of Epochs of Convergence for ResNet-50 with different loss functions on proposed augmented dataset, (b) Average time taken per epochs in sec, for ResNet-50 with different loss functions on proposed augmented dataset.</p></caption><graphic xlink:href="gr7_lrg"/></fig><fig id="fig8"><label>Fig. 8</label><caption><p>Confusion matrix for different experiments on ResNet-50 model, (a) Confusion Matrix (CM1) for ResNet-50 on original dataset, (b) Confusion Matrix (CM2) for ResNet-50+SVD-CLAHE Boosting, (c) Confusion Matrix (CM3) for Proposed methodology (ResNet-50+SVD-CLAHE Boosting+ BWCCE).</p></caption><graphic xlink:href="gr8_lrg"/></fig></p>
      <p id="d1e5334">The confusion matrix in <xref rid="fig8" ref-type="fig">Fig. 8</xref> helps us to visualize and analyze the prediction performances of different experiments in a better way. This gives a definite comparison of the evaluation metrics in a matrix form. For example, the values 705, 1012, 1804, and 270 are the True Positive values for the ResNet-50 model on an original dataset for the classes Covid, Lung Opacity, Normal and Viral Pneumonia, respectively. By comparing <xref rid="fig8" ref-type="fig">Fig. 8</xref>(a) and <xref rid="fig8" ref-type="fig">Fig. 8</xref>(b), we can visualize that true-positives for Lung Opacity (LO) is significantly increased, whereas Normal and VP classes are a little bit improved while utilizing SVD-CLAHE Boosting. However, it fails to improve performances for overall classes (not for Covid class). Because still, little class imbalance was present after SVD-CLAHE Boosting. We incorporated the BWCCE loss function on the ResNet-50 model to resolve this problem. <xref rid="fig8" ref-type="fig">Fig. 8</xref>(c) indicates that BWCCE enables us to improve overall performances in all classes (except little degradation in the VP class). Thus, it gives a balanced result compared to other experiments. The number of true positives in normal and Covid classes is significantly increased. Due to the number of normal class images being higher, it enables the model to achieve its best performance in terms of accuracy, precision, recall, and F1 score, which we already observed in <xref rid="tbl4" ref-type="table">Table 4</xref>.</p>
      <p id="d1e5358">In order to check the validity of the proposed framework, we have conducted the same experiments on the VGG-19 model as well. Because we wanted to check whether the proposed framework works well in a generalized way or not. An ablation study of various experiments on VGG-19 (on the testing dataset), is presented in <xref rid="tbl5" ref-type="table">Table 5</xref>. First, VGG-19 model is experimented with conventional data augmentation with rotation, flipping, etc. However, those data-augmentation techniques are not featured invariant; thus, it produces worse results. This data-augmentation has degraded the overall performance by 3%–8%, shown in <xref rid="tbl5" ref-type="table">Table 5</xref>. The performance on the second augmented dataset, however, has improved a bit. Overall, there is a 2%–4% improvement from this augmented dataset to the original dataset. Furthermore, proposed SVD-CLAHE Boosting based on intra-class variance works very efficiently on the VGG-19 model as well. From <xref rid="tbl5" ref-type="table">Table 5</xref>, this can be observed that SVD-CLAHE Boosting produces an overall 2%–3% boosting of performance, over VGG 19 on the original dataset. Hence, SVD-CLAHE Boosting works in a generalized way, to the best of our knowledge. In order to reduce a little bit of class imbalance still present after SVD-CLAHE Boosting, we employed the proposed loss function BWCCE, incorporated in the VGG-19 model. Experimental results in <xref rid="tbl5" ref-type="table">Table 5</xref> reveal that BWCCE provides a little bit better and more balanced results than the previous results. <xref rid="tbl5" ref-type="table">Table 5</xref> shows that the proposed framework (i.e., VGG-19+ SVD-CLAHE Boosting + BWCCE) provides 1%–3% further improvement in performances than VGG-19 on the proposed augmented dataset (by SVD-CLAHE Boosting).<table-wrap position="float" id="tbl5"><label>Table 5</label><caption><p>Ablation Study of various experiments conducted on VGG-19 model on the testing set.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Methodology</th><th align="left">F1-score</th><th align="left">Accuracy</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">AUC</th></tr></thead><tbody><tr><td align="left">VGG-19 with</td><td align="left">0.91</td><td align="left">0.90</td><td align="left">0.92</td><td align="left">0.90</td><td align="left">0.928</td></tr><tr><td align="left">original dataset</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">VGG-19 with conven-</td><td align="left">0.85</td><td align="left">0.86</td><td align="left">0.89</td><td align="left">0.82</td><td align="left">0.883</td></tr><tr><td align="left">tional data augmentation</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">(flipping,rotation)</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">VGG-19 with SVD-</td><td align="left">0.93</td><td align="left">0.93</td><td align="left">0.95</td><td align="left">0.94</td><td align="left">0.950</td></tr><tr><td align="left">CLAHE Boosting</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">with equal no of images</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">VGG-19 with proposed</td><td align="left">0.94</td><td align="left">0.93</td><td align="left">0.95</td><td align="left">0.92</td><td align="left">0.961</td></tr><tr><td align="left">SVD-CLAHE Boosting</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">VGG-19 with proposed</td><td align="left">0.95</td><td align="left">0.94</td><td align="left">0.95</td><td align="left">0.94</td><td align="left">0.964</td></tr><tr><td align="left">SVD-CLAHE Boosting</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">with WCCE</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td colspan="6"><hr/></td></tr><tr><td align="left"><bold>Proposed method</bold></td><td align="left"><bold>0.95</bold></td><td align="left"><bold>0.94</bold></td><td align="left"><bold>0.96</bold></td><td align="left"><bold>0.95</bold></td><td align="left"><bold>0.967</bold></td></tr><tr><td align="left">(VGG-19+SVD-CLAHE</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">Boosting+BWCCE)</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr></tbody></table></table-wrap></p>
      <p id="d1e5383">Moreover, this can be observed from <xref rid="fig5" ref-type="fig">Fig. 5</xref> that by WCCE loss functions, the VGG-19 model provides a little bit spike (or oscillation) in all validation results (accuracy, F1 score, and loss graph). This is undesirable. However, experiments with the BWCCE loss function in <xref rid="fig5" ref-type="fig">Fig. 5</xref> provides more stable results than WCCE. Overall, the BWCCE loss function provides the best accuracy, F1 score, and least loss metrics than other experiments for both models, shown in <xref rid="fig5" ref-type="fig">Fig. 5</xref>, <xref rid="fig6" ref-type="fig">Fig. 6</xref>. This is a significant improvement. Therefore, the proposed BWCCE justified its usefulness for the VGG-19 model. Hence, the proposed framework (i.e., data-augmentation by SVD-CLAHE boosting and proposed loss function BWCCE) works very efficiently for both models.</p>
    </sec>
  </sec>
  <sec id="sec5">
    <label>5</label>
    <title>Conclusion</title>
    <p id="d1e5402">A novel data-augmentation method SVD-CLAHE Boosting was proposed to solve a multi-class classification task for a highly imbalanced Covid-19 CXR dataset. First, the ResNet-50 model was proposed for the classification task; moreover, ResNet-50 was employed on the augmented dataset (by proposed SVD-CLAHE Boosting), which provided better results than other models. The proposed augmented dataset (of 30,033 images) was able to distinguish between different classes more efficiently, thereby generalizing more and avoiding the issue of class imbalance problem significantly. This augmented dataset was shared online publicly on the Kaggle website. The boosting performance of evaluation metrics further justified its name ‘SVD-CLAHE Boosting’. In order to check the validity of the proposed data-augmentation method, same experiment was conducted on the VGG-19 model as well. The experimental result suggested that the proposed SVD-CLAHE Boosting worked well in a generalized way. However, a little bit of a class imbalance problem was still observed in the model (in their confusion matrix); thus, a novel BWCCE loss function was employed, which assigned the bias weightage of several classes based on the notion of probability. This novel loss function provided improved performance metrics for both ResNet-50 and VGG-19 models and provided a stable validation graph for loss function and accuracy compared to WCCE. Another attractive characteristic of the proposed loss function was that for complicated models (ResNet-50), it converged a little bit faster than conventional CCE and WCCE. Hence, this can be concluded that the proposed data-augmentation technique, ’SVD-CLAHE boosting’ along with BWCCE loss function, worked efficiently for both the ResNet-50 and VGG-19 models for this imbalanced imbalance Covid-19 dataset. The mean of evaluation metrics indicated that the proposed framework outperformed any other methods both qualitatively and quantitatively.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Declaration of Competing Interest</title>
    <p id="d1e5407">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
  </sec>
</body>
<back>
  <ref-list id="bib1">
    <title>References</title>
    <ref id="b1">
      <label>1</label>
      <element-citation publication-type="journal" id="sb1">
        <person-group person-group-type="author">
          <name>
            <surname>of the International</surname>
            <given-names>C.S.G.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The species severe acute respiratory syndrome-related coronavirus: classifying 2019-nCoV and naming it SARS-CoV-2</article-title>
        <source>Nat. Microbiol.</source>
        <volume>5</volume>
        <issue>4</issue>
        <year>2020</year>
        <fpage>536</fpage>
        <pub-id pub-id-type="pmid">32123347</pub-id>
      </element-citation>
    </ref>
    <ref id="b2">
      <label>2</label>
      <element-citation publication-type="journal" id="sb2">
        <person-group person-group-type="author">
          <name>
            <surname>Koh</surname>
            <given-names>H.K.</given-names>
          </name>
          <name>
            <surname>Geller</surname>
            <given-names>A.C.</given-names>
          </name>
          <name>
            <surname>VanderWeele</surname>
            <given-names>T.J.</given-names>
          </name>
        </person-group>
        <article-title>Deaths from COVID-19</article-title>
        <source>JAMA</source>
        <volume>325</volume>
        <issue>2</issue>
        <year>2021</year>
        <fpage>133</fpage>
        <lpage>134</lpage>
        <pub-id pub-id-type="pmid">33331884</pub-id>
      </element-citation>
    </ref>
    <ref id="b3">
      <label>3</label>
      <element-citation publication-type="journal" id="sb3">
        <person-group person-group-type="author">
          <name>
            <surname>Shirani</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Shayganfar</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Hajiahmadi</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>COVID-19 pneumonia: a pictorial review of CT findings and differential diagnosis</article-title>
        <source>Egypt. J. Radiol. Nucl. Med.</source>
        <volume>52</volume>
        <issue>1</issue>
        <year>2021</year>
        <fpage>1</fpage>
        <lpage>8</lpage>
      </element-citation>
    </ref>
    <ref id="b4">
      <label>4</label>
      <element-citation publication-type="journal" id="sb4">
        <person-group person-group-type="author">
          <name>
            <surname>Peng</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>Transmission routes of 2019-nCoV and controls in dental practice</article-title>
        <source>Int. J. Oral Sci.</source>
        <volume>12</volume>
        <issue>1</issue>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>6</lpage>
        <pub-id pub-id-type="pmid">31900382</pub-id>
      </element-citation>
    </ref>
    <ref id="b5">
      <label>5</label>
      <element-citation publication-type="journal" id="sb5">
        <person-group person-group-type="author">
          <name>
            <surname>Smyrlaki</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Ekman</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Lentini</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Rufino de Sousa</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Papanicolaou</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Vondracek</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Aarum</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Safari</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Muradrasoli</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Rothfuchs</surname>
            <given-names>A.G.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Massive and rapid COVID-19 testing is feasible by extraction-free SARS-CoV-2 RT-PCR</article-title>
        <source>Nature Commun.</source>
        <volume>11</volume>
        <issue>1</issue>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="pmid">31911652</pub-id>
      </element-citation>
    </ref>
    <ref id="b6">
      <label>6</label>
      <element-citation publication-type="journal" id="sb6">
        <person-group person-group-type="author">
          <name>
            <surname>Fang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Ying</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Pang</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <article-title>Sensitivity of chest CT for COVID-19: comparison to RT-PCR</article-title>
        <source>Radiology</source>
        <volume>296</volume>
        <issue>2</issue>
        <year>2020</year>
        <fpage>E115</fpage>
        <lpage>E117</lpage>
        <pub-id pub-id-type="pmid">32073353</pub-id>
      </element-citation>
    </ref>
    <ref id="b7">
      <label>7</label>
      <element-citation publication-type="journal" id="sb7">
        <person-group person-group-type="author">
          <name>
            <surname>Nayak</surname>
            <given-names>S.R.</given-names>
          </name>
          <name>
            <surname>Nayak</surname>
            <given-names>D.R.</given-names>
          </name>
          <name>
            <surname>Sinha</surname>
            <given-names>U.</given-names>
          </name>
          <name>
            <surname>Arora</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Pachori</surname>
            <given-names>R.B.</given-names>
          </name>
        </person-group>
        <article-title>Application of deep learning techniques for detection of COVID-19 cases using chest X-ray images: A comprehensive study</article-title>
        <source>Biomed. Signal Process. Control</source>
        <volume>64</volume>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">102365</object-id>
      </element-citation>
    </ref>
    <ref id="b8">
      <label>8</label>
      <element-citation publication-type="book" id="sb8">
        <person-group person-group-type="author">
          <name>
            <surname>Ulhaq</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Khan</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Gomes</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Paul</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <part-title>Computer vision for COVID-19 control: a survey</part-title>
        <year>2020</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:2004.09420" id="interref1">arXiv:2004.09420</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b9">
      <label>9</label>
      <element-citation publication-type="journal" id="sb9">
        <person-group person-group-type="author">
          <name>
            <surname>Soltan</surname>
            <given-names>A.A.</given-names>
          </name>
          <name>
            <surname>Kouchaki</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Kiyasseh</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Taylor</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Hussain</surname>
            <given-names>Z.B.</given-names>
          </name>
          <name>
            <surname>Peto</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Brent</surname>
            <given-names>A.J.</given-names>
          </name>
          <name>
            <surname>Eyre</surname>
            <given-names>D.W.</given-names>
          </name>
          <name>
            <surname>Clifton</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>Artificial intelligence driven assessment of routinely collected healthcare data is an effective screening test for COVID-19 in patients presenting to hospital</article-title>
        <source>MedRxiv</source>
        <year>2020</year>
      </element-citation>
    </ref>
    <ref id="b10">
      <label>10</label>
      <element-citation publication-type="journal" id="sb10">
        <person-group person-group-type="author">
          <name>
            <surname>Buda</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Maki</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Mazurowski</surname>
            <given-names>M.A.</given-names>
          </name>
        </person-group>
        <article-title>A systematic study of the class imbalance problem in convolutional neural networks</article-title>
        <source>Neural Netw.</source>
        <volume>106</volume>
        <year>2018</year>
        <fpage>249</fpage>
        <lpage>259</lpage>
        <pub-id pub-id-type="pmid">30092410</pub-id>
      </element-citation>
    </ref>
    <ref id="b11">
      <label>11</label>
      <element-citation publication-type="journal" id="sb11">
        <person-group person-group-type="author">
          <name>
            <surname>Seiffert</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Khoshgoftaar</surname>
            <given-names>T.M.</given-names>
          </name>
          <name>
            <surname>Van Hulse</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Napolitano</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>RUSBoost: A hybrid approach to alleviating class imbalance</article-title>
        <source>IEEE Trans. Syst. Man Cybern.</source>
        <volume>40</volume>
        <issue>1</issue>
        <year>2009</year>
        <fpage>185</fpage>
        <lpage>197</lpage>
      </element-citation>
    </ref>
    <ref id="b12">
      <label>12</label>
      <element-citation publication-type="book" id="sb12">
        <person-group person-group-type="author">
          <name>
            <surname>Ronneberger</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Fischer</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Brox</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <part-title>U-net: Convolutional networks for biomedical image segmentation</part-title>
        <source>Int. Conf. Med. Imag. Comput. Comput.-Assisted Interv.</source>
        <year>2015</year>
        <publisher-name>Springer</publisher-name>
        <fpage>234</fpage>
        <lpage>241</lpage>
      </element-citation>
    </ref>
    <ref id="b13">
      <label>13</label>
      <element-citation publication-type="journal" id="sb13">
        <person-group person-group-type="author">
          <name>
            <surname>Chawla</surname>
            <given-names>N.V.</given-names>
          </name>
          <name>
            <surname>Bowyer</surname>
            <given-names>K.W.</given-names>
          </name>
          <name>
            <surname>Hall</surname>
            <given-names>L.O.</given-names>
          </name>
          <name>
            <surname>Kegelmeyer</surname>
            <given-names>W.P.</given-names>
          </name>
        </person-group>
        <article-title>SMOTE: synthetic minority over-sampling technique</article-title>
        <source>J. Artificial Intelligence Res.</source>
        <volume>16</volume>
        <year>2002</year>
        <fpage>321</fpage>
        <lpage>357</lpage>
      </element-citation>
    </ref>
    <ref id="b14">
      <label>14</label>
      <element-citation publication-type="journal" id="sb14">
        <person-group person-group-type="author">
          <name>
            <surname>Chaudhari</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Agarwal</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Bhateja</surname>
            <given-names>V.</given-names>
          </name>
        </person-group>
        <article-title>Data augmentation for cancer classification in oncogenomics: an improved KNN based approach</article-title>
        <source>Evol. Intell.</source>
        <volume>14</volume>
        <issue>2</issue>
        <year>2021</year>
        <fpage>489</fpage>
        <lpage>498</lpage>
      </element-citation>
    </ref>
    <ref id="b15">
      <label>15</label>
      <element-citation publication-type="book" id="sb15">
        <person-group person-group-type="author">
          <name>
            <surname>Deepshikha</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Naman</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <source>Removing Class Imbalance using Polarity-GAN: An Uncertainty Sampling Approach</source>
        <year>2020</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:2012.04937" id="interref2">arXiv:2012.04937</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b16">
      <label>16</label>
      <element-citation publication-type="journal" id="sb16">
        <person-group person-group-type="author">
          <name>
            <surname>Waheed</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Goyal</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Gupta</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Khanna</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Al-Turjman</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Pinheiro</surname>
            <given-names>P.R.</given-names>
          </name>
        </person-group>
        <article-title>Covidgan: data augmentation using auxiliary classifier gan for improved covid-19 detection</article-title>
        <source>IEEE Access</source>
        <volume>8</volume>
        <year>2020</year>
        <fpage>91916</fpage>
        <lpage>91923</lpage>
        <pub-id pub-id-type="pmid">34192100</pub-id>
      </element-citation>
    </ref>
    <ref id="b17">
      <label>17</label>
      <element-citation publication-type="journal" id="sb17">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Duan</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Kang</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Qiu</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Class-imbalanced deep learning via a class-balanced ensemble</article-title>
        <source>IEEE Trans. Neural Netw. Learn. Syst.</source>
        <year>2021</year>
      </element-citation>
    </ref>
    <ref id="b18">
      <label>18</label>
      <element-citation publication-type="book" id="sb18">
        <person-group person-group-type="author">
          <name>
            <surname>Drummond</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Holte</surname>
            <given-names>R.C.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>C4. 5, class imbalance, and cost sensitivity: why under-sampling beats over-sampling</part-title>
        <source>Workshop on Learning from Imbalanced Datasets II</source>
        <volume>11</volume>
        <year>2003</year>
        <publisher-name>Citeseer</publisher-name>
        <fpage>1</fpage>
        <lpage>8</lpage>
      </element-citation>
    </ref>
    <ref id="b19">
      <label>19</label>
      <element-citation publication-type="journal" id="sb19">
        <person-group person-group-type="author">
          <name>
            <surname>Kang</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>A noise-filtered under-sampling scheme for imbalanced classification</article-title>
        <source>IEEE Trans. Cybern.</source>
        <volume>47</volume>
        <issue>12</issue>
        <year>2016</year>
        <fpage>4263</fpage>
        <lpage>4274</lpage>
        <pub-id pub-id-type="pmid">28113413</pub-id>
      </element-citation>
    </ref>
    <ref id="b20">
      <label>20</label>
      <element-citation publication-type="book" id="sb20">
        <person-group person-group-type="author">
          <name>
            <surname>Freund</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Schapire</surname>
            <given-names>R.E.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>Experiments with a new boosting algorithm</part-title>
        <source>Icml</source>
        <volume>vol. 96</volume>
        <year>1996</year>
        <publisher-name>Citeseer</publisher-name>
        <fpage>148</fpage>
        <lpage>156</lpage>
      </element-citation>
    </ref>
    <ref id="b21">
      <label>21</label>
      <element-citation publication-type="book" id="sb21">
        <person-group person-group-type="author">
          <name>
            <surname>Chawla</surname>
            <given-names>N.V.</given-names>
          </name>
          <name>
            <surname>Lazarevic</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Hall</surname>
            <given-names>L.O.</given-names>
          </name>
          <name>
            <surname>Bowyer</surname>
            <given-names>K.W.</given-names>
          </name>
        </person-group>
        <part-title>Smoteboost: Improving prediction of the minority class in boosting</part-title>
        <source>European Conference on Principles of Data Mining and Knowledge Discovery</source>
        <year>2003</year>
        <publisher-name>Springer</publisher-name>
        <fpage>107</fpage>
        <lpage>119</lpage>
      </element-citation>
    </ref>
    <ref id="b22">
      <label>22</label>
      <element-citation publication-type="journal" id="sb22">
        <person-group person-group-type="author">
          <name>
            <surname>Sun</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Fujita</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Fu</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Ai</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <article-title>Class-imbalanced dynamic financial distress prediction based on Adaboost-SVM ensemble combined with SMOTE and time weighting</article-title>
        <source>Inf. Fusion</source>
        <volume>54</volume>
        <year>2020</year>
        <fpage>128</fpage>
        <lpage>144</lpage>
      </element-citation>
    </ref>
    <ref id="b23">
      <label>23</label>
      <element-citation publication-type="journal" id="sb23">
        <person-group person-group-type="author">
          <name>
            <surname>Khan</surname>
            <given-names>S.H.</given-names>
          </name>
          <name>
            <surname>Hayat</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Bennamoun</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Sohel</surname>
            <given-names>F.A.</given-names>
          </name>
          <name>
            <surname>Togneri</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Cost-sensitive learning of deep feature representations from imbalanced data</article-title>
        <source>IEEE Trans. Neural Netw. Learn. Syst.</source>
        <volume>29</volume>
        <issue>8</issue>
        <year>2017</year>
        <fpage>3573</fpage>
        <lpage>3587</lpage>
        <pub-id pub-id-type="pmid">28829320</pub-id>
      </element-citation>
    </ref>
    <ref id="b24">
      <label>24</label>
      <element-citation publication-type="book" id="sb24">
        <person-group person-group-type="author">
          <name>
            <surname>Özdemir</surname>
            <given-names>Ö.</given-names>
          </name>
          <name>
            <surname>Sönmez</surname>
            <given-names>E.B.</given-names>
          </name>
        </person-group>
        <part-title>Weighted cross-entropy for unbalanced data with application on COVID X-ray images</part-title>
        <source>2020 Innovations in Intelligent Systems and Applications Conference</source>
        <conf-name>ASYU</conf-name>
        <year>2020</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>1</fpage>
        <lpage>6</lpage>
      </element-citation>
    </ref>
    <ref id="b25">
      <label>25</label>
      <element-citation publication-type="journal" id="sb25">
        <person-group person-group-type="author">
          <name>
            <surname>Ho</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Wookey</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>The real-world-weight cross-entropy loss function: Modeling the costs of mislabeling</article-title>
        <source>IEEE Access</source>
        <volume>8</volume>
        <year>2019</year>
        <fpage>4806</fpage>
        <lpage>4813</lpage>
      </element-citation>
    </ref>
    <ref id="b26">
      <label>26</label>
      <element-citation publication-type="journal" id="sb26">
        <person-group person-group-type="author">
          <name>
            <surname>Pasupa</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Vatathanavaro</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Tungjitnob</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Convolutional neural networks based focal loss for class imbalance problem: A case study of canine red blood cells morphology classification</article-title>
        <source>J. Ambient Intell. Humaniz. Comput.</source>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>17</lpage>
      </element-citation>
    </ref>
    <ref id="b27">
      <label>27</label>
      <element-citation publication-type="book" id="sb27">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <part-title>Combination of ResNet and center loss based metric learning for handwritten Chinese character recognition</part-title>
        <source>2017 14th IAPR International Conference on Document Analysis and Recognition</source>
        <conf-name>ICDAR</conf-name>
        <volume>5</volume>
        <year>2017</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>25</fpage>
        <lpage>29</lpage>
      </element-citation>
    </ref>
    <ref id="b28">
      <label>28</label>
      <element-citation publication-type="book" id="sb28">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <part-title>Distribution-balanced loss for multi-label classification in long-tailed datasets</part-title>
        <source>European Conference on Computer Vision</source>
        <year>2020</year>
        <publisher-name>Springer</publisher-name>
        <fpage>162</fpage>
        <lpage>178</lpage>
      </element-citation>
    </ref>
    <ref id="b29">
      <label>29</label>
      <mixed-citation publication-type="other" id="sb29">S. Ryou, S.-G. Jeong, P. Perona, Anchor loss: Modulating loss scale based on prediction difficulty, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 5992–6001.</mixed-citation>
    </ref>
    <ref id="b30">
      <label>30</label>
      <mixed-citation publication-type="other" id="sb30">Y. Cui, M. Jia, T.-Y. Lin, Y. Song, S. Belongie, Class-balanced loss based on effective number of samples, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 9268–9277.</mixed-citation>
    </ref>
    <ref id="b31">
      <label>31</label>
      <element-citation publication-type="book" id="sb31">
        <person-group person-group-type="author">
          <name>
            <surname>Salehi</surname>
            <given-names>S.S.M.</given-names>
          </name>
          <name>
            <surname>Erdogmus</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Gholipour</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <part-title>Tversky loss function for image segmentation using 3D fully convolutional deep networks</part-title>
        <source>International Workshop on Machine Learning in Medical Imaging</source>
        <year>2017</year>
        <publisher-name>Springer</publisher-name>
        <fpage>379</fpage>
        <lpage>387</lpage>
      </element-citation>
    </ref>
    <ref id="b32">
      <label>32</label>
      <element-citation publication-type="journal" id="sb32">
        <person-group person-group-type="author">
          <name>
            <surname>Yeung</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Sala</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Schönlieb</surname>
            <given-names>C.-B.</given-names>
          </name>
          <name>
            <surname>Rundo</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Unified focal loss: Generalising dice and cross entropy-based losses to handle class imbalanced medical image segmentation</article-title>
        <source>Comput. Med. Imaging Graph.</source>
        <volume>95</volume>
        <year>2022</year>
        <object-id pub-id-type="publisher-id">102026</object-id>
      </element-citation>
    </ref>
    <ref id="b33">
      <label>33</label>
      <element-citation publication-type="journal" id="sb33">
        <person-group person-group-type="author">
          <name>
            <surname>Chowdhury</surname>
            <given-names>M.E.</given-names>
          </name>
          <name>
            <surname>Rahman</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Khandakar</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Mazhar</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Kadir</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Mahbub</surname>
            <given-names>Z.B.</given-names>
          </name>
          <name>
            <surname>Islam</surname>
            <given-names>K.R.</given-names>
          </name>
          <name>
            <surname>Khan</surname>
            <given-names>M.S.</given-names>
          </name>
          <name>
            <surname>Iqbal</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Al Emadi</surname>
            <given-names>N.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Can AI help in screening viral and COVID-19 pneumonia?</article-title>
        <source>IEEE Access</source>
        <volume>8</volume>
        <year>2020</year>
        <fpage>132665</fpage>
        <lpage>132676</lpage>
      </element-citation>
    </ref>
    <ref id="b34">
      <label>34</label>
      <element-citation publication-type="journal" id="sb34">
        <person-group person-group-type="author">
          <name>
            <surname>Rahman</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Khandakar</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Qiblawey</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Tahir</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Kiranyaz</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Kashem</surname>
            <given-names>S.B.A.</given-names>
          </name>
          <name>
            <surname>Islam</surname>
            <given-names>M.T.</given-names>
          </name>
          <name>
            <surname>Al Maadeed</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Zughaier</surname>
            <given-names>S.M.</given-names>
          </name>
          <name>
            <surname>Khan</surname>
            <given-names>M.S.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Exploring the effect of image enhancement techniques on COVID-19 detection using chest X-ray images</article-title>
        <source>Comput. Biol. Med.</source>
        <volume>132</volume>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">104319</object-id>
      </element-citation>
    </ref>
    <ref id="b35">
      <label>35</label>
      <element-citation publication-type="book" id="sb35">
        <person-group person-group-type="author">
          <name>
            <surname>Siddhartha</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Santra</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <part-title>COVIDLite: A depth-wise separable deep neural network with white balance and CLAHE for detection of COVID-19</part-title>
        <year>2020</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:2006.13873" id="interref3">arXiv:2006.13873</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b36">
      <label>36</label>
      <element-citation publication-type="journal" id="sb36">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Z.Q.</given-names>
          </name>
          <name>
            <surname>Wong</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Covid-net: A tailored deep convolutional neural network design for detection of covid-19 cases from chest X-ray images</article-title>
        <source>Sci. Rep.</source>
        <volume>10</volume>
        <issue>1</issue>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="pmid">31913322</pub-id>
      </element-citation>
    </ref>
    <ref id="b37">
      <label>37</label>
      <element-citation publication-type="journal" id="sb37">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Lam</surname>
            <given-names>H.-K.</given-names>
          </name>
          <name>
            <surname>Jia</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>MANet: A two-stage deep learning method for classification of COVID-19 from Chest X-ray images</article-title>
        <source>Neurocomputing</source>
        <volume>443</volume>
        <year>2021</year>
        <fpage>96</fpage>
        <lpage>105</lpage>
        <pub-id pub-id-type="pmid">33753962</pub-id>
      </element-citation>
    </ref>
    <ref id="b38">
      <label>38</label>
      <element-citation publication-type="journal" id="sb38">
        <person-group person-group-type="author">
          <name>
            <surname>Itti</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Koch</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Niebur</surname>
            <given-names>E.</given-names>
          </name>
        </person-group>
        <article-title>A model of saliency-based visual attention for rapid scene analysis</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <volume>20</volume>
        <issue>11</issue>
        <year>1998</year>
        <fpage>1254</fpage>
        <lpage>1259</lpage>
      </element-citation>
    </ref>
    <ref id="b39">
      <label>39</label>
      <element-citation publication-type="journal" id="sb39">
        <person-group person-group-type="author">
          <name>
            <surname>Toğaçar</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Ergen</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Cömert</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>COVID-19 detection using deep learning models to exploit Social Mimic Optimization and structured chest X-ray images using fuzzy color and stacking approaches</article-title>
        <source>Comput. Biol. Med.</source>
        <volume>121</volume>
        <year>2020</year>
        <object-id pub-id-type="publisher-id">103805</object-id>
      </element-citation>
    </ref>
    <ref id="b40">
      <label>40</label>
      <element-citation publication-type="book" id="sb40">
        <person-group person-group-type="author">
          <name>
            <surname>Howard</surname>
            <given-names>A.G.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Kalenichenko</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Weyand</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Andreetto</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Adam</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <part-title>Mobilenets: Efficient convolutional neural networks for mobile vision applications</part-title>
        <year>2017</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:1704.04861" id="interref4">arXiv:1704.04861</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b41">
      <label>41</label>
      <element-citation publication-type="book" id="sb41">
        <person-group person-group-type="author">
          <name>
            <surname>Iandola</surname>
            <given-names>F.N.</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Moskewicz</surname>
            <given-names>M.W.</given-names>
          </name>
          <name>
            <surname>Ashraf</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Dally</surname>
            <given-names>W.J.</given-names>
          </name>
          <name>
            <surname>Keutzer</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <part-title>Squeezenet: AlexNet-level accuracy with 50x fewer parameters and¡ 0.5 MB model size</part-title>
        <year>2016</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:1602.07360" id="interref5">arXiv:1602.07360</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b42">
      <label>42</label>
      <element-citation publication-type="book" id="sb42">
        <person-group person-group-type="author">
          <name>
            <surname>Mamalakis</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Swift</surname>
            <given-names>A.J.</given-names>
          </name>
          <name>
            <surname>Vorselaars</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Ray</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Weeks</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Clayton</surname>
            <given-names>R.H.</given-names>
          </name>
          <name>
            <surname>Mackenzie</surname>
            <given-names>L.S.</given-names>
          </name>
          <name>
            <surname>Banerjee</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <part-title>DenResCov-19: A deep transfer learning network for robust automatic classification of COVID-19, pneumonia, and tuberculosis from X-rays</part-title>
        <year>2021</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:2104.04006" id="interref6">arXiv:2104.04006</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b43">
      <label>43</label>
      <mixed-citation publication-type="other" id="sb43">K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770–778.</mixed-citation>
    </ref>
    <ref id="b44">
      <label>44</label>
      <mixed-citation publication-type="other" id="sb44">G. Huang, Z. Liu, L. Van Der Maaten, K.Q. Weinberger, Densely connected convolutional networks, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 4700–4708.</mixed-citation>
    </ref>
    <ref id="b45">
      <label>45</label>
      <element-citation publication-type="journal" id="sb45">
        <person-group person-group-type="author">
          <name>
            <surname>Das</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Santosh</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Pal</surname>
            <given-names>U.</given-names>
          </name>
        </person-group>
        <article-title>Truncated inception net: COVID-19 outbreak screening using chest X-rays</article-title>
        <source>Australas. Phys. Eng. Sci. Med.</source>
        <volume>43</volume>
        <issue>3</issue>
        <year>2020</year>
        <fpage>915</fpage>
        <lpage>925</lpage>
      </element-citation>
    </ref>
    <ref id="b46">
      <label>46</label>
      <mixed-citation publication-type="other" id="sb46">C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, Z. Wojna, Rethinking the inception architecture for computer vision, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 2818–2826.</mixed-citation>
    </ref>
    <ref id="b47">
      <label>47</label>
      <element-citation publication-type="journal" id="sb47">
        <person-group person-group-type="author">
          <name>
            <surname>Minaee</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Kafieh</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Sonka</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Yazdani</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Soufi</surname>
            <given-names>G.J.</given-names>
          </name>
        </person-group>
        <article-title>Deep-covid: Predicting covid-19 from chest X-ray images using deep transfer learning</article-title>
        <source>Med. Image Anal.</source>
        <volume>65</volume>
        <year>2020</year>
        <object-id pub-id-type="publisher-id">101794</object-id>
      </element-citation>
    </ref>
    <ref id="b48">
      <label>48</label>
      <element-citation publication-type="journal" id="sb48">
        <person-group person-group-type="author">
          <name>
            <surname>Jaiswal</surname>
            <given-names>A.K.</given-names>
          </name>
          <name>
            <surname>Tiwari</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Kumar</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Gupta</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Khanna</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Rodrigues</surname>
            <given-names>J.J.</given-names>
          </name>
        </person-group>
        <article-title>Identifying pneumonia in chest X-rays: a deep learning approach</article-title>
        <source>Measurement</source>
        <volume>145</volume>
        <year>2019</year>
        <fpage>511</fpage>
        <lpage>518</lpage>
      </element-citation>
    </ref>
    <ref id="b49">
      <label>49</label>
      <element-citation publication-type="journal" id="sb49">
        <person-group person-group-type="author">
          <name>
            <surname>Khan</surname>
            <given-names>A.I.</given-names>
          </name>
          <name>
            <surname>Shah</surname>
            <given-names>J.L.</given-names>
          </name>
          <name>
            <surname>Bhat</surname>
            <given-names>M.M.</given-names>
          </name>
        </person-group>
        <article-title>CoroNet: A deep neural network for detection and diagnosis of COVID-19 from chest X-ray images</article-title>
        <source>Comput. Methods Programs Biomed.</source>
        <volume>196</volume>
        <year>2020</year>
        <object-id pub-id-type="publisher-id">105581</object-id>
      </element-citation>
    </ref>
    <ref id="b50">
      <label>50</label>
      <element-citation publication-type="journal" id="sb50">
        <person-group person-group-type="author">
          <name>
            <surname>Narin</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Kaya</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Pamuk</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>Automatic detection of coronavirus disease (covid-19) using X-ray images and deep convolutional neural networks</article-title>
        <source>Pattern Anal. Appl.</source>
        <year>2021</year>
        <fpage>1</fpage>
        <lpage>14</lpage>
      </element-citation>
    </ref>
    <ref id="b51">
      <label>51</label>
      <element-citation publication-type="journal" id="sb51">
        <person-group person-group-type="author">
          <name>
            <surname>Chandra</surname>
            <given-names>T.B.</given-names>
          </name>
          <name>
            <surname>Verma</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Singh</surname>
            <given-names>B.K.</given-names>
          </name>
          <name>
            <surname>Jain</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Netam</surname>
            <given-names>S.S.</given-names>
          </name>
        </person-group>
        <article-title>Coronavirus disease (COVID-19) detection in chest X-ray images using majority voting based classifier ensemble</article-title>
        <source>Expert Syst. Appl.</source>
        <volume>165</volume>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">113909</object-id>
      </element-citation>
    </ref>
    <ref id="b52">
      <label>52</label>
      <element-citation publication-type="journal" id="sb52">
        <person-group person-group-type="author">
          <name>
            <surname>Han</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Hong</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Cong</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <article-title>Accurate screening of COVID-19 using attention-based deep 3D multiple instance learning</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <volume>39</volume>
        <issue>8</issue>
        <year>2020</year>
        <fpage>2584</fpage>
        <lpage>2594</lpage>
        <pub-id pub-id-type="pmid">32730211</pub-id>
      </element-citation>
    </ref>
    <ref id="b53">
      <label>53</label>
      <element-citation publication-type="journal" id="sb53">
        <person-group person-group-type="author">
          <name>
            <surname>Harmon</surname>
            <given-names>S.A.</given-names>
          </name>
          <name>
            <surname>Sanford</surname>
            <given-names>T.H.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Turkbey</surname>
            <given-names>E.B.</given-names>
          </name>
          <name>
            <surname>Roth</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Myronenko</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Anderson</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Amalou</surname>
            <given-names>A.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Artificial intelligence for the detection of COVID-19 pneumonia on chest CT using multinational datasets</article-title>
        <source>Nature Commun.</source>
        <volume>11</volume>
        <issue>1</issue>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>7</lpage>
        <pub-id pub-id-type="pmid">31911652</pub-id>
      </element-citation>
    </ref>
    <ref id="b54">
      <label>54</label>
      <element-citation publication-type="journal" id="sb54">
        <person-group person-group-type="author">
          <name>
            <surname>Ozturk</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Talo</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Yildirim</surname>
            <given-names>E.A.</given-names>
          </name>
          <name>
            <surname>Baloglu</surname>
            <given-names>U.B.</given-names>
          </name>
          <name>
            <surname>Yildirim</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Acharya</surname>
            <given-names>U.R.</given-names>
          </name>
        </person-group>
        <article-title>Automated detection of COVID-19 cases using deep neural networks with X-ray images</article-title>
        <source>Comput. Biol. Med.</source>
        <volume>121</volume>
        <year>2020</year>
        <object-id pub-id-type="publisher-id">103792</object-id>
      </element-citation>
    </ref>
    <ref id="b55">
      <label>55</label>
      <element-citation publication-type="journal" id="sb55">
        <person-group person-group-type="author">
          <name>
            <surname>Krizhevsky</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G.E.</given-names>
          </name>
        </person-group>
        <article-title>Imagenet classification with deep convolutional neural networks</article-title>
        <source>Adv. Neural Inf. Process. Syst.</source>
        <volume>25</volume>
        <year>2012</year>
      </element-citation>
    </ref>
    <ref id="b56">
      <label>56</label>
      <element-citation publication-type="book" id="sb56">
        <person-group person-group-type="author">
          <name>
            <surname>Simonyan</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Zisserman</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <part-title>Very deep convolutional networks for large-scale image recognition</part-title>
        <year>2014</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:1409.1556" id="interref7">arXiv:1409.1556</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b57">
      <label>57</label>
      <mixed-citation publication-type="other" id="sb57">C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, A. Rabinovich, Going deeper with convolutions, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1–9.</mixed-citation>
    </ref>
    <ref id="b58">
      <label>58</label>
      <element-citation publication-type="book" id="sb58">
        <person-group person-group-type="author">
          <name>
            <surname>Cohen</surname>
            <given-names>J.P.</given-names>
          </name>
          <name>
            <surname>Morrison</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Dao</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Roth</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Duong</surname>
            <given-names>T.Q.</given-names>
          </name>
          <name>
            <surname>Ghassemi</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <part-title>Covid-19 image data collection: Prospective predictions are the future</part-title>
        <year>2020</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:2006.11988" id="interref8">arXiv:2006.11988</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b59">
      <label>59</label>
      <mixed-citation publication-type="other" id="sb59">F. Chollet, Xception: Deep learning with depthwise separable convolutions, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 1251–1258.</mixed-citation>
    </ref>
    <ref id="b60">
      <label>60</label>
      <element-citation publication-type="book" id="sb60">
        <person-group person-group-type="author">
          <name>
            <surname>Wall</surname>
            <given-names>M.E.</given-names>
          </name>
          <name>
            <surname>Rechtsteiner</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Rocha</surname>
            <given-names>L.M.</given-names>
          </name>
        </person-group>
        <part-title>Singular value decomposition and principal component analysis</part-title>
        <source>A Practical Approach to Microarray Data Analysis</source>
        <year>2003</year>
        <publisher-name>Springer</publisher-name>
        <fpage>91</fpage>
        <lpage>109</lpage>
      </element-citation>
    </ref>
    <ref id="b61">
      <label>61</label>
      <element-citation publication-type="journal" id="sb61">
        <person-group person-group-type="author">
          <name>
            <surname>Demirel</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Ozcinar</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Anbarjafari</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Satellite image contrast enhancement using discrete wavelet transform and singular value decomposition</article-title>
        <source>IEEE Geosci. Remote Sens. Lett.</source>
        <volume>7</volume>
        <issue>2</issue>
        <year>2009</year>
        <fpage>333</fpage>
        <lpage>337</lpage>
      </element-citation>
    </ref>
    <ref id="b62">
      <label>62</label>
      <element-citation publication-type="journal" id="sb62">
        <person-group person-group-type="author">
          <name>
            <surname>Roy</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Lal</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Kini</surname>
            <given-names>J.R.</given-names>
          </name>
        </person-group>
        <article-title>Novel color normalization method for Hematoxylin &amp; Eosin stained histopathology images</article-title>
        <source>IEEE Access</source>
        <volume>7</volume>
        <year>2019</year>
        <fpage>28982</fpage>
        <lpage>28998</lpage>
      </element-citation>
    </ref>
    <ref id="b63">
      <label>63</label>
      <element-citation publication-type="book" id="sb63">
        <person-group person-group-type="author">
          <name>
            <surname>Roy</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Panda</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Jangid</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <part-title>Modified reinhard algorithm for color normalization of colorectal cancer histopathology images</part-title>
        <source>2021 29th European Signal Processing Conference</source>
        <conf-name>EUSIPCO</conf-name>
        <year>2021</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>1231</fpage>
        <lpage>1235</lpage>
      </element-citation>
    </ref>
    <ref id="b64">
      <label>64</label>
      <element-citation publication-type="journal" id="sb64">
        <person-group person-group-type="author">
          <name>
            <surname>Pizer</surname>
            <given-names>S.M.</given-names>
          </name>
          <name>
            <surname>Amburn</surname>
            <given-names>E.P.</given-names>
          </name>
          <name>
            <surname>Austin</surname>
            <given-names>J.D.</given-names>
          </name>
          <name>
            <surname>Cromartie</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Geselowitz</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Greer</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>ter Haar Romeny</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Zimmerman</surname>
            <given-names>J.B.</given-names>
          </name>
          <name>
            <surname>Zuiderveld</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Adaptive histogram equalization and its variations</article-title>
        <source>Comput. Vis. Graph. Image Process.</source>
        <volume>39</volume>
        <issue>3</issue>
        <year>1987</year>
        <fpage>355</fpage>
        <lpage>368</lpage>
      </element-citation>
    </ref>
    <ref id="b65">
      <label>65</label>
      <element-citation publication-type="book" id="sb65">
        <person-group person-group-type="author">
          <name>
            <surname>Gonzales</surname>
            <given-names>R.C.</given-names>
          </name>
          <name>
            <surname>Woods</surname>
            <given-names>R.E.</given-names>
          </name>
        </person-group>
        <part-title>Digital image processing</part-title>
        <edition>second ed.</edition>
        <year>2001</year>
        <publisher-name>Prentice Hall</publisher-name>
      </element-citation>
    </ref>
    <ref id="b66">
      <label>66</label>
      <element-citation publication-type="journal" id="sb66">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Bovik</surname>
            <given-names>A.C.</given-names>
          </name>
        </person-group>
        <article-title>A universal image quality index</article-title>
        <source>IEEE Signal Process. Lett.</source>
        <volume>9</volume>
        <issue>3</issue>
        <year>2002</year>
        <fpage>81</fpage>
        <lpage>84</lpage>
      </element-citation>
    </ref>
    <ref id="b67">
      <label>67</label>
      <element-citation publication-type="book" id="sb67">
        <person-group person-group-type="author">
          <name>
            <surname>Jadon</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <part-title>A survey of loss functions for semantic segmentation</part-title>
        <source>2020 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology</source>
        <conf-name>CIBCB</conf-name>
        <year>2020</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>1</fpage>
        <lpage>7</lpage>
      </element-citation>
    </ref>
    <ref id="b68">
      <label>68</label>
      <element-citation publication-type="book" id="sb68">
        <person-group person-group-type="author">
          <name>
            <surname>Deng</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Dong</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Socher</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>L.-J.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Fei-Fei</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <part-title>Imagenet: A large-scale hierarchical image database</part-title>
        <source>2009 IEEE Conference on Computer Vision and Pattern Recognition</source>
        <year>2009</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>248</fpage>
        <lpage>255</lpage>
      </element-citation>
    </ref>
  </ref-list>
</back>
