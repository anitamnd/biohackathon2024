<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Commun Biol</journal-id>
    <journal-id journal-id-type="iso-abbrev">Commun Biol</journal-id>
    <journal-title-group>
      <journal-title>Communications Biology</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2399-3642</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9038756</article-id>
    <article-id pub-id-type="publisher-id">3320</article-id>
    <article-id pub-id-type="doi">10.1038/s42003-022-03320-0</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Hidden Markov modeling for maximum probability neuron reconstruction</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8987-3486</contrib-id>
        <name>
          <surname>Athey</surname>
          <given-names>Thomas L.</given-names>
        </name>
        <address>
          <email>tathey1@jhu.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4607-6807</contrib-id>
        <name>
          <surname>Tward</surname>
          <given-names>Daniel J.</given-names>
        </name>
        <address>
          <email>DTward@mednet.ucla.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Mueller</surname>
          <given-names>Ulrich</given-names>
        </name>
        <address>
          <email>umuelle3@jhmi.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2487-6237</contrib-id>
        <name>
          <surname>Vogelstein</surname>
          <given-names>Joshua T.</given-names>
        </name>
        <address>
          <email>jovo@jhu.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff6">6</xref>
        <xref ref-type="aff" rid="Aff7">7</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Miller</surname>
          <given-names>Michael I.</given-names>
        </name>
        <address>
          <email>mim@jhu.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff6">6</xref>
        <xref ref-type="aff" rid="Aff7">7</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.21107.35</institution-id><institution-id institution-id-type="ISNI">0000 0001 2171 9311</institution-id><institution>Department of Biomedical Engineering, </institution><institution>Johns Hopkins University, </institution></institution-wrap>Baltimore, MD USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.21107.35</institution-id><institution-id institution-id-type="ISNI">0000 0001 2171 9311</institution-id><institution>Institute of Computational Medicine, </institution><institution>Johns Hopkins University, </institution></institution-wrap>Baltimore, MD USA </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.19006.3e</institution-id><institution-id institution-id-type="ISNI">0000 0000 9632 6718</institution-id><institution>Department of Computational Medicine, </institution><institution>University of California at Los Angeles, </institution></institution-wrap>Los Angeles, CA USA </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.19006.3e</institution-id><institution-id institution-id-type="ISNI">0000 0000 9632 6718</institution-id><institution>Department of Neurology, </institution><institution>University of California at Los Angeles, </institution></institution-wrap>Los Angeles, CA USA </aff>
      <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.21107.35</institution-id><institution-id institution-id-type="ISNI">0000 0001 2171 9311</institution-id><institution>Department of Neuroscience, </institution><institution>Johns Hopkins University, </institution></institution-wrap>Baltimore, MD USA </aff>
      <aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="GRID">grid.21107.35</institution-id><institution-id institution-id-type="ISNI">0000 0001 2171 9311</institution-id><institution>Center for Imaging Science, </institution><institution>Johns Hopkins University, </institution></institution-wrap>Baltimore, MD USA </aff>
      <aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="GRID">grid.21107.35</institution-id><institution-id institution-id-type="ISNI">0000 0001 2171 9311</institution-id><institution>Kavli Neuroscience Discovery Institute, </institution><institution>Johns Hopkins University, </institution></institution-wrap>Baltimore, MD USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>25</day>
      <month>4</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>25</day>
      <month>4</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>5</volume>
    <elocation-id>388</elocation-id>
    <history>
      <date date-type="received">
        <day>27</day>
        <month>9</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>24</day>
        <month>3</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Recent advances in brain clearing and imaging have made it possible to image entire mammalian brains at sub-micron resolution. These images offer the potential to assemble brain-wide atlases of neuron morphology, but manual neuron reconstruction remains a bottleneck. Several automatic reconstruction algorithms exist, but most focus on single neuron images. In this paper, we present a probabilistic reconstruction method, ViterBrain, which combines a hidden Markov state process that encodes neuron geometry with a random field appearance model of neuron fluorescence. ViterBrain utilizes dynamic programming to compute the global maximizer of what we call the most probable neuron path. We applied our algorithm to imperfect image segmentations, and showed that it can follow axons in the presence of noise or nearby neurons. We also provide an interactive framework where users can trace neurons by fixing start and endpoints. ViterBrain is available in our open-source Python package brainlit.</p>
    </abstract>
    <abstract id="Abs2" abstract-type="web-summary">
      <p id="Par2">ViterBrain is an automated probabilistic reconstruction method that can reconstruct neuronal geometry and processes from microscopy images with code available in the open-source Python package, brainlit.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Image processing</kwd>
      <kwd>Software</kwd>
      <kwd>Computational neuroscience</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/100000002</institution-id>
            <institution>U.S. Department of Health &amp; Human Services | National Institutes of Health (NIH)</institution>
          </institution-wrap>
        </funding-source>
        <award-id>1RF1MH121539-01</award-id>
        <principal-award-recipient>
          <name>
            <surname>Athey</surname>
            <given-names>Thomas L.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1" sec-type="introduction">
    <title>Introduction</title>
    <p id="Par3">Neuron morphology has been a central topic in neuroscience for over a century, as it is the substrate for neural connectivity, and serves as a useful basis for neuron classification. Technological advances in brain clearing and imaging have allowed scientists to probe neurons that extend throughout the brain, and branch hundreds of times<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. It is becoming feasible to assemble a brainwide atlas of cell types in the mammalian brain which would serve as a foundation for understanding how the brain operates as an integrated circuit, or how it fails in neurological disease. One of the main bottlenecks in assembling such an atlas is the manual labor involved in neuron reconstruction.</p>
    <p id="Par4">In an effort to accelerate reconstruction, many automated reconstruction algorithms have been proposed, especially over the last decade. In 2010, the DIADEM project brought multiple institutions together to consolidate existing algorithms, and stimulate further progress by generating open access image datasets, and organizing a contest for reconstruction algorithms<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. Several years later, the BigNeuron project continued the legacy of DIADEM, this time establishing a common software platform, Vaa3D, on which many of the state of the art algorithms were implemented<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. Acciai et al.<sup><xref ref-type="bibr" rid="CR4">4</xref></sup> offers a review of notable reconstruction algorithms up to, and through, the BigNeuron project.</p>
    <p id="Par5">Previous approaches to automated neuron reconstruction have used shortest path/geodesic computation<sup><xref ref-type="bibr" rid="CR5">5</xref>–<xref ref-type="bibr" rid="CR7">7</xref></sup>, minimum spanning trees<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>, Bayesian estimation<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>, tracking<sup><xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR11">11</xref></sup>, and deep learning<sup><xref ref-type="bibr" rid="CR12">12</xref>–<xref ref-type="bibr" rid="CR14">14</xref></sup>. Methods have also been developed to enhance, or extend existing reconstruction algorithms<sup><xref ref-type="bibr" rid="CR15">15</xref>–<xref ref-type="bibr" rid="CR18">18</xref></sup>. Also, some works focus on the subproblem of resolving different neuronal processes that pass by closely to each other<sup><xref ref-type="bibr" rid="CR19">19</xref>–<xref ref-type="bibr" rid="CR21">21</xref></sup>.</p>
    <p id="Par6">Both DIADEM and BigNeuron initiatives focused on the task of single neuron reconstruction so most associated algorithms fail when applied to images with several neurons. However, robust reconstruction in multiple neuron images is essential in order to assemble brainwide atlases of neuron morphology.</p>
    <p id="Par7">We propose a probabilistic model-based algorithm, ViterBrain, that operates on imperfect image segmentations to efficiently reconstruct neuronal processes. Our estimation method does not assume that the image outside the reconstruction is background, and thus allows for the existence of other neurons. Our approach draws upon two major subfields in Computer Vision, appearance modeling and hidden Markov models, and generates globally optimal solutions using dynamic programming. The states of our model are locally connected segments. We score the state transitions using appearance models such as exhibited by Kass and Cohen’s early works<sup><xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR23">23</xref></sup> on active shape modeling and their subsequent application by Wang et al.<sup><xref ref-type="bibr" rid="CR6">6</xref></sup> for neuron reconstruction. For our own approach, we exploit foreground-background models of image intensity for the data likelihood term in the hidden-Markov structure. We quantify the image data using and intensity autocorrelation and kernel density estimates in order to validate our model assumptions.</p>
    <p id="Par8">Our probabilistic models are hidden Markov random fields, but we reduce the computational structure to a hidden Markov model (HMM) since the latent axonal structures have an absolute ordering. Hidden Markov modeling (HMM) involves two sequences of variables, one is observed and one is hidden. A popular application of HMM’s is in speech recognition where the observed sequence is an audio signal, and the hidden variables a sequence of words<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. In our setting, the observed data is the image, and the hidden data is the contour representation of the axon or dendrite’s path.</p>
    <p id="Par9">The key advantages to using HMMs in this context are, first, that neuronal geometry can be explicitly encoded in the state transition distribution. We utilize the Frenet representation of curvature in our transition distribution, which we have studied previously in Athey et al.<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>, Khaneja et al.<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. Secondly, globally optimal estimates can be computed efficiently using dynamic programming in HMMs<sup><xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR27">27</xref></sup>. The well-known Viterbi algorithm computes the MAP estimate of the hidden sequence in an HMM. Our approach, inspired by the Viterbi algorithm, also computes globally optimal estimates. Thus, our optimization method is not susceptible to local optima that exist in filtering methods, or gradient methods in active shape modeling.</p>
    <p id="Par10">In this work, we apply our hidden Markov modeling framework to the output of low-level image segmentation models. Convolutional neural networks have shown impressive results in image segmentation<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>, but it only takes a few false negatives to sever neuronal processes that are often as thin as one micron (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). Our method strings together the locally connected components of the binary image masks into a reconstruction with a global ordering. Thus, our method is modular enough to leverage state of the art methods in machine learning for image segmentation.<fig id="Fig1"><label>Fig. 1</label><caption><title>Image segmentation models sever neuronal processes.</title><p><bold>a</bold> An image subvolume from the MouseLight project containing a single neuron. <bold>b</bold> The same image overlaid with a binary image mask in brown. This mask was generated by the random forest based software Ilastik<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> and illustrates the typical output of an image segmentation model. <bold>c</bold> The same binary image mask, with a different color for each connected component. The variety of colors shows that the neuron has been severed into several pieces. All panels are maximum intensity projections (MIPs), and the scale bar represents 15 microns.</p></caption><graphic xlink:href="42003_2022_3320_Fig1_HTML" id="d32e497"/></fig></p>
    <p id="Par11">We apply our method to data from the MouseLight project at Janelia Research Campus<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>, and focus on the endpoint control problem in a single neuronal process i.e. start and end points are fixed. We introduce the use of Frechet distance to quantify the precision of reconstructions and show that our method has comparable precision to state of the art, when the algorithms are successful.</p>
  </sec>
  <sec id="Sec2" sec-type="results">
    <title>Results</title>
    <sec id="Sec3">
      <title>Overview of ViterBrain</title>
      <p id="Par12">Viterbrain takes in an image, and associated neuron mask produced by some image segmentation model. The algorithm starts by processing the mask into neuron fragments and estimating fragment endpoints and orientation to generate the states. Next, both the prior and likelihood terms of the transition probabilities are computed to construct a directed graph reminiscent of the trellis from the HMM work in Forney<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. Lastly, the shortest path between states is computed with dynamic programming. We have proven that the shortest path is the most probable estimate state sequence formed by a neuronal process (Statement 1). An overview of our algorithm is shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref> and details are given in the Methods. We validated our algorithm on subvolumes of one of the MouseLight whole-brain images<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. The image was acquired via serial two-photon tomography at a resolution of 0.3μm × 0.3μm × 1μm per voxel. Viterbrain is available in our open source Python package brainlit: <ext-link ext-link-type="uri" xlink:href="http://brainlit.neurodata.io/">http://brainlit.neurodata.io/</ext-link>.<fig id="Fig2"><label>Fig. 2</label><caption><title>Summary of the ViterBrain algorithm.</title><p>The algorithm takes in an image and a binary mask that might have severed, or fused neuronal processes. First, the mask is processed into a set of fragments. For each fragment, the endpoints (<italic>x</italic><sup>0</sup>, <italic>x</italic><sup>1</sup>) and endpoint orientations (<italic>τ</italic><sup>0</sup>, <italic>τ</italic><sup>1</sup>) are estimated and added to the state space. Next, transition probabilities are computed from both the image and state data to generate a directed graph reminiscent of the trellis graph in classic hidden Markov modeling. The transition prior depends on spatial distance between fragments, <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$| {x}_{i}^{0}-{x}_{i-1}^{1}|$$\end{document}</tex-math><mml:math id="M2"><mml:mo>∣</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>∣</mml:mo></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq1.gif"/></alternatives></inline-formula>, and curvature of the path that connects them, <italic>κ</italic>(<italic>s</italic><sub><italic>i</italic>−1</sub>, <italic>s</italic><sub><italic>i</italic></sub>), and these two terms are balanced by the hyperparameters <italic>α</italic><sub><italic>d</italic></sub>, <italic>α</italic><sub><italic>κ</italic></sub>. The transition likelihood depends on the local image intensity <italic>α</italic><sub>1</sub>(<italic>I</italic><sub><italic>y</italic></sub>). Finally, a shortest path algorithm is applied to compute the maximally probable state sequence connecting the start to the end state.</p></caption><graphic xlink:href="42003_2022_3320_Fig2_HTML" id="d32e641"/></fig></p>
    </sec>
    <sec id="Sec4">
      <title>Modeling image intensity</title>
      <p id="Par13">Figure <xref rid="Fig3" ref-type="fig">3</xref>a shows the correlations of image intensities between voxels at varying distances of separation. As is typical for natural images, voxels that are close by each other have positively correlated intensities, and those farther away are uncorrelated. In the case of foreground voxels, correlations become weak beyond a distance of about 10 microns, with background voxel correlation decaying rapidly. This lends support to our assumption that voxel intensities are conditionally independent processes, conditioned on the foreground/background model (Eq. (<xref rid="Equ6" ref-type="">3b</xref>)). This assumption is one of the central features of our model because it provides for computational tractability.<fig id="Fig3"><label>Fig. 3</label><caption><title>Characterization of voxel intensity distributions in three different subvolumes of one of the Mouselight whole-brain images.</title><p><bold>a</bold> Correlation of intensities between voxels at varying distances from each other. The curves show that intensities are only weakly correlated (<italic>ρ</italic> &lt; 0.4) at a distance of &gt; 10 microns for foreground voxels, or a distance of &gt; 2 microns for background voxels. Error bars represent a single standard deviation of the Fisher z-transformation of the correlation coefficient. Each curve was generated from all pairs of 5000 randomly sampled voxels. <bold>b</bold> Kernel density estimates (KDEs) of foreground and background intensity distributions. A subset of the voxels in each subvolume was manually labeled, then used to train an Ilastik model to classify the remaining voxels. Each KDE was generated from 5000 voxels, according to the Ilastik classifications. KDEs were computed using scipy’s Gaussian KDE function with default parameters<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>.</p></caption><graphic xlink:href="42003_2022_3320_Fig3_HTML" id="d32e675"/></fig></p>
      <p id="Par14">Figure <xref rid="Fig3" ref-type="fig">3</xref>b shows kernel density estimates (KDEs) of the foreground and background image intensity distributions. The distributions vary greatly between the three image subvolumes, implying that modeling the image process as homogeneous throughout the whole brain would be inappropriate. Additionally, the distributions do not appear to be either Gaussian or Poisson. Indeed, Kolmogorov–Smirnov tests rejected the null hypothesis for both Gaussian and Poisson goodness of fit in all cases, with all p-values below 10<sup>−16</sup>. For that reason, we exploit the independent increments properties of Poisson emission conditioned on the underlying intensity model, but do not assume that the marginal probabilities are Poisson (or Gaussian), instead, we estimate the intensity distributions from the data itself using KDEs (denoted <italic>α</italic><sub>0</sub>(⋅), <italic>α</italic><sub>1</sub>(⋅) in Section The Bayesian Appearance Imaging Model).</p>
    </sec>
    <sec id="Sec5">
      <title>Maximally probable axon reconstructions</title>
      <p id="Par15">Figure <xref rid="Fig4" ref-type="fig">4</xref> demonstrates the reconstruction method on both a satellite image of part of the Great Wall of China, and part of an axon. Different image segmentation models were used to generate the fragments in the two cases, but the process of joining fragments into a reconstruction was the same. The algorithm reconstructs the one dimensional structure in both cases.<fig id="Fig4"><label>Fig. 4</label><caption><title>Demonstration of maximally probable reconstruction on isolated linear structures.</title><p><bold>a</bold> A satellite image of part of the Great Wall of China and <bold>b</bold> a neuronal process from the MouseLight dataset (MIP). Left panels show the original images. Middle panels shows the space of fragments, <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{{{{\mathcal{F}}}}}}}}$$\end{document}</tex-math><mml:math id="M4"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">F</mml:mi></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq2.gif"/></alternatives></inline-formula>, pictured in color. The green and red arrows indicate the start and end states of the reconstruction task, respectively. The right panels show the most probable fragment sequences, where the fragments are colored and overlaid with a blue line connecting the endpoints of the fragments. The scale bar in <bold>b</bold> represents 10 microns.</p></caption><graphic xlink:href="42003_2022_3320_Fig4_HTML" id="d32e730"/></fig></p>
      <p id="Par16">Since the state transition probabilities are modeled with a Gibbs distribution indexed by the state (giving the Markov property), reconstructions are driven by relative energies of different transitions. Thus, our method can still be successful in the presence of luminance dropout, as long as the neuronal process is relatively isolated (Fig. <xref rid="Fig5" ref-type="fig">5</xref>). Our geometric prior has two hyperparameters, <italic>α</italic><sub><italic>d</italic></sub> and <italic>α</italic><sub><italic>κ</italic></sub>, which determine the influence of distance and curvature, respectively, on the probability of connection between two neuronal fragments.<fig id="Fig5"><label>Fig. 5</label><caption><title>ViterBrain is robust to image intensity and fragment dropout when axons are relatively isolated.</title><p><bold>a</bold> An image subvolume from the MouseLight project containing an axon. The scale bar represents 20 microns. <bold>b</bold> The same image, overlaid with the fragments which are depicted in different colors. <bold>c</bold> The image intensity was censored periodically along an axon path (red arrows). <bold>d</bold> The fragments associated with the censored regions were removed from the fragment space (red arrows). <bold>e</bold> Nonetheless, our algorithm was able to jump over the censored regions to reconstruct this axon. All images are MIPs.</p></caption><graphic xlink:href="42003_2022_3320_Fig5_HTML" id="d32e772"/></fig></p>
      <p id="Par17">Figure <xref rid="Fig6" ref-type="fig">6</xref>a shows various examples of maximally probable reconstructions. The algorithm was run with the same hyperparameters in all cases: <italic>α</italic><sub><italic>d</italic></sub> = 10 and <italic>α</italic><sub><italic>κ</italic></sub> = 1000.<fig id="Fig6"><label>Fig. 6</label><caption><title>Demonstration of ViterBrain.</title><p><bold>a</bold> Successful axon reconstructions; the ViterBrain reconstructions are shown by the blue line; the manual reconstructions are shown by the red line. The algorithm was run with the same hyperparameters in each case: <italic>α</italic><sub><italic>d</italic></sub> = 10 and <italic>α</italic><sub><italic>κ</italic></sub> = 1000. <bold>b</bold> Different hyperparameter values lead to different results. Panel i shows the neuron of interest. Panels ii–iv are close-up views of reconstructions with different hyperparameter values that weigh transition distance (<italic>α</italic><sub><italic>d</italic></sub>) and transition curvature (<italic>α</italic><sub><italic>κ</italic></sub>). The red circle in Panel ii indicates where the reconstruction deviated from the true path by jumping ~ 10<italic>μ</italic>m to connect the gray fragment to the light blue fragment. Panel iii shows how a higher <italic>α</italic><sub><italic>d</italic></sub> value avoids the jump in panel ii, but takes a sharp turn to deviate from the true path (red circle). Finally, in panel iv), the reconstruction avoids both the jump from panel ii) and the sharp turn from panel iii) and follows the true path of the axon back to the cell body. All images are MIPs, and all scale bars represent 10 microns.</p></caption><graphic xlink:href="42003_2022_3320_Fig6_HTML" id="d32e839"/></fig></p>
      <p id="Par18">In some cases, reconstruction accuracy is sensitive to hyperparameter values (Fig. <xref rid="Fig6" ref-type="fig">6</xref>b). Higher values of <italic>α</italic><sub><italic>d</italic></sub> penalize transitions between fragment states with large gaps; higher values of <italic>α</italic><sub><italic>κ</italic></sub> penalize state transitions with sharp angles as measured by their discrete curvature. It is important to note that the transition distributions depend on the exact values of <italic>α</italic><sub><italic>d</italic></sub> and <italic>α</italic><sub><italic>κ</italic></sub>, not just, for example, the ratio between them. We found <italic>α</italic><sub><italic>d</italic></sub> = 10 and <italic>α</italic><sub><italic>κ</italic></sub> = 1000 to be effective for the reconstructions shown throughout the paper, but these values should be adjusted according to the quality of fragments, and the geometry of the neurons being reconstructed.</p>
    </sec>
    <sec id="Sec6">
      <title>Signal dropout causes censored fragment states</title>
      <p id="Par19">Though our method is robust to fragment dropout in certain contexts, it is less robust when there is dropout near a parallel neuronal process (Supplementary Fig. <xref rid="MOESM2" ref-type="media">2</xref>). Since the reconstruction is ultimately a sequence of fragments, missing fragments forces the algorithm to choose between jumping to a nearby adjacent fragment at the expense of a penalty due to high curvature or continuing on course at the expense of a penalty due to high inter-fragment distance.</p>
      <p id="Par20">The most obvious source of fragment dropout is low image luminance, leading to false negatives in the initial image segmentation. However, the reason for fragment dropout depends on the underlying segmentation model. Empirically, we found that the reconstruction algorithm fails when the fragment generation process neglects portions that are greater than ~10 μm in length.</p>
    </sec>
    <sec id="Sec7">
      <title>Comparison to state of the art</title>
      <p id="Par21">We examined the accuracy of ViterBrain compared to state of the art reconstruction algorithms. We identified four algorithms that have accompanying publications, and open-source implementations. The first method is APP2 which starts with an oversegmentation of the neuron using a shortest path algorithm, then prunes spurious connections<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>. The second is Snake which is based on active contour modeling<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. The third is called Advantra, based on the particle filtering approach by Radojevic and Meijering<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. The final reconstruction software we use is GTree<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, which uses the algorithm outlined in Quan et al.<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. This algorithm is similar to APP2 in that it starts with an initial reconstruction that spans several neurons, then identifies false connections. APP2, Snake, and Advantra were both used with their default settings and hyperparameters in Vaa3D 3.2 for Mac. GTree version 1.0.4 was used on Linux. For GTree, the binarization threshold was set to 1.0, which was qualitatively identified as a good threshold to capture the neuron. The default soma radius, 3 μm, was used in the soma identification step.</p>
      <p id="Par22">Shown in Fig. <xref rid="Fig7" ref-type="fig">7</xref>a are the results of the various methods on a dataset of 35 subvolumes of a MouseLight whole brain image. Each subvolume contains a cell body, and the initial part of its axon that is covered by the first ten points of the Janelia reconstruction. So, the subvolumes vary in size but usually encompass around 10<sup>6</sup> cubic microns. The algorithms are evaluated on how well they can trace the axon between fixed endpoints (cell body and tenth axon reconstruction point). The outcomes were classified as either successful (if the axon was fully traced), partially successful (if more than half of the axon was reconstructed as evaluated visually), or failures. According to two proportion z-tests the success rate of ViterBrain (11/35) was higher than all other methods at <italic>α</italic> = 0.05. Also, APP2 had a higher success rate (4/35) than Advantra at <italic>α</italic> = 0.05. The success rates for several of the algorithms are discouragingly low, so we discuss the possible reasons for this in the Discussion, and demonstrate the typical failure modes in Supplementary Figs. <xref rid="MOESM2" ref-type="media">3</xref>, <xref rid="MOESM2" ref-type="media">4</xref> and <xref rid="MOESM2" ref-type="media">5</xref>.<fig id="Fig7"><label>Fig. 7</label><caption><title>Results of reconstruction algorithms on a dataset of 35 subvolumes of a MouseLight whole brain image.</title><p>(Snake was only applied to 10 subvolumes due to incoherent results and excessively slow runtimes, see Fig. <xref rid="MOESM2" ref-type="media">S4)</xref>. Each subvolume contained a soma and part of its axon. The task was to reconstruct the portion of the axon that was contained in the image (no branching). First, the algorithms were evaluated visually and classified as successful, partially successful (over half, but not all, of the axon reconstructed), or failed. The table in panel <bold>a</bold> shows these results, along with markers showing statistical significance in a two proportion z-test comparing success rates of the algorithms at <italic>α</italic> = 0.05. For each successful reconstruction, we measured the Frechet distance and spatial distance from the manual ground truth in order to evaluate the precision of the reconstructions. These distances are shown as blue points in <bold>b</bold>, overlaid with standard box and whisker plots (center line, median; box limits, upper and lower quartiles; whiskers, 1.5x interquartile range; points, outliers).</p></caption><graphic xlink:href="42003_2022_3320_Fig7_HTML" id="d32e963"/></fig></p>
      <p id="Par23">For successful reconstructions, we examined the precision of the reconstructions using spatial distance (SD) and Frechet distance. We manually upsampled the Janelia reconstructions to provide the most precise ground truth. All reconstructions were sampled at every 1 μm before distances were measured. Spatial distance represents the average distance from a point on one reconstruction to the closest point on the other reconstruction<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. Frechet distance represents the maximum distance between two reconstructions, and satisfies the criteria to be a mathematical metric and is invariant to reparameterization (see Section Accuracy Metrics for a derivation of Frechet distance). The spatial distance between ViterBrain and manual reconstructions are typically between 1 and 3 microns which is roughly the resolution of the image. Successful reconstructions by the other algorithms are also in this range. Frechet distances are larger, since they indicate maximal distance, not average distance, between curves. We observe that the ~ 5 micron deviations occur most often near the axon hillock where the axon broadens to merge with the soma.</p>
    </sec>
    <sec id="Sec8">
      <title>Proof of concept graphical user interface</title>
      <p id="Par24">Since our method is formulated as an optimal control problem conditioned on the start and states it is most suited for semi-automatic neuron reconstruction where a tracer could click on two different fragments along a neuronal process, and the algorithm would fill in the gap. The user could proceed to trace several neuronal processes until the full neuron is reconstructed. We designed a proof of concept graphical user interface based on this workflow and show an example set of reconstructions made using our GUI in Fig. <xref rid="Fig8" ref-type="fig">8</xref>. The GUI relies on the visualization software napari<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>.<fig id="Fig8"><label>Fig. 8</label><caption><title>Proof of concept graphical user interface.</title><p><bold>a</bold> Image subvolume presented to the user. <bold>b</bold> Neuron fragments also shown in different colors. The user can then click on two fragments and generate the most probable curve between them. <bold>c</bold> Three partial reconstructions (red, green and blue) of different neurons using the GUI. The scale bars represent 20 microns.</p></caption><graphic xlink:href="42003_2022_3320_Fig8_HTML" id="d32e1000"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec9" sec-type="discussion">
    <title>Discussion</title>
    <p id="Par25">This paper presents a hidden Markov model based reconstruction algorithm that connects fragments generated by appearance modeling. Our method converts an image mask into a set of fragments and thus can be applied to the output of an image segmentation model. We chose Ilastik to generate image masks because of its convenient graphical user interface, and high performance on a small number of samples<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>. However, masks could also be generated using a deep learning based model such as Liu et al.<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>, Li and Shen<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> or Wang et al.<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>.</p>
    <p id="Par26">These fragments are assembled based on the associated appearance model score of the observed image and the discrete numerical curvature and distance of adjacent fragments. In the Methods, we derive the Bayes posterior distribution of the hidden state sequence encoding the axon reconstruction. We also show that applying the polynomial time Viterbi algorithm is not possible for maximum a-posteriori estimation since the state space has to grow to account for possible cycles in the unordered image domain. We address the problem with cycles with a modified procedure for defining the path probabilities making it feasible to efficiently calculate the globally optimal neuron path. The solution for efficiently generating the globally optimal path implies that the local minimum associated to gradient and active appearance models solutions is resolved in this setting.</p>
    <p id="Par27">We apply the algorithm to the fixed endpoint problem in two-photon images of mouse neurons. In a dataset of 35 partial axons, our algorithm successfully reconstructs more axons than the existing state of the art algorithms (Fig. <xref rid="Fig7" ref-type="fig">7)</xref>. We observed that the most common failure mode in this dataset was when there are extended (&gt;~10 micron) stretches of the putative axonal path where there is significant loss of luminance signals leading to highly censored fragment generation. This implies that the maximally probable HMM procedure is only effective if paired with effective voxel classification tools. The algorithm can also fail in areas densely populated with neuronal processes. We demonstrate that proper selection of the hyperparameters to reflect the density of the fragments and the geometry of the underlying neurons can resolve these issues. Our algorithm is specifically adapted for reconstructing axons in projection neurons in datasets such as MouseLight in two ways. First, the high image quality as indicated by large KL-divergence values (Fig. <xref rid="Fig3" ref-type="fig">3</xref>a), makes it straightforward to build an effective foreground-background classifier which is an essential part of our HMM state generation. Secondly, our algorithm encodes the geometric properties of axons such as curvature, which allows our solutions to adapt to the occasional sharp turns in projection axons.</p>
    <p id="Par28">The success rates of the other algorithms on our dataset is quite low, considering the performances that they achieved in their accompanying publications. There are two likely reasons for this, dataset differences, and sub-optimal algorithm settings.</p>
    <p id="Par29">When an algorithm is validated on one type of data, the results do not necessarily hold for datasets of a different type. Since none of the existing algorithms had been designed for the MouseLight data, the unique details of our dataset could lead to reduced performance. For example, several subvolumes in our dataset contain multiple neurons, while two of the algorithms, APP2 and Advantra, were explicitly designed for images containing single neurons. Snake is designed to handle multiple neurons, but is largely validated on DIADEM, a single neuron dataset<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. Other dataset differences include different image resolutions (or levels of anisoptropy), different image encodings (8 bit vs. 16 bit), and different signal to noise ratios. Lastly, the existing algorithms are designed to reconstruct all dendrites and axons simultaneously while our task of reconstructing a single section of the axon does not give the algorithms credit for successfully reconstructing other parts of the neuron.</p>
    <p id="Par30">It is also important to note that reconstruction algorithms can be sensitive to hyperparameter settings. All algorithms had different hyperparameter options except for Snake. While we tried various hyperparameter settings for all algorithms, the only non-default setting that clearly improved reconstruction performance was the binarization threshold setting in GTree, all other settings we left to default. It is likely that these settings were not optimal for our dataset, but it is quite time intensive for a typical user to quantitatively determine the optimal settings. Detailed and accessible software documentation makes the process of choosing effective algorithm settings more efficient.</p>
    <p id="Par31">Figures showing the common failure modes for some of the algorithms are shown in <xref rid="MOESM2" ref-type="media">Supplementary Figures</xref>. Advantra and Snake produced incoherent reconstructions on the dataset, and the common failure modes for GTree were early termination of reconstruction, or severing an axon into multiple components. Despite the possible reasons for sub-optimal performance of the other algorithms, we provide evidence that our algorithm is competitive with state-of-the-art methods for reconstructing neuronal processes. Future benchmark comparisons could include reinforcement learning, or recurrent neural network approaches, which have become prevalent in sequential decision processes. However, there is not much scientific literature on these approaches to neuron reconstruction with accompanying functional code.</p>
    <p id="Par32">We have scaled up the ViterBrain pipeline to process images of 3332 × 3332 × 1000 voxels, representing one cubic millimeter of tissue (Supplementary Fig. <xref rid="MOESM2" ref-type="media">6</xref>), and we will continue to improve the pipeline until it can be run on whole brain images. The traces generated by our pipeline could also be paired with tools like the one presented in Li et al.<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> which can turn traces into full neuron segmentations complete with axon and dendrite thickness measurements.</p>
    <p id="Par33">The code used in this work is available in our open-source Python package brainlit: <ext-link ext-link-type="uri" xlink:href="http://brainlit.neurodata.io/">http://brainlit.neurodata.io/</ext-link>, and a tutorial on how to use the code is located at: <ext-link ext-link-type="uri" xlink:href="http://brainlit.neurodata.io/viterbrain.html">http://brainlit.neurodata.io/viterbrain.html</ext-link>.</p>
  </sec>
  <sec id="Sec10">
    <title>Methods</title>
    <sec id="Sec11">
      <title>The Bayesian appearance imaging model</title>
      <p id="Par34">Our Bayes model is comprised of a prior which models the axons as geometric objects and a likelihood which models the image formation process.</p>
      <p id="Par35">We model the axons as simply connected curves in <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathbb{R}}}^{3}$$\end{document}</tex-math><mml:math id="M6"><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq3.gif"/></alternatives></inline-formula> written as a function of arclength<disp-formula id="Equa"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c(\ell ),\ell \ge 0\,,c(\ell )\in {{\mathbb{R}}}^{3}\,.$$\end{document}</tex-math><mml:math id="M8"><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>ℓ</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn><mml:mspace width="0.25em"/><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mspace width="0.25em"/><mml:mo>.</mml:mo></mml:math><graphic xlink:href="42003_2022_3320_Article_Equa.gif" position="anchor"/></alternatives></disp-formula>We denote the entire axon curve in space as <italic>c</italic>(⋅): = {<italic>c</italic>(<italic>ℓ</italic>), <italic>ℓ</italic> ≥ 0}. To interface the geometric object with the imaging volume we represent the underlying curve <italic>c</italic>(⋅) as a delta-dirac impulse train in space. We view the imaging process as the convolution of the delta-dirac impulse train with the point-spread kernel of the imaging platform. We take the point-spread function of the system to be roughly one micron in diameter, implying that the axons are well resolved. The fluorescence process given the axon contour is taken as a relatively narrow path through the imaging domain (~1 μm diameter) with relatively uniform luminance.</p>
      <p id="Par36">We take the image to be defined over the voxel lattice <inline-formula id="IEq4"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$D={\cup }_{i\in {{\mathbb{Z}}}^{{m}^{3}}}{{\Delta }}{y}_{i}\subset {{\mathbb{R}}}^{3}$$\end{document}</tex-math><mml:math id="M10"><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>∪</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⊂</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq4.gif"/></alternatives></inline-formula> with centers <italic>y</italic><sub><italic>i</italic></sub> ∈ Δ<italic>y</italic><sub><italic>i</italic></sub>. We model the image as a random field <inline-formula id="IEq5"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\{{I}_{{y}_{i}},\,{{\Delta }}{y}_{i}\in D\}$$\end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi>D</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq5.gif"/></alternatives></inline-formula> whose elements are independent when conditioned on the underlying axon geometry, similar to an inhomogeneous Poisson process as described in<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. We denote the image random field associated to any subset of sites <italic>Y</italic> ⊂ <italic>D</italic>, with joint probability conditioned on the axon:<disp-formula id="Equ1"><label>1a</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${I}_{Y}=\{{I}_{{y}_{i}}:{{\Delta }}{y}_{i}\in Y\}\,;$$\end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi>Y</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mo>;</mml:mo></mml:math><graphic xlink:href="42003_2022_3320_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ2"><label>1b</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P({I}_{Y}| c(\ell ),\ell \ge 0)=\mathop{\prod}\limits_{y\in Y}p({I}_{y}| c(\ell ),\ell \ge 0)\,.$$\end{document}</tex-math><mml:math id="M16"><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>ℓ</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>∏</mml:mo></mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>ℓ</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mo>.</mml:mo></mml:math><graphic xlink:href="42003_2022_3320_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par37">Because of the conditional independence, the marginal distribution determines the global joint probabilities. Of course, while the conditional probabilities factor and are conditionally independent, the axon geometry is unknown and the measured image random field is completely connected if the latent axon process is removed. We adopt a two hypothesis formulation {<italic>f</italic>, <italic>b</italic>} corresponding to a foreground-background model for the images where the marginal probability of a voxel intensity is:<disp-formula id="Equ3"><label>2a</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P({I}_{y})={\alpha }_{1}({I}_{y}),y\in \,{{\mbox{foreground}}}$$\end{document}</tex-math><mml:math id="M18"><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>foreground</mml:mtext></mml:mstyle></mml:math><graphic xlink:href="42003_2022_3320_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ4"><label>2b</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P({I}_{y})={\alpha }_{0}({I}_{y}),y\in \,{{\mbox{background}}}$$\end{document}</tex-math><mml:math id="M20"><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>background</mml:mtext></mml:mstyle></mml:math><graphic xlink:href="42003_2022_3320_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par38">Our conditional independence assumption implies that the joint distribution of group of foreground or group of background voxels can be decomposed into the corresponding marginal distributions over the foreground-background models (<xref rid="Equ3" ref-type="">2a</xref>),(<xref rid="Equ4" ref-type="">2b</xref>); defining the foreground and background sets <italic>Y</italic> = <italic>Y</italic><sub><italic>f</italic></sub> ∪ <italic>Y</italic><sub><italic>b</italic></sub>, then<disp-formula id="Equ5"><label>3a</label><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P({I}_{Y})=\mathop{\prod}\limits_{y\in {Y}_{f}}{\alpha }_{1}({I}_{y})\,\mathop{\prod}\limits_{y\in {Y}_{b}}{\alpha }_{0}({I}_{y})\,.$$\end{document}</tex-math><mml:math id="M22"><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>∏</mml:mo></mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:munder><mml:mrow><mml:mo>∏</mml:mo></mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mo>.</mml:mo></mml:math><graphic xlink:href="42003_2022_3320_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>We define the notations for the joint probability of the set in the foreground for example (or background)<disp-formula id="Equ6"><label>3b</label><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\alpha }_{1}({I}_{{Y}_{f}}):=\mathop{\prod}\limits_{y\in {Y}_{f}}{\alpha }_{1}({I}_{y}),\,{Y}_{f}\subset \,{{\mbox{foreground}}}.$$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>∏</mml:mo></mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>⊂</mml:mo><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>foreground</mml:mtext></mml:mstyle><mml:mo>.</mml:mo></mml:math><graphic xlink:href="42003_2022_3320_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par39">Despite the Poisson nature of the image acquisition process, simple scaling or shifting of the imaging data would mean the image intensities are no longer Poisson. To accommodate this effect, we estimate the foreground-background intensity distributions (<italic>α</italic><sub>1</sub>(⋅) and <italic>α</italic><sub>0</sub>(⋅) respectively) nonparametrically. The simplest nonparametric density estimation technique is using histograms. However, it can be difficult to choose the origin and bin width of histograms, so we opt for a kernel density estimate (KDE) approach. We estimate <italic>α</italic><sub>0</sub>(⋅), <italic>α</italic><sub>1</sub>(⋅) by labeling a subset of the data as foreground/background then fitting Gaussian KDEs to the labeled data (see Fig. <xref rid="Fig3" ref-type="fig">3</xref>b). We use the scipy implementation of Gaussian KDEs<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>, with Scott’s rule to determine the bandwidth parameter<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. Under some assumptions on the derivatives of the underlying density, our approach converges to the true density as the number of samples increases (Theorem 6.1 in<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>). Further, the Scott’s rule choice for bandwidth is (approximately) optimal with respect to mean integrated square error.</p>
      <p id="Par40">Our approach allows us to empirically estimate the intensity distributions while maintaining the independent increments property of a spatial Poisson process, which is consistent with the autocorrelation curves depicted in Fig. <xref rid="Fig3" ref-type="fig">3</xref>a. This is the key property that allows us to factor joint probabilities into products of marginals for sets of voxels.</p>
      <p id="Par41">We note that the foreground-background imaging model allows us to estimate the error rate of classifying a voxel as either foreground or background. In the Neyman-Pearson framework, foreground-background classification is a simple two hypothesis testing problem and the most powerful test at a given type 1 error rate is the log likelihood ratio test. The Kullback–Leibler (KL) divergence between the foreground and background distributions gives the exponential rate at which error rates converge to zero as the number of independent, identically distributed samples increases<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>. In the case of using Gaussians to model the foreground-background distributions, the KL divergence reduces to the squared signal to noise ratio. In the absence of normality, the KL Divergence is the general information theoretic measure of image quality for arbitrary distributions. We propose KL Divergence as an important statistic in evaluating the quality of fluorescent neuron images.</p>
    </sec>
    <sec id="Sec12">
      <title>The prior distribution via Markov state representation on the axons fragments</title>
      <p id="Par42">Our representation of the observed image is as a hidden Markov random field with the axon as the hidden latent structure. Given the complexity of sub-micron resolution images, we build an intermediate data structure at the micron scale that we call fragments <italic>F</italic> ⊂ <italic>D</italic> defined as collections of voxels without any assumed global ordering between them. Each fragment represents a portion of a neuronal process, with a natural orientation given by one end that is closer to the soma. Depicted in Fig. <xref rid="Fig1" ref-type="fig">1</xref> are the fragments shown via different colors. The fragments are a coarser scale voxel and can be viewed analogously as higher order features.</p>
      <p id="Par43">The axon reconstruction problem becomes the reassembly of the fragments along with the imputation of the censored fragments. In the image examples presented in this work, the complexity of the space of fragments is approximately <inline-formula id="IEq6"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$| {{{{{{{\mathcal{F}}}}}}}}| =100$$\end{document}</tex-math><mml:math id="M26"><mml:mo>∣</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">F</mml:mi><mml:mo>∣</mml:mo><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq6.gif"/></alternatives></inline-formula>,000 for a cubic millimeter of projection neuron image data.</p>
      <p id="Par44">We exploit the computational structures of hidden Markov models (HMMs) when the underlying latent structure is absolutely ordered so that dynamic programming can efficiently compute globally optimal state sequence estimates. From the set of fragments we compute a set of states for the HMM. The states are a simplified, abstract representation of the fragments that contain the minimum information required to specify the HMM. Each state includes endpoints <inline-formula id="IEq7"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x}^{0},{x}^{1}\in {{\mathbb{R}}}^{3}$$\end{document}</tex-math><mml:math id="M28"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq7.gif"/></alternatives></inline-formula> in order to compute “gap” or “censored” probabilities, and unit length tangents <inline-formula id="IEq8"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tau }^{0},{\tau }^{1}\in {{\mathbb{R}}}^{3}$$\end{document}</tex-math><mml:math id="M30"><mml:msup><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq8.gif"/></alternatives></inline-formula> associated with the endpoints in order to compute curvature. Each fragment generates two states, one for each orientation. The two states are identical except their endpoints are swapped, and their tangents are swapped and reversed. We denote the natural mapping from state to fragment by <inline-formula id="IEq9"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F:s\in {{{{{{{\mathcal{S}}}}}}}}\mapsto F(s)\in {{{{{{{\mathcal{F}}}}}}}}$$\end{document}</tex-math><mml:math id="M32"><mml:mi>F</mml:mi><mml:mo>:</mml:mo><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">S</mml:mi><mml:mo>↦</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">F</mml:mi></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq9.gif"/></alternatives></inline-formula>.</p>
      <p id="Par45">The collection of states <inline-formula id="IEq10"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{{{{\mathcal{S}}}}}}}}=\{s\}$$\end{document}</tex-math><mml:math id="M34"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">S</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq10.gif"/></alternatives></inline-formula> is the finite state space of the HMM, and our goal is to estimate the state sequence (<italic>s</italic><sub>1</sub>, . . . , <italic>s</italic><sub><italic>n</italic></sub>) that follows the neuronal process.</p>
      <p id="Par46">Our algorithms exploit two splitting properties, the Markov nature of the state sequence and the splitting of the random field image conditioned on the state sequence. We use the notation <italic>s</italic><sub><italic>i</italic>:<italic>j</italic></sub>: = (<italic>s</italic><sub><italic>i</italic></sub>, <italic>s</italic><sub><italic>i</italic>+1</sub>, …,  <italic>s</italic><sub><italic>j</italic></sub>) for partial state sequences. We model the state sequence <italic>s</italic><sub>1</sub>, …, <italic>s</italic><sub><italic>n</italic></sub> as Markov with splitting property:<disp-formula id="Equb"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p({s}_{i+1:n},{s}_{1:i-1}| {s}_{i})=p({s}_{i+1:n}| {s}_{i})p({s}_{1:i-1}| {s}_{i})\,,\,i=2,\ldots ,n-1\,,$$\end{document}</tex-math><mml:math id="M36"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mspace width="0.25em"/><mml:mo>,</mml:mo></mml:math><graphic xlink:href="42003_2022_3320_Article_Equb.gif" position="anchor"/></alternatives></disp-formula>which implies the 1-order Markov property <italic>p</italic>(<italic>s</italic><sub><italic>i</italic></sub>∣<italic>s</italic><sub><italic>i</italic>−1</sub>, <italic>s</italic><sub>1:<italic>i</italic>−2</sub>) = <italic>p</italic>(<italic>s</italic><sub><italic>i</italic></sub>∣<italic>s</italic><sub><italic>i</italic>−1</sub>).</p>
      <p id="Par47">We define the transition probabilities with a Boltzmann distribution with energy <italic>U</italic>:<disp-formula id="Equc"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p({s}_{i}| {s}_{i-1})=\frac{{e}^{-U({s}_{i-1},{s}_{i})}}{Z({s}_{i-1})}\,,\,\,{{\mbox{with}}}\,\,Z({s}_{i-1})=\mathop{\sum}\limits_{{s}_{i}\in {{{{{{{\mathcal{S}}}}}}}}}{e}^{-U({s}_{i-1},{s}_{i})}\,,$$\end{document}</tex-math><mml:math id="M38"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>Z</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mspace width="0.25em"/><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>with</mml:mtext></mml:mstyle><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mi>Z</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">S</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mspace width="0.25em"/><mml:mo>,</mml:mo></mml:math><graphic xlink:href="42003_2022_3320_Article_Equc.gif" position="anchor"/></alternatives></disp-formula>with energy given by<disp-formula id="Equd"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$U({s}_{i-1},{s}_{i})={\alpha }_{d}| {x}_{i}^{0}-{x}_{i-1}^{1}{| }^{2}+{\alpha }_{\kappa }\kappa {({s}_{i-1},{s}_{i})}^{2}\,,$$\end{document}</tex-math><mml:math id="M40"><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mo>∣</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:msub><mml:mi>κ</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="0.25em"/><mml:mo>,</mml:mo></mml:math><graphic xlink:href="42003_2022_3320_Article_Equd.gif" position="anchor"/></alternatives></disp-formula>where ∣⋅∣ is the standard Euclidean norm. The two hyperparameters <italic>α</italic><sub><italic>d</italic></sub> and <italic>α</italic><sub><italic>κ</italic></sub>, determine the influence of distance and curvature, respectively, on the probability of connection between two neuronal fragments. The term <italic>κ</italic>(<italic>s</italic><sub><italic>i</italic>−1</sub>, <italic>s</italic><sub><italic>i</italic></sub>) approximates the curvature of the path connecting <italic>s</italic><sub><italic>i</italic>−1</sub> to <italic>s</italic><sub><italic>i</italic></sub> as follows:</p>
      <p id="Par48">Define <inline-formula id="IEq11"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tau }_{c}:=\frac{{x}_{i}^{0}-{x}_{i-1}^{1}}{| | {x}_{i}^{0}-{x}_{i-1}^{1}| | }$$\end{document}</tex-math><mml:math id="M42"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mo>∣</mml:mo><mml:mo>∣</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>∣</mml:mo><mml:mo>∣</mml:mo></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq11.gif"/></alternatives></inline-formula> which is the normalized vector connecting <italic>s</italic><sub><italic>i</italic>−1</sub> to <italic>s</italic><sub><italic>i</italic></sub>. Then we can approximate the squared curvature at <inline-formula id="IEq12"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x}_{i-1}^{1}$$\end{document}</tex-math><mml:math id="M44"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq12.gif"/></alternatives></inline-formula> and <inline-formula id="IEq13"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x}_{i}^{0}$$\end{document}</tex-math><mml:math id="M46"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq13.gif"/></alternatives></inline-formula> with <inline-formula id="IEq14"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\kappa }_{1}{({s}_{i-1},{s}_{i})}^{2}=1-{\tau }_{i-1}^{1}\cdot {\tau }_{c}$$\end{document}</tex-math><mml:math id="M48"><mml:msub><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq14.gif"/></alternatives></inline-formula> and <inline-formula id="IEq15"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\kappa }_{2}{({s}_{i-1},{s}_{i})}^{2}=1-{\tau }_{c}\cdot \left(-{\tau }_{i}^{0}\right)$$\end{document}</tex-math><mml:math id="M50"><mml:msub><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq15.gif"/></alternatives></inline-formula> respectively. These formulas are derived in Supplementary Method <xref rid="MOESM2" ref-type="media">2</xref> and they approximate curvature as modeled in Athey et al.<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. Finally, we approximate average squared curvature with the arithmetic mean:<disp-formula id="Eque"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\kappa {({s}_{i-1},{s}_{i})}^{2}=\frac{{\kappa }_{1}{({s}_{i-1},{s}_{i})}^{2}+{\kappa }_{2}{({s}_{i-1},{s}_{i})}^{2}}{2}$$\end{document}</tex-math><mml:math id="M52"><mml:mi>κ</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="42003_2022_3320_Article_Eque.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par49">We control the computational complexity associated to computing prior probabilities for all <inline-formula id="IEq16"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$| {{{{{{{\mathcal{S}}}}}}}}{| }^{2}$$\end{document}</tex-math><mml:math id="M54"><mml:mo>∣</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">S</mml:mi><mml:msup><mml:mrow><mml:mo>∣</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq16.gif"/></alternatives></inline-formula> states by restricting the possible state transitions. We set to 0 probability any transitions where the distance between endpoints is greater than 15 <italic>μ</italic>m or the angle between states is greater than 150<sup>∘</sup>. We also set the probability that a state transitions to itself to zero.</p>
    </sec>
    <sec id="Sec13">
      <title>Global maximally probable solutions via <inline-formula id="IEq17"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$O(n| {{{{{{{\mathcal{S}}}}}}}}{| }^{2})$$\end{document}</tex-math><mml:math id="M56"><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>∣</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">S</mml:mi><mml:msup><mml:mrow><mml:mo>∣</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq17.gif"/></alternatives></inline-formula> calculation</title>
      <p id="Par50">The maximum a-posteriori (MAP) state sequence is defined as the maximizer of the posterior probability (MAP) over the state sequences <inline-formula id="IEq18"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${s}_{1:n}\in {{{{{{{{\mathcal{S}}}}}}}}}^{n}$$\end{document}</tex-math><mml:math id="M58"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq18.gif"/></alternatives></inline-formula>, with <inline-formula id="IEq19"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$| {{{{{{{\mathcal{S}}}}}}}}|$$\end{document}</tex-math><mml:math id="M60"><mml:mo>∣</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">S</mml:mi><mml:mo>∣</mml:mo></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq19.gif"/></alternatives></inline-formula> finite:<disp-formula id="Equ7"><label>4</label><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{s}}_{1:n}:=\arg \mathop{\max }\limits_{{s}_{1:n}\in {{{{{{{{\mathcal{S}}}}}}}}}^{n}}\log p({s}_{1:n}| {I}_{D})\,.$$\end{document}</tex-math><mml:math id="M62"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mi>log</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mo>.</mml:mo></mml:math><graphic xlink:href="42003_2022_3320_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par51">The solution space has cardinality <inline-formula id="IEq20"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$| {{{{{{{\mathcal{S}}}}}}}}{| }^{n}$$\end{document}</tex-math><mml:math id="M64"><mml:mo>∣</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">S</mml:mi><mml:msup><mml:mrow><mml:mo>∣</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq20.gif"/></alternatives></inline-formula> so it is infeasible to compute the global maximizer by exhaustive search. Our approach is to rewrite the probability recursively in order to use the Viterbi algorithm and dynamic programming with <inline-formula id="IEq21"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$O(n| {{{{{{{\mathcal{S}}}}}}}}{| }^{2})$$\end{document}</tex-math><mml:math id="M66"><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>∣</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">S</mml:mi><mml:msup><mml:mrow><mml:mo>∣</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq21.gif"/></alternatives></inline-formula> time complexity. We rewrite the MAP estimator in terms of the joint probability:<disp-formula id="Equf"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{s}}_{1:n}=\arg \mathop{\max }\limits_{{s}_{1:n}\in {{{{{{{{\mathcal{S}}}}}}}}}^{n}}p({s}_{1:n}| {I}_{D})=\arg \mathop{\max }\limits_{{s}_{1:n}\in {{{{{{{{\mathcal{S}}}}}}}}}^{n}}p({s}_{1:n},{I}_{D})\,.$$\end{document}</tex-math><mml:math id="M68"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mo>.</mml:mo></mml:math><graphic xlink:href="42003_2022_3320_Article_Equf.gif" position="anchor"/></alternatives></disp-formula>The image random field is split or conditionally independent conditioned on the fragment states:<disp-formula id="Equg"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p({I}_{F({s}_{i})},{I}_{D\setminus F({s}_{i})}| {s}_{i},)=p({I}_{F({s}_{i})}| {s}_{i})p({I}_{D\setminus F({s}_{i})}| {s}_{i})\,,$$\end{document}</tex-math><mml:math id="M70"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>\</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>\</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mo>,</mml:mo></mml:math><graphic xlink:href="42003_2022_3320_Article_Equg.gif" position="anchor"/></alternatives></disp-formula>which implies <inline-formula id="IEq22"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p({I}_{F({s}_{i})}| {s}_{i},{I}_{D\setminus F({s}_{i})})=p({I}_{F({s}_{i})}| {s}_{i})$$\end{document}</tex-math><mml:math id="M72"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>\</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq22.gif"/></alternatives></inline-formula>.</p>
      <p id="Par52">Define the indicator function <italic>δ</italic><sub><italic>A</italic></sub>(<italic>x</italic>) = 1 for <italic>x</italic> ∈ <italic>A</italic>, 0 otherwise.</p>
      <sec id="FPar1">
        <title>Lemma 1</title>
        <p id="Par53">Defining the shorthand notation identifying fragment sequences with the state sequence:<disp-formula id="Equh"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${F}_{1:n}:=(F({s}_{1}),\ldots ,F({s}_{n}))\,,$$\end{document}</tex-math><mml:math id="M74"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mo>,</mml:mo></mml:math><graphic xlink:href="42003_2022_3320_Article_Equh.gif" position="anchor"/></alternatives></disp-formula>then for <italic>n</italic> &gt; 1 we have the joint probability:<disp-formula id="Equ8"><label>5</label><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p({s}_{1:n},{I}_{D})=\mathop{\prod }\limits_{i = 2}^{n}{\left(\frac{{\alpha }_{1}({I}_{{F}_{i}})}{{\alpha }_{0}({I}_{{F}_{i}})}\right)}^{{\delta }_{D\setminus {F}_{1:i-1}}({F}_{i})}p({s}_{i}| {s}_{i-1})\,p({s}_{1},{I}_{D})\,.$$\end{document}</tex-math><mml:math id="M76"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∏</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>\</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mo>.</mml:mo></mml:math><graphic xlink:href="42003_2022_3320_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>See Supplementary Method <xref rid="MOESM2" ref-type="media">3</xref> for the proof which rewrites the probability recursively giving the factorization. The proof is similar to the classic HMM decomposition in how it uses the two splitting properties, but there are two differences. The first is that the probability needs to account for the full image, including areas outside of the axon estimate, which explains the presence of both <italic>α</italic><sub>1</sub> and <italic>α</italic><sub>0</sub> in Eq. (<xref rid="Equ8" ref-type="">5</xref>). The second difference is that if the state sequence <italic>s</italic><sub>1:<italic>n</italic></sub> contains repeated states, then the corresponding image data should not be double counted in the probability. This is enforced by the delta function <italic>δ</italic>(⋅).</p>
        <p>It is natural to take the negative logarithm of Eq. (<xref rid="Equ8" ref-type="">5</xref>) to obtain a sum that represents and the cost of a path through a directed trellis graph<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. Several algorithms exist that solve the shortest path problem in <inline-formula id="IEq23"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$O(n| {{{{{{{\mathcal{S}}}}}}}}{| }^{2})$$\end{document}</tex-math><mml:math id="M78"><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>∣</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">S</mml:mi><mml:msup><mml:mrow><mml:mo>∣</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq23.gif"/></alternatives></inline-formula> complexity. However, we cannot use these algorithms directly because the cost function is not sequentially-additive due to the dependence of the indicator function on previous states in the sequence. In Supplementary Method <xref rid="MOESM2" ref-type="media">4</xref>, we offer a example demonstrating that directly applying the Viterbi algorithm to this problem does not generate the MAP estimate.</p>
        <p>We adjust our probabilistic representation on the <inline-formula id="IEq24"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$| {{{{{{{\mathcal{S}}}}}}}}{| }^{n}$$\end{document}</tex-math><mml:math id="M80"><mml:mo>∣</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">S</mml:mi><mml:msup><mml:mrow><mml:mo>∣</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq24.gif"/></alternatives></inline-formula> paths in order to utilize shortest path algorithms such as Bellman-Ford or Dijkstra’s<sup><xref ref-type="bibr" rid="CR41">41</xref>,<xref ref-type="bibr" rid="CR42">42</xref></sup>. For this we note that the <inline-formula id="IEq25"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{{\alpha }_{1}({I}_{{F}_{i}})}{{\alpha }_{0}({I}_{{F}_{i}})}$$\end{document}</tex-math><mml:math id="M82"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq25.gif"/></alternatives></inline-formula> term in Eq. (<xref rid="Equ8" ref-type="">5</xref>) may often be greater than 1. In the directed graph formulation (negative logarithm transformation), this can lead to negative cycles in the graph of states. When negative cycles exist, the shortest path problem is ill-posed. To avoid this phenomena we remove the background component of the image from the joint probability, which converts <inline-formula id="IEq26"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{{\alpha }_{1}({I}_{{F}_{i}})}{{\alpha }_{0}({I}_{{F}_{i}})}$$\end{document}</tex-math><mml:math id="M84"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq26.gif"/></alternatives></inline-formula> to <inline-formula id="IEq27"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\alpha }_{1}({I}_{{F}_{i}})$$\end{document}</tex-math><mml:math id="M86"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq27.gif"/></alternatives></inline-formula>, and converts our global posterior probability to our path probability formulation.</p>
        <p><bold>Statement 1</bold>. <italic>Define the most probable solution</italic><inline-formula id="IEq28"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${s}_{1:n}\in {{{{{{{{\mathcal{S}}}}}}}}}^{n}$$\end{document}</tex-math><mml:math id="M88"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq28.gif"/></alternatives></inline-formula><italic>by the joint probability</italic><inline-formula id="IEq29"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\arg \mathop{\max }\nolimits_{{s}_{1:n}\in {{{{{{{{\mathcal{S}}}}}}}}}^{n}}p({s}_{1:n},{I}_{{F}_{1:n}})$$\end{document}</tex-math><mml:math id="M90"><mml:mi>arg</mml:mi><mml:msub><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq29.gif"/></alternatives></inline-formula>. <italic>Then we have</italic><disp-formula id="Equ9"><label>6</label><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathop{\max }\limits_{{s}_{1:n}\in {{{{{{{{\mathcal{S}}}}}}}}}^{n}}p({s}_{1:n},{I}_{{F}_{1:n}})=\mathop{\max }\limits_{{s}_{1:n}\in {{{{{{{{\mathcal{S}}}}}}}}}^{n}}\mathop{\prod }\limits_{i = 2}^{n}{\left({\alpha }_{1}({I}_{{F}_{i}})\right)}^{{\delta }_{D\setminus {F}_{1:i-1}}({F}_{i})}p({s}_{i}| {s}_{i-1})\,p({s}_{1},{I}_{{F}_{1}})\,.$$\end{document}</tex-math><mml:math id="M92"><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∏</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>\</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mo>.</mml:mo></mml:math><graphic xlink:href="42003_2022_3320_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula><italic>Further, if</italic><italic>α</italic><sub>1</sub>(<italic>I</italic><sub><italic>y</italic></sub>) ≤ 1 <italic>for all</italic>
<italic>y</italic>, <italic>then the globally optimal solution to the fixed start and end point problem is a nonrepeating state sequence and can be obtained by computing the shortest path in a directed graph where the vertices are the states, and the edge weight from state</italic>
<italic>s</italic><sub><italic>i</italic>−1</sub>
<italic>to</italic>
<italic>s</italic><sub><italic>i</italic></sub>
<italic>is given by:</italic><disp-formula id="Equ10"><label>7</label><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$e({s}_{i-1},{s}_{i})=-\log {\alpha }_{1}({I}_{{F}_{i}})-\log p({s}_{i}| {s}_{i-1})$$\end{document}</tex-math><mml:math id="M94"><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><graphic xlink:href="42003_2022_3320_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>See Supplementary Method <xref rid="MOESM2" ref-type="media">3</xref> for proof. Our reconstruction problem has now become a shortest path problem, and can be solved using one of the several dynamic programming algorithms.</p>
        <p>We note that since the path (<italic>s</italic><sub>1:<italic>n</italic></sub>) defines the subset of the image in the joint probability (<inline-formula id="IEq30"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${I}_{{F}_{1:n}}$$\end{document}</tex-math><mml:math id="M96"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq30.gif"/></alternatives></inline-formula>) we can define the probability as a function of only the state sequence <inline-formula id="IEq31"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\bar{p}({s}_{1:n}):=p({s}_{1:n},{I}_{{F}_{1:n}})$$\end{document}</tex-math><mml:math id="M98"><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq31.gif"/></alternatives></inline-formula> emphasizing that we are solving the most probable path problem.</p>
      </sec>
    </sec>
    <sec id="Sec14">
      <title>Implementation</title>
      <sec id="Sec15">
        <title>Fragment generation</title>
        <p id="Par60">Fragments are collections of voxels, or supervoxels, and can be viewed analogously as higher order features such as edgelets or corners. As described in Section The Prior Distribution via Markov State Representation on the Axons Fragments, identifying the subset of fragments that compose the axon, then ordering them becomes equivalent to reconstructing the axon contour model.</p>
        <p id="Par61">The first step of fragment generation is obtaining a foreground-background mask, which could be obtained, for example, from a neural network, or by simple thresholding. In this work, we use an Ilastik model that was trained on three image subvolumes, each of which has three slices that were labeled<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>. During prediction, the probability predictions from Ilastik are thresholded at 0.9, a conservative threshold that keeps the number of false positives low.</p>
        <p id="Par62">The connected components of the thresholded image are split into fragments of similar size by identifying the voxel <italic>v</italic> with the largest predicted foreground probability and placing a ball <italic>B</italic><sub><italic>v</italic></sub> with radius 7 <italic>μ</italic>m on that voxel. The voxels within <italic>B</italic><sub><italic>v</italic></sub> are removed and the process is repeated until the component is covered. The component is then split up into pieces by assigning each voxel to the center point from the previous step, <italic>v</italic>, that is closest to it. This procedure ensures that each fragment is no larger than a ball with radius 7 <italic>μ</italic>m. At this size, it is reasonable to assume that each fragment is associated with only one axon branch since no fragment is large enough to extensively cover multiple branches.</p>
        <p id="Par63">Next, the endpoints <italic>x</italic><sup>0</sup>, <italic>x</italic><sup>1</sup> and tangents <italic>τ</italic><sup>0</sup>, <italic>τ</italic><sup>1</sup> are computed as described in Supplementary Method <xref rid="MOESM2" ref-type="media">1</xref>. Each fragment is simplified to the line segment between its endpoints which is rasterized using the Bresenham algorithm<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>. Briefly, the Bresenham algorithm identifies the image axis along which the line segment has the largest range and samples the line once every voxel unit along that axis. Then, the other coordinates are chosen to minimize the distance from the continuous representation line segment.</p>
      </sec>
      <sec id="Sec16">
        <title>Imputing fragment deletions</title>
        <p id="Par64">In practice the imaging data may exhibit significant dropouts leading to significant fragment deletions. While computing the likelihood of the image data, we augment the gaps between any pair of connected fragments in <italic>F</italic><sub>1</sub>, <italic>F</italic><sub>2</sub>, … by augmenting the sequence with imputed fragments <inline-formula id="IEq32"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${F}_{1},{\bar{F}}_{1},{F}_{2},{\bar{F}}_{2},\ldots \,.$$\end{document}</tex-math><mml:math id="M100"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mspace width="0.25em"/><mml:mo>.</mml:mo></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq32.gif"/></alternatives></inline-formula>, with <inline-formula id="IEq33"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\bar{F}}_{i}\subset D$$\end{document}</tex-math><mml:math id="M102"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⊂</mml:mo><mml:mi>D</mml:mi></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq33.gif"/></alternatives></inline-formula> the imputed line of voxels which forms the connection between the pair <italic>F</italic><sub><italic>i</italic></sub>, <italic>F</italic><sub><italic>i</italic>+1</sub>. For this define the start and endpoint of each fragment as <inline-formula id="IEq34"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x}^{0}(F)\in {{\mathbb{R}}}^{3},{x}^{1}(F)\in {{\mathbb{R}}}^{3}$$\end{document}</tex-math><mml:math id="M104"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq34.gif"/></alternatives></inline-formula> with line segment connecting each pair:<disp-formula id="Equi"><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{i,i+1}=\{y:y=a{x}^{1}({F}_{i})+(1-a){x}^{0}({F}_{i+1}),\,a\in [0,1]\}\,.$$\end{document}</tex-math><mml:math id="M106"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>:</mml:mo><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mo>.</mml:mo></mml:math><graphic xlink:href="42003_2022_3320_Article_Equi.gif" position="anchor"/></alternatives></disp-formula>The imputed fragment <inline-formula id="IEq35"><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\bar{F}}_{i}\subset D$$\end{document}</tex-math><mml:math id="M108"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⊂</mml:mo><mml:mi>D</mml:mi></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq35.gif"/></alternatives></inline-formula> for each pair (<italic>F</italic><sub><italic>i</italic></sub>, <italic>F</italic><sub><italic>i</italic>+1</sub>) is computed by rasterizing <italic>L</italic><sub><italic>i</italic>,<italic>i</italic>+1</sub> with the Bresenham algorithm.</p>
        <p id="Par65">The likelihood of the sequence of fragments augmented by the imputations becomes<disp-formula id="Equj"><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p({s}_{1:n},{I}_{{F}_{1:n}})=\mathop{\prod }\limits_{i=2}^{n}{\alpha }_{1}{({I}_{{F}_{i}})}^{{\delta }_{D\setminus {F}_{1:i-1}}({F}_{i})}{\alpha }_{1}({I}_{\bar{{F}_{i}}})p({s}_{i}| {s}_{i-1})p({s}_{1},{I}_{{F}_{1}})$$\end{document}</tex-math><mml:math id="M110"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∏</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>\</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><graphic xlink:href="42003_2022_3320_Article_Equj.gif" position="anchor"/></alternatives></disp-formula></p>
      </sec>
      <sec id="Sec17">
        <title>Initial and endpoint conditions</title>
        <p id="Par66">We take the initial conditions to represent<disp-formula id="Equk"><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p({s}_{1},{I}_{{F}_{1}})=\pi ({s}_{1})p({I}_{{F}_{1}}| {s}_{1})\,,$$\end{document}</tex-math><mml:math id="M112"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mo>,</mml:mo></mml:math><graphic xlink:href="42003_2022_3320_Article_Equk.gif" position="anchor"/></alternatives></disp-formula>with <italic>π</italic> the prior on initial state. For all of our axon reconstructions we specify an axonal fragment as the start state <italic>s</italic><sub><italic>s</italic><italic>t</italic><italic>a</italic><italic>r</italic><italic>t</italic></sub> and set <inline-formula id="IEq36"><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\pi ({s}_{1}):={\delta }_{{s}_{start}}({s}_{1})$$\end{document}</tex-math><mml:math id="M114"><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq36.gif"/></alternatives></inline-formula>.</p>
        <p id="Par67">The endpoint conditions are defined via a user specified terminal state <italic>s</italic><sub><italic>t</italic><italic>e</italic><italic>r</italic><italic>m</italic></sub> where the path ends giving the maximization:<disp-formula id="Equl"><alternatives><tex-math id="M115">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathop{\max }\limits_{{s}_{1:n}\in {{{{{{{{\mathcal{S}}}}}}}}}^{n}}p({s}_{1:n},{I}_{{F}_{1:n}}| {s}_{n}={s}_{term})\,.$$\end{document}</tex-math><mml:math id="M116"><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mo>.</mml:mo></mml:math><graphic xlink:href="42003_2022_3320_Article_Equl.gif" position="anchor"/></alternatives></disp-formula>The marginal probability on the terminal state always transitions to itself, so that <inline-formula id="IEq37"><alternatives><tex-math id="M117">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p({s}_{n}={s}_{term})={\delta }_{{s}_{term}}({s}_{n})$$\end{document}</tex-math><mml:math id="M118"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq37.gif"/></alternatives></inline-formula>. Thus, a state sequence solution of length <italic>n</italic> may end in multiple repetitions of <italic>s</italic><sub><italic>t</italic><italic>e</italic><italic>r</italic><italic>m</italic></sub>, such as<disp-formula id="Equm"><alternatives><tex-math id="M119">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${s}_{1:n}=\{{s}_{1},{s}_{2},...,{s}_{n^{\prime} },{s}_{term},{s}_{term},...,{s}_{term}\}.$$\end{document}</tex-math><mml:math id="M120"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math><graphic xlink:href="42003_2022_3320_Article_Equm.gif" position="anchor"/></alternatives></disp-formula></p>
      </sec>
    </sec>
    <sec id="Sec18">
      <title>Accuracy metrics</title>
      <p id="Par68">We applied several state of the art reconstruction algorithms to several neurons in the brain samples from the MouseLight Project from HHMI Janelia<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. In this dataset, projection neurons were sparsely labeled then imaged with a two-photon microscope at a voxel resolution of 0.3 × 0.3 × 1<italic>μ</italic>m. Each axon reconstruction is generated semi-automatically by two independent annotators. The MouseLight reconstructions are sampled roughly every 10<italic>μ</italic>m, so in some cases we retraced the axons at a higher sampling frequency in order to obtain more precise accuracy metrics.</p>
      <p id="Par69">We quantified reconstruction accuracy using two metrics, the first of which is Frechet distance. Frechet distance is commonly described in the setting of dog walking, where both the dog and owner are following their own predetermined paths. The Frechet distance between the two paths then is the minimum length dog leash needed to complete the walk, where both dog and owner are free to vary their walking speeds but are not allowed to backtrack. In our setting we compute the Frechet distance between two discrete paths <inline-formula id="IEq38"><alternatives><tex-math id="M121">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P:\{1,...,{L}_{p}\}\to {{\mathbb{R}}}^{3}$$\end{document}</tex-math><mml:math id="M122"><mml:mi>P</mml:mi><mml:mo>:</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>→</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq38.gif"/></alternatives></inline-formula>, <inline-formula id="IEq39"><alternatives><tex-math id="M123">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Q:\{1,...,{L}_{q}\}\to {{\mathbb{R}}}^{3}$$\end{document}</tex-math><mml:math id="M124"><mml:mi>Q</mml:mi><mml:mo>:</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>→</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="42003_2022_3320_Article_IEq39.gif"/></alternatives></inline-formula> as defined in Eiter and Mannila<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>. In this definition, a coupling between <italic>P</italic> and <italic>Q</italic> is defined as a sequence of ordered pairs:<disp-formula id="Equn"><alternatives><tex-math id="M125">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(P[{a}_{1}],Q[{b}_{1}]),(P[{a}_{2}],Q[{b}_{2}]),...,(P[{a}_{K}],Q[{b}_{K}])$$\end{document}</tex-math><mml:math id="M126"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>Q</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>Q</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>Q</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><graphic xlink:href="42003_2022_3320_Article_Equn.gif" position="anchor"/></alternatives></disp-formula>where the following conditions are put on {<italic>a</italic><sub><italic>k</italic></sub>}, {<italic>b</italic><sub><italic>k</italic></sub>} to ensure that they enumerate through the whole sequences <italic>P</italic> and <italic>Q</italic>:<list list-type="bullet"><list-item><p id="Par70"><italic>a</italic><sub>1</sub>, <italic>b</italic><sub>1</sub> = 1</p></list-item><list-item><p id="Par71"><italic>a</italic><sub><italic>N</italic></sub> = <italic>L</italic><sub><italic>p</italic></sub>, <italic>b</italic><sub><italic>N</italic></sub> = <italic>L</italic><sub><italic>q</italic></sub></p></list-item><list-item><p id="Par72"><italic>a</italic><sub><italic>k</italic></sub> = <italic>a</italic><sub><italic>k</italic>−1</sub> or <italic>a</italic><sub><italic>k</italic></sub> = <italic>a</italic><sub><italic>k</italic>−1</sub> + 1</p></list-item><list-item><p id="Par73"><italic>b</italic><sub><italic>k</italic></sub> = <italic>b</italic><sub><italic>k</italic>−1</sub> or <italic>b</italic><sub><italic>k</italic></sub> = <italic>b</italic><sub><italic>k</italic>−1</sub> + 1</p></list-item></list></p>
      <p id="Par74">Then the discrete Frechet distance is defined as:<disp-formula id="Equo"><alternatives><tex-math id="M127">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\delta }_{dF}(P,Q)=\mathop{\min }\limits_{\,{{{{{{\rm{coupling}}}}}}}\,\{{a}_{k}\},\{{b}_{k}\}}\mathop{\max }\limits_{k\in \{1,...,K\}}\left|P\left[{a}_{k}\right]-Q\left[{b}_{k}\right]\right|$$\end{document}</tex-math><mml:math id="M128"><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>Q</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">coupling</mml:mi><mml:mspace width="0.25em"/><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mfenced close="∣" open="∣"><mml:mrow><mml:mi>P</mml:mi><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mi>Q</mml:mi><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="42003_2022_3320_Article_Equo.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par75">We use the standard Euclidean norm for ∣⋅∣. The discrete Frechet distance is an upper bound to the continuous Frechet distance between polygonal curves, and it can be computed more efficiently. Further, if we take a discrete Frechet distance of zero to be an equivalence relation, then <italic>δ</italic><sub><italic>d</italic><italic>F</italic></sub> is a metric on this set of equivalence classes and thus is a natural way to compare non-branching neuronal reconstructions. In this work, all reconstruction are sampled at at least one point per micron.</p>
      <p id="Par76">Various other performance metrics have been proposed, including an arc-length based precision and recall<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>, a critical node matching based Miss-Extra-Scores (MES)<sup><xref ref-type="bibr" rid="CR45">45</xref></sup> and a vertex matching based spatial distance (SD)<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. We chose to compute SD since it gives a picture of the average spatial distance between two reconstructions. This complements the Frechet distance described earlier, which computes the maximum spatial distance between two reconstructions.</p>
      <p id="Par77">The first step in computing the SD from reconstruction <italic>P</italic> to reconstruction <italic>Q</italic> is, for each point in <italic>P</italic>, finding the distance to the closest point in <italic>Q</italic>. Directed divergence (DDIV) of <italic>P</italic> from <italic>Q</italic> is then defined as the average of all these distances. Then, SD is computed by averaging the DDIV from <italic>P</italic> to <italic>Q</italic> and the DDIV from <italic>Q</italic> to <italic>P</italic>.</p>
    </sec>
    <sec id="Sec19">
      <title>Statistics and reproducibility</title>
      <p id="Par78">The first statistical analysis in this work pertains to foreground/background intensity distributions, and is encapsulated in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. The figure uses data from three image subvolumes, which are provided in the <xref rid="MOESM3" ref-type="media">Supplemental Data</xref>. In panel a, each curve is computed from 5000 randomly selected voxels in the image subvolume, and each error bar represents a single standard deviation of the Fisher z-transformation of the correlation coefficient. In panel b, each curve depicts a Gaussian kernel estimator that was fit to 5000 random voxels. The scipy implementation of Gaussian kernel density estimator was used, with Scott’s rule to determine the bandwidth<sup><xref ref-type="bibr" rid="CR38">38</xref>,<xref ref-type="bibr" rid="CR39">39</xref></sup>. We provide the code to reproduce this figure in brainlit. Note that since the voxels are randomly chosen, the notebook may not reproduce the exact curves in the figure, but the trends, and our conclusions, are robust to different random samples.</p>
      <p id="Par79">The second statistical analysis in this work pertains to the reconstruction outcomes of the different algorithms, as shown in Fig. <xref rid="Fig7" ref-type="fig">7</xref>. A reconstruction is considered a Failure if the trace follows the axon for less than half its length. A reconstruction is considered a Partial Success if it follows the axon for at least half its length, but clearly deviates onto another structure at some point. Reconstructions that follow the axon from the start point to the soma are considered Successes. We compared success rates between algorithms using a two proportion z-test with a significance threshold of 0.05. We also documented the accuracy metrics (Frechet distance, spatial distance) of the successful traces. The outcome counts, the metrics, and the code to produce the plots are provided in the software package. Further, we provide the code that was used to run the ViterBrain reconstructions, and measure the accuracy metrics.</p>
    </sec>
    <sec id="Sec20">
      <title>Reporting summary</title>
      <p id="Par80">Further information on research design is available in the <xref rid="MOESM4" ref-type="media">Nature Research Reporting Summary</xref> linked to this article.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec21">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="42003_2022_3320_MOESM1_ESM.pdf">
            <caption>
              <p>Peer Review File</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM2">
          <media xlink:href="42003_2022_3320_MOESM2_ESM.pdf">
            <caption>
              <p>Supplementary Information</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM3">
          <media xlink:href="42003_2022_3320_MOESM3_ESM.zip">
            <caption>
              <p>Supplementary Data</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM4">
          <media xlink:href="42003_2022_3320_MOESM4_ESM.pdf">
            <caption>
              <p>Reporting Summary</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p>The online version contains supplementary material available at 10.1038/s42003-022-03320-0.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>This work is supported by the National Institutes of Health grant RF1MH121539, P41EB015909, R01NS086888, U19AG033655, the National Science Foundation Grant 2014862, and the National Institute of General Medical Sciences Grant T32GM119998. We thank the MouseLight team at HHMI Janelia for providing us with access to this data, and answering our questions about it.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>M.I.M. helped to develop the HMM and probabilistic model and D.J.T. advised on the theoretical direction of the manuscript. U.M. coordinated the data acquisition for the experiments. J.T.V. advised on the software design. T.L.A. designed the study, implemented the software, and managed the manuscript text/figures. All authors contributed to manuscript revision.</p>
  </notes>
  <notes notes-type="peer-review">
    <title>Peer review</title>
    <sec id="FPar2">
      <title>Peer review information</title>
      <p id="Par81"><italic>Communications Biology</italic> thanks the anonymous reviewers for their contribution to the peer review of this work. Primary Handling Editors: Chao Zhou and Karli Montague-Cardoso. <xref rid="MOESM1" ref-type="media">Peer reviewer reports</xref> are available.</p>
    </sec>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>The datasets analyzed for this study are available in the Open Neurodata AWS account, <ext-link ext-link-type="uri" xlink:href="https://registry.opendata.aws/open-neurodata/">https://registry.opendata.aws/open-neurodata/</ext-link>. Our package, brainlit provides examples of accessing this data.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Code availability</title>
    <p>The code used in this work is available in our open-source Python package brainlit: <ext-link ext-link-type="uri" xlink:href="http://brainlit.neurodata.io/">http://brainlit.neurodata.io/</ext-link>, and a tutorial on how to use the code is located at: <ext-link ext-link-type="uri" xlink:href="http://brainlit.neurodata.io/viterbrain.html">http://brainlit.neurodata.io/viterbrain.html</ext-link>. The version associated with this paper is 10.5281/zenodo.6323454.</p>
  </notes>
  <notes id="FPar3" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par82">M.I.M. owns a significant share of Anatomy Works with the arrangement being managed by Johns Hopkins University in accordance with its conflict of interest policies. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Winnubst</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Reconstruction of 1,000 projection neurons reveals new cell types and organization of long-range connectivity in the mouse brain</article-title>
        <source>Cell</source>
        <year>2019</year>
        <volume>179</volume>
        <fpage>268</fpage>
        <lpage>281</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2019.07.042</pub-id>
        <?supplied-pmid 31495573?>
        <pub-id pub-id-type="pmid">31495573</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <mixed-citation publication-type="other">Peng, H., Meijering, E. &amp; Ascoli, G. A. From diadem to bigneuron. Springer (2015).</mixed-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Peng</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Ruan</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Long</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Simpson</surname>
            <given-names>JH</given-names>
          </name>
          <name>
            <surname>Myers</surname>
            <given-names>EW</given-names>
          </name>
        </person-group>
        <article-title>V3d enables real-time 3d visualization and quantitative analysis of large-scale biological image data sets</article-title>
        <source>Nat. Biotechnol.</source>
        <year>2010</year>
        <volume>28</volume>
        <fpage>348</fpage>
        <lpage>353</lpage>
        <pub-id pub-id-type="doi">10.1038/nbt.1612</pub-id>
        <?supplied-pmid 20231818?>
        <pub-id pub-id-type="pmid">20231818</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Acciai</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Soda</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Iannello</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Automated neuron tracing methods: an updated account</article-title>
        <source>Neuroinformatics</source>
        <year>2016</year>
        <volume>14</volume>
        <fpage>353</fpage>
        <lpage>367</lpage>
        <pub-id pub-id-type="doi">10.1007/s12021-016-9310-0</pub-id>
        <?supplied-pmid 27447185?>
        <pub-id pub-id-type="pmid">27447185</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Peng</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Ruan</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Atasoy</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Sternson</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Automatic reconstruction of 3d neuron structures using a graph-augmented deformable model</article-title>
        <source>Bioinformatics</source>
        <year>2010</year>
        <volume>26</volume>
        <fpage>38</fpage>
        <lpage>46</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btq212</pub-id>
        <pub-id pub-id-type="pmid">19861355</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Narayanaswamy</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Tsai</surname>
            <given-names>C-L</given-names>
          </name>
          <name>
            <surname>Roysam</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>A broadly applicable 3-d neuron tracing method based on open-curve snake</article-title>
        <source>Neuroinformatics</source>
        <year>2011</year>
        <volume>9</volume>
        <fpage>193</fpage>
        <lpage>217</lpage>
        <pub-id pub-id-type="doi">10.1007/s12021-011-9110-5</pub-id>
        <?supplied-pmid 21399937?>
        <pub-id pub-id-type="pmid">21399937</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <mixed-citation publication-type="other">Turetken, E., Benmansour, F., Andres, B., Pfister, H. &amp; Fua, P. Reconstructing loopy curvilinear structures using integer programming. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1822–1829 (2013).</mixed-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Fmst: an automatic neuron tracing method based on fast marching and minimum spanning tree</article-title>
        <source>Neuroinformatics</source>
        <year>2019</year>
        <volume>17</volume>
        <fpage>185</fpage>
        <lpage>196</lpage>
        <pub-id pub-id-type="doi">10.1007/s12021-018-9392-y</pub-id>
        <?supplied-pmid 30039210?>
        <pub-id pub-id-type="pmid">30039210</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Radojević</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Meijering</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Automated neuron tracing using probability hypothesis density filtering</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>33</volume>
        <fpage>1073</fpage>
        <lpage>1080</lpage>
        <?supplied-pmid 28065895?>
        <pub-id pub-id-type="pmid">28065895</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Choromanska</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>S-F</given-names>
          </name>
          <name>
            <surname>Yuste</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Automatic reconstruction of neural morphologies with multi-scale tracking</article-title>
        <source>Front. Neural Circuits</source>
        <year>2012</year>
        <volume>6</volume>
        <fpage>25</fpage>
        <pub-id pub-id-type="doi">10.3389/fncir.2012.00025</pub-id>
        <?supplied-pmid 22754498?>
        <pub-id pub-id-type="pmid">22754498</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Dai, T. et al. Deep reinforcement learning for subpixel neural tracking. In: International Conference on Medical Imaging with Deep Learning, pp. 130–150 (2019).</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Friedmann</surname>
            <given-names>D</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Mapping mesoscale axonal projections in the mouse brain using a 3d convolutional network</article-title>
        <source>Proc. Natl Acad. Sci. USA</source>
        <year>2020</year>
        <volume>117</volume>
        <fpage>11068</fpage>
        <lpage>11075</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.1918465117</pub-id>
        <?supplied-pmid 32358193?>
        <pub-id pub-id-type="pmid">32358193</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Kuo</surname>
            <given-names>H-C</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Long</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Deepneuron: an open deep learning toolbox for neuron tracing</article-title>
        <source>Brain informatics</source>
        <year>2018</year>
        <volume>5</volume>
        <fpage>1</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1186/s40708-018-0081-2</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Deep learning segmentation of optical microscopy images improves 3-d neuron reconstruction</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2017</year>
        <volume>36</volume>
        <fpage>1533</fpage>
        <lpage>1541</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2017.2679713</pub-id>
        <?supplied-pmid 28287966?>
        <pub-id pub-id-type="pmid">28287966</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Teravr empowers precise reconstruction of complete 3-d neuronal morphology in the whole brain</article-title>
        <source>Nat. Commun.</source>
        <year>2019</year>
        <volume>10</volume>
        <fpage>1</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1038/s41467-018-07882-8</pub-id>
        <pub-id pub-id-type="pmid">30602773</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>C-W</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>Y-C</given-names>
          </name>
          <name>
            <surname>Pradana</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Ensemble neuron tracer for 3d neuron reconstruction</article-title>
        <source>Neuroinformatics</source>
        <year>2017</year>
        <volume>15</volume>
        <fpage>185</fpage>
        <lpage>198</lpage>
        <pub-id pub-id-type="doi">10.1007/s12021-017-9325-1</pub-id>
        <?supplied-pmid 28185058?>
        <pub-id pub-id-type="pmid">28185058</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Peng</surname>
            <given-names>H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Automatic tracing of ultra-volumes of neuronal images</article-title>
        <source>Nat. Methods</source>
        <year>2017</year>
        <volume>14</volume>
        <fpage>332</fpage>
        <lpage>333</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.4233</pub-id>
        <?supplied-pmid 28362437?>
        <pub-id pub-id-type="pmid">28362437</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Smarttracing: self-learning-based neuron reconstruction</article-title>
        <source>Brain informatics</source>
        <year>2015</year>
        <volume>2</volume>
        <fpage>135</fpage>
        <lpage>144</lpage>
        <pub-id pub-id-type="doi">10.1007/s40708-015-0018-y</pub-id>
        <?supplied-pmid 27747506?>
        <pub-id pub-id-type="pmid">27747506</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Xiao, H. &amp; Peng, H. App2: automatic tracing of 3d neuron morphology based on hierarchical pruning of a gray-weighted image distance-tree. <italic>Bioinformatics</italic><bold>29</bold>, 1448–1454 (2013).</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Li, R. et al. Precise segmentation of densely interweaving neuron clusters using g-cut. <italic>Nat. Commun.</italic><bold>10</bold>, 1549 (2019).</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Quan</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Neurogps-tree: automatic reconstruction of large-scale neuronal populations with dense neurites</article-title>
        <source>Nat. Methods</source>
        <year>2016</year>
        <volume>13</volume>
        <fpage>51</fpage>
        <lpage>54</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.3662</pub-id>
        <?supplied-pmid 26595210?>
        <pub-id pub-id-type="pmid">26595210</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kass</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Witkin</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Terzopoulos</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Snakes: active contour models</article-title>
        <source>Int. J. Comput. Vis.</source>
        <year>1988</year>
        <volume>1</volume>
        <fpage>321</fpage>
        <lpage>331</lpage>
        <pub-id pub-id-type="doi">10.1007/BF00133570</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cohen</surname>
            <given-names>LD</given-names>
          </name>
        </person-group>
        <article-title>On active contour models and balloons</article-title>
        <source>CVGIP: Image Understanding</source>
        <year>1991</year>
        <volume>53</volume>
        <fpage>211</fpage>
        <lpage>218</lpage>
        <pub-id pub-id-type="doi">10.1016/1049-9660(91)90028-N</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rabiner</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Juang</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>An introduction to hidden markov models</article-title>
        <source>IEEE assp Magazine</source>
        <year>1986</year>
        <volume>3</volume>
        <fpage>4</fpage>
        <lpage>16</lpage>
        <pub-id pub-id-type="doi">10.1109/MASSP.1986.1165342</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Athey, T. L. et al. Fitting splines to axonal arbors quantifies relationship between branch order and geometry. <italic>Front. Neuroinf</italic>. (2021).</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Khaneja, N., Miller, M. I. &amp; Grenander, U. Dynamic Programming Generation of Curves on Brain Surfaces (1998).</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Forney</surname>
            <given-names>GD</given-names>
          </name>
        </person-group>
        <article-title>The viterbi algorithm</article-title>
        <source>Proc. IEEE</source>
        <year>1973</year>
        <volume>61</volume>
        <fpage>268</fpage>
        <lpage>278</lpage>
        <pub-id pub-id-type="doi">10.1109/PROC.1973.9030</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Ronneberger, O., Fischer, P. &amp; Brox, T. U-net: Convolutional networks for biomedical image segmentation. In: International Conference on Medical Image Computing and Computer-assisted Intervention, pp. 234–241 (2015).</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Radojevic, M. &amp; Meijering, E. Automated neuron reconstruction from 3d fluorescence microscopy images using sequential Monte Carlo estimation. <italic>Neuroinformatics</italic><bold>17</bold>10.1007/s12021-018-9407-8 (2019).</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Gtree: an open-source tool for dense reconstruction of brain-wide neuronal population</article-title>
        <source>Neuroinformatics</source>
        <year>2021</year>
        <volume>19</volume>
        <fpage>305</fpage>
        <lpage>317</lpage>
        <pub-id pub-id-type="doi">10.1007/s12021-020-09484-6</pub-id>
        <?supplied-pmid 32844332?>
        <pub-id pub-id-type="pmid">32844332</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Napari contributors: Napari: a Multi-dimensional Image Viewer for Python. 10.5281/zenodo.3555620</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Berg, S. et al. ilastik: interactive machine learning for (bio)image analysis. <italic>Nature Methods</italic>10.1038/s41592-019-0582-9 (2019).</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">Liu, M., Luo, H., Tan, Y., Wang, X. &amp; Chen, W. Improved v-net based image segmentation for 3d neuron reconstruction. In: 2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), pp. 443–448 (2018).</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>3d neuron reconstruction in tangled neuronal image with deep networks</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2019</year>
        <volume>39</volume>
        <fpage>425</fpage>
        <lpage>435</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2019.2926568</pub-id>
        <?supplied-pmid 31295108?>
        <pub-id pub-id-type="pmid">31295108</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Wang, H. et al. Voxel-wise cross-volume representation learning for 3d neuron reconstruction. In: International Workshop on Machine Learning in Medical Imaging, pp. 248–257 (2021).</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Brain-wide shape reconstruction of a traced neuron using the convex image segmentation method</article-title>
        <source>Neuroinformatics</source>
        <year>2020</year>
        <volume>18</volume>
        <fpage>199</fpage>
        <lpage>218</lpage>
        <pub-id pub-id-type="doi">10.1007/s12021-019-09434-x</pub-id>
        <?supplied-pmid 31396858?>
        <pub-id pub-id-type="pmid">31396858</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Snyder, D. L. &amp; Miller, M. I. Random Point Processes in Time and Space. Springer, New York (2012).</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Virtanen</surname>
            <given-names>P</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SciPy 1.0: fundamental algorithms for scientific computing in python</article-title>
        <source>Nature Methods</source>
        <year>2020</year>
        <volume>17</volume>
        <fpage>261</fpage>
        <lpage>272</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id>
        <?supplied-pmid 32015543?>
        <pub-id pub-id-type="pmid">32015543</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Scott, D. W. Multivariate density estimation: theory, practice, and visualization. John Wiley &amp; Sons, Hoboken, New Jersey (2015).</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">Cover, T. M. &amp; Thomas, J. A. Elements of Information Theory vol. 2. Wiley, Hoboken, New Jersey (1991).</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bellman</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>On a routing problem</article-title>
        <source>Quarterly Appl. Math.</source>
        <year>1958</year>
        <volume>16</volume>
        <fpage>87</fpage>
        <lpage>90</lpage>
        <pub-id pub-id-type="doi">10.1090/qam/102435</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dijkstra</surname>
            <given-names>EW</given-names>
          </name>
        </person-group>
        <article-title>A note on two problems in connexion with graphs</article-title>
        <source>Numerische mathematik</source>
        <year>1959</year>
        <volume>1</volume>
        <fpage>269</fpage>
        <lpage>271</lpage>
        <pub-id pub-id-type="doi">10.1007/BF01386390</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bresenham</surname>
            <given-names>JE</given-names>
          </name>
        </person-group>
        <article-title>Algorithm for computer control of a digital plotter</article-title>
        <source>IBM Systems J.</source>
        <year>1965</year>
        <volume>4</volume>
        <fpage>25</fpage>
        <lpage>30</lpage>
        <pub-id pub-id-type="doi">10.1147/sj.41.0025</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <mixed-citation publication-type="other">Eiter, T. &amp; Mannila, H. Computing discrete fréchet distance. Technical report, Citeseer (1994).</mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xie</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Myers</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Anisotropic path searching for automatic neuron reconstruction</article-title>
        <source>Med. Image Anal.</source>
        <year>2011</year>
        <volume>15</volume>
        <fpage>680</fpage>
        <lpage>689</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2011.05.013</pub-id>
        <?supplied-pmid 21669547?>
        <pub-id pub-id-type="pmid">21669547</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
