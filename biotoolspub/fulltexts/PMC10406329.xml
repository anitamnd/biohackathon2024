<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS One</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-title-group>
      <journal-title>PLOS ONE</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1932-6203</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10406329</article-id>
    <article-id pub-id-type="publisher-id">PONE-D-23-08782</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pone.0289499</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
            <subj-group>
              <subject>Deep Learning</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Neural Networks</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Neural Networks</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Imaging Techniques</subject>
          <subj-group>
            <subject>Image Analysis</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Oncology</subject>
          <subj-group>
            <subject>Cancers and Neoplasms</subject>
            <subj-group>
              <subject>Breast Tumors</subject>
              <subj-group>
                <subject>Breast Cancer</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>People and Places</subject>
        <subj-group>
          <subject>Population Groupings</subject>
          <subj-group>
            <subject>Professions</subject>
            <subj-group>
              <subject>Medical Personnel</subject>
              <subj-group>
                <subject>Pathologists</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Urology</subject>
          <subj-group>
            <subject>Genitourinary Cancers</subject>
            <subj-group>
              <subject>Testicular Cancer</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Oncology</subject>
          <subj-group>
            <subject>Cancers and Neoplasms</subject>
            <subj-group>
              <subject>Genitourinary Tract Tumors</subject>
              <subj-group>
                <subject>Testicular Cancer</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Oncology</subject>
          <subj-group>
            <subject>Cancers and Neoplasms</subject>
            <subj-group>
              <subject>Genitourinary Tract Tumors</subject>
              <subj-group>
                <subject>Prostate Cancer</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Urology</subject>
          <subj-group>
            <subject>Prostate Diseases</subject>
            <subj-group>
              <subject>Prostate Cancer</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Gastroenterology and Hepatology</subject>
          <subj-group>
            <subject>Gastrointestinal Cancers</subject>
          </subj-group>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>SliDL: A toolbox for processing whole-slide images in deep learning</article-title>
      <alt-title alt-title-type="running-head">SliDL: A toolbox for processing whole-slide images in deep learning</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Berman</surname>
          <given-names>Adam G.</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role>
        <role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role>
        <role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/software/">Software</role>
        <role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role>
        <role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Orchard</surname>
          <given-names>William R.</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Gehrung</surname>
          <given-names>Marcel</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/software/">Software</role>
        <xref rid="aff001" ref-type="aff"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Markowetz</surname>
          <given-names>Florian</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role>
        <role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role>
        <role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role>
        <xref rid="aff001" ref-type="aff"/>
        <xref rid="cor001" ref-type="corresp">*</xref>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <addr-line>Cancer Research UK Cambridge Institute, University of Cambridge, Cambridge, United Kingdom</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Fernandez-Lozano</surname>
          <given-names>Carlos</given-names>
        </name>
        <role>Editor</role>
        <xref rid="edit1" ref-type="aff"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>University of A Coruña, SPAIN</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p><bold>Competing Interests: </bold>I have read the journal’s policy and the authors of this manuscript have the following competing interests: M.G. is an employee and shareholder of Cyted Ltd. F.M. is a co-founder and director of Tailor Bio. This does not alter our adherence to PLOS ONE policies on sharing data and materials.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>florian.markowetz@cruk.cam.ac.uk</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>7</day>
      <month>8</month>
      <year>2023</year>
    </pub-date>
    <volume>18</volume>
    <issue>8</issue>
    <elocation-id>e0289499</elocation-id>
    <history>
      <date date-type="received">
        <day>27</day>
        <month>3</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>20</day>
        <month>7</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2023 Berman et al</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder>Berman et al</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pone.0289499.pdf"/>
    <abstract>
      <p>The inspection of stained tissue slides by pathologists is essential for the early detection, diagnosis and monitoring of disease. Recently, deep learning methods for the analysis of whole-slide images (WSIs) have shown excellent performance on these tasks, and have the potential to substantially reduce the workload of pathologists. However, WSIs present a number of unique challenges for analysis, requiring special consideration of image annotations, slide and image artefacts, and evaluation of WSI-trained model performance. Here we introduce SliDL, a Python library for performing pre- and post-processing of WSIs. SliDL makes WSI data handling easy, allowing users to perform essential processing tasks in a few simple lines of code, bridging the gap between standard image analysis and WSI analysis. We introduce each of the main functionalities within SliDL: from annotation and tile extraction to tissue detection and model evaluation. We also provide ‘code snippets’ to guide the user in running SliDL. SliDL has been designed to interact with PyTorch, one of the most widely used deep learning libraries, allowing seamless integration into deep learning workflows. By providing a framework in which deep learning methods for WSI analysis can be developed and applied, SliDL aims to increase the accessibility of an important application of deep learning.</p>
    </abstract>
    <funding-group>
      <award-group id="award001">
        <funding-source>
          <institution>Bill &amp; Melinda Gates Foundation</institution>
        </funding-source>
        <award-id>Gates Cambridge Scholarship</award-id>
        <principal-award-recipient>
          <name>
            <surname>Berman</surname>
            <given-names>Adam G.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award002">
        <funding-source>
          <institution>Cancer Research UK</institution>
        </funding-source>
        <award-id>C14303/A17197</award-id>
        <principal-award-recipient>
          <name>
            <surname>Markowetz</surname>
            <given-names>Florian</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award003">
        <funding-source>
          <institution>Peterhouse, Cambridge</institution>
        </funding-source>
        <award-id>Peterhouse Studentship</award-id>
        <principal-award-recipient>
          <name>
            <surname>Orchard</surname>
            <given-names>William R.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award004">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100012338</institution-id>
            <institution>Alan Turing Institute</institution>
          </institution-wrap>
        </funding-source>
        <award-id>Enrichment Fellowship</award-id>
        <principal-award-recipient>
          <name>
            <surname>Gehrung</surname>
            <given-names>Marcel</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <funding-statement>This research was supported by Cancer Research UK (FM: C14303/A17197, <ext-link xlink:href="https://www.cancerresearchuk.org/" ext-link-type="uri">https://www.cancerresearchuk.org/</ext-link>). A.G.B. acknowledges support from a Gates Cambridge Scholarship from the Bill &amp; Melinda Gates Foundation (<ext-link xlink:href="https://www.gatescambridge.org/" ext-link-type="uri">https://www.gatescambridge.org/</ext-link>). W.R.O. acknowledges support from a Peterhouse Studentship from Peterhouse, Cambridge (<ext-link xlink:href="https://www.pet.cam.ac.uk/" ext-link-type="uri">https://www.pet.cam.ac.uk/</ext-link>). M.G. acknowledges support from an Enrichment Fellowship from the Alan Turing Institute (<ext-link xlink:href="https://www.turing.ac.uk/work-turing/studentships/enrichment" ext-link-type="uri">https://www.turing.ac.uk/work-turing/studentships/enrichment</ext-link>). F.M. is a Royal Society Wolfson Research Merit Award holder (<ext-link xlink:href="https://royalsociety.org/grants-schemes-awards/grants/wolfson-research-merit/" ext-link-type="uri">https://royalsociety.org/grants-schemes-awards/grants/wolfson-research-merit/</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="4"/>
      <table-count count="1"/>
      <page-count count="25"/>
    </counts>
    <custom-meta-group>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>The source code of SliDL is freely available at a public repository: <ext-link xlink:href="https://github.com/markowetzlab/slidl" ext-link-type="uri">https://github.com/markowetzlab/slidl</ext-link>. The source code of a comprehensive SliDL tutorial is also freely available at the following public repository: <ext-link xlink:href="https://github.com/markowetzlab/slidl-tutorial" ext-link-type="uri">https://github.com/markowetzlab/slidl-tutorial</ext-link>. That repository also contains the code used to train SliDL’s deep tissue detector in its deep_tissue_detector subdirectory. Complete documentation of SliDL including its application public interface (API) reference is available at the following URL: <ext-link xlink:href="https://linkprotect.cudasvc.com/url?a=https%3a%2f%2fslidl.readthedocs.io%2fen%2flatest%2f&amp;c=E,1,S9zTxKndcPNyESUe0QhuC-cota0vA0CSPg9ZEs39y3UNkOIHweCHY-B2ognY52rkVtjub0msWdNm276Yj52DPMFfVPVXx3-En7cCLNKYvFHAcgCogA,,&amp;typo=1" ext-link-type="uri">https://linkprotect.cudasvc.com/url?a=https%3a%2f%2fslidl.readthedocs.io%2fen%2flatest%2f&amp;c=E,1,S9zTxKndcPNyESUe0QhuC-cota0vA0CSPg9ZEs39y3UNkOIHweCHY-B2ognY52rkVtjub0msWdNm276Yj52DPMFfVPVXx3-En7cCLNKYvFHAcgCogA,,&amp;typo=1</ext-link>. The CAMELYON-16 WSIs and corresponding annotations used in the SliDL tutorial are freely available and can be downloaded by following the instructions at: <ext-link xlink:href="https://github.com/markowetzlab/slidl-tutorial" ext-link-type="uri">https://github.com/markowetzlab/slidl-tutorial</ext-link>. The WSIs related to the deep tissue detector can be accessed at <ext-link xlink:href="https://linkprotect.cudasvc.com/url?a=https%3a%2f%2fdoi.org%2f10.5281%2fzenodo.7947380&amp;c=E,1,rsogzNLylIHJ4goNx4QP3CJ3g6vTURO4JhL0M9GdLRdapBL-R-DOe0UoTPy6exTung3_MGTjeFNl8ylJcaXF0wIpT89JgjVD4p38UYY91jVClmkF&amp;typo=1" ext-link-type="uri">https://linkprotect.cudasvc.com/url?a=https%3a%2f%2fdoi.org%2f10.5281%2fzenodo.7947380&amp;c=E,1,rsogzNLylIHJ4goNx4QP3CJ3g6vTURO4JhL0M9GdLRdapBL-R-DOe0UoTPy6exTung3_MGTjeFNl8ylJcaXF0wIpT89JgjVD4p38UYY91jVClmkF&amp;typo=1</ext-link>.</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>The source code of SliDL is freely available at a public repository: <ext-link xlink:href="https://github.com/markowetzlab/slidl" ext-link-type="uri">https://github.com/markowetzlab/slidl</ext-link>. The source code of a comprehensive SliDL tutorial is also freely available at the following public repository: <ext-link xlink:href="https://github.com/markowetzlab/slidl-tutorial" ext-link-type="uri">https://github.com/markowetzlab/slidl-tutorial</ext-link>. That repository also contains the code used to train SliDL’s deep tissue detector in its deep_tissue_detector subdirectory. Complete documentation of SliDL including its application public interface (API) reference is available at the following URL: <ext-link xlink:href="https://linkprotect.cudasvc.com/url?a=https%3a%2f%2fslidl.readthedocs.io%2fen%2flatest%2f&amp;c=E,1,S9zTxKndcPNyESUe0QhuC-cota0vA0CSPg9ZEs39y3UNkOIHweCHY-B2ognY52rkVtjub0msWdNm276Yj52DPMFfVPVXx3-En7cCLNKYvFHAcgCogA,,&amp;typo=1" ext-link-type="uri">https://linkprotect.cudasvc.com/url?a=https%3a%2f%2fslidl.readthedocs.io%2fen%2flatest%2f&amp;c=E,1,S9zTxKndcPNyESUe0QhuC-cota0vA0CSPg9ZEs39y3UNkOIHweCHY-B2ognY52rkVtjub0msWdNm276Yj52DPMFfVPVXx3-En7cCLNKYvFHAcgCogA,,&amp;typo=1</ext-link>. The CAMELYON-16 WSIs and corresponding annotations used in the SliDL tutorial are freely available and can be downloaded by following the instructions at: <ext-link xlink:href="https://github.com/markowetzlab/slidl-tutorial" ext-link-type="uri">https://github.com/markowetzlab/slidl-tutorial</ext-link>. The WSIs related to the deep tissue detector can be accessed at <ext-link xlink:href="https://linkprotect.cudasvc.com/url?a=https%3a%2f%2fdoi.org%2f10.5281%2fzenodo.7947380&amp;c=E,1,rsogzNLylIHJ4goNx4QP3CJ3g6vTURO4JhL0M9GdLRdapBL-R-DOe0UoTPy6exTung3_MGTjeFNl8ylJcaXF0wIpT89JgjVD4p38UYY91jVClmkF&amp;typo=1" ext-link-type="uri">https://linkprotect.cudasvc.com/url?a=https%3a%2f%2fdoi.org%2f10.5281%2fzenodo.7947380&amp;c=E,1,rsogzNLylIHJ4goNx4QP3CJ3g6vTURO4JhL0M9GdLRdapBL-R-DOe0UoTPy6exTung3_MGTjeFNl8ylJcaXF0wIpT89JgjVD4p38UYY91jVClmkF&amp;typo=1</ext-link>.</p>
  </notes>
</front>
<body>
  <sec sec-type="intro" id="sec001">
    <title>Introduction</title>
    <p>In histopathology, tissue biopsies are fixed, embedded, sectioned, stained, and placed on a glass slide before being examined under a microscope. Examination of tissue slides to identify pathologically relevant features has been an essential tool for early detection, diagnosis and disease monitoring in medical practice and research for decades. Pathological features can be anything from the presence or absence of certain cell types or populations, changes in cellular or nuclear morphology, changes in the arrangement of cells in a tissue, to changes in the intensity of certain tissue stains. Until recently only expert pathologists have been able to perform this task, requiring years of training, and with individual slides often having to be evaluated by multiple pathologists before a judgement can be made [<xref rid="pone.0289499.ref001" ref-type="bibr">1</xref>]. However, with a shift towards digitisation in pathology, tissue-slides are now routinely scanned to produce high-resolution whole-slide images (WSIs). Such images are amenable to automated image analysis and in the last decade the field has undergone a revolution. Deep learning methods for image analysis have shown excellent performance on diagnostic tasks [<xref rid="pone.0289499.ref001" ref-type="bibr">1</xref>–<xref rid="pone.0289499.ref003" ref-type="bibr">3</xref>], rivalling that of pathologists and further stimulating efforts to digitise glass slides.</p>
    <p>Pathologists have high inter-observer concordance rates on some diagnostic tasks, but in others they frequently disagree [<xref rid="pone.0289499.ref004" ref-type="bibr">4</xref>]. This is compounded by high workload, necessitating rapid screening of individual cases, increasing the risk of introducing diagnostic errors [<xref rid="pone.0289499.ref005" ref-type="bibr">5</xref>]. Deep learning methods are fast, often requiring only a few minutes to evaluate a slide, and give consistent evaluations. Thus, deep learning has the potential to substantially reduce the workload of pathologists, improve the inter-observer concordance rates and accelerate the evaluation of tissue-slides. The application of deep learning to pathological datasets is therefore a quickly growing field, as researchers apply the latest advances in machine learning, such as GANs [<xref rid="pone.0289499.ref006" ref-type="bibr">6</xref>, <xref rid="pone.0289499.ref007" ref-type="bibr">7</xref>] and transformers [<xref rid="pone.0289499.ref008" ref-type="bibr">8</xref>, <xref rid="pone.0289499.ref009" ref-type="bibr">9</xref>], to whole slide image problems [<xref rid="pone.0289499.ref010" ref-type="bibr">10</xref>, <xref rid="pone.0289499.ref011" ref-type="bibr">11</xref>].</p>
    <p>Despite this potential, deep learning based approaches have not yet seen widespread uptake in medical practice. This is in part due to a lack of an accessible framework in which WSI neural network implementations are developed and applied, meaning that individual researchers often must re-implement their own pre- and post-processing pipelines in-house for each new histopathology task. Furthermore, successful implementation of deep learning to WSI analysis requires careful consideration of model hyperparameters, slide and image artefacts and data augmentation beyond those encountered in standard image analysis, and thus application of the latest advances in computer vision to WSI analysis is hampered without a framework for streamlined WSI processing into which such advances can be incorporated.</p>
    <p>Here we introduce SliDL, a new Python library for performing pre- and post-processing of WSI data. SliDL simplifies and streamlines many of the steps required to tackle the unique challenges posed by WSIs. This includes, but is not limited to, detection of tissue, slide and tissue artefacts and background in WSIs, easy implementation of alternative tiling strategies, automatic generation of binary and multi-class segmentation masks from digital annotations, and utility functions for visualisation and evaluation of model outputs (see <xref rid="pone.0289499.g001" ref-type="fig">Fig 1</xref> and <xref rid="pone.0289499.s002" ref-type="supplementary-material">S1 Table</xref> for an overview of the main functionalities in SliDL). Although other tools exist which provide some of the same functionalities for pre-processing, SliDL is unique in its comprehensive support for annotation handling (see Related methods). By simplifying and streamlining these steps, SliDL aims to empower researchers in the clinical sciences to accelerate the application of deep learning to both existing and newly generated WSI data, so that the latest innovations in digital pathology can reach the clinic sooner.</p>
    <fig position="float" id="pone.0289499.g001">
      <object-id pub-id-type="doi">10.1371/journal.pone.0289499.g001</object-id>
      <label>Fig 1</label>
      <caption>
        <title>A deep learning pipeline with SliDL.</title>
        <p><bold>(A)</bold> Creating SliDL Slide objects, exporting and adding annotations, and extracting tiles and segmentation masks. <bold>(B)</bold> Data partitioning, data augmentation, and model training. <bold>(C)</bold> Inferring on the trained model and stitching together overlapping segmentation results (if required).</p>
      </caption>
      <graphic xlink:href="pone.0289499.g001" position="float"/>
    </fig>
    <p>SliDL therefore takes into account all of eccentricities of the WSI data type. For example, their large size makes it is necessary to break up WSIs into ‘tiles’ before they can be analysed by contemporary deep learning architectures (see Tiling). Tiling, however, introduces further difficulties as WSIs are often labelled (e.g. cancerous or non-cancerous) at the slide level, not at the tile level, and so a deep learning approach must be adopted which accounts for how tiles inherit labels from slides (see Annotation). Furthermore, WSIs can contain unique artefacts introduced during slide preparation and imaging, which are not found in other image analysis settings, such as pen marks left by the pathologists reviewing them, or cracks and bubbles in the slide. All of these artefacts must be removed or accounted for when training a deep learning model (see Deep tissue detector). SliDL includes easy-to-use functions to both perform tiling and to filter out artefact and background slides using a built-in deep neural network.</p>
    <p>SliDL has been designed to interact with popular deep learning library PyTorch [<xref rid="pone.0289499.ref012" ref-type="bibr">12</xref>], allowing it to be seamlessly incorporated into deep learning workflows. By tackling the unique challenges posed by WSIs, SliDL can help to translate deep learning methods into the clinic more easily, providing a broad method to replace <italic toggle="yes">ad hoc</italic> solutions, and an accessible entry point to applying deep learning to WSIs for machine learning researchers unfamiliar with the nuances of pathology slides. SliDL is available from <ext-link xlink:href="https://github.com/markowetzlab/slidl" ext-link-type="uri">https://github.com/markowetzlab/slidl</ext-link>, documentation is available at <ext-link xlink:href="https://slidl.readthedocs.io/en/latest/" ext-link-type="uri">https://slidl.readthedocs.io/en/latest/</ext-link>, and a full tutorial and example SliDL workflow is available at <ext-link xlink:href="https://github.com/markowetzlab/slidl-tutorial" ext-link-type="uri">https://github.com/markowetzlab/slidl-tutorial</ext-link>, which trains, infers, and evaluates classification and segmentation models built from a balanced dataset of nine lymph node node section WSIs containing breast cancer metastases, and nine without metastases. The WSIs come from the publicly-available CAMELYON16 dataset [<xref rid="pone.0289499.ref013" ref-type="bibr">13</xref>].</p>
    <p>SliDL was motivated by a perceived gap in existing tools comprising a number of features we see as crucial to the application of deep learning to histopathological tasks, which we therefore implemented and included in SliDL. Among these are its in-built, robust, benchmarked deep tissue detector, its numerous functions for reading in and extracting tiles from digital annotations, its capacity to generate tile-level segmentation masks, and its capacity to perform inference and compute performance metrics (see Distinct advantages of SliDL). All of these features and many more are made immediately accessible to novice and expert digital pathologists: SliDL is capable of shortening hundreds of lines of code requiring in-depth knowledge of image analysis into five to fifteen idiomatic lines which can be understood and implemented quickly and easily.</p>
    <p>In this article we describe each of the major functionalities of SliDL in the order of their application in a typical deep learning pipeline. First showing how WSI data is handled within SliDL and how to import WSIs (Handling whole-slide images), implement different tiling strategies (Tiling). Then, we move on to how to apply the ‘deep tissue detector’ to detect tissue and remove background and artefacts from slides (Deep tissue detector). Next, we show how SliDL enables handling of digital annotations and the extraction of tiles and their corresponding segmentation masks (Annotation), before finally demonstrating how the library can be used to streamline various aspects of model training (Training), inference (Inference) and evaluation (Evaluating model performance). In each section, ‘code snippets’ are provided giving guidance on how SliDL should be run (see <xref rid="pone.0289499.s003" ref-type="supplementary-material">S2 Table</xref> for a table detailing each of the functions displayed in code snippets below and defining their arguments).</p>
  </sec>
  <sec id="sec002">
    <title>Implementation</title>
    <sec id="sec003">
      <title>Handling whole-slide images</title>
      <p>When glass slides are digitised by digital whole-slide image scanners, high-resolution images are taken at multiple magnifications. WSIs therefore have a pyramidal data structure, with the images taken at each magnification each forming a ‘layer’ of the WSI. The maximum magnification of these images is frequently 200X (by convention called ‘20X’, due to scanning being performed using a 20X objective lens at 10X magnification) or 400X (by convention ‘40X’, using a 40X objective lens at 10X magnification) [<xref rid="pone.0289499.ref014" ref-type="bibr">14</xref>].</p>
      <p>SliDL uses the Pyvips library for reading WSIs [<xref rid="pone.0289499.ref015" ref-type="bibr">15</xref>], and so supports a wide range of formats, including NDPI and pyramidal TIFF, including all OpenSlide formats, and those which are loaded via ImageMagick or GraphicsMagick such as DICOM [<xref rid="pone.0289499.ref014" ref-type="bibr">14</xref>]. After importing SliDL, WSIs are instantiated as <monospace specific-use="no-wrap">Slide</monospace> objects by calling the <monospace specific-use="no-wrap">Slide</monospace> class on the file path to the WSI and specifying which layer you would like to access with the <monospace specific-use="no-wrap">level</monospace> argument.</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 1</bold>. <bold>Import SliDL and initialise a Slide object with a path to a WSI</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
from slidl.slide import Slide
slidl_slide = Slide(path_to_wsi, level=0)
</preformat>
      <p>It is through <monospace specific-use="no-wrap">Slide</monospace> objects that the user interacts with their data and performs the pre- and post-processing steps described in the following sections.</p>
      <p>During pre- and post-processing, SliDL <monospace specific-use="no-wrap">Slide</monospace> objects are generally modified in-place, with new information being added to an internal dictionary (called the ‘tile dictionary’, see Tiling below). It is therefore important that users save <monospace specific-use="no-wrap">Slide</monospace> objects after performing an operation on them, particularly time-intensive operations such as applying the deep tissue detector (see Deep tissue detector below). By doing this, the user does not have to wait for expensive functions to complete more than once.</p>
      <p>Saving <monospace specific-use="no-wrap">Slide</monospace> is achieved by using the <monospace specific-use="no-wrap">Slide.save()</monospace> method, preserving the entire <monospace specific-use="no-wrap">Slide</monospace> object in its current state to a <monospace specific-use="no-wrap">.pml</monospace> file in the directory specified by the <monospace specific-use="no-wrap">folder</monospace> argument using lossless compression.</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 2</bold>. <bold>Save a Slide object</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
slidl_slide.save(folder='path/to/folder')
</preformat>
      <p>To reload a <monospace specific-use="no-wrap">Slide</monospace> object which has been saved, simply set the first argument of the <monospace specific-use="no-wrap">Slide</monospace> initialiser to the <monospace specific-use="no-wrap">Slide</monospace> object rather than to a WSI.</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 3</bold>. <bold>Reload a saved Slide object</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
slidl_slide = Slide('/path/to/folder/slidl_slide.pml', level=0)
</preformat>
    </sec>
    <sec id="sec004">
      <title>Tiling</title>
      <p>WSI images are very large; for example, an image scanned at 40X objective power of a 20 x 20 mm sample of tissue has 80,000 x 80,000 pixels; at standard 24-bit colour this would produce a flat image 19.2GB in size. Current neural network architectures are unable to process images of this size in one go. Thus, WSIs are broken up into ‘tiles’ or ‘patches’ upon which the model is trained: small square regions of the original image, typically 32 to 1000 pixels in height. Tiles can be chosen with or without overlap with neighbouring tiles. Choice of tile dimensions and overlap are some of the most important hyperparameters to choose when analysing WSIs [<xref rid="pone.0289499.ref016" ref-type="bibr">16</xref>].</p>
      <p>In SliDL, tile dimensions and overlap are chosen by calling the <monospace specific-use="no-wrap">Slide.setTileProperties()</monospace> method, and setting the <monospace specific-use="no-wrap">tileSize</monospace> and <monospace specific-use="no-wrap">tileOverlap</monospace> arguments, enabling users to easily experiment with different tiling strategies. Here one can also specify how tiles should be stored, and how they will be accessed during training.</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 4</bold>. <bold>Set the tile properties in a Slide</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
slidl_slide.setTileProperties(tileSize=500, tileOverlap=0, unit='px')
</preformat>
      <p>By calling <monospace specific-use="no-wrap">Slide.extractAnnotationTiles()</monospace> or <monospace specific-use="no-wrap">Slide.extractRandomUnannotatedTiles()</monospace>, one can extract and store each tile in individual images files in advance of training or inferring. SliDL will automatically store tile image files according to their class (see Annotation) and slide of origin in a directory structure appropriate for use with PyTorch see <xref rid="pone.0289499.s001" ref-type="supplementary-material">S1 Fig</xref>).</p>
      <p>Although functional, storing each individual tile image file may pose data storage issues. SliDL stores the coordinates of each tile rather than the tile image itself, accessible with <monospace specific-use="no-wrap">Slide.getTile()</monospace> using the tile address as argument (all tile addresses can be iterated over with <monospace specific-use="no-wrap">Slide.iterateTiles()</monospace>), making it easy to build a dataset such that tiles are accessed on-the-fly, saving the need to extract each tile to an image file in situations where this would be too memory intensive.</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 5</bold>. <bold>Iterate through the tiles in a Slide</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
for tile_coords in Slide.iterateTiles():
    pyvips_tile_image = slidl_slide.getTile(tile_coords)
</preformat>
      <p>Apart from being substantially less disk memory intensive, this approach also makes it easier to experiment with different tiling strategies without having to re-extract tiles for each combination of tile size and overlap. The trade off is that on-the-fly tile accession approaches are typically much slower to train with, so are not recommended except in datasets where tiles number in the hundreds of thousands or millions and disk memory for these tile images is not available.</p>
    </sec>
    <sec id="sec005">
      <title>Deep tissue detector</title>
      <p>WSIs can contain unique artefacts introduced during slide preparation and imaging which are not found in other image analysis settings. Tissue may tear and fold during slide preparation, the image may be unevenly illuminated or stained, and parts of the image may be out of focus. Tissue slides also often contain pen marks left by the pathologists reviewing them, and may contain cracks and bubbles. Left unaccounted for, such artefacts can have severe detrimental effects on a deep learning model. For instance, pen marks are often left by pathologists to indicate the presence of a pathological feature of interest, such as the presence of cancerous cells. If not removed, a deep learning model may simply learn to recognise the presence of a pen mark in slides containing cancer, and thereby be completely inapplicable in medical practice where no such annotation will be available. Beyond artefacts, WSIs will typically contain large portions of background, i.e. regions without any tissue, which do not contain any pathologically relevant information. After tiling your WSI, tiles which contain artefacts or which simply display background should therefore be removed speed up the training process and potentially improve performance.</p>
      <p>SliDL provides a built-in deep tissue detector: a DenseNet [<xref rid="pone.0289499.ref017" ref-type="bibr">17</xref>] convolutional neural network architecture (see Convolutional neural networks) trained to classify tiles as either ‘artefact’, ‘background’, or ‘tissue’. SliDL’s deep tissue detector was trained using 9,071 tiles extracted from 393 individual annotations from 61 WSIs scanned across a variety of machines, time periods, and tissue types, and two different species to account for a broad range of the variation of WSI artefacts (including pen marks, folded or torn tissue, slide bubbles, cracks, blurred or out-of-focus regions, uneven illumination, aberrant staining and other marks), background, and tissue appearances (<xref rid="pone.0289499.g002" ref-type="fig">Fig 2A</xref>). An imbalanced dataset sampler [<xref rid="pone.0289499.ref018" ref-type="bibr">18</xref>] was used to ensure that during training, the model was exposed to equal numbers of artefact, background, and tissue tiles. The deep tissue detector is applied by calling the <monospace specific-use="no-wrap">Slide.detectTissue()</monospace> method, enabling robust detection of tissue tiles at any level of the WSI pyramid desired. SliDL then saves the output probabilities that each tile belongs to each of the three classes internally for each tile in the <monospace specific-use="no-wrap">Slide</monospace> object.</p>
      <fig position="float" id="pone.0289499.g002">
        <object-id pub-id-type="doi">10.1371/journal.pone.0289499.g002</object-id>
        <label>Fig 2</label>
        <caption>
          <title>Tissue, artefact, and background detection with SliDL.</title>
          <p><bold>(A)</bold> Tiling, augmenting, and training a DenseNet CNN to classify tissue, artefact, and background regions on WSIs from a robust dataset representing multiple tissue and species types. This already-trained deep tissue detector can be applied to any SliDL Slide object with SliDL’s Slide.detectTissue() function. <bold>(B)</bold> Comparison of classical foreground methods to the Deep tissue detector. All tests performed are two-tailed Wilcoxon signed-rank tests (<italic toggle="yes">n</italic> = 36). All <italic toggle="yes">P</italic> values are Benjamini-Hochberg adjusted, *<italic toggle="yes">P</italic> &lt; 0.05, **<italic toggle="yes">P</italic> &lt; 0.01, ***<italic toggle="yes">P</italic> &lt; 0.001. <bold>(C)</bold> Three representative sample slides on which benchmarking was performed. The top row displaying a case where tissue and background are easily distinguished and all three approaches perform well. The middle row displaying a case where a clear pen mark artefact is incorrectly identified as tissue by the two classical approaches, indicating that artefact removal pre-processing is necessary, but the deep tissue detector has automatically performed both artefact and tissue detection. The bottom row displaying where a large bubble artefact obscures much of the tissue. Otsu classifies almost the entire slide as background, Triangle does not exclude tissue obscured by the artefact, and the deep tissue detector successfully identifies both.</p>
        </caption>
        <graphic xlink:href="pone.0289499.g002" position="float"/>
      </fig>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 6</bold>. <bold>Apply the deep tissue detector to a Slide</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
slidl_slide.detectTissue(tissueDetectionLevel=1,
                            tissueDetectionTileSize=512,
                            tissueDetectionTileOverlap=0,
                            tissueDetectionUpsampleFactor=4,
                            batchSize=20,
                            numWorkers=16)
</preformat>
      <p>There may be applications where the artefacts encountered are not well covered by the deep tissue detector in SliDL, and thus one should always review examples of its output to verify that the detector is behaving as expected using SliDL’s <monospace specific-use="no-wrap">Slide.visualizeTissueDetection()</monospace> function (<xref rid="pone.0289499.g002" ref-type="fig">Fig 2C</xref>).</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 7</bold>. <bold>Verify the results of the deep tissue detector</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
slidl_slide.visualizeTissueDetection(fileName='tissue_detection')
</preformat>
      <p>Furthermore, SliDL makes it easy to apply a user-provided tissue detection model trained on additional, or alternative, annotated images by setting the <monospace specific-use="no-wrap">modelStateDictPath</monospace> and <monospace specific-use="no-wrap">architecture</monospace> parameters when calling <monospace specific-use="no-wrap">Slide.detectTissue()</monospace> to the path to the custom model and its neural network architecture, respectively.</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 8</bold>. <bold>Apply a user-provided tissue detection model to a Slide</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
slidl_slide.detectTissue(tissueDetectionLevel=1,
                            tissueDetectionTileSize=512,
                            tissueDetectionTileOverlap=0,
                            tissueDetectionUpsampleFactor=4,
                            batchSize=20,
                            numWorkers=16,
                            modelStateDictPath='/path/to/state_dict.pt',
                            architecture='vgg19')
</preformat>
      <p>In certain cases, users may want to make use of classical foreground filtering approaches in place of or in addition to the deep tissue detector. In SliDL, this can be achieved by calling the <monospace specific-use="no-wrap">Slide.detectForeground()</monospace> method, specifying the desired approach with the <monospace specific-use="no-wrap">threshold</monospace> argument and the desired WSI pyramid level to perform detection on with the <monospace specific-use="no-wrap">level</monospace> argument. Note that even if the foreground or deep tissue detector is applied at a different level from that which tiles are later extracted, the foreground/tissue/artefact/background predictions will be carried over appropriately across layers. SliDL currently supports Otsu’s method [<xref rid="pone.0289499.ref019" ref-type="bibr">19</xref>], the triangle algorithm [<xref rid="pone.0289499.ref020" ref-type="bibr">20</xref>], as well as simple intensity thresholding. All are automatically applied when <monospace specific-use="no-wrap">Slide.detectForeground()</monospace> so that the user is able to access the results of any algorithm in downstream functions.</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 9</bold>. <bold>Apply foreground detection methods to a Slide</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
slidl_slide.detectForeground(level=3)
</preformat>
      <p>In general, however, these approaches are less robust to the diversity of artefacts observed in WSIs, as well as appearances of background and tissue, often requiring careful supervision for each application [<xref rid="pone.0289499.ref016" ref-type="bibr">16</xref>]. We performed a benchmarking analysis comparing the performance of the deep tissue detector, Otsu’s and the Triangle algorithm at distinguishing tissue from background at the tile level (<xref rid="pone.0289499.g002" ref-type="fig">Fig 2B and 2C</xref>). Benchmarking was performed on 36 WSIs spanning 10 different tissues, a wide variety of different artefacts, and a range of different scanning and fixing protocols, in total comprising nearly 1.5 million tiles (see Benchmarking for full details). The balanced accuracy, sensitivity and specificity statistics indicate the strengths and weaknesses of each of the methods. The deep tissue detector has a significantly higher specificity to the Triangle algorithm, but is not significantly different from Otsu’s method, indicating that Otsu’s method is comparatively more conservative. The deep tissue detector has a significantly higher sensitivity to Otsu’s method, but is not significantly different from Triangle. Finally, the deep tissue detector has a significantly higher balanced accuracy to both methods, above 90%, indicating that while it does not decisively outperform both other methods in sensitivity and specificity individually, it does strike the best balance between the two. Furthermore, the deep tissue detector performs substantially more consistently at the task, indicating its greater robustness (<xref rid="pone.0289499.g002" ref-type="fig">Fig 2B</xref>). In <xref rid="pone.0289499.g002" ref-type="fig">Fig 2C</xref> we display three examples which are representative of the range of behaviours shown across the full set of slides.</p>
    </sec>
    <sec id="sec006">
      <title>Annotation</title>
      <p>Ground-truth labels for a WSI may exist either at the region-level, wherein they are local to particular regions within the WSI, or at the slide-level, wherein a label applies to the WSI as a whole. Region-level labels typically take the form of digital annotations on the WSI, delineating the regions belonging to certain classes. Specialised software such as QuPath and the Automated Slide Analysis Platform (ASAP) allow users to draw digital annotations onto WSIs and then export them for use in image analysis workflows [<xref rid="pone.0289499.ref021" ref-type="bibr">21</xref>, <xref rid="pone.0289499.ref022" ref-type="bibr">22</xref>]. SliDL supports the use of annotation files in the GeoJSON format as well as the XML format output by ASAP [<xref rid="pone.0289499.ref023" ref-type="bibr">23</xref>]. QuPath annotations can be exported as GeoJSON files using the Groovy script, <monospace specific-use="no-wrap">qupath_to_geojson.groovy</monospace> provided in the SliDL tutorial repository (<ext-link xlink:href="https://github.com/markowetzlab/slidl-tutorial" ext-link-type="uri">https://github.com/markowetzlab/slidl-tutorial</ext-link>).</p>
      <p>An annotation file is added to a corresponding <monospace specific-use="no-wrap">Slide</monospace> object by calling the <monospace specific-use="no-wrap">Slide.addAnnotations()</monospace> method, providing the file path to the <monospace specific-use="no-wrap">annotationFilePath</monospace> argument. The annotations do not need to cover the entire WSI, instead SliDL parses annotations by designating all pixels bounded by an annotation for a given class as being positive for that class, and all other pixels as negative. Furthermore, in the case of annotations with ‘doughnut holes’ (annotations which are not polygons because they contain holes in their middle), users need to simply annotate the doughnut holes and assign them to their own class. Then, when calling <monospace specific-use="no-wrap">Slide.addAnnotations()</monospace>, the name of this class can be provided to the <monospace specific-use="no-wrap">negativeClass</monospace> argument. SliDL will automatically geometrically subtract these doughnut hole annotations from every other annotation class they overlap with. By default, <monospace specific-use="no-wrap">Slide.addAnnotations()</monospace> parses the annotations of all non-doughnut hole classes into the <monospace specific-use="no-wrap">Slide</monospace> object’s tile dictionary, but users can choose to include only certain classes present in the annotations by specifying them explicitly as a list to the <monospace specific-use="no-wrap">classesToAdd</monospace> argument.</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 10</bold>. <bold>Add annotations to a Slide</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
slidl_slide.addAnnotations(annotationFilePath='/path/to/annotations.xml',
                            classesToAdd=['normal', 'tumor'],
                            negativeClass='doughnut_holes',
                            level=0)
</preformat>
      <p>It is commonly the case that researchers wish to train their models exclusively on tiles which fall within annotated regions. As such, in SliDL tiles can be extracted from annotated regions by applying the <monospace specific-use="no-wrap">Slide.extractAnnotationTiles()</monospace> method. A simple heuristic is applied: a tile is extracted if there are any annotations that cover more than a given threshold fraction of the area of the tile. Tiles which are not covered above this threshold for any annotations are ignored entirely. The threshold is set using the <monospace specific-use="no-wrap">tileAnnotationOverlapThreshold</monospace> argument (default is 0.5). Different thresholds can be applied for different classes by providing a dictionary with class names as keys and their corresponding overlap thresholds as values to the <monospace specific-use="no-wrap">tileAnnotationOverlapThreshold</monospace> argument.</p>
      <p>A user may also want to extract tissue tiles at random from a slide which has not been annotated. Calling <monospace specific-use="no-wrap">Slide.extractRandomUnannotationTiles()</monospace> achieves this, and involves most of the same arguments as <monospace specific-use="no-wrap">Slide.extractAnnotationTiles()</monospace>. The <monospace specific-use="no-wrap">unannotatedClassName</monospace> argument allows the user to specify how unannotated tiles should be named.</p>
      <p>In addition, after calling the <monospace specific-use="no-wrap">Slide.detectTissue()</monospace> method on a given <monospace specific-use="no-wrap">Slide</monospace> object, each tile will be inferred on the deep tissue detector, and the resulting tissue probabilities will be stored for each tile in the ‘tile dictionary’. The <monospace specific-use="no-wrap">tissueLevelThreshold</monospace> argument can then be used in subsequent functions such as the tile extraction functions (<monospace specific-use="no-wrap">Slide.extractAnnotationTiles()</monospace> and <monospace specific-use="no-wrap">Slide.extractRandomUnannotatedTiles()</monospace>) to set a minimum tissue probability for a tile to be extracted (recommended value of 0.995).</p>
      <p>Likewise, <monospace specific-use="no-wrap">foregroundLevelThreshold</monospace> can be used to restrict extraction of tiles to those reaching a desired foreground detection threshold as determined by foreground detection techniques such as [<xref rid="pone.0289499.ref019" ref-type="bibr">19</xref>] (set the argument to ‘otsu’) or the triangle algorithm [<xref rid="pone.0289499.ref020" ref-type="bibr">20</xref>] (set the argument to ‘triangle’). Simple average greyscale intensity filtering can be achieved by setting <monospace specific-use="no-wrap">foregroundLevelThreshold</monospace> to an integer between 0 and 100.</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 11</bold>. <bold>Extract random unannotated tiles from a Slide’s WSI</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
channel_data = slidl_slide.extractRandomUnannotatedTiles(
                            outputDir=output_dir,
                            numTilesToExtract= 500,
                            unannotatedClassName='tissue',
                            tissueLevelThreshold=0.995,
                            foregroundLevelThreshold=88)
</preformat>
      <p>Whether classifying or segmenting, it is important to consider how to derive the ground-truth labels for individual tiles from slide label data. For segmentation tasks the principle is straightforward because the ground-truth is at the pixel-level and thus whole-slide segmentation masks can also be tiled and directly inherited by the individual image tiles. As such, once annotations have been added, SliDL allows the user to create both binary and multi-class tile-level segmentation masks for provided annotations. When applying the <monospace specific-use="no-wrap">Slide.extractAnnotationTiles()</monospace> method, binary segmentation masks are created by setting the <monospace specific-use="no-wrap">extractSegmentationMasks</monospace> argument to <monospace specific-use="no-wrap">True</monospace>, and the <monospace specific-use="no-wrap">classesToExtract</monospace> argument set to the name of the class (or list of classes) for which the binary segmentation masks should be created. If the user wishes for empty class directories to be created for classes not present in the annotation, they can be defined with the <monospace specific-use="no-wrap">otherClassNames</monospace> argument. The tile-level masks are then saved to a ‘masks’ directory in the location specified by the <monospace specific-use="no-wrap">outputDir</monospace> argument of <monospace specific-use="no-wrap">Slide.extractAnnotationTiles()</monospace>.</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 12</bold>. <bold>Extract tiles from the annotated regions of a Slide’s WSI</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
channel_data = slidl_slide.extractAnnotationTiles(
                            outputDir='/path/to/folder',
                            classesToExtract=['normal', 'tumor']
                            tileAnnotationOverlapThreshold=0.3,
                            numTilesToExtractPerClass=500,
                            extractSegmentationMasks=True,
                            tissueLevelThreshold=0.995,
                            foregroundLevelThreshold=88)
</preformat>
      <p>In the case of binary segmentation, a separate segmentation mask for each desired class will be extracted. In multi-class segmentation problems, users may want to return ‘stacks’ of tile segmentation masks, where each layer of the stack is the segmentation mask of a different class. SliDL allows users to easily generate these ‘stacked’ multi-class segmentation masks for each tile they extract. <monospace specific-use="no-wrap">Slide.extractAnnotationTilesMultiClassSegmentation()</monospace> performs the same basic tasks as <monospace specific-use="no-wrap">Slide.extractAnnotationTiles()</monospace>, extracting tiles and their corresponding segmentation masks from an annotated WSI, but instead of outputting flat segmentation mask images to the segmentation mask directories, it outputs stacked Numpy ndarray matrices (saved as <monospace specific-use="no-wrap">.npy</monospace> files) as multi-class segmentation masks for each tile instead.</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 13</bold>. <bold>Extract segmentation mask tiles from the annotated regions of a Slide’s WSI</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
channel_data = slidl_slide.extractAnnotationTilesMultiClassSegmentation(
                            outputDir=output_dir,
                            classesToExtract=['normal', 'tumor']
                            tileAnnotationOverlapThreshold=0.3,
                            numTilesToExtractPerClass=500,
                            tissueLevelThreshold=0.995,
                            foregroundLevelThreshold=88)
</preformat>
      <p>For classification tasks a tile inherits the label of any annotations that cover more than the given threshold fraction of the area of the tile specified by the <monospace specific-use="no-wrap">tileAnnotationOverlapThreshold</monospace> argument of the <monospace specific-use="no-wrap">Slide.extractAnnotationTiles()</monospace> method discussed above. In addition, this heuristic is also applied for the <monospace specific-use="no-wrap">numTilesToExtractPerClass</monospace> argument which allows the user to set the maximum number of tiles to extract per class from the slide (default is 100 for each class). This ensures that the user does not extract many more tiles than they need for training for a given class. When there are more extractable tiles for a given class than are requested by the user, <monospace specific-use="no-wrap">numTilesToExtractPerClass</monospace> tiles are selected at random from the class. This argument can be used regardless of whether the user is performing a classification or segmentation task downstream of the tile extraction.</p>
      <p>In addition to extracting the tiles to directories, unless <monospace specific-use="no-wrap">returnTileStats</monospace> is set to <monospace specific-use="no-wrap">False</monospace>, <monospace specific-use="no-wrap">Slide.extractAnnotationTiles()</monospace> returns some summary statistics of all the tile images which were extracted. These values can be used to compute the mean and variance of each of the colour channels across the tile dataset, which some users may want to use to normalise their tiles prior to training.</p>
    </sec>
    <sec id="sec007">
      <title>Training</title>
      <p>Building a dataset with labels that can be interpreted by a deep learning library is an important training consideration. SliDL’s tile extraction functions (<monospace specific-use="no-wrap">Slide.extractAnnotationTiles()</monospace> and <monospace specific-use="no-wrap">Slide.extractRandomUnannotatedTiles()</monospace>) output tiles and labels in a directory structures that are by default compliant for direct input into PyTorch’s <monospace specific-use="no-wrap">torchvision.datasets.ImageFolder</monospace> dataset constructor (see <xref rid="pone.0289499.s001" ref-type="supplementary-material">S1 Fig</xref>), making it straightforward to load the tiles SliDL has extracted into a format ready for training.</p>
      <p>For classification tasks, per <monospace specific-use="no-wrap">torchvision.datasets.ImageFolder</monospace>, tile labels are stored as directory names within a parent directory containing the slide or case name [<xref rid="pone.0289499.ref012" ref-type="bibr">12</xref>]. As SliDL has been designed to perform pre- and post-processing, training should be performed by using a separate, complementary library (see Related methods). During inference the inference step (Inference, SliDL is capable of accepting any PyTorch image model which has been trained, which includes both convolutional neural networks (CNNs) and vision transformers (ViTs).</p>
      <sec id="sec008">
        <title>Convolutional neural networks</title>
        <p>The first of the two main types of deep neural networks used for image data today, and the sort of deep neural network used to train the deep tissue detector (Deep tissue detector) is the convolutional neural network, which has shown outstanding performance on image classification tasks [<xref rid="pone.0289499.ref024" ref-type="bibr">24</xref>–<xref rid="pone.0289499.ref026" ref-type="bibr">26</xref>]. CNNs are a type of learning algorithm designed to take as input data which are spatially invariant (also known as “shift invariant”), a characteristic whereby small translations of the input data are tolerated. Since recognising common patterns in image data benefits from this trait, CNNs trained on images have proven to be highly successful and are one of the main deep learning architectures used in computer vision, including on tasks involving WSIs [<xref rid="pone.0289499.ref016" ref-type="bibr">16</xref>, <xref rid="pone.0289499.ref025" ref-type="bibr">25</xref>, <xref rid="pone.0289499.ref027" ref-type="bibr">27</xref>].</p>
        <p>Image-oriented CNNs work by including a set of two-dimensional convolution operations performed in a sweeping motion over the surface of the input image at each layer to transform it into an increasingly abstract representation (<xref rid="pone.0289499.g003" ref-type="fig">Fig 3</xref>). The weights in the filters (also known as the “kernels”) of each convolutional layer are what are used in these operations. The filter weights are learned during backpropagation, so that relevant aspects of the image discerned during training can be retained as the feature maps derived from the image are passed from one layer to the next. In between convolution layers are maximum pooling layers, which select the largest element within each receptive field to shrink the feature maps’ spatial resolution <xref rid="pone.0289499.g003" ref-type="fig">Fig 3</xref>. Through this reduction, pooling layers allow for the spatial invariance characteristic described above [<xref rid="pone.0289499.ref028" ref-type="bibr">28</xref>]. After the alternating convolutional and pooling layers are typically one or several fully-connected layers to coerce the number of features down to the desired class size (in the case of classification models) or down to the pixel mask size (in the case of segmentation models) [<xref rid="pone.0289499.ref028" ref-type="bibr">28</xref>].</p>
        <fig position="float" id="pone.0289499.g003">
          <object-id pub-id-type="doi">10.1371/journal.pone.0289499.g003</object-id>
          <label>Fig 3</label>
          <caption>
            <title>Structure of convolutional neural network and vision transformer.</title>
            <p><bold>(a)</bold> A VGG-like convolutional neural network with alternating convolutional layers and maximum pooling operations before a few fully connected layers at the end. Figure is based on the VGG paper [<xref rid="pone.0289499.ref029" ref-type="bibr">29</xref>] and images of this architecture made by others. <bold>(b)</bold> A vision transformer, based on the figure in the original ViT paper [<xref rid="pone.0289499.ref009" ref-type="bibr">9</xref>].</p>
          </caption>
          <graphic xlink:href="pone.0289499.g003" position="float"/>
        </fig>
        <p>CNNs have been the state of the art for classification and segmentatioon tasks on whole-slide images for several years [<xref rid="pone.0289499.ref001" ref-type="bibr">1</xref>].</p>
      </sec>
      <sec id="sec009">
        <title>Vision transformers</title>
        <p>Transformers are a newer neural network type which was originally designed for sequence-based problems such as natural language processing (NLP), which is the use of machine learning to interpret text [<xref rid="pone.0289499.ref008" ref-type="bibr">8</xref>]. Previously, NLP tasks were primarily performed with recurrent neural networks (RNNs), but a fundamental limitation of RNNs is that the way they process sequences precludes computation from being parallelised during training, thereby limiting the length of sequences that can be trained on [<xref rid="pone.0289499.ref008" ref-type="bibr">8</xref>]. Attention mechanisms began to be used in RNNs to allow longer distances between sequence dependencies to be learned [<xref rid="pone.0289499.ref030" ref-type="bibr">30</xref>, <xref rid="pone.0289499.ref031" ref-type="bibr">31</xref>]. Vaswani et al. [<xref rid="pone.0289499.ref008" ref-type="bibr">8</xref>] developed the first transformer architecture, which is composed entirely of attention mechanisms, entirely sidestepping the sequence dependency issue inherent to RNN-based models; transformers therefore tend to outperform and have functionally replaced RNNs [<xref rid="pone.0289499.ref008" ref-type="bibr">8</xref>, <xref rid="pone.0289499.ref032" ref-type="bibr">32</xref>].</p>
        <p>Dosovitskiy et al. [<xref rid="pone.0289499.ref009" ref-type="bibr">9</xref>] described the Vision Transformer (ViT), a transformer architecture modified to take image data as input. Although several CNNs had previously been designed to incorporate some attention mechanisms within them [<xref rid="pone.0289499.ref033" ref-type="bibr">33</xref>–<xref rid="pone.0289499.ref035" ref-type="bibr">35</xref>], this was the first successful architecture to train on images and use entirely attention mechanisms internally. This was achieved by splitting up inputted images into constituent pieces and treating these pieces (referred to as “tokens” in NLP language) as a sequence (<xref rid="pone.0289499.g003" ref-type="fig">Fig 3b</xref>). Their ViT model achieves state-of-the-art performance on several image classification benchmark datasets and has become a mainstay option for researchers looking for deep learning models trainable on images [<xref rid="pone.0289499.ref009" ref-type="bibr">9</xref>, <xref rid="pone.0289499.ref036" ref-type="bibr">36</xref>–<xref rid="pone.0289499.ref038" ref-type="bibr">38</xref>].</p>
        <p>Researchers have more recently begun applying vision transformers to whole-slide images and histopathological problems, but they form a promising new avenue [<xref rid="pone.0289499.ref011" ref-type="bibr">11</xref>].</p>
      </sec>
    </sec>
    <sec id="sec010">
      <title>Inference</title>
      <p>After a model has been trained, the next step is applying that model to the WSIs in a validation or test set to check the model’s performance. This application of a trained model to a new slide is known as inference. SliDL has two functions that infer a trained model on tiles with a sufficiently high tissue-probability (identified using the deep tissue detector) in a <monospace specific-use="no-wrap">Slide</monospace> object, saving the results into the tile dictionary internal to it: <monospace specific-use="no-wrap">Slide.inferClassifier()</monospace> and <monospace specific-use="no-wrap">Slide.inferSegmenter()</monospace>, which take as input a trained PyTorch model file. Using them is as simple as creating a <monospace specific-use="no-wrap">Slide</monospace> object for the slides one wants to infer on and then applying the function.</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 14</bold>. <bold>Infer a classification or a segmentation model on a Slide’s WSI</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
slidl_slide.inferClassifier(trainedModel=trained_classification_model,
                                classNames=['normal', 'tumor'],
                                dataTransforms=data_transforms,
                                tissueLevelThreshold=0.995,
                                foregroundLevelThreshold=88,
                                batchSize=30,
                                numWorkers=16)

slidl_slide.inferSegmenter(trainedModel=trained_segmentation_model,
                                classNames=['normal', 'tumor'],
                                dataTransforms=data_transforms,
                                tissueLevelThreshold=0.995,
                                foregroundLevelThreshold=88,
                                batchSize=30,
                                numWorkers=16)
</preformat>
      <p>After applying these functions, SliDL stores the predictions of the neural network for each tile in the tile dictionary internal to the <monospace specific-use="no-wrap">Slide</monospace> object.</p>
      <p>To ensure that the model is behaving as expected, it is important to visualise inference results by plotting the inference predictions of tiles spatially as they appear in the WSI. Once inference has been performed on the <monospace specific-use="no-wrap">Slide</monospace> objects, SliDL’s <monospace specific-use="no-wrap">Slide.visualizeClassifierInference()</monospace> and <monospace specific-use="no-wrap">Slide.visualizeSegmenterInference()</monospace> functions create these plots for the user overlaid on a low-resolution image of the WSI, taking the class and WSI pyramid level to visualise as arguments. The user is therefore able to qualitatively assess whether the regions highlighted by the model are as expected, given the ground-truth, and experiment with different training configurations, tile sizes, or other hyperparameters before proceeding if not (<xref rid="pone.0289499.g004" ref-type="fig">Fig 4</xref>).</p>
      <fig position="float" id="pone.0289499.g004">
        <object-id pub-id-type="doi">10.1371/journal.pone.0289499.g004</object-id>
        <label>Fig 4</label>
        <caption>
          <title>Visualising the inference of trained models.</title>
          <p><bold>(A)</bold> A plot of the inference of a trained tile-level classification model on three validation slides from the SliDL tutorial, showing the ability of the classification model to distinguish regions showing breast cancer metastasis from normal lymph node tissue. Plots were generated with Slide.visualizeClassifierInference(). <bold>(B)</bold> A plot of the inference of a trained tile-level segmentation model on three validation slides from the SliDL tutorial, showing the ability of the segmentation model to distinguish the same regions as the classification model. Plots were generated with Slide.visualizeSegmenterInference().</p>
        </caption>
        <graphic xlink:href="pone.0289499.g004" position="float"/>
      </fig>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 15</bold>. <bold>Visualise the inference of a classification or a segmentation model on a Slide’s WSI</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
slidl_slide.visualizeClassifierInference(classToVisualize='tumor',
                                            folder='path/to/folder',
                                            level=3)

slidl_slide.visualizeSegmenterInference(classToVisualize='tumor',
                                            folder='path/to/folder',
                                            level=3)
</preformat>
      <p>When performing segmentation, the user may want to create segmentation masks which are larger than the size of the tiles that were inferred on. When adjacent tiles overlap, tile-level segmentation masks cannot simply be concatenated. SliDL supports the automatic generation of whole-slide segmentation masks with the <monospace specific-use="no-wrap">Slide.getNonOverlappingSegmentationInferenceArray()</monospace> method, returning an inference array with the model’s pixel-level predictions and merging overlapping regions to return single predictions for each pixel. The class for which predictions are desired is specified by the <monospace specific-use="no-wrap">className</monospace> argument. Inference matrices are saved as compressed <monospace specific-use="no-wrap">.npz</monospace> files at the location specified by the <monospace specific-use="no-wrap">folder</monospace> argument. Users have the option of defining a threshold with the <monospace specific-use="no-wrap">probabilityTheshold</monospace> argument to return a binary output matrix, where pixels with a probability for the given class with a probability at or above the threshold are binarized to true, and the others to false. If the users does not define a <monospace specific-use="no-wrap">probabilityTheshold</monospace>, the raw probability value will be scaled to be between 0 and 255 (255 is 100% probability) and returned as a Numpy uint8 integer if the <monospace specific-use="no-wrap">dtype</monospace> argument is set to ‘int’ (the default). The user can also choose to have the probabilities returned as Numpy float32 floats if <monospace specific-use="no-wrap">dtype</monospace> is set to ‘float’, but this is usually undesirable as it results in extremely memory-intensive matrices.</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 16</bold>. <bold>Save the segmentation matrix from an inference on a Slide’s WSI</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
slidl_slide.getNonOverlappingSegmentationInferenceArray(
                                        className='tumor',
                                        dtype='int'
                                        folder='path/to/folder')
</preformat>
    </sec>
    <sec id="sec011">
      <title>Evaluating model performance</title>
      <p>Beyond visual verification, numerical methods are requires to assess model performance. SliDL includes a number of wrapper functions around common performance metrics to enable easy model evaluation. After inferring using a trained model, the <monospace specific-use="no-wrap">Slide.classifierMetricAtThreshold()</monospace> and <monospace specific-use="no-wrap">Slide.segmenterMetricAtThreshold()</monospace> methods can be applied. By providing a list of probability thresholds to the <monospace specific-use="no-wrap">probabilityThresholds</monospace> argument, a metric (‘accuracy’, ‘balanced_accuracy’, ‘f1’, ‘precision’, or ‘recall’ for <monospace specific-use="no-wrap">Slide.classifierMetricAtThreshold()</monospace>, and ‘dice_coeff’ for <monospace specific-use="no-wrap">segmenterMetricAtThreshold()</monospace>), and a class to the <monospace specific-use="no-wrap">classToThreshold</monospace> argument, these methods will calculate the corresponding metric at each probability threshold for the given class.</p>
      <p>For <monospace specific-use="no-wrap">Slide.classifierMetricAtThreshold()</monospace>, the <monospace specific-use="no-wrap">tileAnnotationOverlapThreshold</monospace> argument defines the minimum fraction of a tile’s area that must be covered by an annotation from the <monospace specific-use="no-wrap">classToThreshold</monospace> for that tile to be considered ground-truth positive for that class (default is 0.5).</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 17</bold>. <bold>Compute the classification or segmentation accuracy of an inference on a Slide’s WSI</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
classification_accuracies = slidl_slide.classifierMetricAtThreshold(
                            classToThreshold='tumor',
                            probabilityThresholds=probability_thresholds,
                            tileAnnotationOverlapThreshold=0.3,
                            metric='accuracy')

segmentation_accuracies = slidl_slide.segmenterMetricAtThreshold(
                            classToThreshold='tumor',
                            probabilityThresholds=probability_thresholds)
</preformat>
      <p>The threshold that gives best performance on the validation set can then be applied to the test set with the same two functions; inputting the single best threshold in the <monospace specific-use="no-wrap">probablityThresholds</monospace> argument.</p>
      <p>Although inference is performed on the tile-level, when applied in the clinic a label for the entire slide is often required. Similarly, during training, it is frequently the case that the test set has only a slide-level label. A method is therefore needed to translate tile-level predictions to slide-level labels. One approach is to determine a threshold for the number of tiles which need to be called positive, for a given class at a given probability threshold, in order to call an entire slide positive. To test the performance of a model on data with only slide-level labels, the AUC for a ROC curve can be computed for a range of positive-tile counts. The optimum count can then be used when applying the best model to unlabelled data in the clinic. In order to make such analyses straightforward, SliDL includes the <monospace specific-use="no-wrap">Slide.numTilesAboveClassPredictionThreshold()</monospace> method which returns the number of tiles in a slide whose inference prediction probabilities for a given class (<monospace specific-use="no-wrap">classToThreshold</monospace>) are greater than or equal to given probability thresholds (provided in a list to the <monospace specific-use="no-wrap">probablityThresholds</monospace> argument).</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 18</bold>. <bold>Count the number of tiles in a Slide whose inference value exceeds a certain threshold</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
slidl_slide.numTilesAboveClassPredictionThreshold(
                            classToThreshold='tumor',
                            probabilityThresholds=probability_thresholds)
</preformat>
      <p>These positive-tile counts per slide can then be used to calculate an AUROC. As above, often a range of probability thresholds are tried, and the one which yields the largest AUROC on the validation set is then applied to the test set to give the final performance.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec012">
    <title>Discussion</title>
    <p>SliDL provides an easily-installable (see Installation) Python library with a straightforward API for users seeking to perform tasks involving artefact and background detection, tile extraction from annotation, and model training and inference on WSIs. It is intended for users with basic Python knowledge; it does not demand extensive experience with WSI data or pathology. We see this as one of SliDL’s key advantages over existing methods which expect more rigorous backgrounds in histopathology or image analysis. To best understand the unique value provided by SliDL, one must compare it to existing computation methods in the WSI space.</p>
    <sec id="sec013">
      <title>Related methods</title>
      <p>Several tools have been developed which also provide support for some of the functionalities available in SliDL. While providing a complete description and comparison of these tools is beyond the scope of this manuscript, here we briefly describe these tools and direct the reader towards the articles presenting them for more details (see <xref rid="pone.0289499.s004" ref-type="supplementary-material">S3 Table</xref> for a summary of all of the comparisons made with all the methods discussed below).</p>
      <p>HistoQC [<xref rid="pone.0289499.ref039" ref-type="bibr">39</xref>] is a Python-based tool for performing quality control of WSIs, aiding users in the identification slides containing potential technical artefacts and affected by batch effects. By providing the user with modules for performing a wide range of classical image analysis techniques, HistoQC enables the construction of custom pipelines for performing foreground filtering, detection of slide artefacts such as pen marks, and identification of batch effects such as slides with darker staining compared to the rest. In HistoQC, this is achieved using a combination of approaches including inspection of colour distributions, application of edge and smoothness detectors, and classical filters such as Gabor and Frangi filters for texture analysis. For example, if the background of a WSI is uniformly white, foreground filtering can be performed by applying a threshold to the colour distribution which excludes white pixels. Similarly, a bright green pen mark may be clearly distinguishable from tissue by inspection of the green colour distribution of the WSI. In addition, HistoQC provides an interactive user interface for exploring one’s data. These approaches can achieve competitive results when carefully tuned by the user, but may struggle in more complex cases, such as uneven background, and pen marks with similar colour to the tissue. HistoQC is therefore a useful tool, complementing the wider functionality and robustness of SliDL, and enabling rapid quality control processing of one’s data.</p>
      <p>HistomicsTK [<xref rid="pone.0289499.ref040" ref-type="bibr">40</xref>] is a Python library for performing a number of image analysis tasks specific to WSIs including stain colour deconvolution, normalisation and augmentation, as well as cell/nuclei segmentation and even a user interface for manual annotation of WSIs. Like HistoQC, all image analysis techniques are performed using classical approaches. HistomicsTK is highly complementary to SliDL, and in particular, we envisage that users may make use of HistomicsTK for performing WSI-specific colour augmentations within a SliDL workflow.</p>
      <p>Histolab [<xref rid="pone.0289499.ref041" ref-type="bibr">41</xref>] is a Python library combining features found both in HistoQC and HistomicsTK, including functions for performing classical image analysis techniques to facilitate tissue detection and artefact removal, cell/nuclei segmentation, and colour transformations such as colour deconvolution. In addition, Histolab, like SliDL, supports the extracting of tiles from WSIs, and enables one to easily test alternative tiling strategies, including random extraction of tiles according to tissue detection score thresholds.</p>
      <p>MONAI [<xref rid="pone.0289499.ref042" ref-type="bibr">42</xref>] is an extensive Python library which is part of the PyTorch ecosystem and is designed as a unified framework for performing deep learning on medical imaging data. Like SliDL, MONAI supports tiling of WSIs and provides extensive support model evaluation metrics. In addition, MONAI provides domain-specific support for data augmentation transforms, implementations of neural network architectures, optimisers, loss functions, and AI-assisted annotation, all of which are tuned for application to medical imaging data.</p>
      <p>PathML [<xref rid="pone.0289499.ref043" ref-type="bibr">43</xref>] is a Python library which also supports the tiling of WSIs, and similar to Histolab, HistomicsTK and HistoQC implements an number of classical approaches to foreground and artefact detection. Similarly to MONAI and HistomicsTK, PathML also supports some pre-processing methods such as stain normalisation data augmentation, but is not designed to perform any post-processing steps.</p>
    </sec>
    <sec id="sec014">
      <title>Distinct advantages of SliDL</title>
      <p>Part of the advantage of SliDL is the ease with which it can be learned and used to render complicated digital histopathological techniques and problems immediately accessible to researchers, turning what would be weeks of work and files full of code into one straightforward, linear workflow consisting of just a few lines.</p>
      <p>In addition to these advantages, the SliDL toolbox we present here improves on the other tools mentioned above in several ways, most importantly the handling of digital annotations. None of the tools listed above implement deep tissue detectors, nor do they implement tools for handling of annotations for WSIs to facilitate labelling of tiles, automatic resolution of annotation conflicts, or generation of binary and multi-class tile-level segmentation masks. Furthermore, while all of these tools provide support for the pre-processing of WSIs, only MONAI provides tools for model evaluation, and none support post-processing tasks such as the stitching together of tile-level segmentation masks to produce a slide-level mask. Tools such as MONAI are therefore highly complementary to SliDL: we envisage users, for example, making use of SliDL for pre-processing and handling of annotations, and MONAI for data augmentation and implementation of neural network architectures for training.</p>
    </sec>
    <sec id="sec015">
      <title>Future work</title>
      <p>Despite its many advantages, SliDL can be expanded upon and improved in various ways. For example, it might be useful to eventually provide a graphical user interface (GUI) to SliDL so that some of its basic features are accessible to users with less of a computer science background. This might include sliders, toggles, and text boxes in place of programmatic functions with arguments.</p>
      <p>Furthermore, currently, applying the deep tissue detector with any efficiency to WSIs requires a computer vision-grade GPU, ideally with at least 4 gigabytes of memory and a GPU clock of at least 1200 megahertz. This feature will therefore be slow for users working on most standard commercial laptop or desktop models. For this reason, it is worth exploring alternative methods of implementing SliDL’s deep tissue detector which are compatible with lower-specification hardware. One potential solution is to re-train the deep tissue detector model on a deep neural network with fewer internal parameters, such as MnasNet [<xref rid="pone.0289499.ref044" ref-type="bibr">44</xref>], SqueezeNet [<xref rid="pone.0289499.ref045" ref-type="bibr">45</xref>], or MobileNet [<xref rid="pone.0289499.ref046" ref-type="bibr">46</xref>].</p>
      <p>Finally, there remain many potential features to add to SliDL to expand its breadth. We would like to include a function to automatically generate tile annotations files demarcating the highest probability tiles for a certain class identified during inference. We are also interested in including functions so that SliDL can generate of saliency maps of trained deep learning models [<xref rid="pone.0289499.ref047" ref-type="bibr">47</xref>].</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec016">
    <title>Conclusion</title>
    <p>SliDL is a new and fully-functional new tool for computer scientists looking for a straightforward but powerful Python library for solving some of the most commonly faced and nettlesome problems for training and evaluating deep learning models on WSI data quickly and easily. SliDL also includes a fully illustrative tutorial Jupyter notebook on a real-world example problem on a publicly available dataset from beginning (removing background and slide artefacts, extracting tiles from annotations) to end (training a deep learning model, inferring it on new slides, and evaluating model performance); the tutorial can be found here: <ext-link xlink:href="https://slidl.readthedocs.io/en/latest/" ext-link-type="uri">https://slidl.readthedocs.io/en/latest/</ext-link>. It is our hope and goal that with SliDL and its corresponding tutorial, the application of deep learning to whole-slide images becomes more accessible not just to researchers already involved in digital pathology, but to newcomers as well, making the field on the whole more approachable.</p>
  </sec>
  <sec id="sec017">
    <title>Availability and requirements</title>
    <p><bold>Project name</bold>: SliDL</p>
    <p><bold>Project home page</bold>: <ext-link xlink:href="https://github.com/markowetzlab/slidl" ext-link-type="uri">https://github.com/markowetzlab/slidl</ext-link></p>
    <p><bold>Project tutorial home page</bold>: <ext-link xlink:href="https://github.com/markowetzlab/slidl-tutorial" ext-link-type="uri">https://github.com/markowetzlab/slidl-tutorial</ext-link></p>
    <p><bold>Project documentation home page</bold>: <ext-link xlink:href="https://slidl.readthedocs.io/en/latest/" ext-link-type="uri">https://slidl.readthedocs.io/en/latest/</ext-link></p>
    <p><bold>Operating system(s)</bold>: Not applicable</p>
    <p><bold>Programming language</bold>: Python (version 3.7 or above)</p>
    <p><bold>Other requirements</bold>: Not applicable</p>
    <p><bold>License</bold>: GPL-3.0</p>
    <p><bold>Any restrictions to use by non-academics</bold>: Not applicable</p>
  </sec>
  <sec id="sec018">
    <title>Materials and installation</title>
    <sec id="sec019">
      <title>Benchmarking</title>
      <p>Here is a full breakdown of the slides used for the benchmarking analysis of the deep tissue detector (see Deep tissue detector):</p>
      <list list-type="bullet">
        <list-item>
          <p>36 slides</p>
        </list-item>
        <list-item>
          <p>6 TCGA-TGCT (testicular germ cell tumor) H&amp;E</p>
        </list-item>
        <list-item>
          <p>6 CAMELYON-16 (tiny breast cancer metastases in lymph nodes, including slides with and without metastases present) H&amp;E slides</p>
        </list-item>
        <list-item>
          <p>6 OCCAMS (esophageal adenocarcinoma) H&amp;E slides from esophago-gastro-duodenoscopy</p>
        </list-item>
        <list-item>
          <p>6 BEST2 (cytosponge samples some with Barett’s) P53 and H&amp;E slides</p>
        </list-item>
        <list-item>
          <p>6 TCGA-PRAD (prostate adenocarcinoma) H&amp;E slides</p>
        </list-item>
        <list-item>
          <p>2 TCGA floor of mouth cancer H&amp;E slides</p>
        </list-item>
        <list-item>
          <p>1 TCGA small intestine cancer H&amp;E slide</p>
        </list-item>
        <list-item>
          <p>1 TCGA gum cancer H&amp;E slide</p>
        </list-item>
        <list-item>
          <p>1 TCGA “spinal cord, cranial nerves, and other unspecified parts of central nervous system” cancer H&amp;E slide</p>
        </list-item>
        <list-item>
          <p>1 TCGA tonsil cancer H&amp;E slide</p>
        </list-item>
        <list-item>
          <p>Mixture of samples preserved via FFPE (formalin fixed paraffin embedded) and those preserved via flash freezing</p>
        </list-item>
        <list-item>
          <p>Different scanning file types, scanning machines, times of scanning, scanning locations, stain intensities, stains (H&amp;E and P53), tissue removal methods/surgery types, and tissue types</p>
        </list-item>
        <list-item>
          <p>Slide artefacts included ink of many different colors (including blue, red, black, and reen—some used to mark regions on top of tissue, other times to write labels on a background portion of a slide), slide bubbles, tissue folding/burning, cloudiness or yellowing, slide shifted in scanning machine to leave black region in image, slide edge artefacts, dirt and debris under the slide, black slide crosses, blurry regions</p>
        </list-item>
        <list-item>
          <p>Slides exhibited a wide range of artefact degree and amount—some slides had virtually no artefact whatsoever, others had a few little ink dots or bubbles, still others had large black regions and/or regions of ink etc.</p>
        </list-item>
        <list-item>
          <p>1,489,084 tiles in total across all 36 slides with a massive range in the size of each slide (smallest slide: 47MB, largest slide: 3.9GB / fewest tiles in one slide: 3,304, most: 86,190)</p>
        </list-item>
        <list-item>
          <p>730 individual annotations including tissue and doughnut hole annotations</p>
        </list-item>
        <list-item>
          <p>Slides all scanned in 40x with 500px edge length tiles</p>
        </list-item>
      </list>
      <p>All 36 WSIs used to benchmark the deep tissue detector are available at <ext-link xlink:href="https://doi.org/10.5281/zenodo.7947380" ext-link-type="uri">https://doi.org/10.5281/zenodo.7947380</ext-link>.</p>
    </sec>
    <sec id="sec020">
      <title>Hardware</title>
      <p>It is recommended that SliDL users either work on a machine with at least 32 GB of RAM and enough disk space to hold the number of number of tiles they would like to extract (tiles are not large, but if thousands are extracted, a proportional amount of disk space is required. At least four cores are recommended. WSIs tend to be 0.5–5 GB in size, so if tens or hundreds are used in an analysis, disk space to store them is required (external hard drives work well). Users that wish to utilise the inference functions of <monospace specific-use="no-wrap">Slide</monospace>, <monospace specific-use="no-wrap">Slide.inferClassifier()</monospace> and <monospace specific-use="no-wrap">Slide.inferSegmenter()</monospace> are highly recommended to have a CUDA-compatible Graphics Processing Unit (GPU). SliDL also works well in high performance computing environments that meet these conditions.</p>
    </sec>
    <sec id="sec021">
      <title>Installation</title>
      <p>SliDL is available on the Python Package Index (PyPI) for easy installation:</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 19</bold>. Install SliDL</p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
pip install slidl
</preformat>
    </sec>
    <sec id="sec022">
      <title>Troubleshooting</title>
      <p>There are several mistakes and error messages that can arise when using SliDL. Troubleshooting presents the most common mistakes users might run into, including the error message output by SliDL, the possible reason for the mistake, and the possible solution to it (<xref rid="pone.0289499.t001" ref-type="table">Table 1</xref>).</p>
      <table-wrap position="float" id="pone.0289499.t001">
        <object-id pub-id-type="doi">10.1371/journal.pone.0289499.t001</object-id>
        <label>Table 1</label>
        <caption>
          <title>Common SliDL error messages with explanations and possible solutions.</title>
          <p><italic toggle="yes">X</italic>, <italic toggle="yes">Y</italic>, and <italic toggle="yes">Z</italic> represent numbers or words that will vary depending on the exact error made by the user.</p>
        </caption>
        <alternatives>
          <graphic xlink:href="pone.0289499.t001" id="pone.0289499.t001g" position="float"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="left" rowspan="1" colspan="1">Message</th>
                <th align="left" rowspan="1" colspan="1">Possible reason</th>
                <th align="left" rowspan="1" colspan="1">Solution</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘This image is not compatible. Please refer to the documentation for proper installation of openslide and libvips’</td>
                <td align="left" rowspan="1" colspan="1">The WSI input into the SliDL initialiser was not a supported file format</td>
                <td align="left" rowspan="1" colspan="1">Convert the WSI to a file format supported by lipvips.</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘Tissue detection has already been performed. Use overwriteExistingTissueDetection if you wish to write over it’</td>
                <td align="left" rowspan="1" colspan="1">Slide.detectTissue() has already been called on the Slide object</td>
                <td align="left" rowspan="1" colspan="1">Set the overwriteExistingTissueDetection argument to True, or else don’t perform tissue detection again</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘Annotation with centroid (<italic toggle="yes">X</italic>, <italic toggle="yes">Y</italic>) produces a Shapely <italic toggle="yes">Z</italic> instead of a polygon; check to see if it self-intersects.’</td>
                <td align="left" rowspan="1" colspan="1">The annotation around the specified centroid pixel coordinates of the WSI does not produce a polygon when geometrically parsed</td>
                <td align="left" rowspan="1" colspan="1">Check that annotation on the WSI in a WSI viewer looking for self-overlapping regions and correct it to be a polygon</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘Warning: <italic toggle="yes">X</italic> suitable <italic toggle="yes">Y</italic> tiles found but requested <italic toggle="yes">Z</italic> tiles to extract. Extracting all suitable tiles…’</td>
                <td align="left" rowspan="1" colspan="1">The numTilesToExtractPerClass argument of a tile extraction function exceeds the number of suitable tiles of class <italic toggle="yes">Y</italic></td>
                <td align="left" rowspan="1" colspan="1">Reduce the numTilesToExtractPerClass argument for class <italic toggle="yes">Y</italic>, or else let SliDL will extract all available <italic toggle="yes">Y</italic> tiles by default</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘Model has <italic toggle="yes">X</italic> classes but only Y class names were provided in the classes argument’</td>
                <td align="left" rowspan="1" colspan="1">The number of classes output by the model inputted to Slide.inferClassifier() or Slide.inferSegmenter() does not equal the number of classes present in the classNames argument</td>
                <td align="left" rowspan="1" colspan="1">Verify that classNames includes all the classes that were trained on, and correct this argument as necessary</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘No predictions found in slide. Use inferClassifier() / inferSegmenter() to generate them.’</td>
                <td align="left" rowspan="1" colspan="1">Using a SliDL function reliant on inference results without having added inference results to the Slide object</td>
                <td align="left" rowspan="1" colspan="1">Run Slide.inferClassifier or Slide.inferSegmenter() on the Slide object</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
    </sec>
  </sec>
  <sec id="sec023" sec-type="supplementary-material">
    <title>Supporting information</title>
    <supplementary-material id="pone.0289499.s001" position="float" content-type="local-data">
      <label>S1 Fig</label>
      <caption>
        <title>Directory structure.</title>
        <p>The directory structure output by <monospace specific-use="no-wrap">Slide.extractAnnotationTiles()</monospace> and <monospace specific-use="no-wrap">Slide.extractRandomUnannotatedTiles()</monospace> which is amenable to PyTorch’s ImageFolder dataset.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pone.0289499.s001.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pone.0289499.s002" position="float" content-type="local-data">
      <label>S1 Table</label>
      <caption>
        <title>Table summarising the primary functions of SliDL.</title>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pone.0289499.s002.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pone.0289499.s003" position="float" content-type="local-data">
      <label>S2 Table</label>
      <caption>
        <title>Table describing the SliDL functions discussed in the main text.</title>
        <p>Functions are listed in green, a summary of their purpose in blue, their arguments in yellow and the description of the arguments in white. For a complete description of all SliDL functions and their arguments, see <ext-link xlink:href="https://slidl.readthedocs.io/" ext-link-type="uri">https://slidl.readthedocs.io/</ext-link>.</p>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0289499.s003.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pone.0289499.s004" position="float" content-type="local-data">
      <label>S3 Table</label>
      <caption>
        <title>Table of comparisons to related methods.</title>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0289499.s004.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack>
    <p>We would like to thank Sarah Killcoyne and Winifred Taylor-Williams for their early testing of SliDL.</p>
  </ack>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>
API
</term>
        <def>
          <p>Application Public Interface</p>
        </def>
      </def-item>
      <def-item>
        <term>
BEST2
</term>
        <def>
          <p>Barrett’s oEsophagus Screening Trial 2 [<xref rid="pone.0289499.ref048" ref-type="bibr">48</xref>]</p>
        </def>
      </def-item>
      <def-item>
        <term>
OCCAMS
</term>
        <def>
          <p>Oesophageal Cancer Clinical and Molecular Stratification [<xref rid="pone.0289499.ref049" ref-type="bibr">49</xref>]</p>
        </def>
      </def-item>
      <def-item>
        <term>
TCGA
</term>
        <def>
          <p>The Cancer Genome Atlas [<xref rid="pone.0289499.ref050" ref-type="bibr">50</xref>]</p>
        </def>
      </def-item>
      <def-item>
        <term>
WSI
</term>
        <def>
          <p>Whole-Slide Image</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <ref-list>
    <title>References</title>
    <ref id="pone.0289499.ref001">
      <label>1</label>
      <mixed-citation publication-type="journal"><name><surname>Dimitriou</surname><given-names>N</given-names></name>, <name><surname>Arandjelović</surname><given-names>O</given-names></name>, <name><surname>Caie</surname><given-names>PD</given-names></name>. <article-title>Deep Learning for Whole Slide Image Analysis: An Overview</article-title>. <source>Frontiers in medicine</source>. <year>2019</year>;<volume>6</volume>:<fpage>264</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3389/fmed.2019.00264</pub-id><?supplied-pmid 31824952?><pub-id pub-id-type="pmid">31824952</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref002">
      <label>2</label>
      <mixed-citation publication-type="journal"><name><surname>Kather</surname><given-names>JN</given-names></name>, <name><surname>Heij</surname><given-names>LR</given-names></name>, <name><surname>Grabsch</surname><given-names>HI</given-names></name>, <name><surname>Loeffler</surname><given-names>C</given-names></name>, <name><surname>Echle</surname><given-names>A</given-names></name>, <name><surname>Muti</surname><given-names>HS</given-names></name>, <etal>et al</etal>. <article-title>Pan-cancer image-based detection of clinically actionable genetic alterations</article-title>. <source>Nature cancer</source>. <year>2020</year>;<volume>1</volume>(<issue>8</issue>):<fpage>789</fpage>–<lpage>799</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s43018-020-0087-6</pub-id><?supplied-pmid 33763651?><pub-id pub-id-type="pmid">33763651</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref003">
      <label>3</label>
      <mixed-citation publication-type="journal"><name><surname>Lu</surname><given-names>MY</given-names></name>, <name><surname>Chen</surname><given-names>TY</given-names></name>, <name><surname>Mahmood</surname><given-names>F</given-names></name>, <etal>et al</etal>. <article-title>AI-based pathology predicts origins for cancers of unknown primary</article-title>. <source>Nature</source>. <year>2021</year>;<volume>594</volume>:<fpage>106</fpage>–<lpage>110</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41586-021-03512-4</pub-id><?supplied-pmid 33953404?><pub-id pub-id-type="pmid">33953404</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref004">
      <label>4</label>
      <mixed-citation publication-type="journal"><name><surname>Montgomery</surname><given-names>E</given-names></name>. <article-title>Is there a way for pathologists to decrease interobserver variability in the diagnosis of dysplasia?</article-title><source>Archives of pathology &amp; laboratory medicine</source>. <year>2005</year>;<volume>129</volume>(<issue>2</issue>):<fpage>174</fpage>–<lpage>176</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.5858/2005-129-174-ITAWFP</pub-id><pub-id pub-id-type="pmid">15679414</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref005">
      <label>5</label>
      <mixed-citation publication-type="journal"><name><surname>Raab</surname><given-names>SS</given-names></name>, <name><surname>Grzybicki</surname><given-names>DM</given-names></name>. <source>Anatomic pathology workload and error</source>; <year>2006</year>.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref006">
      <label>6</label>
      <mixed-citation publication-type="journal"><name><surname>Goodfellow</surname><given-names>I</given-names></name>, <name><surname>Pouget-Abadie</surname><given-names>J</given-names></name>, <name><surname>Mirza</surname><given-names>M</given-names></name>, <name><surname>Xu</surname><given-names>B</given-names></name>, <name><surname>Warde-Farley</surname><given-names>D</given-names></name>, <name><surname>Ozair</surname><given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Generative adversarial networks</article-title>. <source>Communications of the ACM</source>. <year>2020</year>;<volume>63</volume>(<issue>11</issue>):<fpage>139</fpage>–<lpage>144</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1145/3422622</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref007">
      <label>7</label>
      <mixed-citation publication-type="journal"><name><surname>Prakash</surname><given-names>CD</given-names></name>, <name><surname>Karam</surname><given-names>LJ</given-names></name>. <article-title>It GAN DO better: GAN-based detection of objects on images with varying quality</article-title>. <source>IEEE Transactions on Image Processing</source>. <year>2021</year>;<volume>30</volume>:<fpage>9220</fpage>–<lpage>9230</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TIP.2021.3124155</pub-id><?supplied-pmid 34735343?><pub-id pub-id-type="pmid">34735343</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref008">
      <label>8</label>
      <mixed-citation publication-type="journal"><name><surname>Vaswani</surname><given-names>A</given-names></name>, <name><surname>Shazeer</surname><given-names>N</given-names></name>, <name><surname>Parmar</surname><given-names>N</given-names></name>, <name><surname>Uszkoreit</surname><given-names>J</given-names></name>, <name><surname>Jones</surname><given-names>L</given-names></name>, <name><surname>Gomez</surname><given-names>AN</given-names></name>, <etal>et al</etal>. <article-title>Attention is all you need</article-title>. <source>Advances in neural information processing systems</source>. <year>2017</year>;<volume>30</volume>.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref009">
      <label>9</label>
      <mixed-citation publication-type="other">Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:201011929. 2020.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref010">
      <label>10</label>
      <mixed-citation publication-type="journal"><name><surname>Jose</surname><given-names>L</given-names></name>, <name><surname>Liu</surname><given-names>S</given-names></name>, <name><surname>Russo</surname><given-names>C</given-names></name>, <name><surname>Nadort</surname><given-names>A</given-names></name>, <name><surname>Di Ieva</surname><given-names>A</given-names></name>. <article-title>Generative adversarial networks in digital pathology and histopathological image processing: A review</article-title>. <source>Journal of Pathology Informatics</source>. <year>2021</year>;<volume>12</volume>(<issue>1</issue>):<fpage>43</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.4103/jpi.jpi_103_20</pub-id><?supplied-pmid 34881098?><pub-id pub-id-type="pmid">34881098</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref011">
      <label>11</label>
      <mixed-citation publication-type="journal"><name><surname>Zheng</surname><given-names>Y</given-names></name>, <name><surname>Gindra</surname><given-names>RH</given-names></name>, <name><surname>Green</surname><given-names>EJ</given-names></name>, <name><surname>Burks</surname><given-names>EJ</given-names></name>, <name><surname>Betke</surname><given-names>M</given-names></name>, <name><surname>Beane</surname><given-names>JE</given-names></name>, <etal>et al</etal>. <article-title>A graph-transformer for whole slide image classification</article-title>. <source>IEEE transactions on medical imaging</source>. <year>2022</year>;<volume>41</volume>(<issue>11</issue>):<fpage>3003</fpage>–<lpage>3015</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TMI.2022.3176598</pub-id><?supplied-pmid 35594209?><pub-id pub-id-type="pmid">35594209</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref012">
      <label>12</label>
      <mixed-citation publication-type="other">Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, et al. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In: Advances in Neural Information Processing Systems 32. Curran Associates, Inc.; 2019. p. 8024–8035. Available from: <ext-link xlink:href="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" ext-link-type="uri">http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref013">
      <label>13</label>
      <mixed-citation publication-type="journal"><name><surname>Litjens</surname><given-names>G</given-names></name>, <name><surname>Bandi</surname><given-names>P</given-names></name>, <name><surname>Bejnordi</surname><given-names>BE</given-names></name>, <name><surname>Van der Laak</surname><given-names>J</given-names></name>. <article-title>1399 H&amp;E-stained sentinel lymph node sections of breast cancer patients: the CAMELYON dataset</article-title>. <source>GigaScience</source>. <year>2018</year>;<volume>7</volume>(<issue>6</issue>). <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/gigascience/giy065</pub-id><?supplied-pmid 29860392?><pub-id pub-id-type="pmid">29860392</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref014">
      <label>14</label>
      <mixed-citation publication-type="other">Medixant. RadiAnt DICOM Viewer; 2021. Available from: <ext-link xlink:href="https://www.radiantviewer.com" ext-link-type="uri">https://www.radiantviewer.com</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref015">
      <label>15</label>
      <mixed-citation publication-type="other">Martinez K, Cupitt J. VIPS—a highly tuned image processing software architecture. In: Proceedings of IEEE International Conference on Image Processing; 2005. p. 574–577.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref016">
      <label>16</label>
      <mixed-citation publication-type="journal"><name><surname>Janowczyk</surname><given-names>A</given-names></name>, <name><surname>Madabhushi</surname><given-names>A</given-names></name>. <article-title>Deep learning for digital pathology image analysis: A comprehensive tutorial with selected use cases</article-title>. <source>J Pathol Inform</source>. <year>2016</year>;<volume>7</volume>(<issue>29</issue>). <comment>doi: </comment><pub-id pub-id-type="doi">10.4103/2153-3539.186902</pub-id><?supplied-pmid 27563488?><pub-id pub-id-type="pmid">27563488</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref017">
      <label>17</label>
      <mixed-citation publication-type="other">Huang G, Liu Z, Van Der Maaten L, Weinberger KQ. Densely connected convolutional networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2017. p. 4700–4708.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref018">
      <label>18</label>
      <mixed-citation publication-type="other">Yang M. A PyTorch imbalanced dataset sampler for oversampling low frequent classes and undersampling high frequent ones; 2018–2020. <ext-link xlink:href="https://github.com/ufoym/imbalanced-dataset-sampler" ext-link-type="uri">https://github.com/ufoym/imbalanced-dataset-sampler</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref019">
      <label>19</label>
      <mixed-citation publication-type="journal"><name><surname>Otsu</surname><given-names>N</given-names></name>. <article-title>A threshold selection method from gray-level histograms</article-title>. <source>IEEE transactions on systems, man, and cybernetics</source>. <year>1979</year>;<volume>9</volume>(<issue>1</issue>):<fpage>62</fpage>–<lpage>66</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TSMC.1979.4310076</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref020">
      <label>20</label>
      <mixed-citation publication-type="journal"><name><surname>Zack</surname><given-names>GW</given-names></name>, <name><surname>Rogers</surname><given-names>WE</given-names></name>, <name><surname>Latt</surname><given-names>SA</given-names></name>. <article-title>Automatic measurement of sister chromatid exchange frequency</article-title>. <source>J Histochem Cytochem</source>. <year>1977</year>;<volume>25</volume>(<issue>7</issue>):<fpage>741</fpage>–<lpage>753</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1177/25.7.70454</pub-id><?supplied-pmid 70454?><pub-id pub-id-type="pmid">70454</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref021">
      <label>21</label>
      <mixed-citation publication-type="journal"><name><surname>Bankhead</surname><given-names>P</given-names></name>, <name><surname>Loughrey</surname><given-names>MB</given-names></name>, <name><surname>Fernández</surname><given-names>JA</given-names></name>. <article-title>QuPath: Open source software for digital pathology image analysis</article-title>. <source>Sci Rep</source>. <year>2017</year>;<volume>7</volume>:<fpage>16878</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41598-017-17204-5</pub-id><?supplied-pmid 29203879?><pub-id pub-id-type="pmid">29203879</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref022">
      <label>22</label>
      <mixed-citation publication-type="other">Computation Pathology Group, part of the Diagnostic Image Analysis Group, at the Radboud University Medical Center. The Automated Slide Analysis Platform (ASAP); 2018. Available from: <ext-link xlink:href="https://github.com/computationalpathologygroup/ASAP" ext-link-type="uri">https://github.com/computationalpathologygroup/ASAP</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref023">
      <label>23</label>
      <mixed-citation publication-type="other">Chamberlain S, Ooms J. geojson: Classes for ‘GeoJSON’; 2023.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref024">
      <label>24</label>
      <mixed-citation publication-type="journal"><name><surname>LeCun</surname><given-names>Y</given-names></name>, <name><surname>Boser</surname><given-names>B</given-names></name>, <name><surname>Denker</surname><given-names>JS</given-names></name>, <name><surname>Henderson</surname><given-names>D</given-names></name>, <name><surname>Howard</surname><given-names>RE</given-names></name>, <name><surname>Hubbard</surname><given-names>W</given-names></name>, <etal>et al</etal>. <article-title>Backpropagation applied to handwritten zip code recognition</article-title>. <source>Neural computation</source>. <year>1989</year>;<volume>1</volume>(<issue>4</issue>):<fpage>541</fpage>–<lpage>551</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1162/neco.1989.1.4.541</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref025">
      <label>25</label>
      <mixed-citation publication-type="other">LeCun Y, Bottou L, Bengio Y, Haffner P. Gradient-based learning applied to document recognition. In: Proceedings of the IEEE; 1998. p. 2278–2324.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref026">
      <label>26</label>
      <mixed-citation publication-type="journal"><name><surname>Miotto</surname><given-names>R</given-names></name>, <name><surname>Wang</surname><given-names>F</given-names></name>, <name><surname>Wang</surname><given-names>S</given-names></name>, <name><surname>Jiang</surname><given-names>X</given-names></name>, <name><surname>Dudley</surname><given-names>JT</given-names></name>. <article-title>Deep learning for healthcare: review, opportunities and challenges</article-title>. <source>Briefings in bioinformatics</source>. <year>2018</year>;<volume>19</volume>(<issue>6</issue>):<fpage>1236</fpage>–<lpage>1246</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bib/bbx044</pub-id><?supplied-pmid 28481991?><pub-id pub-id-type="pmid">28481991</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref027">
      <label>27</label>
      <mixed-citation publication-type="book"><name><surname>Krizhevsky</surname><given-names>A</given-names></name>, <name><surname>Sutskever</surname><given-names>I</given-names></name>, <name><surname>Hinton</surname><given-names>GE</given-names></name>. <part-title>Imagenet classification with deep convolutional neural networks</part-title>. In: <source>Advances in Neural Information Processing Systems</source>; <year>2012</year>. p. <fpage>1097</fpage>–<lpage>1105</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref028">
      <label>28</label>
      <mixed-citation publication-type="journal"><name><surname>Rawat</surname><given-names>W</given-names></name>, <name><surname>Wang</surname><given-names>Z</given-names></name>. <article-title>Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review</article-title>. <source>Neural Computation</source>. <year>2017</year>;<volume>29</volume>(<issue>9</issue>):<fpage>2352</fpage>–<lpage>2449</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1162/neco_a_00990</pub-id><?supplied-pmid 28599112?><pub-id pub-id-type="pmid">28599112</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref029">
      <label>29</label>
      <mixed-citation publication-type="other">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:14091556. 2014.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref030">
      <label>30</label>
      <mixed-citation publication-type="other">Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:14090473. 2014.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref031">
      <label>31</label>
      <mixed-citation publication-type="other">Kim Y, Denton C, Hoang L, Rush AM. Structured attention networks. arXiv preprint arXiv:170200887. 2017.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref032">
      <label>32</label>
      <mixed-citation publication-type="other">Lin T, Wang Y, Liu X, Qiu X. A survey of transformers. arXiv preprint arXiv:210604554. 2021.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref033">
      <label>33</label>
      <mixed-citation publication-type="other">Wang X, Girshick R, Gupta A, He K. Non-local neural networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2018. p. 7794–7803.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref034">
      <label>34</label>
      <mixed-citation publication-type="other">Carion N, Massa F, Synnaeve G, Usunier N, Kirillov A, Zagoruyko S. End-to-end object detection with transformers. In: European conference on computer vision. Springer; 2020. p. 213–229.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref035">
      <label>35</label>
      <mixed-citation publication-type="journal"><name><surname>Ramachandran</surname><given-names>P</given-names></name>, <name><surname>Parmar</surname><given-names>N</given-names></name>, <name><surname>Vaswani</surname><given-names>A</given-names></name>, <name><surname>Bello</surname><given-names>I</given-names></name>, <name><surname>Levskaya</surname><given-names>A</given-names></name>, <name><surname>Shlens</surname><given-names>J</given-names></name>. <article-title>Stand-alone self-attention in vision models</article-title>. <source>Advances in Neural Information Processing Systems</source>. <year>2019</year>;<volume>32</volume>.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref036">
      <label>36</label>
      <mixed-citation publication-type="other">Liu Y, Zhang Y, Wang Y, Hou F, Yuan J, Tian J, et al. A survey of visual transformers. arXiv preprint arXiv:211106091. 2021.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref037">
      <label>37</label>
      <mixed-citation publication-type="journal"><name><surname>Khan</surname><given-names>S</given-names></name>, <name><surname>Naseer</surname><given-names>M</given-names></name>, <name><surname>Hayat</surname><given-names>M</given-names></name>, <name><surname>Zamir</surname><given-names>SW</given-names></name>, <name><surname>Khan</surname><given-names>FS</given-names></name>, <name><surname>Shah</surname><given-names>M</given-names></name>. <article-title>Transformers in vision: A survey</article-title>. <source>ACM computing surveys (CSUR)</source>. <year>2022</year>;<volume>54</volume>(<issue>10s</issue>):<fpage>1</fpage>–<lpage>41</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1145/3505244</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref038">
      <label>38</label>
      <mixed-citation publication-type="journal"><name><surname>Han</surname><given-names>K</given-names></name>, <name><surname>Wang</surname><given-names>Y</given-names></name>, <name><surname>Chen</surname><given-names>H</given-names></name>, <name><surname>Chen</surname><given-names>X</given-names></name>, <name><surname>Guo</surname><given-names>J</given-names></name>, <name><surname>Liu</surname><given-names>Z</given-names></name>, <etal>et al</etal>. <article-title>A survey on vision transformer</article-title>. <source>IEEE transactions on pattern analysis and machine intelligence</source>. <year>2022</year>. <?supplied-pmid 35180075?><pub-id pub-id-type="pmid">35180075</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref039">
      <label>39</label>
      <mixed-citation publication-type="journal"><name><surname>Janowczyk</surname><given-names>A</given-names></name>, <name><surname>Zuo</surname><given-names>R</given-names></name>, <name><surname>Gilmore</surname><given-names>H</given-names></name>, <name><surname>Feldman</surname><given-names>M</given-names></name>, <name><surname>Madabhushi</surname><given-names>A</given-names></name>. <article-title>Automatic Measurement of Sister Chromatid Exchange Frequency</article-title>. <source>JCO Clin Cancer Inform</source>. <year>2019</year>;<volume>3</volume>:<fpage>1</fpage>–<lpage>7</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref040">
      <label>40</label>
      <mixed-citation publication-type="journal"><name><surname>Gutman</surname><given-names>DA</given-names></name>, <name><surname>Khalilia</surname><given-names>M</given-names></name>, <name><surname>Lee</surname><given-names>S</given-names></name>, <name><surname>Nalisnik</surname><given-names>M</given-names></name>, <name><surname>Mullen</surname><given-names>Z</given-names></name>, <name><surname>Beezley</surname><given-names>J</given-names></name>, <etal>et al</etal>. <article-title>The Digital Slide Archive: A Software Platform for Management, Integration and Analysis of Histology for Cancer Research</article-title>. <source>Cancer research</source>. <year>2018</year>;<volume>77</volume>(<issue>21</issue>):<fpage>e75</fpage>–<lpage>e78</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1158/0008-5472.CAN-17-0629</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref041">
      <label>41</label>
      <mixed-citation publication-type="journal"><name><surname>Marcolini</surname><given-names>A</given-names></name>, <name><surname>Bussola</surname><given-names>N</given-names></name>, <name><surname>Arbitrio</surname><given-names>E</given-names></name>, <name><surname>Amgad</surname><given-names>M</given-names></name>, <name><surname>Jurman</surname><given-names>G</given-names></name>, <name><surname>Furlanello</surname><given-names>C</given-names></name>. <article-title>histolab: A Python library for reproducible Digital Pathology preprocessing with automated testing</article-title>. <source>SoftwareX</source>. <year>2022</year>;<volume>20</volume>:<fpage>101237</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.softx.2022.101237</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref042">
      <label>42</label>
      <mixed-citation publication-type="other">MONAI Consortium. MONAI: Medical Open Network for AI; 2020. Available from: <ext-link xlink:href="https://github.com/Project-MONAI/MONAI" ext-link-type="uri">https://github.com/Project-MONAI/MONAI</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref043">
      <label>43</label>
      <mixed-citation publication-type="journal"><name><surname>Rosenthal</surname><given-names>J</given-names></name>, <name><surname>Carelli</surname><given-names>R</given-names></name>, <name><surname>Omar</surname><given-names>M</given-names></name>, <name><surname>Brundage</surname><given-names>D</given-names></name>, <name><surname>Halbert</surname><given-names>E</given-names></name>, <name><surname>Nyman</surname><given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Building Tools for Machine Learning and Artificial Intelligence in Cancer Research: Best Practices and a Case Study with the PathML Toolkit for Computational Pathology</article-title>. <source>Molecular Cancer Research</source>. <year>2022</year>;<volume>20</volume>(<issue>2</issue>):<fpage>202</fpage>–<lpage>206</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1158/1541-7786.MCR-21-0665</pub-id><?supplied-pmid 34880124?><pub-id pub-id-type="pmid">34880124</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref044">
      <label>44</label>
      <mixed-citation publication-type="other">Tan M, Chen B, Pang R, Vasudevan V, Sandler M, Howard A, et al. Mnasnet: Platform-aware neural architecture search for mobile. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; 2019. p. 2820–2828.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref045">
      <label>45</label>
      <mixed-citation publication-type="other">Iandola FN, Han S, Moskewicz MW, Ashraf K, Dally WJ, Keutzer K. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&lt; 0.5 MB model size. arXiv preprint arXiv:160207360. 2016.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref046">
      <label>46</label>
      <mixed-citation publication-type="other">Howard AG, Zhu M, Chen B, Kalenichenko D, Wang W, Weyand T, et al. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:170404861. 2017.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref047">
      <label>47</label>
      <mixed-citation publication-type="other">Selvaraju RR, Cogswell M, Das A, Vedantam R, Parikh D, Batra D. Grad-CAM: Visual explanations from deep networks via gradient-based localization. In: Proceedings of the IEEE International Conference on Computer Vision; 2017. p. 618–626.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref048">
      <label>48</label>
      <mixed-citation publication-type="journal"><name><surname>Ross-Innes</surname><given-names>C</given-names></name>, <etal>et al</etal>. <article-title>Risk stratification of Barrett’s oesophagus using a non-endoscopic sampling method coupled with a biomarker panel: a cohort study</article-title>. <source>Lancet Gastroenterol Hepatol</source>. <year>2017</year>;<volume>2</volume>(<issue>1</issue>):<fpage>23</fpage>–<lpage>31</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/S2468-1253(16)30118-2</pub-id><?supplied-pmid 28404010?><pub-id pub-id-type="pmid">28404010</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref049">
      <label>49</label>
      <mixed-citation publication-type="journal"><name><surname>Peters</surname><given-names>CJ</given-names></name>, <name><surname>Rees</surname><given-names>JR</given-names></name>, <name><surname>Hardwick</surname><given-names>RH</given-names></name>, <name><surname>Hardwick</surname><given-names>JS</given-names></name>, <name><surname>Vowler</surname><given-names>SL</given-names></name>, <name><surname>Ong</surname><given-names>CAJ</given-names></name>, <etal>et al</etal>. <article-title>A 4-gene signature predicts survival of patients with resected adenocarcinoma of the esophagus, junction, and gastric cardia</article-title>. <source>Gastroenterology</source>. <year>2010</year>;<volume>139</volume>(<issue>6</issue>):<fpage>1995</fpage>–<lpage>2004</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1053/j.gastro.2010.05.080</pub-id><?supplied-pmid 20621683?><pub-id pub-id-type="pmid">20621683</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref050">
      <label>50</label>
      <mixed-citation publication-type="journal"><collab>Cancer Genome Atlas Research Network</collab>, <name><surname>Weinstein</surname><given-names>JN</given-names></name>, <name><surname>Collisson</surname><given-names>EA</given-names></name>, <name><surname>Mills</surname><given-names>GB</given-names></name>, <name><surname>Shaw</surname><given-names>KR</given-names></name>, <name><surname>Ozenberger</surname><given-names>BA</given-names></name>, <etal>et al</etal>. <article-title>The Cancer Genome Atlas Pan-Cancer analysis project</article-title>. <source>Nat Genet</source>. <year>2013</year>;<volume>45</volume>(<issue>10</issue>):<fpage>1113</fpage>–<lpage>1120</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/ng.2764</pub-id><?supplied-pmid 24071849?><pub-id pub-id-type="pmid">24071849</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS One</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-title-group>
      <journal-title>PLOS ONE</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1932-6203</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10406329</article-id>
    <article-id pub-id-type="publisher-id">PONE-D-23-08782</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pone.0289499</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
            <subj-group>
              <subject>Deep Learning</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Neural Networks</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Neural Networks</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Imaging Techniques</subject>
          <subj-group>
            <subject>Image Analysis</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Oncology</subject>
          <subj-group>
            <subject>Cancers and Neoplasms</subject>
            <subj-group>
              <subject>Breast Tumors</subject>
              <subj-group>
                <subject>Breast Cancer</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>People and Places</subject>
        <subj-group>
          <subject>Population Groupings</subject>
          <subj-group>
            <subject>Professions</subject>
            <subj-group>
              <subject>Medical Personnel</subject>
              <subj-group>
                <subject>Pathologists</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Urology</subject>
          <subj-group>
            <subject>Genitourinary Cancers</subject>
            <subj-group>
              <subject>Testicular Cancer</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Oncology</subject>
          <subj-group>
            <subject>Cancers and Neoplasms</subject>
            <subj-group>
              <subject>Genitourinary Tract Tumors</subject>
              <subj-group>
                <subject>Testicular Cancer</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Oncology</subject>
          <subj-group>
            <subject>Cancers and Neoplasms</subject>
            <subj-group>
              <subject>Genitourinary Tract Tumors</subject>
              <subj-group>
                <subject>Prostate Cancer</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Urology</subject>
          <subj-group>
            <subject>Prostate Diseases</subject>
            <subj-group>
              <subject>Prostate Cancer</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Gastroenterology and Hepatology</subject>
          <subj-group>
            <subject>Gastrointestinal Cancers</subject>
          </subj-group>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>SliDL: A toolbox for processing whole-slide images in deep learning</article-title>
      <alt-title alt-title-type="running-head">SliDL: A toolbox for processing whole-slide images in deep learning</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Berman</surname>
          <given-names>Adam G.</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role>
        <role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role>
        <role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/software/">Software</role>
        <role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role>
        <role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Orchard</surname>
          <given-names>William R.</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Gehrung</surname>
          <given-names>Marcel</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/software/">Software</role>
        <xref rid="aff001" ref-type="aff"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Markowetz</surname>
          <given-names>Florian</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role>
        <role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role>
        <role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role>
        <xref rid="aff001" ref-type="aff"/>
        <xref rid="cor001" ref-type="corresp">*</xref>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <addr-line>Cancer Research UK Cambridge Institute, University of Cambridge, Cambridge, United Kingdom</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Fernandez-Lozano</surname>
          <given-names>Carlos</given-names>
        </name>
        <role>Editor</role>
        <xref rid="edit1" ref-type="aff"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>University of A Coruña, SPAIN</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p><bold>Competing Interests: </bold>I have read the journal’s policy and the authors of this manuscript have the following competing interests: M.G. is an employee and shareholder of Cyted Ltd. F.M. is a co-founder and director of Tailor Bio. This does not alter our adherence to PLOS ONE policies on sharing data and materials.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>florian.markowetz@cruk.cam.ac.uk</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>7</day>
      <month>8</month>
      <year>2023</year>
    </pub-date>
    <volume>18</volume>
    <issue>8</issue>
    <elocation-id>e0289499</elocation-id>
    <history>
      <date date-type="received">
        <day>27</day>
        <month>3</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>20</day>
        <month>7</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2023 Berman et al</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder>Berman et al</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pone.0289499.pdf"/>
    <abstract>
      <p>The inspection of stained tissue slides by pathologists is essential for the early detection, diagnosis and monitoring of disease. Recently, deep learning methods for the analysis of whole-slide images (WSIs) have shown excellent performance on these tasks, and have the potential to substantially reduce the workload of pathologists. However, WSIs present a number of unique challenges for analysis, requiring special consideration of image annotations, slide and image artefacts, and evaluation of WSI-trained model performance. Here we introduce SliDL, a Python library for performing pre- and post-processing of WSIs. SliDL makes WSI data handling easy, allowing users to perform essential processing tasks in a few simple lines of code, bridging the gap between standard image analysis and WSI analysis. We introduce each of the main functionalities within SliDL: from annotation and tile extraction to tissue detection and model evaluation. We also provide ‘code snippets’ to guide the user in running SliDL. SliDL has been designed to interact with PyTorch, one of the most widely used deep learning libraries, allowing seamless integration into deep learning workflows. By providing a framework in which deep learning methods for WSI analysis can be developed and applied, SliDL aims to increase the accessibility of an important application of deep learning.</p>
    </abstract>
    <funding-group>
      <award-group id="award001">
        <funding-source>
          <institution>Bill &amp; Melinda Gates Foundation</institution>
        </funding-source>
        <award-id>Gates Cambridge Scholarship</award-id>
        <principal-award-recipient>
          <name>
            <surname>Berman</surname>
            <given-names>Adam G.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award002">
        <funding-source>
          <institution>Cancer Research UK</institution>
        </funding-source>
        <award-id>C14303/A17197</award-id>
        <principal-award-recipient>
          <name>
            <surname>Markowetz</surname>
            <given-names>Florian</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award003">
        <funding-source>
          <institution>Peterhouse, Cambridge</institution>
        </funding-source>
        <award-id>Peterhouse Studentship</award-id>
        <principal-award-recipient>
          <name>
            <surname>Orchard</surname>
            <given-names>William R.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award004">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100012338</institution-id>
            <institution>Alan Turing Institute</institution>
          </institution-wrap>
        </funding-source>
        <award-id>Enrichment Fellowship</award-id>
        <principal-award-recipient>
          <name>
            <surname>Gehrung</surname>
            <given-names>Marcel</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <funding-statement>This research was supported by Cancer Research UK (FM: C14303/A17197, <ext-link xlink:href="https://www.cancerresearchuk.org/" ext-link-type="uri">https://www.cancerresearchuk.org/</ext-link>). A.G.B. acknowledges support from a Gates Cambridge Scholarship from the Bill &amp; Melinda Gates Foundation (<ext-link xlink:href="https://www.gatescambridge.org/" ext-link-type="uri">https://www.gatescambridge.org/</ext-link>). W.R.O. acknowledges support from a Peterhouse Studentship from Peterhouse, Cambridge (<ext-link xlink:href="https://www.pet.cam.ac.uk/" ext-link-type="uri">https://www.pet.cam.ac.uk/</ext-link>). M.G. acknowledges support from an Enrichment Fellowship from the Alan Turing Institute (<ext-link xlink:href="https://www.turing.ac.uk/work-turing/studentships/enrichment" ext-link-type="uri">https://www.turing.ac.uk/work-turing/studentships/enrichment</ext-link>). F.M. is a Royal Society Wolfson Research Merit Award holder (<ext-link xlink:href="https://royalsociety.org/grants-schemes-awards/grants/wolfson-research-merit/" ext-link-type="uri">https://royalsociety.org/grants-schemes-awards/grants/wolfson-research-merit/</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="4"/>
      <table-count count="1"/>
      <page-count count="25"/>
    </counts>
    <custom-meta-group>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>The source code of SliDL is freely available at a public repository: <ext-link xlink:href="https://github.com/markowetzlab/slidl" ext-link-type="uri">https://github.com/markowetzlab/slidl</ext-link>. The source code of a comprehensive SliDL tutorial is also freely available at the following public repository: <ext-link xlink:href="https://github.com/markowetzlab/slidl-tutorial" ext-link-type="uri">https://github.com/markowetzlab/slidl-tutorial</ext-link>. That repository also contains the code used to train SliDL’s deep tissue detector in its deep_tissue_detector subdirectory. Complete documentation of SliDL including its application public interface (API) reference is available at the following URL: <ext-link xlink:href="https://linkprotect.cudasvc.com/url?a=https%3a%2f%2fslidl.readthedocs.io%2fen%2flatest%2f&amp;c=E,1,S9zTxKndcPNyESUe0QhuC-cota0vA0CSPg9ZEs39y3UNkOIHweCHY-B2ognY52rkVtjub0msWdNm276Yj52DPMFfVPVXx3-En7cCLNKYvFHAcgCogA,,&amp;typo=1" ext-link-type="uri">https://linkprotect.cudasvc.com/url?a=https%3a%2f%2fslidl.readthedocs.io%2fen%2flatest%2f&amp;c=E,1,S9zTxKndcPNyESUe0QhuC-cota0vA0CSPg9ZEs39y3UNkOIHweCHY-B2ognY52rkVtjub0msWdNm276Yj52DPMFfVPVXx3-En7cCLNKYvFHAcgCogA,,&amp;typo=1</ext-link>. The CAMELYON-16 WSIs and corresponding annotations used in the SliDL tutorial are freely available and can be downloaded by following the instructions at: <ext-link xlink:href="https://github.com/markowetzlab/slidl-tutorial" ext-link-type="uri">https://github.com/markowetzlab/slidl-tutorial</ext-link>. The WSIs related to the deep tissue detector can be accessed at <ext-link xlink:href="https://linkprotect.cudasvc.com/url?a=https%3a%2f%2fdoi.org%2f10.5281%2fzenodo.7947380&amp;c=E,1,rsogzNLylIHJ4goNx4QP3CJ3g6vTURO4JhL0M9GdLRdapBL-R-DOe0UoTPy6exTung3_MGTjeFNl8ylJcaXF0wIpT89JgjVD4p38UYY91jVClmkF&amp;typo=1" ext-link-type="uri">https://linkprotect.cudasvc.com/url?a=https%3a%2f%2fdoi.org%2f10.5281%2fzenodo.7947380&amp;c=E,1,rsogzNLylIHJ4goNx4QP3CJ3g6vTURO4JhL0M9GdLRdapBL-R-DOe0UoTPy6exTung3_MGTjeFNl8ylJcaXF0wIpT89JgjVD4p38UYY91jVClmkF&amp;typo=1</ext-link>.</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>The source code of SliDL is freely available at a public repository: <ext-link xlink:href="https://github.com/markowetzlab/slidl" ext-link-type="uri">https://github.com/markowetzlab/slidl</ext-link>. The source code of a comprehensive SliDL tutorial is also freely available at the following public repository: <ext-link xlink:href="https://github.com/markowetzlab/slidl-tutorial" ext-link-type="uri">https://github.com/markowetzlab/slidl-tutorial</ext-link>. That repository also contains the code used to train SliDL’s deep tissue detector in its deep_tissue_detector subdirectory. Complete documentation of SliDL including its application public interface (API) reference is available at the following URL: <ext-link xlink:href="https://linkprotect.cudasvc.com/url?a=https%3a%2f%2fslidl.readthedocs.io%2fen%2flatest%2f&amp;c=E,1,S9zTxKndcPNyESUe0QhuC-cota0vA0CSPg9ZEs39y3UNkOIHweCHY-B2ognY52rkVtjub0msWdNm276Yj52DPMFfVPVXx3-En7cCLNKYvFHAcgCogA,,&amp;typo=1" ext-link-type="uri">https://linkprotect.cudasvc.com/url?a=https%3a%2f%2fslidl.readthedocs.io%2fen%2flatest%2f&amp;c=E,1,S9zTxKndcPNyESUe0QhuC-cota0vA0CSPg9ZEs39y3UNkOIHweCHY-B2ognY52rkVtjub0msWdNm276Yj52DPMFfVPVXx3-En7cCLNKYvFHAcgCogA,,&amp;typo=1</ext-link>. The CAMELYON-16 WSIs and corresponding annotations used in the SliDL tutorial are freely available and can be downloaded by following the instructions at: <ext-link xlink:href="https://github.com/markowetzlab/slidl-tutorial" ext-link-type="uri">https://github.com/markowetzlab/slidl-tutorial</ext-link>. The WSIs related to the deep tissue detector can be accessed at <ext-link xlink:href="https://linkprotect.cudasvc.com/url?a=https%3a%2f%2fdoi.org%2f10.5281%2fzenodo.7947380&amp;c=E,1,rsogzNLylIHJ4goNx4QP3CJ3g6vTURO4JhL0M9GdLRdapBL-R-DOe0UoTPy6exTung3_MGTjeFNl8ylJcaXF0wIpT89JgjVD4p38UYY91jVClmkF&amp;typo=1" ext-link-type="uri">https://linkprotect.cudasvc.com/url?a=https%3a%2f%2fdoi.org%2f10.5281%2fzenodo.7947380&amp;c=E,1,rsogzNLylIHJ4goNx4QP3CJ3g6vTURO4JhL0M9GdLRdapBL-R-DOe0UoTPy6exTung3_MGTjeFNl8ylJcaXF0wIpT89JgjVD4p38UYY91jVClmkF&amp;typo=1</ext-link>.</p>
  </notes>
</front>
<body>
  <sec sec-type="intro" id="sec001">
    <title>Introduction</title>
    <p>In histopathology, tissue biopsies are fixed, embedded, sectioned, stained, and placed on a glass slide before being examined under a microscope. Examination of tissue slides to identify pathologically relevant features has been an essential tool for early detection, diagnosis and disease monitoring in medical practice and research for decades. Pathological features can be anything from the presence or absence of certain cell types or populations, changes in cellular or nuclear morphology, changes in the arrangement of cells in a tissue, to changes in the intensity of certain tissue stains. Until recently only expert pathologists have been able to perform this task, requiring years of training, and with individual slides often having to be evaluated by multiple pathologists before a judgement can be made [<xref rid="pone.0289499.ref001" ref-type="bibr">1</xref>]. However, with a shift towards digitisation in pathology, tissue-slides are now routinely scanned to produce high-resolution whole-slide images (WSIs). Such images are amenable to automated image analysis and in the last decade the field has undergone a revolution. Deep learning methods for image analysis have shown excellent performance on diagnostic tasks [<xref rid="pone.0289499.ref001" ref-type="bibr">1</xref>–<xref rid="pone.0289499.ref003" ref-type="bibr">3</xref>], rivalling that of pathologists and further stimulating efforts to digitise glass slides.</p>
    <p>Pathologists have high inter-observer concordance rates on some diagnostic tasks, but in others they frequently disagree [<xref rid="pone.0289499.ref004" ref-type="bibr">4</xref>]. This is compounded by high workload, necessitating rapid screening of individual cases, increasing the risk of introducing diagnostic errors [<xref rid="pone.0289499.ref005" ref-type="bibr">5</xref>]. Deep learning methods are fast, often requiring only a few minutes to evaluate a slide, and give consistent evaluations. Thus, deep learning has the potential to substantially reduce the workload of pathologists, improve the inter-observer concordance rates and accelerate the evaluation of tissue-slides. The application of deep learning to pathological datasets is therefore a quickly growing field, as researchers apply the latest advances in machine learning, such as GANs [<xref rid="pone.0289499.ref006" ref-type="bibr">6</xref>, <xref rid="pone.0289499.ref007" ref-type="bibr">7</xref>] and transformers [<xref rid="pone.0289499.ref008" ref-type="bibr">8</xref>, <xref rid="pone.0289499.ref009" ref-type="bibr">9</xref>], to whole slide image problems [<xref rid="pone.0289499.ref010" ref-type="bibr">10</xref>, <xref rid="pone.0289499.ref011" ref-type="bibr">11</xref>].</p>
    <p>Despite this potential, deep learning based approaches have not yet seen widespread uptake in medical practice. This is in part due to a lack of an accessible framework in which WSI neural network implementations are developed and applied, meaning that individual researchers often must re-implement their own pre- and post-processing pipelines in-house for each new histopathology task. Furthermore, successful implementation of deep learning to WSI analysis requires careful consideration of model hyperparameters, slide and image artefacts and data augmentation beyond those encountered in standard image analysis, and thus application of the latest advances in computer vision to WSI analysis is hampered without a framework for streamlined WSI processing into which such advances can be incorporated.</p>
    <p>Here we introduce SliDL, a new Python library for performing pre- and post-processing of WSI data. SliDL simplifies and streamlines many of the steps required to tackle the unique challenges posed by WSIs. This includes, but is not limited to, detection of tissue, slide and tissue artefacts and background in WSIs, easy implementation of alternative tiling strategies, automatic generation of binary and multi-class segmentation masks from digital annotations, and utility functions for visualisation and evaluation of model outputs (see <xref rid="pone.0289499.g001" ref-type="fig">Fig 1</xref> and <xref rid="pone.0289499.s002" ref-type="supplementary-material">S1 Table</xref> for an overview of the main functionalities in SliDL). Although other tools exist which provide some of the same functionalities for pre-processing, SliDL is unique in its comprehensive support for annotation handling (see Related methods). By simplifying and streamlining these steps, SliDL aims to empower researchers in the clinical sciences to accelerate the application of deep learning to both existing and newly generated WSI data, so that the latest innovations in digital pathology can reach the clinic sooner.</p>
    <fig position="float" id="pone.0289499.g001">
      <object-id pub-id-type="doi">10.1371/journal.pone.0289499.g001</object-id>
      <label>Fig 1</label>
      <caption>
        <title>A deep learning pipeline with SliDL.</title>
        <p><bold>(A)</bold> Creating SliDL Slide objects, exporting and adding annotations, and extracting tiles and segmentation masks. <bold>(B)</bold> Data partitioning, data augmentation, and model training. <bold>(C)</bold> Inferring on the trained model and stitching together overlapping segmentation results (if required).</p>
      </caption>
      <graphic xlink:href="pone.0289499.g001" position="float"/>
    </fig>
    <p>SliDL therefore takes into account all of eccentricities of the WSI data type. For example, their large size makes it is necessary to break up WSIs into ‘tiles’ before they can be analysed by contemporary deep learning architectures (see Tiling). Tiling, however, introduces further difficulties as WSIs are often labelled (e.g. cancerous or non-cancerous) at the slide level, not at the tile level, and so a deep learning approach must be adopted which accounts for how tiles inherit labels from slides (see Annotation). Furthermore, WSIs can contain unique artefacts introduced during slide preparation and imaging, which are not found in other image analysis settings, such as pen marks left by the pathologists reviewing them, or cracks and bubbles in the slide. All of these artefacts must be removed or accounted for when training a deep learning model (see Deep tissue detector). SliDL includes easy-to-use functions to both perform tiling and to filter out artefact and background slides using a built-in deep neural network.</p>
    <p>SliDL has been designed to interact with popular deep learning library PyTorch [<xref rid="pone.0289499.ref012" ref-type="bibr">12</xref>], allowing it to be seamlessly incorporated into deep learning workflows. By tackling the unique challenges posed by WSIs, SliDL can help to translate deep learning methods into the clinic more easily, providing a broad method to replace <italic toggle="yes">ad hoc</italic> solutions, and an accessible entry point to applying deep learning to WSIs for machine learning researchers unfamiliar with the nuances of pathology slides. SliDL is available from <ext-link xlink:href="https://github.com/markowetzlab/slidl" ext-link-type="uri">https://github.com/markowetzlab/slidl</ext-link>, documentation is available at <ext-link xlink:href="https://slidl.readthedocs.io/en/latest/" ext-link-type="uri">https://slidl.readthedocs.io/en/latest/</ext-link>, and a full tutorial and example SliDL workflow is available at <ext-link xlink:href="https://github.com/markowetzlab/slidl-tutorial" ext-link-type="uri">https://github.com/markowetzlab/slidl-tutorial</ext-link>, which trains, infers, and evaluates classification and segmentation models built from a balanced dataset of nine lymph node node section WSIs containing breast cancer metastases, and nine without metastases. The WSIs come from the publicly-available CAMELYON16 dataset [<xref rid="pone.0289499.ref013" ref-type="bibr">13</xref>].</p>
    <p>SliDL was motivated by a perceived gap in existing tools comprising a number of features we see as crucial to the application of deep learning to histopathological tasks, which we therefore implemented and included in SliDL. Among these are its in-built, robust, benchmarked deep tissue detector, its numerous functions for reading in and extracting tiles from digital annotations, its capacity to generate tile-level segmentation masks, and its capacity to perform inference and compute performance metrics (see Distinct advantages of SliDL). All of these features and many more are made immediately accessible to novice and expert digital pathologists: SliDL is capable of shortening hundreds of lines of code requiring in-depth knowledge of image analysis into five to fifteen idiomatic lines which can be understood and implemented quickly and easily.</p>
    <p>In this article we describe each of the major functionalities of SliDL in the order of their application in a typical deep learning pipeline. First showing how WSI data is handled within SliDL and how to import WSIs (Handling whole-slide images), implement different tiling strategies (Tiling). Then, we move on to how to apply the ‘deep tissue detector’ to detect tissue and remove background and artefacts from slides (Deep tissue detector). Next, we show how SliDL enables handling of digital annotations and the extraction of tiles and their corresponding segmentation masks (Annotation), before finally demonstrating how the library can be used to streamline various aspects of model training (Training), inference (Inference) and evaluation (Evaluating model performance). In each section, ‘code snippets’ are provided giving guidance on how SliDL should be run (see <xref rid="pone.0289499.s003" ref-type="supplementary-material">S2 Table</xref> for a table detailing each of the functions displayed in code snippets below and defining their arguments).</p>
  </sec>
  <sec id="sec002">
    <title>Implementation</title>
    <sec id="sec003">
      <title>Handling whole-slide images</title>
      <p>When glass slides are digitised by digital whole-slide image scanners, high-resolution images are taken at multiple magnifications. WSIs therefore have a pyramidal data structure, with the images taken at each magnification each forming a ‘layer’ of the WSI. The maximum magnification of these images is frequently 200X (by convention called ‘20X’, due to scanning being performed using a 20X objective lens at 10X magnification) or 400X (by convention ‘40X’, using a 40X objective lens at 10X magnification) [<xref rid="pone.0289499.ref014" ref-type="bibr">14</xref>].</p>
      <p>SliDL uses the Pyvips library for reading WSIs [<xref rid="pone.0289499.ref015" ref-type="bibr">15</xref>], and so supports a wide range of formats, including NDPI and pyramidal TIFF, including all OpenSlide formats, and those which are loaded via ImageMagick or GraphicsMagick such as DICOM [<xref rid="pone.0289499.ref014" ref-type="bibr">14</xref>]. After importing SliDL, WSIs are instantiated as <monospace specific-use="no-wrap">Slide</monospace> objects by calling the <monospace specific-use="no-wrap">Slide</monospace> class on the file path to the WSI and specifying which layer you would like to access with the <monospace specific-use="no-wrap">level</monospace> argument.</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 1</bold>. <bold>Import SliDL and initialise a Slide object with a path to a WSI</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
from slidl.slide import Slide
slidl_slide = Slide(path_to_wsi, level=0)
</preformat>
      <p>It is through <monospace specific-use="no-wrap">Slide</monospace> objects that the user interacts with their data and performs the pre- and post-processing steps described in the following sections.</p>
      <p>During pre- and post-processing, SliDL <monospace specific-use="no-wrap">Slide</monospace> objects are generally modified in-place, with new information being added to an internal dictionary (called the ‘tile dictionary’, see Tiling below). It is therefore important that users save <monospace specific-use="no-wrap">Slide</monospace> objects after performing an operation on them, particularly time-intensive operations such as applying the deep tissue detector (see Deep tissue detector below). By doing this, the user does not have to wait for expensive functions to complete more than once.</p>
      <p>Saving <monospace specific-use="no-wrap">Slide</monospace> is achieved by using the <monospace specific-use="no-wrap">Slide.save()</monospace> method, preserving the entire <monospace specific-use="no-wrap">Slide</monospace> object in its current state to a <monospace specific-use="no-wrap">.pml</monospace> file in the directory specified by the <monospace specific-use="no-wrap">folder</monospace> argument using lossless compression.</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 2</bold>. <bold>Save a Slide object</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
slidl_slide.save(folder='path/to/folder')
</preformat>
      <p>To reload a <monospace specific-use="no-wrap">Slide</monospace> object which has been saved, simply set the first argument of the <monospace specific-use="no-wrap">Slide</monospace> initialiser to the <monospace specific-use="no-wrap">Slide</monospace> object rather than to a WSI.</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 3</bold>. <bold>Reload a saved Slide object</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
slidl_slide = Slide('/path/to/folder/slidl_slide.pml', level=0)
</preformat>
    </sec>
    <sec id="sec004">
      <title>Tiling</title>
      <p>WSI images are very large; for example, an image scanned at 40X objective power of a 20 x 20 mm sample of tissue has 80,000 x 80,000 pixels; at standard 24-bit colour this would produce a flat image 19.2GB in size. Current neural network architectures are unable to process images of this size in one go. Thus, WSIs are broken up into ‘tiles’ or ‘patches’ upon which the model is trained: small square regions of the original image, typically 32 to 1000 pixels in height. Tiles can be chosen with or without overlap with neighbouring tiles. Choice of tile dimensions and overlap are some of the most important hyperparameters to choose when analysing WSIs [<xref rid="pone.0289499.ref016" ref-type="bibr">16</xref>].</p>
      <p>In SliDL, tile dimensions and overlap are chosen by calling the <monospace specific-use="no-wrap">Slide.setTileProperties()</monospace> method, and setting the <monospace specific-use="no-wrap">tileSize</monospace> and <monospace specific-use="no-wrap">tileOverlap</monospace> arguments, enabling users to easily experiment with different tiling strategies. Here one can also specify how tiles should be stored, and how they will be accessed during training.</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 4</bold>. <bold>Set the tile properties in a Slide</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
slidl_slide.setTileProperties(tileSize=500, tileOverlap=0, unit='px')
</preformat>
      <p>By calling <monospace specific-use="no-wrap">Slide.extractAnnotationTiles()</monospace> or <monospace specific-use="no-wrap">Slide.extractRandomUnannotatedTiles()</monospace>, one can extract and store each tile in individual images files in advance of training or inferring. SliDL will automatically store tile image files according to their class (see Annotation) and slide of origin in a directory structure appropriate for use with PyTorch see <xref rid="pone.0289499.s001" ref-type="supplementary-material">S1 Fig</xref>).</p>
      <p>Although functional, storing each individual tile image file may pose data storage issues. SliDL stores the coordinates of each tile rather than the tile image itself, accessible with <monospace specific-use="no-wrap">Slide.getTile()</monospace> using the tile address as argument (all tile addresses can be iterated over with <monospace specific-use="no-wrap">Slide.iterateTiles()</monospace>), making it easy to build a dataset such that tiles are accessed on-the-fly, saving the need to extract each tile to an image file in situations where this would be too memory intensive.</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 5</bold>. <bold>Iterate through the tiles in a Slide</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
for tile_coords in Slide.iterateTiles():
    pyvips_tile_image = slidl_slide.getTile(tile_coords)
</preformat>
      <p>Apart from being substantially less disk memory intensive, this approach also makes it easier to experiment with different tiling strategies without having to re-extract tiles for each combination of tile size and overlap. The trade off is that on-the-fly tile accession approaches are typically much slower to train with, so are not recommended except in datasets where tiles number in the hundreds of thousands or millions and disk memory for these tile images is not available.</p>
    </sec>
    <sec id="sec005">
      <title>Deep tissue detector</title>
      <p>WSIs can contain unique artefacts introduced during slide preparation and imaging which are not found in other image analysis settings. Tissue may tear and fold during slide preparation, the image may be unevenly illuminated or stained, and parts of the image may be out of focus. Tissue slides also often contain pen marks left by the pathologists reviewing them, and may contain cracks and bubbles. Left unaccounted for, such artefacts can have severe detrimental effects on a deep learning model. For instance, pen marks are often left by pathologists to indicate the presence of a pathological feature of interest, such as the presence of cancerous cells. If not removed, a deep learning model may simply learn to recognise the presence of a pen mark in slides containing cancer, and thereby be completely inapplicable in medical practice where no such annotation will be available. Beyond artefacts, WSIs will typically contain large portions of background, i.e. regions without any tissue, which do not contain any pathologically relevant information. After tiling your WSI, tiles which contain artefacts or which simply display background should therefore be removed speed up the training process and potentially improve performance.</p>
      <p>SliDL provides a built-in deep tissue detector: a DenseNet [<xref rid="pone.0289499.ref017" ref-type="bibr">17</xref>] convolutional neural network architecture (see Convolutional neural networks) trained to classify tiles as either ‘artefact’, ‘background’, or ‘tissue’. SliDL’s deep tissue detector was trained using 9,071 tiles extracted from 393 individual annotations from 61 WSIs scanned across a variety of machines, time periods, and tissue types, and two different species to account for a broad range of the variation of WSI artefacts (including pen marks, folded or torn tissue, slide bubbles, cracks, blurred or out-of-focus regions, uneven illumination, aberrant staining and other marks), background, and tissue appearances (<xref rid="pone.0289499.g002" ref-type="fig">Fig 2A</xref>). An imbalanced dataset sampler [<xref rid="pone.0289499.ref018" ref-type="bibr">18</xref>] was used to ensure that during training, the model was exposed to equal numbers of artefact, background, and tissue tiles. The deep tissue detector is applied by calling the <monospace specific-use="no-wrap">Slide.detectTissue()</monospace> method, enabling robust detection of tissue tiles at any level of the WSI pyramid desired. SliDL then saves the output probabilities that each tile belongs to each of the three classes internally for each tile in the <monospace specific-use="no-wrap">Slide</monospace> object.</p>
      <fig position="float" id="pone.0289499.g002">
        <object-id pub-id-type="doi">10.1371/journal.pone.0289499.g002</object-id>
        <label>Fig 2</label>
        <caption>
          <title>Tissue, artefact, and background detection with SliDL.</title>
          <p><bold>(A)</bold> Tiling, augmenting, and training a DenseNet CNN to classify tissue, artefact, and background regions on WSIs from a robust dataset representing multiple tissue and species types. This already-trained deep tissue detector can be applied to any SliDL Slide object with SliDL’s Slide.detectTissue() function. <bold>(B)</bold> Comparison of classical foreground methods to the Deep tissue detector. All tests performed are two-tailed Wilcoxon signed-rank tests (<italic toggle="yes">n</italic> = 36). All <italic toggle="yes">P</italic> values are Benjamini-Hochberg adjusted, *<italic toggle="yes">P</italic> &lt; 0.05, **<italic toggle="yes">P</italic> &lt; 0.01, ***<italic toggle="yes">P</italic> &lt; 0.001. <bold>(C)</bold> Three representative sample slides on which benchmarking was performed. The top row displaying a case where tissue and background are easily distinguished and all three approaches perform well. The middle row displaying a case where a clear pen mark artefact is incorrectly identified as tissue by the two classical approaches, indicating that artefact removal pre-processing is necessary, but the deep tissue detector has automatically performed both artefact and tissue detection. The bottom row displaying where a large bubble artefact obscures much of the tissue. Otsu classifies almost the entire slide as background, Triangle does not exclude tissue obscured by the artefact, and the deep tissue detector successfully identifies both.</p>
        </caption>
        <graphic xlink:href="pone.0289499.g002" position="float"/>
      </fig>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 6</bold>. <bold>Apply the deep tissue detector to a Slide</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
slidl_slide.detectTissue(tissueDetectionLevel=1,
                            tissueDetectionTileSize=512,
                            tissueDetectionTileOverlap=0,
                            tissueDetectionUpsampleFactor=4,
                            batchSize=20,
                            numWorkers=16)
</preformat>
      <p>There may be applications where the artefacts encountered are not well covered by the deep tissue detector in SliDL, and thus one should always review examples of its output to verify that the detector is behaving as expected using SliDL’s <monospace specific-use="no-wrap">Slide.visualizeTissueDetection()</monospace> function (<xref rid="pone.0289499.g002" ref-type="fig">Fig 2C</xref>).</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 7</bold>. <bold>Verify the results of the deep tissue detector</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
slidl_slide.visualizeTissueDetection(fileName='tissue_detection')
</preformat>
      <p>Furthermore, SliDL makes it easy to apply a user-provided tissue detection model trained on additional, or alternative, annotated images by setting the <monospace specific-use="no-wrap">modelStateDictPath</monospace> and <monospace specific-use="no-wrap">architecture</monospace> parameters when calling <monospace specific-use="no-wrap">Slide.detectTissue()</monospace> to the path to the custom model and its neural network architecture, respectively.</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 8</bold>. <bold>Apply a user-provided tissue detection model to a Slide</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
slidl_slide.detectTissue(tissueDetectionLevel=1,
                            tissueDetectionTileSize=512,
                            tissueDetectionTileOverlap=0,
                            tissueDetectionUpsampleFactor=4,
                            batchSize=20,
                            numWorkers=16,
                            modelStateDictPath='/path/to/state_dict.pt',
                            architecture='vgg19')
</preformat>
      <p>In certain cases, users may want to make use of classical foreground filtering approaches in place of or in addition to the deep tissue detector. In SliDL, this can be achieved by calling the <monospace specific-use="no-wrap">Slide.detectForeground()</monospace> method, specifying the desired approach with the <monospace specific-use="no-wrap">threshold</monospace> argument and the desired WSI pyramid level to perform detection on with the <monospace specific-use="no-wrap">level</monospace> argument. Note that even if the foreground or deep tissue detector is applied at a different level from that which tiles are later extracted, the foreground/tissue/artefact/background predictions will be carried over appropriately across layers. SliDL currently supports Otsu’s method [<xref rid="pone.0289499.ref019" ref-type="bibr">19</xref>], the triangle algorithm [<xref rid="pone.0289499.ref020" ref-type="bibr">20</xref>], as well as simple intensity thresholding. All are automatically applied when <monospace specific-use="no-wrap">Slide.detectForeground()</monospace> so that the user is able to access the results of any algorithm in downstream functions.</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 9</bold>. <bold>Apply foreground detection methods to a Slide</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
slidl_slide.detectForeground(level=3)
</preformat>
      <p>In general, however, these approaches are less robust to the diversity of artefacts observed in WSIs, as well as appearances of background and tissue, often requiring careful supervision for each application [<xref rid="pone.0289499.ref016" ref-type="bibr">16</xref>]. We performed a benchmarking analysis comparing the performance of the deep tissue detector, Otsu’s and the Triangle algorithm at distinguishing tissue from background at the tile level (<xref rid="pone.0289499.g002" ref-type="fig">Fig 2B and 2C</xref>). Benchmarking was performed on 36 WSIs spanning 10 different tissues, a wide variety of different artefacts, and a range of different scanning and fixing protocols, in total comprising nearly 1.5 million tiles (see Benchmarking for full details). The balanced accuracy, sensitivity and specificity statistics indicate the strengths and weaknesses of each of the methods. The deep tissue detector has a significantly higher specificity to the Triangle algorithm, but is not significantly different from Otsu’s method, indicating that Otsu’s method is comparatively more conservative. The deep tissue detector has a significantly higher sensitivity to Otsu’s method, but is not significantly different from Triangle. Finally, the deep tissue detector has a significantly higher balanced accuracy to both methods, above 90%, indicating that while it does not decisively outperform both other methods in sensitivity and specificity individually, it does strike the best balance between the two. Furthermore, the deep tissue detector performs substantially more consistently at the task, indicating its greater robustness (<xref rid="pone.0289499.g002" ref-type="fig">Fig 2B</xref>). In <xref rid="pone.0289499.g002" ref-type="fig">Fig 2C</xref> we display three examples which are representative of the range of behaviours shown across the full set of slides.</p>
    </sec>
    <sec id="sec006">
      <title>Annotation</title>
      <p>Ground-truth labels for a WSI may exist either at the region-level, wherein they are local to particular regions within the WSI, or at the slide-level, wherein a label applies to the WSI as a whole. Region-level labels typically take the form of digital annotations on the WSI, delineating the regions belonging to certain classes. Specialised software such as QuPath and the Automated Slide Analysis Platform (ASAP) allow users to draw digital annotations onto WSIs and then export them for use in image analysis workflows [<xref rid="pone.0289499.ref021" ref-type="bibr">21</xref>, <xref rid="pone.0289499.ref022" ref-type="bibr">22</xref>]. SliDL supports the use of annotation files in the GeoJSON format as well as the XML format output by ASAP [<xref rid="pone.0289499.ref023" ref-type="bibr">23</xref>]. QuPath annotations can be exported as GeoJSON files using the Groovy script, <monospace specific-use="no-wrap">qupath_to_geojson.groovy</monospace> provided in the SliDL tutorial repository (<ext-link xlink:href="https://github.com/markowetzlab/slidl-tutorial" ext-link-type="uri">https://github.com/markowetzlab/slidl-tutorial</ext-link>).</p>
      <p>An annotation file is added to a corresponding <monospace specific-use="no-wrap">Slide</monospace> object by calling the <monospace specific-use="no-wrap">Slide.addAnnotations()</monospace> method, providing the file path to the <monospace specific-use="no-wrap">annotationFilePath</monospace> argument. The annotations do not need to cover the entire WSI, instead SliDL parses annotations by designating all pixels bounded by an annotation for a given class as being positive for that class, and all other pixels as negative. Furthermore, in the case of annotations with ‘doughnut holes’ (annotations which are not polygons because they contain holes in their middle), users need to simply annotate the doughnut holes and assign them to their own class. Then, when calling <monospace specific-use="no-wrap">Slide.addAnnotations()</monospace>, the name of this class can be provided to the <monospace specific-use="no-wrap">negativeClass</monospace> argument. SliDL will automatically geometrically subtract these doughnut hole annotations from every other annotation class they overlap with. By default, <monospace specific-use="no-wrap">Slide.addAnnotations()</monospace> parses the annotations of all non-doughnut hole classes into the <monospace specific-use="no-wrap">Slide</monospace> object’s tile dictionary, but users can choose to include only certain classes present in the annotations by specifying them explicitly as a list to the <monospace specific-use="no-wrap">classesToAdd</monospace> argument.</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 10</bold>. <bold>Add annotations to a Slide</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
slidl_slide.addAnnotations(annotationFilePath='/path/to/annotations.xml',
                            classesToAdd=['normal', 'tumor'],
                            negativeClass='doughnut_holes',
                            level=0)
</preformat>
      <p>It is commonly the case that researchers wish to train their models exclusively on tiles which fall within annotated regions. As such, in SliDL tiles can be extracted from annotated regions by applying the <monospace specific-use="no-wrap">Slide.extractAnnotationTiles()</monospace> method. A simple heuristic is applied: a tile is extracted if there are any annotations that cover more than a given threshold fraction of the area of the tile. Tiles which are not covered above this threshold for any annotations are ignored entirely. The threshold is set using the <monospace specific-use="no-wrap">tileAnnotationOverlapThreshold</monospace> argument (default is 0.5). Different thresholds can be applied for different classes by providing a dictionary with class names as keys and their corresponding overlap thresholds as values to the <monospace specific-use="no-wrap">tileAnnotationOverlapThreshold</monospace> argument.</p>
      <p>A user may also want to extract tissue tiles at random from a slide which has not been annotated. Calling <monospace specific-use="no-wrap">Slide.extractRandomUnannotationTiles()</monospace> achieves this, and involves most of the same arguments as <monospace specific-use="no-wrap">Slide.extractAnnotationTiles()</monospace>. The <monospace specific-use="no-wrap">unannotatedClassName</monospace> argument allows the user to specify how unannotated tiles should be named.</p>
      <p>In addition, after calling the <monospace specific-use="no-wrap">Slide.detectTissue()</monospace> method on a given <monospace specific-use="no-wrap">Slide</monospace> object, each tile will be inferred on the deep tissue detector, and the resulting tissue probabilities will be stored for each tile in the ‘tile dictionary’. The <monospace specific-use="no-wrap">tissueLevelThreshold</monospace> argument can then be used in subsequent functions such as the tile extraction functions (<monospace specific-use="no-wrap">Slide.extractAnnotationTiles()</monospace> and <monospace specific-use="no-wrap">Slide.extractRandomUnannotatedTiles()</monospace>) to set a minimum tissue probability for a tile to be extracted (recommended value of 0.995).</p>
      <p>Likewise, <monospace specific-use="no-wrap">foregroundLevelThreshold</monospace> can be used to restrict extraction of tiles to those reaching a desired foreground detection threshold as determined by foreground detection techniques such as [<xref rid="pone.0289499.ref019" ref-type="bibr">19</xref>] (set the argument to ‘otsu’) or the triangle algorithm [<xref rid="pone.0289499.ref020" ref-type="bibr">20</xref>] (set the argument to ‘triangle’). Simple average greyscale intensity filtering can be achieved by setting <monospace specific-use="no-wrap">foregroundLevelThreshold</monospace> to an integer between 0 and 100.</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 11</bold>. <bold>Extract random unannotated tiles from a Slide’s WSI</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
channel_data = slidl_slide.extractRandomUnannotatedTiles(
                            outputDir=output_dir,
                            numTilesToExtract= 500,
                            unannotatedClassName='tissue',
                            tissueLevelThreshold=0.995,
                            foregroundLevelThreshold=88)
</preformat>
      <p>Whether classifying or segmenting, it is important to consider how to derive the ground-truth labels for individual tiles from slide label data. For segmentation tasks the principle is straightforward because the ground-truth is at the pixel-level and thus whole-slide segmentation masks can also be tiled and directly inherited by the individual image tiles. As such, once annotations have been added, SliDL allows the user to create both binary and multi-class tile-level segmentation masks for provided annotations. When applying the <monospace specific-use="no-wrap">Slide.extractAnnotationTiles()</monospace> method, binary segmentation masks are created by setting the <monospace specific-use="no-wrap">extractSegmentationMasks</monospace> argument to <monospace specific-use="no-wrap">True</monospace>, and the <monospace specific-use="no-wrap">classesToExtract</monospace> argument set to the name of the class (or list of classes) for which the binary segmentation masks should be created. If the user wishes for empty class directories to be created for classes not present in the annotation, they can be defined with the <monospace specific-use="no-wrap">otherClassNames</monospace> argument. The tile-level masks are then saved to a ‘masks’ directory in the location specified by the <monospace specific-use="no-wrap">outputDir</monospace> argument of <monospace specific-use="no-wrap">Slide.extractAnnotationTiles()</monospace>.</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 12</bold>. <bold>Extract tiles from the annotated regions of a Slide’s WSI</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
channel_data = slidl_slide.extractAnnotationTiles(
                            outputDir='/path/to/folder',
                            classesToExtract=['normal', 'tumor']
                            tileAnnotationOverlapThreshold=0.3,
                            numTilesToExtractPerClass=500,
                            extractSegmentationMasks=True,
                            tissueLevelThreshold=0.995,
                            foregroundLevelThreshold=88)
</preformat>
      <p>In the case of binary segmentation, a separate segmentation mask for each desired class will be extracted. In multi-class segmentation problems, users may want to return ‘stacks’ of tile segmentation masks, where each layer of the stack is the segmentation mask of a different class. SliDL allows users to easily generate these ‘stacked’ multi-class segmentation masks for each tile they extract. <monospace specific-use="no-wrap">Slide.extractAnnotationTilesMultiClassSegmentation()</monospace> performs the same basic tasks as <monospace specific-use="no-wrap">Slide.extractAnnotationTiles()</monospace>, extracting tiles and their corresponding segmentation masks from an annotated WSI, but instead of outputting flat segmentation mask images to the segmentation mask directories, it outputs stacked Numpy ndarray matrices (saved as <monospace specific-use="no-wrap">.npy</monospace> files) as multi-class segmentation masks for each tile instead.</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 13</bold>. <bold>Extract segmentation mask tiles from the annotated regions of a Slide’s WSI</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
channel_data = slidl_slide.extractAnnotationTilesMultiClassSegmentation(
                            outputDir=output_dir,
                            classesToExtract=['normal', 'tumor']
                            tileAnnotationOverlapThreshold=0.3,
                            numTilesToExtractPerClass=500,
                            tissueLevelThreshold=0.995,
                            foregroundLevelThreshold=88)
</preformat>
      <p>For classification tasks a tile inherits the label of any annotations that cover more than the given threshold fraction of the area of the tile specified by the <monospace specific-use="no-wrap">tileAnnotationOverlapThreshold</monospace> argument of the <monospace specific-use="no-wrap">Slide.extractAnnotationTiles()</monospace> method discussed above. In addition, this heuristic is also applied for the <monospace specific-use="no-wrap">numTilesToExtractPerClass</monospace> argument which allows the user to set the maximum number of tiles to extract per class from the slide (default is 100 for each class). This ensures that the user does not extract many more tiles than they need for training for a given class. When there are more extractable tiles for a given class than are requested by the user, <monospace specific-use="no-wrap">numTilesToExtractPerClass</monospace> tiles are selected at random from the class. This argument can be used regardless of whether the user is performing a classification or segmentation task downstream of the tile extraction.</p>
      <p>In addition to extracting the tiles to directories, unless <monospace specific-use="no-wrap">returnTileStats</monospace> is set to <monospace specific-use="no-wrap">False</monospace>, <monospace specific-use="no-wrap">Slide.extractAnnotationTiles()</monospace> returns some summary statistics of all the tile images which were extracted. These values can be used to compute the mean and variance of each of the colour channels across the tile dataset, which some users may want to use to normalise their tiles prior to training.</p>
    </sec>
    <sec id="sec007">
      <title>Training</title>
      <p>Building a dataset with labels that can be interpreted by a deep learning library is an important training consideration. SliDL’s tile extraction functions (<monospace specific-use="no-wrap">Slide.extractAnnotationTiles()</monospace> and <monospace specific-use="no-wrap">Slide.extractRandomUnannotatedTiles()</monospace>) output tiles and labels in a directory structures that are by default compliant for direct input into PyTorch’s <monospace specific-use="no-wrap">torchvision.datasets.ImageFolder</monospace> dataset constructor (see <xref rid="pone.0289499.s001" ref-type="supplementary-material">S1 Fig</xref>), making it straightforward to load the tiles SliDL has extracted into a format ready for training.</p>
      <p>For classification tasks, per <monospace specific-use="no-wrap">torchvision.datasets.ImageFolder</monospace>, tile labels are stored as directory names within a parent directory containing the slide or case name [<xref rid="pone.0289499.ref012" ref-type="bibr">12</xref>]. As SliDL has been designed to perform pre- and post-processing, training should be performed by using a separate, complementary library (see Related methods). During inference the inference step (Inference, SliDL is capable of accepting any PyTorch image model which has been trained, which includes both convolutional neural networks (CNNs) and vision transformers (ViTs).</p>
      <sec id="sec008">
        <title>Convolutional neural networks</title>
        <p>The first of the two main types of deep neural networks used for image data today, and the sort of deep neural network used to train the deep tissue detector (Deep tissue detector) is the convolutional neural network, which has shown outstanding performance on image classification tasks [<xref rid="pone.0289499.ref024" ref-type="bibr">24</xref>–<xref rid="pone.0289499.ref026" ref-type="bibr">26</xref>]. CNNs are a type of learning algorithm designed to take as input data which are spatially invariant (also known as “shift invariant”), a characteristic whereby small translations of the input data are tolerated. Since recognising common patterns in image data benefits from this trait, CNNs trained on images have proven to be highly successful and are one of the main deep learning architectures used in computer vision, including on tasks involving WSIs [<xref rid="pone.0289499.ref016" ref-type="bibr">16</xref>, <xref rid="pone.0289499.ref025" ref-type="bibr">25</xref>, <xref rid="pone.0289499.ref027" ref-type="bibr">27</xref>].</p>
        <p>Image-oriented CNNs work by including a set of two-dimensional convolution operations performed in a sweeping motion over the surface of the input image at each layer to transform it into an increasingly abstract representation (<xref rid="pone.0289499.g003" ref-type="fig">Fig 3</xref>). The weights in the filters (also known as the “kernels”) of each convolutional layer are what are used in these operations. The filter weights are learned during backpropagation, so that relevant aspects of the image discerned during training can be retained as the feature maps derived from the image are passed from one layer to the next. In between convolution layers are maximum pooling layers, which select the largest element within each receptive field to shrink the feature maps’ spatial resolution <xref rid="pone.0289499.g003" ref-type="fig">Fig 3</xref>. Through this reduction, pooling layers allow for the spatial invariance characteristic described above [<xref rid="pone.0289499.ref028" ref-type="bibr">28</xref>]. After the alternating convolutional and pooling layers are typically one or several fully-connected layers to coerce the number of features down to the desired class size (in the case of classification models) or down to the pixel mask size (in the case of segmentation models) [<xref rid="pone.0289499.ref028" ref-type="bibr">28</xref>].</p>
        <fig position="float" id="pone.0289499.g003">
          <object-id pub-id-type="doi">10.1371/journal.pone.0289499.g003</object-id>
          <label>Fig 3</label>
          <caption>
            <title>Structure of convolutional neural network and vision transformer.</title>
            <p><bold>(a)</bold> A VGG-like convolutional neural network with alternating convolutional layers and maximum pooling operations before a few fully connected layers at the end. Figure is based on the VGG paper [<xref rid="pone.0289499.ref029" ref-type="bibr">29</xref>] and images of this architecture made by others. <bold>(b)</bold> A vision transformer, based on the figure in the original ViT paper [<xref rid="pone.0289499.ref009" ref-type="bibr">9</xref>].</p>
          </caption>
          <graphic xlink:href="pone.0289499.g003" position="float"/>
        </fig>
        <p>CNNs have been the state of the art for classification and segmentatioon tasks on whole-slide images for several years [<xref rid="pone.0289499.ref001" ref-type="bibr">1</xref>].</p>
      </sec>
      <sec id="sec009">
        <title>Vision transformers</title>
        <p>Transformers are a newer neural network type which was originally designed for sequence-based problems such as natural language processing (NLP), which is the use of machine learning to interpret text [<xref rid="pone.0289499.ref008" ref-type="bibr">8</xref>]. Previously, NLP tasks were primarily performed with recurrent neural networks (RNNs), but a fundamental limitation of RNNs is that the way they process sequences precludes computation from being parallelised during training, thereby limiting the length of sequences that can be trained on [<xref rid="pone.0289499.ref008" ref-type="bibr">8</xref>]. Attention mechanisms began to be used in RNNs to allow longer distances between sequence dependencies to be learned [<xref rid="pone.0289499.ref030" ref-type="bibr">30</xref>, <xref rid="pone.0289499.ref031" ref-type="bibr">31</xref>]. Vaswani et al. [<xref rid="pone.0289499.ref008" ref-type="bibr">8</xref>] developed the first transformer architecture, which is composed entirely of attention mechanisms, entirely sidestepping the sequence dependency issue inherent to RNN-based models; transformers therefore tend to outperform and have functionally replaced RNNs [<xref rid="pone.0289499.ref008" ref-type="bibr">8</xref>, <xref rid="pone.0289499.ref032" ref-type="bibr">32</xref>].</p>
        <p>Dosovitskiy et al. [<xref rid="pone.0289499.ref009" ref-type="bibr">9</xref>] described the Vision Transformer (ViT), a transformer architecture modified to take image data as input. Although several CNNs had previously been designed to incorporate some attention mechanisms within them [<xref rid="pone.0289499.ref033" ref-type="bibr">33</xref>–<xref rid="pone.0289499.ref035" ref-type="bibr">35</xref>], this was the first successful architecture to train on images and use entirely attention mechanisms internally. This was achieved by splitting up inputted images into constituent pieces and treating these pieces (referred to as “tokens” in NLP language) as a sequence (<xref rid="pone.0289499.g003" ref-type="fig">Fig 3b</xref>). Their ViT model achieves state-of-the-art performance on several image classification benchmark datasets and has become a mainstay option for researchers looking for deep learning models trainable on images [<xref rid="pone.0289499.ref009" ref-type="bibr">9</xref>, <xref rid="pone.0289499.ref036" ref-type="bibr">36</xref>–<xref rid="pone.0289499.ref038" ref-type="bibr">38</xref>].</p>
        <p>Researchers have more recently begun applying vision transformers to whole-slide images and histopathological problems, but they form a promising new avenue [<xref rid="pone.0289499.ref011" ref-type="bibr">11</xref>].</p>
      </sec>
    </sec>
    <sec id="sec010">
      <title>Inference</title>
      <p>After a model has been trained, the next step is applying that model to the WSIs in a validation or test set to check the model’s performance. This application of a trained model to a new slide is known as inference. SliDL has two functions that infer a trained model on tiles with a sufficiently high tissue-probability (identified using the deep tissue detector) in a <monospace specific-use="no-wrap">Slide</monospace> object, saving the results into the tile dictionary internal to it: <monospace specific-use="no-wrap">Slide.inferClassifier()</monospace> and <monospace specific-use="no-wrap">Slide.inferSegmenter()</monospace>, which take as input a trained PyTorch model file. Using them is as simple as creating a <monospace specific-use="no-wrap">Slide</monospace> object for the slides one wants to infer on and then applying the function.</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 14</bold>. <bold>Infer a classification or a segmentation model on a Slide’s WSI</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
slidl_slide.inferClassifier(trainedModel=trained_classification_model,
                                classNames=['normal', 'tumor'],
                                dataTransforms=data_transforms,
                                tissueLevelThreshold=0.995,
                                foregroundLevelThreshold=88,
                                batchSize=30,
                                numWorkers=16)

slidl_slide.inferSegmenter(trainedModel=trained_segmentation_model,
                                classNames=['normal', 'tumor'],
                                dataTransforms=data_transforms,
                                tissueLevelThreshold=0.995,
                                foregroundLevelThreshold=88,
                                batchSize=30,
                                numWorkers=16)
</preformat>
      <p>After applying these functions, SliDL stores the predictions of the neural network for each tile in the tile dictionary internal to the <monospace specific-use="no-wrap">Slide</monospace> object.</p>
      <p>To ensure that the model is behaving as expected, it is important to visualise inference results by plotting the inference predictions of tiles spatially as they appear in the WSI. Once inference has been performed on the <monospace specific-use="no-wrap">Slide</monospace> objects, SliDL’s <monospace specific-use="no-wrap">Slide.visualizeClassifierInference()</monospace> and <monospace specific-use="no-wrap">Slide.visualizeSegmenterInference()</monospace> functions create these plots for the user overlaid on a low-resolution image of the WSI, taking the class and WSI pyramid level to visualise as arguments. The user is therefore able to qualitatively assess whether the regions highlighted by the model are as expected, given the ground-truth, and experiment with different training configurations, tile sizes, or other hyperparameters before proceeding if not (<xref rid="pone.0289499.g004" ref-type="fig">Fig 4</xref>).</p>
      <fig position="float" id="pone.0289499.g004">
        <object-id pub-id-type="doi">10.1371/journal.pone.0289499.g004</object-id>
        <label>Fig 4</label>
        <caption>
          <title>Visualising the inference of trained models.</title>
          <p><bold>(A)</bold> A plot of the inference of a trained tile-level classification model on three validation slides from the SliDL tutorial, showing the ability of the classification model to distinguish regions showing breast cancer metastasis from normal lymph node tissue. Plots were generated with Slide.visualizeClassifierInference(). <bold>(B)</bold> A plot of the inference of a trained tile-level segmentation model on three validation slides from the SliDL tutorial, showing the ability of the segmentation model to distinguish the same regions as the classification model. Plots were generated with Slide.visualizeSegmenterInference().</p>
        </caption>
        <graphic xlink:href="pone.0289499.g004" position="float"/>
      </fig>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 15</bold>. <bold>Visualise the inference of a classification or a segmentation model on a Slide’s WSI</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
slidl_slide.visualizeClassifierInference(classToVisualize='tumor',
                                            folder='path/to/folder',
                                            level=3)

slidl_slide.visualizeSegmenterInference(classToVisualize='tumor',
                                            folder='path/to/folder',
                                            level=3)
</preformat>
      <p>When performing segmentation, the user may want to create segmentation masks which are larger than the size of the tiles that were inferred on. When adjacent tiles overlap, tile-level segmentation masks cannot simply be concatenated. SliDL supports the automatic generation of whole-slide segmentation masks with the <monospace specific-use="no-wrap">Slide.getNonOverlappingSegmentationInferenceArray()</monospace> method, returning an inference array with the model’s pixel-level predictions and merging overlapping regions to return single predictions for each pixel. The class for which predictions are desired is specified by the <monospace specific-use="no-wrap">className</monospace> argument. Inference matrices are saved as compressed <monospace specific-use="no-wrap">.npz</monospace> files at the location specified by the <monospace specific-use="no-wrap">folder</monospace> argument. Users have the option of defining a threshold with the <monospace specific-use="no-wrap">probabilityTheshold</monospace> argument to return a binary output matrix, where pixels with a probability for the given class with a probability at or above the threshold are binarized to true, and the others to false. If the users does not define a <monospace specific-use="no-wrap">probabilityTheshold</monospace>, the raw probability value will be scaled to be between 0 and 255 (255 is 100% probability) and returned as a Numpy uint8 integer if the <monospace specific-use="no-wrap">dtype</monospace> argument is set to ‘int’ (the default). The user can also choose to have the probabilities returned as Numpy float32 floats if <monospace specific-use="no-wrap">dtype</monospace> is set to ‘float’, but this is usually undesirable as it results in extremely memory-intensive matrices.</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 16</bold>. <bold>Save the segmentation matrix from an inference on a Slide’s WSI</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
slidl_slide.getNonOverlappingSegmentationInferenceArray(
                                        className='tumor',
                                        dtype='int'
                                        folder='path/to/folder')
</preformat>
    </sec>
    <sec id="sec011">
      <title>Evaluating model performance</title>
      <p>Beyond visual verification, numerical methods are requires to assess model performance. SliDL includes a number of wrapper functions around common performance metrics to enable easy model evaluation. After inferring using a trained model, the <monospace specific-use="no-wrap">Slide.classifierMetricAtThreshold()</monospace> and <monospace specific-use="no-wrap">Slide.segmenterMetricAtThreshold()</monospace> methods can be applied. By providing a list of probability thresholds to the <monospace specific-use="no-wrap">probabilityThresholds</monospace> argument, a metric (‘accuracy’, ‘balanced_accuracy’, ‘f1’, ‘precision’, or ‘recall’ for <monospace specific-use="no-wrap">Slide.classifierMetricAtThreshold()</monospace>, and ‘dice_coeff’ for <monospace specific-use="no-wrap">segmenterMetricAtThreshold()</monospace>), and a class to the <monospace specific-use="no-wrap">classToThreshold</monospace> argument, these methods will calculate the corresponding metric at each probability threshold for the given class.</p>
      <p>For <monospace specific-use="no-wrap">Slide.classifierMetricAtThreshold()</monospace>, the <monospace specific-use="no-wrap">tileAnnotationOverlapThreshold</monospace> argument defines the minimum fraction of a tile’s area that must be covered by an annotation from the <monospace specific-use="no-wrap">classToThreshold</monospace> for that tile to be considered ground-truth positive for that class (default is 0.5).</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 17</bold>. <bold>Compute the classification or segmentation accuracy of an inference on a Slide’s WSI</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
classification_accuracies = slidl_slide.classifierMetricAtThreshold(
                            classToThreshold='tumor',
                            probabilityThresholds=probability_thresholds,
                            tileAnnotationOverlapThreshold=0.3,
                            metric='accuracy')

segmentation_accuracies = slidl_slide.segmenterMetricAtThreshold(
                            classToThreshold='tumor',
                            probabilityThresholds=probability_thresholds)
</preformat>
      <p>The threshold that gives best performance on the validation set can then be applied to the test set with the same two functions; inputting the single best threshold in the <monospace specific-use="no-wrap">probablityThresholds</monospace> argument.</p>
      <p>Although inference is performed on the tile-level, when applied in the clinic a label for the entire slide is often required. Similarly, during training, it is frequently the case that the test set has only a slide-level label. A method is therefore needed to translate tile-level predictions to slide-level labels. One approach is to determine a threshold for the number of tiles which need to be called positive, for a given class at a given probability threshold, in order to call an entire slide positive. To test the performance of a model on data with only slide-level labels, the AUC for a ROC curve can be computed for a range of positive-tile counts. The optimum count can then be used when applying the best model to unlabelled data in the clinic. In order to make such analyses straightforward, SliDL includes the <monospace specific-use="no-wrap">Slide.numTilesAboveClassPredictionThreshold()</monospace> method which returns the number of tiles in a slide whose inference prediction probabilities for a given class (<monospace specific-use="no-wrap">classToThreshold</monospace>) are greater than or equal to given probability thresholds (provided in a list to the <monospace specific-use="no-wrap">probablityThresholds</monospace> argument).</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 18</bold>. <bold>Count the number of tiles in a Slide whose inference value exceeds a certain threshold</bold></p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
slidl_slide.numTilesAboveClassPredictionThreshold(
                            classToThreshold='tumor',
                            probabilityThresholds=probability_thresholds)
</preformat>
      <p>These positive-tile counts per slide can then be used to calculate an AUROC. As above, often a range of probability thresholds are tried, and the one which yields the largest AUROC on the validation set is then applied to the test set to give the final performance.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec012">
    <title>Discussion</title>
    <p>SliDL provides an easily-installable (see Installation) Python library with a straightforward API for users seeking to perform tasks involving artefact and background detection, tile extraction from annotation, and model training and inference on WSIs. It is intended for users with basic Python knowledge; it does not demand extensive experience with WSI data or pathology. We see this as one of SliDL’s key advantages over existing methods which expect more rigorous backgrounds in histopathology or image analysis. To best understand the unique value provided by SliDL, one must compare it to existing computation methods in the WSI space.</p>
    <sec id="sec013">
      <title>Related methods</title>
      <p>Several tools have been developed which also provide support for some of the functionalities available in SliDL. While providing a complete description and comparison of these tools is beyond the scope of this manuscript, here we briefly describe these tools and direct the reader towards the articles presenting them for more details (see <xref rid="pone.0289499.s004" ref-type="supplementary-material">S3 Table</xref> for a summary of all of the comparisons made with all the methods discussed below).</p>
      <p>HistoQC [<xref rid="pone.0289499.ref039" ref-type="bibr">39</xref>] is a Python-based tool for performing quality control of WSIs, aiding users in the identification slides containing potential technical artefacts and affected by batch effects. By providing the user with modules for performing a wide range of classical image analysis techniques, HistoQC enables the construction of custom pipelines for performing foreground filtering, detection of slide artefacts such as pen marks, and identification of batch effects such as slides with darker staining compared to the rest. In HistoQC, this is achieved using a combination of approaches including inspection of colour distributions, application of edge and smoothness detectors, and classical filters such as Gabor and Frangi filters for texture analysis. For example, if the background of a WSI is uniformly white, foreground filtering can be performed by applying a threshold to the colour distribution which excludes white pixels. Similarly, a bright green pen mark may be clearly distinguishable from tissue by inspection of the green colour distribution of the WSI. In addition, HistoQC provides an interactive user interface for exploring one’s data. These approaches can achieve competitive results when carefully tuned by the user, but may struggle in more complex cases, such as uneven background, and pen marks with similar colour to the tissue. HistoQC is therefore a useful tool, complementing the wider functionality and robustness of SliDL, and enabling rapid quality control processing of one’s data.</p>
      <p>HistomicsTK [<xref rid="pone.0289499.ref040" ref-type="bibr">40</xref>] is a Python library for performing a number of image analysis tasks specific to WSIs including stain colour deconvolution, normalisation and augmentation, as well as cell/nuclei segmentation and even a user interface for manual annotation of WSIs. Like HistoQC, all image analysis techniques are performed using classical approaches. HistomicsTK is highly complementary to SliDL, and in particular, we envisage that users may make use of HistomicsTK for performing WSI-specific colour augmentations within a SliDL workflow.</p>
      <p>Histolab [<xref rid="pone.0289499.ref041" ref-type="bibr">41</xref>] is a Python library combining features found both in HistoQC and HistomicsTK, including functions for performing classical image analysis techniques to facilitate tissue detection and artefact removal, cell/nuclei segmentation, and colour transformations such as colour deconvolution. In addition, Histolab, like SliDL, supports the extracting of tiles from WSIs, and enables one to easily test alternative tiling strategies, including random extraction of tiles according to tissue detection score thresholds.</p>
      <p>MONAI [<xref rid="pone.0289499.ref042" ref-type="bibr">42</xref>] is an extensive Python library which is part of the PyTorch ecosystem and is designed as a unified framework for performing deep learning on medical imaging data. Like SliDL, MONAI supports tiling of WSIs and provides extensive support model evaluation metrics. In addition, MONAI provides domain-specific support for data augmentation transforms, implementations of neural network architectures, optimisers, loss functions, and AI-assisted annotation, all of which are tuned for application to medical imaging data.</p>
      <p>PathML [<xref rid="pone.0289499.ref043" ref-type="bibr">43</xref>] is a Python library which also supports the tiling of WSIs, and similar to Histolab, HistomicsTK and HistoQC implements an number of classical approaches to foreground and artefact detection. Similarly to MONAI and HistomicsTK, PathML also supports some pre-processing methods such as stain normalisation data augmentation, but is not designed to perform any post-processing steps.</p>
    </sec>
    <sec id="sec014">
      <title>Distinct advantages of SliDL</title>
      <p>Part of the advantage of SliDL is the ease with which it can be learned and used to render complicated digital histopathological techniques and problems immediately accessible to researchers, turning what would be weeks of work and files full of code into one straightforward, linear workflow consisting of just a few lines.</p>
      <p>In addition to these advantages, the SliDL toolbox we present here improves on the other tools mentioned above in several ways, most importantly the handling of digital annotations. None of the tools listed above implement deep tissue detectors, nor do they implement tools for handling of annotations for WSIs to facilitate labelling of tiles, automatic resolution of annotation conflicts, or generation of binary and multi-class tile-level segmentation masks. Furthermore, while all of these tools provide support for the pre-processing of WSIs, only MONAI provides tools for model evaluation, and none support post-processing tasks such as the stitching together of tile-level segmentation masks to produce a slide-level mask. Tools such as MONAI are therefore highly complementary to SliDL: we envisage users, for example, making use of SliDL for pre-processing and handling of annotations, and MONAI for data augmentation and implementation of neural network architectures for training.</p>
    </sec>
    <sec id="sec015">
      <title>Future work</title>
      <p>Despite its many advantages, SliDL can be expanded upon and improved in various ways. For example, it might be useful to eventually provide a graphical user interface (GUI) to SliDL so that some of its basic features are accessible to users with less of a computer science background. This might include sliders, toggles, and text boxes in place of programmatic functions with arguments.</p>
      <p>Furthermore, currently, applying the deep tissue detector with any efficiency to WSIs requires a computer vision-grade GPU, ideally with at least 4 gigabytes of memory and a GPU clock of at least 1200 megahertz. This feature will therefore be slow for users working on most standard commercial laptop or desktop models. For this reason, it is worth exploring alternative methods of implementing SliDL’s deep tissue detector which are compatible with lower-specification hardware. One potential solution is to re-train the deep tissue detector model on a deep neural network with fewer internal parameters, such as MnasNet [<xref rid="pone.0289499.ref044" ref-type="bibr">44</xref>], SqueezeNet [<xref rid="pone.0289499.ref045" ref-type="bibr">45</xref>], or MobileNet [<xref rid="pone.0289499.ref046" ref-type="bibr">46</xref>].</p>
      <p>Finally, there remain many potential features to add to SliDL to expand its breadth. We would like to include a function to automatically generate tile annotations files demarcating the highest probability tiles for a certain class identified during inference. We are also interested in including functions so that SliDL can generate of saliency maps of trained deep learning models [<xref rid="pone.0289499.ref047" ref-type="bibr">47</xref>].</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec016">
    <title>Conclusion</title>
    <p>SliDL is a new and fully-functional new tool for computer scientists looking for a straightforward but powerful Python library for solving some of the most commonly faced and nettlesome problems for training and evaluating deep learning models on WSI data quickly and easily. SliDL also includes a fully illustrative tutorial Jupyter notebook on a real-world example problem on a publicly available dataset from beginning (removing background and slide artefacts, extracting tiles from annotations) to end (training a deep learning model, inferring it on new slides, and evaluating model performance); the tutorial can be found here: <ext-link xlink:href="https://slidl.readthedocs.io/en/latest/" ext-link-type="uri">https://slidl.readthedocs.io/en/latest/</ext-link>. It is our hope and goal that with SliDL and its corresponding tutorial, the application of deep learning to whole-slide images becomes more accessible not just to researchers already involved in digital pathology, but to newcomers as well, making the field on the whole more approachable.</p>
  </sec>
  <sec id="sec017">
    <title>Availability and requirements</title>
    <p><bold>Project name</bold>: SliDL</p>
    <p><bold>Project home page</bold>: <ext-link xlink:href="https://github.com/markowetzlab/slidl" ext-link-type="uri">https://github.com/markowetzlab/slidl</ext-link></p>
    <p><bold>Project tutorial home page</bold>: <ext-link xlink:href="https://github.com/markowetzlab/slidl-tutorial" ext-link-type="uri">https://github.com/markowetzlab/slidl-tutorial</ext-link></p>
    <p><bold>Project documentation home page</bold>: <ext-link xlink:href="https://slidl.readthedocs.io/en/latest/" ext-link-type="uri">https://slidl.readthedocs.io/en/latest/</ext-link></p>
    <p><bold>Operating system(s)</bold>: Not applicable</p>
    <p><bold>Programming language</bold>: Python (version 3.7 or above)</p>
    <p><bold>Other requirements</bold>: Not applicable</p>
    <p><bold>License</bold>: GPL-3.0</p>
    <p><bold>Any restrictions to use by non-academics</bold>: Not applicable</p>
  </sec>
  <sec id="sec018">
    <title>Materials and installation</title>
    <sec id="sec019">
      <title>Benchmarking</title>
      <p>Here is a full breakdown of the slides used for the benchmarking analysis of the deep tissue detector (see Deep tissue detector):</p>
      <list list-type="bullet">
        <list-item>
          <p>36 slides</p>
        </list-item>
        <list-item>
          <p>6 TCGA-TGCT (testicular germ cell tumor) H&amp;E</p>
        </list-item>
        <list-item>
          <p>6 CAMELYON-16 (tiny breast cancer metastases in lymph nodes, including slides with and without metastases present) H&amp;E slides</p>
        </list-item>
        <list-item>
          <p>6 OCCAMS (esophageal adenocarcinoma) H&amp;E slides from esophago-gastro-duodenoscopy</p>
        </list-item>
        <list-item>
          <p>6 BEST2 (cytosponge samples some with Barett’s) P53 and H&amp;E slides</p>
        </list-item>
        <list-item>
          <p>6 TCGA-PRAD (prostate adenocarcinoma) H&amp;E slides</p>
        </list-item>
        <list-item>
          <p>2 TCGA floor of mouth cancer H&amp;E slides</p>
        </list-item>
        <list-item>
          <p>1 TCGA small intestine cancer H&amp;E slide</p>
        </list-item>
        <list-item>
          <p>1 TCGA gum cancer H&amp;E slide</p>
        </list-item>
        <list-item>
          <p>1 TCGA “spinal cord, cranial nerves, and other unspecified parts of central nervous system” cancer H&amp;E slide</p>
        </list-item>
        <list-item>
          <p>1 TCGA tonsil cancer H&amp;E slide</p>
        </list-item>
        <list-item>
          <p>Mixture of samples preserved via FFPE (formalin fixed paraffin embedded) and those preserved via flash freezing</p>
        </list-item>
        <list-item>
          <p>Different scanning file types, scanning machines, times of scanning, scanning locations, stain intensities, stains (H&amp;E and P53), tissue removal methods/surgery types, and tissue types</p>
        </list-item>
        <list-item>
          <p>Slide artefacts included ink of many different colors (including blue, red, black, and reen—some used to mark regions on top of tissue, other times to write labels on a background portion of a slide), slide bubbles, tissue folding/burning, cloudiness or yellowing, slide shifted in scanning machine to leave black region in image, slide edge artefacts, dirt and debris under the slide, black slide crosses, blurry regions</p>
        </list-item>
        <list-item>
          <p>Slides exhibited a wide range of artefact degree and amount—some slides had virtually no artefact whatsoever, others had a few little ink dots or bubbles, still others had large black regions and/or regions of ink etc.</p>
        </list-item>
        <list-item>
          <p>1,489,084 tiles in total across all 36 slides with a massive range in the size of each slide (smallest slide: 47MB, largest slide: 3.9GB / fewest tiles in one slide: 3,304, most: 86,190)</p>
        </list-item>
        <list-item>
          <p>730 individual annotations including tissue and doughnut hole annotations</p>
        </list-item>
        <list-item>
          <p>Slides all scanned in 40x with 500px edge length tiles</p>
        </list-item>
      </list>
      <p>All 36 WSIs used to benchmark the deep tissue detector are available at <ext-link xlink:href="https://doi.org/10.5281/zenodo.7947380" ext-link-type="uri">https://doi.org/10.5281/zenodo.7947380</ext-link>.</p>
    </sec>
    <sec id="sec020">
      <title>Hardware</title>
      <p>It is recommended that SliDL users either work on a machine with at least 32 GB of RAM and enough disk space to hold the number of number of tiles they would like to extract (tiles are not large, but if thousands are extracted, a proportional amount of disk space is required. At least four cores are recommended. WSIs tend to be 0.5–5 GB in size, so if tens or hundreds are used in an analysis, disk space to store them is required (external hard drives work well). Users that wish to utilise the inference functions of <monospace specific-use="no-wrap">Slide</monospace>, <monospace specific-use="no-wrap">Slide.inferClassifier()</monospace> and <monospace specific-use="no-wrap">Slide.inferSegmenter()</monospace> are highly recommended to have a CUDA-compatible Graphics Processing Unit (GPU). SliDL also works well in high performance computing environments that meet these conditions.</p>
    </sec>
    <sec id="sec021">
      <title>Installation</title>
      <p>SliDL is available on the Python Package Index (PyPI) for easy installation:</p>
      <list list-type="simple">
        <list-item>
          <p><bold>Code Listing 19</bold>. Install SliDL</p>
        </list-item>
      </list>
      <preformat position="float" xml:space="preserve">
pip install slidl
</preformat>
    </sec>
    <sec id="sec022">
      <title>Troubleshooting</title>
      <p>There are several mistakes and error messages that can arise when using SliDL. Troubleshooting presents the most common mistakes users might run into, including the error message output by SliDL, the possible reason for the mistake, and the possible solution to it (<xref rid="pone.0289499.t001" ref-type="table">Table 1</xref>).</p>
      <table-wrap position="float" id="pone.0289499.t001">
        <object-id pub-id-type="doi">10.1371/journal.pone.0289499.t001</object-id>
        <label>Table 1</label>
        <caption>
          <title>Common SliDL error messages with explanations and possible solutions.</title>
          <p><italic toggle="yes">X</italic>, <italic toggle="yes">Y</italic>, and <italic toggle="yes">Z</italic> represent numbers or words that will vary depending on the exact error made by the user.</p>
        </caption>
        <alternatives>
          <graphic xlink:href="pone.0289499.t001" id="pone.0289499.t001g" position="float"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="left" rowspan="1" colspan="1">Message</th>
                <th align="left" rowspan="1" colspan="1">Possible reason</th>
                <th align="left" rowspan="1" colspan="1">Solution</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘This image is not compatible. Please refer to the documentation for proper installation of openslide and libvips’</td>
                <td align="left" rowspan="1" colspan="1">The WSI input into the SliDL initialiser was not a supported file format</td>
                <td align="left" rowspan="1" colspan="1">Convert the WSI to a file format supported by lipvips.</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘Tissue detection has already been performed. Use overwriteExistingTissueDetection if you wish to write over it’</td>
                <td align="left" rowspan="1" colspan="1">Slide.detectTissue() has already been called on the Slide object</td>
                <td align="left" rowspan="1" colspan="1">Set the overwriteExistingTissueDetection argument to True, or else don’t perform tissue detection again</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘Annotation with centroid (<italic toggle="yes">X</italic>, <italic toggle="yes">Y</italic>) produces a Shapely <italic toggle="yes">Z</italic> instead of a polygon; check to see if it self-intersects.’</td>
                <td align="left" rowspan="1" colspan="1">The annotation around the specified centroid pixel coordinates of the WSI does not produce a polygon when geometrically parsed</td>
                <td align="left" rowspan="1" colspan="1">Check that annotation on the WSI in a WSI viewer looking for self-overlapping regions and correct it to be a polygon</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘Warning: <italic toggle="yes">X</italic> suitable <italic toggle="yes">Y</italic> tiles found but requested <italic toggle="yes">Z</italic> tiles to extract. Extracting all suitable tiles…’</td>
                <td align="left" rowspan="1" colspan="1">The numTilesToExtractPerClass argument of a tile extraction function exceeds the number of suitable tiles of class <italic toggle="yes">Y</italic></td>
                <td align="left" rowspan="1" colspan="1">Reduce the numTilesToExtractPerClass argument for class <italic toggle="yes">Y</italic>, or else let SliDL will extract all available <italic toggle="yes">Y</italic> tiles by default</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘Model has <italic toggle="yes">X</italic> classes but only Y class names were provided in the classes argument’</td>
                <td align="left" rowspan="1" colspan="1">The number of classes output by the model inputted to Slide.inferClassifier() or Slide.inferSegmenter() does not equal the number of classes present in the classNames argument</td>
                <td align="left" rowspan="1" colspan="1">Verify that classNames includes all the classes that were trained on, and correct this argument as necessary</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘No predictions found in slide. Use inferClassifier() / inferSegmenter() to generate them.’</td>
                <td align="left" rowspan="1" colspan="1">Using a SliDL function reliant on inference results without having added inference results to the Slide object</td>
                <td align="left" rowspan="1" colspan="1">Run Slide.inferClassifier or Slide.inferSegmenter() on the Slide object</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
    </sec>
  </sec>
  <sec id="sec023" sec-type="supplementary-material">
    <title>Supporting information</title>
    <supplementary-material id="pone.0289499.s001" position="float" content-type="local-data">
      <label>S1 Fig</label>
      <caption>
        <title>Directory structure.</title>
        <p>The directory structure output by <monospace specific-use="no-wrap">Slide.extractAnnotationTiles()</monospace> and <monospace specific-use="no-wrap">Slide.extractRandomUnannotatedTiles()</monospace> which is amenable to PyTorch’s ImageFolder dataset.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pone.0289499.s001.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pone.0289499.s002" position="float" content-type="local-data">
      <label>S1 Table</label>
      <caption>
        <title>Table summarising the primary functions of SliDL.</title>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pone.0289499.s002.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pone.0289499.s003" position="float" content-type="local-data">
      <label>S2 Table</label>
      <caption>
        <title>Table describing the SliDL functions discussed in the main text.</title>
        <p>Functions are listed in green, a summary of their purpose in blue, their arguments in yellow and the description of the arguments in white. For a complete description of all SliDL functions and their arguments, see <ext-link xlink:href="https://slidl.readthedocs.io/" ext-link-type="uri">https://slidl.readthedocs.io/</ext-link>.</p>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0289499.s003.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pone.0289499.s004" position="float" content-type="local-data">
      <label>S3 Table</label>
      <caption>
        <title>Table of comparisons to related methods.</title>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0289499.s004.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack>
    <p>We would like to thank Sarah Killcoyne and Winifred Taylor-Williams for their early testing of SliDL.</p>
  </ack>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>
API
</term>
        <def>
          <p>Application Public Interface</p>
        </def>
      </def-item>
      <def-item>
        <term>
BEST2
</term>
        <def>
          <p>Barrett’s oEsophagus Screening Trial 2 [<xref rid="pone.0289499.ref048" ref-type="bibr">48</xref>]</p>
        </def>
      </def-item>
      <def-item>
        <term>
OCCAMS
</term>
        <def>
          <p>Oesophageal Cancer Clinical and Molecular Stratification [<xref rid="pone.0289499.ref049" ref-type="bibr">49</xref>]</p>
        </def>
      </def-item>
      <def-item>
        <term>
TCGA
</term>
        <def>
          <p>The Cancer Genome Atlas [<xref rid="pone.0289499.ref050" ref-type="bibr">50</xref>]</p>
        </def>
      </def-item>
      <def-item>
        <term>
WSI
</term>
        <def>
          <p>Whole-Slide Image</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <ref-list>
    <title>References</title>
    <ref id="pone.0289499.ref001">
      <label>1</label>
      <mixed-citation publication-type="journal"><name><surname>Dimitriou</surname><given-names>N</given-names></name>, <name><surname>Arandjelović</surname><given-names>O</given-names></name>, <name><surname>Caie</surname><given-names>PD</given-names></name>. <article-title>Deep Learning for Whole Slide Image Analysis: An Overview</article-title>. <source>Frontiers in medicine</source>. <year>2019</year>;<volume>6</volume>:<fpage>264</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3389/fmed.2019.00264</pub-id><?supplied-pmid 31824952?><pub-id pub-id-type="pmid">31824952</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref002">
      <label>2</label>
      <mixed-citation publication-type="journal"><name><surname>Kather</surname><given-names>JN</given-names></name>, <name><surname>Heij</surname><given-names>LR</given-names></name>, <name><surname>Grabsch</surname><given-names>HI</given-names></name>, <name><surname>Loeffler</surname><given-names>C</given-names></name>, <name><surname>Echle</surname><given-names>A</given-names></name>, <name><surname>Muti</surname><given-names>HS</given-names></name>, <etal>et al</etal>. <article-title>Pan-cancer image-based detection of clinically actionable genetic alterations</article-title>. <source>Nature cancer</source>. <year>2020</year>;<volume>1</volume>(<issue>8</issue>):<fpage>789</fpage>–<lpage>799</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s43018-020-0087-6</pub-id><?supplied-pmid 33763651?><pub-id pub-id-type="pmid">33763651</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref003">
      <label>3</label>
      <mixed-citation publication-type="journal"><name><surname>Lu</surname><given-names>MY</given-names></name>, <name><surname>Chen</surname><given-names>TY</given-names></name>, <name><surname>Mahmood</surname><given-names>F</given-names></name>, <etal>et al</etal>. <article-title>AI-based pathology predicts origins for cancers of unknown primary</article-title>. <source>Nature</source>. <year>2021</year>;<volume>594</volume>:<fpage>106</fpage>–<lpage>110</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41586-021-03512-4</pub-id><?supplied-pmid 33953404?><pub-id pub-id-type="pmid">33953404</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref004">
      <label>4</label>
      <mixed-citation publication-type="journal"><name><surname>Montgomery</surname><given-names>E</given-names></name>. <article-title>Is there a way for pathologists to decrease interobserver variability in the diagnosis of dysplasia?</article-title><source>Archives of pathology &amp; laboratory medicine</source>. <year>2005</year>;<volume>129</volume>(<issue>2</issue>):<fpage>174</fpage>–<lpage>176</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.5858/2005-129-174-ITAWFP</pub-id><pub-id pub-id-type="pmid">15679414</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref005">
      <label>5</label>
      <mixed-citation publication-type="journal"><name><surname>Raab</surname><given-names>SS</given-names></name>, <name><surname>Grzybicki</surname><given-names>DM</given-names></name>. <source>Anatomic pathology workload and error</source>; <year>2006</year>.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref006">
      <label>6</label>
      <mixed-citation publication-type="journal"><name><surname>Goodfellow</surname><given-names>I</given-names></name>, <name><surname>Pouget-Abadie</surname><given-names>J</given-names></name>, <name><surname>Mirza</surname><given-names>M</given-names></name>, <name><surname>Xu</surname><given-names>B</given-names></name>, <name><surname>Warde-Farley</surname><given-names>D</given-names></name>, <name><surname>Ozair</surname><given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Generative adversarial networks</article-title>. <source>Communications of the ACM</source>. <year>2020</year>;<volume>63</volume>(<issue>11</issue>):<fpage>139</fpage>–<lpage>144</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1145/3422622</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref007">
      <label>7</label>
      <mixed-citation publication-type="journal"><name><surname>Prakash</surname><given-names>CD</given-names></name>, <name><surname>Karam</surname><given-names>LJ</given-names></name>. <article-title>It GAN DO better: GAN-based detection of objects on images with varying quality</article-title>. <source>IEEE Transactions on Image Processing</source>. <year>2021</year>;<volume>30</volume>:<fpage>9220</fpage>–<lpage>9230</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TIP.2021.3124155</pub-id><?supplied-pmid 34735343?><pub-id pub-id-type="pmid">34735343</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref008">
      <label>8</label>
      <mixed-citation publication-type="journal"><name><surname>Vaswani</surname><given-names>A</given-names></name>, <name><surname>Shazeer</surname><given-names>N</given-names></name>, <name><surname>Parmar</surname><given-names>N</given-names></name>, <name><surname>Uszkoreit</surname><given-names>J</given-names></name>, <name><surname>Jones</surname><given-names>L</given-names></name>, <name><surname>Gomez</surname><given-names>AN</given-names></name>, <etal>et al</etal>. <article-title>Attention is all you need</article-title>. <source>Advances in neural information processing systems</source>. <year>2017</year>;<volume>30</volume>.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref009">
      <label>9</label>
      <mixed-citation publication-type="other">Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:201011929. 2020.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref010">
      <label>10</label>
      <mixed-citation publication-type="journal"><name><surname>Jose</surname><given-names>L</given-names></name>, <name><surname>Liu</surname><given-names>S</given-names></name>, <name><surname>Russo</surname><given-names>C</given-names></name>, <name><surname>Nadort</surname><given-names>A</given-names></name>, <name><surname>Di Ieva</surname><given-names>A</given-names></name>. <article-title>Generative adversarial networks in digital pathology and histopathological image processing: A review</article-title>. <source>Journal of Pathology Informatics</source>. <year>2021</year>;<volume>12</volume>(<issue>1</issue>):<fpage>43</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.4103/jpi.jpi_103_20</pub-id><?supplied-pmid 34881098?><pub-id pub-id-type="pmid">34881098</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref011">
      <label>11</label>
      <mixed-citation publication-type="journal"><name><surname>Zheng</surname><given-names>Y</given-names></name>, <name><surname>Gindra</surname><given-names>RH</given-names></name>, <name><surname>Green</surname><given-names>EJ</given-names></name>, <name><surname>Burks</surname><given-names>EJ</given-names></name>, <name><surname>Betke</surname><given-names>M</given-names></name>, <name><surname>Beane</surname><given-names>JE</given-names></name>, <etal>et al</etal>. <article-title>A graph-transformer for whole slide image classification</article-title>. <source>IEEE transactions on medical imaging</source>. <year>2022</year>;<volume>41</volume>(<issue>11</issue>):<fpage>3003</fpage>–<lpage>3015</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TMI.2022.3176598</pub-id><?supplied-pmid 35594209?><pub-id pub-id-type="pmid">35594209</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref012">
      <label>12</label>
      <mixed-citation publication-type="other">Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, et al. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In: Advances in Neural Information Processing Systems 32. Curran Associates, Inc.; 2019. p. 8024–8035. Available from: <ext-link xlink:href="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" ext-link-type="uri">http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref013">
      <label>13</label>
      <mixed-citation publication-type="journal"><name><surname>Litjens</surname><given-names>G</given-names></name>, <name><surname>Bandi</surname><given-names>P</given-names></name>, <name><surname>Bejnordi</surname><given-names>BE</given-names></name>, <name><surname>Van der Laak</surname><given-names>J</given-names></name>. <article-title>1399 H&amp;E-stained sentinel lymph node sections of breast cancer patients: the CAMELYON dataset</article-title>. <source>GigaScience</source>. <year>2018</year>;<volume>7</volume>(<issue>6</issue>). <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/gigascience/giy065</pub-id><?supplied-pmid 29860392?><pub-id pub-id-type="pmid">29860392</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref014">
      <label>14</label>
      <mixed-citation publication-type="other">Medixant. RadiAnt DICOM Viewer; 2021. Available from: <ext-link xlink:href="https://www.radiantviewer.com" ext-link-type="uri">https://www.radiantviewer.com</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref015">
      <label>15</label>
      <mixed-citation publication-type="other">Martinez K, Cupitt J. VIPS—a highly tuned image processing software architecture. In: Proceedings of IEEE International Conference on Image Processing; 2005. p. 574–577.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref016">
      <label>16</label>
      <mixed-citation publication-type="journal"><name><surname>Janowczyk</surname><given-names>A</given-names></name>, <name><surname>Madabhushi</surname><given-names>A</given-names></name>. <article-title>Deep learning for digital pathology image analysis: A comprehensive tutorial with selected use cases</article-title>. <source>J Pathol Inform</source>. <year>2016</year>;<volume>7</volume>(<issue>29</issue>). <comment>doi: </comment><pub-id pub-id-type="doi">10.4103/2153-3539.186902</pub-id><?supplied-pmid 27563488?><pub-id pub-id-type="pmid">27563488</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref017">
      <label>17</label>
      <mixed-citation publication-type="other">Huang G, Liu Z, Van Der Maaten L, Weinberger KQ. Densely connected convolutional networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2017. p. 4700–4708.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref018">
      <label>18</label>
      <mixed-citation publication-type="other">Yang M. A PyTorch imbalanced dataset sampler for oversampling low frequent classes and undersampling high frequent ones; 2018–2020. <ext-link xlink:href="https://github.com/ufoym/imbalanced-dataset-sampler" ext-link-type="uri">https://github.com/ufoym/imbalanced-dataset-sampler</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref019">
      <label>19</label>
      <mixed-citation publication-type="journal"><name><surname>Otsu</surname><given-names>N</given-names></name>. <article-title>A threshold selection method from gray-level histograms</article-title>. <source>IEEE transactions on systems, man, and cybernetics</source>. <year>1979</year>;<volume>9</volume>(<issue>1</issue>):<fpage>62</fpage>–<lpage>66</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TSMC.1979.4310076</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref020">
      <label>20</label>
      <mixed-citation publication-type="journal"><name><surname>Zack</surname><given-names>GW</given-names></name>, <name><surname>Rogers</surname><given-names>WE</given-names></name>, <name><surname>Latt</surname><given-names>SA</given-names></name>. <article-title>Automatic measurement of sister chromatid exchange frequency</article-title>. <source>J Histochem Cytochem</source>. <year>1977</year>;<volume>25</volume>(<issue>7</issue>):<fpage>741</fpage>–<lpage>753</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1177/25.7.70454</pub-id><?supplied-pmid 70454?><pub-id pub-id-type="pmid">70454</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref021">
      <label>21</label>
      <mixed-citation publication-type="journal"><name><surname>Bankhead</surname><given-names>P</given-names></name>, <name><surname>Loughrey</surname><given-names>MB</given-names></name>, <name><surname>Fernández</surname><given-names>JA</given-names></name>. <article-title>QuPath: Open source software for digital pathology image analysis</article-title>. <source>Sci Rep</source>. <year>2017</year>;<volume>7</volume>:<fpage>16878</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41598-017-17204-5</pub-id><?supplied-pmid 29203879?><pub-id pub-id-type="pmid">29203879</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref022">
      <label>22</label>
      <mixed-citation publication-type="other">Computation Pathology Group, part of the Diagnostic Image Analysis Group, at the Radboud University Medical Center. The Automated Slide Analysis Platform (ASAP); 2018. Available from: <ext-link xlink:href="https://github.com/computationalpathologygroup/ASAP" ext-link-type="uri">https://github.com/computationalpathologygroup/ASAP</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref023">
      <label>23</label>
      <mixed-citation publication-type="other">Chamberlain S, Ooms J. geojson: Classes for ‘GeoJSON’; 2023.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref024">
      <label>24</label>
      <mixed-citation publication-type="journal"><name><surname>LeCun</surname><given-names>Y</given-names></name>, <name><surname>Boser</surname><given-names>B</given-names></name>, <name><surname>Denker</surname><given-names>JS</given-names></name>, <name><surname>Henderson</surname><given-names>D</given-names></name>, <name><surname>Howard</surname><given-names>RE</given-names></name>, <name><surname>Hubbard</surname><given-names>W</given-names></name>, <etal>et al</etal>. <article-title>Backpropagation applied to handwritten zip code recognition</article-title>. <source>Neural computation</source>. <year>1989</year>;<volume>1</volume>(<issue>4</issue>):<fpage>541</fpage>–<lpage>551</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1162/neco.1989.1.4.541</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref025">
      <label>25</label>
      <mixed-citation publication-type="other">LeCun Y, Bottou L, Bengio Y, Haffner P. Gradient-based learning applied to document recognition. In: Proceedings of the IEEE; 1998. p. 2278–2324.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref026">
      <label>26</label>
      <mixed-citation publication-type="journal"><name><surname>Miotto</surname><given-names>R</given-names></name>, <name><surname>Wang</surname><given-names>F</given-names></name>, <name><surname>Wang</surname><given-names>S</given-names></name>, <name><surname>Jiang</surname><given-names>X</given-names></name>, <name><surname>Dudley</surname><given-names>JT</given-names></name>. <article-title>Deep learning for healthcare: review, opportunities and challenges</article-title>. <source>Briefings in bioinformatics</source>. <year>2018</year>;<volume>19</volume>(<issue>6</issue>):<fpage>1236</fpage>–<lpage>1246</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bib/bbx044</pub-id><?supplied-pmid 28481991?><pub-id pub-id-type="pmid">28481991</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref027">
      <label>27</label>
      <mixed-citation publication-type="book"><name><surname>Krizhevsky</surname><given-names>A</given-names></name>, <name><surname>Sutskever</surname><given-names>I</given-names></name>, <name><surname>Hinton</surname><given-names>GE</given-names></name>. <part-title>Imagenet classification with deep convolutional neural networks</part-title>. In: <source>Advances in Neural Information Processing Systems</source>; <year>2012</year>. p. <fpage>1097</fpage>–<lpage>1105</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref028">
      <label>28</label>
      <mixed-citation publication-type="journal"><name><surname>Rawat</surname><given-names>W</given-names></name>, <name><surname>Wang</surname><given-names>Z</given-names></name>. <article-title>Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review</article-title>. <source>Neural Computation</source>. <year>2017</year>;<volume>29</volume>(<issue>9</issue>):<fpage>2352</fpage>–<lpage>2449</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1162/neco_a_00990</pub-id><?supplied-pmid 28599112?><pub-id pub-id-type="pmid">28599112</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref029">
      <label>29</label>
      <mixed-citation publication-type="other">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:14091556. 2014.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref030">
      <label>30</label>
      <mixed-citation publication-type="other">Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:14090473. 2014.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref031">
      <label>31</label>
      <mixed-citation publication-type="other">Kim Y, Denton C, Hoang L, Rush AM. Structured attention networks. arXiv preprint arXiv:170200887. 2017.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref032">
      <label>32</label>
      <mixed-citation publication-type="other">Lin T, Wang Y, Liu X, Qiu X. A survey of transformers. arXiv preprint arXiv:210604554. 2021.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref033">
      <label>33</label>
      <mixed-citation publication-type="other">Wang X, Girshick R, Gupta A, He K. Non-local neural networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2018. p. 7794–7803.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref034">
      <label>34</label>
      <mixed-citation publication-type="other">Carion N, Massa F, Synnaeve G, Usunier N, Kirillov A, Zagoruyko S. End-to-end object detection with transformers. In: European conference on computer vision. Springer; 2020. p. 213–229.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref035">
      <label>35</label>
      <mixed-citation publication-type="journal"><name><surname>Ramachandran</surname><given-names>P</given-names></name>, <name><surname>Parmar</surname><given-names>N</given-names></name>, <name><surname>Vaswani</surname><given-names>A</given-names></name>, <name><surname>Bello</surname><given-names>I</given-names></name>, <name><surname>Levskaya</surname><given-names>A</given-names></name>, <name><surname>Shlens</surname><given-names>J</given-names></name>. <article-title>Stand-alone self-attention in vision models</article-title>. <source>Advances in Neural Information Processing Systems</source>. <year>2019</year>;<volume>32</volume>.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref036">
      <label>36</label>
      <mixed-citation publication-type="other">Liu Y, Zhang Y, Wang Y, Hou F, Yuan J, Tian J, et al. A survey of visual transformers. arXiv preprint arXiv:211106091. 2021.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref037">
      <label>37</label>
      <mixed-citation publication-type="journal"><name><surname>Khan</surname><given-names>S</given-names></name>, <name><surname>Naseer</surname><given-names>M</given-names></name>, <name><surname>Hayat</surname><given-names>M</given-names></name>, <name><surname>Zamir</surname><given-names>SW</given-names></name>, <name><surname>Khan</surname><given-names>FS</given-names></name>, <name><surname>Shah</surname><given-names>M</given-names></name>. <article-title>Transformers in vision: A survey</article-title>. <source>ACM computing surveys (CSUR)</source>. <year>2022</year>;<volume>54</volume>(<issue>10s</issue>):<fpage>1</fpage>–<lpage>41</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1145/3505244</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref038">
      <label>38</label>
      <mixed-citation publication-type="journal"><name><surname>Han</surname><given-names>K</given-names></name>, <name><surname>Wang</surname><given-names>Y</given-names></name>, <name><surname>Chen</surname><given-names>H</given-names></name>, <name><surname>Chen</surname><given-names>X</given-names></name>, <name><surname>Guo</surname><given-names>J</given-names></name>, <name><surname>Liu</surname><given-names>Z</given-names></name>, <etal>et al</etal>. <article-title>A survey on vision transformer</article-title>. <source>IEEE transactions on pattern analysis and machine intelligence</source>. <year>2022</year>. <?supplied-pmid 35180075?><pub-id pub-id-type="pmid">35180075</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref039">
      <label>39</label>
      <mixed-citation publication-type="journal"><name><surname>Janowczyk</surname><given-names>A</given-names></name>, <name><surname>Zuo</surname><given-names>R</given-names></name>, <name><surname>Gilmore</surname><given-names>H</given-names></name>, <name><surname>Feldman</surname><given-names>M</given-names></name>, <name><surname>Madabhushi</surname><given-names>A</given-names></name>. <article-title>Automatic Measurement of Sister Chromatid Exchange Frequency</article-title>. <source>JCO Clin Cancer Inform</source>. <year>2019</year>;<volume>3</volume>:<fpage>1</fpage>–<lpage>7</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref040">
      <label>40</label>
      <mixed-citation publication-type="journal"><name><surname>Gutman</surname><given-names>DA</given-names></name>, <name><surname>Khalilia</surname><given-names>M</given-names></name>, <name><surname>Lee</surname><given-names>S</given-names></name>, <name><surname>Nalisnik</surname><given-names>M</given-names></name>, <name><surname>Mullen</surname><given-names>Z</given-names></name>, <name><surname>Beezley</surname><given-names>J</given-names></name>, <etal>et al</etal>. <article-title>The Digital Slide Archive: A Software Platform for Management, Integration and Analysis of Histology for Cancer Research</article-title>. <source>Cancer research</source>. <year>2018</year>;<volume>77</volume>(<issue>21</issue>):<fpage>e75</fpage>–<lpage>e78</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1158/0008-5472.CAN-17-0629</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref041">
      <label>41</label>
      <mixed-citation publication-type="journal"><name><surname>Marcolini</surname><given-names>A</given-names></name>, <name><surname>Bussola</surname><given-names>N</given-names></name>, <name><surname>Arbitrio</surname><given-names>E</given-names></name>, <name><surname>Amgad</surname><given-names>M</given-names></name>, <name><surname>Jurman</surname><given-names>G</given-names></name>, <name><surname>Furlanello</surname><given-names>C</given-names></name>. <article-title>histolab: A Python library for reproducible Digital Pathology preprocessing with automated testing</article-title>. <source>SoftwareX</source>. <year>2022</year>;<volume>20</volume>:<fpage>101237</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.softx.2022.101237</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref042">
      <label>42</label>
      <mixed-citation publication-type="other">MONAI Consortium. MONAI: Medical Open Network for AI; 2020. Available from: <ext-link xlink:href="https://github.com/Project-MONAI/MONAI" ext-link-type="uri">https://github.com/Project-MONAI/MONAI</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref043">
      <label>43</label>
      <mixed-citation publication-type="journal"><name><surname>Rosenthal</surname><given-names>J</given-names></name>, <name><surname>Carelli</surname><given-names>R</given-names></name>, <name><surname>Omar</surname><given-names>M</given-names></name>, <name><surname>Brundage</surname><given-names>D</given-names></name>, <name><surname>Halbert</surname><given-names>E</given-names></name>, <name><surname>Nyman</surname><given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Building Tools for Machine Learning and Artificial Intelligence in Cancer Research: Best Practices and a Case Study with the PathML Toolkit for Computational Pathology</article-title>. <source>Molecular Cancer Research</source>. <year>2022</year>;<volume>20</volume>(<issue>2</issue>):<fpage>202</fpage>–<lpage>206</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1158/1541-7786.MCR-21-0665</pub-id><?supplied-pmid 34880124?><pub-id pub-id-type="pmid">34880124</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref044">
      <label>44</label>
      <mixed-citation publication-type="other">Tan M, Chen B, Pang R, Vasudevan V, Sandler M, Howard A, et al. Mnasnet: Platform-aware neural architecture search for mobile. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; 2019. p. 2820–2828.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref045">
      <label>45</label>
      <mixed-citation publication-type="other">Iandola FN, Han S, Moskewicz MW, Ashraf K, Dally WJ, Keutzer K. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&lt; 0.5 MB model size. arXiv preprint arXiv:160207360. 2016.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref046">
      <label>46</label>
      <mixed-citation publication-type="other">Howard AG, Zhu M, Chen B, Kalenichenko D, Wang W, Weyand T, et al. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:170404861. 2017.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref047">
      <label>47</label>
      <mixed-citation publication-type="other">Selvaraju RR, Cogswell M, Das A, Vedantam R, Parikh D, Batra D. Grad-CAM: Visual explanations from deep networks via gradient-based localization. In: Proceedings of the IEEE International Conference on Computer Vision; 2017. p. 618–626.</mixed-citation>
    </ref>
    <ref id="pone.0289499.ref048">
      <label>48</label>
      <mixed-citation publication-type="journal"><name><surname>Ross-Innes</surname><given-names>C</given-names></name>, <etal>et al</etal>. <article-title>Risk stratification of Barrett’s oesophagus using a non-endoscopic sampling method coupled with a biomarker panel: a cohort study</article-title>. <source>Lancet Gastroenterol Hepatol</source>. <year>2017</year>;<volume>2</volume>(<issue>1</issue>):<fpage>23</fpage>–<lpage>31</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/S2468-1253(16)30118-2</pub-id><?supplied-pmid 28404010?><pub-id pub-id-type="pmid">28404010</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref049">
      <label>49</label>
      <mixed-citation publication-type="journal"><name><surname>Peters</surname><given-names>CJ</given-names></name>, <name><surname>Rees</surname><given-names>JR</given-names></name>, <name><surname>Hardwick</surname><given-names>RH</given-names></name>, <name><surname>Hardwick</surname><given-names>JS</given-names></name>, <name><surname>Vowler</surname><given-names>SL</given-names></name>, <name><surname>Ong</surname><given-names>CAJ</given-names></name>, <etal>et al</etal>. <article-title>A 4-gene signature predicts survival of patients with resected adenocarcinoma of the esophagus, junction, and gastric cardia</article-title>. <source>Gastroenterology</source>. <year>2010</year>;<volume>139</volume>(<issue>6</issue>):<fpage>1995</fpage>–<lpage>2004</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1053/j.gastro.2010.05.080</pub-id><?supplied-pmid 20621683?><pub-id pub-id-type="pmid">20621683</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0289499.ref050">
      <label>50</label>
      <mixed-citation publication-type="journal"><collab>Cancer Genome Atlas Research Network</collab>, <name><surname>Weinstein</surname><given-names>JN</given-names></name>, <name><surname>Collisson</surname><given-names>EA</given-names></name>, <name><surname>Mills</surname><given-names>GB</given-names></name>, <name><surname>Shaw</surname><given-names>KR</given-names></name>, <name><surname>Ozenberger</surname><given-names>BA</given-names></name>, <etal>et al</etal>. <article-title>The Cancer Genome Atlas Pan-Cancer analysis project</article-title>. <source>Nat Genet</source>. <year>2013</year>;<volume>45</volume>(<issue>10</issue>):<fpage>1113</fpage>–<lpage>1120</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/ng.2764</pub-id><?supplied-pmid 24071849?><pub-id pub-id-type="pmid">24071849</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
