<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Nucleic Acids Res</journal-id>
    <journal-id journal-id-type="iso-abbrev">Nucleic Acids Res</journal-id>
    <journal-id journal-id-type="publisher-id">nar</journal-id>
    <journal-title-group>
      <journal-title>Nucleic Acids Research</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">0305-1048</issn>
    <issn pub-type="epub">1362-4962</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">5888241</article-id>
    <article-id pub-id-type="pmid">29315405</article-id>
    <article-id pub-id-type="doi">10.1093/nar/gkx1313</article-id>
    <article-id pub-id-type="publisher-id">gkx1313</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Methods Online</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>HipMCL: a high-performance parallel implementation of the Markov clustering algorithm for large-scale networks</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Azad</surname>
          <given-names>Ariful</given-names>
        </name>
        <!--<email>azad@lbl.gov</email>-->
        <xref ref-type="aff" rid="AFF1">1</xref>
        <xref ref-type="corresp" rid="COR1"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-4577-8276</contrib-id>
        <name>
          <surname>Pavlopoulos</surname>
          <given-names>Georgios A</given-names>
        </name>
        <xref ref-type="aff" rid="AFF2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ouzounis</surname>
          <given-names>Christos A</given-names>
        </name>
        <xref ref-type="aff" rid="AFF3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kyrpides</surname>
          <given-names>Nikos C</given-names>
        </name>
        <xref ref-type="aff" rid="AFF2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Buluç</surname>
          <given-names>Aydin</given-names>
        </name>
        <!--<email>abuluc@lbl.gov</email>-->
        <xref ref-type="aff" rid="AFF1">1</xref>
        <xref ref-type="aff" rid="AFF4">4</xref>
        <xref ref-type="corresp" rid="COR1"/>
      </contrib>
    </contrib-group>
    <aff id="AFF1"><label>1</label>Computational Research Division, Lawrence Berkeley National Laboratory, 1 Cyclotron Road, Berkeley, CA 94720-8150, USA</aff>
    <aff id="AFF2"><label>2</label>DOE Joint Genome Institute, Lawrence Berkeley National Laboratory, 2800 Mitchell Drive, Walnut Creek, CA 94598, USA</aff>
    <aff id="AFF3"><label>3</label>Biological Computation &amp; Process Laboratory, Chemical Process &amp; Energy Resources Institute, Centre for Research &amp; Technology Hellas, Thessalonica 57001, Greece</aff>
    <aff id="AFF4"><label>4</label>Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, CA 94720, USA</aff>
    <author-notes>
      <corresp id="COR1">To whom correspondence should be addressed. Tel: +1 510 486 5197; Fax: +1 510-486-6900; Email: <email>abuluc@lbl.gov</email>. Correspondence may also be addressed to Ariful Azad. Tel: +1 510-486-6292; Fax: +1 510-486-6900; Email: <email>azad@lbl.gov</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <day>06</day>
      <month>4</month>
      <year>2018</year>
    </pub-date>
    <pub-date iso-8601-date="2018-01-05" pub-type="epub">
      <day>05</day>
      <month>1</month>
      <year>2018</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>05</day>
      <month>1</month>
      <year>2018</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>46</volume>
    <issue>6</issue>
    <fpage>e33</fpage>
    <lpage>e33</lpage>
    <history>
      <date date-type="accepted">
        <day>02</day>
        <month>1</month>
        <year>2018</year>
      </date>
      <date date-type="rev-recd">
        <day>18</day>
        <month>12</month>
        <year>2017</year>
      </date>
      <date date-type="received">
        <day>19</day>
        <month>9</month>
        <year>2017</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Published by Oxford University Press on behalf of Nucleic Acids Research 2018.</copyright-statement>
      <copyright-year>2018</copyright-year>
      <license license-type="us-gov">
        <license-p>This work is written by (a) US Government employee(s) and is in the public domain in the US.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="gkx1313.pdf"/>
    <abstract>
      <title>Abstract</title>
      <p>Biological networks capture structural or functional properties of relevant entities such as molecules, proteins or genes. Characteristic examples are gene expression networks or protein–protein interaction networks, which hold information about functional affinities or structural similarities. Such networks have been expanding in size due to increasing scale and abundance of biological data. While various clustering algorithms have been proposed to find highly connected regions, Markov Clustering (MCL) has been one of the most successful approaches to cluster sequence similarity or expression networks. Despite its popularity, MCL’s scalability to cluster large datasets still remains a bottleneck due to high running times and memory demands. Here, we present High-performance MCL (HipMCL), a parallel implementation of the original MCL algorithm that can run on distributed-memory computers. We show that HipMCL can efficiently utilize 2000 compute nodes and cluster a network of ∼70 million nodes with ∼68 billion edges in ∼2.4 h. By exploiting distributed-memory environments, HipMCL clusters large-scale networks several orders of magnitude faster than MCL and enables clustering of even bigger networks. HipMCL is based on MPI and OpenMP and is freely available under a modified BSD license.</p>
    </abstract>
    <counts>
      <page-count count="11"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="SEC1">
    <title>INTRODUCTION</title>
    <p>Graphs and their isomorphic representations as matrices or pairs-lists are one of the principal representations of biological information on a very large scale that has emerged over the past decade (<xref rid="B1" ref-type="bibr">1</xref>). These graphs represent structural similarities or functional affinities, e.g. sequence homology or expression, respectively (<xref rid="B2" ref-type="bibr">2</xref>). Due to the rapid increase of the available information for genome structure and function, large-scale biological network clustering and analysis has become a major challenge (<xref rid="B3" ref-type="bibr">3</xref>). While for functional networks, where typically a whole genome is represented, scalability is easier to address (<xref rid="B4" ref-type="bibr">4</xref>), for sequence similarity networks (SSNs) this is not the case as thousands of genomes need to be covered. In the latter case, very large graphs stored as sparse matrices can contain the relevant homology detection across multiple genomes, which could lead to the generation of massive networks with hundreds of millions of nodes and tens of billions of edges (<xref rid="B5" ref-type="bibr">5</xref>). Protein family detection was first implemented by the semi-automated COG method (<xref rid="B6" ref-type="bibr">6</xref>) and later expanded by the fully-automated protocol TribeMCL (<xref rid="B7" ref-type="bibr">7</xref>) using the Markov Clustering (MCL) graph clustering algorithm (<xref rid="B8" ref-type="bibr">8</xref>). Yet, limitations such as high memory footprint and long running time render the clustering of large-scale networks a real challenge.</p>
    <p>Indeed, despite the great variety of graph-based clustering algorithms available today (<xref rid="B9" ref-type="bibr">9</xref>,<xref rid="B10" ref-type="bibr">10</xref>), only a few manage to handle networks of million nodes and edges. SPICi (<xref rid="B11" ref-type="bibr">11</xref>) for example, is a fast, local network clustering algorithm that detects densely connected communities within a network. It is one of the fastest graph-based clustering algorithms and runs in time O(VlogV+E) and space O(E), where V and E are the number of vertices and edges in the network. While SPICi is very efficient and runs in linear time, it performs better for dense networks and not sparse ones. Louvain (<xref rid="B12" ref-type="bibr">12</xref>) is an efficient and easy-to-implement greedy clustering method for identifying communities in large scale networks. The method can handle networks of sizes up to 100 million nodes and billions of links. Although the exact computational complexity of the method is not known, the method seems to run in time O(VlogV). Molecular Complex Detection (MCODE) (<xref rid="B13" ref-type="bibr">13</xref>) detects densely connected regions in large protein–protein interaction (PPI) networks that may represent molecular complexes. The time complexity of the entire algorithm is polynomial O(VEd<sup>3</sup>) where d is the vertex size of the average vertex neighborhood in the input graph. Restricted neighborhood search clustering (RNSC) (<xref rid="B14" ref-type="bibr">14</xref>) uses stochastic local search. RNSC tries to achieve optimal cost clustering by assigning some cost functions to the set of clusters of a graph. It requires O(V<sup>2</sup>) memory and the complexity of a move in the naive cost function is O(V). Affinity-propagation (<xref rid="B15" ref-type="bibr">15</xref>) is a clustering algorithm based on the concept of ‘message passing’ between data points able to cluster 25 000 data points in a few hours or 120 000 data points in less than a day, based on all pairwise similarities on a 16GB 2.4GHz machine. The latter achieves complexity O(kV<sup>2</sup>), where k is the number of iterations.</p>
    <p>Despite the continuous active research and the new methods that appear to serve the purpose of large-scale biological cluster detection, MCL has been one of the most successful in the field and today it comes as a core module with many Linux distributions and many visualization tools (<xref rid="B16" ref-type="bibr">16–24</xref>). MCL uses random walks to detect clustered structures in graphs by a mathematical bootstrapping procedure and was initially used to detect protein families in sequence similarity information, as well as protein interaction modules (<xref rid="B25" ref-type="bibr">25</xref>). An optimized implementation should have complexity O(Vd<sup>2</sup>), where V is the number of nodes in the graph and d is the average number of neighbors per node. MCL’s popularity stems from its remarkably robustness to graph alterations and its relatively non-parametric nature (<xref rid="B10" ref-type="bibr">10</xref>).</p>
    <p>All above methods including MCL struggle to cope with the observed—and further anticipated—exponential increase of biological data volumes (<xref rid="B26" ref-type="bibr">26</xref>). MCL has been previously parallelized on single GPU (<xref rid="B27" ref-type="bibr">27</xref>), but it can only cluster relatively small networks due to memory limitations of a single GPU. This work presents HipMCL, a scalable distributed-memory parallel implementation of the MCL algorithm. In contrast to previous work, HipMCL takes advantage of the aggregate memory available in all compute nodes, clustering networks that were deemed too large to cluster using MCL. The unprecedented scalability of HipMCL stems from its use of state-of-the-art parallel algorithms for sparse matrix manipulation. HipMCL is written using MPI and OpenMP, with the principal aim to speed up graph clustering and efficiently detect clusters on a very large scale. Notably, MCL’s backbone has remained intact, thus making HipMCL a parallel implementation of the original MCL algorithm. We demonstrate the performance of HipMCL by using datasets from the Integrated Microbial Genomes (IMG) database (<xref rid="B28" ref-type="bibr">28</xref>).</p>
  </sec>
  <sec sec-type="materials|methods" id="SEC2">
    <title>MATERIALS AND METHODS</title>
    <sec id="SEC2-1">
      <title>The markov cluster algorithm</title>
      <p>We first review the MCL procedure here to facilitate the presentation of HipMCL. The MCL algorithm is built upon the following property of clusters in a graph: ‘random walks on the graph will infrequently go from one natural cluster to another’. The MCL algorithm does indeed simulate random walks of higher lengths on a graph. In this process, the algorithm computes the probability of random walks between pairs of vertices and prunes paths with low probability to discover natural clusters in the graph.</p>
      <p>To capture the probability of random walks, the MCL algorithm starts with and maintains column stochastic matrices (also called Markov matrices). A column stochastic matrix is a non-negative matrix with the property that each column sums to (probability) 1. Initially, the adjacency matrix of a graph is converted into a column stochastic matrix by dividing every non-zero entry by the sum of all entries in the column where the entry belongs. The MCL algorithm then iteratively performs two operations called <bold><italic>expansion</italic></bold> and <bold><italic>inflation</italic></bold>. The expansion step performs matrix squaring, which corresponds to computing random walks of higher lengths. The inflation step computes the Hadamard power of the matrix (taking power entrywise) in order to boost the probabilities of intra-cluster walks and demote inter-cluster walks. Expansion and inflations are performed as long as there is ‘significant’ change between successive iterations. After the MCL algorithm converges, connected components of the final graph will form the final set of clusters. The inflation operation combined with the normalization for maintaining column stochasticity ensure convergence, whose properties have been studied extensively (<xref rid="B29" ref-type="bibr">29</xref>). The high-level description of the MCL algorithm is given below:</p>
      <p>
        <inline-graphic xlink:href="gkx1313ufig1.jpg"/>
      </p>
      <p>To reduce the memory requirement in the intermediate iterations, MCL keeps the networks sparse, by pruning low probability terms from the expanded matrices. Pruning in MCL is performed by the sequence of the following three steps:
<list list-type="roman-lower"><list-item><p><bold>Prune:</bold> first, MCL removes from the expanded matrix <bold>B</bold> entries with values smaller than a threshold. This is done early, because it is fast and speeds up subsequent steps.</p></list-item><list-item><p><bold>Recover</bold>: if a column of the pruned matrix becomes very sparse, the recovery step brings back some significant non-zero entries. The goal of this step is controlling the effect of excessive pruning and keeps at least R entries in each column. Here, R is a user-provided parameter, called the recovery number. The default value of R in the current version of MCL is 1400. To perform recovery, MCL identifies the R-th largest entry in each column and then keeps the top R entries. This task is therefore a specialization of the selection problem (<xref rid="B30" ref-type="bibr">30</xref>), which identifies the kth largest number in a list or array.</p></list-item><list-item><p><bold>Select</bold>: if a column of the pruned matrix remains too dense, the selection step prunes it further so that at most S non-zero entries remain in each column. Here, S is a user-provided parameter, called the selection number. The default value of S in the current version of MCL is 1100. To perform selection, MCL identifies the S-th largest entry in each column and then keeps the top S entries. Similar to the recovery step, the selection step is also a specialization of the k-select problem performed on every column of the matrix. Note that selection is only performed on columns where recovery is not attempted.</p></list-item></list></p>
      <p>The number of non-zero entries in a column after pruning/recovery/select steps is at most max(R,S), where R and S are the recovery and selection number, respectively. Hence, the aforementioned steps ensure that the expanded matrix remains sparse while keeping as much information as possible.</p>
      <p>Stijn van Dongen developed an open source implementation of the MCL algorithm available at <ext-link ext-link-type="uri" xlink:href="https://micans.org/mcl/">https://micans.org/mcl/</ext-link>. This implementation employs multithreading to take advantage of the shared-memory parallelism available in modern multicore processors. In this paper, we use ‘MCL’ to refer to both the algorithm and its associated shared-memory parallel software.</p>
    </sec>
    <sec id="SEC2-2">
      <title>The HipMCL algorithm</title>
      <p>While MCL and TribeMCL have been used extensively in clustering sequence similarity and other types of information, at a large scale, MCL becomes very demanding in terms of computational and memory requirements. Consequently, existing MCL software cannot handle large datasets that have trillions of non-zero similarities across billions of protein sequences. For example, MCL is expected to take 45 days to cluster a network with ∼47 million nodes and ∼7 billion edges on a 16-core workstation with 1 terabyte of memory. The expected runtime is extrapolated based on the iterations during the first 10 days. Due to the memory limitations of a single computing node or workstation, clustering even bigger networks is not possible.</p>
      <p>HipMCL is a distributed-memory algorithm implementation based on MCL which employs massive parallelism to cluster networks of unprecedented size. Each component of HipMCL is fully parallelized, taking advantage of both shared- and distributed-memory parallelism available on modern supercomputers (Table <xref ref-type="table" rid="tbl1">1</xref>). We describe the parallelization strategies for different steps of HipMCL below.</p>
      <table-wrap id="tbl1" orientation="portrait" position="float">
        <label>Table 1.</label>
        <caption>
          <title>Computational infrastructure used for HipMCL benchmarking</title>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th align="left" rowspan="1" colspan="1"/>
              <th align="left" rowspan="1" colspan="1"/>
              <th align="left" rowspan="1" colspan="1">Edison (Cray XC30 supercomputer)</th>
              <th align="left" rowspan="1" colspan="1">Cori2 (Cray XC40 supercomputer)</th>
              <th align="left" rowspan="1" colspan="1">In-house system</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" rowspan="1" colspan="1">Overall system</td>
              <td align="left" rowspan="1" colspan="1">#nodes</td>
              <td align="left" rowspan="1" colspan="1">5586</td>
              <td align="left" rowspan="1" colspan="1">9688</td>
              <td align="left" rowspan="1" colspan="1">1</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1"/>
              <td align="left" rowspan="1" colspan="1">#cores</td>
              <td align="left" rowspan="1" colspan="1">134 064</td>
              <td align="left" rowspan="1" colspan="1">658 784</td>
              <td align="left" rowspan="1" colspan="1">8</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1"/>
              <td align="left" rowspan="1" colspan="1">aggregate memory</td>
              <td align="left" rowspan="1" colspan="1">357 terabyte</td>
              <td align="left" rowspan="1" colspan="1">1 petabyte</td>
              <td align="left" rowspan="1" colspan="1">1 terabyte</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1"/>
              <td align="left" rowspan="1" colspan="1">max #nodes used in experiments</td>
              <td align="left" rowspan="1" colspan="1">2025</td>
              <td align="left" rowspan="1" colspan="1">2048</td>
              <td align="left" rowspan="1" colspan="1">8</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">One computing node of the system</td>
              <td align="left" rowspan="1" colspan="1">processor</td>
              <td align="left" rowspan="1" colspan="1">Intel Ivy Bridge</td>
              <td align="left" rowspan="1" colspan="1">Intel KNL</td>
              <td align="left" rowspan="1" colspan="1">Intel Xeon</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1"/>
              <td align="left" rowspan="1" colspan="1">number of cores</td>
              <td align="left" rowspan="1" colspan="1">24</td>
              <td align="left" rowspan="1" colspan="1">68 (272 threads)</td>
              <td align="left" rowspan="1" colspan="1">8</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1"/>
              <td align="left" rowspan="1" colspan="1">memory</td>
              <td align="left" rowspan="1" colspan="1">64 gigabyte</td>
              <td align="left" rowspan="1" colspan="1">112 gigabyte</td>
              <td align="left" rowspan="1" colspan="1">1 terabyte</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <sec id="SEC2-2-1">
        <title>Data distribution and storage</title>
        <p>Similar to MCL, HipMCL represents a network by its sparse adjacency matrix. There are two levels of parallelism available in a distributed-memory computer: parallelism within compute nodes is often handled using OpenMP threads and parallelism across compute nodes is handled using MPI processes. Since the memory of a single compute node is accessible by other threads, the matrix is only distributed across compute nodes (i.e. MPI processes). We logically view the set of MPI processes as a 2D process grid that can be indexed as P(i,j). Processes with the same row (column) index belong to the same process row (column). In our implementation, we use a √p-by-√p process grid, where p is the number of processes. Submatrices are assigned to processors according to a 2D block decomposition: processor P(i,j) stores the submatrix A<sub>ij</sub> of dimensions (N/p) × (N/p) in its local memory, where N is the number of rows/columns in the matrix. For an illustrative example of block-distributed matrices, see section below titled: Distributed-memory parallel SpGEMM algorithm. Local submatrices are stored in a compressed format that requires storage proportional to the number of edges in the network.</p>
      </sec>
      <sec id="SEC2-2-2">
        <title>Parallelizing the expansion step</title>
        <p>Expansion is by far the most compute- and memory-intensive step of MCL (Table <xref ref-type="table" rid="tbl2">2</xref>) and requires efficient parallel algorithms to make good use of hundreds of thousands of processors. To keep our discussion easy to follow, we describe parallelization of expansion in two steps: (i) designing expansion in a way that offers ample parallelism without increasing memory requirements significantly (ii) performing expansion in distributed-memory systems given the choices made in step i.</p>
        <table-wrap id="tbl2" orientation="portrait" position="float">
          <label>Table 2.</label>
          <caption>
            <title>The impact of parallelizing different steps of MCL when clustering a eukaryotic network with 3 million nodes and 359 million edges (Table <xref ref-type="table" rid="tbl3">3</xref>)</title>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th align="left" rowspan="1" colspan="1"/>
                <th align="left" rowspan="1" colspan="1">File I/O (s)</th>
                <th align="left" rowspan="1" colspan="1">Expansion (s)</th>
                <th align="left" rowspan="1" colspan="1">Prune (s)</th>
                <th align="left" rowspan="1" colspan="1">Inflation (ss)</th>
                <th align="left" rowspan="1" colspan="1">Components (s)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">MCL (1 node)</td>
                <td align="left" rowspan="1" colspan="1">600.12</td>
                <td align="left" rowspan="1" colspan="1">1052.11</td>
                <td align="left" rowspan="1" colspan="1">9.93</td>
                <td align="left" rowspan="1" colspan="1">199.97</td>
                <td align="left" rowspan="1" colspan="1">608.77</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">HipMCL (1024 nodes)</td>
                <td align="left" rowspan="1" colspan="1">7.23</td>
                <td align="left" rowspan="1" colspan="1">27.20</td>
                <td align="left" rowspan="1" colspan="1">0.92</td>
                <td align="left" rowspan="1" colspan="1">0.19</td>
                <td align="left" rowspan="1" colspan="1">0.19</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">HipMCL speedup</td>
                <td align="left" rowspan="1" colspan="1">83×</td>
                <td align="left" rowspan="1" colspan="1">39×</td>
                <td align="left" rowspan="1" colspan="1">11×</td>
                <td align="left" rowspan="1" colspan="1">1052×</td>
                <td align="left" rowspan="1" colspan="1">3288×</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="T2TFN1">
              <p>The last row shows the speedups achieved by HipMCL on 1024 nodes of Edison (Table <xref ref-type="table" rid="tbl1">1</xref>). While HipMCL drastically reduces the running time of all five steps, expansion remains the most expensive step in Markov clustering. Hence, we spent the majority of our research effort to make the expansion step scalable.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <sec id="SEC2-2-2-1">
          <title>Parallelism-memory trade-off in expansion</title>
          <p>As described in the high-level description, MCL expands a column-stochastic matrix by first computing <bold>A<sup>2</sup></bold>and then sparsifies it by pruning small entries in each column. Since the pruned version of <bold>A<sup>2</sup></bold> requires significantly less storage, it is memory efficient to fuse expansion and pruning on a subset of columns. Figure <xref ref-type="fig" rid="F1">1</xref> shows an example where we expand a block of b ( = 2) columns, prune these expanded columns and then move to next block of columns. Therefore, we perform the expansion and pruning in h phases where h = N/b and in each phase, we expand and prune b columns. In Figure <xref ref-type="fig" rid="F1">1</xref>, the expansion is performed in three phases. The choice of b (and h) is crucial for both computational and memory efficiency. Setting b to a small value (e.g. <italic>b</italic> = 1) saves memory by not storing many unpruned columns, but it performs a limited number of floating point operations that cannot efficiently utilize available computational power offered by large distributed-memory systems. By contrast, using a large value for b (e.g. <italic>b</italic> = N) can take advantage of many processors and truly deliver high-performance clustering for large-scale networks, but it requires a large amount of memory (Figure <xref ref-type="fig" rid="F1">1</xref>).</p>
          <fig id="F1" orientation="portrait" position="float">
            <label>Figure 1.</label>
            <caption>
              <p>An example of expansion and pruning of b ( = 2) columns of a column stochastic matrix <bold>A</bold>. Non-zero entries are shown with filled circles. Here, <bold>A<sub>b</sub></bold> is a submatrix of <bold>A</bold>, consisting all N rows and b ( = 2) columns that are currently being expanded. The product <bold>AxA<sub>b</sub></bold>is computed and pruned to obtain the final result for these b columns. Parts of matrices that are active in the current expansion are shown in darker shades. For comparison, MCL sets b to 1. HipMCL dynamically selects a large value for b from the range [1,N] such that the expanded columns of A<sup>2</sup> do not overflow memory. When these columns are expanded and pruned, the computation moves to the next set of b columns.</p>
            </caption>
            <graphic xlink:href="gkx1313fig1"/>
          </fig>
          <p>MCL takes the memory-efficient approach by expanding and pruning one column at a time (or t columns at a time when t threads are used). Therefore, in the sequential case, MCL sets b to 1 and h to N and performs N sparse matrix-sparse vector multiplications (SpMSpV). By contrast, HipMCL prefers larger blocks and fewer phases. Let <bold>A<sub>b</sub></bold> be a submatrix of <bold>A</bold> consisting all N rows and b columns that are being expanded in the current phase (Figure <xref ref-type="fig" rid="F1">1</xref>). Then, in a phase, HipMCL computes <bold>AxA<sub>b</sub></bold> by parallel sparse matrix–matrix multiplications (SpGEMM). When the entire unpruned <bold>A<sup>2</sup></bold> can be stored in memory, HipMCL computes the entire sparse matrix-matrix product and stores it for subsequent pruning (in this case, <italic>h</italic> = 1, <italic>b</italic> = N and <bold>A<sub>b</sub></bold>= <bold>A</bold>). At the other extreme case with very limited memory, HipMCL can expand a matrix in N phases and imitate MCL’s column-by-column approach. However, the latter approach offers limited parallelism and diminishes the benefit of HipMCL. Hence, HipMCL selects b and h dynamically based on the available memory. Let mem(<bold>A</bold>), mem(<bold>A<sup>2</sup></bold>) and mem(<bold>C</bold>) be the required memory to store the corresponding matrices (<bold>C</bold> is the pruned version of <bold>A<sup>2</sup></bold>). Then, the number of phases can be estimated as follows: <bold>h = ⌈(mem(A<sup>2</sup>)/(TotalMem - mem(A) – mem(C))⌉</bold>, where TotalMem is the aggregate memory available to HipMCL. This dynamic and incremental SpGEMM enables HipMCL to expand and prune matrices as quickly as possible without overflowing the memory.</p>
        </sec>
        <sec id="SEC2-2-2-2">
          <title>Distributed-memory parallel SpGEMM algorithm</title>
          <p>In HipMCL, the distributed-memory SpGEMM is performed by a variant of Scalable Universal Matrix Multiplication (SUMMA) algorithm (<xref rid="B31" ref-type="bibr">31</xref>) adapted for sparse matrices (<xref rid="B32" ref-type="bibr">32</xref>). While Sparse SUMMA is prior work, we summarize it here to make the presentation self-contained. At first, we describe how Sparse SUMMA computes <bold>A<sup>2</sup></bold> in distributed memory, assuming that the whole unpruned <bold>A<sup>2</sup></bold> can be stored in the aggregated memory distributed across all computing nodes. The special case of computing <bold>A</bold>x<bold>A<sub>b</sub></bold> for any submatrix <bold>A<sub>b</sub></bold> will be discussed later.</p>
          <p>In order to compute <bold>A<sup>2</sup></bold>, Sparse SUMMA distributes input and output matrices on a √p-by-√p process grid, where p is the number of processes. Figure <xref ref-type="fig" rid="F2">2</xref> shows an example of distributing the input and output matrices on a 3 × 3 process grid, using the same input matrix from Figure <xref ref-type="fig" rid="F1">1</xref>. To multiply distributed matrices, processes need to communicate their local submatrices. In Sparse SUMMA, this communication happens in √p stages (e.g. three stages in Figure <xref ref-type="fig" rid="F2">2</xref>). Since each stage performs similar communication and computation, we only describe the first stage using Figure <xref ref-type="fig" rid="F2">2</xref>. In the first stage, members of the first process column broadcast their local piece of <bold>A</bold> horizontally (along the process row) and members of the first process row broadcast their local piece of <bold>A</bold> vertically (along the process column). Each process then locally multiplies the received pieces of <bold>A</bold> and merges the multiplied results to its local piece of the output matrix <bold>A<sup>2</sup></bold>. Figure <xref ref-type="fig" rid="F2">2</xref> illustrates that each process receives two (different) pieces of the input matrix and updates its local part of <bold>A<sup>2</sup></bold>. Other stages of Sparse SUMMA follow the same pattern with the exception that the <italic>i</italic>th process row and column broadcast their local submatrices in the <italic>i</italic>th stage. At the end of √p stages, each process has fully computed its local part of <bold>A<sup>2</sup></bold>, and all processes collectively store the final <bold>A<sup>2</sup></bold>.</p>
          <fig id="F2" orientation="portrait" position="float">
            <label>Figure 2.</label>
            <caption>
              <p>Execution of the sparse SUMMA algorithm for sparse matrix–matrix multiplication A<sup>2</sup> = A*A on a 3-by-3 process grid. We use the same input matrix from Figure <xref ref-type="fig" rid="F1">1</xref> and denote submatrices local to different processes by blue squares. Here, we show the first stage of the sparse SUMMA algorithm where members of the first process column broadcast their local pieces of A horizontally (along the process row) and members of the first process row broadcast their local pieces of A vertically (along the process column). Broadcasting processes in the first stage are marked with blue shades and the direction of data communication is shown with red arrowheads. The rightmost figure depicts each process that locally multiplies the received parts of A and merges the multiplied results to its local part of the output matrix A<sup>2</sup>.</p>
            </caption>
            <graphic xlink:href="gkx1313fig2"/>
          </fig>
          <p>The above computation of <bold>A<sup>2</sup></bold> interleaves communication (broadcasting submatrices) and computation (local multiplication and merging). On a small number of nodes, the computation of <bold>A<sup>2</sup></bold> dominates the communication cost. However, when HipMCL is run on thousands of distributed nodes, communicating data becomes expensive and may dominate the overall runtime of the expansion step, as well as the whole clustering algorithm.</p>
          <p>The computation of <bold>A</bold>x<bold>A<sub>b</sub></bold> is identical to the computation of <bold>A<sup>2</sup></bold>, with the only exception that the second matrix in Figure <xref ref-type="fig" rid="F2">2</xref> is <bold>A<sub>b</sub></bold> instead of <bold>A</bold>. That is, each stage of Sparse SUMMA contributes to local pieces of <bold>A</bold>x<bold>A<sub>b</sub></bold>. Assuming that h phases are used to compute <bold>A<sup>2</sup></bold> from h <bold>A</bold>x<bold>A<sub>b</sub></bold> multiplications, the total computational cost of performing h multiplications of the form <bold>A</bold>x<bold>A<sub>b</sub></bold> is exactly the same as computing the whole <bold>A<sup>2</sup></bold>. However, the former approach communicates more data than the latter because over all h phases, the first input matrix in Figure <xref ref-type="fig" rid="F2">2</xref> is needed to be communicated h times horizontally. The extra communication overhead decreases with increased node counts because the value of h decreases with the increase of total available memory. Once a subset of columns of <bold>A<sup>2</sup></bold> is constructed, it can be sparsified immediately according to the selection, recover and pruning described below. This immediate sparsification is the crux of our approach that allows us to keep memory consumption low while still providing ample parallelism.</p>
        </sec>
      </sec>
      <sec id="SEC2-2-3">
        <title>Parallelizing pruning, selection and recovery</title>
        <p>Pruning non-zero entries that are below a threshold can be trivially parallelized because these pruning decisions are independent of each other. For the selection and recovery operations, we need to identify the k-th largest entry in each column of a matrix. k becomes the selection number S in the selection logic and the recovery number R in the recovery logic. We implemented a simple algorithm, called TournamentSelect (T-S), following the idea of tournament pivoting and optimization, used in solving linear systems (<xref rid="B33" ref-type="bibr">33</xref>). Let P(i,j) be the processor in the <italic>i</italic>th row and <italic>j</italic>th column of the 2D processor grid. Hence, j is the rank of P(i,j) in its processor column. At the beginning of T-S, each processor partially sorts each of its local columns and keeps at most k entries per column. If a column has fewer than k non-zeros, the whole column is kept. The algorithm then performs log(√p) iterations. In the r-th iteration, if the column rank j of a processor P(i,j) is divisible by 2<sup>r</sup>, P(i,j) receives the lists from P(i,j+2<sup>r-1</sup>) when the sending processor exists. After receiving lists from a remote partner, the receiving processor merges its current lists with the received lists and keeps the largest k entries for the next iteration. At the end of log(√p) iterations, the first processor in every processor column stores the largest k entries for the corresponding columns of the matrix. From these lists, T-S returns the k-th largest entries in every column.</p>
      </sec>
      <sec id="SEC2-2-4">
        <title>Parallelizing the inflation step</title>
        <p>The inflation step can be trivially parallelized because each non-zero entry of the matrix can be squared independently. Since the matrices are already distributed, each processor simply computes the square of its own non-zero elements. This step scales perfectly because it does not require any communication.</p>
      </sec>
      <sec id="SEC2-2-5">
        <title>Distributed-memory connected component algorithm</title>
        <p>After the MCL algorithm converges, we identify components in the final graph. These components represent clusters in the original network. For this purpose, we developed a distributed-memory parallel algorithm, following the idea of the Awerbuch-Shiloach algorithm (<xref rid="B34" ref-type="bibr">34</xref>). Relying on well-known graph-matrix duality, we designed our algorithm using a handful of linear-algebraic primitives and a sparse matrix vector multiplication (SpMV) is at the heart of our algorithm. Our parallel connected component algorithm is significantly different from that of MCL, both in terms of algorithmic and implementation techniques. This dual improvement made our implementation several orders of magnitude faster than MCL’s implementation, e.g. an improvement of 3000× on 1024 nodes (Table <xref ref-type="table" rid="tbl2">2</xref>).</p>
      </sec>
      <sec id="SEC2-2-6">
        <title>Input file indexing</title>
        <p>HipMCL currently supports the input file format of MCL where each line specifies an edge (pairs-list), thus making it easier to efficiently read it in parallel. We first get the input size in bytes from the operating system and then assign to each thread a starting position. If the total file is S bytes, the i-th process p<sub>i</sub> (out of p processes) moves its file cursor to location S*(i/p). If this point is in the middle of a line, it fast forward to the beginning of the next line and lets the preceding process p<sub>i-1</sub> read previous partial line in full.</p>
        <p>The key to performance is to have each process open the file in binary format and read using MPI-IO functions with large buffers. In particular, we use ‘MPI_File_read_at’. The parsing of the binary data is done in memory. At this point, all the edges are stored in memory but they are not yet assigned to processes in accordance to the 2D decomposition illustrated in Figure <xref ref-type="fig" rid="F2">2</xref>. Hence, we need to repartition the edges to their correct final destinations. The simplest case is when the original labels are integers from 0 to N-1 where N is the number of vertices. All edges with source vertices in the range [i*√p, (i+1)√p) and target vertices in the range [j*√p, (j+1)√p) are assigned to process P(i,j). The re-partitioning is accomplished by an ‘MPI_Alltoallv’ operation, followed by each process building its local sparse matrix data structure independently.</p>
        <p>In the more general case where vertex labels can be arbitrary strings, we have to first find a mapping from those strings to integers [0,N-1]. The challenge is that the data are already distributed to processes upon reading the file. Our method hashes each vertex identifier to a random number. The hashed values are partitioned to processes in a load balanced way so each vertex is assigned to a unique process. Each process sends all the vertex data it reads from the file to the process assigned to that vertex. The receiving processor then sorts the received data by its hash value and eliminates duplicates. All the processors collectively perform a prefix-sum (‘MPI_Scan’) to calculate the number of vertices owned by all the processors preceding themselves. The i-th processor renumbers the sorted local list with prefix_sum(i), 1+prefix_sum(i),2+prefix_sum(i). This numbering is precisely the mapping from labels to integers [0,N-1] we need to compute. This approach also automatically load balances the input matrix by implicitly applying a random permutation.</p>
      </sec>
      <sec id="SEC2-2-7">
        <title>Implementation details</title>
        <p>Most algorithmic components of HipMCL are based on operations from sparse linear algebra, including sparse matrix-matrix and sparse matrix-vector multiplication. To implement these operations, we relied on basic sparse matrix operations and data structures provided by the Combinatorial BLAS (CombBLAS) library (<xref rid="B35" ref-type="bibr">35</xref>). We have substantially modified and expanded CombBLAS to implement novel parallel algorithms needed for the expansion, pruning and connection components steps. Both HipMCL and CombBLAS are written in C++ and use standard OpenMP and MPI libraries for parallelization. We used g++ 6.1.0 to compile HipMCL on NERSC systems shown in Table <xref ref-type="table" rid="tbl1">1</xref>. For experiments with MCL, we installed mcl-14–137 version from the source.</p>
      </sec>
      <sec id="SEC2-2-8">
        <title>Computational infrastructure</title>
        <p>We ran most of our experiments using two cutting-edge supercomputer systems: Edison and Cori2, hosted at the National Energy Research Scientific Computing Center (NERSC), at Lawrence Berkeley National Laboratory. In addition, we ran one experiment with MCL on a 1-terabyte-memory node using an in-house system at NERSC. We summarize the relevant information of these systems in Table <xref ref-type="table" rid="tbl1">1</xref>. Notably, we did not use any library or software specific to these systems. Our software simply uses standard OpenMP and MPI libraries and can run seamlessly on any other system, including local workstations and laptops.</p>
      </sec>
      <sec id="SEC2-2-9">
        <title>Datasets</title>
        <p>In order to construct several networks to test the behaviour of HipMCL, we collected all viral, eukaryotic and archaeal proteins from the isolate genomes hosted by the IMG platform and created three domain-specific non-redundant datasets at 100% sequence similarity (April 2017). For each dataset, we used the sequence aligner LAST (<xref rid="B36" ref-type="bibr">36</xref>) in order to construct the all-against-all adjacency matrix by keeping all similarities above 30% and at 70% length coverage bidirectionally between the longest and shortest aligned sequences. Each produced matrix was then used as a three-column text input for HipMCL. Details for these datasets are provided in Table <xref ref-type="table" rid="tbl3">3</xref>.</p>
        <table-wrap id="tbl3" orientation="portrait" position="float">
          <label>Table 3.</label>
          <caption>
            <title>Clustering quality results of HipMCL by directly comparing it to the original MCL</title>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th align="left" rowspan="1" colspan="1">Dataset</th>
                <th align="left" rowspan="1" colspan="1">Inflation</th>
                <th align="left" rowspan="1" colspan="1">#clusters from MCL</th>
                <th align="left" rowspan="1" colspan="1">#clusters from HipMCL</th>
                <th align="left" rowspan="1" colspan="1">F-score</th>
                <th align="left" rowspan="1" colspan="1">#mismatched clusters</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">Eukarya |V| = 3 243 106 |E| = 359,744,161</td>
                <td align="left" rowspan="1" colspan="1">1.4</td>
                <td align="left" rowspan="1" colspan="1">228 965</td>
                <td align="left" rowspan="1" colspan="1">228 965</td>
                <td align="left" rowspan="1" colspan="1">0.99</td>
                <td align="left" rowspan="1" colspan="1">8</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">2</td>
                <td align="left" rowspan="1" colspan="1">284 026</td>
                <td align="left" rowspan="1" colspan="1">284 026</td>
                <td align="left" rowspan="1" colspan="1">1.00</td>
                <td align="left" rowspan="1" colspan="1">1</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">4</td>
                <td align="left" rowspan="1" colspan="1">446 216</td>
                <td align="left" rowspan="1" colspan="1">446 216</td>
                <td align="left" rowspan="1" colspan="1">1.00</td>
                <td align="left" rowspan="1" colspan="1">1</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">6</td>
                <td align="left" rowspan="1" colspan="1">597 014</td>
                <td align="left" rowspan="1" colspan="1">597 014</td>
                <td align="left" rowspan="1" colspan="1">1.00</td>
                <td align="left" rowspan="1" colspan="1">0</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Archaea |V| = 1 644 227 |E| = 204 784 551</td>
                <td align="left" rowspan="1" colspan="1">1.4</td>
                <td align="left" rowspan="1" colspan="1">87 559</td>
                <td align="left" rowspan="1" colspan="1">87 559</td>
                <td align="left" rowspan="1" colspan="1">0.99</td>
                <td align="left" rowspan="1" colspan="1">19</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">2</td>
                <td align="left" rowspan="1" colspan="1">107 207</td>
                <td align="left" rowspan="1" colspan="1">107 207</td>
                <td align="left" rowspan="1" colspan="1">1.00</td>
                <td align="left" rowspan="1" colspan="1">0</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">4</td>
                <td align="left" rowspan="1" colspan="1">163 840</td>
                <td align="left" rowspan="1" colspan="1">163 840</td>
                <td align="left" rowspan="1" colspan="1">1.00</td>
                <td align="left" rowspan="1" colspan="1">0</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">6</td>
                <td align="left" rowspan="1" colspan="1">222 937</td>
                <td align="left" rowspan="1" colspan="1">222 937</td>
                <td align="left" rowspan="1" colspan="1">1.00</td>
                <td align="left" rowspan="1" colspan="1">0</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Viruses |V| = 219,715 |E| = 4 583 048</td>
                <td align="left" rowspan="1" colspan="1">1.4</td>
                <td align="left" rowspan="1" colspan="1">34 519</td>
                <td align="left" rowspan="1" colspan="1">34 519</td>
                <td align="left" rowspan="1" colspan="1">1.00</td>
                <td align="left" rowspan="1" colspan="1">0</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">2</td>
                <td align="left" rowspan="1" colspan="1">37 216</td>
                <td align="left" rowspan="1" colspan="1">37 216</td>
                <td align="left" rowspan="1" colspan="1">1.00</td>
                <td align="left" rowspan="1" colspan="1">0</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">4</td>
                <td align="left" rowspan="1" colspan="1">41 835</td>
                <td align="left" rowspan="1" colspan="1">41 835</td>
                <td align="left" rowspan="1" colspan="1">1.00</td>
                <td align="left" rowspan="1" colspan="1">0</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">6</td>
                <td align="left" rowspan="1" colspan="1">45 294</td>
                <td align="left" rowspan="1" colspan="1">45 294</td>
                <td align="left" rowspan="1" colspan="1">1.00</td>
                <td align="left" rowspan="1" colspan="1">0</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="T3TFN1">
              <p>All experiments were run on Edison (NERSC). Column 1: |V| Vertices, |E| Edges. Column 2: The inflation value used for MCL. Column 3: The clusters produced by MCL. Column 4: The number of clusters produced by HipMCL. Column 5: The F-score comparing the results of MCL and HipMCL. As shown, results are identical. Column 6: Very few HipMCL clusters that contain slightly different number of proteins compared to the ones produced by MCL.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <p>In order to further experiment with networks of bigger sizes and various densities, we followed the same approach to generate networks of two categories: (i) three networks containing all pairwise similarities above 30% and at 70% length coverage bidirectionally for all the predicted proteins of the isolate genomes stored in IMG (Isolates 1, 2 and 3) and (ii) similarities of proteins in Metaclust50 (<ext-link ext-link-type="uri" xlink:href="https://metaclust.mmseqs.com/">https://metaclust.mmseqs.com/</ext-link>) dataset which contains predicted genes from metagenomes and metatranscriptomes of assembled contigs from IMG/M and NCBI. For the three isolate datasets, we used different LAST parameters as the more sensitive the LAST is, the bigger and denser the similarity matrix and <italic>vice versa</italic>, the less sensitive the LAST, the sparser the network. Isolate-1 and Isolate-2 were created by utilizing the less sensitive LAST (parameter -m 10) and by using different time snapshots of IMG (April 2016 and April 2017, respectively). The dataset Isolates-3 was created by using the more sensitive LAST (parameter -m 100). The number of vertices and edges of these larger datasets are reported in Table <xref ref-type="table" rid="tbl4">4</xref>.</p>
        <table-wrap id="tbl4" orientation="portrait" position="float">
          <label>Table 4.</label>
          <caption>
            <title>Evaluation of HipMCL clustering for large-scale networks</title>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th align="left" rowspan="1" colspan="1">Network</th>
                <th align="left" rowspan="1" colspan="1">#nodes (millions)</th>
                <th align="left" rowspan="1" colspan="1">#edges (billions)</th>
                <th align="left" rowspan="1" colspan="1">#clusters (millions)</th>
                <th align="left" rowspan="1" colspan="1">HipMCL runtime (h)</th>
                <th align="left" rowspan="1" colspan="1">Running platform</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">Isolate-1</td>
                <td align="left" rowspan="1" colspan="1">47</td>
                <td align="left" rowspan="1" colspan="1">7</td>
                <td align="left" rowspan="1" colspan="1">1.59</td>
                <td align="left" rowspan="1" colspan="1">1</td>
                <td align="left" rowspan="1" colspan="1">1024 nodes on Edison</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Isolate-2</td>
                <td align="left" rowspan="1" colspan="1">69</td>
                <td align="left" rowspan="1" colspan="1">12</td>
                <td align="left" rowspan="1" colspan="1">3.37</td>
                <td align="left" rowspan="1" colspan="1">1.66</td>
                <td align="left" rowspan="1" colspan="1">1024 nodes on Edison</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Isolate-3</td>
                <td align="left" rowspan="1" colspan="1">70</td>
                <td align="left" rowspan="1" colspan="1">68</td>
                <td align="left" rowspan="1" colspan="1">2.88</td>
                <td align="left" rowspan="1" colspan="1">2.41</td>
                <td align="left" rowspan="1" colspan="1">2048 nodes on Cori2</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Metaclust50</td>
                <td align="left" rowspan="1" colspan="1">282</td>
                <td align="left" rowspan="1" colspan="1">37</td>
                <td align="left" rowspan="1" colspan="1">41.52</td>
                <td align="left" rowspan="1" colspan="1">3.23</td>
                <td align="left" rowspan="1" colspan="1">2048 nodes on Cori2</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
    </sec>
  </sec>
  <sec sec-type="results" id="SEC3">
    <title>RESULTS</title>
    <p>The performance and scalability of HipMCL was evaluated using real large biological datasets. We directly compare HipMCL with the original MCL distributed software and show that HipMCL and MCL compute virtually identical clusters, with HipMCL being 100–1000 times faster. The performance numbers we report include both the file I/O operations and the actual clustering (compute) time.</p>
    <sec id="SEC3-1">
      <title>Assessment of clustering quality</title>
      <p>HipMCL and MCL produce identical results given the same input and parameters (e.g. inflation value). For this benchmark, we used three medium-scale networks which can be clustered by both HipMCL and MCL. The properties of these networks (#nodes and #edges) are shown in Table <xref ref-type="table" rid="tbl3">3</xref>.</p>
      <p>In order to show that HipMCL is not sensitive upon parameterization compared to the original MCL distribution, we clustered each network using four different inflation values (1.4, 2, 4 and 6 respectively). Notably, the inflation parameter can be adjusted to obtain clusterings at different levels of granularity. Looking at the third and fourth column of Table <xref ref-type="table" rid="tbl3">3</xref>, we confirmed that HipMCL and MCL always return the same number of clusters.</p>
      <p>As the number of clusters is a poor indicator to directly compare two different clusterings, we use the F-score (or F-measure) to test whether the compositions of the two clusterings also match. The F-score is the harmonic mean of precision (percentage of vertices in a HipMCL cluster which are found in a MCL cluster) and recall (percentage of vertices from a MCL cluster that were recovered by a HipMCL cluster). F-score always takes values between 0 and 1, where 1 indicates a perfect match between two clusterings. The mathematical definition of F-score is further explained in the <xref ref-type="supplementary-material" rid="sup1">supplementary material</xref>.</p>
      <p>Table <xref ref-type="table" rid="tbl3">3</xref> shows that the cluster composition coming from MCL and HipMCL are identical, as the F-score is always close to 1. For fine-grained clusters (obtained with higher values of the inflation parameter), HipMCL and MCL clusters match perfectly. For relatively coarse-grained clusters, we see few, yet insignificant, differences. The number of HipMCL clusters that do not match exactly to a MCL cluster is shown in the rightmost column in Table <xref ref-type="table" rid="tbl3">3</xref>. Even when there is a mismatch, the number of misplaced nodes is insignificant (at most one misplaced node in the mismatched clusters in Table <xref ref-type="table" rid="tbl3">3</xref>). We suspect that those minor differences, stem from different orders of floating-point summation in parallel runs. It is well documented that floating-point addition is not associative, which can affect reproducibility (<xref rid="B37" ref-type="bibr">37</xref>).</p>
    </sec>
    <sec id="SEC3-2">
      <title>Runtime comparison to MCL</title>
      <p>In this benchmark, we compare the runtime of HipMCL and MCL on the Edison supercomputer at NERSC. The most surprising result is the superior performance of HipMCL when running on a large number of nodes, by efficiently exploiting both the increased computational power and the increased aggregate memory of multiple nodes. Figure <xref ref-type="fig" rid="F3">3</xref> shows the runtimes of MCL and HipMCL on the test viral, archaeal, and eukaryotic similarity networks. Notably, these smaller networks are selected so that MCL does not run out of memory and is able to successfully cluster them in a reasonable amount of time.</p>
      <fig id="F3" orientation="portrait" position="float">
        <label>Figure 3.</label>
        <caption>
          <p>Comparison of runtimes of the original MCL and HipMCL using three networks. Both axes are in log scale. Both MCL and HipMCL ran on Edison. MCL ran on a single compute node with 24 cores. HipMCL ran on increasing number of compute nodes to show how the clustering time reduces as we add more computing resources. HipMCL uses all 24 cores available in each node via multithreading. HipMCL ran on up to 64 nodes (1536 cores) for the smaller viruses’ network and on up to 1024 nodes (24 576 cores) for archaea and eukarya networks. The performance improvement of the highest concurrency HipMCL execution compared to single-node MCL and HipMCL executions are shown to the right of each subfigure. See text for details.</p>
        </caption>
        <graphic xlink:href="gkx1313fig3"/>
      </fig>
      <p>For small networks such as the viral network, HipMCL can operate significantly faster than MCL even on a single node. The faster runtime of HipMCL is partially due to the usage of a better parallel algorithm to compute sparse matrix–matrix products. However, for the archaea and eukarya networks, the memory required to store expanded matrices (before pruning) is significantly larger than the available memory on a single node. Therefore, on small concurrency, HipMCL requires many phases to operate with the given available memory, explaining the runtime of HipMCL on a single node for the archaea and eukarya networks. As we add more memory and processors, HipMCL can cluster a network very quickly as shown in the right part of each subfigure (Figure <xref ref-type="fig" rid="F3">3</xref>). For example, HipMCL can cluster the eukarya network in half a minute on 1024 nodes, which is 111 times faster than MCL. Similarly, HipMCL can cluster viruses and archaea networks 36 times and 72 times faster than MCL on 64 and 1024 nodes, respectively. HipMCL is not limited to large supercomputers and its single-node runtime is competitive to the runtime of the baseline MCL implementation.</p>
    </sec>
    <sec id="SEC3-3">
      <title>Speedups of different steps of MCL</title>
      <p>Table <xref ref-type="table" rid="tbl2">2</xref> shows how much time MCL and HipMCL spend for different steps of the algorithm when clustering a network representing sequence similarities of eukaryotic proteins (see Table <xref ref-type="table" rid="tbl3">3</xref>) on 1 and 1024 nodes of Edison, respectively. This shows that HipMCL drastically reduces the running time of inflation and connected components by a factor of 1052× and 3288×, respectively on on 1024 nodes. File I/O time is reduced significantly (83×) within the limit of the hardware. The expansion step becomes significantly faster (39×) in HipMCL; however, it remains the most expensive step. The remarkable improvements in all five steps of MCL are based on novel parallel algorithms and efficient implementations of these algorithms on distributed-memory supercomputers.</p>
    </sec>
    <sec id="SEC3-4">
      <title>Scalability of HipMCL</title>
      <p>The more computing resources (processor and memory) we provide, the faster the HipMCL can cluster a network. Figure <xref ref-type="fig" rid="F3">3</xref> shows that the runtime of HipMCL decreases almost linearly as we increase the number of nodes on Edison. Using 1024 nodes, archaeal and eukaryotic networks can be clustered 672 and 977 times faster than in a single-node. HipMCL scales better when clustering larger networks because of the availability of more work that can keep all processors busy. This can be realized in Figure <xref ref-type="fig" rid="F3">3</xref> where the eukaryotic network scales optimally. It is also possible to achieve superlinear speedups (that is doubling the number of nodes can decrease the runtime by more than 2×) because of the synergistic effect of increased processors and memory. Overall, Figure <xref ref-type="fig" rid="F3">3</xref> demonstrates that the clustering time of HipMCL on high concurrency can be predicted from small-scale experiments. Therefore, to cluster large-scale networks, domain scientists have the freedom to allocate resources depending on the computing budget and expected runtime.</p>
    </sec>
    <sec id="SEC3-5">
      <title>Performance of HipMCL on larger networks</title>
      <p>The real benefit of HipMCL lies in its ability to cluster massive networks that were impossible to cluster with the existing MCL software. Table <xref ref-type="table" rid="tbl4">4</xref> shows four networks created from proteins from isolate genomes hosted on IMG and proteins coming from metagenomes and metatranscriptomes. Of the constructed networks, the smallest one consists of 7 billion edges whereas the largest one of 68 billion edges. HipMCL was able to cluster these networks in a couple of hours using 1024/2048 computing nodes on the two supercomputers at NERSC.</p>
      <p>Notably, none of these networks can be clustered on a single node with MCL due to memory limitations. Attempting to cluster network Isolate1 on a system with 1TB memory and 16 cores failed. MCL was able to finish only one iteration within 5 days. Based on this single finished iteration, we estimated that MCL would have taken 45 days to cluster this smallest network from Table <xref ref-type="table" rid="tbl4">4</xref>. Therefore, clustering bigger networks with MCL can be impractical even with a server with quite a few terabytes of RAM.</p>
    </sec>
    <sec id="SEC3-6">
      <title>Portability of HipMCL</title>
      <p>The HipMCL implementation is highly portable as it is developed with C++ and standard OpenMP and MPI libraries. Hence, it can run seamlessly on any system including laptops, local workstations and large supercomputers. We have extensively tested HipMCL on Intel Haswell, Ivy Bridge and Knights Landing (KNL) processors, using a range of one to two thousand computing nodes, and with up to half a million threads across all processors. HipMCL has successfully clustered networks from thousands to billions of edges. These extensive experiments demonstrate its capability to cluster diverse classes of networks and its portability to run on diverse computing platforms.</p>
    </sec>
    <sec id="SEC3-7">
      <title>I/O performance</title>
      <p>Due to its fast in-memory parsing, HipMCL can read networks at rates close to the peak performance of the NERSC Lustre file systems for large inputs. In particular, HipMCL read and parsed the 1.6 Terabytes Isolate3 network data in ∼1 min using 2025 nodes (both Edison and Cori2 exhibited similar performance). HipMCL also achieves close to linear parallel scaling in terms of I/O throughput until we approach the limits of the hardware. On Edison, HipMCL’s I/O time for the Eukarya network is 300 s on one node and 7 s on 64 nodes. In contrast, MCL takes 600 s to read the same network.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="SEC4">
    <title>DISCUSSION</title>
    <p>HipMCL is a distributed-memory parallel implementation of MCL algorithm which can cluster large-scale networks efficiently and very rapidly. While we see that there is no barrier in the number of processors it can use to run, the memory required to store expanded matrices is significantly larger than any available memory on a single node. Since HipMCL dynamically trades parallelism with memory consumption using its memory-efficient incremental SpGEMM, it is not limited by the memory required to store expanded matrices in unpruned form. However, like MCL, HipMCL also needs to store the expanded matrix after pruning. The density of that intermediate matrix is bounded by the selection, recovery, and pruning parameters. Consider Isolate-1 from Table <xref ref-type="table" rid="tbl4">4</xref>, which has ∼150 non-zeros per column as input. With the default selection parameter, the pruned expanded matrix can have at most 1100 non-zeros per column, effectively requiring seven times more intermediate memory than the input in the worst case. This property is due to the MCL algorithm itself, rather than any specific implementation. Despite this limitation, HipMCL is able to cluster networks × 1000 faster than the original MCL. Compared to the original MCL, HipMCL can easily cluster networks consisting of hundreds of millions of nodes and tens of billions of edges due to its ability to utilize distributed-memory clusters. The denser the network is, the longer it takes HipMCL to perform the file I/O and the first iteration, but density does not significantly affect the other iterations due to selection parameters.</p>
    <p>It should be noted that despite the increase of performance at the clustering step, the input data in the form of sequence similarity matrices is still a requirement, indeed a limiting factor in most cases. Pairwise similarities are generated by sequence comparison software, such as BLAST (<xref rid="B38" ref-type="bibr">38</xref>) or LAST. Apart from the compute time, large disk capacity is also required. These issues are independent of HipMCL, as they require other approaches for the generation of high-quality and high-volume input datasets (<xref rid="B39" ref-type="bibr">39–42</xref>) for subsequent clustering and the detection of protein family clusters.</p>
    <p>While MCL is a very established algorithm for data clustering, it should not be applied to every network blindly, as its performance is dependent on the nature and the topology of a network. In this article for example, we have demonstrated how HipMCL can efficiently cluster large scale SSNs. Other types of networks such as gene expression networks, hierarchical networks, PPI networks, networks from co-occurring terms in the literature or databases and metabolic networks must be treated accordingly. Therefore, checking several topological characteristics of a network prior to any clustering is highly encouraged. For a topological network analysis, tools like NAP (<xref rid="B43" ref-type="bibr">43</xref>), Stanford Network Analysis Project (SNAP) (<xref rid="B44" ref-type="bibr">44</xref>) as well as Cytoscape’s (<xref rid="B23" ref-type="bibr">23</xref>) and Gephi’s (<xref rid="B45" ref-type="bibr">45</xref>) network profilers can be used. One can easily calculate features such as the betweenness centrality, modularity, clustering coefficient, eccentricity, average connectivity, average density and other parameters to get a better sense of the network’s topology. For example, most hierarchical networks have clustering coefficient equal or close to zero and therefore MCL would not be a good choice, as MCL/HipMCL thrives on networks with densely connected neighborhoods. For a very dense network, other algorithms such as SPICi might be more suitable. However, calculating more complicated topological features is not always trivial when analyzing such large-scale networks. Finally, in order to get a more empirical feeling about which clustering algorithm is more suitable for a certain type of network, CLUSTEVAL (<xref rid="B46" ref-type="bibr">46</xref>) is a very useful evaluation platform. CLUSTEVAL offers several biological networks of various types, which were clustered by several clustering algorithms.</p>
    <p>The scope of the present work addresses the parallelization of MCL and does not cover specific aspects of accuracy of the algorithm itself, something that have been covered extensively elsewhere. The original MCL article (<xref rid="B7" ref-type="bibr">7</xref>) as well as several other review articles or original papers provide extensive accuracy comparisons. For example, a direct comparison of MCL against Affinity Propagation in terms of quality and performance has been provided previously (<xref rid="B47" ref-type="bibr">47</xref>). MCL was directly compared against several other algorithms using PPI networks and SSNs (<xref rid="B7" ref-type="bibr">7</xref>). These algorithms include SPICi (<xref rid="B11" ref-type="bibr">11</xref>), MCUPGMA (<xref rid="B48" ref-type="bibr">48</xref>), SPC (<xref rid="B49" ref-type="bibr">49</xref>), MCODE (<xref rid="B13" ref-type="bibr">13</xref>), DPClus (16613608), RNSC (<xref rid="B50" ref-type="bibr">50</xref>) and CFinder (<xref rid="B51" ref-type="bibr">51</xref>). A direct, comparative assessment of four algorithms, namely MCL, RNSC (<xref rid="B50" ref-type="bibr">50</xref>), Super Paramagnetic Clustering (SPC) (<xref rid="B49" ref-type="bibr">49</xref>) and MCODE (<xref rid="B13" ref-type="bibr">13</xref>) was also reported (<xref rid="B10" ref-type="bibr">10</xref>). Finally, MCL was compared against the Spectral Clustering, Affinity Propagation (<xref rid="B15" ref-type="bibr">15</xref>) and RNSC (<xref rid="B50" ref-type="bibr">50</xref>) to show which algorithm performs better in predicting protein complexes (<xref rid="B52" ref-type="bibr">52</xref>). In summary, there is a great plethora of articles mentioned above, reporting detailed benchmarks of several clustering algorithms for different use cases.</p>
    <p>To our knowledge, HipMCL is the first graph-based algorithm that can cluster massive networks efficiently using high performance computing and this indeed represents the main novelty of our work. Overall, the HipMCL implementation presented herein is expected to enable clustering and comparative analysis of very large biological datasets for the foreseeable future, tasks that even recently have seemed unattainable.</p>
  </sec>
  <sec id="SEC5">
    <title>DATA AND SOFTWARE AVAILABILITY</title>
    <p>HipMCL is based on MPI and OpenMP and is freely available under a modified BSD license at: <ext-link ext-link-type="uri" xlink:href="https://bitbucket.org/azadcse/hipmcl/">https://bitbucket.org/azadcse/hipmcl/</ext-link>. Installation instructions and large datasets can be found in the relevant Wiki tab.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>Supplementary Data</label>
      <media xlink:href="gkx1313_supp.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack>
    <title>ACKNOWLEDGEMENTS</title>
    <p>We thank Anton Enright and Stijn van Dongen for their constructive comments on this manuscript.</p>
  </ack>
  <sec id="SEC6">
    <title>SUPPLEMENTARY DATA</title>
    <p><xref ref-type="supplementary-material" rid="sup1">Supplementary Data</xref> are available at NAR Online.</p>
  </sec>
  <sec id="SEC7">
    <title>FUNDING</title>
    <p>US Department of Energy (DOE) Joint Genome Institute [DE-AC02–05CH11231, in part], a DOE Office of Science User Facility; Applied Mathematics program of the DOE Office of Advanced Scientific Computing Research [DE-AC02–05CH11231, in part], Office of Science of the US Department of Energy; Exascale Computing Project (17-SC-20-SC), a collaborative effort of the U.S. Department of Energy Office of Science and the National Nuclear Security Administration. Funding for open access charge: Office of Science of the US Department of Energy [contract DE-AC02-05CH11231].</p>
    <p><italic>Conflict of interest statement</italic>. None declared.</p>
  </sec>
  <ref-list>
    <title>REFERENCES</title>
    <ref id="B1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Barabasi</surname><given-names>A.L.</given-names></name>, <name name-style="western"><surname>Oltvai</surname><given-names>Z.N.</given-names></name></person-group><article-title>Network biology: understanding the cell's functional organization</article-title>. <source>Nat. Rev. Genet.</source><year>2004</year>; <volume>5</volume>:<fpage>101</fpage>–<lpage>113</lpage>.<pub-id pub-id-type="pmid">14735121</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pavlopoulos</surname><given-names>G.A.</given-names></name>, <name name-style="western"><surname>Secrier</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Moschopoulos</surname><given-names>C.N.</given-names></name>, <name name-style="western"><surname>Soldatos</surname><given-names>T.G.</given-names></name>, <name name-style="western"><surname>Kossida</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Aerts</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Schneider</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Bagos</surname><given-names>P.G.</given-names></name></person-group><article-title>Using graph theory to analyze biological networks</article-title>. <source>BioData Min.</source><year>2011</year>; <volume>4</volume>:<fpage>10</fpage>.<pub-id pub-id-type="pmid">21527005</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ouzounis</surname><given-names>C.A.</given-names></name>, <name name-style="western"><surname>Coulson</surname><given-names>R.M.</given-names></name>, <name name-style="western"><surname>Enright</surname><given-names>A.J.</given-names></name>, <name name-style="western"><surname>Kunin</surname><given-names>V.</given-names></name>, <name name-style="western"><surname>Pereira-Leal</surname><given-names>J.B.</given-names></name></person-group><article-title>Classification schemes for protein structure and function</article-title>. <source>Nat. Rev. Genet.</source><year>2003</year>; <volume>4</volume>:<fpage>508</fpage>–<lpage>519</lpage>.<pub-id pub-id-type="pmid">12838343</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Freeman</surname><given-names>T.C.</given-names></name>, <name name-style="western"><surname>Goldovsky</surname><given-names>L.</given-names></name>, <name name-style="western"><surname>Brosch</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>van Dongen</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Maziere</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Grocock</surname><given-names>R.J.</given-names></name>, <name name-style="western"><surname>Freilich</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Thornton</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Enright</surname><given-names>A.J.</given-names></name></person-group><article-title>Construction, visualisation, and clustering of transcription networks from microarray expression data</article-title>. <source>PLoS Comput. Biol.</source><year>2007</year>; <volume>3</volume>:<fpage>2032</fpage>–<lpage>2042</lpage>.<pub-id pub-id-type="pmid">17967053</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Goldovsky</surname><given-names>L.</given-names></name>, <name name-style="western"><surname>Janssen</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Ahren</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Audit</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Cases</surname><given-names>I.</given-names></name>, <name name-style="western"><surname>Darzentas</surname><given-names>N.</given-names></name>, <name name-style="western"><surname>Enright</surname><given-names>A.J.</given-names></name>, <name name-style="western"><surname>Lopez-Bigas</surname><given-names>N.</given-names></name>, <name name-style="western"><surname>Peregrin-Alvarez</surname><given-names>J.M.</given-names></name>, <name name-style="western"><surname>Smith</surname><given-names>M.</given-names></name><etal/></person-group><article-title>CoGenT++: an extensive and extensible data environment for computational genomics</article-title>. <source>Bioinformatics</source>. <year>2005</year>; <volume>21</volume>:<fpage>3806</fpage>–<lpage>3810</lpage>.<pub-id pub-id-type="pmid">16216832</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tatusov</surname><given-names>R.L.</given-names></name>, <name name-style="western"><surname>Koonin</surname><given-names>E.V.</given-names></name>, <name name-style="western"><surname>Lipman</surname><given-names>D.J.</given-names></name></person-group><article-title>A genomic perspective on protein families</article-title>. <source>Science</source>. <year>1997</year>; <volume>278</volume>:<fpage>631</fpage>–<lpage>637</lpage>.<pub-id pub-id-type="pmid">9381173</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Enright</surname><given-names>A.J.</given-names></name>, <name name-style="western"><surname>Van Dongen</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Ouzounis</surname><given-names>C.A.</given-names></name></person-group><article-title>An efficient algorithm for large-scale detection of protein families</article-title>. <source>Nucleic Acids Res.</source><year>2002</year>; <volume>30</volume>:<fpage>1575</fpage>–<lpage>1584</lpage>.<pub-id pub-id-type="pmid">11917018</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Van Dongen</surname><given-names>S.</given-names></name></person-group><article-title>Graph clustering by flow simulation</article-title>. <source>Univ. Utrecht</source>. <year>2000</year>; <comment>Doctoral Dissertation</comment>.</mixed-citation>
    </ref>
    <ref id="B9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Wunsch</surname><given-names>D.</given-names><suffix>2nd</suffix></name></person-group><article-title>Survey of clustering algorithms</article-title>. <source>IEEE Trans. Neural Netw.</source><year>2005</year>; <volume>16</volume>:<fpage>645</fpage>–<lpage>678</lpage>.<pub-id pub-id-type="pmid">15940994</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Brohee</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>van Helden</surname><given-names>J.</given-names></name></person-group><article-title>Evaluation of clustering algorithms for protein-protein interaction networks</article-title>. <source>BMC Bioinformatics</source>. <year>2006</year>; <volume>7</volume>:<fpage>488</fpage>.<pub-id pub-id-type="pmid">17087821</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Singh</surname><given-names>M.</given-names></name></person-group><article-title>SPICi: a fast clustering algorithm for large biological networks</article-title>. <source>Bioinformatics</source>. <year>2010</year>; <volume>26</volume>:<fpage>1105</fpage>–<lpage>1111</lpage>.<pub-id pub-id-type="pmid">20185405</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Blondel</surname><given-names>V.D.</given-names></name>, <name name-style="western"><surname>Guillaume</surname><given-names>J.-L.</given-names></name>, <name name-style="western"><surname>Lambiotte</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Lefebvre</surname><given-names>E.</given-names></name></person-group><article-title>Fast unfolding of communities in large networks</article-title>. <source>J. Stat. Mech. Theory Exp.</source><year>2008</year>; <volume>2008</volume>:<fpage>10008</fpage>.</mixed-citation>
    </ref>
    <ref id="B13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bader</surname><given-names>G.D.</given-names></name>, <name name-style="western"><surname>Hogue</surname><given-names>C.W.</given-names></name></person-group><article-title>An automated method for finding molecular complexes in large protein interaction networks</article-title>. <source>BMC Bioinformatics</source>. <year>2003</year>; <volume>4</volume>:<fpage>2</fpage>.<pub-id pub-id-type="pmid">12525261</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <label>14.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Dhara</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Shukla</surname><given-names>K.</given-names></name></person-group><source>Recent Advances in Information Technology (RAIT)</source>. <year>2012</year>; <publisher-loc>Dhanbad, India</publisher-loc>: <publisher-name>IEEE</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Frey</surname><given-names>B.J.</given-names></name>, <name name-style="western"><surname>Dueck</surname><given-names>D.</given-names></name></person-group><article-title>Clustering by passing messages between data points</article-title>. <source>Science</source>. <year>2007</year>; <volume>315</volume>:<fpage>972</fpage>–<lpage>976</lpage>.<pub-id pub-id-type="pmid">17218491</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <label>16.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Pavlopoulos</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>Iacucci</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Iliopoulos</surname><given-names>I.</given-names></name>, <name name-style="western"><surname>Bagos</surname><given-names>P.</given-names></name></person-group><person-group person-group-type="editor"><name name-style="western"><surname>Tsihrintzis</surname><given-names>GA</given-names></name>, <name name-style="western"><surname>Virvou</surname><given-names>M.</given-names></name></person-group><article-title>Interpreting the omics ‘era’ data</article-title>. <source>Multimedia Services in Intelligent Environments</source>. <year>2013</year>; <volume>25</volume>:<publisher-name>Springer International Publishing</publisher-name><fpage>79</fpage>–<lpage>100</lpage>.</mixed-citation>
    </ref>
    <ref id="B17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pavlopoulos</surname><given-names>G.A.</given-names></name>, <name name-style="western"><surname>Malliarakis</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Papanikolaou</surname><given-names>N.</given-names></name>, <name name-style="western"><surname>Theodosiou</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Enright</surname><given-names>A.J.</given-names></name>, <name name-style="western"><surname>Iliopoulos</surname><given-names>I.</given-names></name></person-group><article-title>Visualizing genome and systems biology: technologies, tools, implementation techniques and trends, past, present and future</article-title>. <source>Gigascience</source>. <year>2015</year>; <volume>4</volume>:<fpage>38</fpage>.<pub-id pub-id-type="pmid">26309733</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pavlopoulos</surname><given-names>G.A.</given-names></name>, <name name-style="western"><surname>Paez-Espino</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Kyrpides</surname><given-names>N.C.</given-names></name>, <name name-style="western"><surname>Iliopoulos</surname><given-names>I.</given-names></name></person-group><article-title>Empirical comparison of visualization tools for larger-scale network analysis</article-title>. <source>Adv. Bioinformatics</source>. <year>2017</year>; <volume>2017</volume>:<fpage>1278932</fpage>.<pub-id pub-id-type="pmid">28804499</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pavlopoulos</surname><given-names>G.A.</given-names></name>, <name name-style="western"><surname>Wegener</surname><given-names>A.L.</given-names></name>, <name name-style="western"><surname>Schneider</surname><given-names>R.</given-names></name></person-group><article-title>A survey of visualization tools for biological network analysis</article-title>. <source>BioData Min</source>. <year>2008</year>; <volume>1</volume>:<fpage>12</fpage>.<pub-id pub-id-type="pmid">19040716</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pavlopoulos</surname><given-names>G.A.</given-names></name>, <name name-style="western"><surname>Moschopoulos</surname><given-names>C.N.</given-names></name>, <name name-style="western"><surname>Hooper</surname><given-names>S.D.</given-names></name>, <name name-style="western"><surname>Schneider</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Kossida</surname><given-names>S.</given-names></name></person-group><article-title>jClust: a clustering and visualization toolbox</article-title>. <source>Bioinformatics</source>. <year>2009</year>; <volume>25</volume>:<fpage>1994</fpage>–<lpage>1996</lpage>.<pub-id pub-id-type="pmid">19454618</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <label>21.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pavlopoulos</surname><given-names>G.A.</given-names></name>, <name name-style="western"><surname>Hooper</surname><given-names>S.D.</given-names></name>, <name name-style="western"><surname>Sifrim</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Schneider</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Aerts</surname><given-names>J.</given-names></name></person-group><article-title>Medusa: A tool for exploring and clustering biological networks</article-title>. <source>BMC Res. Notes</source>. <year>2011</year>; <volume>4</volume>:<fpage>384</fpage>.<pub-id pub-id-type="pmid">21978489</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <label>22.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Auber</surname><given-names>D.</given-names></name></person-group><person-group person-group-type="editor"><name name-style="western"><surname>Jünger</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Mutzel</surname><given-names>P</given-names></name></person-group><article-title>Tulip — A Huge Graph Visualization Framework</article-title>. <source>Graph Drawing Software</source>. <year>2004</year>; <publisher-loc>Berlin, Heidelberg</publisher-loc>: <publisher-name>Springer Berlin Heidelberg</publisher-name><fpage>105</fpage>–<lpage>126</lpage>.</mixed-citation>
    </ref>
    <ref id="B23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shannon</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Markiel</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Ozier</surname><given-names>O.</given-names></name>, <name name-style="western"><surname>Baliga</surname><given-names>N.S.</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>J.T.</given-names></name>, <name name-style="western"><surname>Ramage</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Amin</surname><given-names>N.</given-names></name>, <name name-style="western"><surname>Schwikowski</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Ideker</surname><given-names>T.</given-names></name></person-group><article-title>Cytoscape: a software environment for integrated models of biomolecular interaction networks</article-title>. <source>Genome Res.</source><year>2003</year>; <volume>13</volume>:<fpage>2498</fpage>–<lpage>2504</lpage>.<pub-id pub-id-type="pmid">14597658</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Morris</surname><given-names>J.H.</given-names></name>, <name name-style="western"><surname>Apeltsin</surname><given-names>L.</given-names></name>, <name name-style="western"><surname>Newman</surname><given-names>A.M.</given-names></name>, <name name-style="western"><surname>Baumbach</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Wittkop</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Su</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>Bader</surname><given-names>G.D.</given-names></name>, <name name-style="western"><surname>Ferrin</surname><given-names>T.E.</given-names></name></person-group><article-title>clusterMaker: a multi-algorithm clustering plugin for Cytoscape</article-title>. <source>BMC Bioinformatics</source>. <year>2011</year>; <volume>12</volume>:<fpage>436</fpage>.<pub-id pub-id-type="pmid">22070249</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pereira-Leal</surname><given-names>J.B.</given-names></name>, <name name-style="western"><surname>Enright</surname><given-names>A.J.</given-names></name>, <name name-style="western"><surname>Ouzounis</surname><given-names>C.A.</given-names></name></person-group><article-title>Detection of functional modules from protein interaction networks</article-title>. <source>Proteins</source>. <year>2004</year>; <volume>54</volume>:<fpage>49</fpage>–<lpage>57</lpage>.<pub-id pub-id-type="pmid">14705023</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kyrpides</surname><given-names>N.C.</given-names></name>, <name name-style="western"><surname>Eloe-Fadrosh</surname><given-names>E.A.</given-names></name>, <name name-style="western"><surname>Ivanova</surname><given-names>N.N.</given-names></name></person-group><article-title>Microbiome Data Science: understanding our microbial planet</article-title>. <source>Trends Microbiol.</source><year>2016</year>; <volume>24</volume>:<fpage>425</fpage>–<lpage>427</lpage>.<pub-id pub-id-type="pmid">27197692</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bustamam</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Burrage</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>Hamilton</surname><given-names>N.A.</given-names></name></person-group><article-title>Fast parallel Markov clustering in bioinformatics using massively parallel computing on GPU with CUDA and ELLPACK-R sparse format</article-title>. <source>IEEE/ACM Trans. Comput. Biol. Bioinform.</source><year>2012</year>; <volume>9</volume>:<fpage>679</fpage>–<lpage>692</lpage>.<pub-id pub-id-type="pmid">21483031</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>I.A.</given-names></name>, <name name-style="western"><surname>Markowitz</surname><given-names>V.M.</given-names></name>, <name name-style="western"><surname>Chu</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>Palaniappan</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>Szeto</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Pillay</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Ratner</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Huang</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Andersen</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Huntemann</surname><given-names>M.</given-names></name><etal/></person-group><article-title>IMG/M: integrated genome and metagenome comparative data analysis system</article-title>. <source>Nucleic Acids Res.</source><year>2017</year>; <volume>45</volume>:<fpage>D507</fpage>–<lpage>D516</lpage>.<pub-id pub-id-type="pmid">27738135</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Van Dongen</surname><given-names>S.</given-names></name></person-group><article-title>Graph clustering via a discrete uncoupling process</article-title>. <source>SIAM. J. Matrix Anal. Appl.</source><year>2008</year>; <volume>30</volume>:<fpage>121</fpage>–<lpage>141</lpage>.</mixed-citation>
    </ref>
    <ref id="B30">
      <label>30.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Blum</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Floyd</surname><given-names>R.W.</given-names></name>, <name name-style="western"><surname>Pratt</surname><given-names>V.</given-names></name>, <name name-style="western"><surname>Rivest</surname><given-names>R.L.</given-names></name>, <name name-style="western"><surname>Tarjan</surname><given-names>R.E.</given-names></name></person-group><article-title>Time bounds for selection</article-title>. <source>J. Comput. Syst. Sci.</source><year>1973</year>; <volume>7</volume>:<fpage>448</fpage>–<lpage>461</lpage>.</mixed-citation>
    </ref>
    <ref id="B31">
      <label>31.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Van De Geijn</surname><given-names>R.A.</given-names></name>, <name name-style="western"><surname>Watts</surname><given-names>J.</given-names></name></person-group><article-title>SUMMA: scalable universal matrix multiplication algorithm</article-title>. <source>Concurrency Pract. Exp.</source><year>1997</year>; <volume>9</volume>:<fpage>255</fpage>–<lpage>274</lpage>.</mixed-citation>
    </ref>
    <ref id="B32">
      <label>32.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Buluç</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Gilbert</surname><given-names>J.R.</given-names></name></person-group><article-title>Parallel sparse matrix-matrix multiplication and indexing: Implementation and experiments</article-title>. <source>SIAM J. Sci. Comput.</source><year>2012</year>; <volume>34</volume>:<fpage>C170</fpage>–<lpage>C191</lpage>.</mixed-citation>
    </ref>
    <ref id="B33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Grigori</surname><given-names>L.</given-names></name>, <name name-style="western"><surname>Demmel</surname><given-names>J.W.</given-names></name>, <name name-style="western"><surname>Xiang</surname><given-names>H.</given-names></name></person-group><article-title>CALU: a communication optimal LU factorization algorithm</article-title>. <source>SIAM J. Matrix Anal. Appl.</source><year>2011</year>; <volume>32</volume>:<fpage>1317</fpage>–<lpage>1350</lpage>.</mixed-citation>
    </ref>
    <ref id="B34">
      <label>34.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Awerbuch</surname></name>, <name name-style="western"><surname>Shiloach</surname></name></person-group><article-title>New connectivity and MSF algorithms for shuffle-exchange network and PRAM</article-title>. <source>IEEE Trans. Comput.</source><year>1987</year>; <volume>C-36</volume>:<fpage>1258</fpage>–<lpage>1263</lpage>.</mixed-citation>
    </ref>
    <ref id="B35">
      <label>35.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Buluç</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Gilbert</surname><given-names>J.R.</given-names></name></person-group><article-title>The combinatorial BLAS: design, implementation, and applications</article-title>. <source>Int. J. High Perform. Comput. Appl.</source><year>2011</year>; <volume>25</volume>:<fpage>496</fpage>–<lpage>509</lpage>.</mixed-citation>
    </ref>
    <ref id="B36">
      <label>36.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kielbasa</surname><given-names>S.M.</given-names></name>, <name name-style="western"><surname>Wan</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Sato</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>Horton</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Frith</surname><given-names>M.C.</given-names></name></person-group><article-title>Adaptive seeds tame genomic sequence comparison</article-title>. <source>Genome Res.</source><year>2011</year>; <volume>21</volume>:<fpage>487</fpage>–<lpage>493</lpage>.<pub-id pub-id-type="pmid">21209072</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <label>37.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Demmel</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Nguyen</surname><given-names>H.D.</given-names></name></person-group><article-title>Parallel reproducible summation</article-title>. <source>IEEE Trans. Comput.</source><year>2015</year>; <volume>64</volume>:<fpage>2060</fpage>–<lpage>2070</lpage>.</mixed-citation>
    </ref>
    <ref id="B38">
      <label>38.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Altschul</surname><given-names>S.F.</given-names></name>, <name name-style="western"><surname>Gish</surname><given-names>W.</given-names></name>, <name name-style="western"><surname>Miller</surname><given-names>W.</given-names></name>, <name name-style="western"><surname>Myers</surname><given-names>E.W.</given-names></name>, <name name-style="western"><surname>Lipman</surname><given-names>D.J.</given-names></name></person-group><article-title>Basic local alignment search tool</article-title>. <source>J. Mol. Biol.</source><year>1990</year>; <volume>215</volume>:<fpage>403</fpage>–<lpage>410</lpage>.<pub-id pub-id-type="pmid">2231712</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <label>39.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lam</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Pascoe</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Schaecher</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Lam</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>George</surname><given-names>A.</given-names></name></person-group><article-title>BSW: FPGA-accelerated BLAST-Wrapped Smith-Waterman aligner</article-title>. <year>2013</year>; <comment><italic>2013 International Conference on Reconfigurable Computing and FPGAs</italic></comment>.</mixed-citation>
    </ref>
    <ref id="B40">
      <label>40.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Boratyn</surname><given-names>G.M.</given-names></name>, <name name-style="western"><surname>Schaffer</surname><given-names>A.A.</given-names></name>, <name name-style="western"><surname>Agarwala</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Altschul</surname><given-names>S.F.</given-names></name>, <name name-style="western"><surname>Lipman</surname><given-names>D.J.</given-names></name>, <name name-style="western"><surname>Madden</surname><given-names>T.L.</given-names></name></person-group><article-title>Domain enhanced lookup time accelerated BLAST</article-title>. <source>Biol. Direct.</source><year>2012</year>; <volume>7</volume>:<fpage>12</fpage>.<pub-id pub-id-type="pmid">22510480</pub-id></mixed-citation>
    </ref>
    <ref id="B41">
      <label>41.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ye</surname><given-names>W.</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Xu</surname><given-names>Y.</given-names></name></person-group><article-title>H-BLAST: a fast protein sequence alignment toolkit on heterogeneous computers with GPUs</article-title>. <source>Bioinformatics</source>. <year>2017</year>; <volume>33</volume>:<fpage>1130</fpage>–<lpage>1138</lpage>.<pub-id pub-id-type="pmid">28087515</pub-id></mixed-citation>
    </ref>
    <ref id="B42">
      <label>42.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vaser</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Pavlovic</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Sikic</surname><given-names>M.</given-names></name></person-group><article-title>SWORD—a highly efficient protein database search</article-title>. <source>Bioinformatics</source>. <year>2016</year>; <volume>32</volume>:<fpage>i680</fpage>–<lpage>i684</lpage>.<pub-id pub-id-type="pmid">27587689</pub-id></mixed-citation>
    </ref>
    <ref id="B43">
      <label>43.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Theodosiou</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Efstathiou</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>Papanikolaou</surname><given-names>N.</given-names></name>, <name name-style="western"><surname>Kyrpides</surname><given-names>N.C.</given-names></name>, <name name-style="western"><surname>Bagos</surname><given-names>P.G.</given-names></name>, <name name-style="western"><surname>Iliopoulos</surname><given-names>I.</given-names></name>, <name name-style="western"><surname>Pavlopoulos</surname><given-names>G.A.</given-names></name></person-group><article-title>NAP: the network analysis profiler, a web tool for easier topological analysis and comparison of medium-scale biological networks</article-title>. <source>BMC Res. Notes</source>. <year>2017</year>; <volume>10</volume>:<fpage>278</fpage>.<pub-id pub-id-type="pmid">28705239</pub-id></mixed-citation>
    </ref>
    <ref id="B44">
      <label>44.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Leskovec</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Sosič</surname><given-names>R.</given-names></name></person-group><article-title>SNAP: a general-purpose network analysis and graph-mining library</article-title>. <source>ACM Trans. Intel. Syst. Technol.</source><year>2016</year>; <volume>8</volume>:<fpage>1</fpage>–<lpage>20</lpage>.</mixed-citation>
    </ref>
    <ref id="B45">
      <label>45.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bastian</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Heymann</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Jacomy</surname><given-names>M.</given-names></name></person-group><article-title>Gephi: an open source software for exploring and manipulating networks</article-title>. <year>2009</year>; <comment><italic>International AAAI Conference on Web and Social Media</italic></comment>.</mixed-citation>
    </ref>
    <ref id="B46">
      <label>46.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wiwie</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Baumbach</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Rottger</surname><given-names>R.</given-names></name></person-group><article-title>Comparing the performance of biomedical clustering methods</article-title>. <source>Nat. Methods</source>. <year>2015</year>; <volume>12</volume>:<fpage>1033</fpage>–<lpage>1038</lpage>.<pub-id pub-id-type="pmid">26389570</pub-id></mixed-citation>
    </ref>
    <ref id="B47">
      <label>47.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vlasblom</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Wodak</surname><given-names>S.J.</given-names></name></person-group><article-title>Markov clustering versus affinity propagation for the partitioning of protein interaction graphs</article-title>. <source>BMC Bioinformatics</source>. <year>2009</year>; <volume>10</volume>:<fpage>99</fpage>.<pub-id pub-id-type="pmid">19331680</pub-id></mixed-citation>
    </ref>
    <ref id="B48">
      <label>48.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Loewenstein</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Portugaly</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Fromer</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Linial</surname><given-names>M.</given-names></name></person-group><article-title>Efficient algorithms for accurate hierarchical clustering of huge datasets: tackling the entire protein space</article-title>. <source>Bioinformatics</source>. <year>2008</year>; <volume>24</volume>:<fpage>i41</fpage>–<lpage>i49</lpage>.<pub-id pub-id-type="pmid">18586742</pub-id></mixed-citation>
    </ref>
    <ref id="B49">
      <label>49.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Blatt</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Wiseman</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Domany</surname><given-names>E.</given-names></name></person-group><article-title>Superparamagnetic clustering of data</article-title>. <source>Phys. Rev. Lett.</source><year>1996</year>; <volume>76</volume>:<fpage>3251</fpage>–<lpage>3254</lpage>.<pub-id pub-id-type="pmid">10060920</pub-id></mixed-citation>
    </ref>
    <ref id="B50">
      <label>50.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>King</surname><given-names>A.D.</given-names></name>, <name name-style="western"><surname>Przulj</surname><given-names>N.</given-names></name>, <name name-style="western"><surname>Jurisica</surname><given-names>I.</given-names></name></person-group><article-title>Protein complex prediction via cost-based clustering</article-title>. <source>Bioinformatics</source>. <year>2004</year>; <volume>20</volume>:<fpage>3013</fpage>–<lpage>3020</lpage>.<pub-id pub-id-type="pmid">15180928</pub-id></mixed-citation>
    </ref>
    <ref id="B51">
      <label>51.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Palla</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>Derenyi</surname><given-names>I.</given-names></name>, <name name-style="western"><surname>Farkas</surname><given-names>I.</given-names></name>, <name name-style="western"><surname>Vicsek</surname><given-names>T.</given-names></name></person-group><article-title>Uncovering the overlapping community structure of complex networks in nature and society</article-title>. <source>Nature</source>. <year>2005</year>; <volume>435</volume>:<fpage>814</fpage>–<lpage>818</lpage>.<pub-id pub-id-type="pmid">15944704</pub-id></mixed-citation>
    </ref>
    <ref id="B52">
      <label>52.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Moschopoulos</surname><given-names>C.N.</given-names></name>, <name name-style="western"><surname>Pavlopoulos</surname><given-names>G.A.</given-names></name>, <name name-style="western"><surname>Iacucci</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Aerts</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Likothanassis</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Schneider</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Kossida</surname><given-names>S.</given-names></name></person-group><article-title>Which clustering algorithm is better for predicting protein complexes?</article-title>. <source>BMC Res. Notes</source>. <year>2011</year>; <volume>4</volume>:<fpage>549</fpage>.<pub-id pub-id-type="pmid">22185599</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
