<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_ISCI108073 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEgr4 jpg ?>
<?FILEgr5 jpg ?>
<?FILEfx1 jpg ?>
<?FILEmmc1 pdf ?>
<?FILEmmc2 xlsx ?>
<?FILEmmc3 xlsx ?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">iScience</journal-id>
    <journal-id journal-id-type="iso-abbrev">iScience</journal-id>
    <journal-title-group>
      <journal-title>iScience</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2589-0042</issn>
    <publisher>
      <publisher-name>Elsevier</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10583115</article-id>
    <article-id pub-id-type="pii">S2589-0042(23)02150-8</article-id>
    <article-id pub-id-type="doi">10.1016/j.isci.2023.108073</article-id>
    <article-id pub-id-type="publisher-id">108073</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>HistoMIL: A Python package for training multiple instance learning models on histopathology slides</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au1">
        <name>
          <surname>Pan</surname>
          <given-names>Shi</given-names>
        </name>
        <email>shi.pan@ucl.ac.uk</email>
        <xref rid="aff1" ref-type="aff">1</xref>
        <xref rid="cor1" ref-type="corresp">∗</xref>
      </contrib>
      <contrib contrib-type="author" id="au2">
        <name>
          <surname>Secrier</surname>
          <given-names>Maria</given-names>
        </name>
        <email>m.secrier@ucl.ac.uk</email>
        <xref rid="aff1" ref-type="aff">1</xref>
        <xref rid="fn1" ref-type="fn">2</xref>
        <xref rid="cor2" ref-type="corresp">∗∗</xref>
      </contrib>
      <aff id="aff1"><label>1</label>Department of Genetics, Evolution and Environment, UCL Genetics Institute, University College London, London WC1E 6BT, UK</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>∗</label>Corresponding author <email>shi.pan@ucl.ac.uk</email></corresp>
      <corresp id="cor2"><label>∗∗</label>Corresponding author <email>m.secrier@ucl.ac.uk</email></corresp>
      <fn id="fn1">
        <label>2</label>
        <p id="ntpara0010">Lead contact</p>
      </fn>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>27</day>
      <month>9</month>
      <year>2023</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="collection">
      <day>20</day>
      <month>10</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>27</day>
      <month>9</month>
      <year>2023</year>
    </pub-date>
    <volume>26</volume>
    <issue>10</issue>
    <elocation-id>108073</elocation-id>
    <history>
      <date date-type="received">
        <day>1</day>
        <month>6</month>
        <year>2023</year>
      </date>
      <date date-type="rev-recd">
        <day>21</day>
        <month>8</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>25</day>
        <month>9</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2023 The Author(s)</copyright-statement>
      <copyright-year>2023</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).</license-p>
      </license>
    </permissions>
    <abstract id="abs0010">
      <title>Summary</title>
      <p>Hematoxylin and eosin (H&amp;E) stained slides are widely used in disease diagnosis. Remarkable advances in deep learning have made it possible to detect complex molecular patterns in these histopathology slides, suggesting automated approaches could help inform pathologists’ decisions. Multiple instance learning (MIL) algorithms have shown promise in this context, outperforming transfer learning (TL) methods for various tasks, but their implementation and usage remains complex. We introduce HistoMIL, a Python package designed to streamline the implementation, training and inference process of MIL-based algorithms for computational pathologists and biomedical researchers. It integrates a self-supervised learning module for feature encoding, and a full pipeline encompassing TL and three MIL algorithms: ABMIL, DSMIL, and TransMIL. The PyTorch Lightning framework enables effortless customization and algorithm implementation. We illustrate HistoMIL’s capabilities by building predictive models for 2,487 cancer hallmark genes on breast cancer histology slides, achieving AUROC performances of up to 85%.</p>
    </abstract>
    <abstract abstract-type="graphical" id="abs0015">
      <title>Graphical abstract</title>
      <fig id="undfig1" position="anchor">
        <graphic xlink:href="fx1"/>
      </fig>
    </abstract>
    <abstract abstract-type="author-highlights" id="abs0020">
      <title>Highlights</title>
      <p>
        <list list-type="simple" id="ulist0010">
          <list-item id="u0010">
            <label>•</label>
            <p id="p0010">HistoMIL: a package that streamlines deep learning tasks on histopathology slides</p>
          </list-item>
          <list-item id="u0015">
            <label>•</label>
            <p id="p0015">Comprehensive preprocessing and efficient implementation of MIL algorithms</p>
          </list-item>
          <list-item id="u0020">
            <label>•</label>
            <p id="p0020">Up to 85% AUROC in predicting hallmark gene expression in breast cancer H&amp;E slides</p>
          </list-item>
          <list-item id="u0025">
            <label>•</label>
            <p id="p0025">Cell cycle-related gene expression was most easily captured in pathology slides</p>
          </list-item>
        </list>
      </p>
    </abstract>
    <abstract abstract-type="teaser" id="abs0025">
      <p>Histology; Artificial intelligence; Machine learning</p>
    </abstract>
    <kwd-group id="kwrds0010">
      <title>Subject areas</title>
      <kwd>Histology</kwd>
      <kwd>Artificial intelligence</kwd>
      <kwd>Machine learning</kwd>
    </kwd-group>
  </article-meta>
  <notes>
    <p id="misc9010">Published: September 27, 2023</p>
  </notes>
</front>
<body>
  <sec id="sec1">
    <title>Introduction</title>
    <p id="p0050">Histopathology slides stained with hematoxylin and eosin (H&amp;E) are widely regarded as the gold standard for diagnosing cancer and other diseases. Deep learning (DL)<xref rid="bib5" ref-type="bibr"><sup>1</sup></xref> based approaches have demonstrated remarkable potential for reproducing the workflows of human experts employing such slides in a variety of tasks, e.g., diagnosing cancer and classifying tumor types<xref rid="bib6" ref-type="bibr"><sup>2</sup></xref><sup>,</sup><xref rid="bib7" ref-type="bibr"><sup>3</sup></xref>; segmenting sub-regions at the pixel level to identify nuclei or tissue boundaries<xref rid="bib8" ref-type="bibr"><sup>4</sup></xref><sup>,</sup><xref rid="bib9" ref-type="bibr"><sup>5</sup></xref>; and predicting important clinical metrics such as survival<xref rid="bib10" ref-type="bibr"><sup>6</sup></xref>, recurrence rates<xref rid="bib11" ref-type="bibr"><sup>7</sup></xref><sup>,</sup><xref rid="bib12" ref-type="bibr"><sup>8</sup></xref> and response to treatment.<xref rid="bib13" ref-type="bibr"><sup>9</sup></xref> Several studies have shown that DL-based approaches can also predict more complex molecular characteristics from whole slide image (WSI) datasets, such as microsatellite instability, DNA damage repair deficiencies, mutations or gene expression patterns.<xref rid="bib14" ref-type="bibr"><sup>10</sup></xref><sup>,</sup><xref rid="bib15" ref-type="bibr"><sup>11</sup></xref><sup>,</sup><xref rid="bib16" ref-type="bibr"><sup>12</sup></xref><sup>,</sup><xref rid="bib17" ref-type="bibr"><sup>13</sup></xref></p>
    <p id="p0055">Although transfer learning (TL) was initially widely used for WSI classification tasks, recent research has introduced multiple instance learning (MIL) as an alternative machine learning (ML) framework. MIL is designed to learn from bag-level labels rather than the more precise instance-level labels, and has been shown to outperform TL in certain tasks like survival prediction.<xref rid="bib18" ref-type="bibr"><sup>14</sup></xref><sup>,</sup><xref rid="bib19" ref-type="bibr"><sup>15</sup></xref> However, implementing a MIL-based pipeline to predict molecular labels from WSI datasets presents significant challenges.</p>
    <p id="p0060">Digital pathology WSI datasets consist of large images scanned from original diagnostic tissue slides stained with H&amp;E, often containing billions of pixels in a single file. This brings about specific challenges when applying MIL-based methods: (1) WSI files cannot be directly read by widely used image processing packages such as PIL,<xref rid="bib20" ref-type="bibr"><sup>16</sup></xref> (2) classic architectures of a neural network are designed for lower resolution (i.e., 224 × 224 pixels<xref rid="bib21" ref-type="bibr"><sup>17</sup></xref>), and (3) loading an entire batch of WSIs during training is almost unmanageable and untraceable due to the limited GPU memory. There are various strategies to tackle these issues, and from a toolkit design perspective, they can be categorized into slide reading, preprocessing and ML oriented packages. While earlier implementations primarily focused on user-friendly WSI reading APIs, an increasing number of packages are now being designed to meet the requirements of ML algorithms. However, the existing pipelines do not fully exploit the capabilities of DL approaches.</p>
    <p id="p0065"><bold><italic>WSI reading packages/libraries</italic></bold> such as OpenSlide,<xref rid="bib22" ref-type="bibr"><sup>18</sup></xref> BioFormats,<xref rid="bib23" ref-type="bibr"><sup>19</sup></xref> and HighDicom<xref rid="bib24" ref-type="bibr"><sup>20</sup></xref> offer efficient tools for accessing raw WSI data formats, yet they exhibit shortcomings when integrated with neural network training. Notably, QuPath allows a basic ML pipeline with packages like scikit-learn<xref rid="bib24" ref-type="bibr"><sup>20</sup></xref> through its graphical user interface, but it is not designed for DL or MIL algorithms. Pre-processing packages like PyHIST,<xref rid="bib25" ref-type="bibr"><sup>21</sup></xref> deep-histopath,<xref rid="bib26" ref-type="bibr"><sup>22</sup></xref> Multi_Scale_Tools,<xref rid="bib27" ref-type="bibr"><sup>23</sup></xref> and ASAP<xref rid="bib28" ref-type="bibr"><sup>24</sup></xref> include common processing steps like reading multi-scale information,<xref rid="bib29" ref-type="bibr"><sup>25</sup></xref> tissue segmentation or patching. However, designing a comprehensive and broadly applicable ML-oriented package that encompasses basic components like WSI reading, patch extraction and color normalisation<xref rid="bib30" ref-type="bibr"><sup>26</sup></xref><sup>,</sup><xref rid="bib31" ref-type="bibr"><sup>27</sup></xref> remains difficult. Some preprocessing packages, such as CLAM<xref rid="bib32" ref-type="bibr"><sup>28</sup></xref> and deep-histopath,<xref rid="bib26" ref-type="bibr"><sup>22</sup></xref> deliver quality preprocessing pipelines but may be restrictive when integrated with other algorithms or different datasets.</p>
    <p id="p0070">ML oriented packages like DeepMed,<xref rid="bib31" ref-type="bibr"><sup>27</sup></xref> MONAI<xref rid="bib33" ref-type="bibr"><sup>29</sup></xref> or TIAToolBox<xref rid="bib29" ref-type="bibr"><sup>25</sup></xref> streamline the training process of DL models. For instance, MONAI extends PyTorch’s capabilities for medical data and imaging, providing specialized AI model architectures, transformations and utilities tailored for specific purposes. However, its core code does not specifically optimize for H&amp;E data or various WSI formats. Packages like PathML<xref rid="bib34" ref-type="bibr"><sup>30</sup></xref> aim to deliver richer content for their users by integrating more extensive models such as tissue segmentation. Also, researchers can easily and quickly train DL models on WSI datasets by using the TL protocol. For some of the open source packages, various useful tools have been integrated together, e.g., cell segmentation<xref rid="bib34" ref-type="bibr"><sup>30</sup></xref> or graph aggregator.<xref rid="bib29" ref-type="bibr"><sup>25</sup></xref> Packages like TIAToolBox also offer improved scalability through their module design and unit-testable code.<xref rid="bib29" ref-type="bibr"><sup>25</sup></xref> However, a significant challenge in the context of TL arises when dealing with molecular-level labels lacking pixel-level annotations. This leads to reliance on pseudo-labels, potentially affecting model performance or generalization capabilities.<xref rid="bib1" ref-type="bibr"><sup>31</sup></xref> For instance, consider a scenario where a tumor region is marked as exhibiting high expression of a particular gene that is unique to a subtype of T cells. In such cases, the model being used may end up learning intricate patterns for all cell populations in the tumor tissue, leading to the misclassification of all cells as positive cases. A robust feature encoder can also significantly influence the final results in target classification tasks. Self-Supervised Learning (SSL) has emerged as a prominent training paradigm that leverages the inherent data structure, negating the need for extra labeling. In the context of Whole Slide Imaging (WSI) classification, SSL holds the potential to create highly specialized feature encoders tailored for WSI datasets.<xref rid="bib2" ref-type="bibr"><sup>32</sup></xref> Nevertheless, employing existing SSL packages requires multiple preprocessing steps for WSIs, PNG or JPEG conversion, followed by SSL training with invocation and processing. This essentially entails building a complete SSL pipeline alongside the core training process for WSI classification.</p>
    <p id="p0075">To address some of the challenges highlighted above, we introduce HistoMIL, a DL package based on PyTorch<xref rid="bib35" ref-type="bibr"><sup>33</sup></xref><sup>,</sup><xref rid="bib36" ref-type="bibr"><sup>34</sup></xref> and PyTorch Lightning<xref rid="bib4" ref-type="bibr"><sup>35</sup></xref> that simplifies the training of WSI-based classification models. HistoMIL provides a complete preprocessing pipeline, reducing the complexity of converting raw WSI data into a usable format for DL frameworks. We implement multiple MIL models that allow flexible training against different target labels, along with multiple SSL models that simplify the training of feature encoders. We demonstrate HistoMIL’s performance in predicting over 2,000 cancer-relevant molecular labels in breast cancer H&amp;E slides from the Cancer Genome Atlas (TCGA). Additionally, we pinpoint specific pathways that can be effectively identified within histopathological tissue.</p>
  </sec>
  <sec id="sec2">
    <title>Results</title>
    <p id="p0080">To facilitate the implementation of MIL-based workflows for classifying histopathology images based on specific molecular labels, we have developed a DL Python package entitled HistoMIL. HistoMIL leverages PyTorch and PyTorch Lightning to provide an efficient framework for all essential steps in DL tasks involving H&amp;E slides. These steps encompass preprocessing slide data, training MIL and TL models on the processed dataset to predict molecular labels of interest, and visualizing the classifier performance and prediction results within the examined slides. Below, we showcase the features and capabilities of the package, accompanied by examples of its application to large-scale cancer datasets.</p>
    <sec id="sec2.1">
      <title>Overview of the HistoMIL package</title>
      <p id="p0085">The HistoMIL library is structured into three tiers: <bold>data</bold>, <bold>model</bold>, and <bold>experiment</bold> (<xref rid="fig1" ref-type="fig">Figure 1</xref>A). The <bold>data</bold> level encompasses multiple data preprocessing steps, including WSI reading, tissue segmentation and patching. Similar to other ML-oriented packages, we incorporate functions for image normalization and feature extraction to store intermediate data and expedite model training. Additionally, we introduce a cohort level to handle metadata such as patient details, molecular labels, and other information. The design of HistoMIL aligns with the evolution of relevant packages in the existing literature (<xref rid="fig1" ref-type="fig">Figure 1</xref>B).<fig id="fig1"><label>Figure 1</label><caption><p>Overview of the HistoMIL package design and working pipeline</p><p>(A) The diagram illustrates the relevant modules comprising HistoMIL and the logical structure organizing them. HistoMIL features four levels encompassing file reading, WSI-related processing modules, deep learning algorithms, and experiment management modules.</p><p>(B) The relationship between HistoMIL modules and state-of-the-art libraries. The proposed package covers the entire workflow of WSI reading, preprocessing, and MIL algorithms, a design informed by the primary functionalities of other packages in the literature.</p><p>(C) How HistoMIL’s various modules operate within an actual pipeline. We enumerate a typical processing workflow, including preprocessing, MIL training, and inference components. It is evident that different modules correspond to distinct functional parts within the process.</p></caption><graphic xlink:href="gr1"/></fig></p>
      <p id="p0090">The <bold>data</bold> level draws inspiration from various WSI reading and processing packages. We introduce a universal reader class that allows users to customize different wrappers to access interfaces from other readers. This approach mitigates the limitation of relying solely on one reader package. For instance, OpenSlide,<xref rid="bib22" ref-type="bibr"><sup>18</sup></xref> BioFormats,<xref rid="bib23" ref-type="bibr"><sup>19</sup></xref> HighDicom<xref rid="bib24" ref-type="bibr"><sup>20</sup></xref> are typical tools that provide a user-friendly API to handle WSI data formats. However, the Python interface of OpenSlide<xref rid="bib22" ref-type="bibr"><sup>18</sup></xref> does not support certain formats, such as OME-TIFF.<xref rid="bib22" ref-type="bibr"><sup>18</sup></xref> BioFormats<xref rid="bib23" ref-type="bibr"><sup>19</sup></xref> and QuPath<xref rid="bib24" ref-type="bibr"><sup>20</sup></xref> support a wider array of WSI formats but are heavily dependent on Java environments. In HistoMIL, users only need to adhere to our predefined abstract class, implementing a unified interface function for various reading libraries. This simplifies the conversion of diverse data types into a standard numpy matrix and associated metadata. Consequently, users have the flexibility to choose their preferred reading library based on individual requirements, thereby circumventing issues related to file formats. For the preprocessing steps, we have incorporated features inspired by the strengths of existing packages. For instance, we introduce a multi-threaded preprocessing design inspired by CLAM,<xref rid="bib32" ref-type="bibr"><sup>28</sup></xref> and we allow users to pre-calculate features for the selected patches, a concept drawn from TIAToolBox,<xref rid="bib29" ref-type="bibr"><sup>25</sup></xref> to expedite MIL model training.</p>
      <p id="p0095">The <bold>model</bold> level is divided into two key components: the backbone and the MIL method. Our backbone module seamlessly incorporates the interface of the timm PyTorch image model,<xref rid="bib35" ref-type="bibr"><sup>33</sup></xref> enabling the downloading of various backbone network architectures and pre-trained parameters for feature extraction. This design philosophy is also present in other open source packages like DeepMed<xref rid="bib31" ref-type="bibr"><sup>27</sup></xref> or TIAToolBox.<xref rid="bib29" ref-type="bibr"><sup>25</sup></xref> However, these packages rely on TL methods, which consider only the original slide-level or patient-level labels, assigning them to each partitioned patch as training label information. In TL-based approaches, introducing pseudo-labels becomes necessary when pixel level labels are unavailable, but this introduces additional noise to the model training process. Training a batch of models may also pose challenges for packages like PathML,<xref rid="bib34" ref-type="bibr"><sup>30</sup></xref> as models may learn misleading information from pseudo-labels and become trapped in local optima. In extreme cases, areas containing the cells of interest may be extremely small, with only a handful of patches containing genuinely relevant information. Our package implements multiple MIL methods (ABMIL,<xref rid="bib1" ref-type="bibr"><sup>31</sup></xref> DSMIL,<xref rid="bib2" ref-type="bibr"><sup>32</sup></xref> TransMIL<xref rid="bib3" ref-type="bibr"><sup>36</sup></xref>) alongside a baseline TL model. The entire algorithm implementation is based on PyTorch Lightning,<xref rid="bib4" ref-type="bibr"><sup>35</sup></xref> enabling rapid training and fine-tuning of models for specific labels. Additionally, our package allows users to apply SSL protocols to train the feature extractor from scratch using only WSI datasets. Existing libraries often use pre-trained models from ImageNet as feature extractors. While recent research demonstrates that feature extractor networks trained with SSL on WSIs can enhance final performance,<xref rid="bib18" ref-type="bibr"><sup>14</sup></xref><xref rid="bib2" ref-type="bibr"><sup>32</sup></xref> this feature is not supported by existing ML packages. Furthermore, HistoMIL includes pre-defined parameters as default settings, which allows users to quickly try out and scale-up an algorithm for different targets. Inspired by packages like stainlib,<xref rid="bib30" ref-type="bibr"><sup>26</sup></xref> we have included some utility functions to handle data augmentation, a technique employed to ensure slides with different color ranges are transformed in a uniform way to enable meaningful comparisons.</p>
      <p id="p0100">At the <bold>experiment</bold> level, configuring different models for various datasets and hardware conditions becomes a straightforward task. For instance, researchers may wish to to gain a comprehensive understanding of how multiple biomarkers are spatially distributed by predicting these molecular labels from H&amp;E slides. They might want to predict hundreds of target labels in an initial setting. Most existing packages lack the ability to efficiently scale up algorithms for multiple molecular labels, as they are not designed to deploy a batch of models simultaneously. In HistoMIL, researchers can effortlessly initialize a series of instances to explore the hyperparameter space defined by the customizable <italic>para</italic> class. A <italic>Trainer</italic> class instance will initialize a PyTorch Lightning module, adapting to hardware requirements automatically. Furthermore, third-party tuners like the Ray tuner<xref rid="bib37" ref-type="bibr"><sup>37</sup></xref> can directly utilize these module instances, making it easy to discover the optimal model configuration. <xref rid="fig1" ref-type="fig">Figure 1</xref>C illustrates which modules are used at different stages of a process that employs a MIL algorithm.</p>
      <p id="p0105">Details on the interplay between modules during preprocessing, SSL and MIL can be found in <xref rid="fig2" ref-type="fig">Figure 2</xref>. Moreover, <xref rid="fig2" ref-type="fig">Figure 2</xref> demonstrates how the HistoMIL package streamlines complex pipelines with minimal code. Further information on the implementation of HistoMIL, including data-level design, model training and experimental setup, can be found in the <xref rid="sec4" ref-type="sec">STAR methods</xref> section.<fig id="fig2"><label>Figure 2</label><caption><p>Diagram depicting three primary application scenarios for HistoMIL and their corresponding code blocks</p><p>(A) The invocation method for HistoMIL during batch preprocessing. With predefined processing parameters, HistoMIL can process an entire dataset in one go using just three to four simple commands.</p><p>(B) The code and calling logic for SSL training. HistoMIL facilitates SSL on WSI datasets by predefining SSL-related parameters and modifying the trainer accordingly.</p><p>(C) The code and module calling process for MIL training. Compared to SSL, MIL adds adjustments to the model’s parameters while simplifying the training setup process. For users with access to a GPU server, numerous models can be trained simultaneously by configuring parameters in the bash file.</p></caption><graphic xlink:href="gr2"/></fig></p>
    </sec>
    <sec id="sec2.2">
      <title>Applying HistoMIL to predict cancer hallmarks in H&amp;E stained slides</title>
      <p id="p0110">In this section, we demonstrate the usability and scalability of HistoMIL in WSI-level prediction tasks employing state-of-the-art DL models. One clinically relevant task in cancer that has been recently made feasible by modern computational pathology is assessing the state of specific genes or entire molecular pathways directly within histopathology tissue slides using DL techniques.<xref rid="bib14" ref-type="bibr"><sup>10</sup></xref><sup>,</sup><xref rid="bib38" ref-type="bibr"><sup>38</sup></xref> This rapid evaluation can aid in diagnosis, prognosis and treatment decisions. Typically, such assessments involve gene panel profiling or immunohistochemistry (IHC), but these tests introduce time delays and additional costs, as they are additional steps following visual inspection of routinely stained H&amp;E sections. HistoMIL simplifies this process by providing a fast and efficient implementation for predicting diverse molecular labels in H&amp;E-stained slides.</p>
      <p id="p0115">Here, we showcase HistoMIL’s capacity to streamline the entire analysis workflow required to predict thousands of cancer hallmarks. We used 1,012 H&amp;E-stained slides of breast cancer tissue and matched RNA-seq data from TCGA to train over 8,000 models for the classification of 2,487 cancer hallmark genes. First, we processed the WSI dataset using pre-defined preprocessing functions. This step involves extracting the tissue area from H&amp;E histological images, generating patches automatically, and saving them as H5DF and image files. Subsequently, we trained the Feature Extractor Network using the SSL module and predicted the target gene expression labels using the built-in MIL algorithms. As depicted in <xref rid="fig3" ref-type="fig">Figure 3</xref>A, the implementation of the pipeline is simplified through the use of the generic interface provided by HistoMIL. This reduction in complexity significantly eases the workload for researchers aiming to expand these methods.<fig id="fig3"><label>Figure 3</label><caption><p>Experimental workflow and performance comparison</p><p>(A) The diagram showcases the complete workflow for utilizing HistoMIL in predictive experiments. The raw data from TCGA-BRCA undergoes preprocessing, yielding image patches and associated labels suitable for MIL processing. The experimental task involves predicting gene expression, with the model’s performance displayed as AUROC.</p><p>(B) Comparison of performance distribution among different algorithms in predicting the expression of 2,487 cancer-related genes. Each distribution contains 2,487 data points. The box centerlines depict the medians, and the edges depict the first/third quartiles. TransMIL exhibits superior performance relative to the other algorithms.</p><p>(C) The top 30 genes with the highest AUROC scores. In the test set, the model’s accuracy in predicting gene expression levels (high or low) reaches up to ∼86%. See also <xref rid="mmc1" ref-type="supplementary-material">Table S2</xref>.</p></caption><graphic xlink:href="gr3"/></fig></p>
      <p id="p0120">Despite the clear benefits of utilizing MIL algorithms for H&amp;E-based predictions, researchers with limited coding experience may encounter difficulties when attempting to replicate MIL workflows, where slight variations in code may result in vastly different outcomes. Moreover, the complexity of handling high-dimensional histological data may discourage novice researchers from adopting this approach. In training our models, we adhered to the methodologies detailed in the original papers of TL,<xref rid="bib31" ref-type="bibr"><sup>27</sup></xref> ABMIL,<xref rid="bib1" ref-type="bibr"><sup>31</sup></xref> DSMIL<xref rid="bib2" ref-type="bibr"><sup>32</sup></xref> and TransMIL,<xref rid="bib3" ref-type="bibr"><sup>36</sup></xref> and employed the PyTorch Lightning wrapper designed by the HistoMIL library for the training process. This package simplifies these steps, making them accessible and ensuring reproducibility even for those lacking an in-depth understanding of MIL algorithms. For each algorithm, we trained over 2,000 models to predict the expression levels of selected cancer hallmark genes.</p>
      <p id="p0125">To demonstrate how HistoMIL handles the complex WSI processing steps, we have reproduced the pipeline in an instance notebook. This can be found in the <italic>Notebooks</italic> folder of the HistoMIL GitHub repository (see <xref rid="sec4" ref-type="sec">STAR methods</xref>). We have used code snippets and condensed the code to illustrate the usage of HistoMIL, underscoring its efficiency for WSI prediction tasks. Additionally, our model implementation includes options for computing an attention score or providing patch-level predictions, enabling MIL algorithms to predict the target labels for individual patches. In this particular example, we focused on WSI-level labels, where one expression value is available per gene and per slide. To expedite training and inference times, we did not perform additional data augmentation, yet the models were still able to successfully predict the target label.</p>
      <p id="p0130">Predicting gene expression labels in the benchmark TCGA breast cancer dataset showcases the potential of using HistoMIL on WSI datasets. All of the predictions are based only on WSI data. Our target labels encompass the expression levels of 2,487 different cancer hallmark genes derived from MSigDB, selected as a benchmark task to demonstrate the scalability of our package. We regarded this task as a typical weakly supervised learning challenge and opted for the TransMIL method to learn the aggregation function. We utilized the ResNet-18 pre-trained feature extractor as a backbone network, and all training procedures were executed on the PyTorch Lightning platform with early stopping.</p>
      <p id="p0135">Our experiments involved two training paradigms (TL and MIL) and a total of four different DL algorithms: TL, ABMIL,<xref rid="bib1" ref-type="bibr"><sup>31</sup></xref> DSMIL,<xref rid="bib2" ref-type="bibr"><sup>32</sup></xref> and TransMIL.<xref rid="bib3" ref-type="bibr"><sup>36</sup></xref> The experimental outcomes consistently demonstrated that MIL algorithms generally outperformed TL algorithms when using the same feature encoder (<xref rid="fig3" ref-type="fig">Figure 3</xref>B). Furthermore, variations in performance were observed among different MIL algorithms, with a general trend of TransMIL &gt; DSMIL &gt; ABMIL. Considering the characteristics of these algorithms, it can be reasoned that the introduction of attention mechanisms and neighborhood information have played a significant role in driving these performance differences. Firstly, the slide-level attention mechanism could allow the algorithm to compare the importance of patches within a slide for classification. Additionally, TransMIL, which incorporates neighborhood information, has yielded the best results in our experiments. This is possibly due to the neighbor information capturing the broader changes in tissue structure, which may be relevant in the context of a certain gene being expressed or deactivated.</p>
      <p id="p0140">Among the top genes where different models demonstrated good performance metrics on the test set (<xref rid="fig3" ref-type="fig">Figure 3</xref>C, <xref rid="mmc1" ref-type="supplementary-material">Table S2</xref>) were <italic>MAD2L1, KIF2C</italic> and <italic>AURKA</italic>. These are cell cycle regulators involved in spindle assembly and stabilization, and they promote chromosome segregation during mitosis.<xref rid="bib39" ref-type="bibr"><sup>39</sup></xref><sup>,</sup><xref rid="bib40" ref-type="bibr"><sup>40</sup></xref><sup>,</sup><xref rid="bib41" ref-type="bibr"><sup>41</sup></xref> Other top-ranking genes such as <italic>F2RL2</italic> or <italic>FSHB</italic> are involved in G protein-coupled receptor signaling,<xref rid="bib41" ref-type="bibr"><sup>41</sup></xref> while <italic>MMP2</italic> and <italic>POSTN</italic> are involved in matrix remodeling and cell adhesion.<xref rid="bib42" ref-type="bibr"><sup>42</sup></xref><sup>,</sup><xref rid="bib43" ref-type="bibr"><sup>43</sup></xref> These highly relevant cancer-promoting processes would be expected to leave a clear morphological trace in the tissue. Therefore, the activity of these genes might be more easily identifiable due to linked changes in tumor cell morphology and tissue structure as seen in the H&amp;E slides. In addition, since different MIL algorithms show similar performance in predicting the expression of these genes, it further indicates that the expression patterns of these genes in H&amp;E slides have a certain predictability. These patterns can be easily captured throughout the entire slide using heatmap gradients of expression, thus informing on the spatial distribution of activity for a specific gene throughout the entire tissue (<xref rid="fig4" ref-type="fig">Figure 4</xref>).<fig id="fig4"><label>Figure 4</label><caption><p>Model predictions for gene expression throughout the entire slide</p><p>The first image on the left in the top row is the original tissue slide. The remaining heat maps in the first row demonstrate the spatial distribution of gene activity for selected lowly expressed genes that were predicted by the model. The heat maps in the second row exhibit the spatial distribution of gene activity for highly expressed genes that were accurately predicted by the model. The purple to yellow gradient indicates increasing levels of predicted gene expression per patch. Intratumor heterogeneity of gene expression can be observed, particularly for <italic>LAS1L</italic> or <italic>CYB5R3</italic>.</p></caption><graphic xlink:href="gr4"/></fig></p>
      <p id="p0145">Some genes, on the other hand, are more difficult to predict (<xref rid="mmc1" ref-type="supplementary-material">Table S3</xref>). For instance, the performance of the TransMIL algorithm is poor when predicting the expression levels of <italic>SPRR3</italic>, a marker for terminal squamous cell differentiation linked with tumor progression in early stage breast cancers,<xref rid="bib44" ref-type="bibr"><sup>44</sup></xref> and <italic>PGAM2</italic>, a gene involved in oxidative stress responses.<xref rid="bib45" ref-type="bibr"><sup>45</sup></xref> This could be explained by the complexity of the regulatory processes driven by these genes which may not render clear morphological changes in the cells. This could also be compounded by tumor heterogeneity, e.g., if oxidative stress is present in only part of the cancer tissue. Furthermore, unlike <italic>MAD2L1</italic>, <italic>F2RL2</italic>, and <italic>KIF2C</italic>, genes such as <italic>SPRR3</italic> and <italic>PGAM2</italic> exhibit higher performance variability among different MIL algorithms, and their performance is normally lower than an AUROC of 65%.</p>
      <p id="p0150">To further explore the capability of MIL methods, we focused on the higher-level activity captured by individual genes within the tissue, which can be summarized within hallmark pathways underlying cancer initiation and progression. These included processes such as <italic>angiogenesis</italic>, which plays a crucial role in the formation of blood vessels to support tumor growth, <italic>hypoxia</italic>, triggered when cancer cells experience inadequate levels of oxygen, or the <italic>P53 pathway</italic>, which regulates cell-cycle arrest and apoptosis in response to DNA damage (see <xref rid="mmc3" ref-type="supplementary-material">Table S1</xref> for a complete list). Across 14 key hallmark pathways we observed markedly consistent levels of performance of different MIL algorithms in predicting the expression of the genes involved in the respective pathways (<xref rid="fig5" ref-type="fig">Figure 5</xref>).<fig id="fig5"><label>Figure 5</label><caption><p>Performance of various algorithms on predicting pathway-level activity in cancer</p><p>TransMIL, DSMIL, ABMIL and TL (Transfer Learning) algorithm performance is compared across selected pathways. The boxes depict the AUROC distributions for each algorithm, colored according to the legend. White circles represent the median values, while black lines indicate the mean. The edges of the boxes depict the first/third quartiles and the whiskers depict the minima/maxima. The different hallmark pathway groups are arranged from left to right in descending order of median values according to the TransMIL algorithm.</p></caption><graphic xlink:href="gr5"/></fig></p>
      <p id="p0155">The E2F target genes had the highest average AUROC in our experiments (<xref rid="fig5" ref-type="fig">Figure 5</xref>). This pathway participates in the cell cycle G1/S transition and DNA replication, and is generally upregulated in tumor cells, leading to abnormal cell proliferation.<xref rid="bib46" ref-type="bibr"><sup>46</sup></xref> Such proliferation differences may be more easily captured in the morphology of the cells as well as nuclear staining within H&amp;E-stained sections. In fact, the top 50 highest performing models were for genes involved in cell cycle checkpoints, mitosis and DNA integrity (<xref rid="mmc3" ref-type="supplementary-material">Table S4</xref>), suggesting that cell division-related processes are most easily captured within cancer H&amp;E slides using this methodology. This is not surprising, given the remarkably high performances (&gt;90%) of DL models when it comes to distinguishing tumor areas from normal cells<xref rid="bib47" ref-type="bibr"><sup>47</sup></xref> considering that proliferation is the key defining hallmark of cancer. In contrast, the bottom 50 least performing models (with AUROCs &lt;62%) were for genes involved in tyrosine kinase and apoptotic signaling, nucleotide excision repair and oxidative stress responses (<xref rid="mmc1" ref-type="supplementary-material">Table S5</xref>), which might not be accompanied by visible phenotypic changes in the tumor microenvironment.</p>
      <p id="p0160">Thus, we demonstrate how HistoMIL can be used to assess the detection of thousands of disease-relevant molecules in a speedy and efficient manner, with most informative results obtained for genes that are either expressed throughout the entire slide or not expressed at all within the tissue.</p>
    </sec>
  </sec>
  <sec id="sec3">
    <title>Discussion</title>
    <p id="p0165">Building a MIL pipeline for WSI datasets is a complex undertaking, demanding extensive engineering expertise and a deep understanding of model hyperparameters. The intricacies of WSIs necessitate additional efforts is terms of data retrieval, preprocessing and normalization compared to typical image processing models used in standard classification tasks. In particular, WSI data requires specialized handling during retrieval and preprocessing due to its unique nature. Moreover, since H&amp;E slides are relatively sensitive to color variations, they require an additional normalization step and should carefully select data augmentation steps (i.e., must not undergo extreme cropping operations). In this paper, we introduce HistoMIL, a Python library designed to streamline the pre- and post-processing of WSIs within a MIL-based pipeline. It seamlessly integrates with PyTorch, a widely used DL toolkit, simplifying the training of a batch of MIL-models for diverse targets. The use of PyTorch Lightning further simplifies the training process. Furthermore, our HistoMIL package offers an uncomplicated method for training a feature extractor using an SSL protocol, which can enhance the performance of various methods within the target data domain. We provide a detailed tutorial on how to use this package, tailored to both technical and non-technical users, in our <italic>Notebooks</italic> folder at <ext-link ext-link-type="uri" xlink:href="https://github.com/secrierlab/HistoMIL" id="intref0025">https://github.com/secrierlab/HistoMIL</ext-link>.</p>
    <p id="p0170">HistoMIL aims to facilitate the training and application of MIL algorithms for predicting molecular labels based on digital pathology slides by providing an easy-to-use API. In doing so, we hope to equip users with a comprehensive toolkit that empowers them to concentrate on developing new algorithms and addressing biological questions. To achieve this goal, we have implemented a wide range of modules and functions to streamline the process of training and using SSL and MIL. With HistoMIL’s SSL component, generating a feature extractor for the diagnostic slide dataset becomes effortless, subsequently enabling the prediction of various molecular labels. In our experiments, we illustrate how HistoMIL can be used to predict of the expression status of multiple genes in H&amp;E slides using the built-in MIL algorithms. These pipelines have been implemented in the form of interactive notebooks and can be opened and evaluated on cloud platforms such as Google Colab and Kaggle. This highlights how HistoMIL can be used to greatly simplify the complexity of engineering implementations. We hope that the examples provided will assist other users in incorporating MIL methods as effective tools in their analysis pipelines.</p>
    <p id="p0175">By designing HistoMIL to maintain consistency and ease of use when introducing customized models, we have observed that the data processing steps of different algorithms can be shared due to the repeated use of HistoMIL modules. Additionally, different algorithms can also share the same intermediate data results. Furthermore, batch processing and patch aggregation in HistoMIL come with pre-set values, which greatly reduces the level of difficulty for users. It is important to note that HistoMIL is not limited to the implemented MIL algorithms. Due to its modular and scalable design, users can conveniently implement and modify new algorithms. This flexibility allows for the training of customizable algorithms based on existing work. Therefore, any algorithm implementation involving MIL and WSIs can benefit from the functionality we provide. In addition, many tasks involving WSIs can be easily accommodated through modifications to algorithms or loss functions.</p>
    <p id="p0180">We showcased the strong performance of HistoMIL across more than 2,000 gene expression prediction tasks using a variety of MIL algorithms. Notably, we achieved AUCs exceeding 80% for 130 genes. TransMIL demonstrated superior results compared to other MIL or TL algorithms. We showed that amongst classical cancer hallmark pathways the most identifiable within H&amp;E-stained slides are the ones related to cell proliferation, in line with other findings in the field,<xref rid="bib14" ref-type="bibr"><sup>10</sup></xref> whereas kinase signaling, apoptosis and oxidative stress processes are more difficult to capture from the tissue morphology. This suggests that cancer diagnosis and progression tasks could be more easily automated on histopathology slides than tasks related to targeted treatment decisions. Furthermore, the spatial visualization capabilities of HistoMIL pave the way toward further analyses of spatial patterns of gene activity, which could be used to understand how cancers develop within the tissue and interact with their environment.</p>
    <p id="p0185">HistoMIL is an open-source project that will continue to add additional pre-trained models and functionality. In the future, we intend to enhance the currently available models by training them on additional datasets and including other MIL algorithms. A logical extension of our work is its application in tasks like cancer grading, survival prediction and the exploration of other molecular or clinical labels in different tumor types. Importantly, the package is not restricted to the analysis of cancer datasets alone, and could be readily applied to address a wide range of biomedical questions seeking to uncover meaningful patterns within H&amp;E stained slides. Future versions of HistoMIL could aid with patient diagnosis or help pathologists identify subtle differences in gene activity that might not be readily discernible in traditional histological sections. Extracting features and analyzing results is typically a time-consuming and computationally intensive task, but by using HistoMIL we can train models on a very large scale. This may lead to faster and more efficient analysis of whole slide images in future clinical usage.</p>
    <sec id="sec3.1">
      <title>Limitations of the study</title>
      <p id="p0190">Our experiments have shed light on certain limitations within the existing MIL algorithms. First, as expected, the performance of these algorithms is highly dependent on the image quality of the slides. As the expression levels of some genes are correlated with the color patterns of the images, the models are sensitive to the color changes in the original slides. In our experiment, to maintain simplicity in the training process and reduce extraneous variables, we did not introduce additional data augmentation steps. This might have compromised the generalization capabilities of the resulting model. In practice, different molecular labels may require distinct data augmentation strategies as critical parameters for performance optimization. We underscore the importance of considering this as an essential step for accurate model construction and prediction.</p>
      <p id="p0195">MIL algorithms also incorporate attention mechanisms and neighborhood information, rendering them more susceptible to variations within a single slide, such as local color changes. Moreover, patch-level predictions can introduce biases. Some genes might not be expressed in all regions of the slide, but because a single expression label is available per slide, the models would tend to assume high expression in all areas when this label is high. We frequently observed this phenomenon in our experiments. Whether this reflects a genuine feature of gene activity within the tissue or a model limitation remains to be determined. To address this issue, future work should focus on incorporating additional annotations, such as those obtained through immunohistochemistry, to acquire higher-resolution data for a specific biomarker across the entire tissue rather than relying on a single label per slide.</p>
      <p id="p0200">The interpretability of an AI model in biology and medical research cannot be overstated. In HistoMIL, the ABMIL and TransMIL implementations inherently feature attention scores to demonstrate good interpretability. The DSMIL method generates patch-level prediction values as part of its output. Furthermore, gradient-based interpretability methods can be incorporated with some simple additional functions. In subsequent extensions, we plan to introduce distinct interpretability components to the HistoMIL framework, thereby providing a unified interpretability interface for all the supported models. To further enhance interpretability, future research endeavors in the field should also incorporate experimental validation of the model predictions.</p>
      <p id="p0205">The number of samples in the tested dataset also limits our analyses. In the context of TL, the number of training samples is determined by the number of WSIs in the dataset multiplied by the number of patches within each WSI. In essence, each patch extracted from a WSI is treated as an independent sample, resulting in a substantial number of training samples. However, when employing the MIL method, the number of training samples for the slide-level classifier is equivalent to the number of WSIs in the dataset. This is because MIL treats each WSI as a single sample, rather than considering each patch within the WSI as an individual sample. This fundamental distinction has the potential to impact the generalization capability of the MIL method. It also makes the model aggregation part prone to being misled by certain samples and thus trapped in local optima. Although we selected the largest cancer dataset available from TCGA (TCGA-BRCA), the total number of WSI samples remains relatively limited, which further constrains the potential performance of our models. Furthermore, while TCGA-BRCA is a cancer dataset, due to time and resource constraints, our current experiment did not include testing on other cancer datasets or those related to other diseases. Thus, our biological conclusions are unlikely to be generalizable to other types of cancers or diseases. Our future work will aim to address those issues by expanding the pool of diagnostic slides from other sources.</p>
    </sec>
  </sec>
  <sec id="sec4">
    <title>STAR★Methods</title>
    <sec id="sec4.1">
      <title>Key resources table</title>
      <p id="p0210">
        <table-wrap position="float" id="undtbl1">
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th>REAGENT or RESOURCE</th>
                <th>SOURCE</th>
                <th>IDENTIFIER</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td colspan="3">
                  <bold>Software and algorithms</bold>
                </td>
              </tr>
              <tr>
                <td colspan="3">
                  <hr/>
                </td>
              </tr>
              <tr>
                <td>HistoMIL</td>
                <td>This paper</td>
                <td>GitHub: <ext-link ext-link-type="uri" xlink:href="https://github.com/secrierlab/HistoMIL" id="intref0030">https://github.com/secrierlab/HistoMIL</ext-link>; Zenodo: <ext-link ext-link-type="doi" xlink:href="10.5281/zenodo.8220572" id="intref0035">https://doi.org/10.5281/zenodo.8220572</ext-link></td>
              </tr>
              <tr>
                <td>PyTorch</td>
                <td>Paszke et al.<xref rid="bib36" ref-type="bibr"><sup>34</sup></xref></td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="https://pytorch.org/" id="intref0040">https://pytorch.org/</ext-link>
                </td>
              </tr>
              <tr>
                <td>PyTorch Lightning</td>
                <td>Falcon<xref rid="bib4" ref-type="bibr"><sup>35</sup></xref></td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="https://lightning.ai/docs/pytorch/stable/" id="intref0045">https://lightning.ai/docs/pytorch/stable/</ext-link>
                </td>
              </tr>
              <tr>
                <td>OpenSlide-pytorch</td>
                <td>Goode et al.<xref rid="bib22" ref-type="bibr"><sup>18</sup></xref></td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="https://openslide.org/api/python/" id="intref0050">https://openslide.org/api/python/</ext-link>
                </td>
              </tr>
              <tr>
                <td>ABMIL</td>
                <td>Leiby et al.<xref rid="bib1" ref-type="bibr"><sup>31</sup></xref></td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="https://github.com/AMLab-Amsterdam/AttentionDeepMIL" id="intref0055">https://github.com/AMLab-Amsterdam/AttentionDeepMIL</ext-link>
                </td>
              </tr>
              <tr>
                <td>DSMIL</td>
                <td>Li et al.<xref rid="bib2" ref-type="bibr"><sup>32</sup></xref></td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="https://github.com/binli123/dsmil-wsi" id="intref0060">https://github.com/binli123/dsmil-wsi</ext-link>
                </td>
              </tr>
              <tr>
                <td>TransMIL</td>
                <td>Shao et al.<xref rid="bib3" ref-type="bibr"><sup>36</sup></xref></td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="https://github.com/szc19990412/TransMIL" id="intref0065">https://github.com/szc19990412/TransMIL</ext-link>
                </td>
              </tr>
              <tr>
                <td>TCGAbiolinks</td>
                <td>Colaprico et al.<xref rid="bib48" ref-type="bibr"><sup>48</sup></xref></td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="https://bioconductor.org/packages/release/bioc/html/TCGAbiolinks.html" id="intref0070">https://bioconductor.org/packages/release/bioc/html/TCGAbiolinks.html</ext-link>
                </td>
              </tr>
              <tr>
                <td>msigdbr</td>
                <td>Igor Dolgalev</td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="https://igordot.github.io/msigdbr/" id="intref0075">https://igordot.github.io/msigdbr/</ext-link>
                </td>
              </tr>
              <tr>
                <td>GeneMania</td>
                <td>Warde-Farley et al.<xref rid="bib49" ref-type="bibr"><sup>49</sup></xref></td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="https://genemania.org/" id="intref0080">https://genemania.org/</ext-link>
                </td>
              </tr>
              <tr>
                <td colspan="3">
                  <hr/>
                </td>
              </tr>
              <tr>
                <td colspan="3">
                  <bold>Other</bold>
                </td>
              </tr>
              <tr>
                <td colspan="3">
                  <hr/>
                </td>
              </tr>
              <tr>
                <td>H&amp;E slides and RNA-seq data from the TCGA BRCA cohort (GDC data portal)</td>
                <td>TCGA, Weinstein et al.<xref rid="bib50" ref-type="bibr"><sup>50</sup></xref></td>
                <td><ext-link ext-link-type="uri" xlink:href="https://www.cancer.gov/tcga" id="intref0085">https://www.cancer.gov/tcga</ext-link>; <ext-link ext-link-type="uri" xlink:href="https://portal.gdc.cancer.gov/projects/TCGA-BRCA" id="intref0090">https://portal.gdc.cancer.gov/projects/TCGA-BRCA</ext-link></td>
              </tr>
              <tr>
                <td>MSigDB hallmark gene set</td>
                <td>Liberzon et al.<xref rid="bib51" ref-type="bibr"><sup>51</sup></xref></td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="https://www.gsea-msigdb.org/gsea/msigdb/human/genesets.jsp?collection=H" id="intref0095">https://www.gsea-msigdb.org/gsea/msigdb/human/genesets.jsp?collection=H</ext-link>
                </td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </p>
    </sec>
    <sec id="sec4.2">
      <title>Resource availability</title>
      <sec id="sec4.2.1">
        <title>Lead contact</title>
        <p id="p0215">Further information and requests for resources should be directed to and will be fulfilled by the lead contact, Maria Secrier (<ext-link ext-link-type="uri" xlink:href="mailto:m.secrier@ucl.ac.uk" id="intref0100">m.secrier@ucl.ac.uk</ext-link>).</p>
      </sec>
      <sec id="sec4.2.2">
        <title>Materials availability</title>
        <p id="p0220">This study did not generate new unique reagents.</p>
      </sec>
    </sec>
    <sec id="sec4.3">
      <title>Experimental model and study participant details</title>
      <p id="p0225">This paper analyses publicly available RNA-sequencing and H&amp;E data from TCGA, collected from 1,062 breast cancer patients. All data comply with ethical regulations, with approval and informed consent for collection and sharing already obtained by The Cancer Genome Atlas (TCGA). Sex, age and ethnicity information for the study participants can be found at <ext-link ext-link-type="uri" xlink:href="https://portal.gdc.cancer.gov/projects/TCGA-BRCA" id="intref0105">https://portal.gdc.cancer.gov/projects/TCGA-BRCA</ext-link>. Clinical information was available for 1,048 patients, out of which 99% (n = 1,036) were female and 1% (n = 12) were male, with ages comprised between 26 and 90 (median of 58). The cohort was split by ethnicity as follows: 38 Hispanic or Latino, 842 not Hispanic/Latino, 168 unreported. The tumor stages analyzed in the study were as follows: T1 (n = 266), T2 (n = 607), T3 (n = 134), T4 (n = 39), Tx (n = 2). The sex and ethnicity information were not taken into account in the analysis because breast cancer predominantly affects women (99% in the analyzed cohort) and TCGA data is not sufficiently diverse and powered for ethnicity analyses in the context of our study. This may limit the study’s generalisability. Experimental groups were defined by the median expression of each hallmark gene considered in the study as detailed below.</p>
      <sec id="sec4.3.1">
        <title>Ethics approval and consent to participate</title>
        <p id="p0230">All data employed in this study comply with ethical regulations, with approval and informed consent for collection and sharing already obtained by The Cancer Genome Atlas (TCGA).</p>
      </sec>
    </sec>
    <sec id="sec4.4">
      <title>Method details</title>
      <sec id="sec4.4.1">
        <title>Experimental setting and dataset</title>
        <p id="p0235">In this paper, we exemplify the power and versatility of HistoMIL in oncology-related tasks by building prediction models for 2,487 cancer-related genes.</p>
        <sec id="sec4.4.1.1">
          <title>Dataset</title>
          <p id="p0240">The Cancer Genome Atlas (TCGA) is a collaborative project that aims to characterise the genomic and molecular landscape of various cancers.<xref rid="bib50" ref-type="bibr"><sup>50</sup></xref> We chose TCGA-BRCA as the largest available dataset with diagnostic H&amp;E-stained slides and matched RNA-sequencing which we could use to define cancer-relevant labels (n = 2,487). The TCGA-BRCA dataset contains a large amount of data, including whole slide images, genomic data, clinical data, and more. It includes samples from a diverse patient population, including patients of different ages, races, and ethnicities. This helps avoid biases as potential factors that may affect model performance. As a widely used data source, the TCGA-BRCA dataset has undergone quality control, which ensures that the data is of high quality and well-annotated. This can help reduce the noise and variability in the dataset. We downloaded 1,133 diagnostic WSIs from the Genomic Data Commons (GDC) Data Portal.<xref rid="bib52" ref-type="bibr"><sup>52</sup></xref> In our experiment, we split WSIs into patches, then automatically selected WSIs with more than 1,000 patches, and further removed the images which were blurry or containing marks. This left us with 1,012 WSIs for analyses, which we split into 80% for training and 20% for testing.</p>
        </sec>
        <sec id="sec4.4.1.2">
          <title>Prediction task</title>
          <p id="p0245">The target labels for the experiments were extracted from the RNA-seq profiles of the same TCGA-BRCA patients for which H&amp;E-stained diagnostic slides were also available, downloaded using the TCGAbiolinks package.<xref rid="bib48" ref-type="bibr"><sup>48</sup></xref> We surveyed 2,487 genes involved in various cancer hallmark pathways derived from MSigDB<xref rid="bib51" ref-type="bibr"><sup>51</sup></xref> using the msigdbr R package (see <xref rid="mmc2" ref-type="supplementary-material">Table S1</xref>), and used the FPKM normalised expression values for each gene to categorise tumors as “highly” or “lowly” expressing the respective gene based on the median split. This is a targeted task designed to demonstrate that HistoMIL can assist researchers in rapidly scaling up the classifiers required for their studies. In this task, the HistoMIL package was used to train and validate prediction models for the expression of the selected 2,487 cancer hallmark genes. In the steps involving MIL training and inference, we employed servers powered by A100 and V100 GPUs. Typically, training for a gene expression spans 100 epochs with early stopping. Predictions for some genes might peak as early as the 5th or 6th epoch, triggering the early stopping mechanism. Within the trainer of HistoMIL, we incorporated options for researchers to either randomly split the training set proportionally or choose K-fold validation, facilitating the validation of the training process. By integrating support for TorchMetrics, users can easily select how to evaluate model performance and monitor training progress. Researchers can also determine which slides, generated by specific hospitals, would be placed in a different set as an independent test set to evaluate model performance. This approach facilitates a more robust evaluation of the model’s generalization capabilities. For this experiment, we used an 80% training set and 20% test set strategy to keep a simple experimental setting. We note that we did not employ the SSL module in this analysis example in order to simplify the training process and to focus on comparing the performance of TL and MIL methods.</p>
          <p id="p0250">Finally, functional enrichment analysis was performed using GeneMania.<xref rid="bib49" ref-type="bibr"><sup>49</sup></xref> By predicting the expression of a gene of interest throughout the entire tissue slide, researchers can highlight the features derived from the attention score or gradient vectors.</p>
          <p id="p0255">We believe using HistoMIL could aid in diagnosis or help pathologists identify subtle differences in gene activity that might not be apparent in traditional histological sections. Extracting features and analysing results is normally a time-consuming and computationally intensive task. By using HistoMIL, we can train models on a very large scale. This may lead to faster and more efficient analysis of whole slide images in future clinical usage.</p>
        </sec>
      </sec>
      <sec id="sec4.4.2">
        <title>Module design in HistoMIL</title>
        <sec id="sec4.4.2.1">
          <title>Data level design</title>
          <p id="p0260">The HistoMIL design includes a data level to handle different data formats, preprocessing and other meta-information of the target cohort. Pre-processing WSIs can be time-consuming and computationally expensive in many cases. By integrating a number of configurations and functions, the HistoMIL package offers a functional interface to smoothly run each necessary step in a different pipeline, and save the intermediate output as needed. After initialising a <italic>Slide</italic> instance and reading the slide files, we divide the preprocessing steps into three separate concept categories: <italic>Tissue</italic>, <italic>Patch</italic>, and <italic>Feature</italic>. Each of these concepts is linked with a parameter class to help users modify the preprocessing steps by following their own experimental requirements. A manager class named <italic>WSI_collector</italic> helps users unify the initialisation, calculation and loading process of these concepts to further decrease the complexity of usage.</p>
          <sec id="sec4.4.2.1.1">
            <title>Reading raw WSI data</title>
            <p id="p0265">HistoMIL can handle different data formats by implementing slide_backend wrappers for widely considered packages such as OpenSlide-Python.<xref rid="bib22" ref-type="bibr"><sup>18</sup></xref> This design can help researchers access the raw data of different WSI formats and provides a common interface for the subsequent steps. For instance, the scale pyramid in WSI processing can be more important than other areas as some WSI files naturally include multi-scale representation of the raw data.<xref rid="bib29" ref-type="bibr"><sup>25</sup></xref> A series of downscaled or upscaled versions of the original slide will help the potential deep learning models get more information. HistoMIL will create a common scale pyramid as metadata of a WSI file. Also, researchers can implement customised slide wrappers to access additional slide formats. This modular design enables the rapid integration of customised slide backends into existing pipelines, thereby facilitating the support of different datasets. All these elements are consolidated within the <italic>Slide</italic> instance when using HistoMIL.</p>
          </sec>
          <sec id="sec4.4.2.1.2">
            <title>Tissue masking and patch extraction</title>
            <p id="p0270">WSIs typically involve a considerable area that only includes non-biologically relevant background elements such as glass or marker pen. HistoMIL includes a <italic>Tissue</italic> class to identify and remove these areas by generating a tissue mask. Inspired by other ML-oriented packages, the <italic>Tissue</italic> class includes a wrapper for an Otsu function to categorise pixels into foreground or background.<xref rid="bib32" ref-type="bibr"><sup>28</sup></xref> Some basic morphological operations are integrated to eliminate small holes within the tissue region. Researchers can also implement different wrapper functions for different purposes. Patch extraction, which aims to decrease the cost of GPU memory when training a model, works in a similar manner. A large WSI can be partitioned into small patches by using an integrated function to iteratively walk through the tissue area. All the intermediate data is stored for further usage, and the proposed implementation can avoid filling available memory with enhanced memory efficiency. By using this scale pyramid, we configure the segmentation process in higher scale pyramid levels as our default setting to decrease computational costs. Similarly, the default settings of patch selection functions, which can decide whether current patches need to be processed in a pipeline, are applied at a high pyramid level. We use a multiple processing pool to further accelerate the patching step, similarly to the CLAM package.<xref rid="bib32" ref-type="bibr"><sup>28</sup></xref> All these parameters (such as step size and window size in the patch concept class) can be easily modified in related parameter classes to fit different requests.</p>
          </sec>
          <sec id="sec4.4.2.1.3">
            <title>Feature extraction</title>
            <p id="p0275">A pre-calculation step for the feature extraction part may decrease time costs during training. Some MIL algorithms<xref rid="bib18" ref-type="bibr"><sup>14</sup></xref><xref rid="bib2" ref-type="bibr"><sup>32</sup></xref><sup>,</sup><xref rid="bib3" ref-type="bibr"><sup>36</sup></xref> can work with pre-trained feature extraction networks and choose to calculate feature vectors for each selected patch in a preprocessing step. By following this paradigm, we designed a <italic>Feature</italic> class to synchronise the manipulation and maintenance of the feature vectors generated from each slide. This design can also simplify the potential clustering process for feature vectors within each bag or clustering on the entire dataset. Hence, preprocessing, which would have required the implementation of multiple functions and methods, has been simplified to a few lines of code invocation in HistoMIL (<xref rid="fig2" ref-type="fig">Figure 2</xref>A).</p>
          </sec>
        </sec>
        <sec id="sec4.4.2.2">
          <title>Model training</title>
          <p id="p0280">In HistoMIL, model training is a crucial part especially for the target molecular labels that may be difficult to identify by only considering tissue morphology differences. MIL has been introduced as a higher performing weakly supervised learning protocol for WSI classification. A slide (or WSI) is considered as a bag of instances in the MIL protocol. Each bag may contain hundreds or thousands of instances (patches) that are assembled into slide-level features for classification. To the best of our knowledge, no existing package to date has been designed to handle MIL methods. There is considerable complexity in building a MIL pipeline for WSI classification tasks due to the inherent complexity of WSI formats, heterogeneous nature of cell morphology, and unbalanced target labels. HistoMIL is designed to simplify the implementation of MIL-based pipelines by implementing Self-Supervised Learning and Multiple Instance Learning modules (<xref rid="fig2" ref-type="fig">Figures 2</xref>B and 2C).</p>
          <sec id="sec4.4.2.2.1">
            <title>Self-supervised learning for feature extraction</title>
            <p id="p0285">End-to-End training for a MIL pipeline that consists of feature extractors and aggregators may be computationally expensive, especially in a large WSI. Therefore, the existing model either uses fixed patch features derived from CNNs or simply employs a small number of high-scoring patches to update feature extractors. Inspired by the TL paradigm, the feature extractor network can re-use pre-trained parameters from general image classification domains (e.g., ImageNet). The advantage is that there are different pre-trained feature extraction networks that can be chosen for different tasks. Thus, target classification models can be trained quickly with a pre-trained feature extractor network. However, if a pre-trained feature network is not trained on a WSI dataset, it may require extra effort in the preprocessing steps and the model may not converge. The process of aggregating and classifying may also be susceptible to the issue of overfitting and inadequate supervision, resulting in potential bias and other limitations.</p>
            <p id="p0290">Self-Supervised Learning (SSL) is frequently mentioned as a solution for the mentioned problems. While some studies, such as Lu et al.,<xref rid="bib32" ref-type="bibr"><sup>28</sup></xref> have shown that the SSL phase is a crucial element in a pipeline, integrating the WSI dataset into an SSL pipeline remains difficult. HistoMIL offers an easy way to accomplish this part by using a user-friendly SSL module. To train a feature extractor, the HistoMIL package offers an easy way to apply self-supervised training methods. Firstly, we introduce a trainer class to simplify the training process on a WSI dataset. A wrapper class is created for the timm package<xref rid="bib35" ref-type="bibr"><sup>33</sup></xref> which includes most of the popular backbone architectures and potential pre-trained parameters. Some widely employed SSL methods have been implemented, such as the MoCo<xref rid="bib53" ref-type="bibr"><sup>53</sup></xref><sup>,</sup><xref rid="bib54" ref-type="bibr"><sup>54</sup></xref> and SimCLR<xref rid="bib55" ref-type="bibr"><sup>55</sup></xref> methods. By using these built-in methods, HistoMIL also offers predetermined hyperparameters to further simplify the training process to new users. Also, it is specifically designed for WSI data. Users can simply download raw data from projects such as TCGA and do not need to worry about preprocessing and data augmentation. In addition, utilising the feature extractor network trained by HistoMIL’s SSL module negates the necessity to account for discrepancies in operator implementation that may arise when importing trained parameters from other packages. Moreover, by embedding SSL methods within the HistoMIL package, researchers can effortlessly incorporate the selection of SSL methods and backbone architectures as hyperparameters when optimising their models.</p>
          </sec>
          <sec id="sec4.4.2.2.2">
            <title>MIL methods</title>
            <p id="p0295">To train a slide-level classifier on target labels, HistoMIL offers a variety of algorithms that are ready to use. To the best of our knowledge, no existing package can simplify the training and classification process for MIL algorithms in WSI datasets. Transfer Learning (TL) protocols and Multiple Instance Learning (MIL) protocols have both been widely considered by researchers for classification tasks on WSIs. While packages such as DeepMed<xref rid="bib31" ref-type="bibr"><sup>27</sup></xref> have been designed to apply TL-based models on WSI datasets, there are considerable difficulties in implementing a MIL model for WSI classification tasks. Whole slide images often encompass several gigabytes or even terabytes, and the aggregation function of MIL methods may suffer due to its high-dimensional feature space, with tens of thousands of feature vectors per slide. Moreover, there is the lack of a standard implementation for various MIL algorithms. For molecular labels, training a MIL model may be even more difficult when faced with problems such as heterogeneity of cell morphology and imbalanced data. In some extreme cases, the target classification model may be vulnerable to overfitting, and is unable to explore rich feature representations because of the insufficient supervised signal. All these challenges require users to have considerable experience with implementing and training deep neural networks. This requirement may exclude researchers who need these models but lack the necessary experience. HistoMIL simplifies this aspect by introducing built-in MIL modules with pre-defined hyperparameters.</p>
            <p id="p0300">In HistoMIL, a customizable sampler function in our Dataloader implementation is introduced that only samples some instances from each bag. Different batch sizes can be used if the MIL algorithm needs to sample N instances from each bag. But if all the patches must be read in at once, the batch size should be fixed at 1 and this may lead to an unstable training process. In our default setting, we chose to decrease the initial learning rate and accumulate gradients over the training steps to smooth the optimisation process. We also include a cohort instance during training to handle patient metadata, which means users can also assign pseudo labels for each patch based on the label per slide, and this enables users to train their model using the transfer learning paradigm as well.</p>
          </sec>
        </sec>
      </sec>
      <sec id="sec4.4.3">
        <title>Trainer and experiment manager</title>
        <p id="p0305">A significant amount of time and effort is spent on the tuning process and hyperparameter selection steps when training on WSI datasets. We designed the <italic>trainer</italic> class and <italic>experiment</italic> class in HistoMIL, which are built on the PyTorch Lightning framework, to cut these costs. Our target is to simplify the tuning process for different target labels. The SSL, TL and MIL algorithms mentioned above are implemented using a related PyTorch Lightning module in HistoMIL. Researchers can initialise a PyTorch Lightning instance with HistoMIL, and this instance can be easily fed into third-party tuners such as the Ray tuner package.<xref rid="bib37" ref-type="bibr"><sup>37</sup></xref></p>
      </sec>
      <sec id="sec4.4.4">
        <title>Software and package</title>
        <p id="p0310">HistoMIL utilises Python as the primary implementation language and PyTorch as the underlying deep learning platform.<xref rid="bib35" ref-type="bibr"><sup>33</sup></xref> Using these libraries, HistoMIL can easily implement customised algorithms. Furthermore, we introduced PyTorch Lightning as a higher-level framework to simplify the implementation of training code.<xref rid="bib4" ref-type="bibr"><sup>35</sup></xref> All the built-in algorithms in HistoMIL adhere to PyTorch Lightning’s design philosophy, which decomposes the training process into different functions. This facilitates user customization while also ensuring concise implementation. The HistoMIL package is available at the following GitHub repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/secrierlab/HistoMIL" id="intref0110">https://github.com/secrierlab/HistoMIL</ext-link>. The version of the code used to produce the results of the paper has been deposited at Zenodo (<ext-link ext-link-type="doi" xlink:href="10.5281/zenodo.8220572" id="intref0115">https://doi.org/10.5281/zenodo.8220572</ext-link>).</p>
      </sec>
      <sec id="sec4.4.5">
        <title>Development environment</title>
        <p id="p0315">In the present research, we employed the PyTorch framework due to its renowned flexibility and accessibility for deep learning tasks. The computational experiments were executed on a cluster equipped with NVIDIA A100 or V100 graphics cards and 1TB of storage. Our system runs on the Ubuntu 18.04 operating system, with the Anaconda platform facilitating Python-based development. Additionally, to enhance the training efficiency of our deep learning models, GPU acceleration was leveraged using the CUDA toolkit provided by NVIDIA.</p>
      </sec>
    </sec>
    <sec id="sec4.5">
      <title>Quantification and statistical analysis</title>
      <sec id="sec4.5.1">
        <title>Evaluation metrics</title>
        <p id="p0320">To facilitate a robust comparison with state-of-the-art predictors, we adopted the Area Under the Receiver Operating Characteristic Curve (AUROC) as our primary performance evaluation criterion to validate the efficacy of our model. The dataset was randomly partitioned into training and test sets in an 8:2 ratio. Classification tasks for all datasets utilized in this study pertained to binary classification based on varying levels of gene expression. The AUROC metric encapsulates both sensitivity and specificity of predictions. Generally, a higher AUC score indicates superior classifier performance for a given task. An AUC score of 0.5 signifies random guessing, whereas a score of 1 denotes perfect classification.</p>
        <p id="p0325">The pathway enrichment analysis was performed via a hypergeometric test as implemented in GeneMania. All enriched pathways with an FDR&gt;0.1 were taken into account.</p>
      </sec>
    </sec>
  </sec>
</body>
<back>
  <ref-list id="cebib0010">
    <title>References</title>
    <ref id="bib5">
      <label>1</label>
      <element-citation publication-type="journal" id="sref5">
        <person-group person-group-type="author">
          <name>
            <surname>LeCun</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Deep learning</article-title>
        <source>Nature</source>
        <volume>521</volume>
        <year>2015</year>
        <fpage>436</fpage>
        <lpage>444</lpage>
        <pub-id pub-id-type="pmid">26017442</pub-id>
      </element-citation>
    </ref>
    <ref id="bib6">
      <label>2</label>
      <element-citation publication-type="journal" id="sref6">
        <person-group person-group-type="author">
          <name>
            <surname>Khened</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kori</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Rajkumar</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Krishnamurthi</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Srinivasan</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>A generalized deep learning framework for whole-slide image segmentation and analysis</article-title>
        <source>Sci. Rep.</source>
        <volume>11</volume>
        <year>2021</year>
        <fpage>11579</fpage>
        <pub-id pub-id-type="pmid">34078928</pub-id>
      </element-citation>
    </ref>
    <ref id="bib7">
      <label>3</label>
      <element-citation publication-type="journal" id="sref7">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>C.-L.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>C.-C.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>W.-H.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>S.-H.</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>Y.-C.</given-names>
          </name>
          <name>
            <surname>Hsu</surname>
            <given-names>T.-I.</given-names>
          </name>
          <name>
            <surname>Hsiao</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Yeh</surname>
            <given-names>C.-Y.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>C.-Y.</given-names>
          </name>
        </person-group>
        <article-title>An annotation-free whole-slide training approach to pathological classification of lung cancer types using deep learning</article-title>
        <source>Nat. Commun.</source>
        <volume>12</volume>
        <year>2021</year>
        <fpage>1193</fpage>
        <pub-id pub-id-type="pmid">33608558</pub-id>
      </element-citation>
    </ref>
    <ref id="bib8">
      <label>4</label>
      <element-citation publication-type="journal" id="sref8">
        <person-group person-group-type="author">
          <name>
            <surname>Cui</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Xiong</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>A deep learning algorithm for one-step contour aware nuclei segmentation of histopathology images</article-title>
        <source>Med. Biol. Eng. Comput.</source>
        <volume>57</volume>
        <year>2019</year>
        <fpage>2027</fpage>
        <lpage>2043</lpage>
        <pub-id pub-id-type="pmid">31346949</pub-id>
      </element-citation>
    </ref>
    <ref id="bib9">
      <label>5</label>
      <element-citation publication-type="journal" id="sref9">
        <person-group person-group-type="author">
          <name>
            <surname>Graham</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Vu</surname>
            <given-names>Q.D.</given-names>
          </name>
          <name>
            <surname>Raza</surname>
            <given-names>S.E.A.</given-names>
          </name>
          <name>
            <surname>Azam</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Tsang</surname>
            <given-names>Y.W.</given-names>
          </name>
          <name>
            <surname>Kwak</surname>
            <given-names>J.T.</given-names>
          </name>
          <name>
            <surname>Rajpoot</surname>
            <given-names>N.</given-names>
          </name>
        </person-group>
        <article-title>Hover-net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images</article-title>
        <source>Med. Image Anal.</source>
        <volume>58</volume>
        <year>2019</year>
        <object-id pub-id-type="publisher-id">101563</object-id>
      </element-citation>
    </ref>
    <ref id="bib10">
      <label>6</label>
      <element-citation publication-type="book" id="sref10">
        <person-group person-group-type="author">
          <name>
            <surname>Zhu</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Yao</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <part-title>Wsisa: making survival prediction from whole slide histopathological images</part-title>
        <year>2017</year>
        <fpage>7234</fpage>
        <lpage>7242</lpage>
      </element-citation>
    </ref>
    <ref id="bib11">
      <label>7</label>
      <element-citation publication-type="journal" id="sref11">
        <person-group person-group-type="author">
          <name>
            <surname>Krithiga</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Geetha</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Breast cancer detection, segmentation and classification on histopathology images analysis: a systematic review</article-title>
        <source>Arch. Comput. Methods Eng.</source>
        <volume>28</volume>
        <year>2021</year>
        <fpage>2607</fpage>
        <lpage>2619</lpage>
      </element-citation>
    </ref>
    <ref id="bib12">
      <label>8</label>
      <element-citation publication-type="journal" id="sref12">
        <person-group person-group-type="author">
          <name>
            <surname>Howard</surname>
            <given-names>F.M.</given-names>
          </name>
          <name>
            <surname>Dolezal</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Kochanny</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Khramtsova</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Vickery</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Srisuwananukorn</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Woodard</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Nanda</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Perou</surname>
            <given-names>C.M.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Integration of clinical features and deep learning on pathology for the prediction of breast cancer recurrence assays and risk of recurrence</article-title>
        <source>NPJ Breast Cancer</source>
        <volume>9</volume>
        <year>2023</year>
        <fpage>25</fpage>
        <pub-id pub-id-type="doi">10.1038/s41523-023-00530-5</pub-id>
        <pub-id pub-id-type="pmid">37059742</pub-id>
      </element-citation>
    </ref>
    <ref id="bib13">
      <label>9</label>
      <element-citation publication-type="journal" id="sref13">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>D.M.</given-names>
          </name>
          <name>
            <surname>Rong</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Zhan</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Fujimoto</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Minna</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Wistuba</surname>
            <given-names>I.I.</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Artificial intelligence in lung cancer pathology image analysis</article-title>
        <source>Cancers</source>
        <volume>11</volume>
        <year>2019</year>
        <fpage>1673</fpage>
        <pub-id pub-id-type="pmid">31661863</pub-id>
      </element-citation>
    </ref>
    <ref id="bib14">
      <label>10</label>
      <element-citation publication-type="journal" id="sref14">
        <person-group person-group-type="author">
          <name>
            <surname>Schmauch</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Romagnoni</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Pronier</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Saillard</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Maillé</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Calderaro</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Kamoun</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Sefta</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Toldo</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Zaslavskiy</surname>
            <given-names>M.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A deep learning model to predict RNA-Seq expression of tumours from whole slide images</article-title>
        <source>Nat. Commun.</source>
        <volume>11</volume>
        <year>2020</year>
        <fpage>3877</fpage>
        <pub-id pub-id-type="pmid">32747659</pub-id>
      </element-citation>
    </ref>
    <ref id="bib15">
      <label>11</label>
      <element-citation publication-type="journal" id="sref15">
        <person-group person-group-type="author">
          <name>
            <surname>Schirris</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Gavves</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Nederlof</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Horlings</surname>
            <given-names>H.M.</given-names>
          </name>
          <name>
            <surname>Teuwen</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>DeepSMILE: self-supervised heterogeneity-aware multiple instance learning for DNA damage response defect classification directly from H&amp;E whole-slide images</article-title>
        <comment>Preprint at</comment>
        <source>arXiv</source>
        <year>2021</year>
        <pub-id pub-id-type="doi">10.48550/arXiv.2107.09405</pub-id>
      </element-citation>
    </ref>
    <ref id="bib16">
      <label>12</label>
      <element-citation publication-type="journal" id="sref16">
        <person-group person-group-type="author">
          <name>
            <surname>Kather</surname>
            <given-names>J.N.</given-names>
          </name>
          <name>
            <surname>Pearson</surname>
            <given-names>A.T.</given-names>
          </name>
          <name>
            <surname>Halama</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Jäger</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Krause</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Loosen</surname>
            <given-names>S.H.</given-names>
          </name>
          <name>
            <surname>Marx</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Boor</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Tacke</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Neumann</surname>
            <given-names>U.P.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep learning can predict microsatellite instability directly from histology in gastrointestinal cancer</article-title>
        <source>Nat. Med.</source>
        <volume>25</volume>
        <year>2019</year>
        <fpage>1054</fpage>
        <lpage>1056</lpage>
        <pub-id pub-id-type="doi">10.1038/s41591-019-0462-y</pub-id>
        <pub-id pub-id-type="pmid">31160815</pub-id>
      </element-citation>
    </ref>
    <ref id="bib17">
      <label>13</label>
      <element-citation publication-type="journal" id="sref17">
        <person-group person-group-type="author">
          <name>
            <surname>Coudray</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Ocampo</surname>
            <given-names>P.S.</given-names>
          </name>
          <name>
            <surname>Sakellaropoulos</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Narula</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Snuderl</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Fenyö</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Moreira</surname>
            <given-names>A.L.</given-names>
          </name>
          <name>
            <surname>Razavian</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Tsirigos</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Classification and mutation prediction from non–small cell lung cancer histopathology images using deep learning</article-title>
        <source>Nat. Med.</source>
        <volume>24</volume>
        <year>2018</year>
        <fpage>1559</fpage>
        <lpage>1567</lpage>
        <pub-id pub-id-type="doi">10.1038/s41591-018-0177-5</pub-id>
        <pub-id pub-id-type="pmid">30224757</pub-id>
      </element-citation>
    </ref>
    <ref id="bib18">
      <label>14</label>
      <element-citation publication-type="journal" id="sref18">
        <person-group person-group-type="author">
          <name>
            <surname>Yao</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Jonnagaddala</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Hawkins</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Whole slide images based cancer survival prediction using attention guided deep multiple instance learning networks</article-title>
        <source>Med. Image Anal.</source>
        <volume>65</volume>
        <year>2020</year>
        <object-id pub-id-type="publisher-id">101789</object-id>
      </element-citation>
    </ref>
    <ref id="bib19">
      <label>15</label>
      <element-citation publication-type="book" id="sref19">
        <person-group person-group-type="author">
          <name>
            <surname>Qu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <part-title>Dgmil: Distribution Guided Multiple Instance Learning for Whole Slide Image Classification</part-title>
        <year>2022</year>
        <publisher-name>Springer</publisher-name>
        <fpage>24</fpage>
        <lpage>34</lpage>
      </element-citation>
    </ref>
    <ref id="bib20">
      <label>16</label>
      <element-citation publication-type="book" id="sref20">
        <person-group person-group-type="author">
          <name>
            <surname>Clark</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <part-title>Pillow (Pil Fork) Documentation</part-title>
        <year>2015</year>
        <publisher-name>readthedocs</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib21">
      <label>17</label>
      <element-citation publication-type="book" id="sref21">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <part-title>Deep Residual Learning for Image Recognition</part-title>
        <year>2016</year>
        <fpage>770</fpage>
        <lpage>778</lpage>
      </element-citation>
    </ref>
    <ref id="bib22">
      <label>18</label>
      <element-citation publication-type="journal" id="sref22">
        <person-group person-group-type="author">
          <name>
            <surname>Goode</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Gilbert</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Harkes</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Jukic</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Satyanarayanan</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>OpenSlide: A vendor-neutral software foundation for digital pathology</article-title>
        <source>J. Pathol. Inform.</source>
        <volume>4</volume>
        <year>2013</year>
        <fpage>27</fpage>
        <pub-id pub-id-type="pmid">24244884</pub-id>
      </element-citation>
    </ref>
    <ref id="bib23">
      <label>19</label>
      <element-citation publication-type="book" id="sref23">
        <person-group person-group-type="author">
          <name>
            <surname>Moore</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Linkert</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Blackburn</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Carroll</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Ferguson</surname>
            <given-names>R.K.</given-names>
          </name>
          <name>
            <surname>Flynn</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Gillen</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Leigh</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Lindner</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <part-title>OMERO and Bio-Formats 5: Flexible Access to Large Bioimaging Datasets at Scale</part-title>
        <year>2015</year>
        <publisher-name>SPIE</publisher-name>
        <fpage>37</fpage>
        <lpage>42</lpage>
      </element-citation>
    </ref>
    <ref id="bib24">
      <label>20</label>
      <element-citation publication-type="journal" id="sref24">
        <person-group person-group-type="author">
          <name>
            <surname>Bridge</surname>
            <given-names>C.P.</given-names>
          </name>
          <name>
            <surname>Gorman</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Pieper</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Doyle</surname>
            <given-names>S.W.</given-names>
          </name>
          <name>
            <surname>Lennerz</surname>
            <given-names>J.K.</given-names>
          </name>
          <name>
            <surname>Kalpathy-Cramer</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Clunie</surname>
            <given-names>D.A.</given-names>
          </name>
          <name>
            <surname>Fedorov</surname>
            <given-names>A.Y.</given-names>
          </name>
          <name>
            <surname>Herrmann</surname>
            <given-names>M.D.</given-names>
          </name>
        </person-group>
        <article-title>Highdicom: A python library for standardized encoding of image annotations and machine learning model outputs in pathology and radiology</article-title>
        <source>J. Digit. Imaging</source>
        <volume>35</volume>
        <year>2022</year>
        <fpage>1719</fpage>
        <lpage>1737</lpage>
        <pub-id pub-id-type="pmid">35995898</pub-id>
      </element-citation>
    </ref>
    <ref id="bib25">
      <label>21</label>
      <element-citation publication-type="journal" id="sref25">
        <person-group person-group-type="author">
          <name>
            <surname>Muñoz-Aguirre</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Ntasis</surname>
            <given-names>V.F.</given-names>
          </name>
          <name>
            <surname>Rojas</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Guigó</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>PyHIST: a histological image segmentation tool</article-title>
        <source>PLoS Comput. Biol.</source>
        <volume>16</volume>
        <year>2020</year>
        <object-id pub-id-type="publisher-id">e1008349</object-id>
      </element-citation>
    </ref>
    <ref id="bib26">
      <label>22</label>
      <element-citation publication-type="book" id="sref26">
        <person-group person-group-type="author">
          <name>
            <surname>Korpihalkola</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Sipola</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Kokkonen</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <part-title>Color-optimized One-Pixel Attack against Digital Pathology Images</part-title>
        <year>2021</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>206</fpage>
        <lpage>213</lpage>
      </element-citation>
    </ref>
    <ref id="bib27">
      <label>23</label>
      <element-citation publication-type="journal" id="sref27">
        <person-group person-group-type="author">
          <name>
            <surname>Marini</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Otálora</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Podareanu</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>van Rijthoven</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>van der Laak</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Ciompi</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Müller</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Atzori</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Multi_Scale_Tools: A Python Library to Exploit Multi-Scale Whole Slide Images</article-title>
        <source>Front. Comput. Sci.</source>
        <volume>3</volume>
        <year>2021</year>
        <pub-id pub-id-type="doi">10.3389/fcomp.2021.684521</pub-id>
      </element-citation>
    </ref>
    <ref id="bib28">
      <label>24</label>
      <element-citation publication-type="journal" id="sref28">
        <person-group person-group-type="author">
          <name>
            <surname>Barnabas</surname>
            <given-names>G.D.</given-names>
          </name>
          <name>
            <surname>Goebeler</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Tsui</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Bush</surname>
            <given-names>J.W.</given-names>
          </name>
          <name>
            <surname>Lange</surname>
            <given-names>P.F.</given-names>
          </name>
        </person-group>
        <article-title>ASAP— Automated Sonication-Free Acid-Assisted Proteomes— from Cells and FFPE Tissues</article-title>
        <source>Anal. Chem.</source>
        <volume>95</volume>
        <year>2023</year>
        <fpage>3291</fpage>
        <lpage>3299</lpage>
        <pub-id pub-id-type="pmid">36724070</pub-id>
      </element-citation>
    </ref>
    <ref id="bib29">
      <label>25</label>
      <element-citation publication-type="journal" id="sref29">
        <person-group person-group-type="author">
          <name>
            <surname>Pocock</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Graham</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Vu</surname>
            <given-names>Q.D.</given-names>
          </name>
          <name>
            <surname>Jahanifar</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Deshpande</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Hadjigeorghiou</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Shephard</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Bashir</surname>
            <given-names>R.M.S.</given-names>
          </name>
          <name>
            <surname>Bilal</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>W.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>TIAToolbox as an end-to-end library for advanced tissue image analytics</article-title>
        <source>Commun. Med.</source>
        <volume>2</volume>
        <year>2022</year>
        <fpage>120</fpage>
        <pub-id pub-id-type="pmid">36168445</pub-id>
      </element-citation>
    </ref>
    <ref id="bib30">
      <label>26</label>
      <element-citation publication-type="journal" id="sref30">
        <person-group person-group-type="author">
          <name>
            <surname>Otálora</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Marini</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Podareanu</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Hekster</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Tellez</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>van der Laak</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Müller</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Atzori</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>stainlib: a python library for augmentation and normalization of histopathology H&amp;E images</article-title>
        <comment>Preprint at</comment>
        <source>bioRxiv</source>
        <year>2022</year>
        <pub-id pub-id-type="doi">10.1101/2022.05.17.492245</pub-id>
      </element-citation>
    </ref>
    <ref id="bib31">
      <label>27</label>
      <element-citation publication-type="journal" id="sref31">
        <person-group person-group-type="author">
          <name>
            <surname>van Treeck</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Cifci</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Laleh</surname>
            <given-names>N.G.</given-names>
          </name>
          <name>
            <surname>Saldanha</surname>
            <given-names>O.L.</given-names>
          </name>
          <name>
            <surname>Loeffler</surname>
            <given-names>C.M.</given-names>
          </name>
          <name>
            <surname>Hewitt</surname>
            <given-names>K.J.</given-names>
          </name>
          <name>
            <surname>Muti</surname>
            <given-names>H.S.</given-names>
          </name>
          <name>
            <surname>Echle</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Seibel</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Seraphin</surname>
            <given-names>T.P.</given-names>
          </name>
        </person-group>
        <article-title>DeepMed: A unified, modular pipeline for end-to-end deep learning in computational pathology</article-title>
        <comment>Preprint at</comment>
        <source>bioRxiv</source>
        <year>2021</year>
        <pub-id pub-id-type="doi">10.1101/2021.12.19.473344</pub-id>
      </element-citation>
    </ref>
    <ref id="bib32">
      <label>28</label>
      <element-citation publication-type="journal" id="sref32">
        <person-group person-group-type="author">
          <name>
            <surname>Lu</surname>
            <given-names>M.Y.</given-names>
          </name>
          <name>
            <surname>Williamson</surname>
            <given-names>D.F.K.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>T.Y.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>R.J.</given-names>
          </name>
          <name>
            <surname>Barbieri</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Mahmood</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>Data-efficient and weakly supervised computational pathology on whole-slide images</article-title>
        <source>Nat. Biomed. Eng.</source>
        <volume>5</volume>
        <year>2021</year>
        <fpage>555</fpage>
        <lpage>570</lpage>
        <pub-id pub-id-type="pmid">33649564</pub-id>
      </element-citation>
    </ref>
    <ref id="bib33">
      <label>29</label>
      <element-citation publication-type="journal" id="sref33">
        <person-group person-group-type="author">
          <name>
            <surname>Cardoso</surname>
            <given-names>M.J.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Brown</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Kerfoot</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Murrey</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Myronenko</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>D.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>MONAI: An open-source framework for deep learning in healthcare</article-title>
        <comment>Preprint at</comment>
        <source>arXiv</source>
        <year>2022</year>
        <pub-id pub-id-type="doi">10.48550/arXiv.2211.02701</pub-id>
      </element-citation>
    </ref>
    <ref id="bib34">
      <label>30</label>
      <element-citation publication-type="journal" id="sref34">
        <person-group person-group-type="author">
          <name>
            <surname>Berman</surname>
            <given-names>A.G.</given-names>
          </name>
          <name>
            <surname>Orchard</surname>
            <given-names>W.R.</given-names>
          </name>
          <name>
            <surname>Gehrung</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Markowetz</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>PathML: a unified framework for whole-slide image analysis with deep learning</article-title>
        <comment>Preprint at</comment>
        <source>medRxiv</source>
        <year>2021</year>
        <pub-id pub-id-type="doi">10.1101/2021.07.07.21260138</pub-id>
      </element-citation>
    </ref>
    <ref id="bib1">
      <label>31</label>
      <element-citation publication-type="book" id="sref1">
        <person-group person-group-type="author">
          <name>
            <surname>Leiby</surname>
            <given-names>J.S.</given-names>
          </name>
          <name>
            <surname>Hao</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Kang</surname>
            <given-names>G.H.</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>J.W.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <part-title>Attention-Based Multiple Instance Learning with Self-Supervision to Predict Microsatellite Instability in Colorectal Cancer from Histology Whole-Slide Images</part-title>
        <year>2022</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>3068</fpage>
        <lpage>3071</lpage>
      </element-citation>
    </ref>
    <ref id="bib2">
      <label>32</label>
      <element-citation publication-type="journal" id="sref2">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Eliceiri</surname>
            <given-names>K.W.</given-names>
          </name>
        </person-group>
        <article-title>Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning</article-title>
        <source>Conf. Comput. Vis. Pattern Recognit. Workshops</source>
        <volume>2021</volume>
        <year>2021</year>
        <fpage>14318</fpage>
        <lpage>14328</lpage>
        <pub-id pub-id-type="pmid">35047230</pub-id>
      </element-citation>
    </ref>
    <ref id="bib35">
      <label>33</label>
      <element-citation publication-type="book" id="sref35">
        <person-group person-group-type="author">
          <name>
            <surname>Wightman</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <part-title>Pytorch Image Models</part-title>
        <year>2019</year>
        <publisher-name>GitHub</publisher-name>
        <ext-link ext-link-type="uri" xlink:href="https://github.com/rwightman/pytorch-image-models" id="interref0010">https://github.com/rwightman/pytorch-image-models</ext-link>
      </element-citation>
    </ref>
    <ref id="bib36">
      <label>34</label>
      <element-citation publication-type="book" id="sref36">
        <person-group person-group-type="author">
          <name>
            <surname>Paszke</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Gross</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Massa</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Lerer</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Bradbury</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Chanan</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Killeen</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Gimelshein</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Antiga</surname>
            <given-names>L.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>PyTorch: An Imperative Style, High-Performance Deep Learning Library</part-title>
        <series>Advances in Neural Information Processing Systems</series>
        <volume>32</volume>
        <year>2019</year>
        <publisher-name>Curran Associates Inc</publisher-name>
        <fpage>8024</fpage>
        <lpage>8035</lpage>
      </element-citation>
    </ref>
    <ref id="bib4">
      <label>35</label>
      <element-citation publication-type="journal" id="sref4">
        <person-group person-group-type="author">
          <name>
            <surname>Falcon</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <article-title>The PyTorch Lightning team</article-title>
        <source>Pytorch Lightning</source>
        <volume>3</volume>
        <year>2019</year>
        <fpage>6</fpage>
      </element-citation>
    </ref>
    <ref id="bib3">
      <label>36</label>
      <element-citation publication-type="journal" id="sref3">
        <person-group person-group-type="author">
          <name>
            <surname>Shao</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Bian</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>Transmil: Transformer based correlated multiple instance learning for whole slide image classification</article-title>
        <source>Adv. Neural Inf. Process. Syst.</source>
        <volume>34</volume>
        <year>2021</year>
        <fpage>2136</fpage>
        <lpage>2147</lpage>
      </element-citation>
    </ref>
    <ref id="bib37">
      <label>37</label>
      <element-citation publication-type="journal" id="sref37">
        <person-group person-group-type="author">
          <name>
            <surname>Kiran</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Ozyildirim</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Hyperparameter tuning for deep reinforcement learning applications</article-title>
        <comment>Preprint at</comment>
        <source>arXiv</source>
        <year>2022</year>
        <pub-id pub-id-type="doi">10.48550/arXiv.2201.11182</pub-id>
      </element-citation>
    </ref>
    <ref id="bib38">
      <label>38</label>
      <element-citation publication-type="journal" id="sref38">
        <person-group person-group-type="author">
          <name>
            <surname>Murchan</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Ó'Brien</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>O'Connell</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>McNevin</surname>
            <given-names>C.S.</given-names>
          </name>
          <name>
            <surname>Baird</surname>
            <given-names>A.M.</given-names>
          </name>
          <name>
            <surname>Sheils</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Ó Broin</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Finn</surname>
            <given-names>S.P.</given-names>
          </name>
        </person-group>
        <article-title>Deep Learning of Histopathological Features for the Prediction of Tumour Molecular Genetics</article-title>
        <source>Diagnostics (Basel)</source>
        <volume>11</volume>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">1406</object-id>
        <pub-id pub-id-type="doi">10.3390/diagnostics11081406</pub-id>
      </element-citation>
    </ref>
    <ref id="bib39">
      <label>39</label>
      <element-citation publication-type="journal" id="sref39">
        <person-group person-group-type="author">
          <name>
            <surname>Ji</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Ahmad</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>S.T.</given-names>
          </name>
        </person-group>
        <article-title>Direct interactions of mitotic arrest deficient 1 (MAD1) domains with each other and MAD2 conformers are required for mitotic checkpoint signaling</article-title>
        <source>J. Biol. Chem.</source>
        <volume>293</volume>
        <year>2018</year>
        <fpage>484</fpage>
        <lpage>496</lpage>
        <pub-id pub-id-type="doi">10.1074/jbc.RA117.000555</pub-id>
        <pub-id pub-id-type="pmid">29162720</pub-id>
      </element-citation>
    </ref>
    <ref id="bib40">
      <label>40</label>
      <element-citation publication-type="journal" id="sref40">
        <person-group person-group-type="author">
          <name>
            <surname>Carvalhal</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Ribeiro</surname>
            <given-names>S.A.</given-names>
          </name>
          <name>
            <surname>Arocena</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kasciukovic</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Temme</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Koehler</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Huebner</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Griffis</surname>
            <given-names>E.R.</given-names>
          </name>
        </person-group>
        <article-title>The nucleoporin ALADIN regulates Aurora A localization to ensure robust mitotic spindle formation</article-title>
        <source>Mol. Biol. Cell</source>
        <volume>26</volume>
        <year>2015</year>
        <fpage>3424</fpage>
        <lpage>3438</lpage>
        <pub-id pub-id-type="doi">10.1091/mbc.E15-02-0113</pub-id>
        <pub-id pub-id-type="pmid">26246606</pub-id>
      </element-citation>
    </ref>
    <ref id="bib41">
      <label>41</label>
      <element-citation publication-type="journal" id="sref41">
        <person-group person-group-type="author">
          <name>
            <surname>Tanenbaum</surname>
            <given-names>M.E.</given-names>
          </name>
          <name>
            <surname>Macurek</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>van der Vaart</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Galli</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Akhmanova</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Medema</surname>
            <given-names>R.H.</given-names>
          </name>
        </person-group>
        <article-title>A complex of Kif18b and MCAK promotes microtubule depolymerization and is negatively regulated by Aurora kinases</article-title>
        <source>Curr. Biol.</source>
        <volume>21</volume>
        <year>2011</year>
        <fpage>1356</fpage>
        <lpage>1365</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cub.2011.07.017</pub-id>
        <pub-id pub-id-type="pmid">21820309</pub-id>
      </element-citation>
    </ref>
    <ref id="bib42">
      <label>42</label>
      <element-citation publication-type="journal" id="sref42">
        <person-group person-group-type="author">
          <name>
            <surname>Nelson</surname>
            <given-names>A.R.</given-names>
          </name>
          <name>
            <surname>Fingleton</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Rothenberg</surname>
            <given-names>M.L.</given-names>
          </name>
          <name>
            <surname>Matrisian</surname>
            <given-names>L.M.</given-names>
          </name>
        </person-group>
        <article-title>Matrix metalloproteinases: biologic activity and clinical implications</article-title>
        <source>J. Clin. Oncol.</source>
        <volume>18</volume>
        <year>2000</year>
        <fpage>1135</fpage>
        <lpage>1149</lpage>
        <pub-id pub-id-type="doi">10.1200/JCO.2000.18.5.1135</pub-id>
        <pub-id pub-id-type="pmid">10694567</pub-id>
      </element-citation>
    </ref>
    <ref id="bib43">
      <label>43</label>
      <element-citation publication-type="journal" id="sref43">
        <person-group person-group-type="author">
          <name>
            <surname>Gillan</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Matei</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Fishman</surname>
            <given-names>D.A.</given-names>
          </name>
          <name>
            <surname>Gerbin</surname>
            <given-names>C.S.</given-names>
          </name>
          <name>
            <surname>Karlan</surname>
            <given-names>B.Y.</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>D.D.</given-names>
          </name>
        </person-group>
        <article-title>Periostin secreted by epithelial ovarian carcinoma is a ligand for alpha(V)beta(3) and alpha(V)beta(5) integrins and promotes cell motility</article-title>
        <source>Cancer Res.</source>
        <volume>62</volume>
        <year>2002</year>
        <fpage>5358</fpage>
        <lpage>5364</lpage>
        <pub-id pub-id-type="pmid">12235007</pub-id>
      </element-citation>
    </ref>
    <ref id="bib44">
      <label>44</label>
      <element-citation publication-type="journal" id="sref44">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>J.C.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>J.H.</given-names>
          </name>
          <name>
            <surname>Cho</surname>
            <given-names>Y.K.</given-names>
          </name>
          <name>
            <surname>Jung</surname>
            <given-names>C.S.</given-names>
          </name>
          <name>
            <surname>Ahn</surname>
            <given-names>S.H.</given-names>
          </name>
          <name>
            <surname>Gong</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>Y.S.</given-names>
          </name>
          <name>
            <surname>Cho</surname>
            <given-names>D.-H.</given-names>
          </name>
        </person-group>
        <article-title>Expression of SPRR3 is associated with tumor cell proliferation in less advanced stages of breast cancer</article-title>
        <source>Breast Cancer Res. Treat.</source>
        <volume>133</volume>
        <year>2012</year>
        <fpage>909</fpage>
        <lpage>916</lpage>
        <pub-id pub-id-type="pmid">22076481</pub-id>
      </element-citation>
    </ref>
    <ref id="bib45">
      <label>45</label>
      <element-citation publication-type="journal" id="sref45">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Lv</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>C.-X.</given-names>
          </name>
          <name>
            <surname>Guan</surname>
            <given-names>K.-L.</given-names>
          </name>
          <name>
            <surname>Lei</surname>
            <given-names>Q.-Y.</given-names>
          </name>
          <name>
            <surname>Xiong</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Oxidative stress activates SIRT2 to deacetylate and stimulate phosphoglycerate mutase</article-title>
        <source>Cancer Res.</source>
        <volume>74</volume>
        <year>2014</year>
        <fpage>3630</fpage>
        <lpage>3642</lpage>
        <pub-id pub-id-type="pmid">24786789</pub-id>
      </element-citation>
    </ref>
    <ref id="bib46">
      <label>46</label>
      <element-citation publication-type="journal" id="sref46">
        <person-group person-group-type="author">
          <name>
            <surname>Johnson</surname>
            <given-names>D.G.</given-names>
          </name>
          <name>
            <surname>Schneider-Broussard</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Role of E2F in cell cycle control and cancer</article-title>
        <source>Front. Biosci.</source>
        <volume>3</volume>
        <year>1998</year>
        <fpage>d447</fpage>
        <lpage>d448</lpage>
        <pub-id pub-id-type="doi">10.2741/a291</pub-id>
        <pub-id pub-id-type="pmid">9556498</pub-id>
      </element-citation>
    </ref>
    <ref id="bib47">
      <label>47</label>
      <element-citation publication-type="journal" id="sref47">
        <person-group person-group-type="author">
          <name>
            <surname>Saleh</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Alyami</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Alosaimi</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <article-title>Predicting breast cancer based on optimized deep learning approach</article-title>
        <source>Comput. Intell. Neurosci.</source>
        <volume>2022</volume>
        <year>2022</year>
        <fpage>1820777</fpage>
        <pub-id pub-id-type="pmid">35345799</pub-id>
      </element-citation>
    </ref>
    <ref id="bib48">
      <label>48</label>
      <element-citation publication-type="journal" id="sref48">
        <person-group person-group-type="author">
          <name>
            <surname>Colaprico</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Silva</surname>
            <given-names>T.C.</given-names>
          </name>
          <name>
            <surname>Olsen</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Garofano</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Cava</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Garolini</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Sabedot</surname>
            <given-names>T.S.</given-names>
          </name>
          <name>
            <surname>Malta</surname>
            <given-names>T.M.</given-names>
          </name>
          <name>
            <surname>Pagnotta</surname>
            <given-names>S.M.</given-names>
          </name>
          <name>
            <surname>Castiglioni</surname>
            <given-names>I.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>TCGAbiolinks: an R/Bioconductor package for integrative analysis of TCGA data</article-title>
        <source>Nucleic Acids Res.</source>
        <volume>44</volume>
        <year>2016</year>
        <fpage>e71</fpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkv1507</pub-id>
        <pub-id pub-id-type="pmid">26704973</pub-id>
      </element-citation>
    </ref>
    <ref id="bib49">
      <label>49</label>
      <element-citation publication-type="journal" id="sref49">
        <person-group person-group-type="author">
          <name>
            <surname>Warde-Farley</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Donaldson</surname>
            <given-names>S.L.</given-names>
          </name>
          <name>
            <surname>Comes</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Zuberi</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Badrawi</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Chao</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Franz</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Grouios</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Kazi</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Lopes</surname>
            <given-names>C.T.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The GeneMANIA prediction server: biological network integration for gene prioritization and predicting gene function</article-title>
        <source>Nucleic Acids Res.</source>
        <volume>38</volume>
        <year>2010</year>
        <fpage>W214</fpage>
        <lpage>W220</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkq537</pub-id>
        <pub-id pub-id-type="pmid">20576703</pub-id>
      </element-citation>
    </ref>
    <ref id="bib50">
      <label>50</label>
      <element-citation publication-type="journal" id="sref50">
        <person-group person-group-type="author">
          <collab>Cancer Genome Atlas Research Network</collab>
          <name>
            <surname>Weinstein</surname>
            <given-names>J.N.</given-names>
          </name>
          <name>
            <surname>Collisson</surname>
            <given-names>E.A.</given-names>
          </name>
          <name>
            <surname>Mills</surname>
            <given-names>G.B.</given-names>
          </name>
          <name>
            <surname>Shaw</surname>
            <given-names>K.R.M.</given-names>
          </name>
          <name>
            <surname>Ozenberger</surname>
            <given-names>B.A.</given-names>
          </name>
          <name>
            <surname>Ellrott</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Shmulevich</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Stuart</surname>
            <given-names>J.M.</given-names>
          </name>
        </person-group>
        <article-title>The Cancer Genome Atlas Pan-Cancer analysis project</article-title>
        <source>Nat. Genet.</source>
        <volume>45</volume>
        <year>2013</year>
        <fpage>1113</fpage>
        <lpage>1120</lpage>
        <pub-id pub-id-type="doi">10.1038/ng.2764</pub-id>
        <pub-id pub-id-type="pmid">24071849</pub-id>
      </element-citation>
    </ref>
    <ref id="bib51">
      <label>51</label>
      <element-citation publication-type="journal" id="sref51">
        <person-group person-group-type="author">
          <name>
            <surname>Liberzon</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Birger</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Thorvaldsdóttir</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Ghandi</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Mesirov</surname>
            <given-names>J.P.</given-names>
          </name>
          <name>
            <surname>Tamayo</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>The Molecular Signatures Database (MSigDB) hallmark gene set collection</article-title>
        <source>Cell Syst.</source>
        <volume>1</volume>
        <year>2015</year>
        <fpage>417</fpage>
        <lpage>425</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cels.2015.12.004</pub-id>
        <pub-id pub-id-type="pmid">26771021</pub-id>
      </element-citation>
    </ref>
    <ref id="bib52">
      <label>52</label>
      <element-citation publication-type="journal" id="sref52">
        <person-group person-group-type="author">
          <name>
            <surname>Grossman</surname>
            <given-names>R.L.</given-names>
          </name>
          <name>
            <surname>Heath</surname>
            <given-names>A.P.</given-names>
          </name>
          <name>
            <surname>Ferretti</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Varmus</surname>
            <given-names>H.E.</given-names>
          </name>
          <name>
            <surname>Lowy</surname>
            <given-names>D.R.</given-names>
          </name>
          <name>
            <surname>Kibbe</surname>
            <given-names>W.A.</given-names>
          </name>
          <name>
            <surname>Staudt</surname>
            <given-names>L.M.</given-names>
          </name>
        </person-group>
        <article-title>Toward a Shared Vision for Cancer Genomic Data</article-title>
        <source>N. Engl. J. Med.</source>
        <volume>375</volume>
        <year>2016</year>
        <fpage>1109</fpage>
        <lpage>1112</lpage>
        <pub-id pub-id-type="doi">10.1056/NEJMp1607591</pub-id>
        <pub-id pub-id-type="pmid">27653561</pub-id>
      </element-citation>
    </ref>
    <ref id="bib53">
      <label>53</label>
      <element-citation publication-type="book" id="sref53">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Fan</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Girshick</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <part-title>Momentum contrast for unsupervised visual representation learning</part-title>
        <year>2020</year>
        <fpage>9729</fpage>
        <lpage>9738</lpage>
      </element-citation>
    </ref>
    <ref id="bib54">
      <label>54</label>
      <element-citation publication-type="journal" id="sref54">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Fan</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Girshick</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Improved baselines with momentum contrastive learning</article-title>
        <comment>Preprint at</comment>
        <source>arXiv</source>
        <year>2020</year>
        <pub-id pub-id-type="doi">10.48550/arXiv.2003.04297</pub-id>
      </element-citation>
    </ref>
    <ref id="bib55">
      <label>55</label>
      <element-citation publication-type="book" id="sref55">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Kornblith</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Norouzi</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <part-title>A Simple Framework for Contrastive Learning of Visual Representations</part-title>
        <year>2020</year>
        <publisher-name>PMLR</publisher-name>
        <fpage>1597</fpage>
        <lpage>1607</lpage>
      </element-citation>
    </ref>
  </ref-list>
  <sec id="appsec2" sec-type="supplementary-material">
    <title>Supplemental information</title>
    <p id="p0360">
      <supplementary-material content-type="local-data" id="mmc1">
        <caption>
          <p>Document S1. Tables S2, S3 and S5</p>
        </caption>
        <media xlink:href="mmc1.pdf"/>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="mmc2">
        <caption>
          <p>Table S1. Cancer hallmark genes and pathways considered in the analysis, related to STAR Methods</p>
          <p>The gs categories are retrieved from MSigDB.</p>
        </caption>
        <media xlink:href="mmc2.xlsx"/>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="mmc3">
        <caption>
          <p>Table S4. Functional enrichment analysis for the top 50 genes in the models, related to Figure 3C</p>
          <p>The results were obtained using GeneMania and considering a neighborhood of maximum 20 connecting genes in addition to the given gene set.</p>
        </caption>
        <media xlink:href="mmc3.xlsx"/>
      </supplementary-material>
    </p>
  </sec>
  <sec sec-type="data-availability" id="da0010">
    <title>Data and code availability</title>
    <p id="p0030">
      <list list-type="simple" id="ulist0015">
        <list-item id="u0030">
          <label>•</label>
          <p id="p0035">This paper analyzes existing, publicly available data generated by the TCGA Research Network: <ext-link ext-link-type="uri" xlink:href="https://www.cancer.gov/tcga" id="intref0010">https://www.cancer.gov/tcga</ext-link>. The link to the datasets is listed in the <xref rid="sec4.1" ref-type="sec">key resources table</xref>.</p>
        </list-item>
        <list-item id="u0035">
          <label>•</label>
          <p id="p0040">The HistoMIL package is available at GitHub: <ext-link ext-link-type="uri" xlink:href="https://github.com/secrierlab/HistoMIL" id="intref0015">https://github.com/secrierlab/HistoMIL</ext-link>. The version of the code used to produce the results of the paper has been deposited at Zenodo: <ext-link ext-link-type="doi" xlink:href="10.5281/zenodo.8220572" id="intref0020">https://doi.org/10.5281/zenodo.8220572</ext-link>.</p>
        </list-item>
        <list-item id="u0040">
          <label>•</label>
          <p id="p0045">Any additional information required to reanalyze the data reported in this paper is available from the <xref rid="sec4.2.1" ref-type="sec">lead contact</xref> upon request.</p>
        </list-item>
      </list>
    </p>
  </sec>
  <ack id="ack0010">
    <title>Acknowledgments</title>
    <p id="p0330">M.S. and S.P. were supported by a <funding-source id="gs1"><institution-wrap><institution-id institution-id-type="doi">10.13039/100014013</institution-id><institution>UKRI</institution></institution-wrap></funding-source> Future Leaders Fellowship (MR/T042184/1). Work in M.S.’s lab was supported by a <funding-source id="gs2"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100000268</institution-id><institution>BBSRC</institution></institution-wrap></funding-source> equipment grant (BB/R01356X/1) and a <funding-source id="gs3">Wellcome Institutional Strategic Support Fund</funding-source> (204841/Z/16/Z).</p>
    <p id="p0335">We would like to thank Eloise Withnell for testing and providing feedback on the HistoMIL package.</p>
    <sec id="sec5">
      <title>Author contributions</title>
      <p id="p0340">Conceptualization, M.S. and S.P.; Methodology, S.P.; Software, S.P.; Validation, S.P.; Formal Analysis, S.P.; Investigation, S.P. and M.S.; Data Curation, S.P.; Writing – Original Draft, S.P. and M.S.; Writing – Review and Editing, M.S. and S.P.; Visualization, S.P.; Supervision, M.S; Funding Acquisition, M.S.</p>
    </sec>
    <sec sec-type="COI-statement" id="sec6">
      <title>Declaration of interests</title>
      <p id="p0345">The authors declare no competing interests.</p>
    </sec>
    <sec sec-type="inclusion-and-diversity" id="sec7">
      <title>Inclusion and diversity</title>
      <p id="p0350">One or more of the authors of this paper self-identifies as a gender minority in their field of research.</p>
    </sec>
  </ack>
  <fn-group>
    <fn id="appsec1" fn-type="supplementary-material">
      <p id="p0355">Supplemental information can be found online at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.isci.2023.108073" id="intref0120">https://doi.org/10.1016/j.isci.2023.108073</ext-link>.</p>
    </fn>
  </fn-group>
</back>
<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_ISCI108073 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEgr4 jpg ?>
<?FILEgr5 jpg ?>
<?FILEfx1 jpg ?>
<?FILEmmc1 pdf ?>
<?FILEmmc2 xlsx ?>
<?FILEmmc3 xlsx ?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">iScience</journal-id>
    <journal-id journal-id-type="iso-abbrev">iScience</journal-id>
    <journal-title-group>
      <journal-title>iScience</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2589-0042</issn>
    <publisher>
      <publisher-name>Elsevier</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10583115</article-id>
    <article-id pub-id-type="pii">S2589-0042(23)02150-8</article-id>
    <article-id pub-id-type="doi">10.1016/j.isci.2023.108073</article-id>
    <article-id pub-id-type="publisher-id">108073</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>HistoMIL: A Python package for training multiple instance learning models on histopathology slides</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au1">
        <name>
          <surname>Pan</surname>
          <given-names>Shi</given-names>
        </name>
        <email>shi.pan@ucl.ac.uk</email>
        <xref rid="aff1" ref-type="aff">1</xref>
        <xref rid="cor1" ref-type="corresp">∗</xref>
      </contrib>
      <contrib contrib-type="author" id="au2">
        <name>
          <surname>Secrier</surname>
          <given-names>Maria</given-names>
        </name>
        <email>m.secrier@ucl.ac.uk</email>
        <xref rid="aff1" ref-type="aff">1</xref>
        <xref rid="fn1" ref-type="fn">2</xref>
        <xref rid="cor2" ref-type="corresp">∗∗</xref>
      </contrib>
      <aff id="aff1"><label>1</label>Department of Genetics, Evolution and Environment, UCL Genetics Institute, University College London, London WC1E 6BT, UK</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>∗</label>Corresponding author <email>shi.pan@ucl.ac.uk</email></corresp>
      <corresp id="cor2"><label>∗∗</label>Corresponding author <email>m.secrier@ucl.ac.uk</email></corresp>
      <fn id="fn1">
        <label>2</label>
        <p id="ntpara0010">Lead contact</p>
      </fn>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>27</day>
      <month>9</month>
      <year>2023</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="collection">
      <day>20</day>
      <month>10</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>27</day>
      <month>9</month>
      <year>2023</year>
    </pub-date>
    <volume>26</volume>
    <issue>10</issue>
    <elocation-id>108073</elocation-id>
    <history>
      <date date-type="received">
        <day>1</day>
        <month>6</month>
        <year>2023</year>
      </date>
      <date date-type="rev-recd">
        <day>21</day>
        <month>8</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>25</day>
        <month>9</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2023 The Author(s)</copyright-statement>
      <copyright-year>2023</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).</license-p>
      </license>
    </permissions>
    <abstract id="abs0010">
      <title>Summary</title>
      <p>Hematoxylin and eosin (H&amp;E) stained slides are widely used in disease diagnosis. Remarkable advances in deep learning have made it possible to detect complex molecular patterns in these histopathology slides, suggesting automated approaches could help inform pathologists’ decisions. Multiple instance learning (MIL) algorithms have shown promise in this context, outperforming transfer learning (TL) methods for various tasks, but their implementation and usage remains complex. We introduce HistoMIL, a Python package designed to streamline the implementation, training and inference process of MIL-based algorithms for computational pathologists and biomedical researchers. It integrates a self-supervised learning module for feature encoding, and a full pipeline encompassing TL and three MIL algorithms: ABMIL, DSMIL, and TransMIL. The PyTorch Lightning framework enables effortless customization and algorithm implementation. We illustrate HistoMIL’s capabilities by building predictive models for 2,487 cancer hallmark genes on breast cancer histology slides, achieving AUROC performances of up to 85%.</p>
    </abstract>
    <abstract abstract-type="graphical" id="abs0015">
      <title>Graphical abstract</title>
      <fig id="undfig1" position="anchor">
        <graphic xlink:href="fx1"/>
      </fig>
    </abstract>
    <abstract abstract-type="author-highlights" id="abs0020">
      <title>Highlights</title>
      <p>
        <list list-type="simple" id="ulist0010">
          <list-item id="u0010">
            <label>•</label>
            <p id="p0010">HistoMIL: a package that streamlines deep learning tasks on histopathology slides</p>
          </list-item>
          <list-item id="u0015">
            <label>•</label>
            <p id="p0015">Comprehensive preprocessing and efficient implementation of MIL algorithms</p>
          </list-item>
          <list-item id="u0020">
            <label>•</label>
            <p id="p0020">Up to 85% AUROC in predicting hallmark gene expression in breast cancer H&amp;E slides</p>
          </list-item>
          <list-item id="u0025">
            <label>•</label>
            <p id="p0025">Cell cycle-related gene expression was most easily captured in pathology slides</p>
          </list-item>
        </list>
      </p>
    </abstract>
    <abstract abstract-type="teaser" id="abs0025">
      <p>Histology; Artificial intelligence; Machine learning</p>
    </abstract>
    <kwd-group id="kwrds0010">
      <title>Subject areas</title>
      <kwd>Histology</kwd>
      <kwd>Artificial intelligence</kwd>
      <kwd>Machine learning</kwd>
    </kwd-group>
  </article-meta>
  <notes>
    <p id="misc9010">Published: September 27, 2023</p>
  </notes>
</front>
<body>
  <sec id="sec1">
    <title>Introduction</title>
    <p id="p0050">Histopathology slides stained with hematoxylin and eosin (H&amp;E) are widely regarded as the gold standard for diagnosing cancer and other diseases. Deep learning (DL)<xref rid="bib5" ref-type="bibr"><sup>1</sup></xref> based approaches have demonstrated remarkable potential for reproducing the workflows of human experts employing such slides in a variety of tasks, e.g., diagnosing cancer and classifying tumor types<xref rid="bib6" ref-type="bibr"><sup>2</sup></xref><sup>,</sup><xref rid="bib7" ref-type="bibr"><sup>3</sup></xref>; segmenting sub-regions at the pixel level to identify nuclei or tissue boundaries<xref rid="bib8" ref-type="bibr"><sup>4</sup></xref><sup>,</sup><xref rid="bib9" ref-type="bibr"><sup>5</sup></xref>; and predicting important clinical metrics such as survival<xref rid="bib10" ref-type="bibr"><sup>6</sup></xref>, recurrence rates<xref rid="bib11" ref-type="bibr"><sup>7</sup></xref><sup>,</sup><xref rid="bib12" ref-type="bibr"><sup>8</sup></xref> and response to treatment.<xref rid="bib13" ref-type="bibr"><sup>9</sup></xref> Several studies have shown that DL-based approaches can also predict more complex molecular characteristics from whole slide image (WSI) datasets, such as microsatellite instability, DNA damage repair deficiencies, mutations or gene expression patterns.<xref rid="bib14" ref-type="bibr"><sup>10</sup></xref><sup>,</sup><xref rid="bib15" ref-type="bibr"><sup>11</sup></xref><sup>,</sup><xref rid="bib16" ref-type="bibr"><sup>12</sup></xref><sup>,</sup><xref rid="bib17" ref-type="bibr"><sup>13</sup></xref></p>
    <p id="p0055">Although transfer learning (TL) was initially widely used for WSI classification tasks, recent research has introduced multiple instance learning (MIL) as an alternative machine learning (ML) framework. MIL is designed to learn from bag-level labels rather than the more precise instance-level labels, and has been shown to outperform TL in certain tasks like survival prediction.<xref rid="bib18" ref-type="bibr"><sup>14</sup></xref><sup>,</sup><xref rid="bib19" ref-type="bibr"><sup>15</sup></xref> However, implementing a MIL-based pipeline to predict molecular labels from WSI datasets presents significant challenges.</p>
    <p id="p0060">Digital pathology WSI datasets consist of large images scanned from original diagnostic tissue slides stained with H&amp;E, often containing billions of pixels in a single file. This brings about specific challenges when applying MIL-based methods: (1) WSI files cannot be directly read by widely used image processing packages such as PIL,<xref rid="bib20" ref-type="bibr"><sup>16</sup></xref> (2) classic architectures of a neural network are designed for lower resolution (i.e., 224 × 224 pixels<xref rid="bib21" ref-type="bibr"><sup>17</sup></xref>), and (3) loading an entire batch of WSIs during training is almost unmanageable and untraceable due to the limited GPU memory. There are various strategies to tackle these issues, and from a toolkit design perspective, they can be categorized into slide reading, preprocessing and ML oriented packages. While earlier implementations primarily focused on user-friendly WSI reading APIs, an increasing number of packages are now being designed to meet the requirements of ML algorithms. However, the existing pipelines do not fully exploit the capabilities of DL approaches.</p>
    <p id="p0065"><bold><italic>WSI reading packages/libraries</italic></bold> such as OpenSlide,<xref rid="bib22" ref-type="bibr"><sup>18</sup></xref> BioFormats,<xref rid="bib23" ref-type="bibr"><sup>19</sup></xref> and HighDicom<xref rid="bib24" ref-type="bibr"><sup>20</sup></xref> offer efficient tools for accessing raw WSI data formats, yet they exhibit shortcomings when integrated with neural network training. Notably, QuPath allows a basic ML pipeline with packages like scikit-learn<xref rid="bib24" ref-type="bibr"><sup>20</sup></xref> through its graphical user interface, but it is not designed for DL or MIL algorithms. Pre-processing packages like PyHIST,<xref rid="bib25" ref-type="bibr"><sup>21</sup></xref> deep-histopath,<xref rid="bib26" ref-type="bibr"><sup>22</sup></xref> Multi_Scale_Tools,<xref rid="bib27" ref-type="bibr"><sup>23</sup></xref> and ASAP<xref rid="bib28" ref-type="bibr"><sup>24</sup></xref> include common processing steps like reading multi-scale information,<xref rid="bib29" ref-type="bibr"><sup>25</sup></xref> tissue segmentation or patching. However, designing a comprehensive and broadly applicable ML-oriented package that encompasses basic components like WSI reading, patch extraction and color normalisation<xref rid="bib30" ref-type="bibr"><sup>26</sup></xref><sup>,</sup><xref rid="bib31" ref-type="bibr"><sup>27</sup></xref> remains difficult. Some preprocessing packages, such as CLAM<xref rid="bib32" ref-type="bibr"><sup>28</sup></xref> and deep-histopath,<xref rid="bib26" ref-type="bibr"><sup>22</sup></xref> deliver quality preprocessing pipelines but may be restrictive when integrated with other algorithms or different datasets.</p>
    <p id="p0070">ML oriented packages like DeepMed,<xref rid="bib31" ref-type="bibr"><sup>27</sup></xref> MONAI<xref rid="bib33" ref-type="bibr"><sup>29</sup></xref> or TIAToolBox<xref rid="bib29" ref-type="bibr"><sup>25</sup></xref> streamline the training process of DL models. For instance, MONAI extends PyTorch’s capabilities for medical data and imaging, providing specialized AI model architectures, transformations and utilities tailored for specific purposes. However, its core code does not specifically optimize for H&amp;E data or various WSI formats. Packages like PathML<xref rid="bib34" ref-type="bibr"><sup>30</sup></xref> aim to deliver richer content for their users by integrating more extensive models such as tissue segmentation. Also, researchers can easily and quickly train DL models on WSI datasets by using the TL protocol. For some of the open source packages, various useful tools have been integrated together, e.g., cell segmentation<xref rid="bib34" ref-type="bibr"><sup>30</sup></xref> or graph aggregator.<xref rid="bib29" ref-type="bibr"><sup>25</sup></xref> Packages like TIAToolBox also offer improved scalability through their module design and unit-testable code.<xref rid="bib29" ref-type="bibr"><sup>25</sup></xref> However, a significant challenge in the context of TL arises when dealing with molecular-level labels lacking pixel-level annotations. This leads to reliance on pseudo-labels, potentially affecting model performance or generalization capabilities.<xref rid="bib1" ref-type="bibr"><sup>31</sup></xref> For instance, consider a scenario where a tumor region is marked as exhibiting high expression of a particular gene that is unique to a subtype of T cells. In such cases, the model being used may end up learning intricate patterns for all cell populations in the tumor tissue, leading to the misclassification of all cells as positive cases. A robust feature encoder can also significantly influence the final results in target classification tasks. Self-Supervised Learning (SSL) has emerged as a prominent training paradigm that leverages the inherent data structure, negating the need for extra labeling. In the context of Whole Slide Imaging (WSI) classification, SSL holds the potential to create highly specialized feature encoders tailored for WSI datasets.<xref rid="bib2" ref-type="bibr"><sup>32</sup></xref> Nevertheless, employing existing SSL packages requires multiple preprocessing steps for WSIs, PNG or JPEG conversion, followed by SSL training with invocation and processing. This essentially entails building a complete SSL pipeline alongside the core training process for WSI classification.</p>
    <p id="p0075">To address some of the challenges highlighted above, we introduce HistoMIL, a DL package based on PyTorch<xref rid="bib35" ref-type="bibr"><sup>33</sup></xref><sup>,</sup><xref rid="bib36" ref-type="bibr"><sup>34</sup></xref> and PyTorch Lightning<xref rid="bib4" ref-type="bibr"><sup>35</sup></xref> that simplifies the training of WSI-based classification models. HistoMIL provides a complete preprocessing pipeline, reducing the complexity of converting raw WSI data into a usable format for DL frameworks. We implement multiple MIL models that allow flexible training against different target labels, along with multiple SSL models that simplify the training of feature encoders. We demonstrate HistoMIL’s performance in predicting over 2,000 cancer-relevant molecular labels in breast cancer H&amp;E slides from the Cancer Genome Atlas (TCGA). Additionally, we pinpoint specific pathways that can be effectively identified within histopathological tissue.</p>
  </sec>
  <sec id="sec2">
    <title>Results</title>
    <p id="p0080">To facilitate the implementation of MIL-based workflows for classifying histopathology images based on specific molecular labels, we have developed a DL Python package entitled HistoMIL. HistoMIL leverages PyTorch and PyTorch Lightning to provide an efficient framework for all essential steps in DL tasks involving H&amp;E slides. These steps encompass preprocessing slide data, training MIL and TL models on the processed dataset to predict molecular labels of interest, and visualizing the classifier performance and prediction results within the examined slides. Below, we showcase the features and capabilities of the package, accompanied by examples of its application to large-scale cancer datasets.</p>
    <sec id="sec2.1">
      <title>Overview of the HistoMIL package</title>
      <p id="p0085">The HistoMIL library is structured into three tiers: <bold>data</bold>, <bold>model</bold>, and <bold>experiment</bold> (<xref rid="fig1" ref-type="fig">Figure 1</xref>A). The <bold>data</bold> level encompasses multiple data preprocessing steps, including WSI reading, tissue segmentation and patching. Similar to other ML-oriented packages, we incorporate functions for image normalization and feature extraction to store intermediate data and expedite model training. Additionally, we introduce a cohort level to handle metadata such as patient details, molecular labels, and other information. The design of HistoMIL aligns with the evolution of relevant packages in the existing literature (<xref rid="fig1" ref-type="fig">Figure 1</xref>B).<fig id="fig1"><label>Figure 1</label><caption><p>Overview of the HistoMIL package design and working pipeline</p><p>(A) The diagram illustrates the relevant modules comprising HistoMIL and the logical structure organizing them. HistoMIL features four levels encompassing file reading, WSI-related processing modules, deep learning algorithms, and experiment management modules.</p><p>(B) The relationship between HistoMIL modules and state-of-the-art libraries. The proposed package covers the entire workflow of WSI reading, preprocessing, and MIL algorithms, a design informed by the primary functionalities of other packages in the literature.</p><p>(C) How HistoMIL’s various modules operate within an actual pipeline. We enumerate a typical processing workflow, including preprocessing, MIL training, and inference components. It is evident that different modules correspond to distinct functional parts within the process.</p></caption><graphic xlink:href="gr1"/></fig></p>
      <p id="p0090">The <bold>data</bold> level draws inspiration from various WSI reading and processing packages. We introduce a universal reader class that allows users to customize different wrappers to access interfaces from other readers. This approach mitigates the limitation of relying solely on one reader package. For instance, OpenSlide,<xref rid="bib22" ref-type="bibr"><sup>18</sup></xref> BioFormats,<xref rid="bib23" ref-type="bibr"><sup>19</sup></xref> HighDicom<xref rid="bib24" ref-type="bibr"><sup>20</sup></xref> are typical tools that provide a user-friendly API to handle WSI data formats. However, the Python interface of OpenSlide<xref rid="bib22" ref-type="bibr"><sup>18</sup></xref> does not support certain formats, such as OME-TIFF.<xref rid="bib22" ref-type="bibr"><sup>18</sup></xref> BioFormats<xref rid="bib23" ref-type="bibr"><sup>19</sup></xref> and QuPath<xref rid="bib24" ref-type="bibr"><sup>20</sup></xref> support a wider array of WSI formats but are heavily dependent on Java environments. In HistoMIL, users only need to adhere to our predefined abstract class, implementing a unified interface function for various reading libraries. This simplifies the conversion of diverse data types into a standard numpy matrix and associated metadata. Consequently, users have the flexibility to choose their preferred reading library based on individual requirements, thereby circumventing issues related to file formats. For the preprocessing steps, we have incorporated features inspired by the strengths of existing packages. For instance, we introduce a multi-threaded preprocessing design inspired by CLAM,<xref rid="bib32" ref-type="bibr"><sup>28</sup></xref> and we allow users to pre-calculate features for the selected patches, a concept drawn from TIAToolBox,<xref rid="bib29" ref-type="bibr"><sup>25</sup></xref> to expedite MIL model training.</p>
      <p id="p0095">The <bold>model</bold> level is divided into two key components: the backbone and the MIL method. Our backbone module seamlessly incorporates the interface of the timm PyTorch image model,<xref rid="bib35" ref-type="bibr"><sup>33</sup></xref> enabling the downloading of various backbone network architectures and pre-trained parameters for feature extraction. This design philosophy is also present in other open source packages like DeepMed<xref rid="bib31" ref-type="bibr"><sup>27</sup></xref> or TIAToolBox.<xref rid="bib29" ref-type="bibr"><sup>25</sup></xref> However, these packages rely on TL methods, which consider only the original slide-level or patient-level labels, assigning them to each partitioned patch as training label information. In TL-based approaches, introducing pseudo-labels becomes necessary when pixel level labels are unavailable, but this introduces additional noise to the model training process. Training a batch of models may also pose challenges for packages like PathML,<xref rid="bib34" ref-type="bibr"><sup>30</sup></xref> as models may learn misleading information from pseudo-labels and become trapped in local optima. In extreme cases, areas containing the cells of interest may be extremely small, with only a handful of patches containing genuinely relevant information. Our package implements multiple MIL methods (ABMIL,<xref rid="bib1" ref-type="bibr"><sup>31</sup></xref> DSMIL,<xref rid="bib2" ref-type="bibr"><sup>32</sup></xref> TransMIL<xref rid="bib3" ref-type="bibr"><sup>36</sup></xref>) alongside a baseline TL model. The entire algorithm implementation is based on PyTorch Lightning,<xref rid="bib4" ref-type="bibr"><sup>35</sup></xref> enabling rapid training and fine-tuning of models for specific labels. Additionally, our package allows users to apply SSL protocols to train the feature extractor from scratch using only WSI datasets. Existing libraries often use pre-trained models from ImageNet as feature extractors. While recent research demonstrates that feature extractor networks trained with SSL on WSIs can enhance final performance,<xref rid="bib18" ref-type="bibr"><sup>14</sup></xref><xref rid="bib2" ref-type="bibr"><sup>32</sup></xref> this feature is not supported by existing ML packages. Furthermore, HistoMIL includes pre-defined parameters as default settings, which allows users to quickly try out and scale-up an algorithm for different targets. Inspired by packages like stainlib,<xref rid="bib30" ref-type="bibr"><sup>26</sup></xref> we have included some utility functions to handle data augmentation, a technique employed to ensure slides with different color ranges are transformed in a uniform way to enable meaningful comparisons.</p>
      <p id="p0100">At the <bold>experiment</bold> level, configuring different models for various datasets and hardware conditions becomes a straightforward task. For instance, researchers may wish to to gain a comprehensive understanding of how multiple biomarkers are spatially distributed by predicting these molecular labels from H&amp;E slides. They might want to predict hundreds of target labels in an initial setting. Most existing packages lack the ability to efficiently scale up algorithms for multiple molecular labels, as they are not designed to deploy a batch of models simultaneously. In HistoMIL, researchers can effortlessly initialize a series of instances to explore the hyperparameter space defined by the customizable <italic>para</italic> class. A <italic>Trainer</italic> class instance will initialize a PyTorch Lightning module, adapting to hardware requirements automatically. Furthermore, third-party tuners like the Ray tuner<xref rid="bib37" ref-type="bibr"><sup>37</sup></xref> can directly utilize these module instances, making it easy to discover the optimal model configuration. <xref rid="fig1" ref-type="fig">Figure 1</xref>C illustrates which modules are used at different stages of a process that employs a MIL algorithm.</p>
      <p id="p0105">Details on the interplay between modules during preprocessing, SSL and MIL can be found in <xref rid="fig2" ref-type="fig">Figure 2</xref>. Moreover, <xref rid="fig2" ref-type="fig">Figure 2</xref> demonstrates how the HistoMIL package streamlines complex pipelines with minimal code. Further information on the implementation of HistoMIL, including data-level design, model training and experimental setup, can be found in the <xref rid="sec4" ref-type="sec">STAR methods</xref> section.<fig id="fig2"><label>Figure 2</label><caption><p>Diagram depicting three primary application scenarios for HistoMIL and their corresponding code blocks</p><p>(A) The invocation method for HistoMIL during batch preprocessing. With predefined processing parameters, HistoMIL can process an entire dataset in one go using just three to four simple commands.</p><p>(B) The code and calling logic for SSL training. HistoMIL facilitates SSL on WSI datasets by predefining SSL-related parameters and modifying the trainer accordingly.</p><p>(C) The code and module calling process for MIL training. Compared to SSL, MIL adds adjustments to the model’s parameters while simplifying the training setup process. For users with access to a GPU server, numerous models can be trained simultaneously by configuring parameters in the bash file.</p></caption><graphic xlink:href="gr2"/></fig></p>
    </sec>
    <sec id="sec2.2">
      <title>Applying HistoMIL to predict cancer hallmarks in H&amp;E stained slides</title>
      <p id="p0110">In this section, we demonstrate the usability and scalability of HistoMIL in WSI-level prediction tasks employing state-of-the-art DL models. One clinically relevant task in cancer that has been recently made feasible by modern computational pathology is assessing the state of specific genes or entire molecular pathways directly within histopathology tissue slides using DL techniques.<xref rid="bib14" ref-type="bibr"><sup>10</sup></xref><sup>,</sup><xref rid="bib38" ref-type="bibr"><sup>38</sup></xref> This rapid evaluation can aid in diagnosis, prognosis and treatment decisions. Typically, such assessments involve gene panel profiling or immunohistochemistry (IHC), but these tests introduce time delays and additional costs, as they are additional steps following visual inspection of routinely stained H&amp;E sections. HistoMIL simplifies this process by providing a fast and efficient implementation for predicting diverse molecular labels in H&amp;E-stained slides.</p>
      <p id="p0115">Here, we showcase HistoMIL’s capacity to streamline the entire analysis workflow required to predict thousands of cancer hallmarks. We used 1,012 H&amp;E-stained slides of breast cancer tissue and matched RNA-seq data from TCGA to train over 8,000 models for the classification of 2,487 cancer hallmark genes. First, we processed the WSI dataset using pre-defined preprocessing functions. This step involves extracting the tissue area from H&amp;E histological images, generating patches automatically, and saving them as H5DF and image files. Subsequently, we trained the Feature Extractor Network using the SSL module and predicted the target gene expression labels using the built-in MIL algorithms. As depicted in <xref rid="fig3" ref-type="fig">Figure 3</xref>A, the implementation of the pipeline is simplified through the use of the generic interface provided by HistoMIL. This reduction in complexity significantly eases the workload for researchers aiming to expand these methods.<fig id="fig3"><label>Figure 3</label><caption><p>Experimental workflow and performance comparison</p><p>(A) The diagram showcases the complete workflow for utilizing HistoMIL in predictive experiments. The raw data from TCGA-BRCA undergoes preprocessing, yielding image patches and associated labels suitable for MIL processing. The experimental task involves predicting gene expression, with the model’s performance displayed as AUROC.</p><p>(B) Comparison of performance distribution among different algorithms in predicting the expression of 2,487 cancer-related genes. Each distribution contains 2,487 data points. The box centerlines depict the medians, and the edges depict the first/third quartiles. TransMIL exhibits superior performance relative to the other algorithms.</p><p>(C) The top 30 genes with the highest AUROC scores. In the test set, the model’s accuracy in predicting gene expression levels (high or low) reaches up to ∼86%. See also <xref rid="mmc1" ref-type="supplementary-material">Table S2</xref>.</p></caption><graphic xlink:href="gr3"/></fig></p>
      <p id="p0120">Despite the clear benefits of utilizing MIL algorithms for H&amp;E-based predictions, researchers with limited coding experience may encounter difficulties when attempting to replicate MIL workflows, where slight variations in code may result in vastly different outcomes. Moreover, the complexity of handling high-dimensional histological data may discourage novice researchers from adopting this approach. In training our models, we adhered to the methodologies detailed in the original papers of TL,<xref rid="bib31" ref-type="bibr"><sup>27</sup></xref> ABMIL,<xref rid="bib1" ref-type="bibr"><sup>31</sup></xref> DSMIL<xref rid="bib2" ref-type="bibr"><sup>32</sup></xref> and TransMIL,<xref rid="bib3" ref-type="bibr"><sup>36</sup></xref> and employed the PyTorch Lightning wrapper designed by the HistoMIL library for the training process. This package simplifies these steps, making them accessible and ensuring reproducibility even for those lacking an in-depth understanding of MIL algorithms. For each algorithm, we trained over 2,000 models to predict the expression levels of selected cancer hallmark genes.</p>
      <p id="p0125">To demonstrate how HistoMIL handles the complex WSI processing steps, we have reproduced the pipeline in an instance notebook. This can be found in the <italic>Notebooks</italic> folder of the HistoMIL GitHub repository (see <xref rid="sec4" ref-type="sec">STAR methods</xref>). We have used code snippets and condensed the code to illustrate the usage of HistoMIL, underscoring its efficiency for WSI prediction tasks. Additionally, our model implementation includes options for computing an attention score or providing patch-level predictions, enabling MIL algorithms to predict the target labels for individual patches. In this particular example, we focused on WSI-level labels, where one expression value is available per gene and per slide. To expedite training and inference times, we did not perform additional data augmentation, yet the models were still able to successfully predict the target label.</p>
      <p id="p0130">Predicting gene expression labels in the benchmark TCGA breast cancer dataset showcases the potential of using HistoMIL on WSI datasets. All of the predictions are based only on WSI data. Our target labels encompass the expression levels of 2,487 different cancer hallmark genes derived from MSigDB, selected as a benchmark task to demonstrate the scalability of our package. We regarded this task as a typical weakly supervised learning challenge and opted for the TransMIL method to learn the aggregation function. We utilized the ResNet-18 pre-trained feature extractor as a backbone network, and all training procedures were executed on the PyTorch Lightning platform with early stopping.</p>
      <p id="p0135">Our experiments involved two training paradigms (TL and MIL) and a total of four different DL algorithms: TL, ABMIL,<xref rid="bib1" ref-type="bibr"><sup>31</sup></xref> DSMIL,<xref rid="bib2" ref-type="bibr"><sup>32</sup></xref> and TransMIL.<xref rid="bib3" ref-type="bibr"><sup>36</sup></xref> The experimental outcomes consistently demonstrated that MIL algorithms generally outperformed TL algorithms when using the same feature encoder (<xref rid="fig3" ref-type="fig">Figure 3</xref>B). Furthermore, variations in performance were observed among different MIL algorithms, with a general trend of TransMIL &gt; DSMIL &gt; ABMIL. Considering the characteristics of these algorithms, it can be reasoned that the introduction of attention mechanisms and neighborhood information have played a significant role in driving these performance differences. Firstly, the slide-level attention mechanism could allow the algorithm to compare the importance of patches within a slide for classification. Additionally, TransMIL, which incorporates neighborhood information, has yielded the best results in our experiments. This is possibly due to the neighbor information capturing the broader changes in tissue structure, which may be relevant in the context of a certain gene being expressed or deactivated.</p>
      <p id="p0140">Among the top genes where different models demonstrated good performance metrics on the test set (<xref rid="fig3" ref-type="fig">Figure 3</xref>C, <xref rid="mmc1" ref-type="supplementary-material">Table S2</xref>) were <italic>MAD2L1, KIF2C</italic> and <italic>AURKA</italic>. These are cell cycle regulators involved in spindle assembly and stabilization, and they promote chromosome segregation during mitosis.<xref rid="bib39" ref-type="bibr"><sup>39</sup></xref><sup>,</sup><xref rid="bib40" ref-type="bibr"><sup>40</sup></xref><sup>,</sup><xref rid="bib41" ref-type="bibr"><sup>41</sup></xref> Other top-ranking genes such as <italic>F2RL2</italic> or <italic>FSHB</italic> are involved in G protein-coupled receptor signaling,<xref rid="bib41" ref-type="bibr"><sup>41</sup></xref> while <italic>MMP2</italic> and <italic>POSTN</italic> are involved in matrix remodeling and cell adhesion.<xref rid="bib42" ref-type="bibr"><sup>42</sup></xref><sup>,</sup><xref rid="bib43" ref-type="bibr"><sup>43</sup></xref> These highly relevant cancer-promoting processes would be expected to leave a clear morphological trace in the tissue. Therefore, the activity of these genes might be more easily identifiable due to linked changes in tumor cell morphology and tissue structure as seen in the H&amp;E slides. In addition, since different MIL algorithms show similar performance in predicting the expression of these genes, it further indicates that the expression patterns of these genes in H&amp;E slides have a certain predictability. These patterns can be easily captured throughout the entire slide using heatmap gradients of expression, thus informing on the spatial distribution of activity for a specific gene throughout the entire tissue (<xref rid="fig4" ref-type="fig">Figure 4</xref>).<fig id="fig4"><label>Figure 4</label><caption><p>Model predictions for gene expression throughout the entire slide</p><p>The first image on the left in the top row is the original tissue slide. The remaining heat maps in the first row demonstrate the spatial distribution of gene activity for selected lowly expressed genes that were predicted by the model. The heat maps in the second row exhibit the spatial distribution of gene activity for highly expressed genes that were accurately predicted by the model. The purple to yellow gradient indicates increasing levels of predicted gene expression per patch. Intratumor heterogeneity of gene expression can be observed, particularly for <italic>LAS1L</italic> or <italic>CYB5R3</italic>.</p></caption><graphic xlink:href="gr4"/></fig></p>
      <p id="p0145">Some genes, on the other hand, are more difficult to predict (<xref rid="mmc1" ref-type="supplementary-material">Table S3</xref>). For instance, the performance of the TransMIL algorithm is poor when predicting the expression levels of <italic>SPRR3</italic>, a marker for terminal squamous cell differentiation linked with tumor progression in early stage breast cancers,<xref rid="bib44" ref-type="bibr"><sup>44</sup></xref> and <italic>PGAM2</italic>, a gene involved in oxidative stress responses.<xref rid="bib45" ref-type="bibr"><sup>45</sup></xref> This could be explained by the complexity of the regulatory processes driven by these genes which may not render clear morphological changes in the cells. This could also be compounded by tumor heterogeneity, e.g., if oxidative stress is present in only part of the cancer tissue. Furthermore, unlike <italic>MAD2L1</italic>, <italic>F2RL2</italic>, and <italic>KIF2C</italic>, genes such as <italic>SPRR3</italic> and <italic>PGAM2</italic> exhibit higher performance variability among different MIL algorithms, and their performance is normally lower than an AUROC of 65%.</p>
      <p id="p0150">To further explore the capability of MIL methods, we focused on the higher-level activity captured by individual genes within the tissue, which can be summarized within hallmark pathways underlying cancer initiation and progression. These included processes such as <italic>angiogenesis</italic>, which plays a crucial role in the formation of blood vessels to support tumor growth, <italic>hypoxia</italic>, triggered when cancer cells experience inadequate levels of oxygen, or the <italic>P53 pathway</italic>, which regulates cell-cycle arrest and apoptosis in response to DNA damage (see <xref rid="mmc3" ref-type="supplementary-material">Table S1</xref> for a complete list). Across 14 key hallmark pathways we observed markedly consistent levels of performance of different MIL algorithms in predicting the expression of the genes involved in the respective pathways (<xref rid="fig5" ref-type="fig">Figure 5</xref>).<fig id="fig5"><label>Figure 5</label><caption><p>Performance of various algorithms on predicting pathway-level activity in cancer</p><p>TransMIL, DSMIL, ABMIL and TL (Transfer Learning) algorithm performance is compared across selected pathways. The boxes depict the AUROC distributions for each algorithm, colored according to the legend. White circles represent the median values, while black lines indicate the mean. The edges of the boxes depict the first/third quartiles and the whiskers depict the minima/maxima. The different hallmark pathway groups are arranged from left to right in descending order of median values according to the TransMIL algorithm.</p></caption><graphic xlink:href="gr5"/></fig></p>
      <p id="p0155">The E2F target genes had the highest average AUROC in our experiments (<xref rid="fig5" ref-type="fig">Figure 5</xref>). This pathway participates in the cell cycle G1/S transition and DNA replication, and is generally upregulated in tumor cells, leading to abnormal cell proliferation.<xref rid="bib46" ref-type="bibr"><sup>46</sup></xref> Such proliferation differences may be more easily captured in the morphology of the cells as well as nuclear staining within H&amp;E-stained sections. In fact, the top 50 highest performing models were for genes involved in cell cycle checkpoints, mitosis and DNA integrity (<xref rid="mmc3" ref-type="supplementary-material">Table S4</xref>), suggesting that cell division-related processes are most easily captured within cancer H&amp;E slides using this methodology. This is not surprising, given the remarkably high performances (&gt;90%) of DL models when it comes to distinguishing tumor areas from normal cells<xref rid="bib47" ref-type="bibr"><sup>47</sup></xref> considering that proliferation is the key defining hallmark of cancer. In contrast, the bottom 50 least performing models (with AUROCs &lt;62%) were for genes involved in tyrosine kinase and apoptotic signaling, nucleotide excision repair and oxidative stress responses (<xref rid="mmc1" ref-type="supplementary-material">Table S5</xref>), which might not be accompanied by visible phenotypic changes in the tumor microenvironment.</p>
      <p id="p0160">Thus, we demonstrate how HistoMIL can be used to assess the detection of thousands of disease-relevant molecules in a speedy and efficient manner, with most informative results obtained for genes that are either expressed throughout the entire slide or not expressed at all within the tissue.</p>
    </sec>
  </sec>
  <sec id="sec3">
    <title>Discussion</title>
    <p id="p0165">Building a MIL pipeline for WSI datasets is a complex undertaking, demanding extensive engineering expertise and a deep understanding of model hyperparameters. The intricacies of WSIs necessitate additional efforts is terms of data retrieval, preprocessing and normalization compared to typical image processing models used in standard classification tasks. In particular, WSI data requires specialized handling during retrieval and preprocessing due to its unique nature. Moreover, since H&amp;E slides are relatively sensitive to color variations, they require an additional normalization step and should carefully select data augmentation steps (i.e., must not undergo extreme cropping operations). In this paper, we introduce HistoMIL, a Python library designed to streamline the pre- and post-processing of WSIs within a MIL-based pipeline. It seamlessly integrates with PyTorch, a widely used DL toolkit, simplifying the training of a batch of MIL-models for diverse targets. The use of PyTorch Lightning further simplifies the training process. Furthermore, our HistoMIL package offers an uncomplicated method for training a feature extractor using an SSL protocol, which can enhance the performance of various methods within the target data domain. We provide a detailed tutorial on how to use this package, tailored to both technical and non-technical users, in our <italic>Notebooks</italic> folder at <ext-link ext-link-type="uri" xlink:href="https://github.com/secrierlab/HistoMIL" id="intref0025">https://github.com/secrierlab/HistoMIL</ext-link>.</p>
    <p id="p0170">HistoMIL aims to facilitate the training and application of MIL algorithms for predicting molecular labels based on digital pathology slides by providing an easy-to-use API. In doing so, we hope to equip users with a comprehensive toolkit that empowers them to concentrate on developing new algorithms and addressing biological questions. To achieve this goal, we have implemented a wide range of modules and functions to streamline the process of training and using SSL and MIL. With HistoMIL’s SSL component, generating a feature extractor for the diagnostic slide dataset becomes effortless, subsequently enabling the prediction of various molecular labels. In our experiments, we illustrate how HistoMIL can be used to predict of the expression status of multiple genes in H&amp;E slides using the built-in MIL algorithms. These pipelines have been implemented in the form of interactive notebooks and can be opened and evaluated on cloud platforms such as Google Colab and Kaggle. This highlights how HistoMIL can be used to greatly simplify the complexity of engineering implementations. We hope that the examples provided will assist other users in incorporating MIL methods as effective tools in their analysis pipelines.</p>
    <p id="p0175">By designing HistoMIL to maintain consistency and ease of use when introducing customized models, we have observed that the data processing steps of different algorithms can be shared due to the repeated use of HistoMIL modules. Additionally, different algorithms can also share the same intermediate data results. Furthermore, batch processing and patch aggregation in HistoMIL come with pre-set values, which greatly reduces the level of difficulty for users. It is important to note that HistoMIL is not limited to the implemented MIL algorithms. Due to its modular and scalable design, users can conveniently implement and modify new algorithms. This flexibility allows for the training of customizable algorithms based on existing work. Therefore, any algorithm implementation involving MIL and WSIs can benefit from the functionality we provide. In addition, many tasks involving WSIs can be easily accommodated through modifications to algorithms or loss functions.</p>
    <p id="p0180">We showcased the strong performance of HistoMIL across more than 2,000 gene expression prediction tasks using a variety of MIL algorithms. Notably, we achieved AUCs exceeding 80% for 130 genes. TransMIL demonstrated superior results compared to other MIL or TL algorithms. We showed that amongst classical cancer hallmark pathways the most identifiable within H&amp;E-stained slides are the ones related to cell proliferation, in line with other findings in the field,<xref rid="bib14" ref-type="bibr"><sup>10</sup></xref> whereas kinase signaling, apoptosis and oxidative stress processes are more difficult to capture from the tissue morphology. This suggests that cancer diagnosis and progression tasks could be more easily automated on histopathology slides than tasks related to targeted treatment decisions. Furthermore, the spatial visualization capabilities of HistoMIL pave the way toward further analyses of spatial patterns of gene activity, which could be used to understand how cancers develop within the tissue and interact with their environment.</p>
    <p id="p0185">HistoMIL is an open-source project that will continue to add additional pre-trained models and functionality. In the future, we intend to enhance the currently available models by training them on additional datasets and including other MIL algorithms. A logical extension of our work is its application in tasks like cancer grading, survival prediction and the exploration of other molecular or clinical labels in different tumor types. Importantly, the package is not restricted to the analysis of cancer datasets alone, and could be readily applied to address a wide range of biomedical questions seeking to uncover meaningful patterns within H&amp;E stained slides. Future versions of HistoMIL could aid with patient diagnosis or help pathologists identify subtle differences in gene activity that might not be readily discernible in traditional histological sections. Extracting features and analyzing results is typically a time-consuming and computationally intensive task, but by using HistoMIL we can train models on a very large scale. This may lead to faster and more efficient analysis of whole slide images in future clinical usage.</p>
    <sec id="sec3.1">
      <title>Limitations of the study</title>
      <p id="p0190">Our experiments have shed light on certain limitations within the existing MIL algorithms. First, as expected, the performance of these algorithms is highly dependent on the image quality of the slides. As the expression levels of some genes are correlated with the color patterns of the images, the models are sensitive to the color changes in the original slides. In our experiment, to maintain simplicity in the training process and reduce extraneous variables, we did not introduce additional data augmentation steps. This might have compromised the generalization capabilities of the resulting model. In practice, different molecular labels may require distinct data augmentation strategies as critical parameters for performance optimization. We underscore the importance of considering this as an essential step for accurate model construction and prediction.</p>
      <p id="p0195">MIL algorithms also incorporate attention mechanisms and neighborhood information, rendering them more susceptible to variations within a single slide, such as local color changes. Moreover, patch-level predictions can introduce biases. Some genes might not be expressed in all regions of the slide, but because a single expression label is available per slide, the models would tend to assume high expression in all areas when this label is high. We frequently observed this phenomenon in our experiments. Whether this reflects a genuine feature of gene activity within the tissue or a model limitation remains to be determined. To address this issue, future work should focus on incorporating additional annotations, such as those obtained through immunohistochemistry, to acquire higher-resolution data for a specific biomarker across the entire tissue rather than relying on a single label per slide.</p>
      <p id="p0200">The interpretability of an AI model in biology and medical research cannot be overstated. In HistoMIL, the ABMIL and TransMIL implementations inherently feature attention scores to demonstrate good interpretability. The DSMIL method generates patch-level prediction values as part of its output. Furthermore, gradient-based interpretability methods can be incorporated with some simple additional functions. In subsequent extensions, we plan to introduce distinct interpretability components to the HistoMIL framework, thereby providing a unified interpretability interface for all the supported models. To further enhance interpretability, future research endeavors in the field should also incorporate experimental validation of the model predictions.</p>
      <p id="p0205">The number of samples in the tested dataset also limits our analyses. In the context of TL, the number of training samples is determined by the number of WSIs in the dataset multiplied by the number of patches within each WSI. In essence, each patch extracted from a WSI is treated as an independent sample, resulting in a substantial number of training samples. However, when employing the MIL method, the number of training samples for the slide-level classifier is equivalent to the number of WSIs in the dataset. This is because MIL treats each WSI as a single sample, rather than considering each patch within the WSI as an individual sample. This fundamental distinction has the potential to impact the generalization capability of the MIL method. It also makes the model aggregation part prone to being misled by certain samples and thus trapped in local optima. Although we selected the largest cancer dataset available from TCGA (TCGA-BRCA), the total number of WSI samples remains relatively limited, which further constrains the potential performance of our models. Furthermore, while TCGA-BRCA is a cancer dataset, due to time and resource constraints, our current experiment did not include testing on other cancer datasets or those related to other diseases. Thus, our biological conclusions are unlikely to be generalizable to other types of cancers or diseases. Our future work will aim to address those issues by expanding the pool of diagnostic slides from other sources.</p>
    </sec>
  </sec>
  <sec id="sec4">
    <title>STAR★Methods</title>
    <sec id="sec4.1">
      <title>Key resources table</title>
      <p id="p0210">
        <table-wrap position="float" id="undtbl1">
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th>REAGENT or RESOURCE</th>
                <th>SOURCE</th>
                <th>IDENTIFIER</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td colspan="3">
                  <bold>Software and algorithms</bold>
                </td>
              </tr>
              <tr>
                <td colspan="3">
                  <hr/>
                </td>
              </tr>
              <tr>
                <td>HistoMIL</td>
                <td>This paper</td>
                <td>GitHub: <ext-link ext-link-type="uri" xlink:href="https://github.com/secrierlab/HistoMIL" id="intref0030">https://github.com/secrierlab/HistoMIL</ext-link>; Zenodo: <ext-link ext-link-type="doi" xlink:href="10.5281/zenodo.8220572" id="intref0035">https://doi.org/10.5281/zenodo.8220572</ext-link></td>
              </tr>
              <tr>
                <td>PyTorch</td>
                <td>Paszke et al.<xref rid="bib36" ref-type="bibr"><sup>34</sup></xref></td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="https://pytorch.org/" id="intref0040">https://pytorch.org/</ext-link>
                </td>
              </tr>
              <tr>
                <td>PyTorch Lightning</td>
                <td>Falcon<xref rid="bib4" ref-type="bibr"><sup>35</sup></xref></td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="https://lightning.ai/docs/pytorch/stable/" id="intref0045">https://lightning.ai/docs/pytorch/stable/</ext-link>
                </td>
              </tr>
              <tr>
                <td>OpenSlide-pytorch</td>
                <td>Goode et al.<xref rid="bib22" ref-type="bibr"><sup>18</sup></xref></td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="https://openslide.org/api/python/" id="intref0050">https://openslide.org/api/python/</ext-link>
                </td>
              </tr>
              <tr>
                <td>ABMIL</td>
                <td>Leiby et al.<xref rid="bib1" ref-type="bibr"><sup>31</sup></xref></td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="https://github.com/AMLab-Amsterdam/AttentionDeepMIL" id="intref0055">https://github.com/AMLab-Amsterdam/AttentionDeepMIL</ext-link>
                </td>
              </tr>
              <tr>
                <td>DSMIL</td>
                <td>Li et al.<xref rid="bib2" ref-type="bibr"><sup>32</sup></xref></td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="https://github.com/binli123/dsmil-wsi" id="intref0060">https://github.com/binli123/dsmil-wsi</ext-link>
                </td>
              </tr>
              <tr>
                <td>TransMIL</td>
                <td>Shao et al.<xref rid="bib3" ref-type="bibr"><sup>36</sup></xref></td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="https://github.com/szc19990412/TransMIL" id="intref0065">https://github.com/szc19990412/TransMIL</ext-link>
                </td>
              </tr>
              <tr>
                <td>TCGAbiolinks</td>
                <td>Colaprico et al.<xref rid="bib48" ref-type="bibr"><sup>48</sup></xref></td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="https://bioconductor.org/packages/release/bioc/html/TCGAbiolinks.html" id="intref0070">https://bioconductor.org/packages/release/bioc/html/TCGAbiolinks.html</ext-link>
                </td>
              </tr>
              <tr>
                <td>msigdbr</td>
                <td>Igor Dolgalev</td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="https://igordot.github.io/msigdbr/" id="intref0075">https://igordot.github.io/msigdbr/</ext-link>
                </td>
              </tr>
              <tr>
                <td>GeneMania</td>
                <td>Warde-Farley et al.<xref rid="bib49" ref-type="bibr"><sup>49</sup></xref></td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="https://genemania.org/" id="intref0080">https://genemania.org/</ext-link>
                </td>
              </tr>
              <tr>
                <td colspan="3">
                  <hr/>
                </td>
              </tr>
              <tr>
                <td colspan="3">
                  <bold>Other</bold>
                </td>
              </tr>
              <tr>
                <td colspan="3">
                  <hr/>
                </td>
              </tr>
              <tr>
                <td>H&amp;E slides and RNA-seq data from the TCGA BRCA cohort (GDC data portal)</td>
                <td>TCGA, Weinstein et al.<xref rid="bib50" ref-type="bibr"><sup>50</sup></xref></td>
                <td><ext-link ext-link-type="uri" xlink:href="https://www.cancer.gov/tcga" id="intref0085">https://www.cancer.gov/tcga</ext-link>; <ext-link ext-link-type="uri" xlink:href="https://portal.gdc.cancer.gov/projects/TCGA-BRCA" id="intref0090">https://portal.gdc.cancer.gov/projects/TCGA-BRCA</ext-link></td>
              </tr>
              <tr>
                <td>MSigDB hallmark gene set</td>
                <td>Liberzon et al.<xref rid="bib51" ref-type="bibr"><sup>51</sup></xref></td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="https://www.gsea-msigdb.org/gsea/msigdb/human/genesets.jsp?collection=H" id="intref0095">https://www.gsea-msigdb.org/gsea/msigdb/human/genesets.jsp?collection=H</ext-link>
                </td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </p>
    </sec>
    <sec id="sec4.2">
      <title>Resource availability</title>
      <sec id="sec4.2.1">
        <title>Lead contact</title>
        <p id="p0215">Further information and requests for resources should be directed to and will be fulfilled by the lead contact, Maria Secrier (<ext-link ext-link-type="uri" xlink:href="mailto:m.secrier@ucl.ac.uk" id="intref0100">m.secrier@ucl.ac.uk</ext-link>).</p>
      </sec>
      <sec id="sec4.2.2">
        <title>Materials availability</title>
        <p id="p0220">This study did not generate new unique reagents.</p>
      </sec>
    </sec>
    <sec id="sec4.3">
      <title>Experimental model and study participant details</title>
      <p id="p0225">This paper analyses publicly available RNA-sequencing and H&amp;E data from TCGA, collected from 1,062 breast cancer patients. All data comply with ethical regulations, with approval and informed consent for collection and sharing already obtained by The Cancer Genome Atlas (TCGA). Sex, age and ethnicity information for the study participants can be found at <ext-link ext-link-type="uri" xlink:href="https://portal.gdc.cancer.gov/projects/TCGA-BRCA" id="intref0105">https://portal.gdc.cancer.gov/projects/TCGA-BRCA</ext-link>. Clinical information was available for 1,048 patients, out of which 99% (n = 1,036) were female and 1% (n = 12) were male, with ages comprised between 26 and 90 (median of 58). The cohort was split by ethnicity as follows: 38 Hispanic or Latino, 842 not Hispanic/Latino, 168 unreported. The tumor stages analyzed in the study were as follows: T1 (n = 266), T2 (n = 607), T3 (n = 134), T4 (n = 39), Tx (n = 2). The sex and ethnicity information were not taken into account in the analysis because breast cancer predominantly affects women (99% in the analyzed cohort) and TCGA data is not sufficiently diverse and powered for ethnicity analyses in the context of our study. This may limit the study’s generalisability. Experimental groups were defined by the median expression of each hallmark gene considered in the study as detailed below.</p>
      <sec id="sec4.3.1">
        <title>Ethics approval and consent to participate</title>
        <p id="p0230">All data employed in this study comply with ethical regulations, with approval and informed consent for collection and sharing already obtained by The Cancer Genome Atlas (TCGA).</p>
      </sec>
    </sec>
    <sec id="sec4.4">
      <title>Method details</title>
      <sec id="sec4.4.1">
        <title>Experimental setting and dataset</title>
        <p id="p0235">In this paper, we exemplify the power and versatility of HistoMIL in oncology-related tasks by building prediction models for 2,487 cancer-related genes.</p>
        <sec id="sec4.4.1.1">
          <title>Dataset</title>
          <p id="p0240">The Cancer Genome Atlas (TCGA) is a collaborative project that aims to characterise the genomic and molecular landscape of various cancers.<xref rid="bib50" ref-type="bibr"><sup>50</sup></xref> We chose TCGA-BRCA as the largest available dataset with diagnostic H&amp;E-stained slides and matched RNA-sequencing which we could use to define cancer-relevant labels (n = 2,487). The TCGA-BRCA dataset contains a large amount of data, including whole slide images, genomic data, clinical data, and more. It includes samples from a diverse patient population, including patients of different ages, races, and ethnicities. This helps avoid biases as potential factors that may affect model performance. As a widely used data source, the TCGA-BRCA dataset has undergone quality control, which ensures that the data is of high quality and well-annotated. This can help reduce the noise and variability in the dataset. We downloaded 1,133 diagnostic WSIs from the Genomic Data Commons (GDC) Data Portal.<xref rid="bib52" ref-type="bibr"><sup>52</sup></xref> In our experiment, we split WSIs into patches, then automatically selected WSIs with more than 1,000 patches, and further removed the images which were blurry or containing marks. This left us with 1,012 WSIs for analyses, which we split into 80% for training and 20% for testing.</p>
        </sec>
        <sec id="sec4.4.1.2">
          <title>Prediction task</title>
          <p id="p0245">The target labels for the experiments were extracted from the RNA-seq profiles of the same TCGA-BRCA patients for which H&amp;E-stained diagnostic slides were also available, downloaded using the TCGAbiolinks package.<xref rid="bib48" ref-type="bibr"><sup>48</sup></xref> We surveyed 2,487 genes involved in various cancer hallmark pathways derived from MSigDB<xref rid="bib51" ref-type="bibr"><sup>51</sup></xref> using the msigdbr R package (see <xref rid="mmc2" ref-type="supplementary-material">Table S1</xref>), and used the FPKM normalised expression values for each gene to categorise tumors as “highly” or “lowly” expressing the respective gene based on the median split. This is a targeted task designed to demonstrate that HistoMIL can assist researchers in rapidly scaling up the classifiers required for their studies. In this task, the HistoMIL package was used to train and validate prediction models for the expression of the selected 2,487 cancer hallmark genes. In the steps involving MIL training and inference, we employed servers powered by A100 and V100 GPUs. Typically, training for a gene expression spans 100 epochs with early stopping. Predictions for some genes might peak as early as the 5th or 6th epoch, triggering the early stopping mechanism. Within the trainer of HistoMIL, we incorporated options for researchers to either randomly split the training set proportionally or choose K-fold validation, facilitating the validation of the training process. By integrating support for TorchMetrics, users can easily select how to evaluate model performance and monitor training progress. Researchers can also determine which slides, generated by specific hospitals, would be placed in a different set as an independent test set to evaluate model performance. This approach facilitates a more robust evaluation of the model’s generalization capabilities. For this experiment, we used an 80% training set and 20% test set strategy to keep a simple experimental setting. We note that we did not employ the SSL module in this analysis example in order to simplify the training process and to focus on comparing the performance of TL and MIL methods.</p>
          <p id="p0250">Finally, functional enrichment analysis was performed using GeneMania.<xref rid="bib49" ref-type="bibr"><sup>49</sup></xref> By predicting the expression of a gene of interest throughout the entire tissue slide, researchers can highlight the features derived from the attention score or gradient vectors.</p>
          <p id="p0255">We believe using HistoMIL could aid in diagnosis or help pathologists identify subtle differences in gene activity that might not be apparent in traditional histological sections. Extracting features and analysing results is normally a time-consuming and computationally intensive task. By using HistoMIL, we can train models on a very large scale. This may lead to faster and more efficient analysis of whole slide images in future clinical usage.</p>
        </sec>
      </sec>
      <sec id="sec4.4.2">
        <title>Module design in HistoMIL</title>
        <sec id="sec4.4.2.1">
          <title>Data level design</title>
          <p id="p0260">The HistoMIL design includes a data level to handle different data formats, preprocessing and other meta-information of the target cohort. Pre-processing WSIs can be time-consuming and computationally expensive in many cases. By integrating a number of configurations and functions, the HistoMIL package offers a functional interface to smoothly run each necessary step in a different pipeline, and save the intermediate output as needed. After initialising a <italic>Slide</italic> instance and reading the slide files, we divide the preprocessing steps into three separate concept categories: <italic>Tissue</italic>, <italic>Patch</italic>, and <italic>Feature</italic>. Each of these concepts is linked with a parameter class to help users modify the preprocessing steps by following their own experimental requirements. A manager class named <italic>WSI_collector</italic> helps users unify the initialisation, calculation and loading process of these concepts to further decrease the complexity of usage.</p>
          <sec id="sec4.4.2.1.1">
            <title>Reading raw WSI data</title>
            <p id="p0265">HistoMIL can handle different data formats by implementing slide_backend wrappers for widely considered packages such as OpenSlide-Python.<xref rid="bib22" ref-type="bibr"><sup>18</sup></xref> This design can help researchers access the raw data of different WSI formats and provides a common interface for the subsequent steps. For instance, the scale pyramid in WSI processing can be more important than other areas as some WSI files naturally include multi-scale representation of the raw data.<xref rid="bib29" ref-type="bibr"><sup>25</sup></xref> A series of downscaled or upscaled versions of the original slide will help the potential deep learning models get more information. HistoMIL will create a common scale pyramid as metadata of a WSI file. Also, researchers can implement customised slide wrappers to access additional slide formats. This modular design enables the rapid integration of customised slide backends into existing pipelines, thereby facilitating the support of different datasets. All these elements are consolidated within the <italic>Slide</italic> instance when using HistoMIL.</p>
          </sec>
          <sec id="sec4.4.2.1.2">
            <title>Tissue masking and patch extraction</title>
            <p id="p0270">WSIs typically involve a considerable area that only includes non-biologically relevant background elements such as glass or marker pen. HistoMIL includes a <italic>Tissue</italic> class to identify and remove these areas by generating a tissue mask. Inspired by other ML-oriented packages, the <italic>Tissue</italic> class includes a wrapper for an Otsu function to categorise pixels into foreground or background.<xref rid="bib32" ref-type="bibr"><sup>28</sup></xref> Some basic morphological operations are integrated to eliminate small holes within the tissue region. Researchers can also implement different wrapper functions for different purposes. Patch extraction, which aims to decrease the cost of GPU memory when training a model, works in a similar manner. A large WSI can be partitioned into small patches by using an integrated function to iteratively walk through the tissue area. All the intermediate data is stored for further usage, and the proposed implementation can avoid filling available memory with enhanced memory efficiency. By using this scale pyramid, we configure the segmentation process in higher scale pyramid levels as our default setting to decrease computational costs. Similarly, the default settings of patch selection functions, which can decide whether current patches need to be processed in a pipeline, are applied at a high pyramid level. We use a multiple processing pool to further accelerate the patching step, similarly to the CLAM package.<xref rid="bib32" ref-type="bibr"><sup>28</sup></xref> All these parameters (such as step size and window size in the patch concept class) can be easily modified in related parameter classes to fit different requests.</p>
          </sec>
          <sec id="sec4.4.2.1.3">
            <title>Feature extraction</title>
            <p id="p0275">A pre-calculation step for the feature extraction part may decrease time costs during training. Some MIL algorithms<xref rid="bib18" ref-type="bibr"><sup>14</sup></xref><xref rid="bib2" ref-type="bibr"><sup>32</sup></xref><sup>,</sup><xref rid="bib3" ref-type="bibr"><sup>36</sup></xref> can work with pre-trained feature extraction networks and choose to calculate feature vectors for each selected patch in a preprocessing step. By following this paradigm, we designed a <italic>Feature</italic> class to synchronise the manipulation and maintenance of the feature vectors generated from each slide. This design can also simplify the potential clustering process for feature vectors within each bag or clustering on the entire dataset. Hence, preprocessing, which would have required the implementation of multiple functions and methods, has been simplified to a few lines of code invocation in HistoMIL (<xref rid="fig2" ref-type="fig">Figure 2</xref>A).</p>
          </sec>
        </sec>
        <sec id="sec4.4.2.2">
          <title>Model training</title>
          <p id="p0280">In HistoMIL, model training is a crucial part especially for the target molecular labels that may be difficult to identify by only considering tissue morphology differences. MIL has been introduced as a higher performing weakly supervised learning protocol for WSI classification. A slide (or WSI) is considered as a bag of instances in the MIL protocol. Each bag may contain hundreds or thousands of instances (patches) that are assembled into slide-level features for classification. To the best of our knowledge, no existing package to date has been designed to handle MIL methods. There is considerable complexity in building a MIL pipeline for WSI classification tasks due to the inherent complexity of WSI formats, heterogeneous nature of cell morphology, and unbalanced target labels. HistoMIL is designed to simplify the implementation of MIL-based pipelines by implementing Self-Supervised Learning and Multiple Instance Learning modules (<xref rid="fig2" ref-type="fig">Figures 2</xref>B and 2C).</p>
          <sec id="sec4.4.2.2.1">
            <title>Self-supervised learning for feature extraction</title>
            <p id="p0285">End-to-End training for a MIL pipeline that consists of feature extractors and aggregators may be computationally expensive, especially in a large WSI. Therefore, the existing model either uses fixed patch features derived from CNNs or simply employs a small number of high-scoring patches to update feature extractors. Inspired by the TL paradigm, the feature extractor network can re-use pre-trained parameters from general image classification domains (e.g., ImageNet). The advantage is that there are different pre-trained feature extraction networks that can be chosen for different tasks. Thus, target classification models can be trained quickly with a pre-trained feature extractor network. However, if a pre-trained feature network is not trained on a WSI dataset, it may require extra effort in the preprocessing steps and the model may not converge. The process of aggregating and classifying may also be susceptible to the issue of overfitting and inadequate supervision, resulting in potential bias and other limitations.</p>
            <p id="p0290">Self-Supervised Learning (SSL) is frequently mentioned as a solution for the mentioned problems. While some studies, such as Lu et al.,<xref rid="bib32" ref-type="bibr"><sup>28</sup></xref> have shown that the SSL phase is a crucial element in a pipeline, integrating the WSI dataset into an SSL pipeline remains difficult. HistoMIL offers an easy way to accomplish this part by using a user-friendly SSL module. To train a feature extractor, the HistoMIL package offers an easy way to apply self-supervised training methods. Firstly, we introduce a trainer class to simplify the training process on a WSI dataset. A wrapper class is created for the timm package<xref rid="bib35" ref-type="bibr"><sup>33</sup></xref> which includes most of the popular backbone architectures and potential pre-trained parameters. Some widely employed SSL methods have been implemented, such as the MoCo<xref rid="bib53" ref-type="bibr"><sup>53</sup></xref><sup>,</sup><xref rid="bib54" ref-type="bibr"><sup>54</sup></xref> and SimCLR<xref rid="bib55" ref-type="bibr"><sup>55</sup></xref> methods. By using these built-in methods, HistoMIL also offers predetermined hyperparameters to further simplify the training process to new users. Also, it is specifically designed for WSI data. Users can simply download raw data from projects such as TCGA and do not need to worry about preprocessing and data augmentation. In addition, utilising the feature extractor network trained by HistoMIL’s SSL module negates the necessity to account for discrepancies in operator implementation that may arise when importing trained parameters from other packages. Moreover, by embedding SSL methods within the HistoMIL package, researchers can effortlessly incorporate the selection of SSL methods and backbone architectures as hyperparameters when optimising their models.</p>
          </sec>
          <sec id="sec4.4.2.2.2">
            <title>MIL methods</title>
            <p id="p0295">To train a slide-level classifier on target labels, HistoMIL offers a variety of algorithms that are ready to use. To the best of our knowledge, no existing package can simplify the training and classification process for MIL algorithms in WSI datasets. Transfer Learning (TL) protocols and Multiple Instance Learning (MIL) protocols have both been widely considered by researchers for classification tasks on WSIs. While packages such as DeepMed<xref rid="bib31" ref-type="bibr"><sup>27</sup></xref> have been designed to apply TL-based models on WSI datasets, there are considerable difficulties in implementing a MIL model for WSI classification tasks. Whole slide images often encompass several gigabytes or even terabytes, and the aggregation function of MIL methods may suffer due to its high-dimensional feature space, with tens of thousands of feature vectors per slide. Moreover, there is the lack of a standard implementation for various MIL algorithms. For molecular labels, training a MIL model may be even more difficult when faced with problems such as heterogeneity of cell morphology and imbalanced data. In some extreme cases, the target classification model may be vulnerable to overfitting, and is unable to explore rich feature representations because of the insufficient supervised signal. All these challenges require users to have considerable experience with implementing and training deep neural networks. This requirement may exclude researchers who need these models but lack the necessary experience. HistoMIL simplifies this aspect by introducing built-in MIL modules with pre-defined hyperparameters.</p>
            <p id="p0300">In HistoMIL, a customizable sampler function in our Dataloader implementation is introduced that only samples some instances from each bag. Different batch sizes can be used if the MIL algorithm needs to sample N instances from each bag. But if all the patches must be read in at once, the batch size should be fixed at 1 and this may lead to an unstable training process. In our default setting, we chose to decrease the initial learning rate and accumulate gradients over the training steps to smooth the optimisation process. We also include a cohort instance during training to handle patient metadata, which means users can also assign pseudo labels for each patch based on the label per slide, and this enables users to train their model using the transfer learning paradigm as well.</p>
          </sec>
        </sec>
      </sec>
      <sec id="sec4.4.3">
        <title>Trainer and experiment manager</title>
        <p id="p0305">A significant amount of time and effort is spent on the tuning process and hyperparameter selection steps when training on WSI datasets. We designed the <italic>trainer</italic> class and <italic>experiment</italic> class in HistoMIL, which are built on the PyTorch Lightning framework, to cut these costs. Our target is to simplify the tuning process for different target labels. The SSL, TL and MIL algorithms mentioned above are implemented using a related PyTorch Lightning module in HistoMIL. Researchers can initialise a PyTorch Lightning instance with HistoMIL, and this instance can be easily fed into third-party tuners such as the Ray tuner package.<xref rid="bib37" ref-type="bibr"><sup>37</sup></xref></p>
      </sec>
      <sec id="sec4.4.4">
        <title>Software and package</title>
        <p id="p0310">HistoMIL utilises Python as the primary implementation language and PyTorch as the underlying deep learning platform.<xref rid="bib35" ref-type="bibr"><sup>33</sup></xref> Using these libraries, HistoMIL can easily implement customised algorithms. Furthermore, we introduced PyTorch Lightning as a higher-level framework to simplify the implementation of training code.<xref rid="bib4" ref-type="bibr"><sup>35</sup></xref> All the built-in algorithms in HistoMIL adhere to PyTorch Lightning’s design philosophy, which decomposes the training process into different functions. This facilitates user customization while also ensuring concise implementation. The HistoMIL package is available at the following GitHub repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/secrierlab/HistoMIL" id="intref0110">https://github.com/secrierlab/HistoMIL</ext-link>. The version of the code used to produce the results of the paper has been deposited at Zenodo (<ext-link ext-link-type="doi" xlink:href="10.5281/zenodo.8220572" id="intref0115">https://doi.org/10.5281/zenodo.8220572</ext-link>).</p>
      </sec>
      <sec id="sec4.4.5">
        <title>Development environment</title>
        <p id="p0315">In the present research, we employed the PyTorch framework due to its renowned flexibility and accessibility for deep learning tasks. The computational experiments were executed on a cluster equipped with NVIDIA A100 or V100 graphics cards and 1TB of storage. Our system runs on the Ubuntu 18.04 operating system, with the Anaconda platform facilitating Python-based development. Additionally, to enhance the training efficiency of our deep learning models, GPU acceleration was leveraged using the CUDA toolkit provided by NVIDIA.</p>
      </sec>
    </sec>
    <sec id="sec4.5">
      <title>Quantification and statistical analysis</title>
      <sec id="sec4.5.1">
        <title>Evaluation metrics</title>
        <p id="p0320">To facilitate a robust comparison with state-of-the-art predictors, we adopted the Area Under the Receiver Operating Characteristic Curve (AUROC) as our primary performance evaluation criterion to validate the efficacy of our model. The dataset was randomly partitioned into training and test sets in an 8:2 ratio. Classification tasks for all datasets utilized in this study pertained to binary classification based on varying levels of gene expression. The AUROC metric encapsulates both sensitivity and specificity of predictions. Generally, a higher AUC score indicates superior classifier performance for a given task. An AUC score of 0.5 signifies random guessing, whereas a score of 1 denotes perfect classification.</p>
        <p id="p0325">The pathway enrichment analysis was performed via a hypergeometric test as implemented in GeneMania. All enriched pathways with an FDR&gt;0.1 were taken into account.</p>
      </sec>
    </sec>
  </sec>
</body>
<back>
  <ref-list id="cebib0010">
    <title>References</title>
    <ref id="bib5">
      <label>1</label>
      <element-citation publication-type="journal" id="sref5">
        <person-group person-group-type="author">
          <name>
            <surname>LeCun</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Deep learning</article-title>
        <source>Nature</source>
        <volume>521</volume>
        <year>2015</year>
        <fpage>436</fpage>
        <lpage>444</lpage>
        <pub-id pub-id-type="pmid">26017442</pub-id>
      </element-citation>
    </ref>
    <ref id="bib6">
      <label>2</label>
      <element-citation publication-type="journal" id="sref6">
        <person-group person-group-type="author">
          <name>
            <surname>Khened</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kori</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Rajkumar</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Krishnamurthi</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Srinivasan</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>A generalized deep learning framework for whole-slide image segmentation and analysis</article-title>
        <source>Sci. Rep.</source>
        <volume>11</volume>
        <year>2021</year>
        <fpage>11579</fpage>
        <pub-id pub-id-type="pmid">34078928</pub-id>
      </element-citation>
    </ref>
    <ref id="bib7">
      <label>3</label>
      <element-citation publication-type="journal" id="sref7">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>C.-L.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>C.-C.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>W.-H.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>S.-H.</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>Y.-C.</given-names>
          </name>
          <name>
            <surname>Hsu</surname>
            <given-names>T.-I.</given-names>
          </name>
          <name>
            <surname>Hsiao</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Yeh</surname>
            <given-names>C.-Y.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>C.-Y.</given-names>
          </name>
        </person-group>
        <article-title>An annotation-free whole-slide training approach to pathological classification of lung cancer types using deep learning</article-title>
        <source>Nat. Commun.</source>
        <volume>12</volume>
        <year>2021</year>
        <fpage>1193</fpage>
        <pub-id pub-id-type="pmid">33608558</pub-id>
      </element-citation>
    </ref>
    <ref id="bib8">
      <label>4</label>
      <element-citation publication-type="journal" id="sref8">
        <person-group person-group-type="author">
          <name>
            <surname>Cui</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Xiong</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>A deep learning algorithm for one-step contour aware nuclei segmentation of histopathology images</article-title>
        <source>Med. Biol. Eng. Comput.</source>
        <volume>57</volume>
        <year>2019</year>
        <fpage>2027</fpage>
        <lpage>2043</lpage>
        <pub-id pub-id-type="pmid">31346949</pub-id>
      </element-citation>
    </ref>
    <ref id="bib9">
      <label>5</label>
      <element-citation publication-type="journal" id="sref9">
        <person-group person-group-type="author">
          <name>
            <surname>Graham</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Vu</surname>
            <given-names>Q.D.</given-names>
          </name>
          <name>
            <surname>Raza</surname>
            <given-names>S.E.A.</given-names>
          </name>
          <name>
            <surname>Azam</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Tsang</surname>
            <given-names>Y.W.</given-names>
          </name>
          <name>
            <surname>Kwak</surname>
            <given-names>J.T.</given-names>
          </name>
          <name>
            <surname>Rajpoot</surname>
            <given-names>N.</given-names>
          </name>
        </person-group>
        <article-title>Hover-net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images</article-title>
        <source>Med. Image Anal.</source>
        <volume>58</volume>
        <year>2019</year>
        <object-id pub-id-type="publisher-id">101563</object-id>
      </element-citation>
    </ref>
    <ref id="bib10">
      <label>6</label>
      <element-citation publication-type="book" id="sref10">
        <person-group person-group-type="author">
          <name>
            <surname>Zhu</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Yao</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <part-title>Wsisa: making survival prediction from whole slide histopathological images</part-title>
        <year>2017</year>
        <fpage>7234</fpage>
        <lpage>7242</lpage>
      </element-citation>
    </ref>
    <ref id="bib11">
      <label>7</label>
      <element-citation publication-type="journal" id="sref11">
        <person-group person-group-type="author">
          <name>
            <surname>Krithiga</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Geetha</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Breast cancer detection, segmentation and classification on histopathology images analysis: a systematic review</article-title>
        <source>Arch. Comput. Methods Eng.</source>
        <volume>28</volume>
        <year>2021</year>
        <fpage>2607</fpage>
        <lpage>2619</lpage>
      </element-citation>
    </ref>
    <ref id="bib12">
      <label>8</label>
      <element-citation publication-type="journal" id="sref12">
        <person-group person-group-type="author">
          <name>
            <surname>Howard</surname>
            <given-names>F.M.</given-names>
          </name>
          <name>
            <surname>Dolezal</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Kochanny</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Khramtsova</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Vickery</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Srisuwananukorn</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Woodard</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Nanda</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Perou</surname>
            <given-names>C.M.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Integration of clinical features and deep learning on pathology for the prediction of breast cancer recurrence assays and risk of recurrence</article-title>
        <source>NPJ Breast Cancer</source>
        <volume>9</volume>
        <year>2023</year>
        <fpage>25</fpage>
        <pub-id pub-id-type="doi">10.1038/s41523-023-00530-5</pub-id>
        <pub-id pub-id-type="pmid">37059742</pub-id>
      </element-citation>
    </ref>
    <ref id="bib13">
      <label>9</label>
      <element-citation publication-type="journal" id="sref13">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>D.M.</given-names>
          </name>
          <name>
            <surname>Rong</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Zhan</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Fujimoto</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Minna</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Wistuba</surname>
            <given-names>I.I.</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Artificial intelligence in lung cancer pathology image analysis</article-title>
        <source>Cancers</source>
        <volume>11</volume>
        <year>2019</year>
        <fpage>1673</fpage>
        <pub-id pub-id-type="pmid">31661863</pub-id>
      </element-citation>
    </ref>
    <ref id="bib14">
      <label>10</label>
      <element-citation publication-type="journal" id="sref14">
        <person-group person-group-type="author">
          <name>
            <surname>Schmauch</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Romagnoni</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Pronier</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Saillard</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Maillé</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Calderaro</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Kamoun</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Sefta</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Toldo</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Zaslavskiy</surname>
            <given-names>M.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A deep learning model to predict RNA-Seq expression of tumours from whole slide images</article-title>
        <source>Nat. Commun.</source>
        <volume>11</volume>
        <year>2020</year>
        <fpage>3877</fpage>
        <pub-id pub-id-type="pmid">32747659</pub-id>
      </element-citation>
    </ref>
    <ref id="bib15">
      <label>11</label>
      <element-citation publication-type="journal" id="sref15">
        <person-group person-group-type="author">
          <name>
            <surname>Schirris</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Gavves</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Nederlof</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Horlings</surname>
            <given-names>H.M.</given-names>
          </name>
          <name>
            <surname>Teuwen</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>DeepSMILE: self-supervised heterogeneity-aware multiple instance learning for DNA damage response defect classification directly from H&amp;E whole-slide images</article-title>
        <comment>Preprint at</comment>
        <source>arXiv</source>
        <year>2021</year>
        <pub-id pub-id-type="doi">10.48550/arXiv.2107.09405</pub-id>
      </element-citation>
    </ref>
    <ref id="bib16">
      <label>12</label>
      <element-citation publication-type="journal" id="sref16">
        <person-group person-group-type="author">
          <name>
            <surname>Kather</surname>
            <given-names>J.N.</given-names>
          </name>
          <name>
            <surname>Pearson</surname>
            <given-names>A.T.</given-names>
          </name>
          <name>
            <surname>Halama</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Jäger</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Krause</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Loosen</surname>
            <given-names>S.H.</given-names>
          </name>
          <name>
            <surname>Marx</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Boor</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Tacke</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Neumann</surname>
            <given-names>U.P.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep learning can predict microsatellite instability directly from histology in gastrointestinal cancer</article-title>
        <source>Nat. Med.</source>
        <volume>25</volume>
        <year>2019</year>
        <fpage>1054</fpage>
        <lpage>1056</lpage>
        <pub-id pub-id-type="doi">10.1038/s41591-019-0462-y</pub-id>
        <pub-id pub-id-type="pmid">31160815</pub-id>
      </element-citation>
    </ref>
    <ref id="bib17">
      <label>13</label>
      <element-citation publication-type="journal" id="sref17">
        <person-group person-group-type="author">
          <name>
            <surname>Coudray</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Ocampo</surname>
            <given-names>P.S.</given-names>
          </name>
          <name>
            <surname>Sakellaropoulos</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Narula</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Snuderl</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Fenyö</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Moreira</surname>
            <given-names>A.L.</given-names>
          </name>
          <name>
            <surname>Razavian</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Tsirigos</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Classification and mutation prediction from non–small cell lung cancer histopathology images using deep learning</article-title>
        <source>Nat. Med.</source>
        <volume>24</volume>
        <year>2018</year>
        <fpage>1559</fpage>
        <lpage>1567</lpage>
        <pub-id pub-id-type="doi">10.1038/s41591-018-0177-5</pub-id>
        <pub-id pub-id-type="pmid">30224757</pub-id>
      </element-citation>
    </ref>
    <ref id="bib18">
      <label>14</label>
      <element-citation publication-type="journal" id="sref18">
        <person-group person-group-type="author">
          <name>
            <surname>Yao</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Jonnagaddala</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Hawkins</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Whole slide images based cancer survival prediction using attention guided deep multiple instance learning networks</article-title>
        <source>Med. Image Anal.</source>
        <volume>65</volume>
        <year>2020</year>
        <object-id pub-id-type="publisher-id">101789</object-id>
      </element-citation>
    </ref>
    <ref id="bib19">
      <label>15</label>
      <element-citation publication-type="book" id="sref19">
        <person-group person-group-type="author">
          <name>
            <surname>Qu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <part-title>Dgmil: Distribution Guided Multiple Instance Learning for Whole Slide Image Classification</part-title>
        <year>2022</year>
        <publisher-name>Springer</publisher-name>
        <fpage>24</fpage>
        <lpage>34</lpage>
      </element-citation>
    </ref>
    <ref id="bib20">
      <label>16</label>
      <element-citation publication-type="book" id="sref20">
        <person-group person-group-type="author">
          <name>
            <surname>Clark</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <part-title>Pillow (Pil Fork) Documentation</part-title>
        <year>2015</year>
        <publisher-name>readthedocs</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib21">
      <label>17</label>
      <element-citation publication-type="book" id="sref21">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <part-title>Deep Residual Learning for Image Recognition</part-title>
        <year>2016</year>
        <fpage>770</fpage>
        <lpage>778</lpage>
      </element-citation>
    </ref>
    <ref id="bib22">
      <label>18</label>
      <element-citation publication-type="journal" id="sref22">
        <person-group person-group-type="author">
          <name>
            <surname>Goode</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Gilbert</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Harkes</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Jukic</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Satyanarayanan</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>OpenSlide: A vendor-neutral software foundation for digital pathology</article-title>
        <source>J. Pathol. Inform.</source>
        <volume>4</volume>
        <year>2013</year>
        <fpage>27</fpage>
        <pub-id pub-id-type="pmid">24244884</pub-id>
      </element-citation>
    </ref>
    <ref id="bib23">
      <label>19</label>
      <element-citation publication-type="book" id="sref23">
        <person-group person-group-type="author">
          <name>
            <surname>Moore</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Linkert</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Blackburn</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Carroll</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Ferguson</surname>
            <given-names>R.K.</given-names>
          </name>
          <name>
            <surname>Flynn</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Gillen</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Leigh</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Lindner</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <part-title>OMERO and Bio-Formats 5: Flexible Access to Large Bioimaging Datasets at Scale</part-title>
        <year>2015</year>
        <publisher-name>SPIE</publisher-name>
        <fpage>37</fpage>
        <lpage>42</lpage>
      </element-citation>
    </ref>
    <ref id="bib24">
      <label>20</label>
      <element-citation publication-type="journal" id="sref24">
        <person-group person-group-type="author">
          <name>
            <surname>Bridge</surname>
            <given-names>C.P.</given-names>
          </name>
          <name>
            <surname>Gorman</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Pieper</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Doyle</surname>
            <given-names>S.W.</given-names>
          </name>
          <name>
            <surname>Lennerz</surname>
            <given-names>J.K.</given-names>
          </name>
          <name>
            <surname>Kalpathy-Cramer</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Clunie</surname>
            <given-names>D.A.</given-names>
          </name>
          <name>
            <surname>Fedorov</surname>
            <given-names>A.Y.</given-names>
          </name>
          <name>
            <surname>Herrmann</surname>
            <given-names>M.D.</given-names>
          </name>
        </person-group>
        <article-title>Highdicom: A python library for standardized encoding of image annotations and machine learning model outputs in pathology and radiology</article-title>
        <source>J. Digit. Imaging</source>
        <volume>35</volume>
        <year>2022</year>
        <fpage>1719</fpage>
        <lpage>1737</lpage>
        <pub-id pub-id-type="pmid">35995898</pub-id>
      </element-citation>
    </ref>
    <ref id="bib25">
      <label>21</label>
      <element-citation publication-type="journal" id="sref25">
        <person-group person-group-type="author">
          <name>
            <surname>Muñoz-Aguirre</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Ntasis</surname>
            <given-names>V.F.</given-names>
          </name>
          <name>
            <surname>Rojas</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Guigó</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>PyHIST: a histological image segmentation tool</article-title>
        <source>PLoS Comput. Biol.</source>
        <volume>16</volume>
        <year>2020</year>
        <object-id pub-id-type="publisher-id">e1008349</object-id>
      </element-citation>
    </ref>
    <ref id="bib26">
      <label>22</label>
      <element-citation publication-type="book" id="sref26">
        <person-group person-group-type="author">
          <name>
            <surname>Korpihalkola</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Sipola</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Kokkonen</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <part-title>Color-optimized One-Pixel Attack against Digital Pathology Images</part-title>
        <year>2021</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>206</fpage>
        <lpage>213</lpage>
      </element-citation>
    </ref>
    <ref id="bib27">
      <label>23</label>
      <element-citation publication-type="journal" id="sref27">
        <person-group person-group-type="author">
          <name>
            <surname>Marini</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Otálora</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Podareanu</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>van Rijthoven</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>van der Laak</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Ciompi</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Müller</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Atzori</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Multi_Scale_Tools: A Python Library to Exploit Multi-Scale Whole Slide Images</article-title>
        <source>Front. Comput. Sci.</source>
        <volume>3</volume>
        <year>2021</year>
        <pub-id pub-id-type="doi">10.3389/fcomp.2021.684521</pub-id>
      </element-citation>
    </ref>
    <ref id="bib28">
      <label>24</label>
      <element-citation publication-type="journal" id="sref28">
        <person-group person-group-type="author">
          <name>
            <surname>Barnabas</surname>
            <given-names>G.D.</given-names>
          </name>
          <name>
            <surname>Goebeler</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Tsui</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Bush</surname>
            <given-names>J.W.</given-names>
          </name>
          <name>
            <surname>Lange</surname>
            <given-names>P.F.</given-names>
          </name>
        </person-group>
        <article-title>ASAP— Automated Sonication-Free Acid-Assisted Proteomes— from Cells and FFPE Tissues</article-title>
        <source>Anal. Chem.</source>
        <volume>95</volume>
        <year>2023</year>
        <fpage>3291</fpage>
        <lpage>3299</lpage>
        <pub-id pub-id-type="pmid">36724070</pub-id>
      </element-citation>
    </ref>
    <ref id="bib29">
      <label>25</label>
      <element-citation publication-type="journal" id="sref29">
        <person-group person-group-type="author">
          <name>
            <surname>Pocock</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Graham</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Vu</surname>
            <given-names>Q.D.</given-names>
          </name>
          <name>
            <surname>Jahanifar</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Deshpande</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Hadjigeorghiou</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Shephard</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Bashir</surname>
            <given-names>R.M.S.</given-names>
          </name>
          <name>
            <surname>Bilal</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>W.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>TIAToolbox as an end-to-end library for advanced tissue image analytics</article-title>
        <source>Commun. Med.</source>
        <volume>2</volume>
        <year>2022</year>
        <fpage>120</fpage>
        <pub-id pub-id-type="pmid">36168445</pub-id>
      </element-citation>
    </ref>
    <ref id="bib30">
      <label>26</label>
      <element-citation publication-type="journal" id="sref30">
        <person-group person-group-type="author">
          <name>
            <surname>Otálora</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Marini</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Podareanu</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Hekster</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Tellez</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>van der Laak</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Müller</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Atzori</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>stainlib: a python library for augmentation and normalization of histopathology H&amp;E images</article-title>
        <comment>Preprint at</comment>
        <source>bioRxiv</source>
        <year>2022</year>
        <pub-id pub-id-type="doi">10.1101/2022.05.17.492245</pub-id>
      </element-citation>
    </ref>
    <ref id="bib31">
      <label>27</label>
      <element-citation publication-type="journal" id="sref31">
        <person-group person-group-type="author">
          <name>
            <surname>van Treeck</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Cifci</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Laleh</surname>
            <given-names>N.G.</given-names>
          </name>
          <name>
            <surname>Saldanha</surname>
            <given-names>O.L.</given-names>
          </name>
          <name>
            <surname>Loeffler</surname>
            <given-names>C.M.</given-names>
          </name>
          <name>
            <surname>Hewitt</surname>
            <given-names>K.J.</given-names>
          </name>
          <name>
            <surname>Muti</surname>
            <given-names>H.S.</given-names>
          </name>
          <name>
            <surname>Echle</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Seibel</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Seraphin</surname>
            <given-names>T.P.</given-names>
          </name>
        </person-group>
        <article-title>DeepMed: A unified, modular pipeline for end-to-end deep learning in computational pathology</article-title>
        <comment>Preprint at</comment>
        <source>bioRxiv</source>
        <year>2021</year>
        <pub-id pub-id-type="doi">10.1101/2021.12.19.473344</pub-id>
      </element-citation>
    </ref>
    <ref id="bib32">
      <label>28</label>
      <element-citation publication-type="journal" id="sref32">
        <person-group person-group-type="author">
          <name>
            <surname>Lu</surname>
            <given-names>M.Y.</given-names>
          </name>
          <name>
            <surname>Williamson</surname>
            <given-names>D.F.K.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>T.Y.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>R.J.</given-names>
          </name>
          <name>
            <surname>Barbieri</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Mahmood</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>Data-efficient and weakly supervised computational pathology on whole-slide images</article-title>
        <source>Nat. Biomed. Eng.</source>
        <volume>5</volume>
        <year>2021</year>
        <fpage>555</fpage>
        <lpage>570</lpage>
        <pub-id pub-id-type="pmid">33649564</pub-id>
      </element-citation>
    </ref>
    <ref id="bib33">
      <label>29</label>
      <element-citation publication-type="journal" id="sref33">
        <person-group person-group-type="author">
          <name>
            <surname>Cardoso</surname>
            <given-names>M.J.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Brown</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Kerfoot</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Murrey</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Myronenko</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>D.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>MONAI: An open-source framework for deep learning in healthcare</article-title>
        <comment>Preprint at</comment>
        <source>arXiv</source>
        <year>2022</year>
        <pub-id pub-id-type="doi">10.48550/arXiv.2211.02701</pub-id>
      </element-citation>
    </ref>
    <ref id="bib34">
      <label>30</label>
      <element-citation publication-type="journal" id="sref34">
        <person-group person-group-type="author">
          <name>
            <surname>Berman</surname>
            <given-names>A.G.</given-names>
          </name>
          <name>
            <surname>Orchard</surname>
            <given-names>W.R.</given-names>
          </name>
          <name>
            <surname>Gehrung</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Markowetz</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>PathML: a unified framework for whole-slide image analysis with deep learning</article-title>
        <comment>Preprint at</comment>
        <source>medRxiv</source>
        <year>2021</year>
        <pub-id pub-id-type="doi">10.1101/2021.07.07.21260138</pub-id>
      </element-citation>
    </ref>
    <ref id="bib1">
      <label>31</label>
      <element-citation publication-type="book" id="sref1">
        <person-group person-group-type="author">
          <name>
            <surname>Leiby</surname>
            <given-names>J.S.</given-names>
          </name>
          <name>
            <surname>Hao</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Kang</surname>
            <given-names>G.H.</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>J.W.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <part-title>Attention-Based Multiple Instance Learning with Self-Supervision to Predict Microsatellite Instability in Colorectal Cancer from Histology Whole-Slide Images</part-title>
        <year>2022</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>3068</fpage>
        <lpage>3071</lpage>
      </element-citation>
    </ref>
    <ref id="bib2">
      <label>32</label>
      <element-citation publication-type="journal" id="sref2">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Eliceiri</surname>
            <given-names>K.W.</given-names>
          </name>
        </person-group>
        <article-title>Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning</article-title>
        <source>Conf. Comput. Vis. Pattern Recognit. Workshops</source>
        <volume>2021</volume>
        <year>2021</year>
        <fpage>14318</fpage>
        <lpage>14328</lpage>
        <pub-id pub-id-type="pmid">35047230</pub-id>
      </element-citation>
    </ref>
    <ref id="bib35">
      <label>33</label>
      <element-citation publication-type="book" id="sref35">
        <person-group person-group-type="author">
          <name>
            <surname>Wightman</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <part-title>Pytorch Image Models</part-title>
        <year>2019</year>
        <publisher-name>GitHub</publisher-name>
        <ext-link ext-link-type="uri" xlink:href="https://github.com/rwightman/pytorch-image-models" id="interref0010">https://github.com/rwightman/pytorch-image-models</ext-link>
      </element-citation>
    </ref>
    <ref id="bib36">
      <label>34</label>
      <element-citation publication-type="book" id="sref36">
        <person-group person-group-type="author">
          <name>
            <surname>Paszke</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Gross</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Massa</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Lerer</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Bradbury</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Chanan</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Killeen</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Gimelshein</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Antiga</surname>
            <given-names>L.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>PyTorch: An Imperative Style, High-Performance Deep Learning Library</part-title>
        <series>Advances in Neural Information Processing Systems</series>
        <volume>32</volume>
        <year>2019</year>
        <publisher-name>Curran Associates Inc</publisher-name>
        <fpage>8024</fpage>
        <lpage>8035</lpage>
      </element-citation>
    </ref>
    <ref id="bib4">
      <label>35</label>
      <element-citation publication-type="journal" id="sref4">
        <person-group person-group-type="author">
          <name>
            <surname>Falcon</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <article-title>The PyTorch Lightning team</article-title>
        <source>Pytorch Lightning</source>
        <volume>3</volume>
        <year>2019</year>
        <fpage>6</fpage>
      </element-citation>
    </ref>
    <ref id="bib3">
      <label>36</label>
      <element-citation publication-type="journal" id="sref3">
        <person-group person-group-type="author">
          <name>
            <surname>Shao</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Bian</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>Transmil: Transformer based correlated multiple instance learning for whole slide image classification</article-title>
        <source>Adv. Neural Inf. Process. Syst.</source>
        <volume>34</volume>
        <year>2021</year>
        <fpage>2136</fpage>
        <lpage>2147</lpage>
      </element-citation>
    </ref>
    <ref id="bib37">
      <label>37</label>
      <element-citation publication-type="journal" id="sref37">
        <person-group person-group-type="author">
          <name>
            <surname>Kiran</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Ozyildirim</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Hyperparameter tuning for deep reinforcement learning applications</article-title>
        <comment>Preprint at</comment>
        <source>arXiv</source>
        <year>2022</year>
        <pub-id pub-id-type="doi">10.48550/arXiv.2201.11182</pub-id>
      </element-citation>
    </ref>
    <ref id="bib38">
      <label>38</label>
      <element-citation publication-type="journal" id="sref38">
        <person-group person-group-type="author">
          <name>
            <surname>Murchan</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Ó'Brien</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>O'Connell</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>McNevin</surname>
            <given-names>C.S.</given-names>
          </name>
          <name>
            <surname>Baird</surname>
            <given-names>A.M.</given-names>
          </name>
          <name>
            <surname>Sheils</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Ó Broin</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Finn</surname>
            <given-names>S.P.</given-names>
          </name>
        </person-group>
        <article-title>Deep Learning of Histopathological Features for the Prediction of Tumour Molecular Genetics</article-title>
        <source>Diagnostics (Basel)</source>
        <volume>11</volume>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">1406</object-id>
        <pub-id pub-id-type="doi">10.3390/diagnostics11081406</pub-id>
      </element-citation>
    </ref>
    <ref id="bib39">
      <label>39</label>
      <element-citation publication-type="journal" id="sref39">
        <person-group person-group-type="author">
          <name>
            <surname>Ji</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Ahmad</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>S.T.</given-names>
          </name>
        </person-group>
        <article-title>Direct interactions of mitotic arrest deficient 1 (MAD1) domains with each other and MAD2 conformers are required for mitotic checkpoint signaling</article-title>
        <source>J. Biol. Chem.</source>
        <volume>293</volume>
        <year>2018</year>
        <fpage>484</fpage>
        <lpage>496</lpage>
        <pub-id pub-id-type="doi">10.1074/jbc.RA117.000555</pub-id>
        <pub-id pub-id-type="pmid">29162720</pub-id>
      </element-citation>
    </ref>
    <ref id="bib40">
      <label>40</label>
      <element-citation publication-type="journal" id="sref40">
        <person-group person-group-type="author">
          <name>
            <surname>Carvalhal</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Ribeiro</surname>
            <given-names>S.A.</given-names>
          </name>
          <name>
            <surname>Arocena</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kasciukovic</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Temme</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Koehler</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Huebner</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Griffis</surname>
            <given-names>E.R.</given-names>
          </name>
        </person-group>
        <article-title>The nucleoporin ALADIN regulates Aurora A localization to ensure robust mitotic spindle formation</article-title>
        <source>Mol. Biol. Cell</source>
        <volume>26</volume>
        <year>2015</year>
        <fpage>3424</fpage>
        <lpage>3438</lpage>
        <pub-id pub-id-type="doi">10.1091/mbc.E15-02-0113</pub-id>
        <pub-id pub-id-type="pmid">26246606</pub-id>
      </element-citation>
    </ref>
    <ref id="bib41">
      <label>41</label>
      <element-citation publication-type="journal" id="sref41">
        <person-group person-group-type="author">
          <name>
            <surname>Tanenbaum</surname>
            <given-names>M.E.</given-names>
          </name>
          <name>
            <surname>Macurek</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>van der Vaart</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Galli</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Akhmanova</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Medema</surname>
            <given-names>R.H.</given-names>
          </name>
        </person-group>
        <article-title>A complex of Kif18b and MCAK promotes microtubule depolymerization and is negatively regulated by Aurora kinases</article-title>
        <source>Curr. Biol.</source>
        <volume>21</volume>
        <year>2011</year>
        <fpage>1356</fpage>
        <lpage>1365</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cub.2011.07.017</pub-id>
        <pub-id pub-id-type="pmid">21820309</pub-id>
      </element-citation>
    </ref>
    <ref id="bib42">
      <label>42</label>
      <element-citation publication-type="journal" id="sref42">
        <person-group person-group-type="author">
          <name>
            <surname>Nelson</surname>
            <given-names>A.R.</given-names>
          </name>
          <name>
            <surname>Fingleton</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Rothenberg</surname>
            <given-names>M.L.</given-names>
          </name>
          <name>
            <surname>Matrisian</surname>
            <given-names>L.M.</given-names>
          </name>
        </person-group>
        <article-title>Matrix metalloproteinases: biologic activity and clinical implications</article-title>
        <source>J. Clin. Oncol.</source>
        <volume>18</volume>
        <year>2000</year>
        <fpage>1135</fpage>
        <lpage>1149</lpage>
        <pub-id pub-id-type="doi">10.1200/JCO.2000.18.5.1135</pub-id>
        <pub-id pub-id-type="pmid">10694567</pub-id>
      </element-citation>
    </ref>
    <ref id="bib43">
      <label>43</label>
      <element-citation publication-type="journal" id="sref43">
        <person-group person-group-type="author">
          <name>
            <surname>Gillan</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Matei</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Fishman</surname>
            <given-names>D.A.</given-names>
          </name>
          <name>
            <surname>Gerbin</surname>
            <given-names>C.S.</given-names>
          </name>
          <name>
            <surname>Karlan</surname>
            <given-names>B.Y.</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>D.D.</given-names>
          </name>
        </person-group>
        <article-title>Periostin secreted by epithelial ovarian carcinoma is a ligand for alpha(V)beta(3) and alpha(V)beta(5) integrins and promotes cell motility</article-title>
        <source>Cancer Res.</source>
        <volume>62</volume>
        <year>2002</year>
        <fpage>5358</fpage>
        <lpage>5364</lpage>
        <pub-id pub-id-type="pmid">12235007</pub-id>
      </element-citation>
    </ref>
    <ref id="bib44">
      <label>44</label>
      <element-citation publication-type="journal" id="sref44">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>J.C.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>J.H.</given-names>
          </name>
          <name>
            <surname>Cho</surname>
            <given-names>Y.K.</given-names>
          </name>
          <name>
            <surname>Jung</surname>
            <given-names>C.S.</given-names>
          </name>
          <name>
            <surname>Ahn</surname>
            <given-names>S.H.</given-names>
          </name>
          <name>
            <surname>Gong</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>Y.S.</given-names>
          </name>
          <name>
            <surname>Cho</surname>
            <given-names>D.-H.</given-names>
          </name>
        </person-group>
        <article-title>Expression of SPRR3 is associated with tumor cell proliferation in less advanced stages of breast cancer</article-title>
        <source>Breast Cancer Res. Treat.</source>
        <volume>133</volume>
        <year>2012</year>
        <fpage>909</fpage>
        <lpage>916</lpage>
        <pub-id pub-id-type="pmid">22076481</pub-id>
      </element-citation>
    </ref>
    <ref id="bib45">
      <label>45</label>
      <element-citation publication-type="journal" id="sref45">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Lv</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>C.-X.</given-names>
          </name>
          <name>
            <surname>Guan</surname>
            <given-names>K.-L.</given-names>
          </name>
          <name>
            <surname>Lei</surname>
            <given-names>Q.-Y.</given-names>
          </name>
          <name>
            <surname>Xiong</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Oxidative stress activates SIRT2 to deacetylate and stimulate phosphoglycerate mutase</article-title>
        <source>Cancer Res.</source>
        <volume>74</volume>
        <year>2014</year>
        <fpage>3630</fpage>
        <lpage>3642</lpage>
        <pub-id pub-id-type="pmid">24786789</pub-id>
      </element-citation>
    </ref>
    <ref id="bib46">
      <label>46</label>
      <element-citation publication-type="journal" id="sref46">
        <person-group person-group-type="author">
          <name>
            <surname>Johnson</surname>
            <given-names>D.G.</given-names>
          </name>
          <name>
            <surname>Schneider-Broussard</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Role of E2F in cell cycle control and cancer</article-title>
        <source>Front. Biosci.</source>
        <volume>3</volume>
        <year>1998</year>
        <fpage>d447</fpage>
        <lpage>d448</lpage>
        <pub-id pub-id-type="doi">10.2741/a291</pub-id>
        <pub-id pub-id-type="pmid">9556498</pub-id>
      </element-citation>
    </ref>
    <ref id="bib47">
      <label>47</label>
      <element-citation publication-type="journal" id="sref47">
        <person-group person-group-type="author">
          <name>
            <surname>Saleh</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Alyami</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Alosaimi</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <article-title>Predicting breast cancer based on optimized deep learning approach</article-title>
        <source>Comput. Intell. Neurosci.</source>
        <volume>2022</volume>
        <year>2022</year>
        <fpage>1820777</fpage>
        <pub-id pub-id-type="pmid">35345799</pub-id>
      </element-citation>
    </ref>
    <ref id="bib48">
      <label>48</label>
      <element-citation publication-type="journal" id="sref48">
        <person-group person-group-type="author">
          <name>
            <surname>Colaprico</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Silva</surname>
            <given-names>T.C.</given-names>
          </name>
          <name>
            <surname>Olsen</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Garofano</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Cava</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Garolini</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Sabedot</surname>
            <given-names>T.S.</given-names>
          </name>
          <name>
            <surname>Malta</surname>
            <given-names>T.M.</given-names>
          </name>
          <name>
            <surname>Pagnotta</surname>
            <given-names>S.M.</given-names>
          </name>
          <name>
            <surname>Castiglioni</surname>
            <given-names>I.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>TCGAbiolinks: an R/Bioconductor package for integrative analysis of TCGA data</article-title>
        <source>Nucleic Acids Res.</source>
        <volume>44</volume>
        <year>2016</year>
        <fpage>e71</fpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkv1507</pub-id>
        <pub-id pub-id-type="pmid">26704973</pub-id>
      </element-citation>
    </ref>
    <ref id="bib49">
      <label>49</label>
      <element-citation publication-type="journal" id="sref49">
        <person-group person-group-type="author">
          <name>
            <surname>Warde-Farley</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Donaldson</surname>
            <given-names>S.L.</given-names>
          </name>
          <name>
            <surname>Comes</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Zuberi</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Badrawi</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Chao</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Franz</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Grouios</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Kazi</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Lopes</surname>
            <given-names>C.T.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The GeneMANIA prediction server: biological network integration for gene prioritization and predicting gene function</article-title>
        <source>Nucleic Acids Res.</source>
        <volume>38</volume>
        <year>2010</year>
        <fpage>W214</fpage>
        <lpage>W220</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkq537</pub-id>
        <pub-id pub-id-type="pmid">20576703</pub-id>
      </element-citation>
    </ref>
    <ref id="bib50">
      <label>50</label>
      <element-citation publication-type="journal" id="sref50">
        <person-group person-group-type="author">
          <collab>Cancer Genome Atlas Research Network</collab>
          <name>
            <surname>Weinstein</surname>
            <given-names>J.N.</given-names>
          </name>
          <name>
            <surname>Collisson</surname>
            <given-names>E.A.</given-names>
          </name>
          <name>
            <surname>Mills</surname>
            <given-names>G.B.</given-names>
          </name>
          <name>
            <surname>Shaw</surname>
            <given-names>K.R.M.</given-names>
          </name>
          <name>
            <surname>Ozenberger</surname>
            <given-names>B.A.</given-names>
          </name>
          <name>
            <surname>Ellrott</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Shmulevich</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Stuart</surname>
            <given-names>J.M.</given-names>
          </name>
        </person-group>
        <article-title>The Cancer Genome Atlas Pan-Cancer analysis project</article-title>
        <source>Nat. Genet.</source>
        <volume>45</volume>
        <year>2013</year>
        <fpage>1113</fpage>
        <lpage>1120</lpage>
        <pub-id pub-id-type="doi">10.1038/ng.2764</pub-id>
        <pub-id pub-id-type="pmid">24071849</pub-id>
      </element-citation>
    </ref>
    <ref id="bib51">
      <label>51</label>
      <element-citation publication-type="journal" id="sref51">
        <person-group person-group-type="author">
          <name>
            <surname>Liberzon</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Birger</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Thorvaldsdóttir</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Ghandi</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Mesirov</surname>
            <given-names>J.P.</given-names>
          </name>
          <name>
            <surname>Tamayo</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>The Molecular Signatures Database (MSigDB) hallmark gene set collection</article-title>
        <source>Cell Syst.</source>
        <volume>1</volume>
        <year>2015</year>
        <fpage>417</fpage>
        <lpage>425</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cels.2015.12.004</pub-id>
        <pub-id pub-id-type="pmid">26771021</pub-id>
      </element-citation>
    </ref>
    <ref id="bib52">
      <label>52</label>
      <element-citation publication-type="journal" id="sref52">
        <person-group person-group-type="author">
          <name>
            <surname>Grossman</surname>
            <given-names>R.L.</given-names>
          </name>
          <name>
            <surname>Heath</surname>
            <given-names>A.P.</given-names>
          </name>
          <name>
            <surname>Ferretti</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Varmus</surname>
            <given-names>H.E.</given-names>
          </name>
          <name>
            <surname>Lowy</surname>
            <given-names>D.R.</given-names>
          </name>
          <name>
            <surname>Kibbe</surname>
            <given-names>W.A.</given-names>
          </name>
          <name>
            <surname>Staudt</surname>
            <given-names>L.M.</given-names>
          </name>
        </person-group>
        <article-title>Toward a Shared Vision for Cancer Genomic Data</article-title>
        <source>N. Engl. J. Med.</source>
        <volume>375</volume>
        <year>2016</year>
        <fpage>1109</fpage>
        <lpage>1112</lpage>
        <pub-id pub-id-type="doi">10.1056/NEJMp1607591</pub-id>
        <pub-id pub-id-type="pmid">27653561</pub-id>
      </element-citation>
    </ref>
    <ref id="bib53">
      <label>53</label>
      <element-citation publication-type="book" id="sref53">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Fan</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Girshick</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <part-title>Momentum contrast for unsupervised visual representation learning</part-title>
        <year>2020</year>
        <fpage>9729</fpage>
        <lpage>9738</lpage>
      </element-citation>
    </ref>
    <ref id="bib54">
      <label>54</label>
      <element-citation publication-type="journal" id="sref54">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Fan</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Girshick</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Improved baselines with momentum contrastive learning</article-title>
        <comment>Preprint at</comment>
        <source>arXiv</source>
        <year>2020</year>
        <pub-id pub-id-type="doi">10.48550/arXiv.2003.04297</pub-id>
      </element-citation>
    </ref>
    <ref id="bib55">
      <label>55</label>
      <element-citation publication-type="book" id="sref55">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Kornblith</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Norouzi</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <part-title>A Simple Framework for Contrastive Learning of Visual Representations</part-title>
        <year>2020</year>
        <publisher-name>PMLR</publisher-name>
        <fpage>1597</fpage>
        <lpage>1607</lpage>
      </element-citation>
    </ref>
  </ref-list>
  <sec id="appsec2" sec-type="supplementary-material">
    <title>Supplemental information</title>
    <p id="p0360">
      <supplementary-material content-type="local-data" id="mmc1">
        <caption>
          <p>Document S1. Tables S2, S3 and S5</p>
        </caption>
        <media xlink:href="mmc1.pdf"/>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="mmc2">
        <caption>
          <p>Table S1. Cancer hallmark genes and pathways considered in the analysis, related to STAR Methods</p>
          <p>The gs categories are retrieved from MSigDB.</p>
        </caption>
        <media xlink:href="mmc2.xlsx"/>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="mmc3">
        <caption>
          <p>Table S4. Functional enrichment analysis for the top 50 genes in the models, related to Figure 3C</p>
          <p>The results were obtained using GeneMania and considering a neighborhood of maximum 20 connecting genes in addition to the given gene set.</p>
        </caption>
        <media xlink:href="mmc3.xlsx"/>
      </supplementary-material>
    </p>
  </sec>
  <sec sec-type="data-availability" id="da0010">
    <title>Data and code availability</title>
    <p id="p0030">
      <list list-type="simple" id="ulist0015">
        <list-item id="u0030">
          <label>•</label>
          <p id="p0035">This paper analyzes existing, publicly available data generated by the TCGA Research Network: <ext-link ext-link-type="uri" xlink:href="https://www.cancer.gov/tcga" id="intref0010">https://www.cancer.gov/tcga</ext-link>. The link to the datasets is listed in the <xref rid="sec4.1" ref-type="sec">key resources table</xref>.</p>
        </list-item>
        <list-item id="u0035">
          <label>•</label>
          <p id="p0040">The HistoMIL package is available at GitHub: <ext-link ext-link-type="uri" xlink:href="https://github.com/secrierlab/HistoMIL" id="intref0015">https://github.com/secrierlab/HistoMIL</ext-link>. The version of the code used to produce the results of the paper has been deposited at Zenodo: <ext-link ext-link-type="doi" xlink:href="10.5281/zenodo.8220572" id="intref0020">https://doi.org/10.5281/zenodo.8220572</ext-link>.</p>
        </list-item>
        <list-item id="u0040">
          <label>•</label>
          <p id="p0045">Any additional information required to reanalyze the data reported in this paper is available from the <xref rid="sec4.2.1" ref-type="sec">lead contact</xref> upon request.</p>
        </list-item>
      </list>
    </p>
  </sec>
  <ack id="ack0010">
    <title>Acknowledgments</title>
    <p id="p0330">M.S. and S.P. were supported by a <funding-source id="gs1"><institution-wrap><institution-id institution-id-type="doi">10.13039/100014013</institution-id><institution>UKRI</institution></institution-wrap></funding-source> Future Leaders Fellowship (MR/T042184/1). Work in M.S.’s lab was supported by a <funding-source id="gs2"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100000268</institution-id><institution>BBSRC</institution></institution-wrap></funding-source> equipment grant (BB/R01356X/1) and a <funding-source id="gs3">Wellcome Institutional Strategic Support Fund</funding-source> (204841/Z/16/Z).</p>
    <p id="p0335">We would like to thank Eloise Withnell for testing and providing feedback on the HistoMIL package.</p>
    <sec id="sec5">
      <title>Author contributions</title>
      <p id="p0340">Conceptualization, M.S. and S.P.; Methodology, S.P.; Software, S.P.; Validation, S.P.; Formal Analysis, S.P.; Investigation, S.P. and M.S.; Data Curation, S.P.; Writing – Original Draft, S.P. and M.S.; Writing – Review and Editing, M.S. and S.P.; Visualization, S.P.; Supervision, M.S; Funding Acquisition, M.S.</p>
    </sec>
    <sec sec-type="COI-statement" id="sec6">
      <title>Declaration of interests</title>
      <p id="p0345">The authors declare no competing interests.</p>
    </sec>
    <sec sec-type="inclusion-and-diversity" id="sec7">
      <title>Inclusion and diversity</title>
      <p id="p0350">One or more of the authors of this paper self-identifies as a gender minority in their field of research.</p>
    </sec>
  </ack>
  <fn-group>
    <fn id="appsec1" fn-type="supplementary-material">
      <p id="p0355">Supplemental information can be found online at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.isci.2023.108073" id="intref0120">https://doi.org/10.1016/j.isci.2023.108073</ext-link>.</p>
    </fn>
  </fn-group>
</back>
