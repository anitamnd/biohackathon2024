<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7355231</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btaa478</article-id>
    <article-id pub-id-type="publisher-id">btaa478</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Genome Privacy and Security</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Privacy-preserving construction of generalized linear mixed model for biomedical computation</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Zhu</surname>
          <given-names>Rui</given-names>
        </name>
        <xref ref-type="aff" rid="btaa478-aff1">b1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jiang</surname>
          <given-names>Chao</given-names>
        </name>
        <xref ref-type="aff" rid="btaa478-aff2">b2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Xiaofeng</given-names>
        </name>
        <xref ref-type="aff" rid="btaa478-aff1">b1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Shuang</given-names>
        </name>
        <xref ref-type="aff" rid="btaa478-aff1">b1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zheng</surname>
          <given-names>Hao</given-names>
        </name>
        <xref ref-type="aff" rid="btaa478-aff3">b3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Tang</surname>
          <given-names>Haixu</given-names>
        </name>
        <xref ref-type="aff" rid="btaa478-aff1">b1</xref>
        <xref ref-type="corresp" rid="btaa478-cor1"/>
        <!--<email>hatang@indiana.edu</email>-->
      </contrib>
    </contrib-group>
    <aff id="btaa478-aff1"><label>b1</label><institution>Luddy School of Informatics, Computing, and Engineering, Indiana University</institution>, Bloomington, IN 47405, <country country="US">USA</country></aff>
    <aff id="btaa478-aff2"><label>b2</label><institution>Department of Computer Science and Software Engineering, Auburn University</institution>, Auburn, AL 36849, <country country="US">USA</country></aff>
    <aff id="btaa478-aff3"><label>b3</label><institution>Department of Bioinformatics, Hangzhou Nuowei Information Technology</institution>, Hangzhou 310053, <country country="CN">China</country></aff>
    <author-notes>
      <corresp id="btaa478-cor1">To whom correspondence should be addressed. E-mail: <email>hatang@indiana.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2020-07-13">
      <day>13</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>13</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>36</volume>
    <issue>Suppl 1</issue>
    <issue-title>ISMB 2020 Proceedings</issue-title>
    <fpage>i128</fpage>
    <lpage>i135</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2020. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <license license-type="cc-by-nc" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btaa478.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>The generalized linear mixed model (GLMM) is an extension of the generalized linear model (GLM) in which the linear predictor takes random effects into account. Given its power of precisely modeling the mixed effects from multiple sources of random variations, the method has been widely used in biomedical computation, for instance in the genome-wide association studies (GWASs) that aim to detect genetic variance significantly associated with phenotypes such as human diseases. Collaborative GWAS on large cohorts of patients across multiple institutions is often impeded by the privacy concerns of sharing personal genomic and other health data. To address such concerns, we present in this paper a privacy-preserving Expectation–Maximization (EM) algorithm to build GLMM collaboratively when input data are distributed to multiple participating parties and cannot be transferred to a central server. We assume that the data are <italic>horizontally</italic> partitioned among participating parties: i.e. each party holds a subset of records (including observational values of fixed effect variables and their corresponding outcome), and for all records, the outcome is regulated by the same set of known fixed effects and random effects.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>Our <italic>collaborative</italic> EM algorithm is mathematically equivalent to the original EM algorithm commonly used in GLMM construction. The algorithm also runs efficiently when tested on simulated and real human genomic data, and thus can be practically used for privacy-preserving GLMM construction. We implemented the algorithm for collaborative GLMM (cGLMM) construction in R. The data communication was implemented using the <italic>rsocket</italic> package.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The software is released in open source at <ext-link ext-link-type="uri" xlink:href="https://github.com/huthvincent/cGLMM">https://github.com/huthvincent/cGLMM</ext-link>.</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref ref-type="supplementary-material" rid="sup1">Supplementary data</xref> are available at <italic>Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Institute of Health</institution>
          </institution-wrap>
        </funding-source>
        <award-id>U01EB023685</award-id>
        <award-id>R01HG010798</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Indiana University</institution>
            <institution-id institution-id-type="DOI">10.13039/100006733</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Precision Health Initiative</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Science Foundation</institution>
            <institution-id institution-id-type="DOI">10.13039/100000001</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>CNS-1838083</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="8"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Owing to the rapid advances in DNA-sequencing technologies, human genomic studies, such as genome-wide association studies (GWASs) have been increasingly used to identify the genetic variants susceptible to human diseases (<xref rid="btaa478-B11" ref-type="bibr">Hirschhorn and Daly, 2005</xref>; <xref rid="btaa478-B19" ref-type="bibr">McCarthy <italic>et al.</italic>, 2008</xref>). Meanwhile, many computational methods have been developed to enhance the sensitivity and statistical power of GWAS (<xref rid="btaa478-B20" ref-type="bibr">McCulloch, 2003a</xref>). Among them, the linear mixed model (LMM) aims to explain the variation of a target phenotype in a population by a mixture of fixed effects (variables of interests) and random effects (unknown variables), which are shown to improve the identification rate of potentially causal variants of human disease (<xref rid="btaa478-B10" ref-type="bibr">Golan and Rosset, 2018</xref>). However, LMM is designed for quantitative traits (e.g. blood pressure), and cannot be directly applied to categorical phenotypes. The generalized linear mixed model (GLMM) extends the LMM to support both categorical and quantitative phenotypes, and thus were frequently used in GWAS (e.g. for binary case–control studies or ordered disease stages; <xref rid="btaa478-B21" ref-type="bibr">McCulloch, 2003b</xref>). In a generic setting, a GLMM can be constructed using human genomic data (i.e. genotypes inferred from genome sequences) from a cohort of phenotyped human individuals, which requires data analysts to have direct access to the individual-level genomic data for every member in the cohort. In practice, it may be of great biomedical interest to assemble a large cohort of human genomes from multiple studies of the same disease for GWAS (often referred to as the <italic>meta-analysis</italic>; <xref rid="btaa478-B1" ref-type="bibr">Begum <italic>et al.</italic>, 2012</xref>; <xref rid="btaa478-B13" ref-type="bibr">Jeck <italic>et al.</italic>, 2012</xref>; <xref rid="btaa478-B24" ref-type="bibr">Pharoah <italic>et al.</italic>, 2013</xref>). For this purpose, the genomic data need to be collected and stored across several institutions, since it is difficult to move the data to a central site due to the challenges in data transmission (large data size), privacy protection (personal human genomic data are identifiable and thus sensitive) and the restriction of institutional data disclosure policy. As a result, privacy-preserving algorithms should be in place to enable computation on distributed genomic data without sharing individual-level genomic variants.</p>
    <p>In the past decade, privacy-preserving algorithms have been developed for protecting various statistical methods, including survival analyses [e.g. using the Cox model (<xref rid="btaa478-B3" ref-type="bibr">Bradburn <italic>et al.</italic>, 2003</xref>; <xref rid="btaa478-B18" ref-type="bibr">Lu <italic>et al.</italic>, 2015</xref>; <xref rid="btaa478-B35" ref-type="bibr">Yu <italic>et al.</italic>, 2008</xref>)], missing data imputation (<xref rid="btaa478-B12" ref-type="bibr">Jagannathan and Wright, 2008</xref>) and logistic regression (<xref rid="btaa478-B33" ref-type="bibr">Wu <italic>et al.</italic>, 2012</xref>). These algorithms follow the same <italic>collaborative</italic> computation approach, where the computation for targeted statistical methods is partitioned depending on the required input data: some computation is done on the locally retained genomic data to obtain often less-sensitive intermediate results, whereas the other computation is performed on a central server using the intermediate results as the input. As a step forward on this direction, in this article, we present a privacy-preserving method for constructing a GLMM using a collaborative Expectation–Maximization (EM) algorithm that combines the Metropolis–Hasting (MH) algorithm in the E-step and the Newton–Raphson (NR) algorithm in the M-step to estimate parameters of the GLMM. We partition the computation for both the MH and the NR algorithm into the <italic>private</italic> and <italic>joint</italic> components, which are carried out by each participating party, whose intermediate results are then combined by a central server. The resulting collaborative EM algorithm is mathematically equivalent to the original EM algorithm (i.e. commonly used in GLMM model construction <xref rid="btaa478-B2" ref-type="bibr">Booth and Hobert (1999)</xref>. We implemented the collaborative GLMM (cGLMM) algorithm in R, and evaluated its performance using both simulated and real-world human genome data. The results show that the collaborative EM algorithm is efficient, and also accurate. We note that the cGLMM method can be applied to other biomedical computation tasks where privacy-preserving approaches are needed, e.g. for building predictive models of human diseases using diagnostic information retrieved from patients’ Electronic Health Records (EHRs) that are held by multiple medical institutions but cannot be shared outside each respective institution.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Background</title>
      <sec>
        <title>2.1.1 Generalized linear mixed model</title>
        <p>In an LMM, the outcome is a continuous variable. In GWAS, however, the outcome is often categorical, e.g. the binary outcome representing disease or healthy in a case-control study. The GLMM extends LMM by incorporating a link function to convert a continuous outcome into a categorical outcome (<xref rid="btaa478-B21" ref-type="bibr">McCulloch, 2003b</xref>). Here, we use the logit function: <inline-formula id="IE1"><mml:math id="IM1"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo><mml:mfrac><mml:mi>z</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> as the link function to deal with the binary outcome often used in the case-control GWAS:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>β</mml:mo><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:mi>Z</mml:mi><mml:mi>u</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic>X</italic> represents fixed effects and <italic>Z</italic> represents the random effect, whereas <italic>β</italic> and <italic>u</italic> are the coefficients for the fixed and the random effects, respectively.</p>
        <p>Below, we summarize the EM algorithm for GLMM parameter estimation from a total of <italic>N</italic> input records, each with the given fixed and random effects as well as the desirable outcome (0 or 1). Let <italic>T</italic> be the number of random effect categories and <italic>M</italic> be the number of fixed effect variables. We consider <inline-formula id="IE2"><mml:math id="IM2"><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> as one of the random effect categories, and <inline-formula id="IE3"><mml:math id="IM3"><mml:mrow><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> as one of the fixed effect variables. For the convenience of presenting the privacy-preserving EM algorithm for parameter estimation in the next section, here we assume each random effect category contains the equal number of <italic>P</italic> input records (thus, <inline-formula id="IE4"><mml:math id="IM4"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mo>×</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula>), and <inline-formula id="IE5"><mml:math id="IM5"><mml:mrow><mml:mi>p</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> indexes each record. Our implementation does not have this constraint and allows for each category to contain different number of records. Hence, the outcomes for all records can be represented as a <italic>response matrix</italic>,
<disp-formula id="E2"><label>(2)</label><mml:math id="M2"><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p>
        <p>The input random effects are represented as,
<disp-formula id="E3"><label>(3)</label><mml:math id="M3"><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:msup></mml:mrow></mml:math></disp-formula></p>
        <p><italic>Z</italic> is a vector with length <italic>T</italic>. For <italic>i</italic>th element <italic>Z<sub>i</sub></italic> (<inline-formula id="IE6"><mml:math id="IM6"><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>) is sampled from a normal distribution <inline-formula id="IE7"><mml:math id="IM7"><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>σ</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Note that different with most notation of GLMM, We remove ‘Membership matrix’ <italic>u</italic> by reshape input matrix <italic>X</italic> as <xref ref-type="disp-formula" rid="E4">Equation (4)</xref>. For example, in (4) <italic>X</italic><sub>1</sub> is the first fixed effect variable. It is a matrix which has <italic>T</italic> rows, where <italic>T</italic> is number of levels of random effect. So for the <italic>i</italic>th row of <italic>X</italic><sub>1</sub>, all the record has same random effect level, so that they are all correspond to <italic>Z<sub>i</sub></italic> which is the level of random effect of <italic>i</italic>th record. It different from most notation in GLMM, but this is the only notation way that we think can elaborate cGLMM. Throughout the rest of this article, we will use <inline-formula id="IE8"><mml:math id="IM8"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> to represent the <italic>m</italic>th input fixed effect values in <italic>t</italic>th level of random effect, <italic>p</italic>th record, and use <inline-formula id="IE9"><mml:math id="IM9"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> to represent the desirable outcomes, where <inline-formula id="IE10"><mml:math id="IM10"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is a <italic>M</italic>-dimensional vector (as shown below), and <inline-formula id="IE11"><mml:math id="IM11"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the binary outcome [as shown on (3)].
<disp-formula id="E4"><label>(4)</label><mml:math id="M4"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><italic>m</italic>th fixed effect is a <italic>T </italic>×<italic> P</italic> matrix, denote as <italic>X<sub>m</sub></italic>. So the corresponding fixed effect coefficients <italic>β</italic> are represented in an <italic>M</italic>-dimensional vector,
<disp-formula id="E5"><label>(5)</label><mml:math id="M5"><mml:mrow><mml:mo>β</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mo>β</mml:mo><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mo>β</mml:mo><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mo>β</mml:mo><mml:mi>M</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:msup></mml:mrow></mml:math></disp-formula></p>
        <p>We use an EM algorithm to estimate the parameters (<italic>Z</italic> and <italic>β</italic>) of an optimal GLMM: in the M-step, given the current latent variables (<italic>σ</italic>), we compute the fixed effect coefficients <italic>β</italic> by using the NR algorithm, and in the E-step, we compute the latent variables (<italic>σ</italic>) based on the current model parameters <italic>β</italic> by using the MH algorithm. The EM algorithm iterates between these two steps until convergence.</p>
        <p>Specifically, in the MH algorithm, we started from randomly selected <italic>σ<sub>i</sub></italic> for each category <italic>i</italic>, and then sample random effects <inline-formula id="IE12"><mml:math id="IM12"><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where each <italic>z<sub>i</sub></italic> is randomly sampled from <inline-formula id="IE13"><mml:math id="IM13"><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>σ</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Then in each MH sampling step, we first sample new random effects <inline-formula id="IE14"><mml:math id="IM14"><mml:mrow><mml:mi>z</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula> in the neighborhood of <inline-formula id="IE15"><mml:math id="IM15"><mml:mrow><mml:mi>z</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow></mml:math></inline-formula> based on the current <inline-formula id="IE16"><mml:math id="IM16"><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>σ</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, and then compute a <italic>proposal</italic> probability <italic>A</italic> between <inline-formula id="IE17"><mml:math id="IM17"><mml:mrow><mml:mi>z</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula> and <italic>z</italic> using the current model parameters <italic>β</italic> to decide if <italic>z</italic> should be replaced by <inline-formula id="IE18"><mml:math id="IM18"><mml:mrow><mml:mi>z</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula> in <bold>Z</bold> for the next step. The proposal function is defined as
<disp-formula id="E6"><label>(6)</label><mml:math id="M6"><mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">z</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>min</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="bold">z</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi>q</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">z</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">z</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi>q</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="bold">z</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE19"><mml:math id="IM19"><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the likelihood of random effect according to the current GLMM model, and <inline-formula id="IE20"><mml:math id="IM20"><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> represents the likelihood of observing a record according to the current <inline-formula id="IE21"><mml:math id="IM21"><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>σ</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Specifically, <inline-formula id="IE22"><mml:math id="IM22"><mml:mrow><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> can be written as (<xref rid="btaa478-B20" ref-type="bibr">McCulloch, 2003a</xref>):
<disp-formula id="E7"><label>(7)</label><mml:math id="M7"><mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>min</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msubsup><mml:mo>∏</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:msubsup><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>β</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>β</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mrow><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p>
        <p>Let <italic>M</italic><sub>MH</sub> be the total number of sampling steps in the MH algorithm, which can be set to between 500 and 1000. After we complete <italic>M</italic><sub>MH</sub> iterations, the variance of the final samples are used to compute the updated <italic>σ</italic>. The details of the MH algorithm will be further illustrated in Section 2.3, when we present the privacy-preserving version of the GLMM construction algorithm.</p>
        <p>In the M-step of the EM algorithm, we use the NR algorithm to compute the fixed effect coefficients <italic>β</italic> based on the current random effects <bold>Z</bold> and the parameters <italic>σ</italic> (updated in the E-step). NR is a second-order optimization algorithm, which uses the first-order derivative to choose the optimization direction, and the second-order derivative to choose the step length. Comparing with the first-order optimization algorithms, the NR algorithm takes fewer steps to converge. Specifically, in each NR iteration <italic>i</italic>, <italic>β</italic> is updated by
<disp-formula id="E8"><label>(8)</label><mml:math id="M8"><mml:mrow><mml:msub><mml:mo>β</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mo>β</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mo>′</mml:mo><mml:mo>·</mml:mo><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula>where the first-order derivative <inline-formula id="IE23"><mml:math id="IM23"><mml:mrow><mml:mi>f</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula> and the Hessian matrix <italic>H</italic> can be computed by
<disp-formula id="E9"><label>(9)</label><mml:math id="M9"><mml:mtable><mml:mtr><mml:mtd><mml:msup><mml:mi>f</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:msubsup><mml:mrow><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>β</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>β</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>and</mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:msubsup><mml:mrow><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>β</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow/></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>β</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>Z</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
        <p>We use a threshold <italic>ϵ</italic> to determine if the optimization converges, i.e. the NR algorithm terminates when the distance between the <italic>β</italic>s from two consecutive steps is smaller than <italic>ϵ</italic>. The details of the NR algorithm will be illustrated in Section 2.4.</p>
      </sec>
    </sec>
    <sec>
      <title>2.2 Privacy-preserving GLMM construction on horizontally partitioned data</title>
      <p>In this article, we consider a scenario of collaborative computation for privacy-preserving GLMM construction, where each of the collaborative parties (e.g. medication institutions) holds a sensitive dataset (e.g. the genotypes from a cohort of human individuals including disease patients and healthy controls) and attempts to build a GLMM model collaboratively without sharing its raw dataset to the other parties. This scenario is often referred to as the collaborative computation on <italic>horizontally</italic> partitioned data (<xref rid="btaa478-B14" ref-type="bibr">Jiang <italic>et al.</italic>, 2013</xref>; <xref rid="btaa478-B30" ref-type="bibr">Wang <italic>et al.</italic>, 2013</xref>), where each participating party holds the <italic>complete records</italic> from a subset of subjects, whereas, in a different scenario called <italic>vertically</italic> partitioned data (<xref rid="btaa478-B17" ref-type="bibr">Li <italic>et al.</italic>, 2016</xref>), each participating party holds a different portion of records from the same set of subjects that can be matched among all parties. The general idea of collaborative computation is to partition a target algorithm (e.g. to construct the GLMM) into two parts: the first part of <italic>private</italic> computation can be performed on the partial data held by each participating party that generate intermediate results required for the second part of computation, and the second part <italic>joint</italic> computation has to be performed on a central server using the intermediate results from the private computation of all parties. In this scenario, the central server is considered to be <italic>semi-trusted</italic> (honest-but-curious), and thus the sensitive raw data cannot be directly sent to the central server by each participating party.</p>
      <p>In practice, collaborative algorithms for many tasks (e.g. uncertainty quantification; <xref rid="btaa478-B26" ref-type="bibr">Sciacchitano <italic>et al.</italic>, 2015</xref>) have been developed. In many cases, they involved multi-round communications where in each round, not only the intermediate results were transferred from each party to the central server for the joint computation, but the intermediate results from joint computation needs to be sent back to each party for the next round of private computation. Notably, the collaborative computation for building a machine learning model is also referred to as the <italic>federated learning</italic> approach (<xref rid="btaa478-B16" ref-type="bibr">Konečnỳ <italic>et al.</italic>, 2016</xref>), which aims to save the cost of transferring a large amount of training data among participating parties while protecting the privacy of sensitive training data. In some cases, the intermediate results may carry sensitive information and thus can be used to infer the presence of a record (e.g. by using a re-identification attack (<xref rid="btaa478-B8" ref-type="bibr">El Emam <italic>et al.</italic>, 2011</xref>). In these cases, the joint computation should be performed by using encryption protocols (e.g. homomorphic encryption (HE<xref rid="btaa478-B9" ref-type="bibr">; Gentry, 2009</xref>; <xref rid="btaa478-B31" ref-type="bibr">Wang <italic>et al.</italic>, 2016</xref>), or in a trusted execution environment (TEE; <xref rid="btaa478-B25" ref-type="bibr">Sabt <italic>et al.</italic>, 2015</xref>).</p>
      <p>Consider the input data matrix <italic>X</italic> containing the fix effects (e.g. the genotypes) on a total of <italic>N</italic> records (disease patients) in each of the <italic>T</italic> random effect categories (<xref ref-type="fig" rid="btaa478-F1">Fig. 1a</xref>). In the horizontal data partition scenario, the entire data matrix was not held by a single user, but instead by <italic>K</italic> different parties: the <italic>i</italic>th party holds a subset of <italic>T </italic>×<italic> P</italic> records (<xref ref-type="fig" rid="btaa478-F1">Fig. 1c</xref>). Again, for the convenience of presentation, here we assume each party holds the same number (<italic>P</italic>) of records in each of the <italic>T</italic> random effect categories; as a result, <inline-formula id="IE24"><mml:math id="IM24"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mo>×</mml:mo><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula>. However, in our implementation, we can handle the situation where different parties hold a different number of records, and also the records may be distributed unevenly among random effect categories. Using the separately held input data matrix, our goal is to develop the collaborative computation algorithm for building a GLMM model among the <italic>K</italic> participating parties, through the partition of private and joint computation for the EM algorithm as laid out above.
</p>
      <fig id="btaa478-F1" orientation="portrait" position="float">
        <label>Fig. 1.</label>
        <caption>
          <p>The input fixed effect matrix is jointly held by <italic>K</italic> parties, each holding a subset of records (<bold>a</bold>). Hence, the input matrix can be viewed as <italic>horizontally</italic> partitioned (<bold>b</bold>), and each party holds a submatrix containing a subset of rows (<bold>c</bold>)</p>
        </caption>
        <graphic xlink:href="btaa478f1"/>
      </fig>
      <p>The cGLMM presented here takes as the input the data matrix jointly held by multiple parties and uses the EM algorithm to estimate the parameters of the GLMM (i.e. <italic>β</italic> for fixed effect coefficients and <italic>σ</italic> for the random effect). In each iteration of the E-step (MH algorithm) and M-step (NR algorithm), each party first computes the intermediate results from their own data, and then transfer the results to a central server to compute the updated parameters, which will then be sent back to each party for the next iteration.</p>
      <p><xref ref-type="fig" rid="btaa478-F2">Figure 2</xref> illustrates the workflow of the collaborative EM algorithm for cGLMM, in which each iteration consists of the E-step (MH algorithm; <xref ref-type="fig" rid="btaa478-F2">Fig. 2</xref>, left) and M-step (NR algorithm; <xref ref-type="fig" rid="btaa478-F2">Fig. 2</xref>, right), and each of them is performed in a collaborative manner. Both the MH and NR algorithms can be partitioned into the private computation (executed by each party separately) and the joint computation (executed on a central server) such that the intermediate results generated by the private computation of each party are sent to the central server for joint computation (①), and the intermediate results generated by the joint computation are then sent back to each party for the private computation in the next iteration ②. As a result, the collaborative EM algorithm involves multiple rounds of data communications, where the number of rounds is equal to the number of iterations in the EM algorithm.
</p>
      <fig id="btaa478-F2" orientation="portrait" position="float">
        <label>Fig. 2.</label>
        <caption>
          <p>The workflow of the collaborative EM algorithm for building cGLMM jointly by multiple participating parties</p>
        </caption>
        <graphic xlink:href="btaa478f2"/>
      </fig>
      <p>In the next two sections, we will present the details of the collaborative EM algorithm, specifically the partition of the private and joint computation for the MH and NR algorithms, respectively.</p>
    </sec>
    <sec>
      <title>2.3 Collaborative MH algorithm</title>
      <p>As described in Section 2.1.1, in the E-step of the EM algorithm for the parameter estimation of GLMM, we attempt to estimate the fixed effect coefficients (<italic>β</italic>) based on the current estimation of the random effect parameters (<italic>σ</italic>), using the MH algorithm. Given the horizontally partitioned data, we need to modify the MH algorithm into a collaborative version that consists of the private computation, in which each participating party computes some intermediate results from its partial data, and the joint computation, in which the intermediate results from all parties are transferred to the central server to compute the new fixed effect coefficients for the next step.</p>
      <p><xref ref-type="fig" rid="btaa478-F3">Figure 3a</xref> illustrates the procedure of the collaborative MH algorithm. As an initial step, based on the current estimated <italic>σ</italic>, the central server samples a set of random effects <inline-formula id="IE25"><mml:math id="IM25"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>σ</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> as the initial pool of samples, and sends them to all participating parties. In each subsequent iteration <italic>i</italic>, the central server first samples a new set of random effects <italic>Z<sub>i</sub></italic> from <inline-formula id="IE26"><mml:math id="IM26"><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>σ</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, and sends them to all parties. Then each party <italic>k</italic> (<inline-formula id="IE27"><mml:math id="IM27"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula>) computes the intermediate results <italic>A<sub>k</sub></italic> on its private server by using the random effect from the previous step (<inline-formula id="IE28"><mml:math id="IM28"><mml:mrow><mml:msubsup><mml:mi>Z</mml:mi><mml:mi>l</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>), the new random effect (<inline-formula id="IE29"><mml:math id="IM29"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>), as well as the private data records <inline-formula id="IE30"><mml:math id="IM30"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE31"><mml:math id="IM31"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> held by the party. Then by using <italic>A<sub>k</sub></italic> we can update the result to get <inline-formula id="IE32"><mml:math id="IM32"><mml:mrow><mml:msubsup><mml:mi>Z</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>.
<disp-formula id="E10"><label>(10)</label><mml:math id="M10"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>Z</mml:mi><mml:mi>l</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:msubsup><mml:mrow><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo><mml:mo> </mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>β</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:msubsup><mml:mrow><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo><mml:mo> </mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>β</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>Z</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic>P</italic> is the same number of records in each category of random effect, and <inline-formula id="IE33"><mml:math id="IM33"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the outcome (e.g. <inline-formula id="IE34"><mml:math id="IM34"><mml:mrow><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> in the case-control GWAS study) of the <italic>p</italic>th record, and <inline-formula id="IE35"><mml:math id="IM35"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the input fixed effect variables (e.g. the genotypes in GWAS) in all <italic>T</italic> random effect categories, which are held privately by the <italic>k</italic>th participating party. Again, for the convenience of presentation, here we assume each party <italic>k</italic> holds the same number (<italic>P</italic>) of records in each random effects category. In our implementation, however, different numbers of records are allowed to be held by different parties and in different random effects categories.
</p>
      <fig id="btaa478-F3" orientation="portrait" position="float">
        <label>Fig. 3.</label>
        <caption>
          <p>The data communication between the central server and participating parties in the collaborative MH algorithm (E-step; <bold>a</bold>) and the collaborative NR algorithm (M-step; <bold>b</bold>)</p>
        </caption>
        <graphic xlink:href="btaa478f3"/>
      </fig>
      <p>The intermediate result <italic>A<sub>k</sub></italic> computed in the private computation by the <italic>k</italic>th party is then sent to the central server, where the joint computation is performed for computing the proposal probability density <italic>A</italic>,
<disp-formula id="E11"><label>(11)</label><mml:math id="M11"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mtext>min</mml:mtext><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>to decide if the previous samples of the random effect <inline-formula id="IE36"><mml:math id="IM36"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> should be replaced by the new estimates <italic>z<sub>i</sub></italic>. The proposal probability <italic>A</italic> acts like a filter to replace the less likely (i.e. fitting improperly to the outcome according to the current estimates of fixed effect parameters <italic>β</italic>) random effects parameters sampled in the previous step: if <italic>A </italic>=<italic> </italic>1, the previous parameters are definitively replaced; otherwise, it is replaced with the probability <italic>A</italic>. After the update of iteration <italic>i</italic>, the random effects <italic>Z<sub>i</sub></italic> are stored in the central server for the subsequent iterations in the MH algorithm. The iteration of the algorithm continues until the parameter estimation converges. In our experiments, the MH algorithm usually converges after 1000 iterations.</p>
      <p>After convergence, the central server will retain a pool of random effects <inline-formula id="IE37"><mml:math id="IM37"><mml:mrow><mml:msup><mml:mi>Z</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> obtained in each iteration. In order to better estimate the parameters <inline-formula id="IE38"><mml:math id="IM38"><mml:mrow><mml:msup><mml:mi>Z</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, we adopted the burn-in strategy commonly used in MH algorithms (<xref rid="btaa478-B7" ref-type="bibr">Chib and Greenberg, 1995</xref>), which eliminates some sampled parameters in the initial steps of MH sampling. Based on the final sample pool of random effects <inline-formula id="IE39"><mml:math id="IM39"><mml:mrow><mml:msup><mml:mi>Z</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, the central server updates the parameters <italic>σ</italic>, which will be used in the M-step (see the collaborative NR algorithm in Section 2.4) for estimating the fixed effect coefficients <italic>β</italic>.</p>
      <p>The key idea of the collaborative MH algorithm presented here is the partition of the computation of proposal probability into two components, the private and joint computation, respectively. This partitioning approach is equivalent to the non-collaborative MH algorithm presented in Section 2.1.1. As a result, the collaborative MH algorithm offers mathematically equivalent solutions as the original MH algorithm in the E-step of the EM algorithm for GLMM parameter estimation, even though in practice, the solutions may not be identical due to the stochastic sampling in the MH algorithm.</p>
      <p>Apparently, the collaborative MH algorithm requires the central server and the server at each participating party to remain online to facilitate the in-time communication between servers for the coordinated private/joint computation. The entire computation involves <italic>M</italic><sub>MH</sub> rounds of communications, where <italic>M</italic><sub>MH</sub> represents the number of MH iterations before it converges. In each iteration, the central server sends a <italic>P</italic> dimensional vector (i.e. <italic>z<sub>i</sub></italic>) to each participating party, and receives another <italic>P</italic> dimensional vector i.e. <italic>A<sub>k</sub></italic>, from each party. In addition, the server at each party computes <italic>A<sub>k</sub></italic> privately, which takes <italic>O</italic>(<italic>P</italic>) running time and the central server computes the aggregate <italic>A</italic> (<xref ref-type="disp-formula" rid="E11">Equation 11</xref>), which takes <italic>O</italic>(<italic>P</italic>) time for each iteration. The private computation completed by the participating parties spends a majority of the running time during the entire process.</p>
    </sec>
    <sec>
      <title>2.4 Collaborative NR algorithm</title>
      <p>In the M-step, we use a NR algorithm to estimate the fixed effect coefficients (<italic>β</italic>) based on the current estimates of the random effect parameters <italic>σ</italic>. Similar to the E-step, we propose a collaborative NR algorithm that partitions the computation of the first-order derivative and Hessian matrix of the likelihood function, which are required to update the fixed effect coefficients, into the private and joint computation.</p>
      <p><xref ref-type="fig" rid="btaa478-F3">Figure 3b</xref> illustrates the procedure of the collaborative NR algorithm in the M-step. Before the iteration starts, the central server will pick up the random <italic>β</italic><sub>0</sub> and <italic>Z</italic> as input. In each subsequent iteration <italic>i</italic>, we attempt to update <italic>β<sub>i</sub></italic> by using <xref ref-type="disp-formula" rid="E8">Equation 8</xref>, and thus we need to compute the Hessian matrix <italic>H</italic> and the first-order derivative <inline-formula id="IE40"><mml:math id="IM40"><mml:mrow><mml:mi>f</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula>.</p>
      <p>Notably, the computation of <italic>H</italic> and <inline-formula id="IE41"><mml:math id="IM41"><mml:mrow><mml:mi>f</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula> in the non-collaborative NR algorithm requires the entire input dataset, i.e. <italic>Y<sub>t</sub></italic> and <italic>X<sub>t</sub></italic> of each record <italic>t</italic> (<xref ref-type="disp-formula" rid="E9">Equation 9</xref>). To compute them without sharing the data among participating parties, we re-write the equations for computing the Hessian matrix and the first-order derivative. As a result, each participating party <italic>k</italic> can compute the intermediate results on its private server, including the partial Hessian matrix <italic>H<sub>k</sub></italic> and the partial first-order derivatives <inline-formula id="IE42"><mml:math id="IM42"><mml:mrow><mml:mi>f</mml:mi><mml:msub><mml:mo>′</mml:mo><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. We denote the fixed effects of the <italic>p</italic>th record in the <italic>t</italic>th category of random effect held by the <italic>k</italic>th party as <inline-formula id="IE43"><mml:math id="IM43"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. The <italic>k</italic>*th party then computes
<disp-formula id="E12"><label>(12)</label><mml:math id="M12"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:msubsup><mml:mrow><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>β</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>β</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="E13"><label>(13)</label><mml:math id="M13"><mml:mrow><mml:mi>f</mml:mi><mml:msub><mml:mo>′</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:msubsup><mml:mrow><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mo>β</mml:mo><mml:mi>m</mml:mi></mml:msub><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mo>β</mml:mo><mml:mi>m</mml:mi></mml:msub><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>and sends the intermediate results to the central server for the joint computation of the full Hessian matrix and first-order derivatives,
<disp-formula id="E14"><label>(14)</label><mml:math id="M14"><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo> </mml:mo><mml:mtext>and</mml:mtext><mml:mo> </mml:mo><mml:mi>f</mml:mi><mml:mo>′</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula>which are sent back to each participating party to update the fixed effect coefficients of its records <inline-formula id="IE44"><mml:math id="IM44"><mml:mrow><mml:msub><mml:mo>β</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="E8">Equation 8</xref>) for the next iteration. Finally, the iterative procedure terminates after the estimates of the fixed effect coefficients converge or it reaches the preset maximum number of iterations.</p>
      <p>Notably, the collaborative computation for the Hessian matrix and the first-order derivatives is equivalent to the non-collaborative NR algorithm presented in Section 2.1.1. Therefore, the collaborative NR algorithm generates identical results (i.e. the fixed effect coefficients <italic>β</italic>) as the original NR algorithm in the M-step of the EM algorithm for GLMM parameter estimation.</p>
      <p>Similar to the collaborative MH algorithm, the collaborative NR algorithm also requires the central server and all participating parties’ servers to remain online for in-time communication. The entire computation involves <italic>M</italic><sub>NR</sub> rounds of communications, where <italic>M</italic><sub>NR</sub> represents the number of NR iterations before it converges. In each iteration, the central server receives the Hessian matrix (in <italic>M </italic>×<italic> M</italic> dimension) and first-order derivative (i.e. a M-dimensional vector) computed by each party and then sends back the full Hessian matrix and first-order derivative vector of the same size to each party. In the private computation, the server at each party first computes the partial Hessian matrix and partial first-order derivatives (<xref ref-type="disp-formula" rid="E12">Equations 12</xref> and <xref ref-type="disp-formula" rid="E13">13</xref>), which takes <inline-formula id="IE45"><mml:math id="IM45"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo>|</mml:mo><mml:mo>*</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE46"><mml:math id="IM46"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo>|</mml:mo><mml:mo>*</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> running time, respectively, and also updates the fixed effect coefficients <italic>β</italic>, which takes <inline-formula id="IE47"><mml:math id="IM47"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo>|</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> time, where <inline-formula id="IE48"><mml:math id="IM48"><mml:mrow><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:math></inline-formula> represents the number of fixed effect variables. On the other hand, in the joint computation, the central server combines these intermediate results into the full Hessian matrix and first-order derivatives (<xref ref-type="disp-formula" rid="E14">Equation 14</xref>), which only takes <inline-formula id="IE49"><mml:math id="IM49"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>×</mml:mo><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>K</mml:mi><mml:mo>×</mml:mo><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo>|</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>≈</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo>|</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> time, where <italic>N</italic> is the total number of records held by all parties. Overall, the private and the joint computation take about the same amount of running time in the collaborative NR algorithm.</p>
      <p>Above we presented the collaborative algorithms for the E- and M-step, respectively. It is straightforward to combine these them into a collaborative EM algorithm, which iterates between these two steps until the estimated parameters (including the fixed effect coefficients <italic>β</italic> and the random effect parameter <italic>σ</italic>) converge. The entire process requires <inline-formula id="IE50"><mml:math id="IM50"><mml:mrow><mml:mi>C</mml:mi><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> rounds of communications, where <italic>C</italic> represents the number of iterations of the EM algorithm.</p>
    </sec>
    <sec>
      <title>2.5 Implementation</title>
      <p>We implemented the algorithm for cGLMM construction in R. The data communication was implemented using the <italic>rsocket</italic> package. The software is released in open source at <ext-link ext-link-type="uri" xlink:href="https://github.com/huthvincent/cGLMM">https://github.com/huthvincent/cGLMM</ext-link>.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 Simulation experiments</title>
      <p>We first conducted simulated experiments to test the performance of the cGLMM construction. We simulated three different datasets with 50(3.5 KB), 100 (11.8 KB) and 150 (25.6 KB) fixed effects over 1000, 2000 and 3000 records, respectively, which were held by two participating parties. For each dataset, there is one random effect with five different levels. We note that the numbers of fixed effects simulated here resemble the real cases in a collaborative study, in which a participating party have already identified a small number of putative effects from its own data, and hope to use the data from the other parties to validate these candidates. We compared the results of cGLMM using the collaborative EM algorithm with results of the regular GLMM using the non-collaborative EM algorithm. We considered only two parties; the running time may not be much longer when more parties are involved because the computation carried out by each party remains the same as long as each party has about the same number of records.</p>
      <p>For comparison purpose, we used the same convergence conditions and the same initial selection of parameters in the cGLMM and regular GLMM. Nevertheless, the final results were similar but not identical because the MH algorithm in the E-step may introduce some randomness. We set the maximum MH iteration as 1000, and only retain the last two sets of samples in the pool (i.e. burn-in = 998). The convergence condition of the EM algorithm is that the Euclidean distances between all parameters in three consecutive iterations are all below a threshold of 0.08. The same threshold was also used to determine the convergence of the MH algorithm.</p>
      <p>We simulated the genotypes as the fixed effects. The variable of each fixed effect can take <inline-formula id="IE51"><mml:math id="IM51"><mml:mrow><mml:mi>X</mml:mi><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mn>00</mml:mn><mml:mo>,</mml:mo><mml:mn>01</mml:mn><mml:mo>,</mml:mo><mml:mn>11</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>, where 00, 01 and 11 represents the genotypes of homozygous major, heterozygous and homozygous minor, respectively. The frequencies of the three genotypes were simulated following the Hardy–Weinberg equilibrium with a specific minor allele frequency; e.g. if the minor allele frequency is 0.3, the simulated frequencies of three genotypes follow 0.49, 0.42 and 0.09, respectively. Finally, in the simulation of the LMM, we assume the minor allele has an additive effect on the outcome; as a result, the fixed effect variables were simulated as integer values of {0, 1, 2}, representing the three genotypes, respectively. GLMM can be applied to other models of genetic effect (e.g. the dominant effect model), which were not tested here.</p>
      <p><xref rid="btaa478-T1" ref-type="table">Table 1</xref> compares the results from the collaborative (cGLMM) and non-collaborative (GLMM) EM algorithm on the three simulated datasets. Both algorithms converge after a small number of iterations, while cGLMM runs slower than GLMM, which is mostly due to the communication overhead. We note that our evaluation was conducted using three separate jobs (simulating two servers of the participating parties and the central server, respectively) on the same computer; thus, in reality, the running time of cGLMM can be significantly longer, depending on network bandwidth among the participating servers. However, even though the number of rounds of communication between servers is overall high, the total amount of data transferred is still moderate. Interestingly, we observe that with the increasing size of the GLMM model (i.e. with more fixed effect variables), the overhead of cGLMM comparing with GLMM actually decreases, probably because for complex GLMM problems, significantly more time is spent on the actual computation comparing to data communication. Finally, in all cases, cGLMM reported fixed effect coefficients that are nearly identical to the actual values used in the simulation [Pearson correlation coefficient (PCC) = 0.99] and the results from GLMM, suggesting cGLMM achieved the same accuracy as GLMM.
</p>
      <table-wrap id="btaa478-T1" orientation="portrait" position="float">
        <label>Table 1.</label>
        <caption>
          <p>We run cGLMM and GLMM 10 times, below is the average comparison of GLMM and cGLMM on simulated datasets with <inline-formula id="IE52"><mml:math id="IM52"><mml:mrow><mml:mo>±</mml:mo><mml:mo>σ</mml:mo></mml:mrow></mml:math></inline-formula></p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="2" colspan="1">No. of SNPs</th>
              <th colspan="2" align="center" rowspan="1">Iterations<hr/></th>
              <th colspan="2" align="center" rowspan="1">Running time (min)<hr/></th>
              <th rowspan="2" align="center" colspan="1">Communication round (cGLMM)</th>
              <th rowspan="2" align="center" colspan="1">Communication size (cGLMM; KB)</th>
              <th rowspan="2" align="center" colspan="1">PCCs<xref ref-type="table-fn" rid="tblfn1"><sup>a</sup></xref></th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">GLMM</th>
              <th rowspan="1" colspan="1">cGLMM</th>
              <th rowspan="1" colspan="1">GLMM</th>
              <th rowspan="1" colspan="1">cGLMM</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">50</td>
              <td rowspan="1" colspan="1">7 ± 1</td>
              <td rowspan="1" colspan="1">7 ± 1</td>
              <td rowspan="1" colspan="1">0.062 ± 0.01</td>
              <td rowspan="1" colspan="1">2.9 ± 0.4</td>
              <td rowspan="1" colspan="1">6107 ± 877</td>
              <td rowspan="1" colspan="1">369.62 ± 53</td>
              <td rowspan="1" colspan="1">0.99</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">100</td>
              <td rowspan="1" colspan="1">8 ± 1</td>
              <td rowspan="1" colspan="1">7 ± 1</td>
              <td rowspan="1" colspan="1">0.096 ± 0.01</td>
              <td rowspan="1" colspan="1">4.2 ± 0.6</td>
              <td rowspan="1" colspan="1">6505 ± 930</td>
              <td rowspan="1" colspan="1">807.40.1 ± 116</td>
              <td rowspan="1" colspan="1">0.99</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">150</td>
              <td rowspan="1" colspan="1">9 ± 2</td>
              <td rowspan="1" colspan="1">8 ± 2</td>
              <td rowspan="1" colspan="1">2.6 ± 0.6</td>
              <td rowspan="1" colspan="1">26.6 ± 6.4</td>
              <td rowspan="1" colspan="1">7825 ± 1842</td>
              <td rowspan="1" colspan="1">1403.22 ± 342</td>
              <td rowspan="1" colspan="1">0.99</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <label>a</label>
            <p>PCCs, Pearson correlation coefficients between the actual fixed effect coefficients used in simulation and those reported by GLMM and cGLMM, respectively.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p><xref ref-type="fig" rid="btaa478-F4">Figure 4</xref> compares the results of GLMM and cGLMM on the simulated dataset containing 150 fixed effect variables (genotypes) on 3000 records. As shown in <xref ref-type="fig" rid="btaa478-F4">Figure 4a and b</xref>, respectively, the distances between the fixed effect coefficients (<italic>β</italic>) and the random effect parameters (<italic>σ</italic>) in consecutive iterations becomes close to zero after only a few iterations of the collaborative EM algorithm, indicating it converges as fast as the non-collaborative EM algorithm. Furthermore, the fixed effect coefficients reported in cGLMM are very to those in GLMM and the actual coefficients used in the simulation (<xref ref-type="fig" rid="btaa478-F4">Fig. 4c;</xref> PCC &gt; 0.99 with the actual values), indicating cGLMM reported comparable results as GLMM. The results from the other two datasets (containing 50 and 100 fixed effect variables) are generally similar, and are shown in <xref ref-type="supplementary-material" rid="sup1">Supplementary Figures S1 and S2</xref>. These results suggested the collaborative EM algorithm is accurate and efficient for constructing GLMM collaboratively among multiple parties.
</p>
      <fig id="btaa478-F4" orientation="portrait" position="float">
        <label>Fig. 4.</label>
        <caption>
          <p>The comparison between the results from cGLMM and GLMM on the simulated dataset containing 150 fixed effects in a typical run, for the distances between the fixed effect coefficients (<italic>β</italic>) (<bold>a</bold>), and the distances between the random effect parameters (<italic>σ</italic>) in consecutive iterations of the collaborative (in red) and non-collaborative (in green) EM algorithm, (b) as well as the actual fixed effect coefficients (<italic>β</italic>, sorted in decreasing order; in blue) and those reported by GLMM (in green) and cGLMM (in red), (c) respectively</p>
        </caption>
        <graphic xlink:href="btaa478f4"/>
      </fig>
    </sec>
    <sec>
      <title>3.2 Real human genomic data</title>
      <p>Next, we used the real human genomic data from the 1000 Genomes Project (<xref rid="btaa478-B32" ref-type="bibr">Wang <italic>et al.</italic>, 2018</xref>) to test our algorithm. The 1000 Genomes Project has total 2000 records, contain 1000 records in control group and 1000 records in case group. We selected all 2000 records from the whole dataset. We then selected the 10 most significant SNPs (by using a <inline-formula id="IE53"><mml:math id="IM53"><mml:mrow><mml:msup><mml:mo>χ</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> test; including five positively and five negatively correlated with Group I versus Group II) between these two groups (Group I and II, simulating the case and control groups in GWAS), and another randomly selected 40, 90 and 140 SNPs to form the three testing datasets containing 50, 100 and 150 SNPs (used as the fixed effect variables in GLMM), with the total data size of 28.6, 60.5 and 84.0 KB, respectively. We considered the gender of the individuals as the random effect: Group I contains 505 males and 495 females while Group II contains 479 males and 521 females. Similar to the simulation experiment, we consider the additive genetic model, in which the fixed effect variables take 0, 1 or 2, representing the genotypes of homozygous major, heterozygous and homozygous minor, respectively. We randomly split the genomes into two subsets containing 907 and 1093 records, respectively, assuming each participating party holds one of them.</p>
      <p><xref rid="btaa478-T2" ref-type="table">Table 2</xref> compares the results from the collaborative (cGLMM) and non-collaborative (GLMM) EM algorithm on the three real human genomic datasets. Similar to the results from the simulated data, both algorithms converge after a few EM iterations, while cGLMM runs about 30 times slower than GLMM, which is mostly due to the communication overhead. In all cases, cGLMM reported fixed effect coefficients that nearly identical to the results from GLMM (PCC &gt; 0.99).
</p>
      <table-wrap id="btaa478-T2" orientation="portrait" position="float">
        <label>Table 2.</label>
        <caption>
          <p>Comparison between the results from cGLMM and GLMM on real human genomic datasets</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="2" colspan="1">No. of SNPs</th>
              <th colspan="2" align="center" rowspan="1">Iterations<hr/></th>
              <th colspan="2" align="center" rowspan="1">Running time (min)<hr/></th>
              <th rowspan="2" colspan="1">Communication round (cGLMM)</th>
              <th rowspan="2" colspan="1">Communication size (cGLMM; KB)</th>
              <th rowspan="2" colspan="1">PCCs<xref ref-type="table-fn" rid="tblfn2"><sup>a</sup></xref></th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">GLMM</th>
              <th rowspan="1" colspan="1">cGLMM</th>
              <th rowspan="1" colspan="1">GLMM</th>
              <th rowspan="1" colspan="1">cGLMM</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">50</td>
              <td rowspan="1" colspan="1">4 ± 0.2</td>
              <td rowspan="1" colspan="1">4 ± 0.2</td>
              <td rowspan="1" colspan="1">0.05 ± 0.003</td>
              <td rowspan="1" colspan="1">1.6 ± 0.08</td>
              <td rowspan="1" colspan="1">3006 ± 110</td>
              <td rowspan="1" colspan="1">55.8 ± 5.9</td>
              <td rowspan="1" colspan="1">0.99</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">100</td>
              <td rowspan="1" colspan="1">5 ± 0.3</td>
              <td rowspan="1" colspan="1">5 ± 0.3</td>
              <td rowspan="1" colspan="1">0.06 ± 0.003</td>
              <td rowspan="1" colspan="1">3.1 ± 0.2</td>
              <td rowspan="1" colspan="1">4004 ± 235</td>
              <td rowspan="1" colspan="1">145.6 ± 11.5</td>
              <td rowspan="1" colspan="1">0.99</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">150</td>
              <td rowspan="1" colspan="1">5 ± 0.3</td>
              <td rowspan="1" colspan="1">5 ± 0.3</td>
              <td rowspan="1" colspan="1">0.2 ± 0.01</td>
              <td rowspan="1" colspan="1">5.0 ± 0.3</td>
              <td rowspan="1" colspan="1">4006 ± 241</td>
              <td rowspan="1" colspan="1">615.6 ± 48.7</td>
              <td rowspan="1" colspan="1">0.99</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn2">
            <label>a</label>
            <p>PCCs, Pearson correlation coefficients between the fixed effect coefficients reported by GLMM and those reported by cGLMM. All resource list in here are averages of 10 runs with <inline-formula id="IE54"><mml:math id="IM54"><mml:mrow><mml:mo>±</mml:mo><mml:mo>σ</mml:mo></mml:mrow></mml:math></inline-formula>.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p><xref ref-type="fig" rid="btaa478-F5">Figure 5</xref> compares the results of GLMM and cGLMM on the real human genomic dataset containing 150 SNPs (including 10 significant SNPs between the two groups) over the 2000 genomes in two groups. Similar to the results from the simulated datasets, the cGLMM algorithm converges as fast as the GLMM algorithm (<xref ref-type="fig" rid="btaa478-F5">Fig. 5a and b</xref>), and achieved nearly identical results as the regular GLMM algorithm (<xref ref-type="fig" rid="btaa478-F5">Fig. 5c</xref>), where the 10 SNPs (leftmost and rightmost SNPs in <xref ref-type="fig" rid="btaa478-F5">Fig. 5c</xref>) which are highly correlated with the two groups receive high fixed effect coefficients. The testing on the other two datasets (containing 50 and 100 SNPs) showed similar results (see <xref ref-type="supplementary-material" rid="sup1">Supplementary Figs S3 and S4</xref>).These results confirmed the satisfactory performance of our cGLMM algorithm on real-world human genomic data.
</p>
      <fig id="btaa478-F5" orientation="portrait" position="float">
        <label>Fig. 5.</label>
        <caption>
          <p>The comparison between the results from cGLMM and GLMM on the human genomic dataset in a typical run, containing 150 SNPs (including 10 SNPs significantly associated with the two groups), for the distances between the fixed effect coefficients (<italic>β</italic>) (<bold>a</bold>), and the distances between the random effect parameters (<italic>σ</italic>) in consecutive iterations of the collaborative (in red) and non-collaborative (in blue) EM algorithm, as well as the fixed effect coefficients (<italic>β</italic>, sorted in decreasing order) reported by GLMM (in blue) and cGLMM (in red), respectively</p>
        </caption>
        <graphic xlink:href="btaa478f5"/>
      </fig>
      <p>We note that the total size of transferred from the participating party to the central server is comparable or greater than the input data size. For example, the input data size is 28.6 KB for 50 SNP dataset, while on average 27.9 KB data are transferred from participating parties to the central server (the data of equal size are transferred from the central server to the parties and thus in total 55.8 KB are communicated) in the computation. Even though the collaborative algorithm does not reduce the size of data transferred from participating party to the central server, the transferred data are aggregate intermediate results instead of the raw genetic data. Furthermore, the communication data size is only dependent on the number of fixed effects (SNPs) and thus does not increase with more genomes (i.e. 1000, 2000 and 3000 in our experiments). Finally, we are implementing a HE version of cGLMM using <italic>homomorpheR</italic> (<ext-link ext-link-type="uri" xlink:href="https://github.com/bnaras/homomorpheR">https://github.com/bnaras/homomorpheR</ext-link>), in which the intermediate results from each party will be encrypted and the collaborative computation will be implemented using the Paillier homomorphic addition. The updated implementation will be made available at <ext-link ext-link-type="uri" xlink:href="https://github.com/huthvincent/cGLMM">https://github.com/huthvincent/cGLMM</ext-link>.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Discussions</title>
    <p>Our current collaborative EM algorithm follows a distributed computing approach, in which the input data are partitioned and the computation is split among participating institutions. As a result, each partition of the input data can remain on the server of the respective participant that holds the partial data, and only the intermediate results (e.g. the partial proposal probabilities and the partial Hessian matrix) need to be sent out. Still, some computation needs to be performed on a central server, and the intermediate results need to be communicated between the central server and the server at each participating party. Our experimental results showed that even though several thousands rounds of communication are required to complete a reasonable-size GLMM task, the total amount of data transferred between servers is moderate (&lt;100 MB). Therefore, we think our privacy-preserving algorithm can be practically used for collaboratively building GLMMs. However, we note that the many rounds of communication due to the required data exchange in each iteration of the MH algorithm may reduce the applicability of the method. We plan to explore the approximation approaches to the MH algorithm that may reduce the rounds of communication while without sacrificing its accuracy.</p>
    <p>We note that our current approach can only allow one random effect exist in the model and does not protect the intermediate results sent by each participating institution to the central server: they are sent in plain text. The assumption here is that the intermediate results do not carry sufficient sensitive information about the individual records, which is commonly adopted by existing privacy-preserving algorithms for other biomedical computation tasks, e.g. for logistic regression <xref rid="btaa478-B33" ref-type="bibr">Wu <italic>et al.</italic> (2012)</xref>. However, our method can be combined with encryption methods to provide additional protection on the intermediate results. For example, we may use HE (<xref rid="btaa478-B15" ref-type="bibr">Kim <italic>et al.</italic>, 2018</xref>) to compute the proposal probability (<xref ref-type="disp-formula" rid="E11">Equation 11</xref>), the Hessian matrix and the first-order derivatives (<xref ref-type="disp-formula" rid="E14">Equation 14</xref>), respectively, on the central server. Because only additions are needed for these computations, one can use the light-weight Paillier cryptosystem with additive homomorphic properties (<xref rid="btaa478-B23" ref-type="bibr">Parmar <italic>et al.</italic>, 2014</xref>), which introduces moderate overhead on running time and communication. A more efficient solution may also be implemented using the recently available TEEs (<xref rid="btaa478-B4" ref-type="bibr">Chen <italic>et al.</italic>, 2016</xref>, <xref rid="btaa478-B5" ref-type="bibr">2017a</xref>,<xref rid="btaa478-B6" ref-type="bibr">b</xref>), such as Intel’s Software Guard Extensions (<xref rid="btaa478-B22" ref-type="bibr">McKeen <italic>et al.</italic>, 2016</xref>). We are currently implementing the HE version of the cGLMM model using <italic>homomorpheR</italic>, and will release the updated version of cGLMM on its website.</p>
    <p>Our current approach addresses the cGLMM construction on horizontally partitioned data, when each participating institution holds the complete records of a subset of subjects. In comparison, the data may be <italic>vertically</italic> partitioned in some scenarios of collaborative computation. For examples, two medical institutions may have complementary clinical data (i.e. a subset of fixed effects) of the same patients, and attempt to combine their partial data for some joint analyses. Privacy-preserving methods have been developed for some data mining and machine learning algorithms on vertically partitioned data, e.g. for association rule mining (<xref rid="btaa478-B27" ref-type="bibr">Vaidya and Clifton, 2002</xref>), k-means clustering (<xref rid="btaa478-B28" ref-type="bibr">Vaidya and Clifton, 2003</xref>), naive Bayes classifier (<xref rid="btaa478-B29" ref-type="bibr">Vaidya and Clifton, 2004</xref>) and support vector machine (<xref rid="btaa478-B34" ref-type="bibr">Yu <italic>et al.</italic>, 2006</xref>). We plan to develop privacy-preserving algorithms for cGLMM construction on vertically partitioned data in the future.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>btaa478_Supplementary_Data</label>
      <media xlink:href="btaa478_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>We thank Diyue Bu for her help of using the human genomic data from the 1000 Genomes Project.</p>
    <sec>
      <title>Funding</title>
      <p>The research was partially supported by the National Institute of Health [grants U01EB023685 and R01HG010798], Indiana University (IU) Precision Health Initiative (PHI) and National Science Foundation Grant CNS-1838083.</p>
      <p><italic>Conflict of Interest</italic>: none declared.</p>
    </sec>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btaa478-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Begum</surname><given-names>F.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>Comprehensive literature review and statistical considerations for GWAS meta-analysis</article-title>. <source>Nucleic Acids Res</source>., <volume>40</volume>, <fpage>3777</fpage>–<lpage>3784</lpage>.<pub-id pub-id-type="pmid">22241776</pub-id></mixed-citation>
    </ref>
    <ref id="btaa478-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Booth</surname><given-names>J.G.</given-names></name>, <name name-style="western"><surname>Hobert</surname><given-names>J.P.</given-names></name></person-group> (<year>1999</year>) 
<article-title>Maximizing generalized linear mixed model likelihoods with an automated Monte Carlo EM algorithm</article-title>. <source>J. R. Stat. Soc. B Stat. Methodol</source>., <volume>61</volume>, <fpage>265</fpage>–<lpage>285</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa478-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bradburn</surname><given-names>M.J.</given-names></name></person-group><etal>et al</etal> (<year>2003</year>) 
<article-title>Survival analysis part II: multivariate data analysis —an introduction to concepts and methods</article-title>. <source>Br. J. Cancer</source>, <volume>89</volume>, <fpage>431</fpage>–<lpage>436</lpage>.<pub-id pub-id-type="pmid">12888808</pub-id></mixed-citation>
    </ref>
    <ref id="btaa478-B4">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>F.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) <chapter-title>Premix: privacy-preserving estimation of individual admixture</chapter-title> In: <source>AMIA Annual Symposium Proceedings</source>, Vol. <volume>2016</volume>, pp. <fpage>1747</fpage>
<publisher-name>American Medical Informatics Association, USA</publisher-name>.<pub-id pub-id-type="pmid">28269933</pub-id></mixed-citation>
    </ref>
    <ref id="btaa478-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>F.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>a) 
<article-title>Presage: privacy-preserving genetic testing via software guard extension</article-title>. <source>BMC Med. Genomics</source>, <volume>10</volume>, <fpage>48</fpage>.<pub-id pub-id-type="pmid">28786365</pub-id></mixed-citation>
    </ref>
    <ref id="btaa478-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>F.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>b) 
<article-title>Princess: privacy-protecting rare disease international network collaboration via encryption through software guard extensions</article-title>. <source>Bioinformatics</source>, <volume>33</volume>, <fpage>871</fpage>–<lpage>878</lpage>.<pub-id pub-id-type="pmid">28065902</pub-id></mixed-citation>
    </ref>
    <ref id="btaa478-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chib</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Greenberg</surname><given-names>E.</given-names></name></person-group> (<year>1995</year>) 
<article-title>Understanding the Metropolis-Hastings algorithm</article-title>. <source>Am. Stat</source>., <volume>49</volume>, <fpage>327</fpage>–<lpage>335</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa478-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>El Emam</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) 
<article-title>A systematic review of re-identification attacks on health data</article-title>. <source>PLoS One</source>, <volume>6</volume>, <fpage>e28071,</fpage><pub-id pub-id-type="pmid">22164229</pub-id></mixed-citation>
    </ref>
    <ref id="btaa478-B9">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Gentry</surname><given-names>C.</given-names></name></person-group> (<year>2009</year>) Fully homomorphic encryption using ideal lattices. In: <italic>Proceedings of the forty-first annual ACM symposium on Theory of computing</italic>, pp. <fpage>169</fpage>–<lpage>178</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa478-B10">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Golan</surname><given-names>D.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) <chapter-title>Mixed models for case-control genome-wide association studies: major challenges and partial solutions</chapter-title>
<person-group person-group-type="editor"><name name-style="western"><surname>Breslow</surname><given-names>B.</given-names></name></person-group><etal>et al</etal> (eds) <source>Handbook of Statistical Methods for Case-Control Studies</source>, <edition>1st edn.</edition><publisher-loc>Boca Raton, FL</publisher-loc>: 
<publisher-name>Chapman and Hall/CRC</publisher-name>, pp. <fpage>495</fpage>–<lpage>514</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa478-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hirschhorn</surname><given-names>J.N.</given-names></name>, <name name-style="western"><surname>Daly</surname><given-names>M.J.</given-names></name></person-group> (<year>2005</year>) 
<article-title>Genome-wide association studies for common diseases and complex traits</article-title>. <source>Nat. Rev. Genet</source>., <volume>6</volume>, <fpage>95</fpage>–<lpage>108</lpage>.<pub-id pub-id-type="pmid">15716906</pub-id></mixed-citation>
    </ref>
    <ref id="btaa478-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jagannathan</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>Wright</surname><given-names>R.N.</given-names></name></person-group> (<year>2008</year>) 
<article-title>Privacy-preserving imputation of missing data</article-title>. <source>Data Knowl. Eng</source>., <volume>65</volume>, <fpage>40</fpage>–<lpage>56</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa478-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jeck</surname><given-names>W.R.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>a meta-analysis of GWAS and age-associated diseases</article-title>. <source>Aging Cell</source>, <volume>11</volume>, <fpage>727</fpage>–<lpage>731</lpage>.<pub-id pub-id-type="pmid">22888763</pub-id></mixed-citation>
    </ref>
    <ref id="btaa478-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>W.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>WebGLORE: a web service for grid logistic regression</article-title>. <source>Bioinformatics</source>, <volume>29</volume>, <fpage>3238</fpage>–<lpage>3240</lpage>.<pub-id pub-id-type="pmid">24072732</pub-id></mixed-citation>
    </ref>
    <ref id="btaa478-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Secure logistic regression based on homomorphic encryption: design and evaluation</article-title>. <source>JMIR Med. Inform</source>., <volume>6</volume>, <fpage>e19</fpage>.<pub-id pub-id-type="pmid">29666041</pub-id></mixed-citation>
    </ref>
    <ref id="btaa478-B16">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Konečnỳ</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) Federated learning: strategies for improving communication efficiency. arXiv Preprint arXiv:1610.05492.</mixed-citation>
    </ref>
    <ref id="btaa478-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>VERTIcal Grid lOgistic Regression (VERTIGO)</article-title>. <source>J. Am. Med. Inform. Assoc</source>., <volume>23</volume>, <fpage>570</fpage>–<lpage>579</lpage>.<pub-id pub-id-type="pmid">26554428</pub-id></mixed-citation>
    </ref>
    <ref id="btaa478-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lu</surname><given-names>C.-L.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>WebDISCO: a web service for distributed cox model learning without patient-level data sharing</article-title>. <source>J. Am. Med. Inform. Assoc</source>., <volume>22</volume>, <fpage>1212</fpage>–<lpage>1219</lpage>.<pub-id pub-id-type="pmid">26159465</pub-id></mixed-citation>
    </ref>
    <ref id="btaa478-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>McCarthy</surname><given-names>M.I.</given-names></name></person-group><etal>et al</etal> (<year>2008</year>) 
<article-title>Genome-wide association studies for complex traits: consensus, uncertainty and challenges</article-title>. <source>Nat. Rev. Genet</source>., <volume>9</volume>, <fpage>356</fpage>–<lpage>369</lpage>.<pub-id pub-id-type="pmid">18398418</pub-id></mixed-citation>
    </ref>
    <ref id="btaa478-B21">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>McCulloch</surname><given-names>C.E.</given-names></name></person-group> (<year>2003</year>b) <chapter-title>Generalized linear mixed models.</chapter-title> In: <source>NSF-CBMS Regional Conference Series in Probability and Statistics</source>, pp. <fpage>i</fpage>–<lpage>84</lpage>. 
<publisher-name>JSTOR</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btaa478-B22">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>McKeen</surname><given-names>F.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) Intel<sup>®</sup> software guard extensions (intel<sup>®</sup> sgx) support for dynamic memory management inside an enclave. In: <italic>Proceedings of the Hardware and Architectural Support for Security and Privacy 2016</italic>, pp. <fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa478-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Parmar</surname><given-names>P.V.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Survey of various homomorphic encryption algorithms and schemes</article-title>. <source>Int. J. Comput. Appl</source>., <volume>91, pp 26-32.</volume></mixed-citation>
    </ref>
    <ref id="btaa478-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pharoah</surname><given-names>P.D.</given-names></name></person-group><etal>et al</etal>; Australian Cancer Study (<year>2013</year>) 
<article-title>GWAS meta-analysis and replication identifies three new susceptibility loci for ovarian cancer</article-title>. <source>Nat. Genet</source>., <volume>45</volume>, <fpage>362</fpage>–<lpage>370</lpage>.<pub-id pub-id-type="pmid">23535730</pub-id></mixed-citation>
    </ref>
    <ref id="btaa478-B25">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Sabt</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) <chapter-title>Trusted execution environment: what it is, and what it is not.</chapter-title> In: <source>2015 IEEE Trustcom/BigDataSE/ISPA</source>, Vol. <volume>1</volume>, pp. <fpage>57</fpage>–<lpage>64</lpage>. 
<publisher-name>IEEE</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btaa478-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sciacchitano</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Collaborative framework for PIV uncertainty quantification: comparative assessment of methods</article-title>. <source>Meas. Sci. Technol</source>., <volume>26</volume>, <fpage>074004</fpage>.</mixed-citation>
    </ref>
    <ref id="btaa478-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Stroup</surname><given-names>W.W.</given-names></name></person-group> (2012) 
<article-title><italic>Generalized linear mixed models: modern concepts, methods and applications</italic>. CRC press, 2012</article-title>.</mixed-citation>
    </ref>
    <ref id="btaa478-B27">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Vaidya</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Clifton</surname><given-names>C.</given-names></name></person-group> (<year>2002</year>) Privacy preserving association rule mining in vertically partitioned data. In: <italic>Proceedings of the eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</italic>, pp. <fpage>639</fpage>–<lpage>644</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa478-B28">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Vaidya</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Clifton</surname><given-names>C.</given-names></name></person-group> (<year>2003</year>) Privacy-preserving k-means clustering over vertically partitioned data. In: <italic>Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</italic>, pp. <fpage>206</fpage>–<lpage>215</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa478-B29">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Vaidya</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Clifton</surname><given-names>C.</given-names></name></person-group> (<year>2004</year>) Privacy preserving naive Bayes classifier for vertically partitioned data. In: <italic>Proceedings of the 2004 SIAM International Conference on Data Mining</italic>, pp. 522–526. SIAM.</mixed-citation>
    </ref>
    <ref id="btaa478-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Expectation propagation logistic regression (explorer): distributed privacy-preserving online model learning</article-title>. <source>J. Biomed. Inform</source>., <volume>46</volume>, <fpage>480</fpage>–<lpage>496</lpage>.<pub-id pub-id-type="pmid">23562651</pub-id></mixed-citation>
    </ref>
    <ref id="btaa478-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Healer: homomorphic computation of exact logistic regression for secure rare disease variants analysis in GWAS</article-title>. <source>Bioinformatics</source>, <volume>32</volume>, <fpage>211</fpage>–<lpage>218</lpage>.<pub-id pub-id-type="pmid">26446135</pub-id></mixed-citation>
    </ref>
    <ref id="btaa478-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) <italic>idash Secure Genome Analysis Competition 2017. </italic><italic>BMC Med Genomics</italic>, <volume>11(suppl 4)</volume>, 1–2.</mixed-citation>
    </ref>
    <ref id="btaa478-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>Grid binary LOgistic Regression (GLORE): building shared models without sharing data</article-title>. <source>J. Am. Med. Inform. Assoc</source>., <volume>19</volume>, <fpage>758</fpage>–<lpage>764</lpage>.<pub-id pub-id-type="pmid">22511014</pub-id></mixed-citation>
    </ref>
    <ref id="btaa478-B34">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>H.</given-names></name></person-group><etal>et al</etal> (<year>2006</year>) Privacy-preserving SVM classification on vertically partitioned data. In: <italic>Pacific-Asia Conference on Knowledge Discovery and Data Mining</italic>, pp. <fpage>647</fpage>–<lpage>656</lpage>. Springer.</mixed-citation>
    </ref>
    <ref id="btaa478-B35">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2008</year>) Privacy-preserving cox regression for survival analysis. In: <italic>Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</italic>, pp. <fpage>1034</fpage>–<lpage>1042</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
