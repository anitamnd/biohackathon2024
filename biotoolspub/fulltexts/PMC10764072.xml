<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD with MathML3 v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1-mathml3.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?properties manuscript?>
<?origin hhspa?>
<?iso-abbr Pac Symp Biocomput?>
<?submitter-userid 8068881?>
<?submitter-authority myNCBI?>
<?submitter-login worldscience?>
<?submitter-name World Scientific Publishing?>
<?domain hhspa?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">9711271</journal-id>
    <journal-id journal-id-type="pubmed-jr-id">20660</journal-id>
    <journal-id journal-id-type="nlm-ta">Pac Symp Biocomput</journal-id>
    <journal-id journal-id-type="iso-abbrev">Pac Symp Biocomput</journal-id>
    <journal-title-group>
      <journal-title>Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">2335-6928</issn>
    <issn pub-type="epub">2335-6936</issn>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10764072</article-id>
    <article-id pub-id-type="pmid">38160311</article-id>
    <article-id pub-id-type="manuscript">hhspa1952198</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>intCC: An efficient weighted integrative consensus clustering of multimodal data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Huang</surname>
          <given-names>Can</given-names>
        </name>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kuan</surname>
          <given-names>Pei Fen</given-names>
        </name>
        <xref rid="CR1" ref-type="corresp">*</xref>
      </contrib>
      <aff id="A1">Department of Applied Mathematics and Statistics, Stony Brook University, Stony Brook, NY 11794, USA</aff>
    </contrib-group>
    <author-notes>
      <corresp id="CR1"><label>*</label><email>peifen.kuan@stonybrook.edu</email>, <ext-link xlink:href="http://www.ams.sunysb.edu" ext-link-type="uri">www.ams.sunysb.edu</ext-link></corresp>
    </author-notes>
    <pub-date pub-type="nihms-submitted">
      <day>23</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>03</day>
      <month>1</month>
      <year>2024</year>
    </pub-date>
    <volume>29</volume>
    <fpage>627</fpage>
    <lpage>640</lpage>
    <permissions>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>Open Access chapter published by World Scientific Publishing Company and distributed under the terms of the Creative Commons Attribution Non-Commercial (CC BY-NC) 4.0 License.</license-p>
      </license>
    </permissions>
    <abstract id="ABS1">
      <p id="P1">High throughput profiling of multiomics data provides a valuable resource to better understand the complex human disease such as cancer and to potentially uncover new subtypes. Integrative clustering has emerged as a powerful unsupervised learning framework for subtype discovery. In this paper, we propose an efficient weighted integrative clustering called intCC by combining ensemble method, consensus clustering and kernel learning integrative clustering. We illustrate that intCC can accurately uncover the latent cluster structures via extensive simulation studies and a case study on the TCGA pan cancer datasets. An R package intCC implementing our proposed method is available at <ext-link xlink:href="https://github.com/candsj/intCC" ext-link-type="uri">https://github.com/candsj/intCC</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>Integrative clustering</kwd>
      <kwd>Consensus clustering</kwd>
      <kwd>Multiomics data</kwd>
      <kwd>Ensemble learning</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <label>1.</label>
    <title>Introduction</title>
    <p id="P2">Recent advancements in high throughput technologies have enabled rapid profiling of different omics data, including genomics, epigenomics, transcriptomics, proteomics and metabolomics which allow for in-depth study of the complex regulatory patterns from a systems biology perspective. For example, the Cancer Genome Atlas (TCGA) has generated over 2.5 petabytes of multiomics data. Such datasets offer the opportunity to explore the heterogeneity underpinning diseases such as cancer via unsupervised learning based on clustering framework, which could help define cancer subtypes, bringing us a step closer towards personalized medicine.</p>
    <p id="P3">In multimodal data structure, e.g., the different omics data, a key challenge in data analysis is in identifying the most appropriate approach for data integration. For unsupervised clustering over multimodal data, these include the choice of a single step <italic toggle="yes">versus</italic> two-step approach. A single step approach is also known as joint modeling which combines all datasets together. Two-step approach works by clustering each dataset separately, followed by integration of these clusters.</p>
    <p id="P4">A number of integrative clustering methods and tools have been proposed to date. This includes Bayesian Consensus Clustering (BCC<sup><xref rid="R1" ref-type="bibr">1</xref></sup>), iCluster,<sup><xref rid="R2" ref-type="bibr">2</xref></sup> iClusterPlus,<sup><xref rid="R3" ref-type="bibr">3</xref></sup> Cluster Of Clusters Analysis (COCA<sup><xref rid="R4" ref-type="bibr">4</xref></sup>), Clusternomics<sup><xref rid="R5" ref-type="bibr">5</xref></sup> and kernel learning integrative clustering (KLIC<sup><xref rid="R6" ref-type="bibr">6</xref></sup>). BCC, Clusternomics and iClusterPlus are based on Bayesian modeling framework and rely on Markov Chain Monte Carlo (MCMC) algorithm for fitting the model. These methods also assume that the probability model for each dataset is specified. However, softwares for BCC and Clusternomics currently only implement the algorithms for Gaussian distributed dataset, thus limiting the applicability of these methods to non-Gaussian datasets such as SNPs, mutation or copy number datasets.</p>
    <p id="P5">On the other hand, iCluster works by assuming a Gaussian latent variable model for inferring the cluster structures, whereas iClusterPlus increases the versatility of iCluster by incorporating statistical models for continuous, binary, multinomial count datasets via a Bayesian latent variable model and employs MCMC algorithm for sampling from its posterior distribution for statistical inference. However, software implementation of iClusterPlus currently is limited to integrative clustering of at most four datasets. Since the model involves tuning a number of parameters, the bottleneck is the computational time when the number of datasets or features increases.</p>
    <p id="P6">Another popular integrative clustering approach is COCA<sup><xref rid="R4" ref-type="bibr">4</xref></sup> which was first introduced to define cancer subtypes by clustering six different datasets, namely DNA copy number, DNA methylation, mRNA expression, microRNA expression, protein expression, and somatic point mutation. COCA works by first clustering each dataset using consensus clustering,<sup><xref rid="R7" ref-type="bibr">7</xref></sup> followed by clustering the binary matrix generated by aggregating the clusters obtained from each dataset. While this approach is robust and easily scalable to a large number of datasets, a limitation of COCA is that all datasets contribute equally to the final clustering which affects the accuracy of the clusters obtained, especially in scenario in which certain dataset is less reliable.</p>
    <p id="P7">Taking inspiration from COCA and multiple kernel learning,<sup><xref rid="R8" ref-type="bibr">8</xref>,<xref rid="R9" ref-type="bibr">9</xref></sup> KLIC<sup><xref rid="R6" ref-type="bibr">6</xref></sup> was developed to address the pitfall of COCA. Similar to COCA, KLIC works by first applying consensus clustering to each dataset. The authors proved that these consensus matrices are positive semi-definite kernels, which can then be used as input in multiple kernel <italic toggle="yes">k</italic>-means clustering and allows for weights to be estimated for each kernel via a two-step optimization strategy and convex quadratic programming. This approach allows for more informative dataset to contribute more to the overall clustering. Currently, KLIC runs one clustering algorithm on each dataset to generate the consensus matrix.</p>
    <p id="P8">In this paper, we seek to extend the KLIC framework to a more robust integrative clustering by proposing a two layer weighted integrative clustering which allows for more than one clustering algorithm to be run on each dataset, i.e, ensemble clustering and aggregated together via an efficient weight estimation.</p>
  </sec>
  <sec id="S2">
    <label>2.</label>
    <title>Methods</title>
    <p id="P9">Our proposed method can be viewed as a combination of (a) ensemble clustering, i.e, aggregating multiple clustering algorithms, (b) consensus clustering, i.e., resampling, and (c) kernel learning integrative clustering. While some papers use ensemble and consensus clustering interchangeably, in this paper, we refer to ensemble clustering as a collection of multiple clustering algorithms, e.g., <italic toggle="yes">k</italic>-means, hierarchical clustering or partitioning around medoid (PAM), whereas consensus clustering as a framework which draws a random sample from either the sample or feature space. We now briefly describe the consensus clustering and kernel learning integrative clustering framework.</p>
    <p id="P10">Consensus clustering was originally proposed by Monti et al (2003).<sup><xref rid="R7" ref-type="bibr">7</xref></sup> The main idea behind consensus clustering is to apply a resampling scheme on the sample or feature dimension under the assumption that different subsamples drawn from the dataset should not differ much in the clustering results. The resampling scheme allows one to assess the stability of the cluster assignments and the robustness of the dataset to perturbations, thus could aid in deriving a more stable and reliable result that reveals the real structure underlying the dataset.</p>
    <p id="P11">A key element derived from the consensus clustering is the consensus matrix which measures the agreement among samples. For a dataset with <inline-formula><mml:math id="M2" display="inline"><mml:mi>N</mml:mi></mml:math></inline-formula> samples, the consensus matrix <inline-formula><mml:math id="M3" display="inline"><mml:mi>ℳ</mml:mi></mml:math></inline-formula> is a <inline-formula><mml:math id="M4" display="inline"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> matrix whose element <inline-formula><mml:math id="M5" display="inline"><mml:mrow><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> denotes the proportion of sample <inline-formula><mml:math id="M6" display="inline"><mml:mi>i</mml:mi></mml:math></inline-formula> and sample <inline-formula><mml:math id="M7" display="inline"><mml:mi>j</mml:mi></mml:math></inline-formula> in the same cluster during the resampling iterations. Values which are close to 1 (and vice versa 0) indicate that the two samples are always assigned to the same cluster (and vice versa different clusters). <inline-formula><mml:math id="M8" display="inline"><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ℳ</mml:mi></mml:mrow></mml:math></inline-formula> is a distance measure which can be used to derive a final clustering result.</p>
    <p id="P12">Cabassi and Kirk (2020)<sup><xref rid="R6" ref-type="bibr">6</xref></sup> proved that the consensus matrix is positive semi-definite and thus can be used as input in kernel learning integrative clustering via the application of multiple kernel <italic toggle="yes">k</italic>-means algorithm. The kernel <italic toggle="yes">k</italic>-means algorithm utilizes the kernel trick by projecting the data into a non-linear feature space via a kernel. This overcomes the drawback of regular <italic toggle="yes">k</italic>-means clustering which cannot identify clusters that are not linearly separable in the original input space. The integration of the multimodal data within the kernel learning integrative clustering involves a convex sum of the kernels, i.e., consensus matrix from each dataset, and the estimation of the weights in the convex sum. In the KLIC integrative clustering algorithm of Cabassi and Kirk (2020),<sup><xref rid="R6" ref-type="bibr">6</xref></sup> the authors adopted the optimization strategy proposed by Gonen and Margolin (2014)<sup><xref rid="R10" ref-type="bibr">10</xref></sup> which involves a convex quadratic programming.</p>
    <p id="P13">In this paper, we reason that the weights in the kernel learning integrative clustering can be estimated by utilizing the fuzziness in the consensus matrix. Furthermore, we extend the framework of KLIC by allowing multiple base clustering algorithms, e.g., <italic toggle="yes">k</italic>-means, hierarchical clustering, PAM, to be applied within each dataset and aggregated, i.e., ensemble clustering<sup><xref rid="R11" ref-type="bibr">11</xref></sup> which has been shown to enhance the robustness of clustering results compared to individual clustering algorithm. To this end, we propose an efficient weight estimation method and a two layer weighted integrative consensus clustering.</p>
    <sec id="S3">
      <label>2.1.</label>
      <title>Weight estimation</title>
      <p id="P14">The consensus matrix can be used to assess cluster stability and composition. As a motivating example, we generate two datasets, each with 10 features and 100 samples. For both datasets, we assume that there are 3 clusters with cluster sizes 20, 30 and 50. All the features are generated from the Gaussian distribution. For dataset 1, 9 out of the 10 features are informative, where the means of cluster 1, 2 and 3 are 1, −1 and 0, respectively with unit variance. For dataset 2, 3 out of the 10 features are informative, where the means of cluster 1, 2 and 3 are 0.2, −0.2 and 0, respectively with unit variance. Non-informative features are generated from standard Gaussian distribution. We designate datasets 1 and 2 as having high and and low signal-to-noise-ratio (SNR), respectively and run consensus clustering on both datasets using 100 iterations of <italic toggle="yes">k</italic>-means and resampling 80% of samples and features in each iteration. <xref rid="F1" ref-type="fig">Figure 1</xref> shows the heatmaps of the consensus matrices. The diagonal blocks plot the in-cluster values, whereas the off diagonals blocks plot the out-of-cluster values. For the low SNR dataset, the off diagonal blocks are much noisier compared to the high SNR dataset. We argue that this can be used to derive the weights in the multiple kernel integrative clustering. Specifically, we define the weights based on the ratio of in-cluster proportion to out-of-cluster proportion using the cluster estimated by the algorithm itself. Clustering result closer to the real structure tends to have higher in-cluster proportion and lower out-of-cluster proportion. In other words, datasets with a higher ratio of in-cluster proportion to out-of-cluster proportion will be assigned larger weights.</p>
      <p id="P15">Without loss of generality, we consider <inline-formula><mml:math id="M9" display="inline"><mml:mi>𝒫</mml:mi></mml:math></inline-formula> consensus matrices <inline-formula><mml:math id="M10" display="inline"><mml:mrow><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> for number of clusters <italic toggle="yes">K</italic>. Here, the consensus matrices could arise by applying different clustering algorithms to the same dataset or could denote consensus matrices derived from different datasets. We further define:</p>
      <list list-type="simple" id="L2">
        <list-item>
          <p id="P16"><inline-formula><mml:math id="M11" display="inline"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>: in-cluster proportion for cluster <inline-formula><mml:math id="M12" display="inline"><mml:mi>k</mml:mi></mml:math></inline-formula> of consensus matrix <inline-formula><mml:math id="M13" display="inline"><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P17"><inline-formula><mml:math id="M14" display="inline"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>: out-of-cluster proportion for cluster <inline-formula><mml:math id="M15" display="inline"><mml:mi>k</mml:mi></mml:math></inline-formula> of consensus matrix <inline-formula><mml:math id="M16" display="inline"><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P18"><inline-formula><mml:math id="M17" display="inline"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>: average in-cluster proportion across all clusters of consensus matrix <inline-formula><mml:math id="M18" display="inline"><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P19"><inline-formula><mml:math id="M19" display="inline"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>: average out-of-cluster proportion across all clusters of consensus matrix <inline-formula><mml:math id="M20" display="inline"><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P20"><inline-formula><mml:math id="M21" display="inline"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>: ratio of in-cluster proportion to out-of-cluster proportion for consensus matrix <inline-formula><mml:math id="M22" display="inline"><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P21"><inline-formula><mml:math id="M23" display="inline"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>: weight for consensus matrix <inline-formula><mml:math id="M24" display="inline"><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
      </list>
      <p id="P22">We propose calculating the weights as follows:
<disp-formula id="FD1"><mml:math id="M1" display="block"><mml:mspace width="0.4em"/><mml:mrow><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∑</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mspace width="thinmathspace"/><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>K</mml:mi></mml:mfrac><mml:mspace linebreak="newline"/><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>∉</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∑</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>∉</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mspace width="thinmathspace"/><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>K</mml:mi></mml:mfrac><mml:mspace linebreak="newline"/><mml:mspace width="2.1em"/><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow></mml:mfrac><mml:mspace linebreak="newline"/><mml:mspace width="1.9em"/><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mi>p</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
      <p id="P23">In practice, true cluster membership is unknown, thus the weights will be computed based on predicted cluster membership. Using this formula, <italic toggle="yes">W</italic><sub>1</sub> = 0.726 and <italic toggle="yes">W</italic><sub>2</sub> = 0.274 for the consensus matrices derived based on predicted cluster membership of the two datasets above.</p>
    </sec>
    <sec id="S4">
      <label>2.2.</label>
      <title>Two Layer Weighted Integrative Consensus Clustering</title>
      <p id="P24">We now describe our proposed two layer weighted integrative consensus clustering. We assume that there are <inline-formula><mml:math id="M25" display="inline"><mml:mi>D</mml:mi></mml:math></inline-formula> datasets, <inline-formula><mml:math id="M26" display="inline"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and number of clusters <italic toggle="yes">K</italic>.</p>
      <p id="P25">Layer 1: For each dataset <inline-formula><mml:math id="M27" display="inline"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="M28" display="inline"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:math></inline-formula>:</p>
      <list list-type="order" id="L4">
        <list-item>
          <p id="P26">Perform ensemble clustering using <italic toggle="yes">P</italic> different clustering methods, where <inline-formula><mml:math id="M29" display="inline"><mml:mrow><mml:mi>p</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula>. This will generate consensus matrices <inline-formula><mml:math id="M30" display="inline"><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M31" display="inline"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P27">Compute the weights <inline-formula><mml:math id="M32" display="inline"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mn>1</mml:mn><mml:mi>d</mml:mi></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mn>2</mml:mn><mml:mi>d</mml:mi></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>P</mml:mi><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> for each consensus matrix <inline-formula><mml:math id="M33" display="inline"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mn>1</mml:mn><mml:mi>d</mml:mi></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mn>2</mml:mn><mml:mi>d</mml:mi></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mi>P</mml:mi><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P28">Define the weighted consensus matrix <inline-formula><mml:math id="M34" display="inline"><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> as <inline-formula><mml:math id="M35" display="inline"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:msubsup></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P29">Apply a clustering algorithm, e.g., PAM or hierarchical clustering, to each weighted consensus matrix <inline-formula><mml:math id="M36" display="inline"><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
      </list>
      <p id="P30">Layer 2:</p>
      <list list-type="order" id="L6">
        <list-item>
          <p id="P31">For the weighted consensus matrix <inline-formula><mml:math id="M37" display="inline"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, compute the weights <inline-formula><mml:math id="M38" display="inline"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P32">Define the weighted of weighted consensus matrix <inline-formula><mml:math id="M39" display="inline"><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> as <inline-formula><mml:math id="M40" display="inline"><mml:mrow><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P33">Apply a clustering algorithm, e.g., PAM or hierarchical clustering, to <inline-formula><mml:math id="M41" display="inline"><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> to derive a final clustering result.</p>
        </list-item>
      </list>
      <p id="P34">We provide a flowchart in <xref rid="F2" ref-type="fig">Figure 2</xref> summarizing our proposed two layer weighted integrative consensus clustering. Our method is implemented as a GitHub R package intCC available at <ext-link xlink:href="https://github.com/candsj/intCC" ext-link-type="uri">https://github.com/candsj/intCC</ext-link>.</p>
    </sec>
  </sec>
  <sec id="S5">
    <label>3.</label>
    <title>Simulation studies</title>
    <p id="P35">We conduct simulation studies to compare the performance of our proposed two layer weighted integrative consensus clustering intCC against other integrative clustering methods which are implemented for both Gaussian and non-Gaussian distributed datasets, namely KLIC<sup><xref rid="R6" ref-type="bibr">6</xref></sup> and iClusterPlus.<sup><xref rid="R3" ref-type="bibr">3</xref></sup></p>
    <sec id="S6">
      <label>3.1.</label>
      <title>Datasets</title>
      <p id="P36">Unlike Cabassi and Kirk (2020)<sup><xref rid="R6" ref-type="bibr">6</xref></sup> which only considered data simulated from Gaussian distributions, we follow the strategy of Mo et al. (2013)<sup><xref rid="R3" ref-type="bibr">3</xref></sup> where we generate datasets from different distributions, including Gaussian (e.g., M-values from DNA methylation, microarray data such as gene expression), binomial (e.g., somatic mutations), Poisson (e.g., count data from sequencing technologies such as RNA-Seq data or copy number data represented as number of copies gained or lost) and multinomial (e.g., copy number data states represented as gain, normal or loss, or SNP data) distributions. This is to ensure that our proposed method is applicable to integration of continuous, binary, count and categorical types of datasets. For Settings 1-6, we set the sample size and the true number of clusters to be 60 and 3, respectively in which each cluster consists of 20 samples. We vary the number of informative and non-informative, i.e., noise features. The parameters used in our simulations for Settings 1-6 are provided in <xref rid="SD1" ref-type="supplementary-material">Supplementary Table 1</xref>. Settings 7-9 follow from the simulation setup of of Cabassi and Kirk (2020).<sup><xref rid="R6" ref-type="bibr">6</xref></sup> We consider several simulation settings, namely:</p>
      <list list-type="order" id="L8">
        <list-item>
          <p id="P37">Setting 1: 4 datasets following Gaussian, binomial, Poisson and multinomial distribution, respectively. Each dataset has 30 features, in which 15 features are informative and the rest are noise features.</p>
        </list-item>
        <list-item>
          <p id="P38">Setting 2: 4 datasets includes normal, binomial, Poisson and multinomial distribution, respectively. Each dataset has 30 features, in which 15 features are informative and the rest are noise features. Informative features have slighly lower signal compared to the Setting 1.</p>
        </list-item>
        <list-item>
          <p id="P39">Setting 3: 3 datasets following Gaussian, binomial and Poisson distribution, respectively. Each dataset has 30 features, in which 15 features are informative and the rest are noise features.</p>
        </list-item>
        <list-item>
          <p id="P40">Setting 4: 5 datasets following Gaussian, binomial, Poisson, multinomial and Gaussian distribution, respectively. Each dataset has 30 features. For the first 4 datasets, 15 features are informative and the rest are noise features. The 5th dataset follows a Gaussian distribution in which all features are noise features.</p>
        </list-item>
        <list-item>
          <p id="P41">Setting 5: 4 datasets following Gaussian, binomial, Poisson and multinomial distribution, respectively. Each dataset has 500 features, in which 100 features are informative and the rest are noise features.</p>
        </list-item>
        <list-item>
          <p id="P42">Setting 6: 4 datasets following Gaussian, binomial, Poisson and multinomial distribution, respectively. Each dataset has 500 features, in which 250 features are informative and the rest are noise features.</p>
        </list-item>
        <list-item>
          <p id="P43">Setting 7: 4 datasets following Gaussian distribution with similar parameter setting. Each dataset consists of 300 samples with 6 clusters of size 50 samples each. There are 2 features with no noise feature. For cluster <italic toggle="yes">k</italic>, <italic toggle="yes">μ</italic> = <italic toggle="yes">k</italic> × (separation level − 1)/2, <italic toggle="yes">σ</italic> = 1, <italic toggle="yes">k</italic> = 1, 2, 3, 4, 5, 6. Separation level = 4 is used in this setting.</p>
        </list-item>
        <list-item>
          <p id="P44">Setting 8: 4 datasets following Gaussian distribution with different parameter setting. Each dataset consists of 300 samples with 6 clusters of size 50 samples each. There are 2 features with no noise feature. For cluster <italic toggle="yes">k</italic>, <italic toggle="yes">μ</italic> = <italic toggle="yes">k</italic> × (separation level − 1)/2, <italic toggle="yes">σ</italic> = 1, <italic toggle="yes">k</italic> = 1, 2, 3, 4, 5, 6. Varying separation levels = 1, 2, 3, 4 are used in this setting. Only 3 datasets are used as input. We consider 4 dataset combinations, namely 123, 124, 134, 234. Here 123 implies that the clustering algorithms are applied to only datasets 1, 2 and 3.</p>
        </list-item>
        <list-item>
          <p id="P45">Setting 9 (nested cluster structure): 2 datasets following Gaussian distribution, in which each dataset consists of 300 samples. There are 2 features with no noise feature. Dataset 1 has 6 clusters of size 50 samples each. Dataset 2 has 3 clusters of size 100 samples each. For cluster <italic toggle="yes">k</italic>, <italic toggle="yes">μ</italic> = <italic toggle="yes">k</italic> × (separation level − 1)/2, <italic toggle="yes">σ</italic> = 1, <italic toggle="yes">k</italic> = 1, 2, 3, 4, 5, 6 for dataset 1 and <italic toggle="yes">k</italic> = 1, 2, 3 for dataset 2. Separation level = 4 is used in this setting.</p>
        </list-item>
      </list>
      <p id="P46">Each setting is repeated 100 times. Additional simulation settings including multivariate Gaussian distribution are provided in <xref rid="SD1" ref-type="supplementary-material">Supplementary Material</xref>.</p>
    </sec>
    <sec id="S7">
      <label>3.2.</label>
      <title>Clustering algorithms</title>
      <p id="P47">We apply several clustering strategies based on our proposed method intCC, KLIC<sup><xref rid="R6" ref-type="bibr">6</xref></sup> and iClusterPlus.<sup><xref rid="R3" ref-type="bibr">3</xref></sup> To evaluate the advantage of ensemble clustering, i.e., applying multiple clustering algorithms to each dataset, we also include our proposed method which only runs a single clustering algorithm to each dataset. We denote this as one layer weighted integrative consensus clustering. We also compare application of PAM and hierarchical clustering to the weighted consensus matrix in deriving a final clustering result. These methods are denoted as:</p>
      <list list-type="order" id="L10">
        <list-item>
          <p id="P48">iClusterPlus: applying iClusterPlus with the data type specified.</p>
        </list-item>
        <list-item>
          <p id="P49">KLIC-k-means: KLIC by applying <italic toggle="yes">k</italic>-means to each dataset for generating the consensus matrix.</p>
        </list-item>
        <list-item>
          <p id="P50">KLIC-Hclust: KLIC by applying hierarchical clustering to each dataset for generating the consensus matrix.</p>
        </list-item>
        <list-item>
          <p id="P51">1 layer intCC-k-means (PAM): One layer weighted integrative consensus clustering by applying <italic toggle="yes">k</italic>-means to each dataset for generating the consensus matrix, followed by PAM to derive a final clustering result.</p>
        </list-item>
        <list-item>
          <p id="P52">1 layer intCC-Hclust (PAM): One layer weighted integrative consensus clustering by applying hierarchical clustering to each dataset for generating the consensus matrix, followed by PAM to derive a final clustering result.</p>
        </list-item>
        <list-item>
          <p id="P53">1 layer intCC-k-means (Hclust): One layer weighted integrative consensus clustering by applying <italic toggle="yes">k</italic>-means to each dataset for generating the consensus matrix, followed by hierarchical clustering to derive a final clustering result.</p>
        </list-item>
        <list-item>
          <p id="P54">1 layer intCC-Hclust (Hclust): One layer weighted integrative consensus clustering by applying hierarchical clustering to each dataset for generating the consensus matrix, followed by hierarchical clustering to derive a final clustering result.</p>
        </list-item>
      </list>
      <p id="P55">To obtain an unbiased comparison to our two layer approach, we also apply KLIC with multiple clustering algorithms. In other words, suppose there are 4 datasets and two clustering algorithms are applied to each dataset, there will be a total of 8 consensus matrices, i.e., akin to applying KLIC to 8 datasets. KLIC is applied using these 8 consensus matrices as input in the multiple kernel integrative clustering. Additionally, to illustrate the advantage of two layer approach, we also include another one layer approach in which we apply a single layer weight estimation to the 8 consensus matrices. These methods are denoted as:</p>
      <list list-type="simple" id="L12">
        <list-item>
          <label>(8)</label>
          <p id="P56">2 layer intCC-2 methods (PAM): Two layer weighted integrative consensus clustering by applying both <italic toggle="yes">k</italic>-means and hierarchical clustering to each dataset for generating the consensus matrices, followed by PAM to derive a final clustering result.</p>
        </list-item>
        <list-item>
          <label>(9)</label>
          <p id="P57">2 layer intCC-2 methods (Hclust): Two layer weighted integrative consensus clustering by applying both <italic toggle="yes">k</italic>-means and hierarchical clustering to each dataset for generating the consensus matrices, followed by hierarchical clustering to derive a final clustering result.</p>
        </list-item>
        <list-item>
          <label>(10)</label>
          <p id="P58">KLIC-2-methods: KLIC by applying both <italic toggle="yes">k</italic>-means and hierarchical clustering to each dataset for generating the consensus matrices.</p>
        </list-item>
        <list-item>
          <label>(11)</label>
          <p id="P59">1 layer intCC-2 methods (PAM): One layer weighted integrative consensus clustering by applying both <italic toggle="yes">k</italic>-means and hierarchical clustering to each dataset for generating the consensus matrices, followed by PAM to derive a final clustering result.</p>
        </list-item>
        <list-item>
          <label>(12)</label>
          <p id="P60">1 layer intCC-2 methods (Hclust): One layer weighted integrative consensus clustering by applying both <italic toggle="yes">k</italic>-means and hierarchical clustering to each dataset for generating the consensus matrices, followed by hierarchical clustering to derive a final clustering result.</p>
        </list-item>
      </list>
      <p id="P61">For Settings 1-8, we apply each method by setting the number of clusters to be the true number of clusters. In practice, one can tune the optimal number of clusters using criteria such as the silhouette method,<sup><xref rid="R12" ref-type="bibr">12</xref></sup> gap statistics,<sup><xref rid="R13" ref-type="bibr">13</xref></sup> Dunn index<sup><xref rid="R14" ref-type="bibr">14</xref></sup> or the delta <italic toggle="yes">K</italic> method.<sup><xref rid="R7" ref-type="bibr">7</xref></sup> For Setting 9, we consider (a) global clustering, where we set the number of clusters to be the same throughout for both individual dataset and final integrative clustering, i.e., either 3 or 6 throughout (we denote these strategies as “Global K=3” and “Global K=6”), and (b) separate clustering, where we use the true number of clusters for individual dataset, i.e., 6 for dataset 1 and 3 for dataset 2, and consider both <italic toggle="yes">K</italic> = 3 and <italic toggle="yes">K</italic> = 6 in the final integrative clustering (we denote these strategies as “Separate K=3” and “Separate K=6”). Additionally, due to the poor performance of iClusterPlus and the long computational time, we omit iClusterPlus for Settings 4-6. We compare the performance of the clustering methods via the average adjusted rand index (ARI). We also report the weight estimation time of intCC and KLIC.</p>
    </sec>
    <sec id="S8">
      <label>3.3.</label>
      <title>Results</title>
      <p id="P62">We summarize the ARI for each simulation setting in <xref rid="F3" ref-type="fig">Figure 3</xref>. Overall, results show that our proposed methods, namely 2 layer intCC-2 methods (PAM) and 1 layer intCC-k-means (PAM) perform well across all simulation settings. To explain this observation, without loss of generality, we summarize the ARI within each simulated dataset of Setting 4 in <xref rid="F4" ref-type="fig">Figures 4A</xref> and <xref rid="F4" ref-type="fig">4B</xref>. The ARI by applying <italic toggle="yes">k</italic>-means as the base algorithm in the consensus clustering within each dataset is significantly better than hierarchical clustering in the simulated datasets considered in this paper. Thus, it is not surprising that methods which use <italic toggle="yes">k</italic>-means as the base clustering algorithm in the consensus clustering yield better performance. However, in practice the best base clustering algorithm is sometimes unknown. Thus, the 2 layer intCC which aggregates multiple base clustering algorithms can automatically assign higher weights to the better algorithm as shown in our simulation studies, as evident from the estimated weights in <xref rid="F4" ref-type="fig">Figures 4C</xref> and <xref rid="F4" ref-type="fig">4D</xref>. It is also worth noting that our method assigns significantly smaller weights to the 5th dataset in which all the features are noise features. Additionally, using PAM to derive a final clustering result in general yields better performance compared to hierarchical clustering. We also note that the performance of iClusterPlus is significantly poorer compared to other methods, consistent with the findings of Cabassi and Kirk (2020).<sup><xref rid="R6" ref-type="bibr">6</xref></sup> Moreover, extending KLIC to run multiple base clustering algorithms, i.e., KLIC-2-methods has lower ARI compared to our proposed method, implying that the current KLIC framework does not yield a straightforward extension to incorporate ensemble clustering.</p>
      <p id="P63">Without loss of generality, we also report the weight calculation time for KLIC and our proposed method intCC for Setting 1 (60 samples) and Setting 7 (300 samples) in <xref rid="T1" ref-type="table">Table 1</xref>, which shows that our proposed weight calculation is computationally efficient and yields good operating characteristics.</p>
    </sec>
  </sec>
  <sec id="S9">
    <label>4.</label>
    <title>Case study</title>
    <p id="P64">We illustrate our proposed method intCC on the TCGA pan cancer datasets.<sup><xref rid="R15" ref-type="bibr">15</xref></sup> There are 5 datasets across 12 cancer types which represent different tissues of origin, including DNA copy number, DNA methylation, mRNA expression, microRNA expression and protein expression data. To minimize bias in the comparison, we use the same preprocessing pipeline as previously described.<sup><xref rid="R6" ref-type="bibr">6</xref>,<xref rid="R15" ref-type="bibr">15</xref></sup></p>
    <p id="P65">Cabassi and Kirk (2020)<sup><xref rid="R6" ref-type="bibr">6</xref></sup> followed the same procedures described in Hoadley et al. (2014)<sup><xref rid="R15" ref-type="bibr">15</xref></sup> in setting the number of clusters for each dataset, except for microRNA expression in which the authors identified 8 as the number of clusters. We also set the number of clusters for each dataset following Cabassi and Kirk (2020).<sup><xref rid="R6" ref-type="bibr">6</xref></sup> Subsequently, we apply our proposed method intCC to obtain an integrative clustering across these datasets using the PAM algorithm to derive a final clustering result. Our method also selects 10 as the optimal number of clusters based on the average silhouette criterion, similar to KLIC.<sup><xref rid="R6" ref-type="bibr">6</xref></sup>
<xref rid="F5" ref-type="fig">Figure 5A</xref> compares the cluster membership of our method intCC against the results of KLIC, with ARI 0.693, whereas <xref rid="F5" ref-type="fig">Figures 5B</xref> and <xref rid="F5" ref-type="fig">5C</xref> compare the cluster membership of intCC and KLIC against the 12 cancer type annotation, respectively. The ARI between intCC and cancer type annotation associated with tissues of origin is 0.754, whereas the ARI between KLIC and cancer type annotation is 585, indicating that the cluster membership of intCC yields a higher consistency with tissues of origin in the TCGA pan cancer datasets. Further investigation into the clusters obtained by intCC versus KLIC among subset of breast invasive carcinoma (BRCA) indicates that the results from intCC yield a higher consistency with the TCGA-BRCA molecular subtypes compared to the results from KLIC (<xref rid="SD1" ref-type="supplementary-material">Supplementary Material</xref>).</p>
    <p id="P66">The estimated weights of each dataset for intCC and KLIC are (DNA copy number, DNA methylation, mRNA expression, miRNA expression, protein expression) =(0.073, 0.401,0.045, 0.272, 0.209) and (0.309, 0.192, 0.168, 0.183, 0.148), respectively. intCC assigns a higher weight to DNA methylation data, whereas KLIC assigns a higher weight to the copy number data, which could explain the differences observed in cluster memberships obtained by these two methods. Finally, the weight calculation time for intCC is 0.43 second, whereas the weight calculation time for KLIC via quadratic programming is &gt; 10 hours on an Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz.</p>
  </sec>
  <sec id="S10">
    <label>5.</label>
    <title>Discussion</title>
    <p id="P67">The rapid development of high throughput technologies has provided an avenue to scientists to decipher the complex human diseases from a systems biology perspective via multiomics profiling. Integrative clustering has become a powerful approach to dissect the heterogeneity underpinning these diseases, e.g., to define new cancer subtypes which may help inform treatment efforts. In this paper, we extend the framework of KLIC<sup><xref rid="R6" ref-type="bibr">6</xref></sup> which recasts the integrative clustering model into multiple kernel learning framework by utilizing the consensus matrices estimated from consensus clustering as input. Specifically, our model further incorporates the ensemble learning via an aggregation of multiple base clustering algorithms to enhance the robustness of multiple kernel integrative clustering model. This is to safeguard against applying a single base clustering algorithm that performs poorly on the dataset. Additionally, we also propose an efficient weight estimation to combine the consensus matrices. Our simulation studies show that the proposed two layer weighted integrative clustering yields better performance overall.</p>
    <p id="P68">Conceptually, the weight estimation is analogous to the heuristics of multiple kernel support vector machine (MKL-SVM) based on kernel-target alignment.<sup><xref rid="R16" ref-type="bibr">16</xref>-<xref rid="R18" ref-type="bibr">18</xref></sup> Specifically, MKL-SVM is developed for supervised learning and the kernel-target alignment depends on the true binary class labels. For a fixed cluster membership, this is equivalent to multi-class classification. One can extend the kernel-target alignment for multi-class classification by dividing the problem into several binary classification subproblems (e.g., one-versus-all or all-pairs). However, how to optimally combine the results across these binary subproblems is not trivial and may require longer computational time compared to our proposed method.</p>
    <p id="P69">Besides identifying appropriate and robust clustering algorithms, another important research question in unsupervised learning is in tuning the optimal number of clusters. Several metrics have been proposed for this task, including the silhouette method,<sup><xref rid="R12" ref-type="bibr">12</xref></sup> gap statistics,<sup><xref rid="R13" ref-type="bibr">13</xref></sup> Dunn index<sup><xref rid="R14" ref-type="bibr">14</xref></sup> and the delta <italic toggle="yes">K</italic> method.<sup><xref rid="R7" ref-type="bibr">7</xref></sup> An immediate extension to our intCC framework is to aggregate the different metrics/criteria for selecting the optimal number of clusters.</p>
  </sec>
  <sec sec-type="supplementary-material" id="SM1">
    <title>Supplementary Material</title>
    <supplementary-material id="SD1" position="float" content-type="local-data">
      <label>suppl-1952198</label>
      <media xlink:href="NIHMS1952198-supplement-suppl-1952198.pdf" id="d64e1721" position="anchor"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="S11">
    <title>Acknowledgments</title>
    <p id="P70">This work is supported in part by CDC/NIOSH award U01OH012257. The findings and conclusions presented in this article are those of the authors and do not represent the official position of NIOSH, the CDC or the U.S. Public Health Service.</p>
  </ack>
  <fn-group>
    <fn fn-type="COI-statement" id="FN1">
      <p id="P71"><italic toggle="yes">Conflict of Interest</italic>: None declared.</p>
    </fn>
    <fn id="FN2">
      <p id="P72">Supplementary Material and Code</p>
      <p id="P73">Supplementary Material is available online at <ext-link xlink:href="http://www.ams.sunysb.edu/~pfkuan/PDF/SM_PSB2024.pdf" ext-link-type="uri">http://www.ams.sunysb.edu/~pfkuan/PDF/SM_PSB2024.pdf</ext-link>.</p>
      <p id="P74">The R code implementing intCC is available online at <ext-link xlink:href="https://github.com/candsj/intCC" ext-link-type="uri">https://github.com/candsj/intCC</ext-link>.</p>
    </fn>
  </fn-group>
  <ref-list>
    <title>References</title>
    <ref id="R1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><name><surname>Lock</surname><given-names>EF</given-names></name> and <name><surname>Dunson</surname><given-names>DB</given-names></name>, <article-title>Bayesian consensus clustering</article-title>, <source>Bioinformatics</source>
<volume>29</volume>, <fpage>2610</fpage> (<year>2013</year>).<pub-id pub-id-type="pmid">23990412</pub-id>
</mixed-citation>
    </ref>
    <ref id="R2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><name><surname>Shen</surname><given-names>R</given-names></name>, <name><surname>Olshen</surname><given-names>AB</given-names></name> and <name><surname>Ladanyi</surname><given-names>M</given-names></name>, <article-title>Integrative clustering of multiple genomic data types using a joint latent variable model with application to breast and lung cancer subtype analysis</article-title>, <source>Bioinformatics</source>
<volume>25</volume>, <fpage>2906</fpage> (<year>2009</year>).<pub-id pub-id-type="pmid">19759197</pub-id>
</mixed-citation>
    </ref>
    <ref id="R3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><name><surname>Mo</surname><given-names>Q</given-names></name>, <name><surname>Wang</surname><given-names>S</given-names></name>, <name><surname>Seshan</surname><given-names>VE</given-names></name>, <name><surname>Olshen</surname><given-names>AB</given-names></name>, <name><surname>Schultz</surname><given-names>N</given-names></name>, <name><surname>Sander</surname><given-names>C</given-names></name>, <name><surname>Powers</surname><given-names>RS</given-names></name>, <name><surname>Ladanyi</surname><given-names>M</given-names></name> and <name><surname>Shen</surname><given-names>R</given-names></name>, <article-title>Pattern discovery and cancer gene identification in integrated cancer genomic data</article-title>, <source>Proceedings of the National Academy of Sciences</source>
<volume>110</volume>, <fpage>4245</fpage> (<year>2013</year>).</mixed-citation>
    </ref>
    <ref id="R4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><collab>B. . W. H. . H. M. S.</collab><name><surname>C.</surname><given-names>L</given-names></name> . . <name><surname>P.</surname><given-names>PJ</given-names></name> . <name><surname>K.</surname><given-names>R</given-names></name> 13, <collab>G. data analysis: Baylor College of Medicine</collab>
<name><surname>Creighton</surname><given-names>Chad J.</given-names></name> 22 23 <name><surname>Donehower</surname><given-names>Lawrence A.</given-names></name> 22 23 24 25, <collab>I. for Systems Biology</collab>
<name><surname>Reynolds</surname><given-names>Sheila</given-names></name> 31 <name><surname>Kreisberg</surname><given-names>Richard B.</given-names></name> 31 <name><surname>Bernard</surname><given-names>Brady</given-names></name> 31 <name><surname>Bressler</surname><given-names>Ryan</given-names></name> 31 <name><surname>Erkkila</surname><given-names>Timo</given-names></name> 32 <name><surname>Lin</surname><given-names>Jake</given-names></name> 31 <name><surname>Thorsson</surname><given-names>Vesteinn</given-names></name> 31 <name><surname>Zhang</surname><given-names>Wei</given-names></name> 33 <name><surname>Shmulevich</surname><given-names>Ilya</given-names></name> 31 <etal/>, <article-title>Comprehensive molecular portraits of human breast tumours</article-title>, <source>Nature</source>
<volume>490</volume>, <fpage>61</fpage> (<year>2012</year>).<pub-id pub-id-type="pmid">23000897</pub-id>
</mixed-citation>
    </ref>
    <ref id="R5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><name><surname>Gabasova</surname><given-names>E</given-names></name>, <name><surname>Reid</surname><given-names>J</given-names></name> and <name><surname>Wernisch</surname><given-names>L</given-names></name>, <article-title>Clusternomics: Integrative context-dependent clustering for heterogeneous datasets</article-title>, <source>PLoS Computational Biology</source>
<volume>13</volume>, p. <fpage>e1005781</fpage> (<year>2017</year>).<pub-id pub-id-type="pmid">29036190</pub-id>
</mixed-citation>
    </ref>
    <ref id="R6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><name><surname>Cabassi</surname><given-names>A</given-names></name> and <name><surname>Kirk</surname><given-names>PD</given-names></name>, <article-title>Multiple kernel learning for integrative consensus clustering of omic datasets</article-title>, <source>Bioinformatics</source>
<volume>36</volume>, <fpage>4789</fpage> (<year>2020</year>).<pub-id pub-id-type="pmid">32592464</pub-id>
</mixed-citation>
    </ref>
    <ref id="R7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><name><surname>Monti</surname><given-names>S</given-names></name>, <name><surname>Tamayo</surname><given-names>P</given-names></name>, <name><surname>Mesirov</surname><given-names>J</given-names></name> and <name><surname>Golub</surname><given-names>T</given-names></name>, <article-title>Consensus clustering: a resampling-based method for class discovery and visualization of gene expression microarray data</article-title>, <source>Machine Learning</source>
<volume>52</volume>, <fpage>91</fpage> (<year>2003</year>).</mixed-citation>
    </ref>
    <ref id="R8">
      <label>8.</label>
      <mixed-citation publication-type="confproc"><name><surname>Bach</surname><given-names>FR</given-names></name>, <name><surname>Lanckriet</surname><given-names>GR</given-names></name> and <name><surname>Jordan</surname><given-names>MI</given-names></name>, <source>Multiple kernel learning, conic duality, and the smo algorithm</source>, in <conf-name>Proceedings of the Twenty-First International Conference on Machine Learning</conf-name>, (<conf-loc>Banff, Canada</conf-loc>, <year>2004</year>).</mixed-citation>
    </ref>
    <ref id="R9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><name><surname>Lanckriet</surname><given-names>GR</given-names></name>, <name><surname>Cristianini</surname><given-names>N</given-names></name>, <name><surname>Bartlett</surname><given-names>P</given-names></name>, <name><surname>Ghaoui</surname><given-names>LE</given-names></name> and <name><surname>Jordan</surname><given-names>MI</given-names></name>, <article-title>Learning the kernel matrix with semidefinite programming</article-title>, <source>Journal of Machine Learning Research</source>
<volume>5</volume>, <fpage>27</fpage> (<year>2004</year>).</mixed-citation>
    </ref>
    <ref id="R10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><name><surname>Gönen</surname><given-names>M</given-names></name> and <name><surname>Margolin</surname><given-names>AA</given-names></name>, <article-title>Localized data fusion for kernel k-means clustering with application to cancer biology</article-title>, <source>Advances in Neural Information Processing Systems</source>
<volume>27</volume> (<year>2014</year>).</mixed-citation>
    </ref>
    <ref id="R11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><name><surname>Sagi</surname><given-names>O</given-names></name> and <name><surname>Rokach</surname><given-names>L</given-names></name>, <article-title>Ensemble learning: A survey</article-title>, <source>Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</source>
<volume>8</volume>, p. <fpage>e1249</fpage> (<year>2018</year>).</mixed-citation>
    </ref>
    <ref id="R12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><name><surname>Rousseeuw</surname><given-names>PJ</given-names></name>, <article-title>Silhouettes: a graphical aid to the interpretation and validation of cluster analysis</article-title>, <source>Journal of Computational and Applied Mathematics</source><volume>20</volume>, <fpage>53</fpage> (<year>1987</year>).</mixed-citation>
    </ref>
    <ref id="R13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><name><surname>Tibshirani</surname><given-names>R</given-names></name>, <name><surname>Walther</surname><given-names>G</given-names></name> and <name><surname>Hastie</surname><given-names>T</given-names></name>, <article-title>Estimating the number of clusters in a data set via the gap statistic</article-title>, <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source>
<volume>63</volume>, <fpage>411</fpage> (<year>2001</year>).</mixed-citation>
    </ref>
    <ref id="R14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><name><surname>Halkidi</surname><given-names>M</given-names></name>, <name><surname>Batistakis</surname><given-names>Y</given-names></name> and <name><surname>Vazirgiannis</surname><given-names>M</given-names></name>, <article-title>On clustering validation techniques</article-title>, <source>Journal of Intelligent Information Systems</source>
<volume>17</volume>, <fpage>107</fpage> (<year>2001</year>).</mixed-citation>
    </ref>
    <ref id="R15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><name><surname>Hoadley</surname><given-names>KA</given-names></name>, <name><surname>Yau</surname><given-names>C</given-names></name>, <name><surname>Wolf</surname><given-names>DM</given-names></name>, <name><surname>Cherniack</surname><given-names>AD</given-names></name>, <name><surname>Tamborero</surname><given-names>D</given-names></name>, <name><surname>Ng</surname><given-names>S</given-names></name>, <name><surname>Leiserson</surname><given-names>MD</given-names></name>, <name><surname>Niu</surname><given-names>B</given-names></name>, <name><surname>McLellan</surname><given-names>MD</given-names></name>, <name><surname>Uzunangelov</surname><given-names>V</given-names></name><etal/>, <article-title>Multiplatform analysis of 12 cancer types reveals molecular classification within and across tissues of origin</article-title>, <source>Cell</source><volume>158</volume>, <fpage>929</fpage> (<year>2014</year>).<pub-id pub-id-type="pmid">25109877</pub-id>
</mixed-citation>
    </ref>
    <ref id="R16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><name><surname>Cristianini</surname><given-names>N</given-names></name>, <name><surname>Shawe-Taylor</surname><given-names>J</given-names></name>, <name><surname>Elisseeff</surname><given-names>A</given-names></name> and <name><surname>Kandola</surname><given-names>J</given-names></name>, <article-title>On kernel-target alignment</article-title>, <source>Advances in neural information processing systems</source>
<volume>14</volume> (<year>2001</year>).</mixed-citation>
    </ref>
    <ref id="R17">
      <label>17.</label>
      <mixed-citation publication-type="confproc"><name><surname>Cortes</surname><given-names>C</given-names></name>, <name><surname>Mohri</surname><given-names>M</given-names></name> and <name><surname>Rostamizadeh</surname><given-names>A</given-names></name>, <source>Two-stage learning kernel algorithms</source>, <conf-name>Proceedings of the 27 th International Conference on Machine Learning</conf-name> , <fpage>239</fpage> (<year>2010</year>).</mixed-citation>
    </ref>
    <ref id="R18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><name><surname>Qiu</surname><given-names>S</given-names></name> and <name><surname>Lane</surname><given-names>T</given-names></name>, <article-title>A framework for multiple kernel support vector regression and its applications to sirna efficacy prediction</article-title>, <source>IEEE/ACM Transactions on Computational Biology and Bioinformatics</source>
<volume>6</volume>, <fpage>190</fpage> (<year>2008</year>).</mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig position="float" id="F1">
    <label>Fig. 1.</label>
    <caption>
      <p id="P75">Heatmaps of consensus matrices for high and low signal-to-noise ratio (SNR) datasets. True cluster membership is given in the annotation above each heatmap. Predicted cluster membership corresponds to the three gap-separated blocks in each heatmap.</p>
    </caption>
    <graphic xlink:href="nihms-1952198-f0001" position="float"/>
  </fig>
  <fig position="float" id="F2">
    <label>Fig. 2.</label>
    <caption>
      <p id="P76">Flowchat describing our proposed algorithm.</p>
    </caption>
    <graphic xlink:href="nihms-1952198-f0002" position="float"/>
  </fig>
  <fig position="float" id="F3">
    <label>Fig. 3.</label>
    <caption>
      <p id="P77">Distribution of ARI across all methods and simulation settings. Blue (red) boxplots are methods which apply one (two) clustering algorithm(s) per dataset. A-G. Settings 1-7. H-K. Setting 8 with different dataset combinations as input. L-O. Setting 9 with different strategies for setting number of clusters for individual dataset and final integrative clustering.</p>
    </caption>
    <graphic xlink:href="nihms-1952198-f0003" position="float"/>
  </fig>
  <fig position="float" id="F4">
    <label>Fig. 4.</label>
    <caption>
      <p id="P78">A-B. Distribution of ARI within each simulated dataset of Setting 4. C-D. Distribution of estimated weights from intCC within each simulated dataset of Setting 4. Purple (green) boxplots are results by applying <italic toggle="yes">k</italic>-means (hierarchical clustering) algorithm in the consensus clustering. A, C. Using PAM to derive a final clustering result. B, D. Using hierarchical clustering to derive a final clustering result.</p>
    </caption>
    <graphic xlink:href="nihms-1952198-f0004" position="float"/>
  </fig>
  <fig position="float" id="F5">
    <label>Fig. 5.</label>
    <caption>
      <p id="P79">Heatmaps of coincidence matrices comparing A. intCC clusters to KLIC clusters, B. intCC clusters to cancer type annotation, C. KLIC clusters to cancer type annotation. The ARI is reported in the header of each plot.</p>
    </caption>
    <graphic xlink:href="nihms-1952198-f0005" position="float"/>
  </fig>
  <table-wrap position="float" id="T1">
    <label>Table 1.</label>
    <caption>
      <p id="P80">Weight calculation time comparison.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Method</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Setting 1 (seconds)</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Setting 7 (seconds)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">KLIC-k-means</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.541</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">7.209</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">KLIC-Hclust</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.791</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">8.241</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">1 layer intCC-k-means (PAM)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.000879</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00330</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">1 layer intCC-Hclust (PAM)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.000882</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00335</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">1 layer intCC-k-means (Hclust)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.000876</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00333</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">1 layer intCC-Hclust (Hclust)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.000909</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00332</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">2 layer intCC-2 methods (PAM)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00273</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.0103</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">2 layer intCC-2 methods (Hclust)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00265</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.0102</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">KLIC-2-methods</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.428</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">27.592</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">1 layer intCC-2 methods (PAM)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00155</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00673</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">1 layer intCC-2 methods (Hclust)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00152</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00686</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD with MathML3 v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1-mathml3.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?properties manuscript?>
<?origin hhspa?>
<?iso-abbr Pac Symp Biocomput?>
<?submitter-userid 8068881?>
<?submitter-authority myNCBI?>
<?submitter-login worldscience?>
<?submitter-name World Scientific Publishing?>
<?domain hhspa?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">9711271</journal-id>
    <journal-id journal-id-type="pubmed-jr-id">20660</journal-id>
    <journal-id journal-id-type="nlm-ta">Pac Symp Biocomput</journal-id>
    <journal-id journal-id-type="iso-abbrev">Pac Symp Biocomput</journal-id>
    <journal-title-group>
      <journal-title>Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">2335-6928</issn>
    <issn pub-type="epub">2335-6936</issn>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10764072</article-id>
    <article-id pub-id-type="pmid">38160311</article-id>
    <article-id pub-id-type="manuscript">hhspa1952198</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>intCC: An efficient weighted integrative consensus clustering of multimodal data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Huang</surname>
          <given-names>Can</given-names>
        </name>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kuan</surname>
          <given-names>Pei Fen</given-names>
        </name>
        <xref rid="CR1" ref-type="corresp">*</xref>
      </contrib>
      <aff id="A1">Department of Applied Mathematics and Statistics, Stony Brook University, Stony Brook, NY 11794, USA</aff>
    </contrib-group>
    <author-notes>
      <corresp id="CR1"><label>*</label><email>peifen.kuan@stonybrook.edu</email>, <ext-link xlink:href="http://www.ams.sunysb.edu" ext-link-type="uri">www.ams.sunysb.edu</ext-link></corresp>
    </author-notes>
    <pub-date pub-type="nihms-submitted">
      <day>23</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>03</day>
      <month>1</month>
      <year>2024</year>
    </pub-date>
    <volume>29</volume>
    <fpage>627</fpage>
    <lpage>640</lpage>
    <permissions>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>Open Access chapter published by World Scientific Publishing Company and distributed under the terms of the Creative Commons Attribution Non-Commercial (CC BY-NC) 4.0 License.</license-p>
      </license>
    </permissions>
    <abstract id="ABS1">
      <p id="P1">High throughput profiling of multiomics data provides a valuable resource to better understand the complex human disease such as cancer and to potentially uncover new subtypes. Integrative clustering has emerged as a powerful unsupervised learning framework for subtype discovery. In this paper, we propose an efficient weighted integrative clustering called intCC by combining ensemble method, consensus clustering and kernel learning integrative clustering. We illustrate that intCC can accurately uncover the latent cluster structures via extensive simulation studies and a case study on the TCGA pan cancer datasets. An R package intCC implementing our proposed method is available at <ext-link xlink:href="https://github.com/candsj/intCC" ext-link-type="uri">https://github.com/candsj/intCC</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>Integrative clustering</kwd>
      <kwd>Consensus clustering</kwd>
      <kwd>Multiomics data</kwd>
      <kwd>Ensemble learning</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <label>1.</label>
    <title>Introduction</title>
    <p id="P2">Recent advancements in high throughput technologies have enabled rapid profiling of different omics data, including genomics, epigenomics, transcriptomics, proteomics and metabolomics which allow for in-depth study of the complex regulatory patterns from a systems biology perspective. For example, the Cancer Genome Atlas (TCGA) has generated over 2.5 petabytes of multiomics data. Such datasets offer the opportunity to explore the heterogeneity underpinning diseases such as cancer via unsupervised learning based on clustering framework, which could help define cancer subtypes, bringing us a step closer towards personalized medicine.</p>
    <p id="P3">In multimodal data structure, e.g., the different omics data, a key challenge in data analysis is in identifying the most appropriate approach for data integration. For unsupervised clustering over multimodal data, these include the choice of a single step <italic toggle="yes">versus</italic> two-step approach. A single step approach is also known as joint modeling which combines all datasets together. Two-step approach works by clustering each dataset separately, followed by integration of these clusters.</p>
    <p id="P4">A number of integrative clustering methods and tools have been proposed to date. This includes Bayesian Consensus Clustering (BCC<sup><xref rid="R1" ref-type="bibr">1</xref></sup>), iCluster,<sup><xref rid="R2" ref-type="bibr">2</xref></sup> iClusterPlus,<sup><xref rid="R3" ref-type="bibr">3</xref></sup> Cluster Of Clusters Analysis (COCA<sup><xref rid="R4" ref-type="bibr">4</xref></sup>), Clusternomics<sup><xref rid="R5" ref-type="bibr">5</xref></sup> and kernel learning integrative clustering (KLIC<sup><xref rid="R6" ref-type="bibr">6</xref></sup>). BCC, Clusternomics and iClusterPlus are based on Bayesian modeling framework and rely on Markov Chain Monte Carlo (MCMC) algorithm for fitting the model. These methods also assume that the probability model for each dataset is specified. However, softwares for BCC and Clusternomics currently only implement the algorithms for Gaussian distributed dataset, thus limiting the applicability of these methods to non-Gaussian datasets such as SNPs, mutation or copy number datasets.</p>
    <p id="P5">On the other hand, iCluster works by assuming a Gaussian latent variable model for inferring the cluster structures, whereas iClusterPlus increases the versatility of iCluster by incorporating statistical models for continuous, binary, multinomial count datasets via a Bayesian latent variable model and employs MCMC algorithm for sampling from its posterior distribution for statistical inference. However, software implementation of iClusterPlus currently is limited to integrative clustering of at most four datasets. Since the model involves tuning a number of parameters, the bottleneck is the computational time when the number of datasets or features increases.</p>
    <p id="P6">Another popular integrative clustering approach is COCA<sup><xref rid="R4" ref-type="bibr">4</xref></sup> which was first introduced to define cancer subtypes by clustering six different datasets, namely DNA copy number, DNA methylation, mRNA expression, microRNA expression, protein expression, and somatic point mutation. COCA works by first clustering each dataset using consensus clustering,<sup><xref rid="R7" ref-type="bibr">7</xref></sup> followed by clustering the binary matrix generated by aggregating the clusters obtained from each dataset. While this approach is robust and easily scalable to a large number of datasets, a limitation of COCA is that all datasets contribute equally to the final clustering which affects the accuracy of the clusters obtained, especially in scenario in which certain dataset is less reliable.</p>
    <p id="P7">Taking inspiration from COCA and multiple kernel learning,<sup><xref rid="R8" ref-type="bibr">8</xref>,<xref rid="R9" ref-type="bibr">9</xref></sup> KLIC<sup><xref rid="R6" ref-type="bibr">6</xref></sup> was developed to address the pitfall of COCA. Similar to COCA, KLIC works by first applying consensus clustering to each dataset. The authors proved that these consensus matrices are positive semi-definite kernels, which can then be used as input in multiple kernel <italic toggle="yes">k</italic>-means clustering and allows for weights to be estimated for each kernel via a two-step optimization strategy and convex quadratic programming. This approach allows for more informative dataset to contribute more to the overall clustering. Currently, KLIC runs one clustering algorithm on each dataset to generate the consensus matrix.</p>
    <p id="P8">In this paper, we seek to extend the KLIC framework to a more robust integrative clustering by proposing a two layer weighted integrative clustering which allows for more than one clustering algorithm to be run on each dataset, i.e, ensemble clustering and aggregated together via an efficient weight estimation.</p>
  </sec>
  <sec id="S2">
    <label>2.</label>
    <title>Methods</title>
    <p id="P9">Our proposed method can be viewed as a combination of (a) ensemble clustering, i.e, aggregating multiple clustering algorithms, (b) consensus clustering, i.e., resampling, and (c) kernel learning integrative clustering. While some papers use ensemble and consensus clustering interchangeably, in this paper, we refer to ensemble clustering as a collection of multiple clustering algorithms, e.g., <italic toggle="yes">k</italic>-means, hierarchical clustering or partitioning around medoid (PAM), whereas consensus clustering as a framework which draws a random sample from either the sample or feature space. We now briefly describe the consensus clustering and kernel learning integrative clustering framework.</p>
    <p id="P10">Consensus clustering was originally proposed by Monti et al (2003).<sup><xref rid="R7" ref-type="bibr">7</xref></sup> The main idea behind consensus clustering is to apply a resampling scheme on the sample or feature dimension under the assumption that different subsamples drawn from the dataset should not differ much in the clustering results. The resampling scheme allows one to assess the stability of the cluster assignments and the robustness of the dataset to perturbations, thus could aid in deriving a more stable and reliable result that reveals the real structure underlying the dataset.</p>
    <p id="P11">A key element derived from the consensus clustering is the consensus matrix which measures the agreement among samples. For a dataset with <inline-formula><mml:math id="M2" display="inline"><mml:mi>N</mml:mi></mml:math></inline-formula> samples, the consensus matrix <inline-formula><mml:math id="M3" display="inline"><mml:mi>ℳ</mml:mi></mml:math></inline-formula> is a <inline-formula><mml:math id="M4" display="inline"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> matrix whose element <inline-formula><mml:math id="M5" display="inline"><mml:mrow><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> denotes the proportion of sample <inline-formula><mml:math id="M6" display="inline"><mml:mi>i</mml:mi></mml:math></inline-formula> and sample <inline-formula><mml:math id="M7" display="inline"><mml:mi>j</mml:mi></mml:math></inline-formula> in the same cluster during the resampling iterations. Values which are close to 1 (and vice versa 0) indicate that the two samples are always assigned to the same cluster (and vice versa different clusters). <inline-formula><mml:math id="M8" display="inline"><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ℳ</mml:mi></mml:mrow></mml:math></inline-formula> is a distance measure which can be used to derive a final clustering result.</p>
    <p id="P12">Cabassi and Kirk (2020)<sup><xref rid="R6" ref-type="bibr">6</xref></sup> proved that the consensus matrix is positive semi-definite and thus can be used as input in kernel learning integrative clustering via the application of multiple kernel <italic toggle="yes">k</italic>-means algorithm. The kernel <italic toggle="yes">k</italic>-means algorithm utilizes the kernel trick by projecting the data into a non-linear feature space via a kernel. This overcomes the drawback of regular <italic toggle="yes">k</italic>-means clustering which cannot identify clusters that are not linearly separable in the original input space. The integration of the multimodal data within the kernel learning integrative clustering involves a convex sum of the kernels, i.e., consensus matrix from each dataset, and the estimation of the weights in the convex sum. In the KLIC integrative clustering algorithm of Cabassi and Kirk (2020),<sup><xref rid="R6" ref-type="bibr">6</xref></sup> the authors adopted the optimization strategy proposed by Gonen and Margolin (2014)<sup><xref rid="R10" ref-type="bibr">10</xref></sup> which involves a convex quadratic programming.</p>
    <p id="P13">In this paper, we reason that the weights in the kernel learning integrative clustering can be estimated by utilizing the fuzziness in the consensus matrix. Furthermore, we extend the framework of KLIC by allowing multiple base clustering algorithms, e.g., <italic toggle="yes">k</italic>-means, hierarchical clustering, PAM, to be applied within each dataset and aggregated, i.e., ensemble clustering<sup><xref rid="R11" ref-type="bibr">11</xref></sup> which has been shown to enhance the robustness of clustering results compared to individual clustering algorithm. To this end, we propose an efficient weight estimation method and a two layer weighted integrative consensus clustering.</p>
    <sec id="S3">
      <label>2.1.</label>
      <title>Weight estimation</title>
      <p id="P14">The consensus matrix can be used to assess cluster stability and composition. As a motivating example, we generate two datasets, each with 10 features and 100 samples. For both datasets, we assume that there are 3 clusters with cluster sizes 20, 30 and 50. All the features are generated from the Gaussian distribution. For dataset 1, 9 out of the 10 features are informative, where the means of cluster 1, 2 and 3 are 1, −1 and 0, respectively with unit variance. For dataset 2, 3 out of the 10 features are informative, where the means of cluster 1, 2 and 3 are 0.2, −0.2 and 0, respectively with unit variance. Non-informative features are generated from standard Gaussian distribution. We designate datasets 1 and 2 as having high and and low signal-to-noise-ratio (SNR), respectively and run consensus clustering on both datasets using 100 iterations of <italic toggle="yes">k</italic>-means and resampling 80% of samples and features in each iteration. <xref rid="F1" ref-type="fig">Figure 1</xref> shows the heatmaps of the consensus matrices. The diagonal blocks plot the in-cluster values, whereas the off diagonals blocks plot the out-of-cluster values. For the low SNR dataset, the off diagonal blocks are much noisier compared to the high SNR dataset. We argue that this can be used to derive the weights in the multiple kernel integrative clustering. Specifically, we define the weights based on the ratio of in-cluster proportion to out-of-cluster proportion using the cluster estimated by the algorithm itself. Clustering result closer to the real structure tends to have higher in-cluster proportion and lower out-of-cluster proportion. In other words, datasets with a higher ratio of in-cluster proportion to out-of-cluster proportion will be assigned larger weights.</p>
      <p id="P15">Without loss of generality, we consider <inline-formula><mml:math id="M9" display="inline"><mml:mi>𝒫</mml:mi></mml:math></inline-formula> consensus matrices <inline-formula><mml:math id="M10" display="inline"><mml:mrow><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> for number of clusters <italic toggle="yes">K</italic>. Here, the consensus matrices could arise by applying different clustering algorithms to the same dataset or could denote consensus matrices derived from different datasets. We further define:</p>
      <list list-type="simple" id="L2">
        <list-item>
          <p id="P16"><inline-formula><mml:math id="M11" display="inline"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>: in-cluster proportion for cluster <inline-formula><mml:math id="M12" display="inline"><mml:mi>k</mml:mi></mml:math></inline-formula> of consensus matrix <inline-formula><mml:math id="M13" display="inline"><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P17"><inline-formula><mml:math id="M14" display="inline"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>: out-of-cluster proportion for cluster <inline-formula><mml:math id="M15" display="inline"><mml:mi>k</mml:mi></mml:math></inline-formula> of consensus matrix <inline-formula><mml:math id="M16" display="inline"><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P18"><inline-formula><mml:math id="M17" display="inline"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>: average in-cluster proportion across all clusters of consensus matrix <inline-formula><mml:math id="M18" display="inline"><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P19"><inline-formula><mml:math id="M19" display="inline"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>: average out-of-cluster proportion across all clusters of consensus matrix <inline-formula><mml:math id="M20" display="inline"><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P20"><inline-formula><mml:math id="M21" display="inline"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>: ratio of in-cluster proportion to out-of-cluster proportion for consensus matrix <inline-formula><mml:math id="M22" display="inline"><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P21"><inline-formula><mml:math id="M23" display="inline"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>: weight for consensus matrix <inline-formula><mml:math id="M24" display="inline"><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
      </list>
      <p id="P22">We propose calculating the weights as follows:
<disp-formula id="FD1"><mml:math id="M1" display="block"><mml:mspace width="0.4em"/><mml:mrow><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∑</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mspace width="thinmathspace"/><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>K</mml:mi></mml:mfrac><mml:mspace linebreak="newline"/><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>∉</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∑</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>∉</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mspace width="thinmathspace"/><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>K</mml:mi></mml:mfrac><mml:mspace linebreak="newline"/><mml:mspace width="2.1em"/><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow></mml:mfrac><mml:mspace linebreak="newline"/><mml:mspace width="1.9em"/><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mi>p</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
      <p id="P23">In practice, true cluster membership is unknown, thus the weights will be computed based on predicted cluster membership. Using this formula, <italic toggle="yes">W</italic><sub>1</sub> = 0.726 and <italic toggle="yes">W</italic><sub>2</sub> = 0.274 for the consensus matrices derived based on predicted cluster membership of the two datasets above.</p>
    </sec>
    <sec id="S4">
      <label>2.2.</label>
      <title>Two Layer Weighted Integrative Consensus Clustering</title>
      <p id="P24">We now describe our proposed two layer weighted integrative consensus clustering. We assume that there are <inline-formula><mml:math id="M25" display="inline"><mml:mi>D</mml:mi></mml:math></inline-formula> datasets, <inline-formula><mml:math id="M26" display="inline"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and number of clusters <italic toggle="yes">K</italic>.</p>
      <p id="P25">Layer 1: For each dataset <inline-formula><mml:math id="M27" display="inline"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="M28" display="inline"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:math></inline-formula>:</p>
      <list list-type="order" id="L4">
        <list-item>
          <p id="P26">Perform ensemble clustering using <italic toggle="yes">P</italic> different clustering methods, where <inline-formula><mml:math id="M29" display="inline"><mml:mrow><mml:mi>p</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula>. This will generate consensus matrices <inline-formula><mml:math id="M30" display="inline"><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M31" display="inline"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P27">Compute the weights <inline-formula><mml:math id="M32" display="inline"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mn>1</mml:mn><mml:mi>d</mml:mi></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mn>2</mml:mn><mml:mi>d</mml:mi></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>P</mml:mi><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> for each consensus matrix <inline-formula><mml:math id="M33" display="inline"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mn>1</mml:mn><mml:mi>d</mml:mi></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mn>2</mml:mn><mml:mi>d</mml:mi></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mi>P</mml:mi><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P28">Define the weighted consensus matrix <inline-formula><mml:math id="M34" display="inline"><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> as <inline-formula><mml:math id="M35" display="inline"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:msubsup></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P29">Apply a clustering algorithm, e.g., PAM or hierarchical clustering, to each weighted consensus matrix <inline-formula><mml:math id="M36" display="inline"><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
      </list>
      <p id="P30">Layer 2:</p>
      <list list-type="order" id="L6">
        <list-item>
          <p id="P31">For the weighted consensus matrix <inline-formula><mml:math id="M37" display="inline"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, compute the weights <inline-formula><mml:math id="M38" display="inline"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P32">Define the weighted of weighted consensus matrix <inline-formula><mml:math id="M39" display="inline"><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> as <inline-formula><mml:math id="M40" display="inline"><mml:mrow><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P33">Apply a clustering algorithm, e.g., PAM or hierarchical clustering, to <inline-formula><mml:math id="M41" display="inline"><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> to derive a final clustering result.</p>
        </list-item>
      </list>
      <p id="P34">We provide a flowchart in <xref rid="F2" ref-type="fig">Figure 2</xref> summarizing our proposed two layer weighted integrative consensus clustering. Our method is implemented as a GitHub R package intCC available at <ext-link xlink:href="https://github.com/candsj/intCC" ext-link-type="uri">https://github.com/candsj/intCC</ext-link>.</p>
    </sec>
  </sec>
  <sec id="S5">
    <label>3.</label>
    <title>Simulation studies</title>
    <p id="P35">We conduct simulation studies to compare the performance of our proposed two layer weighted integrative consensus clustering intCC against other integrative clustering methods which are implemented for both Gaussian and non-Gaussian distributed datasets, namely KLIC<sup><xref rid="R6" ref-type="bibr">6</xref></sup> and iClusterPlus.<sup><xref rid="R3" ref-type="bibr">3</xref></sup></p>
    <sec id="S6">
      <label>3.1.</label>
      <title>Datasets</title>
      <p id="P36">Unlike Cabassi and Kirk (2020)<sup><xref rid="R6" ref-type="bibr">6</xref></sup> which only considered data simulated from Gaussian distributions, we follow the strategy of Mo et al. (2013)<sup><xref rid="R3" ref-type="bibr">3</xref></sup> where we generate datasets from different distributions, including Gaussian (e.g., M-values from DNA methylation, microarray data such as gene expression), binomial (e.g., somatic mutations), Poisson (e.g., count data from sequencing technologies such as RNA-Seq data or copy number data represented as number of copies gained or lost) and multinomial (e.g., copy number data states represented as gain, normal or loss, or SNP data) distributions. This is to ensure that our proposed method is applicable to integration of continuous, binary, count and categorical types of datasets. For Settings 1-6, we set the sample size and the true number of clusters to be 60 and 3, respectively in which each cluster consists of 20 samples. We vary the number of informative and non-informative, i.e., noise features. The parameters used in our simulations for Settings 1-6 are provided in <xref rid="SD1" ref-type="supplementary-material">Supplementary Table 1</xref>. Settings 7-9 follow from the simulation setup of of Cabassi and Kirk (2020).<sup><xref rid="R6" ref-type="bibr">6</xref></sup> We consider several simulation settings, namely:</p>
      <list list-type="order" id="L8">
        <list-item>
          <p id="P37">Setting 1: 4 datasets following Gaussian, binomial, Poisson and multinomial distribution, respectively. Each dataset has 30 features, in which 15 features are informative and the rest are noise features.</p>
        </list-item>
        <list-item>
          <p id="P38">Setting 2: 4 datasets includes normal, binomial, Poisson and multinomial distribution, respectively. Each dataset has 30 features, in which 15 features are informative and the rest are noise features. Informative features have slighly lower signal compared to the Setting 1.</p>
        </list-item>
        <list-item>
          <p id="P39">Setting 3: 3 datasets following Gaussian, binomial and Poisson distribution, respectively. Each dataset has 30 features, in which 15 features are informative and the rest are noise features.</p>
        </list-item>
        <list-item>
          <p id="P40">Setting 4: 5 datasets following Gaussian, binomial, Poisson, multinomial and Gaussian distribution, respectively. Each dataset has 30 features. For the first 4 datasets, 15 features are informative and the rest are noise features. The 5th dataset follows a Gaussian distribution in which all features are noise features.</p>
        </list-item>
        <list-item>
          <p id="P41">Setting 5: 4 datasets following Gaussian, binomial, Poisson and multinomial distribution, respectively. Each dataset has 500 features, in which 100 features are informative and the rest are noise features.</p>
        </list-item>
        <list-item>
          <p id="P42">Setting 6: 4 datasets following Gaussian, binomial, Poisson and multinomial distribution, respectively. Each dataset has 500 features, in which 250 features are informative and the rest are noise features.</p>
        </list-item>
        <list-item>
          <p id="P43">Setting 7: 4 datasets following Gaussian distribution with similar parameter setting. Each dataset consists of 300 samples with 6 clusters of size 50 samples each. There are 2 features with no noise feature. For cluster <italic toggle="yes">k</italic>, <italic toggle="yes">μ</italic> = <italic toggle="yes">k</italic> × (separation level − 1)/2, <italic toggle="yes">σ</italic> = 1, <italic toggle="yes">k</italic> = 1, 2, 3, 4, 5, 6. Separation level = 4 is used in this setting.</p>
        </list-item>
        <list-item>
          <p id="P44">Setting 8: 4 datasets following Gaussian distribution with different parameter setting. Each dataset consists of 300 samples with 6 clusters of size 50 samples each. There are 2 features with no noise feature. For cluster <italic toggle="yes">k</italic>, <italic toggle="yes">μ</italic> = <italic toggle="yes">k</italic> × (separation level − 1)/2, <italic toggle="yes">σ</italic> = 1, <italic toggle="yes">k</italic> = 1, 2, 3, 4, 5, 6. Varying separation levels = 1, 2, 3, 4 are used in this setting. Only 3 datasets are used as input. We consider 4 dataset combinations, namely 123, 124, 134, 234. Here 123 implies that the clustering algorithms are applied to only datasets 1, 2 and 3.</p>
        </list-item>
        <list-item>
          <p id="P45">Setting 9 (nested cluster structure): 2 datasets following Gaussian distribution, in which each dataset consists of 300 samples. There are 2 features with no noise feature. Dataset 1 has 6 clusters of size 50 samples each. Dataset 2 has 3 clusters of size 100 samples each. For cluster <italic toggle="yes">k</italic>, <italic toggle="yes">μ</italic> = <italic toggle="yes">k</italic> × (separation level − 1)/2, <italic toggle="yes">σ</italic> = 1, <italic toggle="yes">k</italic> = 1, 2, 3, 4, 5, 6 for dataset 1 and <italic toggle="yes">k</italic> = 1, 2, 3 for dataset 2. Separation level = 4 is used in this setting.</p>
        </list-item>
      </list>
      <p id="P46">Each setting is repeated 100 times. Additional simulation settings including multivariate Gaussian distribution are provided in <xref rid="SD1" ref-type="supplementary-material">Supplementary Material</xref>.</p>
    </sec>
    <sec id="S7">
      <label>3.2.</label>
      <title>Clustering algorithms</title>
      <p id="P47">We apply several clustering strategies based on our proposed method intCC, KLIC<sup><xref rid="R6" ref-type="bibr">6</xref></sup> and iClusterPlus.<sup><xref rid="R3" ref-type="bibr">3</xref></sup> To evaluate the advantage of ensemble clustering, i.e., applying multiple clustering algorithms to each dataset, we also include our proposed method which only runs a single clustering algorithm to each dataset. We denote this as one layer weighted integrative consensus clustering. We also compare application of PAM and hierarchical clustering to the weighted consensus matrix in deriving a final clustering result. These methods are denoted as:</p>
      <list list-type="order" id="L10">
        <list-item>
          <p id="P48">iClusterPlus: applying iClusterPlus with the data type specified.</p>
        </list-item>
        <list-item>
          <p id="P49">KLIC-k-means: KLIC by applying <italic toggle="yes">k</italic>-means to each dataset for generating the consensus matrix.</p>
        </list-item>
        <list-item>
          <p id="P50">KLIC-Hclust: KLIC by applying hierarchical clustering to each dataset for generating the consensus matrix.</p>
        </list-item>
        <list-item>
          <p id="P51">1 layer intCC-k-means (PAM): One layer weighted integrative consensus clustering by applying <italic toggle="yes">k</italic>-means to each dataset for generating the consensus matrix, followed by PAM to derive a final clustering result.</p>
        </list-item>
        <list-item>
          <p id="P52">1 layer intCC-Hclust (PAM): One layer weighted integrative consensus clustering by applying hierarchical clustering to each dataset for generating the consensus matrix, followed by PAM to derive a final clustering result.</p>
        </list-item>
        <list-item>
          <p id="P53">1 layer intCC-k-means (Hclust): One layer weighted integrative consensus clustering by applying <italic toggle="yes">k</italic>-means to each dataset for generating the consensus matrix, followed by hierarchical clustering to derive a final clustering result.</p>
        </list-item>
        <list-item>
          <p id="P54">1 layer intCC-Hclust (Hclust): One layer weighted integrative consensus clustering by applying hierarchical clustering to each dataset for generating the consensus matrix, followed by hierarchical clustering to derive a final clustering result.</p>
        </list-item>
      </list>
      <p id="P55">To obtain an unbiased comparison to our two layer approach, we also apply KLIC with multiple clustering algorithms. In other words, suppose there are 4 datasets and two clustering algorithms are applied to each dataset, there will be a total of 8 consensus matrices, i.e., akin to applying KLIC to 8 datasets. KLIC is applied using these 8 consensus matrices as input in the multiple kernel integrative clustering. Additionally, to illustrate the advantage of two layer approach, we also include another one layer approach in which we apply a single layer weight estimation to the 8 consensus matrices. These methods are denoted as:</p>
      <list list-type="simple" id="L12">
        <list-item>
          <label>(8)</label>
          <p id="P56">2 layer intCC-2 methods (PAM): Two layer weighted integrative consensus clustering by applying both <italic toggle="yes">k</italic>-means and hierarchical clustering to each dataset for generating the consensus matrices, followed by PAM to derive a final clustering result.</p>
        </list-item>
        <list-item>
          <label>(9)</label>
          <p id="P57">2 layer intCC-2 methods (Hclust): Two layer weighted integrative consensus clustering by applying both <italic toggle="yes">k</italic>-means and hierarchical clustering to each dataset for generating the consensus matrices, followed by hierarchical clustering to derive a final clustering result.</p>
        </list-item>
        <list-item>
          <label>(10)</label>
          <p id="P58">KLIC-2-methods: KLIC by applying both <italic toggle="yes">k</italic>-means and hierarchical clustering to each dataset for generating the consensus matrices.</p>
        </list-item>
        <list-item>
          <label>(11)</label>
          <p id="P59">1 layer intCC-2 methods (PAM): One layer weighted integrative consensus clustering by applying both <italic toggle="yes">k</italic>-means and hierarchical clustering to each dataset for generating the consensus matrices, followed by PAM to derive a final clustering result.</p>
        </list-item>
        <list-item>
          <label>(12)</label>
          <p id="P60">1 layer intCC-2 methods (Hclust): One layer weighted integrative consensus clustering by applying both <italic toggle="yes">k</italic>-means and hierarchical clustering to each dataset for generating the consensus matrices, followed by hierarchical clustering to derive a final clustering result.</p>
        </list-item>
      </list>
      <p id="P61">For Settings 1-8, we apply each method by setting the number of clusters to be the true number of clusters. In practice, one can tune the optimal number of clusters using criteria such as the silhouette method,<sup><xref rid="R12" ref-type="bibr">12</xref></sup> gap statistics,<sup><xref rid="R13" ref-type="bibr">13</xref></sup> Dunn index<sup><xref rid="R14" ref-type="bibr">14</xref></sup> or the delta <italic toggle="yes">K</italic> method.<sup><xref rid="R7" ref-type="bibr">7</xref></sup> For Setting 9, we consider (a) global clustering, where we set the number of clusters to be the same throughout for both individual dataset and final integrative clustering, i.e., either 3 or 6 throughout (we denote these strategies as “Global K=3” and “Global K=6”), and (b) separate clustering, where we use the true number of clusters for individual dataset, i.e., 6 for dataset 1 and 3 for dataset 2, and consider both <italic toggle="yes">K</italic> = 3 and <italic toggle="yes">K</italic> = 6 in the final integrative clustering (we denote these strategies as “Separate K=3” and “Separate K=6”). Additionally, due to the poor performance of iClusterPlus and the long computational time, we omit iClusterPlus for Settings 4-6. We compare the performance of the clustering methods via the average adjusted rand index (ARI). We also report the weight estimation time of intCC and KLIC.</p>
    </sec>
    <sec id="S8">
      <label>3.3.</label>
      <title>Results</title>
      <p id="P62">We summarize the ARI for each simulation setting in <xref rid="F3" ref-type="fig">Figure 3</xref>. Overall, results show that our proposed methods, namely 2 layer intCC-2 methods (PAM) and 1 layer intCC-k-means (PAM) perform well across all simulation settings. To explain this observation, without loss of generality, we summarize the ARI within each simulated dataset of Setting 4 in <xref rid="F4" ref-type="fig">Figures 4A</xref> and <xref rid="F4" ref-type="fig">4B</xref>. The ARI by applying <italic toggle="yes">k</italic>-means as the base algorithm in the consensus clustering within each dataset is significantly better than hierarchical clustering in the simulated datasets considered in this paper. Thus, it is not surprising that methods which use <italic toggle="yes">k</italic>-means as the base clustering algorithm in the consensus clustering yield better performance. However, in practice the best base clustering algorithm is sometimes unknown. Thus, the 2 layer intCC which aggregates multiple base clustering algorithms can automatically assign higher weights to the better algorithm as shown in our simulation studies, as evident from the estimated weights in <xref rid="F4" ref-type="fig">Figures 4C</xref> and <xref rid="F4" ref-type="fig">4D</xref>. It is also worth noting that our method assigns significantly smaller weights to the 5th dataset in which all the features are noise features. Additionally, using PAM to derive a final clustering result in general yields better performance compared to hierarchical clustering. We also note that the performance of iClusterPlus is significantly poorer compared to other methods, consistent with the findings of Cabassi and Kirk (2020).<sup><xref rid="R6" ref-type="bibr">6</xref></sup> Moreover, extending KLIC to run multiple base clustering algorithms, i.e., KLIC-2-methods has lower ARI compared to our proposed method, implying that the current KLIC framework does not yield a straightforward extension to incorporate ensemble clustering.</p>
      <p id="P63">Without loss of generality, we also report the weight calculation time for KLIC and our proposed method intCC for Setting 1 (60 samples) and Setting 7 (300 samples) in <xref rid="T1" ref-type="table">Table 1</xref>, which shows that our proposed weight calculation is computationally efficient and yields good operating characteristics.</p>
    </sec>
  </sec>
  <sec id="S9">
    <label>4.</label>
    <title>Case study</title>
    <p id="P64">We illustrate our proposed method intCC on the TCGA pan cancer datasets.<sup><xref rid="R15" ref-type="bibr">15</xref></sup> There are 5 datasets across 12 cancer types which represent different tissues of origin, including DNA copy number, DNA methylation, mRNA expression, microRNA expression and protein expression data. To minimize bias in the comparison, we use the same preprocessing pipeline as previously described.<sup><xref rid="R6" ref-type="bibr">6</xref>,<xref rid="R15" ref-type="bibr">15</xref></sup></p>
    <p id="P65">Cabassi and Kirk (2020)<sup><xref rid="R6" ref-type="bibr">6</xref></sup> followed the same procedures described in Hoadley et al. (2014)<sup><xref rid="R15" ref-type="bibr">15</xref></sup> in setting the number of clusters for each dataset, except for microRNA expression in which the authors identified 8 as the number of clusters. We also set the number of clusters for each dataset following Cabassi and Kirk (2020).<sup><xref rid="R6" ref-type="bibr">6</xref></sup> Subsequently, we apply our proposed method intCC to obtain an integrative clustering across these datasets using the PAM algorithm to derive a final clustering result. Our method also selects 10 as the optimal number of clusters based on the average silhouette criterion, similar to KLIC.<sup><xref rid="R6" ref-type="bibr">6</xref></sup>
<xref rid="F5" ref-type="fig">Figure 5A</xref> compares the cluster membership of our method intCC against the results of KLIC, with ARI 0.693, whereas <xref rid="F5" ref-type="fig">Figures 5B</xref> and <xref rid="F5" ref-type="fig">5C</xref> compare the cluster membership of intCC and KLIC against the 12 cancer type annotation, respectively. The ARI between intCC and cancer type annotation associated with tissues of origin is 0.754, whereas the ARI between KLIC and cancer type annotation is 585, indicating that the cluster membership of intCC yields a higher consistency with tissues of origin in the TCGA pan cancer datasets. Further investigation into the clusters obtained by intCC versus KLIC among subset of breast invasive carcinoma (BRCA) indicates that the results from intCC yield a higher consistency with the TCGA-BRCA molecular subtypes compared to the results from KLIC (<xref rid="SD1" ref-type="supplementary-material">Supplementary Material</xref>).</p>
    <p id="P66">The estimated weights of each dataset for intCC and KLIC are (DNA copy number, DNA methylation, mRNA expression, miRNA expression, protein expression) =(0.073, 0.401,0.045, 0.272, 0.209) and (0.309, 0.192, 0.168, 0.183, 0.148), respectively. intCC assigns a higher weight to DNA methylation data, whereas KLIC assigns a higher weight to the copy number data, which could explain the differences observed in cluster memberships obtained by these two methods. Finally, the weight calculation time for intCC is 0.43 second, whereas the weight calculation time for KLIC via quadratic programming is &gt; 10 hours on an Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz.</p>
  </sec>
  <sec id="S10">
    <label>5.</label>
    <title>Discussion</title>
    <p id="P67">The rapid development of high throughput technologies has provided an avenue to scientists to decipher the complex human diseases from a systems biology perspective via multiomics profiling. Integrative clustering has become a powerful approach to dissect the heterogeneity underpinning these diseases, e.g., to define new cancer subtypes which may help inform treatment efforts. In this paper, we extend the framework of KLIC<sup><xref rid="R6" ref-type="bibr">6</xref></sup> which recasts the integrative clustering model into multiple kernel learning framework by utilizing the consensus matrices estimated from consensus clustering as input. Specifically, our model further incorporates the ensemble learning via an aggregation of multiple base clustering algorithms to enhance the robustness of multiple kernel integrative clustering model. This is to safeguard against applying a single base clustering algorithm that performs poorly on the dataset. Additionally, we also propose an efficient weight estimation to combine the consensus matrices. Our simulation studies show that the proposed two layer weighted integrative clustering yields better performance overall.</p>
    <p id="P68">Conceptually, the weight estimation is analogous to the heuristics of multiple kernel support vector machine (MKL-SVM) based on kernel-target alignment.<sup><xref rid="R16" ref-type="bibr">16</xref>-<xref rid="R18" ref-type="bibr">18</xref></sup> Specifically, MKL-SVM is developed for supervised learning and the kernel-target alignment depends on the true binary class labels. For a fixed cluster membership, this is equivalent to multi-class classification. One can extend the kernel-target alignment for multi-class classification by dividing the problem into several binary classification subproblems (e.g., one-versus-all or all-pairs). However, how to optimally combine the results across these binary subproblems is not trivial and may require longer computational time compared to our proposed method.</p>
    <p id="P69">Besides identifying appropriate and robust clustering algorithms, another important research question in unsupervised learning is in tuning the optimal number of clusters. Several metrics have been proposed for this task, including the silhouette method,<sup><xref rid="R12" ref-type="bibr">12</xref></sup> gap statistics,<sup><xref rid="R13" ref-type="bibr">13</xref></sup> Dunn index<sup><xref rid="R14" ref-type="bibr">14</xref></sup> and the delta <italic toggle="yes">K</italic> method.<sup><xref rid="R7" ref-type="bibr">7</xref></sup> An immediate extension to our intCC framework is to aggregate the different metrics/criteria for selecting the optimal number of clusters.</p>
  </sec>
  <sec sec-type="supplementary-material" id="SM1">
    <title>Supplementary Material</title>
    <supplementary-material id="SD1" position="float" content-type="local-data">
      <label>suppl-1952198</label>
      <media xlink:href="NIHMS1952198-supplement-suppl-1952198.pdf" id="d64e1721" position="anchor"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="S11">
    <title>Acknowledgments</title>
    <p id="P70">This work is supported in part by CDC/NIOSH award U01OH012257. The findings and conclusions presented in this article are those of the authors and do not represent the official position of NIOSH, the CDC or the U.S. Public Health Service.</p>
  </ack>
  <fn-group>
    <fn fn-type="COI-statement" id="FN1">
      <p id="P71"><italic toggle="yes">Conflict of Interest</italic>: None declared.</p>
    </fn>
    <fn id="FN2">
      <p id="P72">Supplementary Material and Code</p>
      <p id="P73">Supplementary Material is available online at <ext-link xlink:href="http://www.ams.sunysb.edu/~pfkuan/PDF/SM_PSB2024.pdf" ext-link-type="uri">http://www.ams.sunysb.edu/~pfkuan/PDF/SM_PSB2024.pdf</ext-link>.</p>
      <p id="P74">The R code implementing intCC is available online at <ext-link xlink:href="https://github.com/candsj/intCC" ext-link-type="uri">https://github.com/candsj/intCC</ext-link>.</p>
    </fn>
  </fn-group>
  <ref-list>
    <title>References</title>
    <ref id="R1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><name><surname>Lock</surname><given-names>EF</given-names></name> and <name><surname>Dunson</surname><given-names>DB</given-names></name>, <article-title>Bayesian consensus clustering</article-title>, <source>Bioinformatics</source>
<volume>29</volume>, <fpage>2610</fpage> (<year>2013</year>).<pub-id pub-id-type="pmid">23990412</pub-id>
</mixed-citation>
    </ref>
    <ref id="R2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><name><surname>Shen</surname><given-names>R</given-names></name>, <name><surname>Olshen</surname><given-names>AB</given-names></name> and <name><surname>Ladanyi</surname><given-names>M</given-names></name>, <article-title>Integrative clustering of multiple genomic data types using a joint latent variable model with application to breast and lung cancer subtype analysis</article-title>, <source>Bioinformatics</source>
<volume>25</volume>, <fpage>2906</fpage> (<year>2009</year>).<pub-id pub-id-type="pmid">19759197</pub-id>
</mixed-citation>
    </ref>
    <ref id="R3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><name><surname>Mo</surname><given-names>Q</given-names></name>, <name><surname>Wang</surname><given-names>S</given-names></name>, <name><surname>Seshan</surname><given-names>VE</given-names></name>, <name><surname>Olshen</surname><given-names>AB</given-names></name>, <name><surname>Schultz</surname><given-names>N</given-names></name>, <name><surname>Sander</surname><given-names>C</given-names></name>, <name><surname>Powers</surname><given-names>RS</given-names></name>, <name><surname>Ladanyi</surname><given-names>M</given-names></name> and <name><surname>Shen</surname><given-names>R</given-names></name>, <article-title>Pattern discovery and cancer gene identification in integrated cancer genomic data</article-title>, <source>Proceedings of the National Academy of Sciences</source>
<volume>110</volume>, <fpage>4245</fpage> (<year>2013</year>).</mixed-citation>
    </ref>
    <ref id="R4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><collab>B. . W. H. . H. M. S.</collab><name><surname>C.</surname><given-names>L</given-names></name> . . <name><surname>P.</surname><given-names>PJ</given-names></name> . <name><surname>K.</surname><given-names>R</given-names></name> 13, <collab>G. data analysis: Baylor College of Medicine</collab>
<name><surname>Creighton</surname><given-names>Chad J.</given-names></name> 22 23 <name><surname>Donehower</surname><given-names>Lawrence A.</given-names></name> 22 23 24 25, <collab>I. for Systems Biology</collab>
<name><surname>Reynolds</surname><given-names>Sheila</given-names></name> 31 <name><surname>Kreisberg</surname><given-names>Richard B.</given-names></name> 31 <name><surname>Bernard</surname><given-names>Brady</given-names></name> 31 <name><surname>Bressler</surname><given-names>Ryan</given-names></name> 31 <name><surname>Erkkila</surname><given-names>Timo</given-names></name> 32 <name><surname>Lin</surname><given-names>Jake</given-names></name> 31 <name><surname>Thorsson</surname><given-names>Vesteinn</given-names></name> 31 <name><surname>Zhang</surname><given-names>Wei</given-names></name> 33 <name><surname>Shmulevich</surname><given-names>Ilya</given-names></name> 31 <etal/>, <article-title>Comprehensive molecular portraits of human breast tumours</article-title>, <source>Nature</source>
<volume>490</volume>, <fpage>61</fpage> (<year>2012</year>).<pub-id pub-id-type="pmid">23000897</pub-id>
</mixed-citation>
    </ref>
    <ref id="R5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><name><surname>Gabasova</surname><given-names>E</given-names></name>, <name><surname>Reid</surname><given-names>J</given-names></name> and <name><surname>Wernisch</surname><given-names>L</given-names></name>, <article-title>Clusternomics: Integrative context-dependent clustering for heterogeneous datasets</article-title>, <source>PLoS Computational Biology</source>
<volume>13</volume>, p. <fpage>e1005781</fpage> (<year>2017</year>).<pub-id pub-id-type="pmid">29036190</pub-id>
</mixed-citation>
    </ref>
    <ref id="R6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><name><surname>Cabassi</surname><given-names>A</given-names></name> and <name><surname>Kirk</surname><given-names>PD</given-names></name>, <article-title>Multiple kernel learning for integrative consensus clustering of omic datasets</article-title>, <source>Bioinformatics</source>
<volume>36</volume>, <fpage>4789</fpage> (<year>2020</year>).<pub-id pub-id-type="pmid">32592464</pub-id>
</mixed-citation>
    </ref>
    <ref id="R7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><name><surname>Monti</surname><given-names>S</given-names></name>, <name><surname>Tamayo</surname><given-names>P</given-names></name>, <name><surname>Mesirov</surname><given-names>J</given-names></name> and <name><surname>Golub</surname><given-names>T</given-names></name>, <article-title>Consensus clustering: a resampling-based method for class discovery and visualization of gene expression microarray data</article-title>, <source>Machine Learning</source>
<volume>52</volume>, <fpage>91</fpage> (<year>2003</year>).</mixed-citation>
    </ref>
    <ref id="R8">
      <label>8.</label>
      <mixed-citation publication-type="confproc"><name><surname>Bach</surname><given-names>FR</given-names></name>, <name><surname>Lanckriet</surname><given-names>GR</given-names></name> and <name><surname>Jordan</surname><given-names>MI</given-names></name>, <source>Multiple kernel learning, conic duality, and the smo algorithm</source>, in <conf-name>Proceedings of the Twenty-First International Conference on Machine Learning</conf-name>, (<conf-loc>Banff, Canada</conf-loc>, <year>2004</year>).</mixed-citation>
    </ref>
    <ref id="R9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><name><surname>Lanckriet</surname><given-names>GR</given-names></name>, <name><surname>Cristianini</surname><given-names>N</given-names></name>, <name><surname>Bartlett</surname><given-names>P</given-names></name>, <name><surname>Ghaoui</surname><given-names>LE</given-names></name> and <name><surname>Jordan</surname><given-names>MI</given-names></name>, <article-title>Learning the kernel matrix with semidefinite programming</article-title>, <source>Journal of Machine Learning Research</source>
<volume>5</volume>, <fpage>27</fpage> (<year>2004</year>).</mixed-citation>
    </ref>
    <ref id="R10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><name><surname>Gönen</surname><given-names>M</given-names></name> and <name><surname>Margolin</surname><given-names>AA</given-names></name>, <article-title>Localized data fusion for kernel k-means clustering with application to cancer biology</article-title>, <source>Advances in Neural Information Processing Systems</source>
<volume>27</volume> (<year>2014</year>).</mixed-citation>
    </ref>
    <ref id="R11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><name><surname>Sagi</surname><given-names>O</given-names></name> and <name><surname>Rokach</surname><given-names>L</given-names></name>, <article-title>Ensemble learning: A survey</article-title>, <source>Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</source>
<volume>8</volume>, p. <fpage>e1249</fpage> (<year>2018</year>).</mixed-citation>
    </ref>
    <ref id="R12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><name><surname>Rousseeuw</surname><given-names>PJ</given-names></name>, <article-title>Silhouettes: a graphical aid to the interpretation and validation of cluster analysis</article-title>, <source>Journal of Computational and Applied Mathematics</source><volume>20</volume>, <fpage>53</fpage> (<year>1987</year>).</mixed-citation>
    </ref>
    <ref id="R13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><name><surname>Tibshirani</surname><given-names>R</given-names></name>, <name><surname>Walther</surname><given-names>G</given-names></name> and <name><surname>Hastie</surname><given-names>T</given-names></name>, <article-title>Estimating the number of clusters in a data set via the gap statistic</article-title>, <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source>
<volume>63</volume>, <fpage>411</fpage> (<year>2001</year>).</mixed-citation>
    </ref>
    <ref id="R14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><name><surname>Halkidi</surname><given-names>M</given-names></name>, <name><surname>Batistakis</surname><given-names>Y</given-names></name> and <name><surname>Vazirgiannis</surname><given-names>M</given-names></name>, <article-title>On clustering validation techniques</article-title>, <source>Journal of Intelligent Information Systems</source>
<volume>17</volume>, <fpage>107</fpage> (<year>2001</year>).</mixed-citation>
    </ref>
    <ref id="R15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><name><surname>Hoadley</surname><given-names>KA</given-names></name>, <name><surname>Yau</surname><given-names>C</given-names></name>, <name><surname>Wolf</surname><given-names>DM</given-names></name>, <name><surname>Cherniack</surname><given-names>AD</given-names></name>, <name><surname>Tamborero</surname><given-names>D</given-names></name>, <name><surname>Ng</surname><given-names>S</given-names></name>, <name><surname>Leiserson</surname><given-names>MD</given-names></name>, <name><surname>Niu</surname><given-names>B</given-names></name>, <name><surname>McLellan</surname><given-names>MD</given-names></name>, <name><surname>Uzunangelov</surname><given-names>V</given-names></name><etal/>, <article-title>Multiplatform analysis of 12 cancer types reveals molecular classification within and across tissues of origin</article-title>, <source>Cell</source><volume>158</volume>, <fpage>929</fpage> (<year>2014</year>).<pub-id pub-id-type="pmid">25109877</pub-id>
</mixed-citation>
    </ref>
    <ref id="R16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><name><surname>Cristianini</surname><given-names>N</given-names></name>, <name><surname>Shawe-Taylor</surname><given-names>J</given-names></name>, <name><surname>Elisseeff</surname><given-names>A</given-names></name> and <name><surname>Kandola</surname><given-names>J</given-names></name>, <article-title>On kernel-target alignment</article-title>, <source>Advances in neural information processing systems</source>
<volume>14</volume> (<year>2001</year>).</mixed-citation>
    </ref>
    <ref id="R17">
      <label>17.</label>
      <mixed-citation publication-type="confproc"><name><surname>Cortes</surname><given-names>C</given-names></name>, <name><surname>Mohri</surname><given-names>M</given-names></name> and <name><surname>Rostamizadeh</surname><given-names>A</given-names></name>, <source>Two-stage learning kernel algorithms</source>, <conf-name>Proceedings of the 27 th International Conference on Machine Learning</conf-name> , <fpage>239</fpage> (<year>2010</year>).</mixed-citation>
    </ref>
    <ref id="R18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><name><surname>Qiu</surname><given-names>S</given-names></name> and <name><surname>Lane</surname><given-names>T</given-names></name>, <article-title>A framework for multiple kernel support vector regression and its applications to sirna efficacy prediction</article-title>, <source>IEEE/ACM Transactions on Computational Biology and Bioinformatics</source>
<volume>6</volume>, <fpage>190</fpage> (<year>2008</year>).</mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig position="float" id="F1">
    <label>Fig. 1.</label>
    <caption>
      <p id="P75">Heatmaps of consensus matrices for high and low signal-to-noise ratio (SNR) datasets. True cluster membership is given in the annotation above each heatmap. Predicted cluster membership corresponds to the three gap-separated blocks in each heatmap.</p>
    </caption>
    <graphic xlink:href="nihms-1952198-f0001" position="float"/>
  </fig>
  <fig position="float" id="F2">
    <label>Fig. 2.</label>
    <caption>
      <p id="P76">Flowchat describing our proposed algorithm.</p>
    </caption>
    <graphic xlink:href="nihms-1952198-f0002" position="float"/>
  </fig>
  <fig position="float" id="F3">
    <label>Fig. 3.</label>
    <caption>
      <p id="P77">Distribution of ARI across all methods and simulation settings. Blue (red) boxplots are methods which apply one (two) clustering algorithm(s) per dataset. A-G. Settings 1-7. H-K. Setting 8 with different dataset combinations as input. L-O. Setting 9 with different strategies for setting number of clusters for individual dataset and final integrative clustering.</p>
    </caption>
    <graphic xlink:href="nihms-1952198-f0003" position="float"/>
  </fig>
  <fig position="float" id="F4">
    <label>Fig. 4.</label>
    <caption>
      <p id="P78">A-B. Distribution of ARI within each simulated dataset of Setting 4. C-D. Distribution of estimated weights from intCC within each simulated dataset of Setting 4. Purple (green) boxplots are results by applying <italic toggle="yes">k</italic>-means (hierarchical clustering) algorithm in the consensus clustering. A, C. Using PAM to derive a final clustering result. B, D. Using hierarchical clustering to derive a final clustering result.</p>
    </caption>
    <graphic xlink:href="nihms-1952198-f0004" position="float"/>
  </fig>
  <fig position="float" id="F5">
    <label>Fig. 5.</label>
    <caption>
      <p id="P79">Heatmaps of coincidence matrices comparing A. intCC clusters to KLIC clusters, B. intCC clusters to cancer type annotation, C. KLIC clusters to cancer type annotation. The ARI is reported in the header of each plot.</p>
    </caption>
    <graphic xlink:href="nihms-1952198-f0005" position="float"/>
  </fig>
  <table-wrap position="float" id="T1">
    <label>Table 1.</label>
    <caption>
      <p id="P80">Weight calculation time comparison.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Method</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Setting 1 (seconds)</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Setting 7 (seconds)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">KLIC-k-means</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.541</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">7.209</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">KLIC-Hclust</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.791</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">8.241</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">1 layer intCC-k-means (PAM)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.000879</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00330</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">1 layer intCC-Hclust (PAM)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.000882</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00335</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">1 layer intCC-k-means (Hclust)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.000876</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00333</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">1 layer intCC-Hclust (Hclust)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.000909</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00332</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">2 layer intCC-2 methods (PAM)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00273</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.0103</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">2 layer intCC-2 methods (Hclust)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00265</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.0102</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">KLIC-2-methods</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.428</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">27.592</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">1 layer intCC-2 methods (PAM)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00155</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00673</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">1 layer intCC-2 methods (Hclust)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00152</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00686</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD with MathML3 v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1-mathml3.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?properties manuscript?>
<?origin hhspa?>
<?iso-abbr Pac Symp Biocomput?>
<?submitter-userid 8068881?>
<?submitter-authority myNCBI?>
<?submitter-login worldscience?>
<?submitter-name World Scientific Publishing?>
<?domain hhspa?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">9711271</journal-id>
    <journal-id journal-id-type="pubmed-jr-id">20660</journal-id>
    <journal-id journal-id-type="nlm-ta">Pac Symp Biocomput</journal-id>
    <journal-id journal-id-type="iso-abbrev">Pac Symp Biocomput</journal-id>
    <journal-title-group>
      <journal-title>Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">2335-6928</issn>
    <issn pub-type="epub">2335-6936</issn>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10764072</article-id>
    <article-id pub-id-type="pmid">38160311</article-id>
    <article-id pub-id-type="manuscript">hhspa1952198</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>intCC: An efficient weighted integrative consensus clustering of multimodal data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Huang</surname>
          <given-names>Can</given-names>
        </name>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kuan</surname>
          <given-names>Pei Fen</given-names>
        </name>
        <xref rid="CR1" ref-type="corresp">*</xref>
      </contrib>
      <aff id="A1">Department of Applied Mathematics and Statistics, Stony Brook University, Stony Brook, NY 11794, USA</aff>
    </contrib-group>
    <author-notes>
      <corresp id="CR1"><label>*</label><email>peifen.kuan@stonybrook.edu</email>, <ext-link xlink:href="http://www.ams.sunysb.edu" ext-link-type="uri">www.ams.sunysb.edu</ext-link></corresp>
    </author-notes>
    <pub-date pub-type="nihms-submitted">
      <day>23</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>03</day>
      <month>1</month>
      <year>2024</year>
    </pub-date>
    <volume>29</volume>
    <fpage>627</fpage>
    <lpage>640</lpage>
    <permissions>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>Open Access chapter published by World Scientific Publishing Company and distributed under the terms of the Creative Commons Attribution Non-Commercial (CC BY-NC) 4.0 License.</license-p>
      </license>
    </permissions>
    <abstract id="ABS1">
      <p id="P1">High throughput profiling of multiomics data provides a valuable resource to better understand the complex human disease such as cancer and to potentially uncover new subtypes. Integrative clustering has emerged as a powerful unsupervised learning framework for subtype discovery. In this paper, we propose an efficient weighted integrative clustering called intCC by combining ensemble method, consensus clustering and kernel learning integrative clustering. We illustrate that intCC can accurately uncover the latent cluster structures via extensive simulation studies and a case study on the TCGA pan cancer datasets. An R package intCC implementing our proposed method is available at <ext-link xlink:href="https://github.com/candsj/intCC" ext-link-type="uri">https://github.com/candsj/intCC</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>Integrative clustering</kwd>
      <kwd>Consensus clustering</kwd>
      <kwd>Multiomics data</kwd>
      <kwd>Ensemble learning</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <label>1.</label>
    <title>Introduction</title>
    <p id="P2">Recent advancements in high throughput technologies have enabled rapid profiling of different omics data, including genomics, epigenomics, transcriptomics, proteomics and metabolomics which allow for in-depth study of the complex regulatory patterns from a systems biology perspective. For example, the Cancer Genome Atlas (TCGA) has generated over 2.5 petabytes of multiomics data. Such datasets offer the opportunity to explore the heterogeneity underpinning diseases such as cancer via unsupervised learning based on clustering framework, which could help define cancer subtypes, bringing us a step closer towards personalized medicine.</p>
    <p id="P3">In multimodal data structure, e.g., the different omics data, a key challenge in data analysis is in identifying the most appropriate approach for data integration. For unsupervised clustering over multimodal data, these include the choice of a single step <italic toggle="yes">versus</italic> two-step approach. A single step approach is also known as joint modeling which combines all datasets together. Two-step approach works by clustering each dataset separately, followed by integration of these clusters.</p>
    <p id="P4">A number of integrative clustering methods and tools have been proposed to date. This includes Bayesian Consensus Clustering (BCC<sup><xref rid="R1" ref-type="bibr">1</xref></sup>), iCluster,<sup><xref rid="R2" ref-type="bibr">2</xref></sup> iClusterPlus,<sup><xref rid="R3" ref-type="bibr">3</xref></sup> Cluster Of Clusters Analysis (COCA<sup><xref rid="R4" ref-type="bibr">4</xref></sup>), Clusternomics<sup><xref rid="R5" ref-type="bibr">5</xref></sup> and kernel learning integrative clustering (KLIC<sup><xref rid="R6" ref-type="bibr">6</xref></sup>). BCC, Clusternomics and iClusterPlus are based on Bayesian modeling framework and rely on Markov Chain Monte Carlo (MCMC) algorithm for fitting the model. These methods also assume that the probability model for each dataset is specified. However, softwares for BCC and Clusternomics currently only implement the algorithms for Gaussian distributed dataset, thus limiting the applicability of these methods to non-Gaussian datasets such as SNPs, mutation or copy number datasets.</p>
    <p id="P5">On the other hand, iCluster works by assuming a Gaussian latent variable model for inferring the cluster structures, whereas iClusterPlus increases the versatility of iCluster by incorporating statistical models for continuous, binary, multinomial count datasets via a Bayesian latent variable model and employs MCMC algorithm for sampling from its posterior distribution for statistical inference. However, software implementation of iClusterPlus currently is limited to integrative clustering of at most four datasets. Since the model involves tuning a number of parameters, the bottleneck is the computational time when the number of datasets or features increases.</p>
    <p id="P6">Another popular integrative clustering approach is COCA<sup><xref rid="R4" ref-type="bibr">4</xref></sup> which was first introduced to define cancer subtypes by clustering six different datasets, namely DNA copy number, DNA methylation, mRNA expression, microRNA expression, protein expression, and somatic point mutation. COCA works by first clustering each dataset using consensus clustering,<sup><xref rid="R7" ref-type="bibr">7</xref></sup> followed by clustering the binary matrix generated by aggregating the clusters obtained from each dataset. While this approach is robust and easily scalable to a large number of datasets, a limitation of COCA is that all datasets contribute equally to the final clustering which affects the accuracy of the clusters obtained, especially in scenario in which certain dataset is less reliable.</p>
    <p id="P7">Taking inspiration from COCA and multiple kernel learning,<sup><xref rid="R8" ref-type="bibr">8</xref>,<xref rid="R9" ref-type="bibr">9</xref></sup> KLIC<sup><xref rid="R6" ref-type="bibr">6</xref></sup> was developed to address the pitfall of COCA. Similar to COCA, KLIC works by first applying consensus clustering to each dataset. The authors proved that these consensus matrices are positive semi-definite kernels, which can then be used as input in multiple kernel <italic toggle="yes">k</italic>-means clustering and allows for weights to be estimated for each kernel via a two-step optimization strategy and convex quadratic programming. This approach allows for more informative dataset to contribute more to the overall clustering. Currently, KLIC runs one clustering algorithm on each dataset to generate the consensus matrix.</p>
    <p id="P8">In this paper, we seek to extend the KLIC framework to a more robust integrative clustering by proposing a two layer weighted integrative clustering which allows for more than one clustering algorithm to be run on each dataset, i.e, ensemble clustering and aggregated together via an efficient weight estimation.</p>
  </sec>
  <sec id="S2">
    <label>2.</label>
    <title>Methods</title>
    <p id="P9">Our proposed method can be viewed as a combination of (a) ensemble clustering, i.e, aggregating multiple clustering algorithms, (b) consensus clustering, i.e., resampling, and (c) kernel learning integrative clustering. While some papers use ensemble and consensus clustering interchangeably, in this paper, we refer to ensemble clustering as a collection of multiple clustering algorithms, e.g., <italic toggle="yes">k</italic>-means, hierarchical clustering or partitioning around medoid (PAM), whereas consensus clustering as a framework which draws a random sample from either the sample or feature space. We now briefly describe the consensus clustering and kernel learning integrative clustering framework.</p>
    <p id="P10">Consensus clustering was originally proposed by Monti et al (2003).<sup><xref rid="R7" ref-type="bibr">7</xref></sup> The main idea behind consensus clustering is to apply a resampling scheme on the sample or feature dimension under the assumption that different subsamples drawn from the dataset should not differ much in the clustering results. The resampling scheme allows one to assess the stability of the cluster assignments and the robustness of the dataset to perturbations, thus could aid in deriving a more stable and reliable result that reveals the real structure underlying the dataset.</p>
    <p id="P11">A key element derived from the consensus clustering is the consensus matrix which measures the agreement among samples. For a dataset with <inline-formula><mml:math id="M2" display="inline"><mml:mi>N</mml:mi></mml:math></inline-formula> samples, the consensus matrix <inline-formula><mml:math id="M3" display="inline"><mml:mi>ℳ</mml:mi></mml:math></inline-formula> is a <inline-formula><mml:math id="M4" display="inline"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> matrix whose element <inline-formula><mml:math id="M5" display="inline"><mml:mrow><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> denotes the proportion of sample <inline-formula><mml:math id="M6" display="inline"><mml:mi>i</mml:mi></mml:math></inline-formula> and sample <inline-formula><mml:math id="M7" display="inline"><mml:mi>j</mml:mi></mml:math></inline-formula> in the same cluster during the resampling iterations. Values which are close to 1 (and vice versa 0) indicate that the two samples are always assigned to the same cluster (and vice versa different clusters). <inline-formula><mml:math id="M8" display="inline"><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ℳ</mml:mi></mml:mrow></mml:math></inline-formula> is a distance measure which can be used to derive a final clustering result.</p>
    <p id="P12">Cabassi and Kirk (2020)<sup><xref rid="R6" ref-type="bibr">6</xref></sup> proved that the consensus matrix is positive semi-definite and thus can be used as input in kernel learning integrative clustering via the application of multiple kernel <italic toggle="yes">k</italic>-means algorithm. The kernel <italic toggle="yes">k</italic>-means algorithm utilizes the kernel trick by projecting the data into a non-linear feature space via a kernel. This overcomes the drawback of regular <italic toggle="yes">k</italic>-means clustering which cannot identify clusters that are not linearly separable in the original input space. The integration of the multimodal data within the kernel learning integrative clustering involves a convex sum of the kernels, i.e., consensus matrix from each dataset, and the estimation of the weights in the convex sum. In the KLIC integrative clustering algorithm of Cabassi and Kirk (2020),<sup><xref rid="R6" ref-type="bibr">6</xref></sup> the authors adopted the optimization strategy proposed by Gonen and Margolin (2014)<sup><xref rid="R10" ref-type="bibr">10</xref></sup> which involves a convex quadratic programming.</p>
    <p id="P13">In this paper, we reason that the weights in the kernel learning integrative clustering can be estimated by utilizing the fuzziness in the consensus matrix. Furthermore, we extend the framework of KLIC by allowing multiple base clustering algorithms, e.g., <italic toggle="yes">k</italic>-means, hierarchical clustering, PAM, to be applied within each dataset and aggregated, i.e., ensemble clustering<sup><xref rid="R11" ref-type="bibr">11</xref></sup> which has been shown to enhance the robustness of clustering results compared to individual clustering algorithm. To this end, we propose an efficient weight estimation method and a two layer weighted integrative consensus clustering.</p>
    <sec id="S3">
      <label>2.1.</label>
      <title>Weight estimation</title>
      <p id="P14">The consensus matrix can be used to assess cluster stability and composition. As a motivating example, we generate two datasets, each with 10 features and 100 samples. For both datasets, we assume that there are 3 clusters with cluster sizes 20, 30 and 50. All the features are generated from the Gaussian distribution. For dataset 1, 9 out of the 10 features are informative, where the means of cluster 1, 2 and 3 are 1, −1 and 0, respectively with unit variance. For dataset 2, 3 out of the 10 features are informative, where the means of cluster 1, 2 and 3 are 0.2, −0.2 and 0, respectively with unit variance. Non-informative features are generated from standard Gaussian distribution. We designate datasets 1 and 2 as having high and and low signal-to-noise-ratio (SNR), respectively and run consensus clustering on both datasets using 100 iterations of <italic toggle="yes">k</italic>-means and resampling 80% of samples and features in each iteration. <xref rid="F1" ref-type="fig">Figure 1</xref> shows the heatmaps of the consensus matrices. The diagonal blocks plot the in-cluster values, whereas the off diagonals blocks plot the out-of-cluster values. For the low SNR dataset, the off diagonal blocks are much noisier compared to the high SNR dataset. We argue that this can be used to derive the weights in the multiple kernel integrative clustering. Specifically, we define the weights based on the ratio of in-cluster proportion to out-of-cluster proportion using the cluster estimated by the algorithm itself. Clustering result closer to the real structure tends to have higher in-cluster proportion and lower out-of-cluster proportion. In other words, datasets with a higher ratio of in-cluster proportion to out-of-cluster proportion will be assigned larger weights.</p>
      <p id="P15">Without loss of generality, we consider <inline-formula><mml:math id="M9" display="inline"><mml:mi>𝒫</mml:mi></mml:math></inline-formula> consensus matrices <inline-formula><mml:math id="M10" display="inline"><mml:mrow><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> for number of clusters <italic toggle="yes">K</italic>. Here, the consensus matrices could arise by applying different clustering algorithms to the same dataset or could denote consensus matrices derived from different datasets. We further define:</p>
      <list list-type="simple" id="L2">
        <list-item>
          <p id="P16"><inline-formula><mml:math id="M11" display="inline"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>: in-cluster proportion for cluster <inline-formula><mml:math id="M12" display="inline"><mml:mi>k</mml:mi></mml:math></inline-formula> of consensus matrix <inline-formula><mml:math id="M13" display="inline"><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P17"><inline-formula><mml:math id="M14" display="inline"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>: out-of-cluster proportion for cluster <inline-formula><mml:math id="M15" display="inline"><mml:mi>k</mml:mi></mml:math></inline-formula> of consensus matrix <inline-formula><mml:math id="M16" display="inline"><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P18"><inline-formula><mml:math id="M17" display="inline"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>: average in-cluster proportion across all clusters of consensus matrix <inline-formula><mml:math id="M18" display="inline"><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P19"><inline-formula><mml:math id="M19" display="inline"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>: average out-of-cluster proportion across all clusters of consensus matrix <inline-formula><mml:math id="M20" display="inline"><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P20"><inline-formula><mml:math id="M21" display="inline"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>: ratio of in-cluster proportion to out-of-cluster proportion for consensus matrix <inline-formula><mml:math id="M22" display="inline"><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P21"><inline-formula><mml:math id="M23" display="inline"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>: weight for consensus matrix <inline-formula><mml:math id="M24" display="inline"><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
      </list>
      <p id="P22">We propose calculating the weights as follows:
<disp-formula id="FD1"><mml:math id="M1" display="block"><mml:mspace width="0.4em"/><mml:mrow><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∑</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mspace width="thinmathspace"/><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>K</mml:mi></mml:mfrac><mml:mspace linebreak="newline"/><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>∉</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∑</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>∉</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mspace width="thinmathspace"/><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>K</mml:mi></mml:mfrac><mml:mspace linebreak="newline"/><mml:mspace width="2.1em"/><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow></mml:mfrac><mml:mspace linebreak="newline"/><mml:mspace width="1.9em"/><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mi>p</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
      <p id="P23">In practice, true cluster membership is unknown, thus the weights will be computed based on predicted cluster membership. Using this formula, <italic toggle="yes">W</italic><sub>1</sub> = 0.726 and <italic toggle="yes">W</italic><sub>2</sub> = 0.274 for the consensus matrices derived based on predicted cluster membership of the two datasets above.</p>
    </sec>
    <sec id="S4">
      <label>2.2.</label>
      <title>Two Layer Weighted Integrative Consensus Clustering</title>
      <p id="P24">We now describe our proposed two layer weighted integrative consensus clustering. We assume that there are <inline-formula><mml:math id="M25" display="inline"><mml:mi>D</mml:mi></mml:math></inline-formula> datasets, <inline-formula><mml:math id="M26" display="inline"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and number of clusters <italic toggle="yes">K</italic>.</p>
      <p id="P25">Layer 1: For each dataset <inline-formula><mml:math id="M27" display="inline"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="M28" display="inline"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:math></inline-formula>:</p>
      <list list-type="order" id="L4">
        <list-item>
          <p id="P26">Perform ensemble clustering using <italic toggle="yes">P</italic> different clustering methods, where <inline-formula><mml:math id="M29" display="inline"><mml:mrow><mml:mi>p</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula>. This will generate consensus matrices <inline-formula><mml:math id="M30" display="inline"><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M31" display="inline"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P27">Compute the weights <inline-formula><mml:math id="M32" display="inline"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mn>1</mml:mn><mml:mi>d</mml:mi></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mn>2</mml:mn><mml:mi>d</mml:mi></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>P</mml:mi><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> for each consensus matrix <inline-formula><mml:math id="M33" display="inline"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mn>1</mml:mn><mml:mi>d</mml:mi></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mn>2</mml:mn><mml:mi>d</mml:mi></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mi>P</mml:mi><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P28">Define the weighted consensus matrix <inline-formula><mml:math id="M34" display="inline"><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> as <inline-formula><mml:math id="M35" display="inline"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:msubsup></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P29">Apply a clustering algorithm, e.g., PAM or hierarchical clustering, to each weighted consensus matrix <inline-formula><mml:math id="M36" display="inline"><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
      </list>
      <p id="P30">Layer 2:</p>
      <list list-type="order" id="L6">
        <list-item>
          <p id="P31">For the weighted consensus matrix <inline-formula><mml:math id="M37" display="inline"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, compute the weights <inline-formula><mml:math id="M38" display="inline"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P32">Define the weighted of weighted consensus matrix <inline-formula><mml:math id="M39" display="inline"><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> as <inline-formula><mml:math id="M40" display="inline"><mml:mrow><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msubsup><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p id="P33">Apply a clustering algorithm, e.g., PAM or hierarchical clustering, to <inline-formula><mml:math id="M41" display="inline"><mml:mrow><mml:msub><mml:mi>ℳ</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> to derive a final clustering result.</p>
        </list-item>
      </list>
      <p id="P34">We provide a flowchart in <xref rid="F2" ref-type="fig">Figure 2</xref> summarizing our proposed two layer weighted integrative consensus clustering. Our method is implemented as a GitHub R package intCC available at <ext-link xlink:href="https://github.com/candsj/intCC" ext-link-type="uri">https://github.com/candsj/intCC</ext-link>.</p>
    </sec>
  </sec>
  <sec id="S5">
    <label>3.</label>
    <title>Simulation studies</title>
    <p id="P35">We conduct simulation studies to compare the performance of our proposed two layer weighted integrative consensus clustering intCC against other integrative clustering methods which are implemented for both Gaussian and non-Gaussian distributed datasets, namely KLIC<sup><xref rid="R6" ref-type="bibr">6</xref></sup> and iClusterPlus.<sup><xref rid="R3" ref-type="bibr">3</xref></sup></p>
    <sec id="S6">
      <label>3.1.</label>
      <title>Datasets</title>
      <p id="P36">Unlike Cabassi and Kirk (2020)<sup><xref rid="R6" ref-type="bibr">6</xref></sup> which only considered data simulated from Gaussian distributions, we follow the strategy of Mo et al. (2013)<sup><xref rid="R3" ref-type="bibr">3</xref></sup> where we generate datasets from different distributions, including Gaussian (e.g., M-values from DNA methylation, microarray data such as gene expression), binomial (e.g., somatic mutations), Poisson (e.g., count data from sequencing technologies such as RNA-Seq data or copy number data represented as number of copies gained or lost) and multinomial (e.g., copy number data states represented as gain, normal or loss, or SNP data) distributions. This is to ensure that our proposed method is applicable to integration of continuous, binary, count and categorical types of datasets. For Settings 1-6, we set the sample size and the true number of clusters to be 60 and 3, respectively in which each cluster consists of 20 samples. We vary the number of informative and non-informative, i.e., noise features. The parameters used in our simulations for Settings 1-6 are provided in <xref rid="SD1" ref-type="supplementary-material">Supplementary Table 1</xref>. Settings 7-9 follow from the simulation setup of of Cabassi and Kirk (2020).<sup><xref rid="R6" ref-type="bibr">6</xref></sup> We consider several simulation settings, namely:</p>
      <list list-type="order" id="L8">
        <list-item>
          <p id="P37">Setting 1: 4 datasets following Gaussian, binomial, Poisson and multinomial distribution, respectively. Each dataset has 30 features, in which 15 features are informative and the rest are noise features.</p>
        </list-item>
        <list-item>
          <p id="P38">Setting 2: 4 datasets includes normal, binomial, Poisson and multinomial distribution, respectively. Each dataset has 30 features, in which 15 features are informative and the rest are noise features. Informative features have slighly lower signal compared to the Setting 1.</p>
        </list-item>
        <list-item>
          <p id="P39">Setting 3: 3 datasets following Gaussian, binomial and Poisson distribution, respectively. Each dataset has 30 features, in which 15 features are informative and the rest are noise features.</p>
        </list-item>
        <list-item>
          <p id="P40">Setting 4: 5 datasets following Gaussian, binomial, Poisson, multinomial and Gaussian distribution, respectively. Each dataset has 30 features. For the first 4 datasets, 15 features are informative and the rest are noise features. The 5th dataset follows a Gaussian distribution in which all features are noise features.</p>
        </list-item>
        <list-item>
          <p id="P41">Setting 5: 4 datasets following Gaussian, binomial, Poisson and multinomial distribution, respectively. Each dataset has 500 features, in which 100 features are informative and the rest are noise features.</p>
        </list-item>
        <list-item>
          <p id="P42">Setting 6: 4 datasets following Gaussian, binomial, Poisson and multinomial distribution, respectively. Each dataset has 500 features, in which 250 features are informative and the rest are noise features.</p>
        </list-item>
        <list-item>
          <p id="P43">Setting 7: 4 datasets following Gaussian distribution with similar parameter setting. Each dataset consists of 300 samples with 6 clusters of size 50 samples each. There are 2 features with no noise feature. For cluster <italic toggle="yes">k</italic>, <italic toggle="yes">μ</italic> = <italic toggle="yes">k</italic> × (separation level − 1)/2, <italic toggle="yes">σ</italic> = 1, <italic toggle="yes">k</italic> = 1, 2, 3, 4, 5, 6. Separation level = 4 is used in this setting.</p>
        </list-item>
        <list-item>
          <p id="P44">Setting 8: 4 datasets following Gaussian distribution with different parameter setting. Each dataset consists of 300 samples with 6 clusters of size 50 samples each. There are 2 features with no noise feature. For cluster <italic toggle="yes">k</italic>, <italic toggle="yes">μ</italic> = <italic toggle="yes">k</italic> × (separation level − 1)/2, <italic toggle="yes">σ</italic> = 1, <italic toggle="yes">k</italic> = 1, 2, 3, 4, 5, 6. Varying separation levels = 1, 2, 3, 4 are used in this setting. Only 3 datasets are used as input. We consider 4 dataset combinations, namely 123, 124, 134, 234. Here 123 implies that the clustering algorithms are applied to only datasets 1, 2 and 3.</p>
        </list-item>
        <list-item>
          <p id="P45">Setting 9 (nested cluster structure): 2 datasets following Gaussian distribution, in which each dataset consists of 300 samples. There are 2 features with no noise feature. Dataset 1 has 6 clusters of size 50 samples each. Dataset 2 has 3 clusters of size 100 samples each. For cluster <italic toggle="yes">k</italic>, <italic toggle="yes">μ</italic> = <italic toggle="yes">k</italic> × (separation level − 1)/2, <italic toggle="yes">σ</italic> = 1, <italic toggle="yes">k</italic> = 1, 2, 3, 4, 5, 6 for dataset 1 and <italic toggle="yes">k</italic> = 1, 2, 3 for dataset 2. Separation level = 4 is used in this setting.</p>
        </list-item>
      </list>
      <p id="P46">Each setting is repeated 100 times. Additional simulation settings including multivariate Gaussian distribution are provided in <xref rid="SD1" ref-type="supplementary-material">Supplementary Material</xref>.</p>
    </sec>
    <sec id="S7">
      <label>3.2.</label>
      <title>Clustering algorithms</title>
      <p id="P47">We apply several clustering strategies based on our proposed method intCC, KLIC<sup><xref rid="R6" ref-type="bibr">6</xref></sup> and iClusterPlus.<sup><xref rid="R3" ref-type="bibr">3</xref></sup> To evaluate the advantage of ensemble clustering, i.e., applying multiple clustering algorithms to each dataset, we also include our proposed method which only runs a single clustering algorithm to each dataset. We denote this as one layer weighted integrative consensus clustering. We also compare application of PAM and hierarchical clustering to the weighted consensus matrix in deriving a final clustering result. These methods are denoted as:</p>
      <list list-type="order" id="L10">
        <list-item>
          <p id="P48">iClusterPlus: applying iClusterPlus with the data type specified.</p>
        </list-item>
        <list-item>
          <p id="P49">KLIC-k-means: KLIC by applying <italic toggle="yes">k</italic>-means to each dataset for generating the consensus matrix.</p>
        </list-item>
        <list-item>
          <p id="P50">KLIC-Hclust: KLIC by applying hierarchical clustering to each dataset for generating the consensus matrix.</p>
        </list-item>
        <list-item>
          <p id="P51">1 layer intCC-k-means (PAM): One layer weighted integrative consensus clustering by applying <italic toggle="yes">k</italic>-means to each dataset for generating the consensus matrix, followed by PAM to derive a final clustering result.</p>
        </list-item>
        <list-item>
          <p id="P52">1 layer intCC-Hclust (PAM): One layer weighted integrative consensus clustering by applying hierarchical clustering to each dataset for generating the consensus matrix, followed by PAM to derive a final clustering result.</p>
        </list-item>
        <list-item>
          <p id="P53">1 layer intCC-k-means (Hclust): One layer weighted integrative consensus clustering by applying <italic toggle="yes">k</italic>-means to each dataset for generating the consensus matrix, followed by hierarchical clustering to derive a final clustering result.</p>
        </list-item>
        <list-item>
          <p id="P54">1 layer intCC-Hclust (Hclust): One layer weighted integrative consensus clustering by applying hierarchical clustering to each dataset for generating the consensus matrix, followed by hierarchical clustering to derive a final clustering result.</p>
        </list-item>
      </list>
      <p id="P55">To obtain an unbiased comparison to our two layer approach, we also apply KLIC with multiple clustering algorithms. In other words, suppose there are 4 datasets and two clustering algorithms are applied to each dataset, there will be a total of 8 consensus matrices, i.e., akin to applying KLIC to 8 datasets. KLIC is applied using these 8 consensus matrices as input in the multiple kernel integrative clustering. Additionally, to illustrate the advantage of two layer approach, we also include another one layer approach in which we apply a single layer weight estimation to the 8 consensus matrices. These methods are denoted as:</p>
      <list list-type="simple" id="L12">
        <list-item>
          <label>(8)</label>
          <p id="P56">2 layer intCC-2 methods (PAM): Two layer weighted integrative consensus clustering by applying both <italic toggle="yes">k</italic>-means and hierarchical clustering to each dataset for generating the consensus matrices, followed by PAM to derive a final clustering result.</p>
        </list-item>
        <list-item>
          <label>(9)</label>
          <p id="P57">2 layer intCC-2 methods (Hclust): Two layer weighted integrative consensus clustering by applying both <italic toggle="yes">k</italic>-means and hierarchical clustering to each dataset for generating the consensus matrices, followed by hierarchical clustering to derive a final clustering result.</p>
        </list-item>
        <list-item>
          <label>(10)</label>
          <p id="P58">KLIC-2-methods: KLIC by applying both <italic toggle="yes">k</italic>-means and hierarchical clustering to each dataset for generating the consensus matrices.</p>
        </list-item>
        <list-item>
          <label>(11)</label>
          <p id="P59">1 layer intCC-2 methods (PAM): One layer weighted integrative consensus clustering by applying both <italic toggle="yes">k</italic>-means and hierarchical clustering to each dataset for generating the consensus matrices, followed by PAM to derive a final clustering result.</p>
        </list-item>
        <list-item>
          <label>(12)</label>
          <p id="P60">1 layer intCC-2 methods (Hclust): One layer weighted integrative consensus clustering by applying both <italic toggle="yes">k</italic>-means and hierarchical clustering to each dataset for generating the consensus matrices, followed by hierarchical clustering to derive a final clustering result.</p>
        </list-item>
      </list>
      <p id="P61">For Settings 1-8, we apply each method by setting the number of clusters to be the true number of clusters. In practice, one can tune the optimal number of clusters using criteria such as the silhouette method,<sup><xref rid="R12" ref-type="bibr">12</xref></sup> gap statistics,<sup><xref rid="R13" ref-type="bibr">13</xref></sup> Dunn index<sup><xref rid="R14" ref-type="bibr">14</xref></sup> or the delta <italic toggle="yes">K</italic> method.<sup><xref rid="R7" ref-type="bibr">7</xref></sup> For Setting 9, we consider (a) global clustering, where we set the number of clusters to be the same throughout for both individual dataset and final integrative clustering, i.e., either 3 or 6 throughout (we denote these strategies as “Global K=3” and “Global K=6”), and (b) separate clustering, where we use the true number of clusters for individual dataset, i.e., 6 for dataset 1 and 3 for dataset 2, and consider both <italic toggle="yes">K</italic> = 3 and <italic toggle="yes">K</italic> = 6 in the final integrative clustering (we denote these strategies as “Separate K=3” and “Separate K=6”). Additionally, due to the poor performance of iClusterPlus and the long computational time, we omit iClusterPlus for Settings 4-6. We compare the performance of the clustering methods via the average adjusted rand index (ARI). We also report the weight estimation time of intCC and KLIC.</p>
    </sec>
    <sec id="S8">
      <label>3.3.</label>
      <title>Results</title>
      <p id="P62">We summarize the ARI for each simulation setting in <xref rid="F3" ref-type="fig">Figure 3</xref>. Overall, results show that our proposed methods, namely 2 layer intCC-2 methods (PAM) and 1 layer intCC-k-means (PAM) perform well across all simulation settings. To explain this observation, without loss of generality, we summarize the ARI within each simulated dataset of Setting 4 in <xref rid="F4" ref-type="fig">Figures 4A</xref> and <xref rid="F4" ref-type="fig">4B</xref>. The ARI by applying <italic toggle="yes">k</italic>-means as the base algorithm in the consensus clustering within each dataset is significantly better than hierarchical clustering in the simulated datasets considered in this paper. Thus, it is not surprising that methods which use <italic toggle="yes">k</italic>-means as the base clustering algorithm in the consensus clustering yield better performance. However, in practice the best base clustering algorithm is sometimes unknown. Thus, the 2 layer intCC which aggregates multiple base clustering algorithms can automatically assign higher weights to the better algorithm as shown in our simulation studies, as evident from the estimated weights in <xref rid="F4" ref-type="fig">Figures 4C</xref> and <xref rid="F4" ref-type="fig">4D</xref>. It is also worth noting that our method assigns significantly smaller weights to the 5th dataset in which all the features are noise features. Additionally, using PAM to derive a final clustering result in general yields better performance compared to hierarchical clustering. We also note that the performance of iClusterPlus is significantly poorer compared to other methods, consistent with the findings of Cabassi and Kirk (2020).<sup><xref rid="R6" ref-type="bibr">6</xref></sup> Moreover, extending KLIC to run multiple base clustering algorithms, i.e., KLIC-2-methods has lower ARI compared to our proposed method, implying that the current KLIC framework does not yield a straightforward extension to incorporate ensemble clustering.</p>
      <p id="P63">Without loss of generality, we also report the weight calculation time for KLIC and our proposed method intCC for Setting 1 (60 samples) and Setting 7 (300 samples) in <xref rid="T1" ref-type="table">Table 1</xref>, which shows that our proposed weight calculation is computationally efficient and yields good operating characteristics.</p>
    </sec>
  </sec>
  <sec id="S9">
    <label>4.</label>
    <title>Case study</title>
    <p id="P64">We illustrate our proposed method intCC on the TCGA pan cancer datasets.<sup><xref rid="R15" ref-type="bibr">15</xref></sup> There are 5 datasets across 12 cancer types which represent different tissues of origin, including DNA copy number, DNA methylation, mRNA expression, microRNA expression and protein expression data. To minimize bias in the comparison, we use the same preprocessing pipeline as previously described.<sup><xref rid="R6" ref-type="bibr">6</xref>,<xref rid="R15" ref-type="bibr">15</xref></sup></p>
    <p id="P65">Cabassi and Kirk (2020)<sup><xref rid="R6" ref-type="bibr">6</xref></sup> followed the same procedures described in Hoadley et al. (2014)<sup><xref rid="R15" ref-type="bibr">15</xref></sup> in setting the number of clusters for each dataset, except for microRNA expression in which the authors identified 8 as the number of clusters. We also set the number of clusters for each dataset following Cabassi and Kirk (2020).<sup><xref rid="R6" ref-type="bibr">6</xref></sup> Subsequently, we apply our proposed method intCC to obtain an integrative clustering across these datasets using the PAM algorithm to derive a final clustering result. Our method also selects 10 as the optimal number of clusters based on the average silhouette criterion, similar to KLIC.<sup><xref rid="R6" ref-type="bibr">6</xref></sup>
<xref rid="F5" ref-type="fig">Figure 5A</xref> compares the cluster membership of our method intCC against the results of KLIC, with ARI 0.693, whereas <xref rid="F5" ref-type="fig">Figures 5B</xref> and <xref rid="F5" ref-type="fig">5C</xref> compare the cluster membership of intCC and KLIC against the 12 cancer type annotation, respectively. The ARI between intCC and cancer type annotation associated with tissues of origin is 0.754, whereas the ARI between KLIC and cancer type annotation is 585, indicating that the cluster membership of intCC yields a higher consistency with tissues of origin in the TCGA pan cancer datasets. Further investigation into the clusters obtained by intCC versus KLIC among subset of breast invasive carcinoma (BRCA) indicates that the results from intCC yield a higher consistency with the TCGA-BRCA molecular subtypes compared to the results from KLIC (<xref rid="SD1" ref-type="supplementary-material">Supplementary Material</xref>).</p>
    <p id="P66">The estimated weights of each dataset for intCC and KLIC are (DNA copy number, DNA methylation, mRNA expression, miRNA expression, protein expression) =(0.073, 0.401,0.045, 0.272, 0.209) and (0.309, 0.192, 0.168, 0.183, 0.148), respectively. intCC assigns a higher weight to DNA methylation data, whereas KLIC assigns a higher weight to the copy number data, which could explain the differences observed in cluster memberships obtained by these two methods. Finally, the weight calculation time for intCC is 0.43 second, whereas the weight calculation time for KLIC via quadratic programming is &gt; 10 hours on an Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz.</p>
  </sec>
  <sec id="S10">
    <label>5.</label>
    <title>Discussion</title>
    <p id="P67">The rapid development of high throughput technologies has provided an avenue to scientists to decipher the complex human diseases from a systems biology perspective via multiomics profiling. Integrative clustering has become a powerful approach to dissect the heterogeneity underpinning these diseases, e.g., to define new cancer subtypes which may help inform treatment efforts. In this paper, we extend the framework of KLIC<sup><xref rid="R6" ref-type="bibr">6</xref></sup> which recasts the integrative clustering model into multiple kernel learning framework by utilizing the consensus matrices estimated from consensus clustering as input. Specifically, our model further incorporates the ensemble learning via an aggregation of multiple base clustering algorithms to enhance the robustness of multiple kernel integrative clustering model. This is to safeguard against applying a single base clustering algorithm that performs poorly on the dataset. Additionally, we also propose an efficient weight estimation to combine the consensus matrices. Our simulation studies show that the proposed two layer weighted integrative clustering yields better performance overall.</p>
    <p id="P68">Conceptually, the weight estimation is analogous to the heuristics of multiple kernel support vector machine (MKL-SVM) based on kernel-target alignment.<sup><xref rid="R16" ref-type="bibr">16</xref>-<xref rid="R18" ref-type="bibr">18</xref></sup> Specifically, MKL-SVM is developed for supervised learning and the kernel-target alignment depends on the true binary class labels. For a fixed cluster membership, this is equivalent to multi-class classification. One can extend the kernel-target alignment for multi-class classification by dividing the problem into several binary classification subproblems (e.g., one-versus-all or all-pairs). However, how to optimally combine the results across these binary subproblems is not trivial and may require longer computational time compared to our proposed method.</p>
    <p id="P69">Besides identifying appropriate and robust clustering algorithms, another important research question in unsupervised learning is in tuning the optimal number of clusters. Several metrics have been proposed for this task, including the silhouette method,<sup><xref rid="R12" ref-type="bibr">12</xref></sup> gap statistics,<sup><xref rid="R13" ref-type="bibr">13</xref></sup> Dunn index<sup><xref rid="R14" ref-type="bibr">14</xref></sup> and the delta <italic toggle="yes">K</italic> method.<sup><xref rid="R7" ref-type="bibr">7</xref></sup> An immediate extension to our intCC framework is to aggregate the different metrics/criteria for selecting the optimal number of clusters.</p>
  </sec>
  <sec sec-type="supplementary-material" id="SM1">
    <title>Supplementary Material</title>
    <supplementary-material id="SD1" position="float" content-type="local-data">
      <label>suppl-1952198</label>
      <media xlink:href="NIHMS1952198-supplement-suppl-1952198.pdf" id="d64e1721" position="anchor"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="S11">
    <title>Acknowledgments</title>
    <p id="P70">This work is supported in part by CDC/NIOSH award U01OH012257. The findings and conclusions presented in this article are those of the authors and do not represent the official position of NIOSH, the CDC or the U.S. Public Health Service.</p>
  </ack>
  <fn-group>
    <fn fn-type="COI-statement" id="FN1">
      <p id="P71"><italic toggle="yes">Conflict of Interest</italic>: None declared.</p>
    </fn>
    <fn id="FN2">
      <p id="P72">Supplementary Material and Code</p>
      <p id="P73">Supplementary Material is available online at <ext-link xlink:href="http://www.ams.sunysb.edu/~pfkuan/PDF/SM_PSB2024.pdf" ext-link-type="uri">http://www.ams.sunysb.edu/~pfkuan/PDF/SM_PSB2024.pdf</ext-link>.</p>
      <p id="P74">The R code implementing intCC is available online at <ext-link xlink:href="https://github.com/candsj/intCC" ext-link-type="uri">https://github.com/candsj/intCC</ext-link>.</p>
    </fn>
  </fn-group>
  <ref-list>
    <title>References</title>
    <ref id="R1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><name><surname>Lock</surname><given-names>EF</given-names></name> and <name><surname>Dunson</surname><given-names>DB</given-names></name>, <article-title>Bayesian consensus clustering</article-title>, <source>Bioinformatics</source>
<volume>29</volume>, <fpage>2610</fpage> (<year>2013</year>).<pub-id pub-id-type="pmid">23990412</pub-id>
</mixed-citation>
    </ref>
    <ref id="R2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><name><surname>Shen</surname><given-names>R</given-names></name>, <name><surname>Olshen</surname><given-names>AB</given-names></name> and <name><surname>Ladanyi</surname><given-names>M</given-names></name>, <article-title>Integrative clustering of multiple genomic data types using a joint latent variable model with application to breast and lung cancer subtype analysis</article-title>, <source>Bioinformatics</source>
<volume>25</volume>, <fpage>2906</fpage> (<year>2009</year>).<pub-id pub-id-type="pmid">19759197</pub-id>
</mixed-citation>
    </ref>
    <ref id="R3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><name><surname>Mo</surname><given-names>Q</given-names></name>, <name><surname>Wang</surname><given-names>S</given-names></name>, <name><surname>Seshan</surname><given-names>VE</given-names></name>, <name><surname>Olshen</surname><given-names>AB</given-names></name>, <name><surname>Schultz</surname><given-names>N</given-names></name>, <name><surname>Sander</surname><given-names>C</given-names></name>, <name><surname>Powers</surname><given-names>RS</given-names></name>, <name><surname>Ladanyi</surname><given-names>M</given-names></name> and <name><surname>Shen</surname><given-names>R</given-names></name>, <article-title>Pattern discovery and cancer gene identification in integrated cancer genomic data</article-title>, <source>Proceedings of the National Academy of Sciences</source>
<volume>110</volume>, <fpage>4245</fpage> (<year>2013</year>).</mixed-citation>
    </ref>
    <ref id="R4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><collab>B. . W. H. . H. M. S.</collab><name><surname>C.</surname><given-names>L</given-names></name> . . <name><surname>P.</surname><given-names>PJ</given-names></name> . <name><surname>K.</surname><given-names>R</given-names></name> 13, <collab>G. data analysis: Baylor College of Medicine</collab>
<name><surname>Creighton</surname><given-names>Chad J.</given-names></name> 22 23 <name><surname>Donehower</surname><given-names>Lawrence A.</given-names></name> 22 23 24 25, <collab>I. for Systems Biology</collab>
<name><surname>Reynolds</surname><given-names>Sheila</given-names></name> 31 <name><surname>Kreisberg</surname><given-names>Richard B.</given-names></name> 31 <name><surname>Bernard</surname><given-names>Brady</given-names></name> 31 <name><surname>Bressler</surname><given-names>Ryan</given-names></name> 31 <name><surname>Erkkila</surname><given-names>Timo</given-names></name> 32 <name><surname>Lin</surname><given-names>Jake</given-names></name> 31 <name><surname>Thorsson</surname><given-names>Vesteinn</given-names></name> 31 <name><surname>Zhang</surname><given-names>Wei</given-names></name> 33 <name><surname>Shmulevich</surname><given-names>Ilya</given-names></name> 31 <etal/>, <article-title>Comprehensive molecular portraits of human breast tumours</article-title>, <source>Nature</source>
<volume>490</volume>, <fpage>61</fpage> (<year>2012</year>).<pub-id pub-id-type="pmid">23000897</pub-id>
</mixed-citation>
    </ref>
    <ref id="R5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><name><surname>Gabasova</surname><given-names>E</given-names></name>, <name><surname>Reid</surname><given-names>J</given-names></name> and <name><surname>Wernisch</surname><given-names>L</given-names></name>, <article-title>Clusternomics: Integrative context-dependent clustering for heterogeneous datasets</article-title>, <source>PLoS Computational Biology</source>
<volume>13</volume>, p. <fpage>e1005781</fpage> (<year>2017</year>).<pub-id pub-id-type="pmid">29036190</pub-id>
</mixed-citation>
    </ref>
    <ref id="R6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><name><surname>Cabassi</surname><given-names>A</given-names></name> and <name><surname>Kirk</surname><given-names>PD</given-names></name>, <article-title>Multiple kernel learning for integrative consensus clustering of omic datasets</article-title>, <source>Bioinformatics</source>
<volume>36</volume>, <fpage>4789</fpage> (<year>2020</year>).<pub-id pub-id-type="pmid">32592464</pub-id>
</mixed-citation>
    </ref>
    <ref id="R7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><name><surname>Monti</surname><given-names>S</given-names></name>, <name><surname>Tamayo</surname><given-names>P</given-names></name>, <name><surname>Mesirov</surname><given-names>J</given-names></name> and <name><surname>Golub</surname><given-names>T</given-names></name>, <article-title>Consensus clustering: a resampling-based method for class discovery and visualization of gene expression microarray data</article-title>, <source>Machine Learning</source>
<volume>52</volume>, <fpage>91</fpage> (<year>2003</year>).</mixed-citation>
    </ref>
    <ref id="R8">
      <label>8.</label>
      <mixed-citation publication-type="confproc"><name><surname>Bach</surname><given-names>FR</given-names></name>, <name><surname>Lanckriet</surname><given-names>GR</given-names></name> and <name><surname>Jordan</surname><given-names>MI</given-names></name>, <source>Multiple kernel learning, conic duality, and the smo algorithm</source>, in <conf-name>Proceedings of the Twenty-First International Conference on Machine Learning</conf-name>, (<conf-loc>Banff, Canada</conf-loc>, <year>2004</year>).</mixed-citation>
    </ref>
    <ref id="R9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><name><surname>Lanckriet</surname><given-names>GR</given-names></name>, <name><surname>Cristianini</surname><given-names>N</given-names></name>, <name><surname>Bartlett</surname><given-names>P</given-names></name>, <name><surname>Ghaoui</surname><given-names>LE</given-names></name> and <name><surname>Jordan</surname><given-names>MI</given-names></name>, <article-title>Learning the kernel matrix with semidefinite programming</article-title>, <source>Journal of Machine Learning Research</source>
<volume>5</volume>, <fpage>27</fpage> (<year>2004</year>).</mixed-citation>
    </ref>
    <ref id="R10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><name><surname>Gönen</surname><given-names>M</given-names></name> and <name><surname>Margolin</surname><given-names>AA</given-names></name>, <article-title>Localized data fusion for kernel k-means clustering with application to cancer biology</article-title>, <source>Advances in Neural Information Processing Systems</source>
<volume>27</volume> (<year>2014</year>).</mixed-citation>
    </ref>
    <ref id="R11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><name><surname>Sagi</surname><given-names>O</given-names></name> and <name><surname>Rokach</surname><given-names>L</given-names></name>, <article-title>Ensemble learning: A survey</article-title>, <source>Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</source>
<volume>8</volume>, p. <fpage>e1249</fpage> (<year>2018</year>).</mixed-citation>
    </ref>
    <ref id="R12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><name><surname>Rousseeuw</surname><given-names>PJ</given-names></name>, <article-title>Silhouettes: a graphical aid to the interpretation and validation of cluster analysis</article-title>, <source>Journal of Computational and Applied Mathematics</source><volume>20</volume>, <fpage>53</fpage> (<year>1987</year>).</mixed-citation>
    </ref>
    <ref id="R13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><name><surname>Tibshirani</surname><given-names>R</given-names></name>, <name><surname>Walther</surname><given-names>G</given-names></name> and <name><surname>Hastie</surname><given-names>T</given-names></name>, <article-title>Estimating the number of clusters in a data set via the gap statistic</article-title>, <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source>
<volume>63</volume>, <fpage>411</fpage> (<year>2001</year>).</mixed-citation>
    </ref>
    <ref id="R14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><name><surname>Halkidi</surname><given-names>M</given-names></name>, <name><surname>Batistakis</surname><given-names>Y</given-names></name> and <name><surname>Vazirgiannis</surname><given-names>M</given-names></name>, <article-title>On clustering validation techniques</article-title>, <source>Journal of Intelligent Information Systems</source>
<volume>17</volume>, <fpage>107</fpage> (<year>2001</year>).</mixed-citation>
    </ref>
    <ref id="R15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><name><surname>Hoadley</surname><given-names>KA</given-names></name>, <name><surname>Yau</surname><given-names>C</given-names></name>, <name><surname>Wolf</surname><given-names>DM</given-names></name>, <name><surname>Cherniack</surname><given-names>AD</given-names></name>, <name><surname>Tamborero</surname><given-names>D</given-names></name>, <name><surname>Ng</surname><given-names>S</given-names></name>, <name><surname>Leiserson</surname><given-names>MD</given-names></name>, <name><surname>Niu</surname><given-names>B</given-names></name>, <name><surname>McLellan</surname><given-names>MD</given-names></name>, <name><surname>Uzunangelov</surname><given-names>V</given-names></name><etal/>, <article-title>Multiplatform analysis of 12 cancer types reveals molecular classification within and across tissues of origin</article-title>, <source>Cell</source><volume>158</volume>, <fpage>929</fpage> (<year>2014</year>).<pub-id pub-id-type="pmid">25109877</pub-id>
</mixed-citation>
    </ref>
    <ref id="R16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><name><surname>Cristianini</surname><given-names>N</given-names></name>, <name><surname>Shawe-Taylor</surname><given-names>J</given-names></name>, <name><surname>Elisseeff</surname><given-names>A</given-names></name> and <name><surname>Kandola</surname><given-names>J</given-names></name>, <article-title>On kernel-target alignment</article-title>, <source>Advances in neural information processing systems</source>
<volume>14</volume> (<year>2001</year>).</mixed-citation>
    </ref>
    <ref id="R17">
      <label>17.</label>
      <mixed-citation publication-type="confproc"><name><surname>Cortes</surname><given-names>C</given-names></name>, <name><surname>Mohri</surname><given-names>M</given-names></name> and <name><surname>Rostamizadeh</surname><given-names>A</given-names></name>, <source>Two-stage learning kernel algorithms</source>, <conf-name>Proceedings of the 27 th International Conference on Machine Learning</conf-name> , <fpage>239</fpage> (<year>2010</year>).</mixed-citation>
    </ref>
    <ref id="R18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><name><surname>Qiu</surname><given-names>S</given-names></name> and <name><surname>Lane</surname><given-names>T</given-names></name>, <article-title>A framework for multiple kernel support vector regression and its applications to sirna efficacy prediction</article-title>, <source>IEEE/ACM Transactions on Computational Biology and Bioinformatics</source>
<volume>6</volume>, <fpage>190</fpage> (<year>2008</year>).</mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig position="float" id="F1">
    <label>Fig. 1.</label>
    <caption>
      <p id="P75">Heatmaps of consensus matrices for high and low signal-to-noise ratio (SNR) datasets. True cluster membership is given in the annotation above each heatmap. Predicted cluster membership corresponds to the three gap-separated blocks in each heatmap.</p>
    </caption>
    <graphic xlink:href="nihms-1952198-f0001" position="float"/>
  </fig>
  <fig position="float" id="F2">
    <label>Fig. 2.</label>
    <caption>
      <p id="P76">Flowchat describing our proposed algorithm.</p>
    </caption>
    <graphic xlink:href="nihms-1952198-f0002" position="float"/>
  </fig>
  <fig position="float" id="F3">
    <label>Fig. 3.</label>
    <caption>
      <p id="P77">Distribution of ARI across all methods and simulation settings. Blue (red) boxplots are methods which apply one (two) clustering algorithm(s) per dataset. A-G. Settings 1-7. H-K. Setting 8 with different dataset combinations as input. L-O. Setting 9 with different strategies for setting number of clusters for individual dataset and final integrative clustering.</p>
    </caption>
    <graphic xlink:href="nihms-1952198-f0003" position="float"/>
  </fig>
  <fig position="float" id="F4">
    <label>Fig. 4.</label>
    <caption>
      <p id="P78">A-B. Distribution of ARI within each simulated dataset of Setting 4. C-D. Distribution of estimated weights from intCC within each simulated dataset of Setting 4. Purple (green) boxplots are results by applying <italic toggle="yes">k</italic>-means (hierarchical clustering) algorithm in the consensus clustering. A, C. Using PAM to derive a final clustering result. B, D. Using hierarchical clustering to derive a final clustering result.</p>
    </caption>
    <graphic xlink:href="nihms-1952198-f0004" position="float"/>
  </fig>
  <fig position="float" id="F5">
    <label>Fig. 5.</label>
    <caption>
      <p id="P79">Heatmaps of coincidence matrices comparing A. intCC clusters to KLIC clusters, B. intCC clusters to cancer type annotation, C. KLIC clusters to cancer type annotation. The ARI is reported in the header of each plot.</p>
    </caption>
    <graphic xlink:href="nihms-1952198-f0005" position="float"/>
  </fig>
  <table-wrap position="float" id="T1">
    <label>Table 1.</label>
    <caption>
      <p id="P80">Weight calculation time comparison.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Method</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Setting 1 (seconds)</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Setting 7 (seconds)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">KLIC-k-means</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.541</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">7.209</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">KLIC-Hclust</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.791</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">8.241</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">1 layer intCC-k-means (PAM)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.000879</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00330</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">1 layer intCC-Hclust (PAM)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.000882</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00335</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">1 layer intCC-k-means (Hclust)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.000876</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00333</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">1 layer intCC-Hclust (Hclust)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.000909</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00332</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">2 layer intCC-2 methods (PAM)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00273</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.0103</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">2 layer intCC-2 methods (Hclust)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00265</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.0102</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">KLIC-2-methods</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.428</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">27.592</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">1 layer intCC-2 methods (PAM)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00155</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00673</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">1 layer intCC-2 methods (Hclust)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00152</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.00686</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
