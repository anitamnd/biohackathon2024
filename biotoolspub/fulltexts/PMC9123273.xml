<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_YNICL103044 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEgr4 jpg ?>
<?FILEgr5 jpg ?>
<?FILEgr6 jpg ?>
<?FILEgr7 jpg ?>
<?FILEgr8 jpg ?>
<?FILEga1 jpg ?>
<?FILEsi1 svg ?>
<?FILEsi2 svg ?>
<?FILEsi3 svg ?>
<?FILEsi4 svg ?>
<?FILEsi5 svg ?>
<?FILEsi6 svg ?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Neuroimage Clin</journal-id>
    <journal-id journal-id-type="iso-abbrev">Neuroimage Clin</journal-id>
    <journal-title-group>
      <journal-title>NeuroImage : Clinical</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2213-1582</issn>
    <publisher>
      <publisher-name>Elsevier</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9123273</article-id>
    <article-id pub-id-type="pii">S2213-1582(22)00109-7</article-id>
    <article-id pub-id-type="doi">10.1016/j.nicl.2022.103044</article-id>
    <article-id pub-id-type="publisher-id">103044</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Regular Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Semantic segmentation guided detector for segmentation, classification, and lesion mapping of acute ischemic stroke in MRI images</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au005">
        <name>
          <surname>Wei</surname>
          <given-names>Yi-Chia</given-names>
        </name>
        <xref rid="af005" ref-type="aff">a</xref>
        <xref rid="af010" ref-type="aff">b</xref>
        <xref rid="af015" ref-type="aff">c</xref>
        <xref rid="fn1" ref-type="fn">1</xref>
      </contrib>
      <contrib contrib-type="author" id="au010">
        <name>
          <surname>Huang</surname>
          <given-names>Wen-Yi</given-names>
        </name>
        <xref rid="af005" ref-type="aff">a</xref>
        <xref rid="af015" ref-type="aff">c</xref>
        <xref rid="af020" ref-type="aff">d</xref>
        <xref rid="fn1" ref-type="fn">1</xref>
      </contrib>
      <contrib contrib-type="author" id="au015">
        <name>
          <surname>Jian</surname>
          <given-names>Chih-Yu</given-names>
        </name>
        <xref rid="af025" ref-type="aff">e</xref>
      </contrib>
      <contrib contrib-type="author" id="au020">
        <name>
          <surname>Hsu</surname>
          <given-names>Chih-Chin Heather</given-names>
        </name>
        <xref rid="af010" ref-type="aff">b</xref>
        <xref rid="af030" ref-type="aff">f</xref>
      </contrib>
      <contrib contrib-type="author" id="au025">
        <name>
          <surname>Hsu</surname>
          <given-names>Chih-Chung</given-names>
        </name>
        <xref rid="af035" ref-type="aff">g</xref>
      </contrib>
      <contrib contrib-type="author" id="au030">
        <name>
          <surname>Lin</surname>
          <given-names>Ching-Po</given-names>
        </name>
        <xref rid="af010" ref-type="aff">b</xref>
      </contrib>
      <contrib contrib-type="author" id="au035">
        <name>
          <surname>Cheng</surname>
          <given-names>Chi-Tung</given-names>
        </name>
        <xref rid="af020" ref-type="aff">d</xref>
        <xref rid="af040" ref-type="aff">h</xref>
      </contrib>
      <contrib contrib-type="author" id="au040">
        <name>
          <surname>Chen</surname>
          <given-names>Yao-Liang</given-names>
        </name>
        <xref rid="af045" ref-type="aff">i</xref>
        <xref rid="af050" ref-type="aff">j</xref>
      </contrib>
      <contrib contrib-type="author" id="au045">
        <name>
          <surname>Wei</surname>
          <given-names>Hung-Yu</given-names>
        </name>
        <xref rid="af055" ref-type="aff">k</xref>
      </contrib>
      <contrib contrib-type="author" id="au050">
        <name>
          <surname>Chen</surname>
          <given-names>Kuan-Fu</given-names>
        </name>
        <email>kfchen@cgmh.org.tw</email>
        <xref rid="af015" ref-type="aff">c</xref>
        <xref rid="af025" ref-type="aff">e</xref>
        <xref rid="af060" ref-type="aff">l</xref>
        <xref rid="cor1" ref-type="corresp">⁎</xref>
      </contrib>
      <aff id="af005"><label>a</label>Department of Neurology, Chang Gung Memorial Hospital, Keelung, Taiwan</aff>
      <aff id="af010"><label>b</label>Institute of Neuroscience, National Yang Ming Chiao Tung University, Taipei, Taiwan</aff>
      <aff id="af015"><label>c</label>Community Medicine Research Center, Chang Gung Memorial Hospital, Keelung, Taiwan</aff>
      <aff id="af020"><label>d</label>College of Medicine, Chang Gung University, Taoyuan, Taiwan</aff>
      <aff id="af025"><label>e</label>Clinical Informatics and Medical Statistics Research Center, Chung Gung University, Taoyuan, Taiwan</aff>
      <aff id="af030"><label>f</label>Center for Geriatrics and Gerontology, Taipei Veterans General Hospital, Taipei, Taiwan</aff>
      <aff id="af035"><label>g</label>Institute of Data Science, National Cheng Kung University, Tainan, Taiwan</aff>
      <aff id="af040"><label>h</label>Department of Trauma and Emergency Surgery, Chang Gung Memorial Hospital, Linkou, Taiwan</aff>
      <aff id="af045"><label>i</label>Department of Radiology, Chang Gung Memorial Hospital, Keelung, Taiwan</aff>
      <aff id="af050"><label>j</label>Department of Medical Imaging and Radiological Sciences, Chang Gung University, Taoyuan, Taiwan</aff>
      <aff id="af055"><label>k</label>Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan</aff>
      <aff id="af060"><label>l</label>Department of Emergency Medicine, Chang Gung Memorial Hospital, Keelung, Taiwan</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>⁎</label>Corresponding author at: No.5, Fusing St., Gueishan Dist, Taoyuan City 33302, Taiwan, ROC. <email>kfchen@cgmh.org.tw</email></corresp>
      <fn id="fn1">
        <label>1</label>
        <p id="np005">The authors contribute equally.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>12</day>
      <month>5</month>
      <year>2022</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>12</day>
      <month>5</month>
      <year>2022</year>
    </pub-date>
    <volume>35</volume>
    <elocation-id>103044</elocation-id>
    <history>
      <date date-type="received">
        <day>2</day>
        <month>3</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>7</day>
        <month>5</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>9</day>
        <month>5</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022 The Author(s)</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).</license-p>
      </license>
    </permissions>
    <abstract abstract-type="graphical" id="ab005">
      <title>Graphical abstract</title>
      <fig id="f0045" position="anchor">
        <graphic xlink:href="ga1"/>
      </fig>
    </abstract>
    <abstract abstract-type="author-highlights" id="ab010">
      <title>Highlights</title>
      <p>
        <list list-type="simple" id="l0005">
          <list-item id="o0005">
            <label>•</label>
            <p id="p0005">MRI images provide a wealth of information about acute ischemic stroke (AIS).</p>
          </list-item>
          <list-item id="o0010">
            <label>•</label>
            <p id="p0010">SGD-Net uses a two-stage design to segment and classify AIS lesions in MRI images.</p>
          </list-item>
          <list-item id="o0015">
            <label>•</label>
            <p id="p0015">SGD-Net as a lesion-based classifier outperforms traditional one-stage models.</p>
          </list-item>
          <list-item id="o0020">
            <label>•</label>
            <p id="p0020">SGD-Net Plus applies multimodal images to quantify AIS lesion distribution.</p>
          </list-item>
          <list-item id="o0025">
            <label>•</label>
            <p id="p0025">SGD-Net and SGD-Net Plus automatically segment, classify, and maps AIS lesions.</p>
          </list-item>
        </list>
      </p>
    </abstract>
    <abstract id="ab015">
      <sec>
        <title>Background and purpose</title>
        <p>MRI images timely and accurately reflect ischemic injuries to the brain tissues and, therefore, can support clinical decision-making of acute ischemic stroke (AIS). To maximize the information provided by the MRI images, we leverage deep learning models to segment, classify, and map lesion distributions of AIS.</p>
      </sec>
      <sec>
        <title>Methods</title>
        <p>We evaluated brain MRI images of AIS patients from 2017 to 2020 at a tertiary teaching hospital and developed the Semantic Segmentation Guided Detector Network (SGD-Net), composed of the first U-shaped model for segmentation in diffusion-weighted imaging (DWI) and the second model for binary classification of lesion size (lacune vs. non-lacune) and circulatory territory of lesion location (anterior vs. posterior circulation). Next, we modified the two-stage deep learning model into SGD-Net Plus by automatically segmenting AIS lesions in DWI images and registering the lesion in T1-weighted images and the brain atlases.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>The final enrollment (216 patients with 4606 slices) was divided into 80% for model development and 20% for testing. S1 model segmented AIS lesions in DWI images accurately with a pixel accuracy &gt; 99% (Dice 0.806–0.828 and IoU 0.675–707). In comprehensive evaluation of classification performance, the two-stage SGD-Net outperformed the traditional one-stage models in classifying AIS lesion size (accuracy 0.867–0.956 vs. 0.511–0.867, AUROC 0.962–0.992 vs. 0.528–0.937, AUPRC 0.964–0.994 vs. 0.549–0.938) and location (accuracy 0.860–0.930 vs. 0.326–0.721, AUROC 0.936–0.988 vs. 0.493–0.833, AUPRC 0.883–0.978 vs. 0.365–0.695). The precise lesion segmentation at the first stage of the deep learning model was the basis for further application. After that, the modified two-stage model SGD-Net Plus accurately reported the volume, region percentage, and lesion percentage of each region on the selected brain atlas. Its reports provided clear descriptions and quantifications of the AIS-related brain injuries on white matter tracts, Brodmann areas, and cytoarchitectonic areas.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p>Domain knowledge-oriented design of artificial intelligence applications can deepen our understanding of patients’ conditions and strengthen the use of MRI for patient care. SGD-Net precisely segments AIS lesions on DWI and accurately classifies the lesions. In addition, SGD-Net Plus maps the AIS lesions and quantifies their occupancy in each brain region. They are practical tools to meet the clinical needs and enrich educational resources of neuroimage.</p>
      </sec>
    </abstract>
    <kwd-group id="kg005">
      <title>Abbreviations</title>
      <kwd>AIS, Acute Ischemic Stroke</kwd>
      <kwd>MRI, Magnetic Resonance Imaging</kwd>
      <kwd>PACS, Picture Archiving and Communication System</kwd>
      <kwd>DWI, Diffusion-Weighted Imaging</kwd>
      <kwd>T1W, T1-Weighted imaging</kwd>
      <kwd>SGD-Net, Semantic Segmentation Guided Detector Network</kwd>
      <kwd>SGD-Net Plus, Semantic Segmentation Guided Detector Network Plus</kwd>
      <kwd>SegMap, Segmented lesion Map</kwd>
      <kwd>T2W, T2-weighted imaging</kwd>
      <kwd>FLAIR, FLuid-Attenuated Inversion Recovery</kwd>
      <kwd>DICOM, Digital Imaging and Communications in Medicine</kwd>
      <kwd>NIFTI, Neuroimaging Informatics Technology Initiative</kwd>
      <kwd>S1, Stage-One</kwd>
      <kwd>S2, Stage-Two</kwd>
      <kwd>MPRAGE, Magnetization Prepared RApid Gradient Echo</kwd>
      <kwd>MNI, Montreal Neurological Institute</kwd>
      <kwd>VGG, Visual Geometry Group</kwd>
      <kwd>ResNet, Residual Net</kwd>
      <kwd>DenseNet, Dense Convolutional Network</kwd>
      <kwd>3D-CNNs, Three-Dimensional Convolutional Neural Networks</kwd>
      <kwd>IoU, Intersection-over-Union</kwd>
      <kwd>AUROC, the Area Under the Receiver Operating Characteristics</kwd>
      <kwd>AUPRC, the Area Under the Precision-Recall Curve</kwd>
      <kwd>ADC, Apparent Diffusion Coefficient</kwd>
    </kwd-group>
    <kwd-group id="kg010">
      <title>Keywords</title>
      <kwd>SGD-net</kwd>
      <kwd>SGD-net plus</kwd>
      <kwd>Acute ischemic stroke</kwd>
      <kwd>Diffusion-weighted imaging</kwd>
      <kwd>Joint segmentation and classification</kwd>
      <kwd>Lesion distribution and mapping</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="s0005">
    <label>1</label>
    <title>Introduction</title>
    <p id="p0030">Stroke is a leading cause of healthcare burden globally, and around 70% of stroke cases are ischemic strokes. Every year, 6 million new ischemic stroke cases lead to 6 million disability-adjusted life years (<xref rid="b0065" ref-type="bibr">Feigin et al., 2017</xref>). The estimated lifetime risk of ischemic stroke after 25 years of age is 18.3%, with significant regional differences and the highest risk in East Asia (38.8%) (<xref rid="b0050" ref-type="bibr">Collaborators et al., 2018</xref>). Contemporary medical doctors face challenges in timely and accurate determination of acute stroke lesions, predicting changes in acute neurological symptoms, and providing individualized precision medical treatment to patients with acute ischemic stroke (AIS).</p>
    <p id="p0035">Magnetic resonance imaging (MRI) detects AIS and supports the clinical decision-making process in AIS by applying multiple sequences to collect comprehensive information simultaneously. It helps clinicians precisely locate AIS lesions (<xref rid="b0055" ref-type="bibr">Davis et al., 2006</xref>), grade severity (<xref rid="b0095" ref-type="bibr">Kellner et al., 2019</xref>), and predict tissue survival (<xref rid="b0240" ref-type="bibr">Yu et al., 2020</xref>) and clinical outcomes (<xref rid="b0170" ref-type="bibr">Schaefer et al., 2015</xref>). However, for non-radiologist clinical professionals, the knowledge required for proper interpretation of MRI images limits the optimal utilization of instantaneously available images on the Picture Archiving and Communication System (PACS).</p>
    <p id="p0040">Segmentation is the central task of machine learning in AIS, especially when using deep learning to recognize lesions (<xref rid="b0015" ref-type="bibr">Akkus et al., 2017</xref>). Previous studies have tried to use multimodal MRI images to segment AIS lesions (<xref rid="b0215" ref-type="bibr">Winzeck et al., 2018</xref>). However, a single valid MRI sequence can be sufficient to show AIS lesions. Therefore, a growing number of studies have used diffusion-weighted MRI to reveal AIS lesions because the corresponding changes of the water-restricted AIS lesions appear timely, sensitively, and precisely in diffusion-weighted imaging (DWI) images (<xref rid="b0055" ref-type="bibr">Davis et al., 2006</xref>). However, derivative clinical applications of DWI-based AIS lesion segmentation are still under development. Only a few research groups have applied deep learning-assisted segmentation in scoring ischemic damage on brain tissue (<xref rid="b0235" ref-type="bibr">Yoshimoto et al., 2019</xref>) or performing stroke phenotyping (<xref rid="b0230" ref-type="bibr">Wu et al., 2019</xref>) based on DWI images.</p>
    <p id="p0045">This study leveraged deep learning models to transform MRI images into real-time knowledge to realize explainable artificial intelligence (AI). The directly connected two-stage Semantic Segmentation Guided Detector Network (SGD-Net) utilized 2D segmentation and 3D classification on AIS DWI to determine lesion size and location. Furthermore, the modified two-stage Semantic Segmentation Guided Detector Network Plus (SGD-Net Plus) model registered the segmented AIS lesions to T1-weighted imaging (T1W) images and the standard space to calculate their distribution on anatomical regions to provide clinical physicians with helpful information.</p>
  </sec>
  <sec id="s0010">
    <label>2</label>
    <title>Methods</title>
    <sec id="s0015">
      <label>2.1</label>
      <title>Database</title>
      <p id="p0050">DWI images of adult patients with a clinical diagnosis of AIS confirmed by neurologists were obtained from 2017 to 2020. Only patients with MRI acquisition&lt;14 days after stroke onset were included in the study. All MRI images were acquired using a Siemens Magneton Skyra 3 T scanner (Siemens Healthineers, Germany) at Keelung Chang Gung Memorial Hospital, a tertiary teaching hospital in Taiwan. The data were anonymized and de-identified before entering the raw digital medical imaging database. Our institutional review board approved this study (approval numbers 202101915B0).</p>
    </sec>
    <sec id="s0020">
      <label>2.2</label>
      <title>AIS lesion labeling</title>
      <p id="p0055">The image review board was composited of three members and directed by neurologists. The ITK-SNAP toolbox (<ext-link ext-link-type="uri" xlink:href="https://www.itksnap.org" id="ir005">https://www.itksnap.org</ext-link>) was used to manually label AIS lesions on DWI to create a segmented lesion map (SegMap). Each image was reviewed by three members of the image review board to reach an agreement by consensus. The three board members agreed on AIS lesion segmentation and classifications after a comprehensive review of structural MRI, including T1W, T2-weighted imaging (T2W), fluid-attenuated inversion recovery (FLAIR) images, formal imaging reports of radiologists, and the diagnoses at hospital discharge. In addition, during image review and labeling, patients with other structural lesions (e.g., brain tumor or post-surgical encephalomalacia), other acute brain diseases (e.g., hemorrhagic stroke or meningitis), or motion artifacts were excluded. In addition, we excluded images without any recognizable lesions for the AIS.</p>
      <p id="p0060">The first target of classification was the size of AIS lesions in binary classes: lacune and non-lacune. The definition of lacune was an isolated small (&lt;20 mm) water restricted lesion in DWI images, and the lesion was explainable to the patient’s AIS symptoms (<xref rid="b0030" ref-type="bibr">Arsava et al., 2010</xref>, <xref rid="b0205" ref-type="bibr">Wardlaw, 2005</xref>). Any other multiple scattered small lesions were not considered lacunes because they were highly possible from embolic origin but not small vessel occlusion (<xref rid="b0210" ref-type="bibr">Wessels et al., 2005</xref>). The second classifying target was the vascular territory of the AIS lesion by anterior and posterior circulatory territories. The references of classification were based on the atlas by Tatu et al. and its adaptation to multimodal MRI (<xref rid="b0090" ref-type="bibr">Kabir et al., 2007</xref>, <xref rid="b0190" ref-type="bibr">Tatu et al., 1998</xref>). In image labeling, we made extra annotations for the patients with lesions in both the anterior and posterior circulatory. Those patients were excluded from classifying lesion location because the binary classification did not allow an answer true for both classes.</p>
    </sec>
    <sec id="s0025">
      <label>2.3</label>
      <title>Image preprocessing</title>
      <p id="p0065">DWI images were exported from Digital Imaging and Communications in Medicine (DICOM) to Neuroimaging Informatics Technology Initiative (NIFTI) format. The image scale of DWI was 384 × 384 pixels per transverse slice and from 20 to 28 serial transverse slices per patient (spatial resolution = 0.57 × 0.57 × 7 mm; b value = 1000 s/mm<sup>2</sup>). The image set for each patient was replenished with blank images to 32 slices to maintain the consistency of the image input size. The DWI intensity of the raw images was re-scaled by z-score normalization. In addition, data augmentation was performed on Albumentations (<ext-link ext-link-type="uri" xlink:href="https://albumentations.ai" id="ir010">https://albumentations.ai</ext-link>) with functions of rotation, horizontal flip, and shift-scale-rotate (<xref rid="b0035" ref-type="bibr">Buslaev et al., 2020</xref>).</p>
    </sec>
    <sec id="s0030">
      <label>2.4</label>
      <title>SGD-net</title>
      <sec id="s0035">
        <label>2.4.1</label>
        <title>Two-stage SGD-net model development</title>
        <p id="p0070">SGD-Net contained a U-shaped Stage-One (S1) for segmentation and a fully connected Stage-Two (S2) for classification (<xref rid="f0005" ref-type="fig">Fig. 1</xref>). U-Net was the central architecture for constructing the S1 model (<xref rid="b0155" ref-type="bibr">Ronneberger et al., 2015</xref>). We tested several backbones for S1 modeling, including the convolutional network of the Visual Geometry Group (VGG) (<xref rid="b0180" ref-type="bibr">Simonyan and Zisserman, 2015</xref>), Residual Net (ResNet) (<xref rid="b0080" ref-type="bibr">He et al., 2016</xref>), and Dense Convolutional Network (DenseNet) (<xref rid="b0085" ref-type="bibr">Huang et al., 2017</xref>). They were modified into U-shaped DenseUNet (<xref rid="b0110" ref-type="bibr">Li et al., 2018</xref>), ResUNet (<xref rid="b0060" ref-type="bibr">Diakogiannis et al., 2020</xref>), and VGGUNet (<xref rid="b0070" ref-type="bibr">Ghosh et al., 2021</xref>). The segmentation in S1 was based on 2D images of each DWI slice. The model later composed all the slices of each patient and yielded a prediction of SegMap.<fig id="f0005"><label>Fig. 1</label><caption><p>The architecture of SGD-Net. The S1 architecture of the SGD-Net is a modification of U-Net from VGG, DenseNet, and ResNet to form VGGUNet, Dense-UNet, and Res-UNet. The DWI inputs passed through a hierarchical contraction path and a corresponding extraction path. The illustration describes the combination of DenseUNet121 in S1 and 3D-ResNet18 in S2. Along the paths, the convolution blocks are connected to the pooling of dense blocks. Every convolutional block contains convolution layers, a rectified linear unit (ReLu), and a batch normalizer as the basic unit. The hierarchical concatenation between contraction and extraction pathways is pooled into the output layer, and the images are exported in the same size as the input images. Finally, the extracted stroke area images are input to the fully connected (FC) convolutional neural networks S2 to classify AIS lesion size and location.</p></caption><graphic xlink:href="gr1"/></fig></p>
        <p id="p0075">The S2 classified AIS lesions based on the predicted SegMap from S1. Image sets per patient defined the classification to consider the three-dimensional construction of AIS lesions. The S2 backbones of comparisons were 3D-ResNet18, 3D-ResNet50 (<xref rid="b0080" ref-type="bibr">He et al., 2016</xref>), and three-dimensional convolutional neural networks (3D-CNNs) (<xref rid="b0245" ref-type="bibr">Zunair et al., 2020</xref>) (<xref rid="t0005" ref-type="table">Table 1</xref>).<table-wrap position="float" id="t0005"><label>Table 1</label><caption><p>Backbones of the machine learning models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th/><th>Model</th><th>Data type</th><th>Backbone</th><th>Epochs</th><th>Loss function</th></tr></thead><tbody><tr><td rowspan="6">Two-stage model (SGD-Net)</td><td rowspan="3">S1</td><td>VGGUNet</td><td>2D MRI</td><td>VGG16</td><td>150</td><td>Focal Tversky Loss</td></tr><tr><td>ResUNet</td><td>2D MRI</td><td>ResNet50</td><td>150</td><td>Focal Tversky Loss</td></tr><tr><td>DenseUNet</td><td>2D MRI</td><td>DenseNet121</td><td>150</td><td>Focal Tversky Loss</td></tr><tr><td rowspan="3">S2</td><td>3D-CNNs</td><td>3D MRI</td><td>CNNs</td><td>100</td><td>Focal Loss</td></tr><tr><td>3D-ResNet50</td><td>3D MRI</td><td>ResNet50</td><td>100</td><td>Focal Loss</td></tr><tr><td>3D-ResNet18</td><td>3D MRI</td><td>ResNet18</td><td>100</td><td>Focal Loss</td></tr><tr><td colspan="2" rowspan="3">One-stage model</td><td>3D-ResNet50</td><td>3D MRI</td><td>ResNet50</td><td>100</td><td>Focal Loss</td></tr><tr><td>3D-ResNet18</td><td>3D MRI</td><td>ResNet18</td><td>100</td><td>Focal Loss</td></tr><tr><td>3D-CNNs</td><td>3D MRI</td><td>CNNs</td><td>100</td><td>Focal Loss</td></tr></tbody></table><table-wrap-foot><fn><p>S1, stage 1 for segmentation; S2, stage 2 for classification. The loss function of the one-stage and two-stage models was based on focal loss for imbalanced classes of data. In addition, for S1 of the two-stage models, the Tversky index was combined with focal loss to form focal Tversky loss for enhanced learning for lesion segmentation in the U-Net structure.</p></fn></table-wrap-foot></table-wrap></p>
      </sec>
      <sec id="s0040">
        <label>2.4.2</label>
        <title>One-stage model</title>
        <p id="p0080">One-stage 3D models were the traditional design for classifying 3D objects. We tested the performance of 3D-ResNet18, 3D-ResNet50 (<xref rid="b0075" ref-type="bibr">Hara et al., 2017</xref>), and 3D-CNNs (<xref rid="b0245" ref-type="bibr">Zunair et al., 2020</xref>) to compare their performance with the two-stage SGD-Net model (<xref rid="t0005" ref-type="table">Table 1</xref>).</p>
      </sec>
      <sec id="s0045">
        <label>2.4.3</label>
        <title>Model tuning</title>
        <p id="p0085">Focal Tversky Loss (<xref rid="b0005" ref-type="bibr">Abraham and Khan, 2019</xref>) in S1 and Focal Loss in S2 (<xref rid="b0115" ref-type="bibr">Lin et al., 2020</xref>) further tuned model hyperparameters. Focal Tversky Loss contained Tversky Index (TI) for weighting the ratio of true positives (TPs) among the linear combination of TP, false-negative (FN), and false-positive (FP):<disp-formula id="e0005"><mml:math id="M1" altimg="si1.svg"><mml:mrow><mml:mi mathvariant="italic">TverskyIndex</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>T</mml:mi><mml:mi>I</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="e0010"><mml:math id="M2" altimg="si2.svg"><mml:mrow><mml:mi>F</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>T</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>y</mml:mi><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo linebreak="goodbreak">=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>T</mml:mi><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>r</mml:mi></mml:msup></mml:mrow></mml:math></disp-formula></p>
        <p id="p0090">Focal Loss function contained a modulating factor (1 − <italic>p<sub>t</sub></italic>)<italic><sup>γ</sup></italic> to the cross-entropy loss, a focusing parameter <italic>γ</italic>, and a balancing variant <italic>α</italic> and was a guide for fine-tuning the hyperparameter:<disp-formula id="e0015"><mml:math id="M3" altimg="si3.svg"><mml:mrow><mml:mi>F</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo linebreak="goodbreak">=</mml:mo><mml:mo linebreak="badbreak">-</mml:mo><mml:mi>a</mml:mi><mml:msup><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mi>r</mml:mi></mml:msup><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math></disp-formula></p>
        <p id="p0095">Adam optimizer with an adaptive learning rate by cosine annealing optimized model performance during training (<xref rid="b0100" ref-type="bibr">Kingma and Ba, 2015</xref>). In addition, the highest score of Youden's Index of each model determined its threshold (<xref rid="b0160" ref-type="bibr">Ruopp et al., 2008</xref>).</p>
      </sec>
      <sec id="s0050">
        <label>2.4.4</label>
        <title>Model performance evaluation</title>
        <p id="p0100">Performance analyses were on different bases for the two stages: S1 performance was based on pixels, and S2 performance was based on patients. So first, we used the confusion matrix to evaluate S1 semantic segmentation including pixel accuracy, intersection-over-union (IoU), and Dice coefficient. Pixel accuracy was the percentage of the area of correct segmentation prediction over the ground truth, i.e., the pixel sum of TP and TN divided by the pixel sum of TP, FP, true negative (TN), and FN:<disp-formula id="e0020"><mml:math id="M4" altimg="si4.svg"><mml:mrow><mml:mi mathvariant="italic">Pixelaccuracy</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">areaofcorrectprediction</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">totalareaofprediction</mml:mi></mml:mrow></mml:mfrac><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
        <p id="p0105">IoU and Dice were defined as follows:<disp-formula id="e0025"><mml:math id="M5" altimg="si5.svg"><mml:mrow><mml:mi mathvariant="italic">IoU</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">areaofoverlap</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">areaofunion</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="e0030"><mml:math id="M6" altimg="si6.svg"><mml:mrow><mml:mi mathvariant="italic">Dice</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">totalareofpredictedsegmentationandgroundtruth</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
        <p id="p0110">Next, the performance comparison of S2 classifiers was evaluated by the area under the receiver operating characteristics (AUROC) and the area under the precision-recall curve (AUPRC) (<xref rid="b0165" ref-type="bibr">Saito et al., 2015</xref>).</p>
      </sec>
      <sec id="s0055">
        <label>2.4.5</label>
        <title>Data for development and testing of the SGD-Net</title>
        <p id="p0115">The dataset was chronologically separated into 80% images acquired from 2017 to early 2020 for model development and 20% images in late 2020 for testing. The test dataset contained separate images from different patients and did not overlap the development dataset for validation. In addition, the development dataset was further randomly divided into 80% for training and 20% for internal validation.</p>
      </sec>
      <sec id="s0060">
        <label>2.4.6</label>
        <title>SGD-Net plus</title>
        <p id="p0120">SGD-Net Plus was modified from SGD-Net by preserving the two-stage design and combining multimodal imaging to identify the anatomical spaces occupied by the AIS lesions (<xref rid="f0010" ref-type="fig">Fig. 2</xref>). The model first generated an AIS lesion SegMap from DWI by the S1 model and then registered the SegMap to standard space on T1W images and brain atlases. Because the S1 model had already completed segmentation from the DWI image and the standard space of the brain served as a platform for co-registration, the model allowed using multi-modal images with different dimensions and resolutions. After co-registration, the lesion mapping used known brain atlases as ground truth.<fig id="f0010"><label>Fig. 2</label><caption><p>Design of SGD-Net Plus. SGD-Net Plus connected the S1 of SGD-Net and serial multimodal imaging processes. The AIS lesion SegMap was projected back to the DWI images and co-registered with T1W images and brain atlases on standard space of the Montreal Neurological Institute (MNI). The model calculated the percentage of each brain region affected by the AIS lesions and the percentage of AIS lesion volume in each brain region. The brain atlases used in SGD-Net Plus were Brodmann Area (BA) for functional cortical segmentation, Johns Hopkins University (JHU) white matter atlas for white matter tract distribution, Anatomical Automated Anatomical Labeling 3 (AAL3), and Julich Brain Cytoarchitectonic Atlas for anatomical brain segmentation.</p></caption><graphic xlink:href="gr2"/></fig></p>
        <p id="p0125">The image registration process was applied as follows: (1) skull removal was applied on all T1W images. We used T1 magnetization prepared rapid gradient echo (MPRAGE) images in this study. The dimension of MPRAGE images was 256x256x176, the spacing was 0.9x0.9x1mm. (2) T1W images were co-registered to the corresponding DWIs using linear registration. (3) brain extracted T1W images were nonlinear registered and spatially normalized to the Montreal Neurological Institute (MNI) space. (4) by multiplying two transformation matrices from step 2 and 3, the atlases of Brodmann area (<xref rid="b0185" ref-type="bibr">Strotzer, 2009</xref>), Johns Hopkins University white matter atlas (<xref rid="b0140" ref-type="bibr">Mori et al., 2008</xref>), Automated Anatomical Labeling 3 (<xref rid="b0150" ref-type="bibr">Rolls et al., 2020</xref>), and Julich Brain Cytoarchitectonic Atlas (<xref rid="b0025" ref-type="bibr">Amunts et al., 2020</xref>) were backward transformed from standard MNI space to each individual's diffusion native space. (5) three lesion-related indices were generated: (a) how many lesion voxels were located in each brain region, (b) the number of voxels was converted into the percentage of lesion distribution in each brain region, and (c) the percentage of lesion voxels in each brain region. All of the above steps were achieved using FSL (Functional Magnetic Resonance Imaging of the Brain Software Library; <ext-link ext-link-type="uri" xlink:href="https://www.fmrib.ox.uk/fsl" id="ir015">https://www.fmrib.ox.uk/fsl</ext-link>).</p>
      </sec>
    </sec>
    <sec sec-type="data-availability" id="s0065">
      <label>2.5</label>
      <title>Data availability</title>
      <p id="p0130">The codes of SGD-Net and samples of AIS lesion segmentation and classification on DWI are available online (<ext-link ext-link-type="uri" xlink:href="https://github.com/IlikeBB/SGD-Net" id="ir020">https://github.com/IlikeBB/SGD-Net</ext-link>). The codes and demonstrations of SGD-Net Plus are available on (<ext-link ext-link-type="uri" xlink:href="https://github.com/IlikeBB/SGD-Plus" id="ir025">https://github.com/IlikeBB/SGD-Plus</ext-link>).</p>
    </sec>
  </sec>
  <sec id="s0070">
    <label>3</label>
    <title>Result</title>
    <sec id="s0075">
      <label>3.1</label>
      <title>Dataset</title>
      <p id="p0135">A total of 239 patients with 5095 DWI slices were enrolled for image review. Diagnostic checks excluded seven patients with non-AIS lesions. In addition, we excluded 11 patients with DWI-negative AIS (4.6%). Image quality checks further excluded five patients with images containing motion artifacts. All image labeling was agreed upon at the consensus meeting of the image review board. The final included image sets of 216 patients with 4606 slices were divided into two parts: the development dataset (80%) and the test datasets (20%) (<xref rid="f0015" ref-type="fig">Fig. 3</xref>).<fig id="f0015"><label>Fig. 3</label><caption><p>Data enrollment. Datasets were created by retrospective enrollment of patients diagnosed with AIS from 2017 to 2020. The AIS patients with brain MRI images for AIS were screened for eligibility. The final enrollment was divided chronologically into the development dataset (80%) and the test dataset (20%). The development dataset was further separated for training (80%) and validation (20%). For the classifier of the vascular territory, the image sets containing lesions in both anterior and posterior circulatory territories were excluded (5.7%, 3.2%, and 4.4% in training, validation, and testing data).</p></caption><graphic xlink:href="gr3"/></fig></p>
      <p id="p0140">The demographics, including age (66.32 ± 12.20 and 68.29 ± 12.58 years old, <italic>p</italic> = 0.338) and sex (male ratio 69.0% and 71.1%, <italic>p</italic> = 0.785), and the stroke-to-scan interval (4.46 ± 3.78 vs. 3.94 ± 3.61 days, <italic>p</italic> = 0.434) did not differ between the development and test datasets. A summary of the targets of interest for AIS lesion size revealed that the lacune and the non-lacune ratio was 35.1% to 55.6% (60 and 111 patients) in the development dataset and 55.3% to 46.7% (24 and 21 patients) in the test dataset. For the classes of lesion location, the anterior to posterior circulatory AIS ratio was 68.5% to 31.5% (111 and 51 patients) in the development dataset and 67.4% to 32.6% (29 and 14 patients) in the test dataset (<xref rid="t0010" ref-type="table">Table 2</xref>).<table-wrap position="float" id="t0010"><label>Table 2</label><caption><p>Basic information of development and test data set.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2">Variables</th><th>Development dataset (Train and validation)</th><th>Test dataset</th></tr></thead><tbody><tr><td colspan="2">Patient number</td><td>171</td><td>45</td></tr><tr><td colspan="2">Age, mean ± std</td><td>66.32 ± 12.20</td><td>68.29 ± 12.58</td></tr><tr><td colspan="2">Sex, male</td><td>118 (69.0%)</td><td>32 (71.1%)</td></tr><tr><td colspan="2">Year of stroke</td><td>2017-2020</td><td>2020</td></tr><tr><td rowspan="2">Lesion size</td><td>Lacune (≤ 20 mm)</td><td>60 (35.1%)</td><td>24 (55.3%)</td></tr><tr><td>Non-lacune (&gt; 20 mm)</td><td>111 (55.6%)</td><td>21 (46.7%)</td></tr><tr><td rowspan="2">Lesion location</td><td>Anterior circulatory territory</td><td>111 (68.5%)</td><td>29 (67.4%)</td></tr><tr><td>Posterior circulatory territory</td><td>51 (31.5%)</td><td>14 (32.6%)</td></tr></tbody></table><table-wrap-foot><fn><p>The development dataset was composed of the training and the validation datasets. Data in patient number (%). * <italic>p</italic> &lt; 0.05. <sup>†</sup> Images containing lesions in both anterior and posterior circulatory were excluded from the classification. Abbreviations: std, standard deviation.</p></fn></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="s0080">
      <label>3.2</label>
      <title>Performance of SGD-Net S1 segmentation</title>
      <p id="p0145">In S1 segmentation, three backbones performed comparably to segment AIS lesion accurately, with Dice coefficient 0.813 for DenseUNet121, 0.828 for ResUNet50, and 0.806 for VGGUNet16 (<xref rid="t0015" ref-type="table">Table 3</xref><bold>-A</bold>). Examples of the prediction of SegMap were shown in <xref rid="f0020" ref-type="fig">Fig. 4</xref>.<table-wrap position="float" id="t0015"><label>Table 3</label><caption><p>Performance of SGD-Net S1 segmentation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="4">A. Comparison of different backbones in SGD-Net S1 segmentation<hr/></th></tr><tr><th/><th>Dice</th><th>IoU</th><th>Accuracy</th></tr></thead><tbody><tr><td>DenseUNet121</td><td align="char">0.813</td><td align="char">0.685</td><td align="char">0.999</td></tr><tr><td>ResUNet50</td><td align="char">0.828</td><td align="char">0.707</td><td align="char">0.999</td></tr><tr><td>VGGUNet16</td><td align="char">0.806</td><td align="char">0.675</td><td align="char">0.999</td></tr><tr><td colspan="4">  </td></tr><tr><td colspan="4">B. Test for effects of lesion size on S1 segmentation (ResUNet50)</td></tr><tr><td>Small (&lt;769 pixels)</td><td align="char">0.761</td><td align="char">0.614</td><td align="char">0.999</td></tr><tr><td>Large (&gt;769 pixels)</td><td align="char">0.830</td><td align="char">0.710</td><td align="char">0.999</td></tr><tr><td colspan="4">  </td></tr><tr><td colspan="4">C. Test for effects of mean DWI intensity on S1 segmentation (ResUNet50)</td></tr><tr><td>Low (&lt;30.1)</td><td align="char">0.805</td><td align="char">0.674</td><td align="char">0.999</td></tr><tr><td>High (&gt;30.1)</td><td align="char">0.855</td><td align="char">0.747</td><td align="char">0.999</td></tr><tr><td colspan="4">  </td></tr><tr><td colspan="4">D. Test for effects of training sample size (N) on S1 segmentation (ResUNet50)</td></tr><tr><td>N = 20</td><td align="char">0.668</td><td align="char">0.502</td><td align="char">0.999</td></tr><tr><td>N = 50</td><td align="char">0.775</td><td align="char">0.633</td><td align="char">0.999</td></tr><tr><td>N = 100</td><td align="char">0.783</td><td align="char">0.643</td><td align="char">0.999</td></tr><tr><td>N = 140 (final enrollment)</td><td align="char">0.828</td><td align="char">0.707</td><td align="char">0.999</td></tr></tbody></table><table-wrap-foot><fn><p>N, sample size of train dataset. IoU, intersection-over-union.</p></fn></table-wrap-foot></table-wrap><fig id="f0020"><label>Fig. 4</label><caption><p>Demonstrations of SGD-Net S1 for segmentation of AIS lesions in DWI images. The four samples demonstrated the four classes of different combinations of lesion size and location. The demonstration was yielded by the SGD-Net S1 model with DenseUNet121 backbone. Ground truth was labeled by the image review board. The SegMap was the prediction by the SGD-Net S1 model. Abbreviations: GT, ground truth. SegMap: predicted segmentation map of AIS lesion.</p></caption><graphic xlink:href="gr4"/></fig></p>
      <p id="p0150">In addition, we examined if lesion size or raw DWI intensity affects model performance, using ResUNet50 as an example. The test data set was divided into two groups by the median of sum pixel or the median of mean DWI intensity of each patient. The median lesion size was 769 pixels (about 1.75 ml) in a resolution of 0.57x0.57 mm per pixel, 384x384 pixels per slice, and 7 mm thickness of a slice. The performance differences of ResUNet50 in patients with smaller (&lt;769 pixels) and larger lesions (&gt;769 pixels) were minor (Dice 0.761 vs. 0.830; IoU 0.614 vs. 0.710; accuracy 0.999 vs. 0.999) (<xref rid="t0015" ref-type="table">Table 3</xref><bold>-B</bold>). The median of each patient's average DWI intensity in the test dataset was 30.1. The performance in lower (&lt;30.1) and higher (&gt;30.1) DWI intensities was also similar (Dice 0.805 vs. 0.855; IoU 0.674 vs. 0.747; accuracy 0.999 vs. 0.999) (<xref rid="t0015" ref-type="table">Table 3</xref>-C). Besides, we already use z-score transformation on the raw DWI intensity in image preprocessing to reduce interpersonal (inter-scan) variance. Therefore, SGD-Net S1 worked steadily without considerable size and intensity effects.</p>
      <p id="p0155">We also tested if training sample size affected performance of S1 segmentation. The performance of ResUNet50 improved steadily with training sample sizes 20, 50, 100, to 140 (Dice: from 0.668 to 0.828; IoU: from 0.502 to 0.707, accuracy: from 0.999 to 0.999) (<xref rid="t0015" ref-type="table">Table 3</xref>-D). The segmentation performance seemed to be improving with currently available samples and has not yet reached a plateau. However, the accuracy was believed to be enough for us the continue the S2 classification task.</p>
    </sec>
    <sec id="s0085">
      <label>3.3</label>
      <title>Performance of SGD-Net classifier</title>
      <p id="p0160">Next, a comprehensive evaluation compared the performance of 11 classifiers, including nine different combinations of S1 and S2 backbones of SGD-Net and three traditional one-stage classifiers. For classification of lesion size, all the two-stage models (Model 1–9) performed well with an accuracy &gt; 0.86, AUROC &gt; 0.96, and AUPRC &gt; 0.96. To specify the best model, Model 1 (composition: S1 DenseUNet121 + S2 3D-ResNet18; performance: accuracy 0.956, AUROC 0.992, AUPRC 0.993) and Model 4 (composition: S1 ResUNet50 + S2 3D-ResNet18; performance: accuracy 0.956, AUROC 0.992, AUPRC 0.994) outperformed the others. In contrast, the one-stage classifiers (Models 10–11 and 3D-CNNs) were less outstanding with an AUROC 0.528–0.937 (<xref rid="f0025" ref-type="fig">Fig. 5</xref>).<fig id="f0025"><label>Fig. 5</label><caption><p>Model performance for classification of lacune and non-lacune stroke. Model performance was evaluated by accuracy, AUROC, and AUPRC. Two-stage models generally outperformed traditional one-stage models for classifying AIS lesion size as lacune and non-lacune (diameter ≤ and &gt; 20 mm). The bold labeled the best-performed models. Model 1 (S1 DenseUNet121 + S2 3D-ResNet18) and Model 4 (S1 ResUNet50 + S2 3D-ResNet18) were the best performer. In contrast, Model 12 (one-stage 3D-CNNs) fell behind other models in classifying lesion size, and its curve was not shown in the subgraph.</p></caption><graphic xlink:href="gr5"/></fig></p>
      <p id="p0165">In classifying the circulatory territory of AIS lesions, the two-stage models (Model 1–9) again outperformed the one-stage models (Model 10–11 and 3D-CNNs) (AUROC 0.936–0.988 vs. 0.493–0.833). For a detailed comparison of the nine SGD-Net models, Model 3 (composition: S1 DenseUNet121 + S2 3D-CNNs; performance: accuracy 0.907, AUROC 0.985, AUPRC 0.975) and Model 6 (composition: S1 ResUNet50 + S2 3D-CNNs; performance: accuracy 0.930, AUROC 0.988, AUPRC 0.978) were better than the others (<xref rid="f0030" ref-type="fig">Fig. 6</xref>).<fig id="f0030"><label>Fig. 6</label><caption><p>Model performance for classification of the anterior and posterior circulation. The two-stage models outperformed the one-stage models in classifying AIS lesion location in anterior or posterior circulatory territories. In addition, Model 6 (S1 ResUNet50 + S2 3D-CNNs) outperformed other models, as marked in bold. Model 12 (one-stage 3D-CNNs) performed inferior to other models in classifying circulatory territory (curve not shown in the subgraph).</p></caption><graphic xlink:href="gr6"/></fig></p>
    </sec>
    <sec id="s0090">
      <label>3.4</label>
      <title>SGD-Net plus for AIS lesion mapping</title>
      <p id="p0170">The multimodal image analysis identified the distribution of AIS lesions on brain atlases. After that, the SGD-Net Plus yielded reports of lesion volume, lesion percentage on each region, and brain region percentages occupied by the lesion (<xref rid="f0035" ref-type="fig">Fig. 7</xref>).<fig id="f0035"><label>Fig. 7</label><caption><p>Reports of SGD-Net Plus for AIS lesion distribution. The example of the SGD-Net Plus report was a patient with acute right anterior cerebral artery territory infarction. The predicated SegMap (red) overlapped the AIS lesion on DWI images and registered on T1 MPRAGE images. The SGD-Net Plus yielded the distributions of AIS lesions on each brain atlas. The figure only showed a few lines of the report of each atlas. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</p></caption><graphic xlink:href="gr7"/></fig></p>
    </sec>
    <sec id="s0095">
      <label>3.5</label>
      <title>User interface of SGD-net and SGD-net plus</title>
      <p id="p0175">The user interface (<xref rid="f0040" ref-type="fig">Fig. 8</xref>) combines the results of SGD-Net classification for AIS lesion size (lacune and non-lacune) and lesion location (anterior and posterior circulatory territory) and the reports of SGD-Net Plus for AIS lesion distribution. It also visualizes the original DWI images, the predicted AIS lesion SegMap from S1 model, and the red-marked AIS lesions on serial cuts of the brain atlases.<fig id="f0040"><label>Fig. 8</label><caption><p>SGD-Net and SGD-Net Plus for AIS lesion report. The user interface provided the results of SGD-Net classification for AIS lesion size (lacune and non-lacune) and lesion location (anterior and posterior circulatory territory) and the results of SGD-Net Plus for AIS lesion volume in each brain region, lesion percentage in that brain region, and region percentage occupied by the lesion. It also visualized the original DWI images, S1 segmentation, and serial cuts for the atlas with red-marked AIS lesions. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</p></caption><graphic xlink:href="gr8"/></fig></p>
    </sec>
  </sec>
  <sec id="s0100">
    <label>4</label>
    <title>Discussion</title>
    <sec id="s0105">
      <label>4.1</label>
      <title>Summary</title>
      <p id="p0180">To improve the MRI utilization in AIS by trainees, educators, students, and medical doctors who are not familiar with neuroimaging, we developed a two-stage deep learning model, SGD-Net, for the joint lesion segmentation and classification in DWI images. The two-stage models outperform the one-stage models in classifying AIS lesion size and location. The accurate segmentation in the U-shaped S1 network is the foundation of satisfactory overall performance. In addition, lesion-based classification preserves geometric and stereoscopic features of AIS lesions but avoids unnecessary learning on non-lesioned areas. It is more effective in classifying AIS than searching for important features in whole sets of images. Next, we created SGD-Net Plus by modifying the second stage of SGD-Net and applying multimodal imaging to calculate the distributions of AIS lesions on standard anatomical spaces. It provides clinical physicians valuable information to explain the patients' neurological deficits and cognitive damages.</p>
    </sec>
    <sec id="s0110">
      <label>4.2</label>
      <title>DWI images in AIS study</title>
      <p id="p0185">In AIS, cytotoxic edema caused by cellular ischemia is quickly reflected in the increase in DWI signals and a decrease in apparent diffusion coefficient (ADC) signals (<xref rid="b0020" ref-type="bibr">Albers, 1998</xref>). DWI and ADC detect AIS lesions earlier than T2W or FLAIR images (<xref rid="b0125" ref-type="bibr">Lovblad et al., 1998</xref>). Furthermore, DWI is superior to ADC in reflecting the final lesion of AIS (<xref rid="b0130" ref-type="bibr">Madai et al., 2014</xref>) and more sensitive and accurate than ADC for lesion segmentation in machine learning (<xref rid="b0220" ref-type="bibr">Winzeck et al., 2019</xref>). Therefore, DWI is a promising image modality for developing image-based applications in AIS.</p>
      <p id="p0190">Previous machine learning studies developed auto-segmentation of AIS lesions on DWI images using CNNs-based models. Chen et al. used the ensembled model to segment and re-evaluate the extracted lesion area to reduce the false-positive rate and reach an acceptable performance (Dice 0.67). However, lesion size significantly affected model performance with Dice 0.61 for small and 0.83 for large lesions (<xref rid="b0045" ref-type="bibr">Chen et al., 2017</xref>). Later, Woo et al. published the CNNs-based models with backbones of U-Net and DenseUNet. It proved that DWI-alone performed better than conventional DWI-ADC methods for AIS lesion segmentation (Dice 0.85 vs. 0.58) (<xref rid="b0225" ref-type="bibr">Woo et al., 2019</xref>). In addition, Winzeck et al. compared DWI, ADC, DWI + ADC, and DWI + ADC + low-b diffusion images in CNNs for AIS lesion segmentation. In that study, DWI-only (Dice 0.723) outperformed ADC-only (Dice 0.564) and performed comparably with other combinations (Dice 0.756–0.789) (<xref rid="b0220" ref-type="bibr">Winzeck et al., 2019</xref>). In our study, the SGD-Net S1 model with different backboned segmented AIS lesions used DWI-only and performed equally or better than previous studies (Dice 0.806–0.828 vs. 0.67–0.86) (<xref rid="b0045" ref-type="bibr">Chen et al., 2017</xref>, <xref rid="b0220" ref-type="bibr">Winzeck et al., 2019</xref>, <xref rid="b0225" ref-type="bibr">Woo et al., 2019</xref>). Furthermore, unlike previous studies that found significant impacts on model performance by small lesion size and low DWI intensity (<xref rid="b0045" ref-type="bibr">Chen et al., 2017</xref>, <xref rid="b0220" ref-type="bibr">Winzeck et al., 2019</xref>), SGD-Net S1 segmentation performed steadily in conditions with either small/large lesions and low/high DWI intensities. The strategies to keep good image quality might be the reasons for this success. They were (1) enrolling acute stage images and not considering chronic stage images to prevent fading of water-restricted lesions on DWI and (2) using 3 T but not 1.5 T MRI images to promise a good signal–noise ratio. Therefore, excellent performance in segmentation is the basis for subsequent classification and lesion mapping.</p>
    </sec>
    <sec id="s0115">
      <label>4.3</label>
      <title>SGD-net improved stroke classification</title>
      <p id="p0195">The novelty of this study is the merging of segmentation and classification into one model. The merging avoids learning clinically meaningless regions and has also been proven to improve classifiers' performance extensively in previous studies. For example, the segmentation-based deep fusion network (SDFN) first identified lung regions and then classified thoracic diseases based on segmentation knowledge; its AUROC of 0.815 was better than learning on an entire chest radiograph (AUROC 0.804) (<xref rid="b0120" ref-type="bibr">Liu et al., 2019</xref>). Another case was the hierarchical classification based on the segmented brain hemorrhage lesions in computer tomography images, which reached an excellent accuracy of 94% (<xref rid="b0175" ref-type="bibr">Shahangian and Pourghassem, 2016</xref>). Therefore, lesion-based classification is a promising method to replace traditional pixel-based classification in medical imaging analysis.</p>
      <p id="p0200">During the model development of SGD-Net, we found that the precision of segmentation in the first stage is crucial for correct classification in the successive stage. U-Net is a well-known fully convolutional network developed in 2015 to segment the limited annotated biomedical image efficiently (<xref rid="b0155" ref-type="bibr">Ronneberger et al., 2015</xref>). The U-shaped architecture contained one symmetric contracting path and another expanding path to ensure precision encoding and decoding to preserve essential features in medical images. Because the composition of medical images is relatively concrete and straightforward and most of the features in medical images are essential, the U-shaped architecture is suitable for preserving basic and semantic features without losing necessary information. In addition, complex models in limited data can easily lead to overfitting problems. In this situation, using a simple architected U-Net can avoid the overfitting problem and achieve adequate performance.</p>
      <p id="p0205">However, precise segmentation of single slices of brain images was not sufficient to classify AIS lesions accurately. Our design of two-stage models simulated clinicians' image interpretation sequences that first focus on identifying lesions, recognizing the topologic and radiological features of the lesioned area, and then considering its spatial correlations to the whole brain (<xref rid="b0010" ref-type="bibr">Adam et al., 2020</xref>). Thus, the S2 of SGD-Net emphasized the 3D stereoscopic features of AIS lesions by bundling DWI images of each patient. Mastery of these principles, even models with a basic structure, such as 3D-ResNet18, performed non-inferiorly to other deep layered models.</p>
      <p id="p0210">We also observed that the best S1/S2 combination differed for lesion size and vascular territory. 3D-ResNet18 is the top choice for classifying lacunes and non-lacunes, whereas 3D-CNNs are superior for anterior and posterior circulation classifications. We postulate that determining the size of targets does not mandate large, complicated networks such as 3D-ResNet50 and 3D-CNNs but only needs to stand on precisely segmented masks and classify them with simple, effective networks (such as 3D-ResNet18) to extract undistorted information (<xref rid="b0080" ref-type="bibr">He et al., 2016</xref>). For detecting lesion location, the architecture of 3D-CNNs is advantageous for capturing spatial information, akin to the sequential graphic information of serial images. The 3D-CNNs backbone we used in this study was designed to detect tuberculosis in chest CT (<xref rid="b0245" ref-type="bibr">Zunair et al., 2020</xref>); their simulation environment was similar to our setting, to detect solid lesions with either a single condensed, a group of satellite-like lesions, or multiple sparsely distributed nature. Therefore, goal-oriented programming based on our domain knowledge improves AI performances, and that is why 3D-CNNs-composited SGD-Net outperformed in the stereoscopic classification of AIS lesions.</p>
    </sec>
    <sec id="s0120">
      <label>4.4</label>
      <title>Clinical utilities of SGD-Net plus</title>
      <p id="p0215">The initiation of developing SGD-Net Plus was based on neurologists' clinical needs for knowing the exact damages to functional or anatomical structures of the brain. In addition, multiple factors may cloud the presentations of AIS patients. For example, medical complications like infection process, abnormal blood sugar level, imbalanced electrolyte state, limb pain, or old age can increase bedside examination difficulties and mask the patients' neurological deficits (<xref rid="b0105" ref-type="bibr">Kumar et al., 2010</xref>). In these situations, the report of SGD-Net Plus hints to the clinical physicians about potential neurologic sequelae and cognitive impairment after the acute stage of stroke. More than that, some post-stroke cognitive impairments are the distant effects of white matter tract damage and can be localized by the SGD-Net Plus (<xref rid="b0200" ref-type="bibr">Wang et al., 2016</xref>). Physicians can be more alert to the development of post-stroke psychiatric symptoms (e.g., post-stroke depression) by knowing the damages to related brain areas and circuits (e.g., the left limbic-cortical-striatal-pallidal-thalamic circuit and its connected left amygdala and cingulum) (<xref rid="b0195" ref-type="bibr">Terroni et al., 2011</xref>). From previous experiences, we expect future deployment of SGD-Net Plus to provide precise localization of AIS. In addition, the deep learning-assisted image-based AIS lesions analysis will help advanced evaluate stroke lesion damages and offer references to personalized post-stroke rehabilitation.</p>
      <p id="p0220">Besides, SGD-Net Plus can be a valuable medium for neuroimage education. When evaluating a stroke patient, the trainees will benefit from referencing the patient's clinical presentations to the wealthy information from the precision lesion localization in SGD-Net Plus. An advanced case analysis can efficiently bring the maximal professional skill gaining for single case studies. Integrating AI and clinical knowledge is a challenge for medical education (<xref rid="b0040" ref-type="bibr">Chan and Zary, 2019</xref>), and a framework for stepwise education in professional development may shape the new generation to integrate and utilize AI-assisted information effectively (<xref rid="b0145" ref-type="bibr">Paranjape et al., 2019</xref>). Domain knowledge is the way to unlock the full potential of AI tools (<xref rid="b0135" ref-type="bibr">Masters, 2019</xref>). Indeed, practical orientation is the original intention behind the creation of SGD-Net Plus. When classical neurology collects symptoms and signs to localize lesions in the brain, SGD-Net Plus applies retrograde engineering by first knowing the lesion occupancy in the brain, degrading the affected regions and tracts, and then projecting to the patients' manifestations. Therefore, SGD-Net Plus strengthens and deepens the clinical-anatomical connections and generalizes brain MRI to general users. Its deployment will be helpful to professional educators, medical trainees, clinical physicians, and those who want to learn AIS lesion information from MRI images.</p>
    </sec>
    <sec id="s0125">
      <label>4.5</label>
      <title>Limitations</title>
      <p id="p0225">There are several limitations to this study. First, although we have used chronologically separated datasets for validation, we did lack external validation datasets from different sites and different patient populations. Stroke patients in different geographic sites may present with varied stroke patterns because of demographic factors, genetic variance, and lifestyle. In addition, we did not test SGD-Net on datasets acquired from different MRI machines in the current study. The current dataset was collected from the same 3 T MRI machines and could not verify whether machine effects affect SGD-Net performance. We do not know if the model would be stable enough on images from 1.5 T MRI machines, which are still prevalent in clinical services. Therefore, multi-site validation is warranted to test the generalizability and stability of the SGD-Net.</p>
      <p id="p0230">Second, DWI had its resolution limitations to show small lesions, multiple scattered lesions, or low DWI intensity lesions. These imaging process-related restrictions would affect machine learning model performance. To reduce these confounding factors, we used images from 3 T MRI that produced higher quality in imaging production than 1.5 T MRI. Some other studies used integrative analysis of DWI and ADC images for AIS lesion segmentation. However, combining DWI and ADC did not improve AIS lesion segmentation much more than DWI alone (<xref rid="b0220" ref-type="bibr">Winzeck et al., 2019</xref>). Therefore, we used 3 T MRI DWI alone as an acceptable modality for AIS lesion segmentation.</p>
      <p id="p0235">Third, we excluded patients with lesions in both anterior and posterior circulatory territories in location classification. Because of the low prevalence of patients with both territories in our dataset (N = 11/216, 5.1%, in <xref rid="f0015" ref-type="fig">Fig. 3</xref>), we excluded them from the current location classification study. However, a multi-class classifier is warranted for multiple-lesion localization for practical utility in the future.</p>
      <p id="p0240">Finally, we tested the SGD-Net and SGD-Net Plus only on retrospective datasets. Whether these tools improve real-world practice requires clinical deployment to evaluate their efficiencies in supporting clinical decision-making.</p>
    </sec>
  </sec>
  <sec id="s0130">
    <label>5</label>
    <title>Conclusions</title>
    <p id="p0245">Combining automatic segmentation and classification of AIS is a developing field for applying deep learning in medical care. These applications are promising and may assist clinicians with interpreting brain MRI images in AIS patients and supporting clinical decision-making. SGD-Net for joint segmentation and classification of AIS lesions outperforms traditional 3D models for classifying AIS lesion size and location in DWI images. SGD-Net Plus, the modification of SGD-Net, realizes the precision localization of AIS lesions on standard brain atlases and provides informative reports to clinical physicians. Therefore, the auto-classifier can help those non-expert users to get information about AIS patients from MRI images. Future deployment of SGD-Net and SGD-Net Plus in medical care systems is promising to improve the quality of AIS patient care.</p>
    <sec id="s0135">
      <title>CRediT authorship contribution statement</title>
      <p id="p0250"><bold>Yi-Chia Wei:</bold> Conceptualization, Validation, Investigation, Data curation, Writing – original draft, Visualization, Project administration, Funding acquisition. <bold>Wen-Yi Huang:</bold> Project administration, Funding acquisition. <bold>Chih-Yu Jian:</bold> Methodology, Software, Formal analysis, Data curation, Visualization. <bold>Chih-Chin Heather Hsu:</bold> Conceptualization, Software, Visualization, Writing – original draft. <bold>Chih-Chung Hsu:</bold> Methodology, Validation. <bold>Ching-Po Lin:</bold> Resources, Supervision. <bold>Chi-Tung Cheng:</bold> Methodology, Supervision. <bold>Yao-Liang Chen:</bold> Data curation. <bold>Hung-Yu Wei:</bold> Methodology, Supervision. <bold>Kuan-Fu Chen:</bold> Conceptualization, Methodology, Validation, Investigation, Writing – review &amp; editing, Project administration, Funding acquisition.</p>
    </sec>
  </sec>
  <sec sec-type="COI-statement">
    <title>Declaration of Competing Interest</title>
    <p id="p0255">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
  </sec>
</body>
<back>
  <ref-list id="bi005">
    <title>References</title>
    <ref id="b0005">
      <mixed-citation publication-type="other" id="h0005">Abraham, N., Khan, N.M., 2019. A novel focal tversky loss function with improved attention u-net for lesion segmentation. 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019). IEEE, pp. 683–687.</mixed-citation>
    </ref>
    <ref id="b0010">
      <mixed-citation publication-type="other" id="h0010">Adam, A., Dixon, A.K., Gillard, J.H., Schaefer-Prokop, C., 2020. Grainger &amp; Allison's Diagnostic Radiology, 2 Volume Set E-Book. Elsevier Health Sciences.</mixed-citation>
    </ref>
    <ref id="b0015">
      <element-citation publication-type="journal" id="h0015">
        <person-group person-group-type="author">
          <name>
            <surname>Akkus</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Galimzianova</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Hoogi</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Rubin</surname>
            <given-names>D.L.</given-names>
          </name>
          <name>
            <surname>Erickson</surname>
            <given-names>B.J.</given-names>
          </name>
        </person-group>
        <article-title>Deep learning for brain MRI segmentation: state of the art and future directions</article-title>
        <source>J. Digit. Imaging</source>
        <volume>30</volume>
        <issue>4</issue>
        <year>2017</year>
        <fpage>449</fpage>
        <lpage>459</lpage>
        <pub-id pub-id-type="pmid">28577131</pub-id>
      </element-citation>
    </ref>
    <ref id="b0020">
      <mixed-citation publication-type="other" id="h0020">Albers, G.W., 1998. Diffusion-weighted MRI for evaluation of acute stroke. Neurology 51, S47-49.</mixed-citation>
    </ref>
    <ref id="b0025">
      <element-citation publication-type="journal" id="h0025">
        <person-group person-group-type="author">
          <name>
            <surname>Amunts</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Mohlberg</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Bludau</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Zilles</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Julich-Brain: A 3D probabilistic atlas of the human brain's cytoarchitecture</article-title>
        <source>Science</source>
        <volume>369</volume>
        <issue>6506</issue>
        <year>2020</year>
        <fpage>988</fpage>
        <lpage>992</lpage>
        <pub-id pub-id-type="pmid">32732281</pub-id>
      </element-citation>
    </ref>
    <ref id="b0030">
      <element-citation publication-type="journal" id="h0030">
        <person-group person-group-type="author">
          <name>
            <surname>Arsava</surname>
            <given-names>E.M.</given-names>
          </name>
          <name>
            <surname>Ballabio</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Benner</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Cole</surname>
            <given-names>J.W.</given-names>
          </name>
          <name>
            <surname>Delgado-Martinez</surname>
            <given-names>M.P.</given-names>
          </name>
          <name>
            <surname>Dichgans</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Fazekas</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Furie</surname>
            <given-names>K.L.</given-names>
          </name>
          <name>
            <surname>Illoh</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Jood</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Kittner</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Lindgren</surname>
            <given-names>A.G.</given-names>
          </name>
          <name>
            <surname>Majersik</surname>
            <given-names>J.J.</given-names>
          </name>
          <name>
            <surname>Macleod</surname>
            <given-names>M.J.</given-names>
          </name>
          <name>
            <surname>Meurer</surname>
            <given-names>W.J.</given-names>
          </name>
          <name>
            <surname>Montaner</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Olugbodi</surname>
            <given-names>A.A.</given-names>
          </name>
          <name>
            <surname>Pasdar</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Redfors</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Schmidt</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Sharma</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Singhal</surname>
            <given-names>A.B.</given-names>
          </name>
          <name>
            <surname>Sorensen</surname>
            <given-names>A.G.</given-names>
          </name>
          <name>
            <surname>Sudlow</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Thijs</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Worrall</surname>
            <given-names>B.B.</given-names>
          </name>
          <name>
            <surname>Rosand</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Ay</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>The Causative Classification of Stroke system: an international reliability and optimization study</article-title>
        <source>Neurology</source>
        <volume>75</volume>
        <issue>14</issue>
        <year>2010</year>
        <fpage>1277</fpage>
        <lpage>1284</lpage>
        <pub-id pub-id-type="pmid">20921513</pub-id>
      </element-citation>
    </ref>
    <ref id="b0035">
      <element-citation publication-type="journal" id="h0035">
        <person-group person-group-type="author">
          <name>
            <surname>Buslaev</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Iglovikov</surname>
            <given-names>V.I.</given-names>
          </name>
          <name>
            <surname>Khvedchenya</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Parinov</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Druzhinin</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kalinin</surname>
            <given-names>A.A.</given-names>
          </name>
        </person-group>
        <article-title>Albumentations: fast and flexible image augmentations</article-title>
        <source>Information</source>
        <volume>11</volume>
        <year>2020</year>
        <fpage>125</fpage>
      </element-citation>
    </ref>
    <ref id="b0040">
      <mixed-citation publication-type="other" id="h0040">Chan, K.S., Zary, N., 2019. Applications and Challenges of Implementing Artificial Intelligence in Medical Education: Integrative Review. JMIR Med Educ 5, e13930.</mixed-citation>
    </ref>
    <ref id="b0045">
      <element-citation publication-type="journal" id="h0045">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Bentley</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Rueckert</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>Fully automatic acute ischemic lesion segmentation in DWI using convolutional neural networks</article-title>
        <source>Neuroimage Clin</source>
        <volume>15</volume>
        <year>2017</year>
        <fpage>633</fpage>
        <lpage>643</lpage>
        <pub-id pub-id-type="pmid">28664034</pub-id>
      </element-citation>
    </ref>
    <ref id="b0050">
      <mixed-citation publication-type="other" id="h0050">Collaborators, G.B.D.L.R.o.S., Feigin, V.L., Nguyen, G., Cercy, K., Johnson, C.O., Alam, T., Parmar, P.G., Abajobir, A.A., Abate, K.H., Abd-Allah, F., Abejie, A.N., Abyu, G.Y., Ademi, Z., Agarwal, G., Ahmed, M.B., Akinyemi, R.O., Al-Raddadi, R., Aminde, L.N., Amlie-Lefond, C., Ansari, H., Asayesh, H., Asgedom, S.W., Atey, T.M., Ayele, H.T., Banach, M., Banerjee, A., Barac, A., Barker-Collo, S.L., Barnighausen, T., Barregard, L., Basu, S., Bedi, N., Behzadifar, M., Bejot, Y., Bennett, D.A., Bensenor, I.M., Berhe, D.F., Boneya, D.J., Brainin, M., Campos-Nonato, I.R., Caso, V., Castaneda-Orjuela, C.A., Rivas, J.C., Catala-Lopez, F., Christensen, H., Criqui, M.H., Damasceno, A., Dandona, L., Dandona, R., Davletov, K., de Courten, B., deVeber, G., Dokova, K., Edessa, D., Endres, M., Faraon, E.J.A., Farvid, M.S., Fischer, F., Foreman, K., Forouzanfar, M.H., Gall, S.L., Gebrehiwot, T.T., Geleijnse, J.M., Gillum, R.F., Giroud, M., Goulart, A.C., Gupta, R., Gupta, R., Hachinski, V., Hamadeh, R.R., Hankey, G.J., Hareri, H.A., Havmoeller, R., Hay, S.I., Hegazy, M.I., Hibstu, D.T., James, S.L., Jeemon, P., John, D., Jonas, J.B., Jozwiak, J., Kalani, R., Kandel, A., Kasaeian, A., Kengne, A.P., Khader, Y.S., Khan, A.R., Khang, Y.H., Khubchandani, J., Kim, D., Kim, Y.J., Kivimaki, M., Kokubo, Y., Kolte, D., Kopec, J.A., Kosen, S., Kravchenko, M., Krishnamurthi, R., Kumar, G.A., Lafranconi, A., Lavados, P.M., Legesse, Y., Li, Y., Liang, X., Lo, W.D., Lorkowski, S., Lotufo, P.A., Loy, C.T., Mackay, M.T., Abd El Razek, H.M., Mahdavi, M., Majeed, A., Malekzadeh, R., Malta, D.C., Mamun, A.A., Mantovani, L.G., Martins, S.C.O., Mate, K.K., Mazidi, M., Mehata, S., Meier, T., Melaku, Y.A., Mendoza, W., Mensah, G.A., Meretoja, A., Mezgebe, H.B., Miazgowski, T., Miller, T.R., Ibrahim, N.M., Mohammed, S., Mokdad, A.H., Moosazadeh, M., Moran, A.E., Musa, K.I., Negoi, R.I., Nguyen, M., Nguyen, Q.L., Nguyen, T.H., Tran, T.T., Nguyen, T.T., Anggraini Ningrum, D.N., Norrving, B., Noubiap, J.J., O'Donnell, M.J., Olagunju, A.T., Onuma, O.K., Owolabi, M.O., Parsaeian, M., Patton, G.C., Piradov, M., Pletcher, M.A., Pourmalek, F., Prakash, V., Qorbani, M., Rahman, M., Rahman, M.A., Rai, R.K., Ranta, A., Rawaf, D., Rawaf, S., Renzaho, A.M., Robinson, S.R., Sahathevan, R., Sahebkar, A., Salomon, J.A., Santalucia, P., Santos, I.S., Sartorius, B., Schutte, A.E., Sepanlou, S.G., Shafieesabet, A., Shaikh, M.A., Shamsizadeh, M., Sheth, K.N., Sisay, M., Shin, M.J., Shiue, I., Silva, D.A.S., Sobngwi, E., Soljak, M., Sorensen, R.J.D., Sposato, L.A., Stranges, S., Suliankatchi, R.A., Tabares-Seisdedos, R., Tanne, D., Nguyen, C.T., Thakur, J.S., Thrift, A.G., Tirschwell, D.L., Topor-Madry, R., Tran, B.X., Nguyen, L.T., Truelsen, T., Tsilimparis, N., Tyrovolas, S., Ukwaja, K.N., Uthman, O.A., Varakin, Y., Vasankari, T., Venketasubramanian, N., Vlassov, V.V., Wang, W., Werdecker, A., Wolfe, C.D.A., Xu, G., Yano, Y., Yonemoto, N., Yu, C., Zaidi, Z., El Sayed Zaki, M., Zhou, M., Ziaeian, B., Zipkin, B., Vos, T., Naghavi, M., Murray, C.J.L., Roth, G.A., 2018. Global, Regional, and Country-Specific Lifetime Risks of Stroke, 1990 and 2016. N Engl J Med 379, 2429-2437.</mixed-citation>
    </ref>
    <ref id="b0055">
      <element-citation publication-type="journal" id="h0055">
        <person-group person-group-type="author">
          <name>
            <surname>Davis</surname>
            <given-names>D.P.</given-names>
          </name>
          <name>
            <surname>Robertson</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Imbesi</surname>
            <given-names>S.G.</given-names>
          </name>
        </person-group>
        <article-title>Diffusion-weighted magnetic resonance imaging versus computed tomography in the diagnosis of acute ischemic stroke</article-title>
        <source>J. Emerg. Med.</source>
        <volume>31</volume>
        <issue>3</issue>
        <year>2006</year>
        <fpage>269</fpage>
        <lpage>277</lpage>
        <pub-id pub-id-type="pmid">16982360</pub-id>
      </element-citation>
    </ref>
    <ref id="b0060">
      <element-citation publication-type="journal" id="h0060">
        <person-group person-group-type="author">
          <name>
            <surname>Diakogiannis</surname>
            <given-names>F.I.</given-names>
          </name>
          <name>
            <surname>Waldner</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Caccetta</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>ResUNet-a: A deep learning framework for semantic segmentation of remotely sensed data</article-title>
        <source>ISPRS J. Photogramm. Remote Sens.</source>
        <volume>162</volume>
        <year>2020</year>
        <fpage>94</fpage>
        <lpage>114</lpage>
      </element-citation>
    </ref>
    <ref id="b0065">
      <element-citation publication-type="journal" id="h0065">
        <person-group person-group-type="author">
          <name>
            <surname>Feigin</surname>
            <given-names>V.L.</given-names>
          </name>
          <name>
            <surname>Norrving</surname>
            <given-names>B.o.</given-names>
          </name>
          <name>
            <surname>Mensah</surname>
            <given-names>G.A.</given-names>
          </name>
        </person-group>
        <article-title>Global burden of stroke</article-title>
        <source>Circ. Res.</source>
        <volume>120</volume>
        <issue>3</issue>
        <year>2017</year>
        <fpage>439</fpage>
        <lpage>448</lpage>
        <pub-id pub-id-type="pmid">28154096</pub-id>
      </element-citation>
    </ref>
    <ref id="b0070">
      <element-citation publication-type="journal" id="h0070">
        <person-group person-group-type="author">
          <name>
            <surname>Ghosh</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Chaki</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Santosh</surname>
            <given-names>K.C.</given-names>
          </name>
        </person-group>
        <article-title>Improved U-Net architecture with VGG-16 for brain tumor segmentation</article-title>
        <source>Phys. Eng. Sci. Med.</source>
        <volume>44</volume>
        <issue>3</issue>
        <year>2021</year>
        <fpage>703</fpage>
        <lpage>712</lpage>
        <pub-id pub-id-type="pmid">34047928</pub-id>
      </element-citation>
    </ref>
    <ref id="b0075">
      <element-citation publication-type="book" id="h0075">
        <person-group person-group-type="author">
          <name>
            <surname>Hara</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Kataoka</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Satoh</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <part-title>Learning spatio-temporal features with 3d residual networks for action recognition</part-title>
        <source>Proceedings of the IEEE International Conference on Computer Vision Workshops</source>
        <year>2017</year>
        <fpage>3154</fpage>
        <lpage>3160</lpage>
      </element-citation>
    </ref>
    <ref id="b0080">
      <element-citation publication-type="book" id="h0080">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <part-title>Deep residual learning for image recognition</part-title>
        <source>Proceedings of the IEEE conference on computer vision and pattern recognition</source>
        <year>2016</year>
        <fpage>770</fpage>
        <lpage>778</lpage>
      </element-citation>
    </ref>
    <ref id="b0085">
      <element-citation publication-type="book" id="h0085">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Van Der Maaten</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Weinberger</surname>
            <given-names>K.Q.</given-names>
          </name>
        </person-group>
        <part-title>Densely connected convolutional networks</part-title>
        <source>Proceedings of the IEEE conference on computer vision and pattern recognition</source>
        <year>2017</year>
        <fpage>4700</fpage>
        <lpage>4708</lpage>
      </element-citation>
    </ref>
    <ref id="b0090">
      <element-citation publication-type="journal" id="h0090">
        <person-group person-group-type="author">
          <name>
            <surname>Kabir</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Dojat</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Scherrer</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Forbes</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Garbay</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>Multimodal MRI segmentation of ischemic stroke lesions</article-title>
        <source>Annu. Int. Conf. IEEE Eng. Med. Biol. Soc.</source>
        <volume>2007</volume>
        <year>2007</year>
        <fpage>1595</fpage>
        <lpage>1598</lpage>
        <pub-id pub-id-type="pmid">18002276</pub-id>
      </element-citation>
    </ref>
    <ref id="b0095">
      <element-citation publication-type="journal" id="h0095">
        <person-group person-group-type="author">
          <name>
            <surname>Kellner</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Reisert</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kiselev</surname>
            <given-names>V.G.</given-names>
          </name>
          <name>
            <surname>Maurer</surname>
            <given-names>C.J.</given-names>
          </name>
          <name>
            <surname>Urbach</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Egger</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Comparison of automated and visual DWI ASPECTS in acute ischemic stroke</article-title>
        <source>J. Neuroradiol.</source>
        <volume>46</volume>
        <issue>5</issue>
        <year>2019</year>
        <fpage>288</fpage>
        <lpage>293</lpage>
        <pub-id pub-id-type="pmid">30862461</pub-id>
      </element-citation>
    </ref>
    <ref id="b0100">
      <element-citation publication-type="book" id="h0100">
        <person-group person-group-type="author">
          <name>
            <surname>Kingma</surname>
            <given-names>D.P.</given-names>
          </name>
          <name>
            <surname>Ba</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <part-title>Adam: A method for stochastic optimization</part-title>
        <source>Proc. of the 3rd International Conference for Learning Representations (ICLR)</source>
        <year>2015</year>
      </element-citation>
    </ref>
    <ref id="b0105">
      <element-citation publication-type="journal" id="h0105">
        <person-group person-group-type="author">
          <name>
            <surname>Kumar</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Selim</surname>
            <given-names>M.H.</given-names>
          </name>
          <name>
            <surname>Caplan</surname>
            <given-names>L.R.</given-names>
          </name>
        </person-group>
        <article-title>Medical complications after stroke</article-title>
        <source>Lancet Neurol.</source>
        <volume>9</volume>
        <issue>1</issue>
        <year>2010</year>
        <fpage>105</fpage>
        <lpage>118</lpage>
        <pub-id pub-id-type="pmid">20083041</pub-id>
      </element-citation>
    </ref>
    <ref id="b0110">
      <element-citation publication-type="journal" id="h0110">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Qi</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Dou</surname>
            <given-names>Q.i.</given-names>
          </name>
          <name>
            <surname>Fu</surname>
            <given-names>C.-W.</given-names>
          </name>
          <name>
            <surname>Heng</surname>
            <given-names>P.-A.</given-names>
          </name>
        </person-group>
        <article-title>H-DenseUNet: hybrid densely connected UNet for liver and tumor segmentation from CT volumes</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <volume>37</volume>
        <issue>12</issue>
        <year>2018</year>
        <fpage>2663</fpage>
        <lpage>2674</lpage>
        <pub-id pub-id-type="pmid">29994201</pub-id>
      </element-citation>
    </ref>
    <ref id="b0115">
      <element-citation publication-type="journal" id="h0115">
        <person-group person-group-type="author">
          <name>
            <surname>Lin</surname>
            <given-names>T.-Y.</given-names>
          </name>
          <name>
            <surname>Goyal</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Girshick</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Dollar</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Focal loss for dense object detection</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <volume>42</volume>
        <issue>2</issue>
        <year>2020</year>
        <fpage>318</fpage>
        <lpage>327</lpage>
        <pub-id pub-id-type="pmid">30040631</pub-id>
      </element-citation>
    </ref>
    <ref id="b0120">
      <element-citation publication-type="journal" id="h0120">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Nan</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Jin</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Pu</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>SDFN: Segmentation-based deep fusion network for thoracic disease classification in chest X-ray images</article-title>
        <source>Comput. Med. Imaging Graph.</source>
        <volume>75</volume>
        <year>2019</year>
        <fpage>66</fpage>
        <lpage>73</lpage>
        <pub-id pub-id-type="pmid">31174100</pub-id>
      </element-citation>
    </ref>
    <ref id="b0125">
      <element-citation publication-type="journal" id="h0125">
        <person-group person-group-type="author">
          <name>
            <surname>Lovblad</surname>
            <given-names>K.O.</given-names>
          </name>
          <name>
            <surname>Laubach</surname>
            <given-names>H.J.</given-names>
          </name>
          <name>
            <surname>Baird</surname>
            <given-names>A.E.</given-names>
          </name>
          <name>
            <surname>Curtin</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Schlaug</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Edelman</surname>
            <given-names>R.R.</given-names>
          </name>
          <name>
            <surname>Warach</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Clinical experience with diffusion-weighted MR in patients with acute stroke</article-title>
        <source>AJNR Am. J. Neuroradiol.</source>
        <volume>19</volume>
        <year>1998</year>
        <fpage>1061</fpage>
        <lpage>1066</lpage>
        <pub-id pub-id-type="pmid">9672012</pub-id>
      </element-citation>
    </ref>
    <ref id="b0130">
      <mixed-citation publication-type="other" id="h0130">Madai, V.I., Galinovic, I., Grittner, U., Zaro-Weber, O., Schneider, A., Martin, S.Z., von Samson-Himmelstjerna, F.C., Stengl, K.L., Mutke, M.A., Moeller-Hartmann, W., Ebinger, M., Fiebach, J.B., Sobesky, J., 2014. DWI intensity values predict FLAIR lesions in acute ischemic stroke. PLoS One 9, e92295.</mixed-citation>
    </ref>
    <ref id="b0135">
      <element-citation publication-type="journal" id="h0135">
        <person-group person-group-type="author">
          <name>
            <surname>Masters</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Artificial intelligence in medical education</article-title>
        <source>Med. Teach.</source>
        <volume>41</volume>
        <issue>9</issue>
        <year>2019</year>
        <fpage>976</fpage>
        <lpage>980</lpage>
        <pub-id pub-id-type="pmid">31007106</pub-id>
      </element-citation>
    </ref>
    <ref id="b0140">
      <element-citation publication-type="journal" id="h0140">
        <person-group person-group-type="author">
          <name>
            <surname>Mori</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Oishi</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>L.i.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Akhter</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Hua</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Faria</surname>
            <given-names>A.V.</given-names>
          </name>
          <name>
            <surname>Mahmood</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Woods</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Toga</surname>
            <given-names>A.W.</given-names>
          </name>
          <name>
            <surname>Pike</surname>
            <given-names>G.B.</given-names>
          </name>
          <name>
            <surname>Neto</surname>
            <given-names>P.R.</given-names>
          </name>
          <name>
            <surname>Evans</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Miller</surname>
            <given-names>M.I.</given-names>
          </name>
          <name>
            <surname>van Zijl</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Mazziotta</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Stereotaxic white matter atlas based on diffusion tensor imaging in an ICBM template</article-title>
        <source>Neuroimage</source>
        <volume>40</volume>
        <issue>2</issue>
        <year>2008</year>
        <fpage>570</fpage>
        <lpage>582</lpage>
        <pub-id pub-id-type="pmid">18255316</pub-id>
      </element-citation>
    </ref>
    <ref id="b0145">
      <mixed-citation publication-type="other" id="h0145">Paranjape, K., Schinkel, M., Nannan Panday, R., Car, J., Nanayakkara, P., 2019. Introducing Artificial Intelligence Training in Medical Education. JMIR Med Educ 5, e16048.</mixed-citation>
    </ref>
    <ref id="b0150">
      <mixed-citation publication-type="other" id="h0150">Rolls, E.T., Huang, C.C., Lin, C.P., Feng, J., Joliot, M., 2020. Automated anatomical labelling atlas 3. Neuroimage 206, 116189.</mixed-citation>
    </ref>
    <ref id="b0155">
      <element-citation publication-type="book" id="h0155">
        <person-group person-group-type="author">
          <name>
            <surname>Ronneberger</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Fischer</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Brox</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <part-title>U-net: Convolutional networks for biomedical image segmentation</part-title>
        <source>International Conference on Medical image computing and computer-assisted intervention. Springer</source>
        <year>2015</year>
        <fpage>234</fpage>
        <lpage>241</lpage>
      </element-citation>
    </ref>
    <ref id="b0160">
      <element-citation publication-type="journal" id="h0160">
        <person-group person-group-type="author">
          <name>
            <surname>Ruopp</surname>
            <given-names>M.D.</given-names>
          </name>
          <name>
            <surname>Perkins</surname>
            <given-names>N.J.</given-names>
          </name>
          <name>
            <surname>Whitcomb</surname>
            <given-names>B.W.</given-names>
          </name>
          <name>
            <surname>Schisterman</surname>
            <given-names>E.F.</given-names>
          </name>
        </person-group>
        <article-title>Youden Index and optimal cut-point estimated from observations affected by a lower limit of detection</article-title>
        <source>Biomet. J. Biometr. Z.</source>
        <volume>50</volume>
        <issue>3</issue>
        <year>2008</year>
        <fpage>419</fpage>
        <lpage>430</lpage>
      </element-citation>
    </ref>
    <ref id="b0165">
      <mixed-citation publication-type="other" id="h0165">Saito, T., Rehmsmeier, M., 2015. The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets. PLoS One 10, e0118432.</mixed-citation>
    </ref>
    <ref id="b0170">
      <element-citation publication-type="journal" id="h0170">
        <person-group person-group-type="author">
          <name>
            <surname>Schaefer</surname>
            <given-names>P.W.</given-names>
          </name>
          <name>
            <surname>Pulli</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Copen</surname>
            <given-names>W.A.</given-names>
          </name>
          <name>
            <surname>Hirsch</surname>
            <given-names>J.A.</given-names>
          </name>
          <name>
            <surname>Leslie-Mazwi</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Schwamm</surname>
            <given-names>L.H.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>González</surname>
            <given-names>R.G.</given-names>
          </name>
          <name>
            <surname>Yoo</surname>
            <given-names>A.J.</given-names>
          </name>
        </person-group>
        <article-title>Combining MRI with NIHSS thresholds to predict outcome in acute ischemic stroke: value for patient selection</article-title>
        <source>AJNR Am. J. Neuroradiol.</source>
        <volume>36</volume>
        <issue>2</issue>
        <year>2015</year>
        <fpage>259</fpage>
        <lpage>264</lpage>
        <pub-id pub-id-type="pmid">25258369</pub-id>
      </element-citation>
    </ref>
    <ref id="b0175">
      <element-citation publication-type="journal" id="h0175">
        <person-group person-group-type="author">
          <name>
            <surname>Shahangian</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Pourghassem</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Automatic brain hemorrhage segmentation and classification algorithm based on weighted grayscale histogram feature in a hierarchical classification structure</article-title>
        <source>Biocybernet. Biomed. Eng.</source>
        <volume>36</volume>
        <issue>1</issue>
        <year>2016</year>
        <fpage>217</fpage>
        <lpage>232</lpage>
      </element-citation>
    </ref>
    <ref id="b0180">
      <mixed-citation publication-type="other" id="h0180">Simonyan, K., Zisserman, A., 2015. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.</mixed-citation>
    </ref>
    <ref id="b0185">
      <element-citation publication-type="journal" id="h0185">
        <person-group person-group-type="author">
          <name>
            <surname>Strotzer</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>One century of brain mapping using Brodmann areas100 Jahre Hirnkartierung nach Brodmann</article-title>
        <source>Klin. Neuroradiol.</source>
        <volume>19</volume>
        <issue>3</issue>
        <year>2009</year>
        <fpage>179</fpage>
        <lpage>186</lpage>
        <pub-id pub-id-type="pmid">19727583</pub-id>
      </element-citation>
    </ref>
    <ref id="b0190">
      <element-citation publication-type="journal" id="h0190">
        <person-group person-group-type="author">
          <name>
            <surname>Tatu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Moulin</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Bogousslavsky</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Duvernoy</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Arterial territories of the human brain: cerebral hemispheres</article-title>
        <source>Neurology</source>
        <volume>50</volume>
        <issue>6</issue>
        <year>1998</year>
        <fpage>1699</fpage>
        <lpage>1708</lpage>
        <pub-id pub-id-type="pmid">9633714</pub-id>
      </element-citation>
    </ref>
    <ref id="b0195">
      <element-citation publication-type="journal" id="h0195">
        <person-group person-group-type="author">
          <name>
            <surname>Terroni</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Amaro</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Iosifescu</surname>
            <given-names>D.V.</given-names>
          </name>
          <name>
            <surname>Tinone</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Sato</surname>
            <given-names>J.R.</given-names>
          </name>
          <name>
            <surname>Leite</surname>
            <given-names>C.C.</given-names>
          </name>
          <name>
            <surname>Sobreiro</surname>
            <given-names>M.F.M.</given-names>
          </name>
          <name>
            <surname>Lucia</surname>
            <given-names>M.C.S.</given-names>
          </name>
          <name>
            <surname>Scaff</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Fráguas</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Stroke lesion in cortical neural circuits and post-stroke incidence of major depressive episode: a 4-month prospective study</article-title>
        <source>World J. Biol. Psychiatry</source>
        <volume>12</volume>
        <issue>7</issue>
        <year>2011</year>
        <fpage>539</fpage>
        <lpage>548</lpage>
        <pub-id pub-id-type="pmid">21486107</pub-id>
      </element-citation>
    </ref>
    <ref id="b0200">
      <element-citation publication-type="journal" id="h0200">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Hong</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>White matter injury in ischemic stroke</article-title>
        <source>Prog. Neurobiol.</source>
        <volume>141</volume>
        <year>2016</year>
        <fpage>45</fpage>
        <lpage>60</lpage>
        <pub-id pub-id-type="pmid">27090751</pub-id>
      </element-citation>
    </ref>
    <ref id="b0205">
      <element-citation publication-type="journal" id="h0205">
        <person-group person-group-type="author">
          <name>
            <surname>Wardlaw</surname>
            <given-names>J.M.</given-names>
          </name>
        </person-group>
        <article-title>What causes lacunar stroke?</article-title>
        <source>J. Neurol. Neurosurg. Psychiatry</source>
        <volume>76</volume>
        <issue>5</issue>
        <year>2005</year>
        <fpage>617</fpage>
        <lpage>619</lpage>
        <pub-id pub-id-type="pmid">15834013</pub-id>
      </element-citation>
    </ref>
    <ref id="b0210">
      <element-citation publication-type="journal" id="h0210">
        <person-group person-group-type="author">
          <name>
            <surname>Wessels</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Röttger</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Jauss</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kaps</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Traupe</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Stolz</surname>
            <given-names>E.</given-names>
          </name>
        </person-group>
        <article-title>Identification of embolic stroke patterns by diffusion-weighted MRI in clinically defined lacunar stroke syndromes</article-title>
        <source>Stroke</source>
        <volume>36</volume>
        <issue>4</issue>
        <year>2005</year>
        <fpage>757</fpage>
        <lpage>761</lpage>
        <pub-id pub-id-type="pmid">15746460</pub-id>
      </element-citation>
    </ref>
    <ref id="b0215">
      <element-citation publication-type="journal" id="h0215">
        <person-group person-group-type="author">
          <name>
            <surname>Winzeck</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Hakim</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>McKinley</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Pinto</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Alves</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Silva</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Pisov</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Krivov</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Belyaev</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Monteiro</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Oliveira</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Choi</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Paik</surname>
            <given-names>M.C.</given-names>
          </name>
          <name>
            <surname>Kwon</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>B.J.</given-names>
          </name>
          <name>
            <surname>Won</surname>
            <given-names>J.H.</given-names>
          </name>
          <name>
            <surname>Islam</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Robben</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Suetens</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Gong</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Niu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Pauly</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Lucas</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Heinrich</surname>
            <given-names>M.P.</given-names>
          </name>
          <name>
            <surname>Rivera</surname>
            <given-names>L.C.</given-names>
          </name>
          <name>
            <surname>Castillo</surname>
            <given-names>L.S.</given-names>
          </name>
          <name>
            <surname>Daza</surname>
            <given-names>L.A.</given-names>
          </name>
          <name>
            <surname>Beers</surname>
            <given-names>A.L.</given-names>
          </name>
          <name>
            <surname>Arbelaezs</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Maier</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Brown</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Kalpathy-Cramer</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zaharchuk</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Wiest</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Reyes</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>ISLES 2016 and 2017-benchmarking ischemic stroke lesion outcome prediction based on multispectral MRI</article-title>
        <source>Front. Neurol.</source>
        <volume>9</volume>
        <year>2018</year>
        <fpage>679</fpage>
        <pub-id pub-id-type="pmid">30271370</pub-id>
      </element-citation>
    </ref>
    <ref id="b0220">
      <element-citation publication-type="journal" id="h0220">
        <person-group person-group-type="author">
          <name>
            <surname>Winzeck</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Mocking</surname>
            <given-names>S.J.T.</given-names>
          </name>
          <name>
            <surname>Bezerra</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Bouts</surname>
            <given-names>M.J.R.J.</given-names>
          </name>
          <name>
            <surname>McIntosh</surname>
            <given-names>E.C.</given-names>
          </name>
          <name>
            <surname>Diwan</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Garg</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Chutinet</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Kimberly</surname>
            <given-names>W.T.</given-names>
          </name>
          <name>
            <surname>Copen</surname>
            <given-names>W.A.</given-names>
          </name>
          <name>
            <surname>Schaefer</surname>
            <given-names>P.W.</given-names>
          </name>
          <name>
            <surname>Ay</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Singhal</surname>
            <given-names>A.B.</given-names>
          </name>
          <name>
            <surname>Kamnitsas</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Glocker</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Sorensen</surname>
            <given-names>A.G.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>O.</given-names>
          </name>
        </person-group>
        <article-title>Ensemble of convolutional neural networks improves automated segmentation of acute ischemic lesions using multiparametric diffusion-weighted MRI</article-title>
        <source>AJNR Am. J. Neuroradiol.</source>
        <volume>40</volume>
        <issue>6</issue>
        <year>2019</year>
        <fpage>938</fpage>
        <lpage>945</lpage>
        <pub-id pub-id-type="pmid">31147354</pub-id>
      </element-citation>
    </ref>
    <ref id="b0225">
      <element-citation publication-type="journal" id="h0225">
        <person-group person-group-type="author">
          <name>
            <surname>Woo</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Jung</surname>
            <given-names>S.C.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Cho</surname>
            <given-names>S.J.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Sunwoo</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Kang</surname>
            <given-names>D.W.</given-names>
          </name>
        </person-group>
        <article-title>Fully automatic segmentation of acute ischemic lesions on diffusion-weighted imaging using convolutional neural networks: comparison with conventional algorithms</article-title>
        <source>Korean J. Radiol.</source>
        <volume>20</volume>
        <year>2019</year>
        <fpage>1275</fpage>
        <lpage>1284</lpage>
        <pub-id pub-id-type="pmid">31339015</pub-id>
      </element-citation>
    </ref>
    <ref id="b0230">
      <element-citation publication-type="journal" id="h0230">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Winzeck</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Giese</surname>
            <given-names>A.-K.</given-names>
          </name>
          <name>
            <surname>Hancock</surname>
            <given-names>B.L.</given-names>
          </name>
          <name>
            <surname>Etherton</surname>
            <given-names>M.R.</given-names>
          </name>
          <name>
            <surname>Bouts</surname>
            <given-names>M.J.R.J.</given-names>
          </name>
          <name>
            <surname>Donahue</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Schirmer</surname>
            <given-names>M.D.</given-names>
          </name>
          <name>
            <surname>Irie</surname>
            <given-names>R.E.</given-names>
          </name>
          <name>
            <surname>Mocking</surname>
            <given-names>S.J.T.</given-names>
          </name>
          <name>
            <surname>McIntosh</surname>
            <given-names>E.C.</given-names>
          </name>
          <name>
            <surname>Bezerra</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Kamnitsas</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Frid</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Wasselius</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Cole</surname>
            <given-names>J.W.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Holmegaard</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Jiménez-Conde</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Lemmens</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Lorentzen</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>McArdle</surname>
            <given-names>P.F.</given-names>
          </name>
          <name>
            <surname>Meschia</surname>
            <given-names>J.F.</given-names>
          </name>
          <name>
            <surname>Roquer</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Rundek</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Sacco</surname>
            <given-names>R.L.</given-names>
          </name>
          <name>
            <surname>Schmidt</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Sharma</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Slowik</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Stanne</surname>
            <given-names>T.M.</given-names>
          </name>
          <name>
            <surname>Thijs</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Vagal</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Woo</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Bevan</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Kittner</surname>
            <given-names>S.J.</given-names>
          </name>
          <name>
            <surname>Mitchell</surname>
            <given-names>B.D.</given-names>
          </name>
          <name>
            <surname>Rosand</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Worrall</surname>
            <given-names>B.B.</given-names>
          </name>
          <name>
            <surname>Jern</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Lindgren</surname>
            <given-names>A.G.</given-names>
          </name>
          <name>
            <surname>Maguire</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Rost</surname>
            <given-names>N.S.</given-names>
          </name>
        </person-group>
        <article-title>Big data approaches to phenotyping acute ischemic stroke using automated lesion segmentation of multi-center magnetic resonance imaging data</article-title>
        <source>Stroke</source>
        <volume>50</volume>
        <issue>7</issue>
        <year>2019</year>
        <fpage>1734</fpage>
        <lpage>1741</lpage>
        <pub-id pub-id-type="pmid">31177973</pub-id>
      </element-citation>
    </ref>
    <ref id="b0235">
      <element-citation publication-type="journal" id="h0235">
        <person-group person-group-type="author">
          <name>
            <surname>Yoshimoto</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Inoue</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Yamagami</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Fujita</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Tanaka</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Ando</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Sonoda</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Kamogawa</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Koga</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Ihara</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Toyoda</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Use of diffusion-weighted imaging-alberta stroke program early computed tomography score (DWI-ASPECTS) and ischemic core volume to determine the malignant profile in acute stroke</article-title>
        <source>J. Am. Heart Assoc.</source>
        <volume>8</volume>
        <year>2019</year>
        <fpage>e012558</fpage>
        <pub-id pub-id-type="pmid">31698986</pub-id>
      </element-citation>
    </ref>
    <ref id="b0240">
      <mixed-citation publication-type="other" id="h0240">Yu, Y., Xie, Y., Thamm, T., Gong, E., Ouyang, J., Huang, C., Christensen, S., Marks, M.P., Lansberg, M.G., Albers, G.W., Zaharchuk, G., 2020. Use of Deep Learning to Predict Final Ischemic Stroke Lesions From Initial Magnetic Resonance Imaging. JAMA Netw Open 3, e200772.</mixed-citation>
    </ref>
    <ref id="b0245">
      <element-citation publication-type="book" id="h0245">
        <person-group person-group-type="author">
          <name>
            <surname>Zunair</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Rahman</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Mohammed</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Cohen</surname>
            <given-names>J.P.</given-names>
          </name>
        </person-group>
        <part-title>Uniformizing Techniques to Process CT Scans with 3D CNNs for Tuberculosis Prediction</part-title>
        <year>2020</year>
        <publisher-name>Springer International Publishing</publisher-name>
        <publisher-loc>Cham</publisher-loc>
        <fpage>156</fpage>
        <lpage>168</lpage>
      </element-citation>
    </ref>
  </ref-list>
  <ack id="ak005">
    <title>Acknowledgments</title>
    <p id="p0260">The authors are grateful for the support of the Department of Medical Research and Development of Keelung Chang Gung Memorial Hospital. This research was supported by grants of the Chang Gung Research Project to Dr. Y-C. Wei and Dr. W-Y. Huang (CMRPG2J0121) and to Dr. K-F. Chen (CMRPG2H0323, CMRPG2H0312, CMRPG2K0211, and CMRPG2K0241). The study was also supported by the Ministry of Science (MOST 109-2314-B-182-036) to Dr. K-F. Chen.</p>
  </ack>
</back>
