<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9396797</article-id>
    <article-id pub-id-type="publisher-id">4847</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-022-04847-z</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>LTPConstraint: a transfer learning based end-to-end method for RNA secondary structure prediction</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Fei</surname>
          <given-names>Yinchao</given-names>
        </name>
        <address>
          <email>feiyc20@163.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhang</surname>
          <given-names>Hao</given-names>
        </name>
        <address>
          <email>zhangh@mails.jlu.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Yili</given-names>
        </name>
        <address>
          <email>wangyili0328@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Liu</surname>
          <given-names>Zhen</given-names>
        </name>
        <address>
          <email>liu_zhen@nias.ac.jp</email>
        </address>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Liu</surname>
          <given-names>Yuanning</given-names>
        </name>
        <address>
          <email>liuyn@mails.jlu.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.64924.3d</institution-id><institution-id institution-id-type="ISNI">0000 0004 1760 5735</institution-id><institution>College of Computer Science and Technology, </institution><institution>Jilin University, </institution></institution-wrap>Changchun, China </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.64924.3d</institution-id><institution-id institution-id-type="ISNI">0000 0004 1760 5735</institution-id><institution>Key Laboratory of Symbolic Computation and Knowledge Engineering, Ministry of Education, </institution><institution>Jilin University, </institution></institution-wrap>Changchun, China </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.444367.6</institution-id><institution-id institution-id-type="ISNI">0000 0000 9853 5396</institution-id><institution>Graduate School of Engineering, </institution><institution>Nagasaki Institute of Applied Science, </institution></institution-wrap>Nagasaki, Japan </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>23</day>
      <month>8</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>23</day>
      <month>8</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>23</volume>
    <elocation-id>354</elocation-id>
    <history>
      <date date-type="received">
        <day>16</day>
        <month>5</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>18</day>
        <month>7</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">RNA secondary structure is very important for deciphering cell’s activity and disease occurrence. The first method which was used by the academics to predict this structure is biological experiment, But this method is too expensive, causing the promotion to be affected. Then, computing methods emerged, which has good efficiency and low cost. However, the accuracy of computing methods are not satisfactory. Many machine learning methods have also been applied to this area, but the accuracy has not improved significantly. Deep learning has matured and achieves great success in many areas such as computer vision and natural language processing. It uses neural network which is a kind of structure that has good functionality and versatility, but its effect is highly correlated with the quantity and quality of the data. At present, there is no model with high accuracy, low data dependence and high convenience in predicting RNA secondary structure.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">This paper designs a neural network called LTPConstraint to predict RNA secondary structure. The network is based on many network structure such as Bidirectional LSTM, Transformer and generator. It also uses transfer learning to train modelso that the data dependence can be reduced.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">LTPConstraint has achieved high accuracy in RNA secondary structure prediction. Compared with the previous methods, the accuracy improves obviously both in predicting the structure with pseudoknot and the structure without pseudoknot. At the same time, LTPConstraint is easy to operate and can achieve result very quickly.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>RNA secondary structure</kwd>
      <kwd>Bi-LSTM</kwd>
      <kwd>Transformer</kwd>
      <kwd>Generator</kwd>
      <kwd>Transfer learning</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>National Natural Science Foundation of China</institution>
        </funding-source>
        <award-id>61471181</award-id>
        <award-id>61471181</award-id>
        <principal-award-recipient>
          <name>
            <surname>Zhang</surname>
            <given-names>Hao</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Yuanning</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>National Key Research and Development Program of China</institution>
        </funding-source>
        <award-id>2020YFB1709800</award-id>
        <award-id>2020YFB1709800</award-id>
        <principal-award-recipient>
          <name>
            <surname>Zhang</surname>
            <given-names>Hao</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Yuanning</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>National Key Research and Development Project of China</institution>
        </funding-source>
        <award-id>[2020]151</award-id>
        <award-id>[2020]151</award-id>
        <principal-award-recipient>
          <name>
            <surname>Zhang</surname>
            <given-names>Hao</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Yuanning</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Jilin Province Industrial Innovation Special Fund Project</institution>
        </funding-source>
        <award-id>2019C053-2 and 2019C053-6</award-id>
        <award-id>2019C053-2 and 2019C053-6</award-id>
        <principal-award-recipient>
          <name>
            <surname>Zhang</surname>
            <given-names>Hao</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Yuanning</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par4">Ribonucleic Acid (RNA) is a carrier of life genetic information. The regular activities of living organisms depend on the correct expression of coding RNA (such as tRNA, mRNA) and non-coding RNA [<xref ref-type="bibr" rid="CR1">1</xref>]. It acts on all processes of cell activity. It directly or indirectly relates to the regulation and occurrence of diseases [<xref ref-type="bibr" rid="CR2">2</xref>]. RNA is a long-chain-like molecule composed. It is usually composed of four kinds of bases which are connected by phosphoric diester bond. Hydrogen bonds can also be formed between bases and such two bases connected by Hydrogen bonds are called a pair. Pairs can be divided into canonical pair and non-canonical pair. The canonical pair refers to the pairing of AU, GC, and GU, while the non-canonical pair is a pairing style other than above [<xref ref-type="bibr" rid="CR3">3</xref>]. RNA has a quaternary structure academically. The primary structure of RNA is a single strand composed of base pairs. The secondary structure of RNA is a hairpin-shaped composite structure formed by convolutional folding of the primary structure of RNA. The tertiary structure of RNA is a spatial structure formed by further bending the spiral based on the secondary structure. The quaternary structure of RNA is a mixture of nucleic acid and protein produced by the interaction of RNA and protein. As we can see in the Fig. <xref rid="Fig1" ref-type="fig">1</xref>, the secondary structure of RNA forms various structure after helical folding, including hairpin loop, stem, interior loop and pseudoknot.<fig id="Fig1"><label>Fig. 1</label><caption><p>RNA secondary structure. The red, yellow, blue, and green spheres represent adenine, guanine, cytosine, and uracil, respectively. Legends of common structures in RNA secondary structures such as multiloop and stem are marked in the figure</p></caption><graphic xlink:href="12859_2022_4847_Fig1_HTML" id="MO1"/></fig></p>
    <p id="Par5">Pseudoknot generally appears in the pair of single-stranded ring surrounded by the stem [<xref ref-type="bibr" rid="CR4">4</xref>]. This structure is different from some of the above planar structure in that it is related to the spatial structure of RNA. The prediction of pseudoknot is of great importance because pseudoknot has an important influence on the life activities involved by RNA. However, the secondary structure containing pseudoknot will form a non-nested structure from a planar view, as we can see in the Fig. <xref rid="Fig2" ref-type="fig">2</xref>. All possible nested structure can be quickly obtained by using dynamic programming algorithms, but the secondary structure contains pseudoknot can’t, so it is difficulty to predict the secondary structure containing pseudoknot [<xref ref-type="bibr" rid="CR5">5</xref>].<fig id="Fig2"><label>Fig. 2</label><caption><p>Nested and non-nested structure. The diagram above represents a nested structure and the diagram below represents a non-nested structure</p></caption><graphic xlink:href="12859_2022_4847_Fig2_HTML" id="MO2"/></fig></p>
    <p id="Par6">The tertiary structure of RNA is a key to interpreting the relationship between RNA’s structure and function, especially the structure called noncoding RNA [<xref ref-type="bibr" rid="CR6">6</xref>]. Meanwhile, the tertiary structure of RNA is also the most direct material to analysing the state of RNA that is difficult to characterize [<xref ref-type="bibr" rid="CR7">7</xref>]. Generally speaking, the secondary structure tends to be formed quicker than tertiary structure [<xref ref-type="bibr" rid="CR8">8</xref>], so before predicting the tertiary structure of RNA, obtaining accurate secondary structure is the basis. Also, the secondary structure of RNA is related to RNA’s function [<xref ref-type="bibr" rid="CR9">9</xref>]. Therefore, accurately predicting secondary structure is vital for studying RNA.</p>
    <p id="Par7">Scholars tried many methods from different fields to predict secondary structure. Initially, they obtained RNA secondary structure from biological experiments. DMS-MaPseq is a robust assay method that uses the advantages of dimethyl sulfate (DMS)-mutational profiling and sequencing (MaPseq), making it easy to modify RNA in vitro, in cells, and virions [<xref ref-type="bibr" rid="CR10">10</xref>], thereby enabling Determine the various levels of RNA structure. SHAPE [<xref ref-type="bibr" rid="CR11">11</xref>](Selective 2’-hydroxyl acylation analyzed by primer extension) method can be used to analyze selective 2’-hydroxyl acylation reactions in living cells by primer extension, and the High-throughput data of relevant nucleotides in paired or unpaired state can be obtained with a single base-pair resolution by SHAPE [<xref ref-type="bibr" rid="CR12">12</xref>]. In addition to the above methods, the most commonly used experimental method is the X-ray crystallography and nuclear magnetic resonance [<xref ref-type="bibr" rid="CR13">13</xref>]. Both methods can also provide structural information with a single base pair resolution. In summary, these experimental methods have two common characteristics. They are high cost and low yield. These performances make it inefficient when predicting a large number of RNA sequences and make experimental methods hard to be used on a large scale.</p>
    <p id="Par8">To reduce the cost of prediction and improve efficiency. Academics has turned to computing methods to predict RNA secondary structure. The computing methods can be divided into two types, comparative sequence analysis and folding algorithms using thermodynamic, statistical or probabilistic scoring schemes [<xref ref-type="bibr" rid="CR14">14</xref>]. Comparative sequence analysis [<xref ref-type="bibr" rid="CR15">15</xref>] predicts the secondary structure of RNA by using the conservative base pairs between the homologous sequences [<xref ref-type="bibr" rid="CR16">16</xref>]. This method is highly accurate if homologous sequences can be obtained, but there are only a few known RNA families causing not enough data, affecting this method to be promated. The folding algorithm often divides the entire sequence into sub-blocks. It then generates the optimal secondary structure after scoring each sub-block based on thermodynamic principles or scoring schemes such as statistics and probability. The representative one is the minimum free energy model using a dynamic programming algorithm [<xref ref-type="bibr" rid="CR17">17</xref>]. Its implementation include RNAstructure and RNAfold. According to the principle of minimum free energy algorithm, RNAstructure [<xref ref-type="bibr" rid="CR18">18</xref>] uses Zuker algorithm [<xref ref-type="bibr" rid="CR19">19</xref>] to obtain the optimal secondary structure. The biggest advantage of this software is that it adds many additional modules to extend the function of Zuker algorithm, enriching the user experience, and its graphical interface makes users operate conveniently. The thermodynamic data provided by Turner was used to calculate the free energy of each substructure [<xref ref-type="bibr" rid="CR20">20</xref>]. RNAfold [<xref ref-type="bibr" rid="CR21">21</xref>] also uses the free energy parameters. The minimum free energy method has an upper limit of accuracy. This is because many real RNA secondary structure are not necessarily the structure with the minimum free energy, which leads to the assumption of the minimum free energy method cannot always hold.</p>
    <p id="Par9">Other computing methods use machine learning. CONTRAfold [<xref ref-type="bibr" rid="CR22">22</xref>] uses stochastic context-free grammar (SCFG). SCFG model parameters are derived using an automatic statistical learning algorithm. This is a big innovation, but even the best SCFG model doesn’t perform so well as the method of minimum free energy model. In [<xref ref-type="bibr" rid="CR23">23</xref>], the author successfully combined deep learning with the thermodynamic nearest-neighbor model. According to the relationship between SHAPE data and state inference, bidirectional LSTM was used to extract sequence features, and then the state inference of the sequence was obtained through classifier based on these features. The SHAPE value is obtained according to the relation formula between SHPAE data and state inference. Then, the calculated SHAPE value was used as a soft constraint for a recent thermodynamic model called GTfold [<xref ref-type="bibr" rid="CR24">24</xref>]. This method achieved high accuracy according to the author’s description. However this method is still not a complete end-to-end RNA secondary structure prediction method. Also, this method predict RNA secondary structure based on GTfold, and the SHAPE value only serves as supplementary information to improve the prediction accuracy of GTfold.</p>
    <p id="Par10">In recent years, deep learning has achieved breakthrough in computer vision and natural language processing. On image translation, A model called Pix2pix makes this kind of problem obtain a general and good enough solution. The traditional method of image translation only uses an original CNN model to minimize the Euclidean distance between the prediction and target without a good loss, and the result can only get a fuzzy output [<xref ref-type="bibr" rid="CR25">25</xref>]. Therefore, traditional models often require mannually designing precise loss functions to guide CNN to complete tasks. Pix2pix [<xref ref-type="bibr" rid="CR26">26</xref>] model is designed based on the basis of GAN [<xref ref-type="bibr" rid="CR27">27</xref>]. It makes the output indistinguishable from reality by optimizing a high-dimensional problem. To be specific, Pix2pix constructed a generator with strong feature extraction ability and a discriminator that scores the difference between input and output, making this structure a universal method in image translation [<xref ref-type="bibr" rid="CR28">28</xref>, <xref ref-type="bibr" rid="CR29">29</xref>].</p>
    <p id="Par11">LSTM and Transformer are two excellent structure that have emerged in natural language processing. LSTM(Long Short-Term Memory) [<xref ref-type="bibr" rid="CR30">30</xref>] is the most commonly used model structure for processing indefinite length linear sequences. It is improved from RNN [<xref ref-type="bibr" rid="CR31">31</xref>] structure. As can be seen in the Fig. <xref rid="Fig3" ref-type="fig">3</xref> In LSTM, three gate structure called forgetting gate, information enhancement gate, and output gate are added. The LSTM’s three-gate structure enhances RNN’s ability to extract features over long distances. By overlaying network structure to increase the depth of information processing, LSTM can handle almost all semantic problems using an encoder-decoder [<xref ref-type="bibr" rid="CR32">32</xref>] framework conbining with the Attention [<xref ref-type="bibr" rid="CR33">33</xref>] mechanism [<xref ref-type="bibr" rid="CR34">34</xref>].<fig id="Fig3"><label>Fig. 3</label><caption><p>The structure of LSTM. The diagram shows a basic unit of the LSTM. Each time the data passes through three gates, it will get a cell state, a hidden state and an output state of this moment. The hidden state will be passed to the next moment for calculation, while the output state will be directly output</p></caption><graphic xlink:href="12859_2022_4847_Fig3_HTML" id="MO3"/></fig></p>
    <p id="Par12">Transformer [<xref ref-type="bibr" rid="CR35">35</xref>] is one of the most notable achievements of deep learning in recent years. Transformer has achieved breakthrough achievements in several areas, especially in natural language processing [<xref ref-type="bibr" rid="CR36">36</xref>] and computer vision [<xref ref-type="bibr" rid="CR37">37</xref>, <xref ref-type="bibr" rid="CR38">38</xref>]. It cleverly uses self-attention or multi-head self-attention for semantic extraction. As can be seen from the Fig. <xref rid="Fig4" ref-type="fig">4</xref>, Transformer is a structure similar to full connection, which can extract the connection between each word in a sentence. Its mult-head self-attention can focus on different positions in a sentence, so as to better extract semantics. Transformer operates on all words of the entire sentence at the same time, rather than processing each word sequentially, which brings strong parallel computing to Transformer. Similar to LSTM, Transformer can be nested into an encoder-decoder model to accomplish various semantic tasks, and its performance in many tasks is even better than the model using LSTM.<fig id="Fig4"><label>Fig. 4</label><caption><p>The mechanism of self-attention. The graph shows a numerical transformation of self-attention. Three vectors of Query, Key and Value are obtained from the input X according to different weights, and then these three vectors are put into the following formula to obtain self-attention</p></caption><graphic xlink:href="12859_2022_4847_Fig4_HTML" id="MO4"/></fig></p>
    <p id="Par13">Both LSTM and Transformer have their strengths and weaknesses. In [<xref ref-type="bibr" rid="CR39">39</xref>], the author compares LSTM with Transformer in terms of semantic feature extraction ability, long-distance capture ability, task comprehensive feature extraction ability and parallel computing ability. The results show that Transformer is better than LSTM in four aspects, especially in terms of parallel computing capability. As LSTM is a time-sequence linear structure, its parallel computing capability is very weak, which is a structural defect that is difficult to make up for. Moreover, the transformer structure solves the problem of long memory loss that can still occur in LSTM when the sequence is too long [<xref ref-type="bibr" rid="CR40">40</xref>]. However, the self-attention mechanism lacks modeling of time dimension. In other words, Transformer is not sensitive to the word order of input statements. As a result, the position encoding mechanism is used in the current Transformer structure. It adds sequential timing data into Transformer directly. Of course, such a mechanism is not as good as the natural temporal structure of LSTM. This is obviously a stopgap [<xref ref-type="bibr" rid="CR41">41</xref>], and this can result in the Transformer not performing well on word order sensitive tasks. Although LSTM is suitable for the above scenarios, it is incapable of training in the face of large data sets. Its lack of parallel computing time series structure leads to slow operation. At the same time, when the amount of learned data exceeds a certain threshold, LSTM can no longer be improved. Transformer can handle such scenarios very well according the description above.</p>
    <p id="Par14">With the development of neural network, the depth of the model is getting larger and larger, And the amount of data required to fit the network also increases. Nowadays, it has become a time-consuming and laborious task to train a model from scratch [<xref ref-type="bibr" rid="CR42">42</xref>]. To solve this problem, the transfer learning of deep neural networks was born [<xref ref-type="bibr" rid="CR43">43</xref>]. The core of transfer learning [<xref ref-type="bibr" rid="CR44">44</xref>] is to use the pre-trained model. Pre-trained model [<xref ref-type="bibr" rid="CR45">45</xref>] was obtained by training some network structure with high robustness using high quality data set. Pre-trained model can then be trasfered to train other relevant data. In other words, There is no need to train a model from scratch for a specific problem. We can find a pre-trained model of similar problems and then train it with a small amount of problem-specific data, it will significantly reduce the training time and the amount of data that required to fit because the pre-trained model has learned much relevant feature during pre-training, so the more features pre-training data shares with the problem-specific data, the easier the transfer learning process will be. We just need to design the fine-tuning mechanism [<xref ref-type="bibr" rid="CR46">46</xref>]. The operation is simple and easy to understand, but the effect is significant [<xref ref-type="bibr" rid="CR47">47</xref>].</p>
    <p id="Par15">The prediction of RNA secondary structure depends on the data of biochemistry experiment for a long time, which affects the progress of the research on this problem. We believe that we can create a model based on deep learning, and then correct the output of our model by taking some thermodynamic or biological research results of RNA secondary structure as prior knowledge, so the efficiency and accuracy of this model will be significantly improved. At the same time, the neural network model learns the structural features completely according to the input data, so we believe that if we have enough RNA structure containing pseudoknot, the feature of pseudoknot can also be learned by our network, so as to make up for the shortcomings of the past methods in predicting pseudoknot.</p>
  </sec>
  <sec id="Sec2">
    <title>Methods and materials</title>
    <p id="Par16">In this section, we will describe the structure of LTPConstraint according to the idea and the correctness analysis of LTPConstraint. We will also describe the transfer learning method used by LTPConstraint and the data set after processing.</p>
    <sec id="Sec3">
      <title>Methods</title>
      <p id="Par17">LTPConstraint uses a complex deep neural network to predict RNA secondary structure. The model’s input is the sequence after preprocessing, and the output is a <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x\times x$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mi>x</mml:mi><mml:mo>×</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4847_Article_IEq1.gif"/></alternatives></inline-formula> matrix, where <italic>x</italic> is the length of the sequence. The matrix values only from <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {S}=\{0,1\}$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mi mathvariant="bold">S</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4847_Article_IEq2.gif"/></alternatives></inline-formula>, the value 0 represents two bases do not match, the value 1 represents two base pairing. The whole network framework can be seen from the Fig. <xref rid="Fig5" ref-type="fig">5</xref>.<fig id="Fig5"><label>Fig. 5</label><caption><p>Architecture of LTPConstraint Network. Input module 1 after the sequence data goes through the Embedding layer. Pairwise concat is used for the output of module 1 and then input module 2. Until module 2, it is pretrained network. Finally, the preTrained network was modified with the hard constraint layer of module 3 to get the output</p></caption><graphic xlink:href="12859_2022_4847_Fig5_HTML" id="MO5"/></fig></p>
      <p id="Par18">The model is made up of three modules. The first module is a global semantic extraction module. The input of this module is a preprocessed sequence vector, which is converted to a word vector with channel 10 by an embedding layer. This step is different from the direct one-hot processing method of RNA sequence. The method of one-hot encoding gives the word vector a high-dimensional representation in an artificial way. However, a suitable representation of each base in the sequence is related to the distribution of input data and the structure of the model, so it is more appropriate for the model to learn the representation of the word vector directly from the data. Therefore, an embedding layer is used instead of one-hot. The global semantic extractor consists of two parts. The whole structure can be seen from the Fig. <xref rid="Fig6" ref-type="fig">6</xref>. The extractor starts with a single-layer bidirectional LSTM network, and then a Transformer Encoder. The Transformer Encoder contains six layers of 2-head self-attention module. This configuration of Transformer Encoder has been experimentally confirmed to achieve best cost-effective. This whole structure is to realize the complementarity of Bi-LSTM and Transformer. Bi-LSTM has good semantic extraction capability and implicit location information. Transformer Encoder is superior in semantic extraction and parallelism due to its computing structure. The global semantic extractor can take advantage of the two semantic extraction capabilities. Meanwhile, Transformer Encoder takes the output of Bi-LSTM in each timing sequence as input, which always implies the location information without manual input.<fig id="Fig6"><label>Fig. 6</label><caption><p>The structure of the global semantic extractor. This layer consists mainly of a Bi-LSTM layer and a 6-layer dual-headed Transformer encoder</p></caption><graphic xlink:href="12859_2022_4847_Fig6_HTML" id="MO6"/></fig></p>
      <p id="Par19">The output of the global semantic extractor is a matrix of word vector. Then we need to fold the word vector. In detail, each word vector in the matrix is conbined with other word vector one by one to get a new 2-dimension matrix. it is similar to an adjacency matrix, and the difference is just that elements of each position are the splicing of two word vectors. The new matrix will become input of next module of our network. The next module will further refine these semantics to get a score for each pair which represents the probability that the model predicts pairs of each base, and this module is called the local feature extraction module. The output is like the Fig. <xref rid="Fig7" ref-type="fig">7</xref>. Input and output of this module has the characteristics that they have different data appearance but similar underlying structure coincidentally with the characteristics of image translation. However, we cannot apply the method of image translation directly. Generated adversarial network is used in image translation, but our output target is an adjacency matrix. Each element of this matrix values from 0 to 1. We know that one base can only be paired with another base, so the output matrix will be a quasi sparse matrix. We found in the experiment that generated adversarial network performs poorly when optimizing such targets. Because data with values close to one has insufficient impact on loss, The network optimized by generating adversarial network will output an adjacency matrix with all data of zero. Therefore, some experience in image translation can be used for reference in the problem of RNA secondary structure prediction. but the model structure and loss function need to be modified to adapt to the characteristics of this scenarios. Similarly, our label set is also an adjacency matrix <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Y_{ij}$$\end{document}</tex-math><mml:math id="M6"><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4847_Article_IEq3.gif"/></alternatives></inline-formula> like the Fig. <xref rid="Fig7" ref-type="fig">7</xref>, but the internal elements are only 0 and 1. 0 means that the base labeled i and the base labeled j are not paired, and 1 On the contrary.<fig id="Fig7"><label>Fig. 7</label><caption><p>The structure of the scoring matrix. Light green represents the position that favors the score of negative cases, and dark green represents the position that favors the score of positive cases</p></caption><graphic xlink:href="12859_2022_4847_Fig7_HTML" id="MO7"/></fig></p>
      <p id="Par20">In this paper, the generator of pix2pix model is used as the main body of the second module. The structure of generator can be seen in the Fig. <xref rid="Fig8" ref-type="fig">8</xref>.It uses the model structure of skip connection like Unet. output of layer i is directly added to layer n i, so that the input and output can share the underlying information, which helps extract the potential structural features between the input and output. However, the conditional discriminator in the pix2pix network is abandoned, and the characteristics of its antagonist network are transformed into a filtering network and placed in the third module. Meanwhile, specific loss functions are used to adapt to the characteristics of RNA secondary structure data. We have designed three kinds of loss functions and selected the one with the best performance as the final loss function after comparison. This experimental result is recorded in “Results” section. The design ideas of these three loss functions will be discussed below.<fig id="Fig8"><label>Fig. 8</label><caption><p>The structure of the generator network. This is a Unet structure. Each computing unit is a downsample or upsample structure. The structure given in the figure is designed for data with a sequence length of 128</p></caption><graphic xlink:href="12859_2022_4847_Fig8_HTML" id="MO8"/></fig></p>
      <p id="Par21">The label set used in this paper is an adjacency matrix values only from <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {S}=\{0,1\}$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mi mathvariant="bold">S</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4847_Article_IEq4.gif"/></alternatives></inline-formula>. The label means which base pairs up with each base in the actual RNA secondary structure. After statistical analysis, the number of value zero in the label accounts for 99.75% of the total, much larger than the value one. For the optimization goal of such extremely unbalanced data, both precision and recall should be considered when designing loss function.<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} p(x, y) = \frac{\langle x, y \rangle }{\langle x , y \rangle + \langle x , (1-y) \rangle } \end{aligned}$$\end{document}</tex-math><mml:math id="M10" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">⟩</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">⟨</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4847_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} r(x, y) = \frac{\langle x , y \rangle }{\langle x , y\rangle + \langle (1-x) , y\rangle } \end{aligned}$$\end{document}</tex-math><mml:math id="M12" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">⟩</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">⟨</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4847_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>In Eqs. (<xref rid="Equ1" ref-type="">1</xref>) and (<xref rid="Equ2" ref-type="">2</xref>), <italic>x</italic> represents the output of the model and the <italic>y</italic> represents the label, and the function <inline-formula id="IEq5"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\langle \rangle$$\end{document}</tex-math><mml:math id="M14"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4847_Article_IEq5.gif"/></alternatives></inline-formula> means the matrix inner product. There are three types of loss functions that can be used, which are weighed cross-entropy loss, F1 loss, and AUC loss. The weight <italic>pw</italic> is added to the normal binary cross-entropy loss function to enlarge the impact of data with the value one on loss. If the <italic>pw</italic> value is too large, it will reduces the network’s ability to correct errors because the model attaches too much importance to the training of positive data and neglects the training of negative data, then false negative output will not get enough attention, resulting in premature network fitting. If <italic>pw</italic> value is too small, then the model will tend to be conservative, because the negative data account for the vast majority in the data set. The model will tend to predict all the data as negative examples, which is the conservative type caused by the data imbalance. In this paper, the binary search method is used to test and optimize <italic>pw</italic>, and finally achieved that the optimal value of <italic>pw</italic> is 256. The formula of weighed cross-entropy loss is Eq. (<xref rid="Equ3" ref-type="">3</xref>)<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} l(x, y) = pw \cdot y \cdot -log(sigmoid(x)) + (1-y) \cdot -log(1-sigmoid(x)) \end{aligned}$$\end{document}</tex-math><mml:math id="M16" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mi>w</mml:mi><mml:mo>·</mml:mo><mml:mi>y</mml:mi><mml:mo>·</mml:mo><mml:mo>-</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>·</mml:mo><mml:mo>-</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4847_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>In (<xref rid="Equ3" ref-type="">3</xref>), <italic>x</italic> represents the output of the model and the <italic>y</italic> represents the label. Considering the precision and recall of the model, F1 scores can well reflect the situation of these two values and it is not affected by the balance of data. Therefore, this The distortion of F1 formula is very suitable to be the loss function of module 2, eventually becomes the Eq. (<xref rid="Equ4" ref-type="">4</xref>).<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} F1(x, y) = -\frac{2 \cdot p(x, y) \cdot r(x, y)}{p(x, y) + r(x, y)} \end{aligned}$$\end{document}</tex-math><mml:math id="M18" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>·</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>·</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4847_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>From the perspective of precision and recall, there is another way to construct loss function, to calculate the area under P-R curve, namely P-R AUC. Compared with ROC AUC, P-R AUC is much more useful in Binomial classification problems with unbalanced data distribution [<xref ref-type="bibr" rid="CR48">48</xref>]. P-R curve models the sorting ability of positive and negative data using precision and recall. The sort ability embodies the careful consideration of learning under different tasks called expected generalization performance, and the AUC can reflect this ability. AUC value is the sum of the area under P-R curve. While the larger AUC value is, the better the performance of the model is [<xref ref-type="bibr" rid="CR49">49</xref>]. Considering the calculation burden, this paper only takes 100 steps from 0 to 1 for the classification threshold, so that AUC is the sum of the areas of these 100 segments.</p>
      <p id="Par22">When calculating the AUC value, the model’s predicted value is divided into discrete values of positive and negative examples, bounded by the classification threshold. The predicted value greater than the threshold is positive example, and the predicted value less than the threshold is negative. However, the loss function needs to generate a gradient, so the decision process needs to be modified into a calculation process. Therefore, we use the continuous function relu to replace the discrete discriminant method. The formula for the entire AUC loss is Eq. (<xref rid="Equ5" ref-type="">5</xref>), In Eq. (<xref rid="Equ5" ref-type="">5</xref>), <italic>x</italic> means the recall value, and <italic>y</italic> means the precision value. We divide the circular which arc surrounded by the P-R curve, the P axis and the R axis into 100 parts according to the R axis. The height of each part is <inline-formula id="IEq6"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{1}{2}(y_i + y_{i+1})$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4847_Article_IEq6.gif"/></alternatives></inline-formula>, the width of each part is <inline-formula id="IEq7"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_{i+1} - x_i$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4847_Article_IEq7.gif"/></alternatives></inline-formula>, and the areas of the 100 parts are added up to get AUC.<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} AUC = -\frac{1}{2}\sum _{i=1}^{100}(x_{i+1} - x_i)\cdot (y_i + y_{i+1}) \end{aligned}$$\end{document}</tex-math><mml:math id="M24" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>A</mml:mi><mml:mi>U</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>100</mml:mn></mml:munderover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>·</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4847_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>After testing, we used the Weighed-logistic as the loss function for our pre-trained network of module one and module two.</p>
      <p id="Par23">The third module of the model is a filter network because RNA secondary structure has its own structure rules, and the model needs to correct and fit the output accurately according to these rules. Its main task is to design a series of constraint rules, and then apply constraint rules to the model fitting. In [<xref ref-type="bibr" rid="CR50">50</xref>], five kinds of hard constraints on RNA secondary structure were proposed. However, LTPConstraint needs to predict pseudoknot, so we need to remove some rules. The following four hard constraint rules were obtained: <list list-type="order"><list-item><p id="Par24">Only canonical base pairing is allowed.</p></list-item><list-item><p id="Par25">The base cannot pair with itself.</p></list-item><list-item><p id="Par26">The paired two bases are at least four bases apart</p></list-item><list-item><p id="Par27">Each base can only be paired with one base at most.</p></list-item></list>If we set the input sequence as <inline-formula id="IEq8"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x=\{ x_1, x_2,\ldots,x_n\}$$\end{document}</tex-math><mml:math id="M26"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4847_Article_IEq8.gif"/></alternatives></inline-formula> and the canonical base pairing set as <inline-formula id="IEq9"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha =\{AU | UA\} \cup \{GC | CG\} \cup \{GU | CG\}$$\end{document}</tex-math><mml:math id="M28"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi>A</mml:mi><mml:mi>U</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>U</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo>∪</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi>G</mml:mi><mml:mi>C</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>C</mml:mi><mml:mi>G</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo>∪</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi>G</mml:mi><mml:mi>U</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>C</mml:mi><mml:mi>G</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4847_Article_IEq9.gif"/></alternatives></inline-formula>, the First three constraints can be expressed using the filter matrix <italic>M</italic> as follows (Table <xref rid="Tab1" ref-type="table">1</xref>).<table-wrap id="Tab1"><label>Table 1</label><caption><p>Implementation of constraint rules</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Constraint number</th><th align="left">Implementation</th></tr></thead><tbody><tr><td align="left">Constraint 1</td><td align="left"><inline-formula id="IEq10"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$if \{x_ix_j\} \subseteq \alpha , M_{ij}=M_{ji}=1$$\end{document}</tex-math><mml:math id="M30"><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>⊆</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi mathvariant="italic">ji</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4847_Article_IEq10.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">Constraint 2</td><td align="left"><inline-formula id="IEq11"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$if\ i=j, then\ M_{ij}=0$$\end{document}</tex-math><mml:math id="M32"><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="4pt"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mspace width="4pt"/><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4847_Article_IEq11.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">Constraint 3</td><td align="left"><inline-formula id="IEq12"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\forall |i-j|&lt; 4, M_{ij}=M_{ji}=0$$\end{document}</tex-math><mml:math id="M34"><mml:mrow><mml:mrow><mml:mo>∀</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo></mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi mathvariant="italic">ji</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4847_Article_IEq12.gif"/></alternatives></inline-formula></td></tr></tbody></table><table-wrap-foot><p>Three hard constraints that can be implemented using constraint matrice, corresponding to the first three of the four constraints listed in the text</p></table-wrap-foot></table-wrap></p>
      <p id="Par28">The output of pre-trained model is a adjacency matrix <inline-formula id="IEq13"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_\theta (x)$$\end{document}</tex-math><mml:math id="M36"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4847_Article_IEq13.gif"/></alternatives></inline-formula> whose element values from 0 to 1. It’s a score of how likely each base is to pair. Module 3 will use the rules defined above to constrain the output of the upper network, and at the same time, we will also convert the score into a judgment of whether the reffered two bases matche. First, we define the output of the upper layer as <inline-formula id="IEq14"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_\theta (x)$$\end{document}</tex-math><mml:math id="M38"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4847_Article_IEq14.gif"/></alternatives></inline-formula> representing that it is the value obtained by the input through the previous network operation. Assuming that the output of each iteration is a new matrix <italic>A</italic>, then our optimization goal is the Eq. (<xref rid="Equ7" ref-type="">7</xref>). Constant <italic>b</italic> represents the threshold value, part <italic>f</italic> of the formula expects the element of <italic>A</italic> to approach one when <inline-formula id="IEq15"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_\theta \ge b$$\end{document}</tex-math><mml:math id="M40"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>≥</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4847_Article_IEq15.gif"/></alternatives></inline-formula>, otherwise it expects the value of this position to be zero. The matrix <italic>y</italic> represents label. Part <italic>g</italic> of the formula is set to let A be as close to y as possible. The rest of the formula is an L1 regularization parameters. The real secondary structure label is a sparse matrix, so we add this item to make <italic>A</italic> as sparse as possible. <inline-formula id="IEq16"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A1 \in R^N$$\end{document}</tex-math><mml:math id="M42"><mml:mrow><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mi>N</mml:mi></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4847_Article_IEq16.gif"/></alternatives></inline-formula> is a new matrix achieved by summing each row of <italic>A</italic>. <inline-formula id="IEq17"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A1 \le 1$$\end{document}</tex-math><mml:math id="M44"><mml:mrow><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4847_Article_IEq17.gif"/></alternatives></inline-formula> represents the 4th constraint which mentioned above.<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \begin{aligned} \max _{A\in R^{N\times N}}\underbrace{\langle S_\theta - b,A\rangle }_{\text {f}} + \underbrace{\langle y,A\rangle }_{\text {g}} - \eta \parallel A\parallel _1\quad \text {s.t.} \quad A1 \le 1 \end{aligned} \end{aligned}$$\end{document}</tex-math><mml:math id="M46" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munder><mml:mo movablelimits="true">max</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>⏟</mml:mo></mml:munder><mml:mtext>f</mml:mtext></mml:munder><mml:mo>+</mml:mo><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>⏟</mml:mo></mml:munder><mml:mtext>g</mml:mtext></mml:munder><mml:mo>-</mml:mo><mml:mi>η</mml:mi><mml:mo stretchy="false">‖</mml:mo><mml:mi>A</mml:mi><mml:msub><mml:mo stretchy="false">‖</mml:mo><mml:mn>1</mml:mn></mml:msub><mml:mspace width="1em"/><mml:mtext>s.t.</mml:mtext><mml:mspace width="1em"/><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4847_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>This is a conditional extremum problem. So we introduce the Lagrange multiplier <inline-formula id="IEq18"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda \in R^N$$\end{document}</tex-math><mml:math id="M48"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mi>N</mml:mi></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4847_Article_IEq18.gif"/></alternatives></inline-formula> and generate the Lagrange function.<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \begin{aligned} \min _{\lambda \ge 0}\max _{A\in R^{N\times N}}\underbrace{\langle S_\theta - b,A\rangle + \langle y,A\rangle - \langle \lambda , relu(A1-1) \rangle }_{\text {L}} - \eta \parallel A\parallel _1 \end{aligned} \end{aligned}$$\end{document}</tex-math><mml:math id="M50" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munder><mml:mo movablelimits="true">min</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:munder><mml:mo movablelimits="true">max</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mrow><mml:mo>⏟</mml:mo></mml:munder><mml:mtext>L</mml:mtext></mml:munder><mml:mo>-</mml:mo><mml:mi>η</mml:mi><mml:mo stretchy="false">‖</mml:mo><mml:mi>A</mml:mi><mml:msub><mml:mo stretchy="false">‖</mml:mo><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4847_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>The corresponding duality problem is the Eq. (<xref rid="Equ8" ref-type="">8</xref>).<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \begin{aligned} \max _{A\in R^{N\times N}}\min _{\lambda \ge 0}\underbrace{\langle S_\theta - b,A\rangle + \langle y,A\rangle - \langle \lambda , relu(A1-1) \rangle }_{\text {L}} - \eta \parallel A\parallel _1 \end{aligned} \end{aligned}$$\end{document}</tex-math><mml:math id="M52" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munder><mml:mo movablelimits="true">max</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:munder><mml:mo movablelimits="true">min</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mrow><mml:mo>⏟</mml:mo></mml:munder><mml:mtext>L</mml:mtext></mml:munder><mml:mo>-</mml:mo><mml:mi>η</mml:mi><mml:mo stretchy="false">‖</mml:mo><mml:mi>A</mml:mi><mml:msub><mml:mo stretchy="false">‖</mml:mo><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4847_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>We use gradient descent to solve the extreme point of each subproblems in the primal and dual problem. Therefore, it is necessary to find the derivation results of the L formula for <inline-formula id="IEq19"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda$$\end{document}</tex-math><mml:math id="M54"><mml:mi>λ</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_4847_Article_IEq19.gif"/></alternatives></inline-formula> and <italic>A</italic> respectively, and then substitute them into the gradient descent formula to update the values of <inline-formula id="IEq20"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda$$\end{document}</tex-math><mml:math id="M56"><mml:mi>λ</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_4847_Article_IEq20.gif"/></alternatives></inline-formula> and <italic>A</italic>. Then we can get the following equations.<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \begin{aligned} A_{t+1}=A_t + \rho _{\alpha }^{t} \cdot A_t \circ M \circ (\frac{\partial L}{\partial A_t} + \frac{\partial L}{\partial A_t}^\mathrm {T}) \end{aligned} \end{aligned}$$\end{document}</tex-math><mml:math id="M58" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo>·</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∘</mml:mo><mml:mi>M</mml:mi><mml:mo>∘</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msup><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mi mathvariant="normal">T</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4847_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \begin{aligned} \frac{\partial L}{\partial A_t}=S_\theta - b + y - (\lambda \circ sign(A1 - 1))1^\mathrm {T} \end{aligned} \end{aligned}$$\end{document}</tex-math><mml:math id="M60" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mi>b</mml:mi><mml:mo>+</mml:mo><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo>∘</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msup><mml:mn>1</mml:mn><mml:mi mathvariant="normal">T</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4847_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \begin{aligned} \lambda _{t+1}=\lambda _{t}+ \rho _{\beta }^{t} \cdot relu(A1-1) \end{aligned} \end{aligned}$$\end{document}</tex-math><mml:math id="M62" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo>·</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4847_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula>In the above formula, a learning rate <inline-formula id="IEq21"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rho &lt;1$$\end{document}</tex-math><mml:math id="M64"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4847_Article_IEq21.gif"/></alternatives></inline-formula> which is a power relationship with the number of cycles <italic>t</italic> is introduced, which constitutes a self-regulating learning rate. As the number of cycles increases, the learning rate will continue to decrease. In Eq. (<xref rid="Equ9" ref-type="">9</xref><xref rid="Equ10" ref-type="">10</xref><xref rid="Equ11" ref-type="">11</xref>), we also add the matrix <italic>M</italic> formed according to the three hard constraint rules mentioned above, and symmetric the gradient of each time.</p>
      <p id="Par29">In the subsequent iterations, <italic>A</italic> needs to be denoised every time. The method used is the hard threshold algorithm, which is equivalent to <inline-formula id="IEq22"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A_{ij}=\left\{ \begin{aligned} A_{ij}, \quad |A_{ij} |&gt; \eta \cdot \rho _{\alpha }^{t} \\ 0, \quad |A_{ij} |\le \eta \cdot \rho _{\alpha }^{t} \end{aligned} \right.$$\end{document}</tex-math><mml:math id="M66"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mi>η</mml:mi><mml:mo>·</mml:mo><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mrow/><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mo>≤</mml:mo><mml:mi>η</mml:mi><mml:mo>·</mml:mo></mml:mrow><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4847_Article_IEq22.gif"/></alternatives></inline-formula>. We can get Eq. (<xref rid="Equ12" ref-type="">12</xref>).<disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \begin{aligned} A_{t+1} = relu(|A_{t+1} |- \eta \cdot \rho _{\alpha }^{t}) \end{aligned} \end{aligned}$$\end{document}</tex-math><mml:math id="M68" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mo>-</mml:mo><mml:mi>η</mml:mi><mml:mo>·</mml:mo><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4847_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula>After a certain number of s rounds of iterations, we need to put hard constraints on the <inline-formula id="IEq23"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A_s$$\end{document}</tex-math><mml:math id="M70"><mml:msub><mml:mi>A</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4847_Article_IEq23.gif"/></alternatives></inline-formula> and symmetric it to get the optimized result matrix <italic>A</italic>.<disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \begin{aligned} A=\frac{1}{2}(A_s \circ M + (A_s \circ M)^\mathrm {T}) \end{aligned} \end{aligned}$$\end{document}</tex-math><mml:math id="M72" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>∘</mml:mo><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>∘</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4847_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula>After obtaining the output <italic>A</italic>, the loss function (<xref rid="Equ4" ref-type="">4</xref>) is used to calculate the loss value and the Adam optimization function is used to optimize the network.</p>
      <p id="Par30">Finally, this paper will explain the process of transfer learning. When training the pre-trained model, we remove the third layer of the network structure. We train the pre-trained model using the Weighed-logisitc function as the loss function. The data used for training is the data of the Rfam database. Training the network containing Transformer requires constant fine-tuning of the hyperparameters, so we set the learning rate to be reduced when the difference between the loss values of two epochs is less than 0.0004. After getting the pre-trained model, We reload the network and parameters of the pretrained model and add the aforementioned module 3 to the top layer of the network, and train this new network using data from the target RNA family. We use different fine-tuning strategies for different RNA families. For families with low data volume or high similarity to the Rfam data set which was used for pre-training, we freeze all the layers outside the top layer and train. For families with large data volume and low similarity, we train the whole network after loading the parameters of the pre-trained model. In the comparative experiments, only the SPR database satisfies the characteristics of small data volume and high similarity with pre-training dataset. In order to ensure that the model can obtain the best prediction effect, we use the first strategy in the transfer learning of SPR database, The rest of the databases use the second strategy. After fine-tuning and transfer learning, a series of models for different families can be obtained.</p>
    </sec>
    <sec id="Sec4">
      <title>Data collection and processing</title>
      <p id="Par31">In this paper, Transfer learning is used to train the network. The basis of transfer learning is pre-trained model. In order to train a good pre-trained model, a data set with wide coverage and uniform distribution in the data domain is needed. RNAs can be divided into different families according to the sequence length, shape and function. RNA sequences which belong to the same family share many features. To train a pre-trained model that can be widely used in RNA secondary structure prediction, a database with a group of family containing various RNA sequence data is needed.</p>
      <p id="Par32">The Rfam database [<xref ref-type="bibr" rid="CR51">51</xref>] is a collection of RNA families. As of Rfam v14.5, the database contains a total of 3940 RNA families, with a data volume of 43,273. It can meet the needs of the pre-trained model. The raw data in this paper came from two databases, bpRNA [<xref ref-type="bibr" rid="CR52">52</xref>] and RNAStralign [<xref ref-type="bibr" rid="CR53">53</xref>]. Firstly, the pre-trained model is trained using Rfam data. We can then use the pre-trained model to perform transfer learning on the data of a specific family that needs to be predicted, which is a process of deriving a model with a specific function from a generalized model.</p>
      <p id="Par33">The following describes the data processing methods. Firstly, according to the sequence length of the input data, two encoding lengths 128 and 512 are set to separate the data with the sequence length of no more than 128 from the other data between 128 and 512. We name them PT-128 and PT-512 respectively. Then, We need to de-redundant the data set. For the Rfam data that needs to be used for pre-trained model, this paper uses CD-HIT-EST [<xref ref-type="bibr" rid="CR54">54</xref>] to remove redundant data in the whole 43,273 pieces of data. The homology rate is set to 80%. Then the data with more than 80% homology rate is removed. For the data required for transfer learning, we still use the CD-HIT-EST to remove redundant data in each family separately. Above is to separate the two types of data to remove redundancy, and then we need to merge the two data to remove redundancy again. Finally, In Rfam data, 30,249 sequence data with low redundancy were obtained. Among them, PT-128 contains 21,487 pieces. PT-512 contains 8762 pieces. The data volume of each database is visible in the Table <xref rid="Tab4" ref-type="table">4</xref></p>
      <p id="Par34">The following will explain why we split the data into two encoding lengths. The length distribution of data from each family after processing can be seen in the Fig. <xref rid="Fig9" ref-type="fig">9</xref>.<fig id="Fig9"><label>Fig. 9</label><caption><p>Length profiles of different sequences. The vertical axis of the box chart is the name of the database, and the horizontal axis is the sequence length, and the orange line on the box represents the median</p></caption><graphic xlink:href="12859_2022_4847_Fig9_HTML" id="MO22"/></fig></p>
      <p id="Par35">It can be seen that the data of basically all databases are distributed into two sections between 0-128 and 128-512. Therefore, different pre-trained models should be used for transfer learning. In this paper, PT-128 data and PT-512 data will be used to train two pre-trained models aiming at the two kinds of sequence lengths respectively. This idea is based on such a scenario, if only one kind of pre-trained model is used, then the sequence with length less than 128 also needs to be processed into 512-length data for training. The effective length of input data will be less than 128, and the length of useless encodings will exceed 384. The ratio of the two will be less than 1:3. Valid sequences will be in an absolute minority. Then the model can consider the useless encodings as a valid sequence and affect the process of feature extraction. On the other hand, the 512 length of the training data is more expensive than 128 length of training data. The memory occupied by the length of the 512-length sequence is four times the memory size of the 128-length sequence, and the label set memory usage will be 16 times, the training network parameters will also be in the index level of 4 for growth. It can be seen from the Table <xref rid="Tab2" ref-type="table">2</xref>, the number of 128-length sequences is far greater than that of 512-length sequences. If these 128-length sequences are processed into 512-length input data, it will cause huge computational force and storage resource waste. Therefore, this paper uses two kinds of encoding length and pre-trained models to train the data of two sections respectively.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Sequence length tables for different databases</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Length</th><th align="left">Samples</th></tr></thead><tbody><tr><td align="left">Rfam</td><td align="left">19–3932</td><td align="left">30,249</td></tr><tr><td align="left">5SrRNA</td><td align="left">104–132</td><td align="left">10,788</td></tr><tr><td align="left">tRNA</td><td align="left">59–95</td><td align="left">9245</td></tr><tr><td align="left">PDB</td><td align="left">4–2902</td><td align="left">669</td></tr><tr><td align="left">SPR</td><td align="left">54–93</td><td align="left">622</td></tr><tr><td align="left">grpI</td><td align="left">163–615</td><td align="left">2135</td></tr><tr><td align="left">RNP</td><td align="left">189–486</td><td align="left">466</td></tr><tr><td align="left">SRP</td><td align="left">28–533</td><td align="left">959</td></tr><tr><td align="left">telomerase</td><td align="left">382–559</td><td align="left">37</td></tr><tr><td align="left">tmRNA</td><td align="left">102–437</td><td align="left">637</td></tr></tbody></table><table-wrap-foot><p>The length column represents the distribution of all sequences’ length in the database, and the samples column represents the number of sequences in the database</p></table-wrap-foot></table-wrap></p>
      <p id="Par36">The Embedding layer in the network structure can independently learn the word vector of the base pair through the data. Therefore, we just need to maps the literal representation of the bases once and the input them into network. The mapping relationship is shown in the Table <xref rid="Tab3" ref-type="table">3</xref>:<table-wrap id="Tab3"><label>Table 3</label><caption><p>Numeric coding of different bases</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Encoding</th></tr></thead><tbody><tr><td align="left">A</td><td align="left">1</td></tr><tr><td align="left">U</td><td align="left">2</td></tr><tr><td align="left">G</td><td align="left">3</td></tr><tr><td align="left">C</td><td align="left">4</td></tr><tr><td align="left">#</td><td align="left">0</td></tr></tbody></table></table-wrap></p>
      <p id="Par37">Before pre-training and transfer learning, this paper binds data and labels one by one and then randomly disrupts the order. 80% of the data is taken as the training set, 10% of the data is taken as the test set, and the remaining 10% is the verification set used in the experiment. The composition of the entire data set is shown in the Table <xref rid="Tab4" ref-type="table">4</xref>:<table-wrap id="Tab4"><label>Table 4</label><caption><p>The composition of the entire data set</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Samples</th><th align="left">Train</th><th align="left">Test</th><th align="left">Validate</th></tr></thead><tbody><tr><td align="left">Rfam_128</td><td align="left">21487</td><td align="left">17190</td><td align="left">2149</td><td align="left">2148</td></tr><tr><td align="left">Rfam_512</td><td align="left">8762</td><td align="left">7010</td><td align="left">876</td><td align="left">876</td></tr><tr><td align="left">5SrRNA</td><td align="left">10788</td><td align="left">8630</td><td align="left">1079</td><td align="left">1079</td></tr><tr><td align="left">tRNA</td><td align="left">9245</td><td align="left">7396</td><td align="left">925</td><td align="left">924</td></tr><tr><td align="left">PDB</td><td align="left">669</td><td align="left">535</td><td align="left">67</td><td align="left">67</td></tr><tr><td align="left">SPR</td><td align="left">622</td><td align="left">498</td><td align="left">62</td><td align="left">62</td></tr><tr><td align="left">grpI</td><td align="left">2135</td><td align="left">1708</td><td align="left">214</td><td align="left">213</td></tr><tr><td align="left">RNP</td><td align="left">466</td><td align="left">373</td><td align="left">47</td><td align="left">46</td></tr><tr><td align="left">SRP</td><td align="left">959</td><td align="left">767</td><td align="left">96</td><td align="left">96</td></tr><tr><td align="left">telomerase</td><td align="left">37</td><td align="left">30</td><td align="left">3</td><td align="left">4</td></tr><tr><td align="left">tmRNA</td><td align="left">637</td><td align="left">510</td><td align="left">64</td><td align="left">63</td></tr></tbody></table><table-wrap-foot><p>The column of samples represents the total number of a data set. The column train represents the data volume of the training set, and similarly test and validate represent the data volume of the test set and validation set</p></table-wrap-foot></table-wrap></p>
    </sec>
  </sec>
  <sec id="Sec5">
    <title>Results</title>
    <p id="Par38">This paper is committed to using a novel end-to-end method to solve the problem of predicting RNA secondary structure. Roughly speaking, we use transfer learning method to train the whole network mentioned above and get model to predict RNA secondary structure. Now we will design comparative experiments to test the performance of our model.</p>
    <p id="Par39">In each experiment, the state of prediction for each base pairs can be divided into four situations: true positive, false positive, true negative and false negative. <italic>TP</italic>, <italic>FP</italic>, <italic>TN</italic> and <italic>FN</italic> were used to represent the number of samples corresponding to the four scenarios, and the confusion matrix of the classification results was shown in the Table <xref rid="Tab5" ref-type="table">5</xref>.<table-wrap id="Tab5"><label>Table 5</label><caption><p>Classification confusion matrix for dichotomy problems</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Label</th><th align="left" colspan="2">Prediction</th></tr><tr><th align="left">Positive</th><th align="left">Negative</th></tr></thead><tbody><tr><td align="left">Positive</td><td align="left">TP</td><td align="left">FN</td></tr><tr><td align="left">Negative</td><td align="left">FP</td><td align="left">TN</td></tr></tbody></table><table-wrap-foot><p>The row name represents the state of that position in the label set, Positive represents 1, and Negative represents 0. The column name represents the state of that position in the prediction results. TP and FP represent the predicted results are consistent with the real results. FP represents false positives, and FN represents false negatives</p></table-wrap-foot></table-wrap></p>
    <p id="Par40">To better verify the model’s accuracy, the precision ratio P and recall ratio R were introduced to measure the model’s prediction ability.<disp-formula id="Equ14"><label>14</label><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} P=\frac{TP}{TP+FP} \end{aligned}$$\end{document}</tex-math><mml:math id="M74" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4847_Article_Equ14.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ15"><label>15</label><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} R=\frac{TP}{TP+FN} \end{aligned}$$\end{document}</tex-math><mml:math id="M76" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4847_Article_Equ15.gif" position="anchor"/></alternatives></disp-formula>However, recall rate and precision rate are mutually restricted. When recall rate is high, precision rate tends to be low. Similarly, when recall rate is low, precision rate tends to be high. If we need a win-win outcome, we need to introduce a measure <italic>F</italic>1.<disp-formula id="Equ16"><label>16</label><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} F1=\frac{2\times P \times R}{P+R} \end{aligned}$$\end{document}</tex-math><mml:math id="M78" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4847_Article_Equ16.gif" position="anchor"/></alternatives></disp-formula>When the value of <italic>F</italic>1 is high, it means that the values of <italic>P</italic> and <italic>R</italic> are both high. Therefore, in the experimental part of this paper, <italic>P</italic>, <italic>R</italic> and <italic>F</italic>1 will be used simultaneously to measure the model’s accuracy. All experimental data in this section are reserved to four decimal places.</p>
    <sec id="Sec6">
      <title>Experiment for the effect of different pre-training loss and constraint layer</title>
      <p id="Par41">In the section of method and materials, three loss functions are proposed for the fitting of the pre-trained model. In order to confirm which loss is most suitable for this topic, a comparative experiment should be designed for verification. Since this loss function is used to train the pre-trained model, this paper conducts tests on RFAM data, and the test results are as follows.<table-wrap id="Tab6"><label>Table 6</label><caption><p>Comparison of effects of models using different pre-training loss</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Loss</th><th align="left" colspan="3">Pre-train</th><th align="left" colspan="3">Train</th></tr><tr><th align="left">Precision</th><th align="left">Recall</th><th align="left">F1-score</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">F1-score</th></tr></thead><tbody><tr><td align="left">PRAUC-loss</td><td align="left">0.0351</td><td align="left">0.6177</td><td align="left">0.0665</td><td align="left">0.8599</td><td align="left">0.7897</td><td align="left">0.8233</td></tr><tr><td align="left">Negative-F1</td><td align="left">0.1564</td><td align="left">0.0987</td><td align="left">0.1210</td><td align="left">0.3587</td><td align="left">0.2107</td><td align="left">0.2655</td></tr><tr><td align="left">Weighed-logistic</td><td align="left">0.0283</td><td align="left">0.7559</td><td align="left">0.0546</td><td align="left">0.8963</td><td align="left">0.8015</td><td align="left">0.8462</td></tr><tr><td align="left">No-pretraining</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">0.8907</td><td align="left">0.5861</td><td align="left">0.7070</td></tr></tbody></table><table-wrap-foot><p>In the pre-training phase, the hard constraint layer is removed from the network and different loss functions are used for training. In the training phase, we used different pre-trained models for transfer learning to obtain prediction models on the Rfam dataset. The loss function of training phase is Neagtive-F1 function</p></table-wrap-foot></table-wrap></p>
      <p id="Par42">As can be seen from the Table <xref rid="Tab6" ref-type="table">6</xref>, the results obtained by using Negative-F1 are significantly different from the other two functions. Judging from the performance of pre-training, the <italic>P</italic> value of the Negative F1 model was higher than the other two models, but the <italic>R</italic> value was far lower than the other two models, indicating that the prediction of this model contains too many <italic>FP</italic> samples. In addition, the other two models have achieved better performance during transfer learning process, but the performance of the negative-F1 model has not improved much. This is because the loss function used in transfer learning is still the negative-F1 function. The same loss function used in the two training process makes it impossible for training to get rid of the local optimum trapped in the pre-training process. As can be seen from the data in the table, in the training phase, the performance of the model pre-trained by Weighed-logistic function is slightly better than PRAUC-loss. In terms of operation overhead, since PRAUC-loss needs to calculate the partitioned area under the P-R curve for 90 times, the storage overhead and operation time are not as good as Weighed-logistic. In conclusion, using Weighed-logisitc function as the pre-training loss can achieve extremely high prediction accuracy and relatively small operation overhead, which is the best scheme for pre-training.</p>
      <p id="Par43">Comparing the pre-train and train(transfer learning) phase in the Table <xref rid="Tab6" ref-type="table">6</xref>, it can be found that the addition of hard constraint layer can significantly improve the performance of the model. According to the data analysis, in the PRAUC model, the transfer learning model with the constraint layer which is also called module 3 increases by 1138% in the <italic>F</italic>1 score and 2350% in the <italic>P</italic>% value over the pre-trained model, and the <italic>R</italic> value is increased by 27.85%. In Negative-F1, the model with constraint layer improved by 119.4% in the <italic>F</italic>1 score, 129.3% in the <italic>P</italic> value, and 113.5% in the <italic>R</italic> value. In Weighed-logistic, the model with hard constraints was improved by 1138% in the <italic>F</italic>1 score, 3067% in the <italic>P</italic> value and 6.03% in the <italic>R</italic> value. To more clearly show how the hard constraint layer improves the prediction of the model, the Table <xref rid="Tab7" ref-type="table">7</xref> lists the values of <italic>TP</italic>, <italic>FP</italic>, <italic>TN</italic> and <italic>FN</italic> in the pre-training and training process for analysis.<table-wrap id="Tab7"><label>Table 7</label><caption><p>Comparison of confusion matrix value between pre-trained and trained model</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Loss</th><th align="left" colspan="4">Pre-train</th><th align="left" colspan="4">Train</th></tr><tr><th align="left">TP</th><th align="left">FP</th><th align="left">TN</th><th align="left">FN</th><th align="left">TP</th><th align="left">FP</th><th align="left">TN</th><th align="left">FN</th></tr></thead><tbody><tr><td align="left">PRAUC-loss</td><td align="left">98,045</td><td align="left">2,691,310</td><td align="left">66,601,770</td><td align="left">60,662</td><td align="left">125,331</td><td align="left">20,411</td><td align="left">69,272,660</td><td align="left">33,376</td></tr><tr><td align="left">Negative-F1</td><td align="left">15,665</td><td align="left">84,478</td><td align="left">69,208,610</td><td align="left">143,043</td><td align="left">33,447</td><td align="left">59,797</td><td align="left">69,233,270</td><td align="left">125,260</td></tr><tr><td align="left">Weighed-logistic</td><td align="left">119,968</td><td align="left">4,113,699</td><td align="left">65,179,376</td><td align="left">38,740</td><td align="left">127,198</td><td align="left">14,718</td><td align="left">69,278,350</td><td align="left">31,510</td></tr></tbody></table><table-wrap-foot><p>Columns of Pre-train is the prediction confusion matrix of different pre-trained model, and columns of Train belongs to the model after transfer learning</p></table-wrap-foot></table-wrap></p>
      <p id="Par44">As can be seen from the Table <xref rid="Tab7" ref-type="table">7</xref>, between the two processes of pre-training and training, the number of <italic>FP</italic> samples decreased significantly. Except for the Negative-F1 model, the number of <italic>FN</italic> also decreased significantly, while the number of <italic>TP</italic> and <italic>TN</italic> increased slightly. This shows that the main function of the hard constraint layer is to screen out the wrong pairs. At the same time, since each bases can only have one pair, by constantly screening the legitimate pairs and retraining the whole network, some pairs that once fell into local minima and failed to generate the gradient are also corrected. The above experimental shows that the constraint layer plays a very necessary role in screening out errors and breaking local minima.</p>
    </sec>
    <sec id="Sec7">
      <title>Contrast experiments with other models</title>
      <p id="Par45">In this subsection, the validation set is used to test the ability of LTPConstrain against other good methods. The methods used for the comparison are CONTRAfold, LinearFold [<xref ref-type="bibr" rid="CR55">55</xref>], ProbKnot [<xref ref-type="bibr" rid="CR56">56</xref>], RNAfold and CycleFold [<xref ref-type="bibr" rid="CR57">57</xref>]. The predicting results were converted into three values, <italic>P</italic>, <italic>R</italic> and <italic>F</italic>1, as shown in the Table <xref rid="Tab8" ref-type="table">8</xref>:<table-wrap id="Tab8"><label>Table 8</label><caption><p>Contrast experiments with other models</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Length</th><th align="left" rowspan="2">Family</th><th align="left" colspan="3">LTPConstraint</th><th align="left" colspan="3">CONTRAfold</th><th align="left" colspan="3">LinearFold</th><th align="left" colspan="3">ProbKnot</th><th align="left" colspan="3">RNAfold</th><th align="left" colspan="3">CycleFold</th></tr><tr><th align="left">Precision</th><th align="left">Recall</th><th align="left">F1-score</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">F1-score</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">F1-score</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">F1-score</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">F1-score</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">F1-score</th></tr></thead><tbody><tr><td align="left" rowspan="5">Encoding_128</td><td align="left">Rfam</td><td align="left">0.8599</td><td align="left">0.7897</td><td align="left">0.8233</td><td align="left">0.5016</td><td align="left">0.6186</td><td align="left">0.5540</td><td align="left">0.5429</td><td align="left">0.5502</td><td align="left">0.5465</td><td align="left">0.4705</td><td align="left">0.6118</td><td align="left">0.5319</td><td align="left">0.4598</td><td align="left">0.5965</td><td align="left">0.5193</td><td align="left">0.2887</td><td align="left">0.5329</td><td align="left">0.3745</td></tr><tr><td align="left">5SrRNA</td><td align="left">0.9857</td><td align="left">0.9804</td><td align="left">0.9831</td><td align="left">0.6541</td><td align="left">0.7337</td><td align="left">0.6916</td><td align="left">0.7330</td><td align="left">0.7306</td><td align="left">0.7318</td><td align="left">0.5748</td><td align="left">0.6036</td><td align="left">0.5888</td><td align="left">0.5716</td><td align="left">0.6401</td><td align="left">0.6039</td><td align="left">0.3094</td><td align="left">0.4984</td><td align="left">0.3818</td></tr><tr><td align="left">tRNA</td><td align="left">0.9985</td><td align="left">0.9992</td><td align="left">0.9988</td><td align="left">0.7047</td><td align="left">0.7787</td><td align="left">0.7399</td><td align="left">0.7445</td><td align="left">0.7504</td><td align="left">0.7474</td><td align="left">0.6777</td><td align="left">0.7691</td><td align="left">0.7205</td><td align="left">0.6659</td><td align="left">0.7378</td><td align="left">0.7000</td><td align="left">0.3097</td><td align="left">0.4695</td><td align="left">0.3732</td></tr><tr><td align="left">PDB</td><td align="left">0.6695</td><td align="left">0.3050</td><td align="left">0.4190</td><td align="left">0.0180</td><td align="left">0.0102</td><td align="left">0.0130</td><td align="left">0.0142</td><td align="left">0.0071</td><td align="left">0.0095</td><td align="left">0.0228</td><td align="left">0.0128</td><td align="left">0.0164</td><td align="left">0.0175</td><td align="left">0.0105</td><td align="left">0.0132</td><td align="left">0.0324</td><td align="left">0.0274</td><td align="left">0.0297</td></tr><tr><td align="left">SPR</td><td align="left">0.9929</td><td align="left">0.9971</td><td align="left">0.9950</td><td align="left">0.6730</td><td align="left">0.7496</td><td align="left">0.7092</td><td align="left">0.6990</td><td align="left">0.6763</td><td align="left">0.6875</td><td align="left">0.6338</td><td align="left">0.7319</td><td align="left">0.6793</td><td align="left">0.6355</td><td align="left">0.7208</td><td align="left">0.6755</td><td align="left">0.3182</td><td align="left">0.4865</td><td align="left">0.3848</td></tr><tr><td align="left" rowspan="5">Encoding_512</td><td align="left">grpl</td><td align="left">0.8304</td><td align="left">0.8894</td><td align="left">0.8589</td><td align="left">0.6589</td><td align="left">0.6509</td><td align="left">0.6549</td><td align="left">0.6713</td><td align="left">0.5557</td><td align="left">0.6080</td><td align="left">0.6124</td><td align="left">0.6401</td><td align="left">0.6259</td><td align="left">0.6013</td><td align="left">0.6383</td><td align="left">0.6192</td><td align="left">0.2068</td><td align="left">0.1055</td><td align="left">0.1397</td></tr><tr><td align="left">RNP</td><td align="left">0.5334</td><td align="left">0.7000</td><td align="left">0.6054</td><td align="left">0.5952</td><td align="left">0.5998</td><td align="left">0.5975</td><td align="left">0.5335</td><td align="left">0.4654</td><td align="left">0.4972</td><td align="left">0.5548</td><td align="left">0.5383</td><td align="left">0.5464</td><td align="left">0.4956</td><td align="left">0.5813</td><td align="left">0.5350</td><td align="left">0.2931</td><td align="left">0.2008</td><td align="left">0.2383</td></tr><tr><td align="left">SRP</td><td align="left">0.7130</td><td align="left">0.7378</td><td align="left">0.7252</td><td align="left">0.5731</td><td align="left">0.6279</td><td align="left">0.5992</td><td align="left">0.6204</td><td align="left">0.6099</td><td align="left">0.6151</td><td align="left">0.5693</td><td align="left">0.6179</td><td align="left">0.5926</td><td align="left">0.5670</td><td align="left">0.6234</td><td align="left">0.5938</td><td align="left">0.2621</td><td align="left">0.3810</td><td align="left">0.3105</td></tr><tr><td align="left">telomerase</td><td align="left">0.3752</td><td align="left">0.8728</td><td align="left">0.5248</td><td align="left">0.4327</td><td align="left">0.6083</td><td align="left">0.5057</td><td align="left">0.4334</td><td align="left">0.5670</td><td align="left">0.4913</td><td align="left">0.4026</td><td align="left">0.5483</td><td align="left">0.4643</td><td align="left">0.3912</td><td align="left">0.5572</td><td align="left">0.4597</td><td align="left">0.1193</td><td align="left">0.2330</td><td align="left">0.1578</td></tr><tr><td align="left">tmRNA</td><td align="left">0.7550</td><td align="left">0.8767</td><td align="left">0.8113</td><td align="left">0.4367</td><td align="left">0.4592</td><td align="left">0.4477</td><td align="left">0.4208</td><td align="left">0.3618</td><td align="left">0.3891</td><td align="left">0.3862</td><td align="left">0.4350</td><td align="left">0.4092</td><td align="left">0.3828</td><td align="left">0.4378</td><td align="left">0.4085</td><td align="left">0.1549</td><td align="left">0.2107</td><td align="left">0.1786</td></tr></tbody></table><table-wrap-foot><p>Six RNA secondary structure prediction models including LTPConstraint were tested using the processed validation set described in “Data collection and processing” section. Since LTPConstraint uses transfer learning, we need to use the pre-trained model. For the 5 families of Encoding_128 (see Table <xref rid="Tab4" ref-type="table">4</xref>), LTPConstraint uses pre-trained model trained by using Rfam_128 data, while for the 5 families of Encoding_512, LTPConstraint uses the pre-trained model trained from Rfam_512 data</p></table-wrap-foot></table-wrap></p>
      <p id="Par46">As can be seen from the Table <xref rid="Tab8" ref-type="table">8</xref>, except for PDB database where the prediction effect of all models is poor, LTPConstraint’s performance on other databases is better than that of all other models, which reflects the accuracy of the LTPConstraint model. Cause the <italic>F</italic>1 value can reflect the level of <italic>P</italic> value and <italic>R</italic> value simultaneously, we extracted the <italic>F</italic>1 value for further analysis. Since the prediction results on the PDB database will affect the overall distribution, we remove the results on the PDB dataset. We use box diagram and histograms to show the distribution of <italic>F</italic>1 values and the mean values of <italic>P</italic>, <italic>R</italic> and <italic>F</italic>1.<fig id="Fig10"><label>Fig. 10</label><caption><p>Box diagram showing all the <italic>F</italic>1 value predicted by the model. The orange line on the box represents the median, and the green triangle represents the average</p></caption><graphic xlink:href="12859_2022_4847_Fig10_HTML" id="MO26"/></fig><fig id="Fig11"><label>Fig. 11</label><caption><p>Histogram of the mean value of <italic>F</italic>1, <italic>P</italic> and <italic>R</italic>. The vertical axis of the histogram represents the value, and the horizontal axis is the name of each model. The purple column on the graph represents precision value, the orange column represents recall value, and the red column represents F1 value</p></caption><graphic xlink:href="12859_2022_4847_Fig11_HTML" id="MO27"/></fig></p>
      <p id="Par47">It is clear from the Fig. <xref rid="Fig10" ref-type="fig">10</xref> that LTPConstraint has stable and good performance. Comparing the stable areas of each model from the top quartile to the bottom quartile, it can be seen that LTPConstraint’s F1 value is closer to 1.0. The overall stability region of LTPConstraint has crossed chiefly the line of 0.8. In addition, the stability region is close to the upper edge line while the upper edge line basically coexists with 1.0. That is to say, the overall stability region is close to 1.0. This shows that the LTPConstraint model performs well in accuracy. Then we analyze the average prediction ability of each model from the Fig. <xref rid="Fig11" ref-type="fig">11</xref>. It can be clearly seen from the bar chart that LTPConstraint does better in the value of <italic>P</italic>, <italic>R</italic> and <italic>F</italic>1. By comparing with the best value of other methods, we find that LTPConstraint has achieved an increase of 0.1841 on the value <italic>P</italic> with an increase of 30.69% and 0.2259 on the value <italic>R</italic> with an increase of 34.89%, and 0.2046 on the value <italic>F</italic>1 with an increase of 33.48%. Through comparative experiments, we find that in the prediction of RNA sequences of different databases, the accuracy of LTPConstraint using transfer learning is obviously better than that of other models, and its performance is stable and can basically maintain the value <italic>F</italic>1 of 0.8 or above.</p>
    </sec>
    <sec id="Sec8">
      <title>Experiment on the role of transfer learning</title>
      <p id="Par48">This subsection will design a comparative experiment to test whether transfer learning really improves the performance of LTPConstraint. We used the control variable method to divide the experiment into two groups. The training set, test set and validation set of the two groups are identical. The first group uses the pre-trained model trained using Rfam data for transfer learning, while the second group does not carry out transfer learning and directly uses the database where the target data resides to train the whole network. In the whole experiment, the data set mentioned in subsection called data collection and processing is still used. Experimental results are shown in the Table <xref rid="Tab9" ref-type="table">9</xref>.<table-wrap id="Tab9"><label>Table 9</label><caption><p>Table of comparative results to examine transfer learning</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Length</th><th align="left" rowspan="2">Family</th><th align="left" colspan="4">Transfer_learning</th><th align="left" colspan="4">Non_transfer_learning</th></tr><tr><th align="left">Precision</th><th align="left">Recall</th><th align="left">F1-score</th><th align="left">ave-F1</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">F1-score</th><th align="left">ave-F1</th></tr></thead><tbody><tr><td align="left" rowspan="4">Encoding_128</td><td align="left">5SrRNA</td><td align="left">0.9857</td><td align="left">0.9804</td><td align="left">0.9831</td><td align="left" rowspan="4">0.8489</td><td align="left">0.9777</td><td align="left">0.8128</td><td align="left">0.8876</td><td align="left" rowspan="4">0.6862</td></tr><tr><td align="left">tRNA</td><td align="left">0.9985</td><td align="left">0.9992</td><td align="left">0.9988</td><td align="left">0.8968</td><td align="left">0.7873</td><td align="left">0.8385</td></tr><tr><td align="left">PDB</td><td align="left">0.6695</td><td align="left">0.3050</td><td align="left">0.4190</td><td align="left">0.2985</td><td align="left">0.1600</td><td align="left">0.2083</td></tr><tr><td align="left">SPR</td><td align="left">0.9929</td><td align="left">0.9971</td><td align="left">0.9950</td><td align="left">0.8833</td><td align="left">0.7489</td><td align="left">0.8106</td></tr><tr><td align="left" rowspan="5">Encoding_512</td><td align="left">grpl</td><td align="left">0.8304</td><td align="left">0.8894</td><td align="left">0.8589</td><td align="left" rowspan="5">0.7051</td><td align="left">0.3002</td><td align="left">0.0843</td><td align="left">0.1317</td><td align="left" rowspan="5">0.1235</td></tr><tr><td align="left">RNP</td><td align="left">0.5334</td><td align="left">0.7000</td><td align="left">0.6054</td><td align="left">0.2432</td><td align="left">0.1647</td><td align="left">0.1964</td></tr><tr><td align="left">SRP</td><td align="left">0.7130</td><td align="left">0.7378</td><td align="left">0.7252</td><td align="left">0.2236</td><td align="left">0.3636</td><td align="left">0.2769</td></tr><tr><td align="left">telomerase</td><td align="left">0.3752</td><td align="left">0.8728</td><td align="left">0.5248</td><td align="left">0.0033</td><td align="left">0.0072</td><td align="left">0.0045</td></tr><tr><td align="left">tmRNA</td><td align="left">0.7550</td><td align="left">0.8767</td><td align="left">0.8113</td><td align="left">0.0041</td><td align="left">0.9996</td><td align="left">0.0081</td></tr></tbody></table><table-wrap-foot><p>In the group of transfer learning, two kinds of pre-trained model are still trained by using the data of Rfam_128 and Rfam_512. After the results of each group are available, two average F1 value is calculated for the families with two kinds of coding length</p></table-wrap-foot></table-wrap></p>
      <p id="Par49">It can be seen from the Table <xref rid="Tab9" ref-type="table">9</xref> that the model with transfer learning has improved on the value of <italic>P</italic>, <italic>R</italic> and <italic>F</italic>1, especially in the data group with encoding length of 512, the improvement is more significant. In order to reflect the gap more clearly, the table also shows the average value of <italic>F</italic>1. According to the results, in the data with the encoding length of 128, the value <italic>F</italic>1 of the model using transfer learning is increased by 0.1673 on average, with a growth of 24.37% comparing with the model which don’t use transfer learning. While in the data with the encoding length of 512, it is increased by 0.5816 on average with a growth of 470.9%. It can be seen that transfer learning can significantly improve the accuracy of our model. However, the significance of transfer learning is obviously more than that. Why does a network perform well from the data with a short encoding length but not so well from the data with a encoding length of 512? This can be analyzed from the composition of data. The increase of encoding length will lead to the increase of features contained in the data. When the encoding length increases from 128 to 512, the amount of information increases by 16 times, and the features contained in the data may be more than 16 times. Therefore, more data should be input when training the data with the encoding length of 512 so that the training model can obtain the best effect of feature extraction, but in reality, the amount of data with the encoding length of 128 is about 5.1854:1 to that with the encoding length of 512. The effect of deep learning largely depends on the quantity and quality of data. If such a small amount of data is used for training, model is not able to learn enough. However, this problem can be solved well if we use transfer learning. The data with code length of 512 has a volume of 8762. After sufficient pre-training these data, the model has been mature in learning the shared features among data sets. Then transfer learning method is used to train for the target data. It can be seen in the table that a large improvement is produced using transfer learning. Therefore, Transfer learning is very necessary for training our network. It has the effect of improving accuracy and enabling the model to train some data sets with complex data but small data amount to partially overcome the dependence of deep learning on data.</p>
    </sec>
    <sec id="Sec9">
      <title>Experiment on predicting secondary structure containing pseudoknot</title>
      <p id="Par50">Since LTPConstraint uses pairing scores to indicate the pairing possibilities of two bases, the training of the network is not affected by the special spatial structure of pseudoknot, and the network is theoretically inherently suitable for predicting RNA secondary structure containing pseudoknot. To test this hypothesis, we need to design experiments. In the bpRNA database, there are many RNA sequences contain pseudoknot, and these sequences come from different families with different kinds of encoding length. Even if these data do not contain pseudoknot, it is still a difficult task to accurately predict these data’s secondary structure. Before describing the specific experiment, it is important to explain the data used. The raw data containing pseudoknot comes from bpRNA. Sequences are divided into two groups according to their length. Among these data, there are 2146 sequences with the length under 128, while there are 3011 medium-length sequences with the length between 128 and 512. The segmentation ratio of 0.8:0.1:0.1 is still adopted. This experiment will use such dataset to test the LTPConstraint model to see if the accuracy of LTPConstraint will be significantly affected in the presence of pseudoknot. Also, some good models that can predict pseudoknot will also predict the same data. We compare the results between LTPConstraint and other models to see if the LTPConstraint model has the ability to predict pseudoknot. There are very few models that can predict pseudoknot and two of the most excellent ones are selected and compared with LTPConstraint. They are ProbKnot and Knotty [<xref ref-type="bibr" rid="CR58">58</xref>]. The results are shown in the Table <xref rid="Tab10" ref-type="table">10</xref>.<table-wrap id="Tab10"><label>Table 10</label><caption><p>Comparative test results of predicting sequence with pseudoknot</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Model</th><th align="left" colspan="3">Pseudoknot_128</th><th align="left" colspan="3">Pseudoknot_512</th></tr><tr><th align="left">Precision</th><th align="left">Recall</th><th align="left">F1-score</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">F1-score</th></tr></thead><tbody><tr><td align="left">LTPConstraint</td><td align="left">0.9314</td><td align="left">0.8769</td><td align="left">0.9034</td><td align="left">0.7477</td><td align="left">0.7151</td><td align="left">0.7310</td></tr><tr><td align="left">ProbKnot</td><td align="left">0.5305</td><td align="left">0.5522</td><td align="left">0.5411</td><td align="left">0.3772</td><td align="left">0.4136</td><td align="left">0.3946</td></tr><tr><td align="left">Knotty</td><td align="left">0.5317</td><td align="left">0.6708</td><td align="left">0.5932</td><td align="left">0.3290</td><td align="left">0.3886</td><td align="left">0.3563</td></tr></tbody></table><table-wrap-foot><p>In the training of LTPConstraint, two kinds of pre-trained model are still trained by using the data of Rfam_128 and Rfam_512</p></table-wrap-foot></table-wrap></p>
      <p id="Par51">As seen from the Table <xref rid="Tab10" ref-type="table">10</xref>, LTPConstraint’s accuracy improves by more than 50% over the other two models, which is a significant improvement. Moreover, compared to the value <italic>F</italic>1 obtained from the data without pseudoknot (Table <xref rid="Tab9" ref-type="table">9</xref>), the LTPConstraint’s value <italic>F</italic>1 obtained from the data containing pseudoknot is even higher than the average. It can be seen that pseudoknot have no significant effect on LTPContraint’s results, confirming that LTPConstraint does have solid ability to predict structure containing pseudoknot.</p>
    </sec>
  </sec>
  <sec id="Sec10">
    <title>Discussion</title>
    <p id="Par52">When constructing the network, this paper uses a variety of neural network structure so that they can make up for their own shortcomings. The combined LTPConstraint network can give full play to the advantages of each substructure and improve the prediction accuracy. However, too much substructure causes the LTPConstraint network to be too large, which requires a large amount of memory resources during training. At the same time, the network transforms the sequence into an adjacency matrix, which leads to the two-dimensional labeling and the further expansion of the consumption of video memory, memory and computing resources, thus doubling the cost of training as the sequence is longer. In this paper, the limit encoding length is 512. For longer sequences, the training equipment used in this paper cannot meet the training requirements, so the training cost will affect the promotion of the LTPConstraint model. In addition, LTPConstraint’s dependence on the amount of data increases as sequence length increases, however, the number of long sequences is significantly less than the number of short sequences, which can cause LTPConstraint accuracy to slip when predicting medium and long sequences. At the same time, using transfer learning leads to the requirement of high quality data, but in the medium and long sequence under the condition of shortage, to ensure the data fields abundance has become a difficult task. This problem can be solved in the future if better databases provide more high-quality medium and long sequences.</p>
  </sec>
  <sec id="Sec11">
    <title>Conclusions</title>
    <p id="Par53">This paper constructs an end-to-end prediction model of RNA secondary structure LTPConstraint. The network has a variety of substructure, and different substructure cooperates with each other and complements each other, making up three modules of LTPConstraint network. The first module is composed of Bi-LSTM and Transformer Encoder to extract the deep semantic and matching information of the base sequence. In the second part, the local pairing information is transformed by a generator network, and the scoring matrix of each base pairing is generated. Then, the output in the form of an adjacency matrix is obtained by the modification and evolution of the hard constraint layer in the third module. We divided all the sequences into 128 and 512 levels according to the sequence length, and the data set obtained through careful selection was used for pre-training. Based on the pre-trained model, we use fine-tuning strategies to train models for the data set from different families, which reduced the training cost and improved the prediction accuracy of the model. That is the process of transfer learning. Through comparative experiments, we found that the use of appropriate loss function for pre-trained model can improve the effect of training. At the same time, we use the transfer learning method to greatly improve the accuracy of LTPConstraint in other RNA families that lack sufficient data. We compared the LTPConstraint model using transfer learning with other good models, and the results showed that LTPConstraint is better than other models in terms of accuracy and stability. On the premise of ensuring accuracy, the method used in this paper also partially overcomes the problem of deep learning’s dependence on data volume. Although LTPConstraint does good in RNA secondary structure prediction, we still think that our work is just a supplement of deep learning method for the problem of predicting RNA secondary structure. Simultaneously, the model still has many problems such as the high cost of training, the prediction accuracy is reduced due to the insufficient number of long sequences. In future work, we will optimize the compatibility of LTPConstraint for predicting long sequences while reducing the computational resources used by the model. We will also make the model more user-friendly and easier to generalize. We will apply this secondary structure prediction method to biological experiments, so as to provide biologists with more accurate reference and make contributions to the field of life science.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>The authors wish to thank Jilin Provincial Key Laboratory of New Biometrics Technology for its support to this project.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>YL, HZ and ZL are responsible for guiding the idea of LTPConstraint. YW are responsible for data collection and data preprocessing, and YF is responsible for model building and writing. All authors read and approved the fnal manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work was supported by the National Natural Science Foundation of China (61471181), the National Key Research and Development Program of China (2020YFB1709800), the National Key Research and Development Project of China ([2020]151) and Jilin Province Industrial Innovation Special Fund Project (2019C053-2, and 2019C053-6).</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The datasets generated and analysed during the current study are available in the LTPConstraint repository, <ext-link ext-link-type="uri" xlink:href="https://github.com/jluF/LTPConstraint.git">https://github.com/jluF/LTPConstraint.git</ext-link>.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar1">
      <title>Ethical approval and consent to participate</title>
      <p id="Par54">Not applicable.</p>
    </notes>
    <notes id="FPar2">
      <title>Consent for publication</title>
      <p id="Par55">Not applicable.</p>
    </notes>
    <notes id="FPar3" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par56">The authors declare that they have no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cooper</surname>
            <given-names>TA</given-names>
          </name>
          <name>
            <surname>Wan</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Dreyfuss</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>RNA and disease</article-title>
        <source>Cell</source>
        <year>2012</year>
        <volume>136</volume>
        <issue>4</issue>
        <fpage>777</fpage>
        <lpage>793</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2009.02.011</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Jie</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Cui</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <article-title>SpliceDisease database: linking RNA splicing and disease</article-title>
        <source>Nucleic Acids Res</source>
        <year>2012</year>
        <volume>40</volume>
        <issue>D1</issue>
        <fpage>1055</fpage>
        <lpage>1059</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkr1171</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sloma</surname>
            <given-names>MF</given-names>
          </name>
          <name>
            <surname>Mathews</surname>
            <given-names>DH</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>SJ</given-names>
          </name>
        </person-group>
        <article-title>Base pair probability estimates improve the prediction accuracy of RNA non-canonical base pairs</article-title>
        <source>PLOS Comput Biol</source>
        <year>2017</year>
        <volume>13</volume>
        <fpage>e1005827</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1005827</pub-id>
        <pub-id pub-id-type="pmid">29107980</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pleij Wa</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>RNA pseudoknot: structure, detection, and prediction</article-title>
        <source>Methods Enzymol</source>
        <year>1989</year>
        <volume>180</volume>
        <fpage>289</fpage>
        <lpage>303</lpage>
        <pub-id pub-id-type="doi">10.1016/0076-6879(89)80107-7</pub-id>
        <pub-id pub-id-type="pmid">2482419</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <mixed-citation publication-type="other">Chowdhury L, Khan MI. Pseudoknots prediction on RNA secondary structure using term rewriting. In: International conference on bioinformatics &amp; biomedical engineering</mixed-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Magdalena</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Kristian</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Tomasz</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Bujnicki</surname>
            <given-names>JM</given-names>
          </name>
        </person-group>
        <article-title>RNA tertiary structure prediction with modeRNA</article-title>
        <source>Brief Bioinform</source>
        <year>2011</year>
        <volume>6</volume>
        <fpage>601</fpage>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hajdin</surname>
            <given-names>CE</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Dokholyan</surname>
            <given-names>NV</given-names>
          </name>
          <name>
            <surname>Weeks</surname>
            <given-names>KM</given-names>
          </name>
        </person-group>
        <article-title>On the significance of an RNA tertiary structure prediction</article-title>
        <source>RNA</source>
        <year>2010</year>
        <volume>16</volume>
        <issue>7</issue>
        <fpage>1340</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1261/rna.1837410</pub-id>
        <pub-id pub-id-type="pmid">20498460</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Seetin</surname>
            <given-names>MG</given-names>
          </name>
          <name>
            <surname>Mathews</surname>
            <given-names>DH</given-names>
          </name>
        </person-group>
        <article-title>Automated RNA tertiary structure prediction from secondary structure and low-resolution restraints</article-title>
        <source>J Comput Chem</source>
        <year>2011</year>
        <volume>32</volume>
        <issue>10</issue>
        <fpage>2232</fpage>
        <lpage>2244</lpage>
        <pub-id pub-id-type="doi">10.1002/jcc.21806</pub-id>
        <pub-id pub-id-type="pmid">21509787</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mortimer</surname>
            <given-names>SA</given-names>
          </name>
          <name>
            <surname>Kidwell</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Doudna</surname>
            <given-names>JA</given-names>
          </name>
        </person-group>
        <article-title>Insights into RNA structure and function from genome-wide studies</article-title>
        <source>Nat Rev Genet</source>
        <year>2014</year>
        <volume>15</volume>
        <issue>7</issue>
        <fpage>469</fpage>
        <lpage>479</lpage>
        <pub-id pub-id-type="doi">10.1038/nrg3681</pub-id>
        <pub-id pub-id-type="pmid">24821474</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cordero</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Kladwang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Vanlang</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Das</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Quantitative DMS mapping for automated RNA secondary structure inference</article-title>
        <source>Biochemistry</source>
        <year>2012</year>
        <volume>51</volume>
        <issue>36</issue>
        <fpage>7037</fpage>
        <pub-id pub-id-type="doi">10.1021/bi3008802</pub-id>
        <pub-id pub-id-type="pmid">22913637</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wilkinson</surname>
            <given-names>KA</given-names>
          </name>
          <name>
            <surname>Merino</surname>
            <given-names>EJ</given-names>
          </name>
          <name>
            <surname>Weeks</surname>
            <given-names>KM</given-names>
          </name>
        </person-group>
        <article-title>Selective 2’-hydroxyl acylation analyzed by primer extension (shape): quantitative RNA structure analysis at single nucleotide resolution</article-title>
        <source>Nat Protoc</source>
        <year>2006</year>
        <volume>1</volume>
        <issue>3</issue>
        <fpage>1610</fpage>
        <lpage>1616</lpage>
        <pub-id pub-id-type="doi">10.1038/nprot.2006.249</pub-id>
        <pub-id pub-id-type="pmid">17406453</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mortimer</surname>
            <given-names>SA</given-names>
          </name>
          <name>
            <surname>Weeks</surname>
            <given-names>KM</given-names>
          </name>
        </person-group>
        <article-title>A fast-acting reagent for accurate analysis of RNA secondary and tertiary structure by shape chemistry</article-title>
        <source>J Am Chem Soc</source>
        <year>2007</year>
        <volume>129</volume>
        <issue>14</issue>
        <fpage>4144</fpage>
        <lpage>5</lpage>
        <pub-id pub-id-type="doi">10.1021/ja0704028</pub-id>
        <pub-id pub-id-type="pmid">17367143</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Benhlima S, Fatmi AE, Chentoufi A, Bekri MA, Sabbane M. A heuristic algorithm for RNA secondary structure based on genetic algorithm. In: Proceedings of the IEEE (2017).</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Oluoch IK, Akalin A, Vural Y, Canbay Y. A review on RNA secondary structure prediction algorithms. In: 2018 international congress on big data, deep learning and fighting cyber terrorism (IBIGDELFT).</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mattocks</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Tarpey</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Whittaker</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Comparative sequence analysis</article-title>
        <source>Methods Mol Med</source>
        <year>2004</year>
        <volume>92</volume>
        <issue>22</issue>
        <fpage>115</fpage>
        <?supplied-pmid 14733309?>
        <pub-id pub-id-type="pmid">14733309</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gautheret</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Damberger</surname>
            <given-names>SH</given-names>
          </name>
          <name>
            <surname>Gutell</surname>
            <given-names>RR</given-names>
          </name>
        </person-group>
        <article-title>Identification of base-triples in RNA using comparative sequence analysis</article-title>
        <source>J Mol Biol</source>
        <year>1995</year>
        <volume>248</volume>
        <issue>1</issue>
        <fpage>27</fpage>
        <lpage>43</lpage>
        <pub-id pub-id-type="doi">10.1006/jmbi.1995.0200</pub-id>
        <pub-id pub-id-type="pmid">7537339</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Williams</surname>
            <given-names>AL</given-names>
          </name>
          <name>
            <surname>Ignacio</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>A dynamic programming algorithm for finding alternative RNA secondary structures</article-title>
        <source>Nucleic Acids Res</source>
        <year>1986</year>
        <volume>1</volume>
        <fpage>299</fpage>
        <pub-id pub-id-type="doi">10.1093/nar/14.1.299</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Reuter</surname>
            <given-names>JS</given-names>
          </name>
          <name>
            <surname>Mathews</surname>
            <given-names>DH</given-names>
          </name>
        </person-group>
        <article-title>RNAstructure: software for RNA secondary structure prediction and analysis</article-title>
        <source>BMC Bioinform</source>
        <year>2010</year>
        <volume>11</volume>
        <issue>1</issue>
        <fpage>873</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-11-129</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Michael</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Patrick</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Optimal computer folding of large RNA sequences using thermodynamics and auxiliary information</article-title>
        <source>Nucleic Acids Res</source>
        <year>1981</year>
        <volume>9</volume>
        <issue>1</issue>
        <fpage>133</fpage>
        <lpage>148</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/9.1.133</pub-id>
        <pub-id pub-id-type="pmid">6163133</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Turner</surname>
            <given-names>DH</given-names>
          </name>
          <name>
            <surname>Mathews</surname>
            <given-names>DH</given-names>
          </name>
        </person-group>
        <article-title>NNDB: the nearest neighbor parameter database for predicting stability of nucleic acid secondary structure</article-title>
        <source>Nucleic Acids Res</source>
        <year>2009</year>
        <volume>38</volume>
        <issue>Database issue</issue>
        <fpage>280</fpage>
        <lpage>2</lpage>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lorenz</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Bernhart</surname>
            <given-names>SH</given-names>
          </name>
          <name>
            <surname>Siederdissen</surname>
            <given-names>CHZ</given-names>
          </name>
          <name>
            <surname>Tafer</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Flamm</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Stadler</surname>
            <given-names>PF</given-names>
          </name>
          <name>
            <surname>Hofacker</surname>
            <given-names>IL</given-names>
          </name>
        </person-group>
        <article-title>Viennarna package 2.0</article-title>
        <source>Algorithms Mol Biol</source>
        <year>2011</year>
        <volume>6</volume>
        <issue>1</issue>
        <fpage>26</fpage>
        <pub-id pub-id-type="doi">10.1186/1748-7188-6-26</pub-id>
        <pub-id pub-id-type="pmid">22115189</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Do</surname>
            <given-names>CB</given-names>
          </name>
          <name>
            <surname>Woods</surname>
            <given-names>DA</given-names>
          </name>
          <name>
            <surname>Batzoglou</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>CONTRAfold: RNA secondary structure prediction without physics-based models</article-title>
        <source>Bioinformatics</source>
        <year>2006</year>
        <volume>22</volume>
        <issue>14</issue>
        <fpage>90</fpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btl246</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Willmott</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Murrugarra</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Ye</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <article-title>Improving RNA secondary structure prediction via state inference with deep recurrent neural networks</article-title>
        <source>Comput Math Biophys</source>
        <year>2020</year>
        <volume>8</volume>
        <issue>1</issue>
        <fpage>36</fpage>
        <lpage>50</lpage>
        <pub-id pub-id-type="doi">10.1515/cmb-2020-0002</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Swenson</surname>
            <given-names>MS</given-names>
          </name>
          <name>
            <surname>Anderson</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ash</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gaurav</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>GTfold: enabling parallel RNA secondary structure prediction on multi-core desktops</article-title>
        <source>BMC Res Notes</source>
        <year>2012</year>
        <volume>5</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>6</lpage>
        <pub-id pub-id-type="doi">10.1186/1756-0500-5-341</pub-id>
        <pub-id pub-id-type="pmid">22214347</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Pathak D, Krahenbuhl P, Donahue J, Darrell T, Efros AA. Context encoders: feature learning by inpainting. IEEE (2016).</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Isola P, Zhu JY, Zhou T, Efros AA. Image-to-image translation with conditional adversarial networks. In: IEEE conference on computer vision &amp; pattern recognition.</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Radford A, Metz L, Chintala S. Unsupervised representation learning with deep convolutional generative adversarial networks. In: ICLR.</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Richardson E, Alaluf Y, Patashnik O, Nitzan Y, Azar Y, Shapiro S, Cohen-Or D. Encoding in style: a stylegan encoder for image-to-image translation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 2287–96.</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Patashnik O, Danon D, Zhang H, Cohen-Or D. Balagan: cross-modal image translation between imbalanced domains. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 2659–67.</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hochreiter</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Schmidhuber</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Long short-term memory</article-title>
        <source>Neural Comput</source>
        <year>1997</year>
        <volume>9</volume>
        <issue>8</issue>
        <fpage>1735</fpage>
        <lpage>1780</lpage>
        <pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id>
        <pub-id pub-id-type="pmid">9377276</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Karpathy A, Johnson J, Li FF. Visualizing and understanding recurrent networks (2015).</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Cho K, Merrienboer BV, Gulcehre C, Ba Hdanau D, Bougares F, Schwenk H, Bengio Y. Learning phrase representations using RNN encoder-decoder for statistical machine translation. Comput Sci. 2014.</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">Chorowski J, Bahdanau D, Cho K, Bengio Y. End-to-end continuous speech recognition using attention-based recurrent NN: first results. 2014.</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zheng</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>An attention-based Bi-LSTM method for visual object classification via EEG</article-title>
        <source>Biomed Signal Process Control</source>
        <year>2021</year>
        <volume>63</volume>
        <fpage>102174</fpage>
        <pub-id pub-id-type="doi">10.1016/j.bspc.2020.102174</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser L, Polosukhin I. Attention is all you need. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1706.03762">arXiv:1706.03762</ext-link> (2017).</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Chen L, Lu K, Rajeswaran A, Lee K, Grover A, Laskin M, Abbeel P, Srinivas A, Mordatch I. Decision transformer: reinforcement learning via sequence modeling. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2106.01345">arXiv:2106.01345</ext-link> (2021).</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, Lin S, Guo B. Swin transformer: hierarchical vision transformer using shifted windows. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2103.14030">arXiv:2103.14030</ext-link> (2021).</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Arnab A, Dehghani M, Heigold G, Sun C, Lučić M, Schmid C. Vivit: a video vision transformer. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2103.15691">arXiv:2103.15691</ext-link> (2021).</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Müller G, Rios M, Sennrich A. Rico: why self-attention? A targeted evaluation of neural machine translation architectures. In: Proceedings of the 2018 conference on empirical methods in natural language processing; 2018.</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">2020. <ext-link ext-link-type="uri" xlink:href="http://proceedings.mlr.press/v119/zhao20c.html">http://proceedings.mlr.press/v119/zhao20c.html</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">2020. <ext-link ext-link-type="uri" xlink:href="http://proceedings.mlr.press/v119/liu20n.html">http://proceedings.mlr.press/v119/liu20n.html</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <mixed-citation publication-type="other">Tan C, Sun F, Kong T, Zhang W, Yang C, Liu C. A survey on deep transfer learning. In: International conference on artificial neural networks. pp. 270–79. Springer.</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Yosinski J, Clune J, Bengio Y, Lipson H. How transferable are features in deep neural networks? arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1411.1792">arXiv:1411.1792</ext-link> (2014).</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <mixed-citation publication-type="other">Bousmalis K, Trigeorgis G, Silberman N, Krishnan D, Erhan D. Domain separation networks. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1608.06019">arXiv:1608.06019</ext-link> (2016).</mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Marmanis</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Datcu</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Esch</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Stilla</surname>
            <given-names>U</given-names>
          </name>
        </person-group>
        <article-title>Deep learning earth observation classification using ImageNet pretrained networks</article-title>
        <source>IEEE Geosci Remote Sens Lett</source>
        <year>2015</year>
        <volume>13</volume>
        <issue>1</issue>
        <fpage>105</fpage>
        <lpage>109</lpage>
        <pub-id pub-id-type="doi">10.1109/LGRS.2015.2499239</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <mixed-citation publication-type="other">Gupta D, Jain S, Shaikh F, Singh G. Transfer learning &amp; the art of using pre-trained models in deep learning. Analytics Vidhya. 2017.</mixed-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <mixed-citation publication-type="other">Shermin T, Teng SW, Murshed M, Lu G, Sohel F, Paul M. Enhanced transfer learning with ImageNet trained classification layer. In: Pacific-rim symposium on image and video technology. pp. 142–155. Springer.</mixed-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Takaya</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Marc</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Guy</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets</article-title>
        <source>PLoS ONE</source>
        <year>2015</year>
        <volume>10</volume>
        <issue>3</issue>
        <fpage>0118432</fpage>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <mixed-citation publication-type="other">John AC, A, LR, M, TJ, Mona S, Thomas AF. Area under the precision-recall curve (PR-AUC) for ligand-binding residue prediction methods on apo (unbound) and holo (bound) versions of ligasite. 2009.</mixed-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Steeg</surname>
            <given-names>EW</given-names>
          </name>
        </person-group>
        <article-title>Neural networks, adaptive optimization, and RNA secondary structure prediction</article-title>
        <source>Artif Intell Mol Biol</source>
        <year>1993</year>
        <volume>25</volume>
        <fpage>121</fpage>
        <lpage>160</lpage>
      </element-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <mixed-citation publication-type="other">Sam G-J, Alex B, Mhairi MA, Khanna SR, Eddy. RFAM: an RNA family database. Nucleic Acids Res. 2003.</mixed-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <mixed-citation publication-type="other">Padideh D, Mason R, Michelle W, Dezhong D, Liang H, David H. BPRNA: large-scale automated annotation and analysis of RNA secondary structure. Nucleic Acids Res 11, 11.</mixed-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhen</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Yinghan</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Gaurav</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Mathew</surname>
            <given-names>DH</given-names>
          </name>
        </person-group>
        <article-title>TurboFold II: RNA structural alignment and secondary structure prediction informed by multiple homologs</article-title>
        <source>Nucleic Acids Res</source>
        <year>2017</year>
        <volume>45</volume>
        <issue>20</issue>
        <fpage>25</fpage>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fu</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Niu</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>CD-HIT: accelerated for clustering the next-generation sequencing data</article-title>
        <source>Bioinformatics</source>
        <year>2012</year>
        <volume>23</volume>
        <fpage>3150</fpage>
        <lpage>3152</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bts565</pub-id>
      </element-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Hendrix</surname>
            <given-names>DA</given-names>
          </name>
          <name>
            <surname>Mathews</surname>
            <given-names>DH</given-names>
          </name>
        </person-group>
        <article-title>LinearFold: linear-time approximate RNA folding by 5’-to-3’dynamic programming and beam search</article-title>
        <source>Bioinformatics</source>
        <year>2019</year>
        <volume>35</volume>
        <issue>14</issue>
        <fpage>295</fpage>
        <lpage>304</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btz375</pub-id>
      </element-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bellaousov</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Mathews</surname>
            <given-names>DH</given-names>
          </name>
        </person-group>
        <article-title>ProbKnot: fast prediction of RNA secondary structure including pseudoknots</article-title>
        <source>RNA</source>
        <year>2010</year>
        <volume>16</volume>
        <issue>10</issue>
        <fpage>1870</fpage>
        <lpage>1880</lpage>
        <pub-id pub-id-type="doi">10.1261/rna.2125310</pub-id>
        <pub-id pub-id-type="pmid">20699301</pub-id>
      </element-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sloma</surname>
            <given-names>MF</given-names>
          </name>
          <name>
            <surname>Mathews</surname>
            <given-names>DH</given-names>
          </name>
        </person-group>
        <article-title>Base pair probability estimates improve the prediction accuracy of RNA non-canonical base pairs</article-title>
        <source>PLoS Comput Biol</source>
        <year>2017</year>
        <volume>13</volume>
        <issue>11</issue>
        <fpage>1005827</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1005827</pub-id>
      </element-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jabbari</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Wark</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Montemagno</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Will</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Knotty: efficient and accurate prediction of complex RNA pseudoknot structures</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>34</volume>
        <issue>22</issue>
        <fpage>3849</fpage>
        <lpage>3856</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty420</pub-id>
        <pub-id pub-id-type="pmid">29868872</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
