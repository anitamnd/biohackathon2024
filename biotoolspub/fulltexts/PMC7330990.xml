<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7330990</article-id>
    <article-id pub-id-type="publisher-id">3631</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-020-03631-1</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>USMPep: universal sequence models for major histocompatibility complex binding affinity prediction</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Vielhaben</surname>
          <given-names>Johanna</given-names>
        </name>
        <address>
          <email>johanna.vielhaben@hhi.fraunhofer.de</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wenzel</surname>
          <given-names>Markus</given-names>
        </name>
        <address>
          <email>markus.wenzel@hhi.fraunhofer.de</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Samek</surname>
          <given-names>Wojciech</given-names>
        </name>
        <address>
          <email>wojciech.samek@hhi.fraunhofer.de</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4447-0162</contrib-id>
        <name>
          <surname>Strodthoff</surname>
          <given-names>Nils</given-names>
        </name>
        <address>
          <email>nils.strodthoff@hhi.fraunhofer.de</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="GRID">grid.435231.2</institution-id><institution-id institution-id-type="ISNI">0000 0004 0495 5488</institution-id><institution>Fraunhofer Heinrich Hertz Institute, </institution></institution-wrap>Einsteinufer 37, Berlin, 10587 Germany </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>2</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>2</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2020</year>
    </pub-date>
    <volume>21</volume>
    <elocation-id>279</elocation-id>
    <history>
      <date date-type="received">
        <day>14</day>
        <month>11</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>23</day>
        <month>6</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Immunotherapy is a promising route towards personalized cancer treatment. A key algorithmic challenge in this process is to decide if a given peptide (neoepitope) binds with the major histocompatibility complex (MHC). This is an active area of research and there are many MHC binding prediction algorithms that can predict the MHC binding affinity for a given peptide to a high degree of accuracy. However, most of the state-of-the-art approaches make use of complicated training and model selection procedures, are restricted to peptides of a certain length and/or rely on heuristics.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">We put forward <italic>USMPep</italic>, a simple recurrent neural network that reaches state-of-the-art approaches on MHC class I binding prediction with a single, generic architecture and even a single set of hyperparameters both on IEDB benchmark datasets and on the very recent HPV dataset. Moreover, the algorithm is competitive for a single model trained from scratch, while ensembling multiple regressors and language model pretraining can still slightly improve the performance. The direct application of the approach to MHC class II binding prediction shows a solid performance despite of limited training data.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">We demonstrate that competitive performance in MHC binding affinity prediction can be reached with a standard architecture and training procedure without relying on any heuristics.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Major histocompatibility complex</kwd>
      <kwd>Binding affinity prediction</kwd>
      <kwd>Peptide data</kwd>
      <kwd>Recurrent neural networks</kwd>
      <kwd>Language modeling</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2020</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p>Immunotherapy is a promising route towards personalized cancer treatment with a variety of possible realizations, see [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR4">4</xref>] for recent reviews. One path is the administration of nanoparticle vaccines customized with neoantigens. The major histocompatibility complex plays a central role in this process as it is supposed to bind to peptides derived from proteins of the cell or from pathogens in order to display them on the surface of the cell for recognition by T-cells. There are three classes of MHC molecules, where MHC class I and II are most important due to their involvement in the targeted immune response. Due to the special nature of the MHC protein, it can bind to peptides that are potentially structurally very different from each other. Therefore, the prediction if a MHC molecule binds to certain peptide is a very challenging task that is, however, a crucial sub task for neoantigen identification for practical realizations of personalized immunotherapy [<xref ref-type="bibr" rid="CR1">1</xref>].</p>
    <p>MHC binding prediction is a well-established problem in bioinformatics with a large number of existing algorithmic solutions. Although many of these algorithms show an excellent performance, they typically rely on complicated training procedures to achieve this performance, such as pretraining on prediction tasks for related alleles or training with artificial negative peptides. In addition, existing solutions use complicated model selection procedures to select a small number of well-performing models from potentially hundreds of trained models to eventually construct an ensemble classifier. Most of the existing approaches are restricted to peptides of fixed length, where shorter sequences are padded or longer sequences are trimmed to an appropriate length by well-motivated but still heuristic rules to identify so-called binding regions. The most prominent MHC I prediction algorithms are summarized in Table <xref rid="Tab1" ref-type="table">1</xref>. We refer to dedicated reviews for more detailed comparisons [<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR6">6</xref>].
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Comparison of MHC I prediction tools</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" colspan="2"><italic>Architecture</italic></td></tr><tr><td align="left">SMMPMBEC [<xref ref-type="bibr" rid="CR7">7</xref>]</td><td align="left">One-hot encoding, linear model (scoring matrix)</td></tr><tr><td align="left">consensus [<xref ref-type="bibr" rid="CR8">8</xref>]</td><td align="left">Linear model (scoring matrix), median rank as prediction</td></tr><tr><td align="left">NetMHC4 [<xref ref-type="bibr" rid="CR9">9</xref>]</td><td align="left">Input: 9mer fixed length blocks substitution matrix (BLOSUM) encoding plus additional features; multilayer perceptron with one hidden layer</td></tr><tr><td align="left">NetMHCpan4 [<xref ref-type="bibr" rid="CR10">10</xref>]</td><td align="left">Input: 9mer fixed length BLOSUM encoding for peptide, pseudo-sequence for MHC molecule plus additional features; multilayer perceptron with one hidden layer</td></tr><tr><td align="left">MHCFlurry [<xref ref-type="bibr" rid="CR11">11</xref>]</td><td align="left">Input: 15mer fixed length BLOSUM62 encoding, missing residues filled with wildcard amino acid (AA); feedforward neural network (NN) with 0 to 2 locally connected and one fully connected hidden layer</td></tr><tr><td align="left"><bold>USMPep</bold> (this work)</td><td align="left">Learned embedding layer; AWD LSTM with one hidden layer</td></tr><tr><td align="left" colspan="2"><italic>Training procedure</italic></td></tr><tr><td align="left">SMMPMBEC</td><td align="left">Ridge regression with modified regularization, peptide MHC binding energy covariance (PMBEC) similarity matrix as Bayesian prior</td></tr><tr><td align="left">consensus</td><td align="left">Four scoring matrices from existing algorithms</td></tr><tr><td align="left">NetMHC4</td><td align="left">Training on non 9mer peptides by insertion of wildcard AA or deletion at all possible positions; augmented training set with natural peptides for each length assumed to be negative</td></tr><tr><td align="left">NetMHCpan4</td><td align="left">Same insertion/ deletion procedure as NetMHC4; augmented training set with random artificial negatives</td></tr><tr><td align="left">MHCFlurry</td><td align="left">Pretraining on BLOSUM62 similar allele for alleles with little training data; augmented training set with artificial negative peptides</td></tr><tr><td align="left"><bold>USMPep</bold></td><td align="left">Optional: language model pretraining on unlabeled sequences</td></tr><tr><td align="left" colspan="2"><italic>Model selection</italic></td></tr><tr><td align="left">SMMPMBEC</td><td align="left">Single model</td></tr><tr><td align="left">consensus</td><td align="left">Single model</td></tr><tr><td align="left">NetMHC4</td><td align="left">Ensemble of 4 NNs</td></tr><tr><td align="left">NetMHCpan4</td><td align="left">Ensemble of 100 NNs</td></tr><tr><td align="left">MHCFlurry</td><td align="left">Ensemble of 8-16 NNs selected from 320 models on a validation set</td></tr><tr><td align="left"><bold>USMPep</bold></td><td align="left">Optional: ensemble of 10 NNs with identical architectures and hyperparameters</td></tr></tbody></table></table-wrap></p>
    <p>Finally, not all binding prediction tools are evaluated on standard benchmark datasets, which reduces the comparability, and, even where this is the case, it is often hard to disentangle algorithmic advancements from improvements due to larger amounts of training data. In addition, statements about the generalization in the sense of the algorithm’s performance when applied to unseen data are often difficult due to potential overlaps between train and test sets, in particular as training sets often remain undisclosed. This urges for the creation of benchmark repositories, where the existing data are processed in a standardized fashion and split into training, validation and test sets.</p>
    <p>In this manuscript, we demonstrate that state-of-the-art performance can be reached with a straightforward approach: We use a single-layer recurrent neural network that is <italic>trained end-to-end</italic> on a regression task <italic>without</italic> any task-specific <italic>prior knowledge</italic> such as fixed embeddings in the form of amino acid similarity matrices. By construction, this model is able to incorporate <italic>input of variable length without</italic> the need for <italic>heuristics</italic>, such as for the identification of binding regions. The model is trained using <italic>standard training procedures</italic> without any artificial data or pretraining on related classification tasks. Even <italic>single models</italic> are <italic>very competitive</italic>. Ensembling or language model pretraining only slightly improve this performance. We fix hyperparameters only once and use standard benchmark datasets to assess the model performance. We provide, amongst others, evaluation results on the recently published HPV dataset [<xref ref-type="bibr" rid="CR12">12</xref>], demonstrating an excellent performance, which strongly suggests that the measured model performance generalizes to unseen peptide data.</p>
    <p>Recurrent architectures have already been used previously for MHC binding prediction [<xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR14">14</xref>] and we discuss in more detail how <italic>USMPep</italic> stands out from these approaches. MHCnuggets [<xref ref-type="bibr" rid="CR13">13</xref>] is rather similar to the proposed approach (apart from the use of fixed embeddings), but relies on a complex transfer learning protocol to achieve its performance. Only limited benchmarking results are available, which makes it difficult to realistically assess its prediction performance. The very recent MHCSeqNet [<xref ref-type="bibr" rid="CR14">14</xref>] also uses a recurrent architecture, again with pretrained rather than learned embeddings, incorporating both peptide and allele sequence to train a single prediction model for all alleles. However, the paper frames the prediction task as a classification task, which makes it difficult to align the results with the large number of existing benchmark datasets that are predominantly targeted at regression tasks. Nevertheless, the inclusion of the allele sequence represents an exciting opportunity for MHC binding affinity prediction in particular in the light of recent advances in natural language processing on tasks involving two input sequences such as question answering tasks.</p>
  </sec>
  <sec id="Sec2">
    <title>Implementation</title>
    <sec id="Sec3">
      <title>USMPep: universal sequence models for peptide binding prediction</title>
      <p>The approach builds on the <italic>UDSMProt</italic>-framework [<xref ref-type="bibr" rid="CR15">15</xref>] and related work in natural language processing [<xref ref-type="bibr" rid="CR16">16</xref>]. We distinguish two variants of our approach, either train the regression from scratch or employ language model pretraining. A language model tries to predict the next token given the sequence up to this token, on unlabeled sequence data, here: of simulated proteasome-cleaved peptides. The architecture of the language model is, at its core, a recurrent neural network regularized by different kinds of dropout, and more specifically an averaged stochastic gradient descent weight-dropped long short-term memory (AWD LSTM) model [<xref ref-type="bibr" rid="CR17">17</xref>]. After the language model pretraining step, the model is finetuned on the regression task of MHC binding prediction by replacing the output layer with a concat pooling layer and two fully connected layers, see Fig. <xref rid="Fig1" ref-type="fig">1</xref> for a schematic representation. The setup closely follows that used in [<xref ref-type="bibr" rid="CR15">15</xref>], where protein properties were predicted. The smaller dataset sizes and shorter sequence lengths in the peptide setting (in comparison to protein classification) do not allow for building up large contexts and were accounted for by the reduction of the number of layers from 3 to 1, of the number of hidden units from 1150 to 64 and of the embedding size from 400 to 50.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Schematic representation of the model architecture</p></caption><graphic xlink:href="12859_2020_3631_Fig1_HTML" id="MO1"/></fig></p>
      <p>Similar to [<xref ref-type="bibr" rid="CR15">15</xref>], the training procedure included 1-cycle learning rate scheduling [<xref ref-type="bibr" rid="CR18">18</xref>] and discriminative learning rates [<xref ref-type="bibr" rid="CR16">16</xref>] during finetuning. Target variables for the regression model were log-transformed half-maximal inhibitory concentration (<italic>I</italic><italic>C</italic><sub>50</sub>)-values and a modified mean-squared error loss function [<xref ref-type="bibr" rid="CR11">11</xref>] that allows to incorporate qualitative data.</p>
      <p>Dropout rate, the number of training epochs, hidden layers, hidden units and embedding dimensions, were set based on selected alleles of a particular MHC class I dataset (<italic>Kim14</italic> [<xref ref-type="bibr" rid="CR19">19</xref>], see the detailed description below) by using the score on one of the provided cross-validation folds. The learning rate was determined based on range tests [<xref ref-type="bibr" rid="CR18">18</xref>]. After this step, the aforementioned hyperparameters were kept fixed for all datasets and alleles both for MHC class I and class II prediction. In particular, neither hyperparameters nor models were selected based on test set scores.</p>
      <p>For later convenience, the following acronyms refer to the prediction tools introduced in this work:
<list list-type="bullet"><list-item><p><bold>USMPep_FS_sng</bold> single prediction model trained from scratch</p></list-item><list-item><p><bold>USMPep_FS_ens</bold> ensemble of ten prediction models trained from scratch</p></list-item><list-item><p><bold>USMPep_LM_sng</bold> single prediction model with language model pretraining</p></list-item><list-item><p><bold>USMPep_LM_ens</bold> ensemble of ten prediction models with language model pretraining</p></list-item></list></p>
      <p>For simplicity, we consider ensembles of models with identical architectures and hyperparameters and average the final individual predictions.</p>
    </sec>
    <sec id="Sec4">
      <title>MHC binding prediction datasets</title>
      <p>For the downstream task of peptide MHC binding prediction, we benchmarked our model on three MHC class I and one MHC class II binding affinity datasets (details listed in Table <xref rid="Tab2" ref-type="table">2</xref>). These datasets comprise peptide sequences and the corresponding binding affinities to specific MHC alleles.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Details of training and test datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Usage</th><th align="left">Total size</th><th align="left">Share of binders</th><th align="left"># alleles</th><th align="left">Median size</th><th align="left">Share of quant. meas.</th><th align="left">Sequence length</th></tr></thead><tbody><tr><td align="left" colspan="8">MHC class I</td></tr><tr><td align="left"><italic>BD2009</italic></td><td align="left">train</td><td align="left">117326</td><td align="left">0.25</td><td align="left">53</td><td align="left">1971</td><td align="left">0.58</td><td align="left">8–11</td></tr><tr><td align="left"><italic>Blind</italic></td><td align="left">test</td><td align="left">27680</td><td align="left">0.33</td><td align="left">53</td><td align="left">470</td><td align="left">0.58</td><td align="left">8–11</td></tr><tr><td align="left"><italic>MHCFlurry18</italic></td><td align="left">train</td><td align="left">120720</td><td align="left">0.25</td><td align="left">32</td><td align="left">3659</td><td align="left">0.68</td><td align="left">8–15</td></tr><tr><td align="left"><italic>IEDB16_I</italic></td><td align="left">test</td><td align="left">2827</td><td align="left">0.54</td><td align="left">32</td><td align="left">73</td><td align="left">1.0</td><td align="left">9</td></tr><tr><td align="left"><italic>MHCFlurry18</italic></td><td align="left">train</td><td align="left">68117</td><td align="left">0.26</td><td align="left">7</td><td align="left">6884</td><td align="left">0.64</td><td align="left">8–15</td></tr><tr><td align="left"><italic>HPV</italic></td><td align="left">test</td><td align="left">743</td><td align="left">0.34</td><td align="left">7</td><td align="left">125</td><td align="left">0.37</td><td align="left">8–11</td></tr><tr><td align="left" colspan="8">MHC class II</td></tr><tr><td align="left"><italic>Wang10</italic></td><td align="left">train</td><td align="left">23203</td><td align="left">0.37</td><td align="left">24</td><td align="left">999</td><td align="left">1.0</td><td align="left">15–37</td></tr><tr><td align="left"><italic>IEDB16_II</italic></td><td align="left">test</td><td align="left">15691</td><td align="left">0.33</td><td align="left">24</td><td align="left">641</td><td align="left">1.0</td><td align="left">15</td></tr></tbody></table><table-wrap-foot><p>The threshold for MHC class I binders is 500nM, except for the HPV dataset, where the threshold is 100 000nM. For MHC class II binders, the threshold is 1000nM</p></table-wrap-foot></table-wrap></p>
      <p><italic>Kim14</italic> is a commonly used binding affinity dataset compiled by [<xref ref-type="bibr" rid="CR19">19</xref>], available on the Immune Epitope Database (IEDB)<xref ref-type="fn" rid="Fn1">1</xref> [<xref ref-type="bibr" rid="CR20">20</xref>], and is split into a non-overlapping training (<italic>BD2009</italic>) and test set (<italic>Blind</italic>). Similar peptides (of same length with at least 80% sequence identity) shared by training and test set were removed from <italic>Blind</italic>. For <italic>BD2009</italic>, we selected the provided cross-validation split without similar peptides between the subsamples (<italic>”cv_gs"</italic>). There are 53 class I alleles (human and mouse/ macaque alleles) with respectively 117326 and 27680 affinity measurements in <italic>BD2009</italic> and <italic>Blind</italic>. For comparability with recently developed systematical benchmarks [<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR12">12</xref>], we tested <italic>USMPep</italic> on two further MHC I datasets, which we refer to as <italic>HPV</italic> and IEDB_16. The training data of the tools reported in the literature vary in size and compilation.</p>
      <p>We trained our models on data provided by [<xref ref-type="bibr" rid="CR11">11</xref>] and refer to this dataset as <italic>MHCFlurry18</italic>. It is assembled from an IEDB snapshot of December 2017 and the <italic>Kim14</italic> dataset.</p>
      <p><italic>HPV</italic> is a recently published dataset of 743 affinity measurements of peptides derived from two human paillomavirus 16 (HPV16) proteins binding to seven human leukocyte antigen (HLA) class I alleles [<xref ref-type="bibr" rid="CR12">12</xref>]. Peptides were considered as binders if they had <italic>I</italic><italic>C</italic><sub>50</sub>-values below 100000nM. For peptides classified as non-binders, quantitative measurements are not available.</p>
      <p><italic>IEDB16_I</italic> is made up of an IEDB snapshot of October 2016 [<xref ref-type="bibr" rid="CR5">5</xref>]. It was filtered for quantitative measurements with <italic>I</italic><italic>C</italic><sub>50</sub>≤ 50000nM and 9mer peptides. Training sequences of other tools were removed from the dataset. It consists of 2827 affinity measurements across 32 class I alleles. We removed any sequences occurring in the test dataset from our training data <italic>MHCFlurry18</italic>.</p>
      <p>In addition, we trained and tested <italic>USMPep</italic> on MHC class II binding data: <italic>Wang10</italic> is an experimental binding affinity dataset from the IEDB site based on the dataset by [<xref ref-type="bibr" rid="CR21">21</xref>]. We used it to train our prediction tools.</p>
      <p><italic>IEDB16_II</italic> is a MHC II test dataset provided by [<xref ref-type="bibr" rid="CR5">5</xref>] from the same IEDB snapshot as the MHC I <italic>IEDB16_I</italic> test set above, filtered for quantitative measurements with <italic>I</italic><italic>C</italic><sub>50</sub>≤50000nM and 15mer peptides. After removing sequences present in the training data, 15034 affinity measurements covering 24 alleles remained in the test dataset. We benchmarked our models on this dataset.</p>
    </sec>
    <sec id="Sec5">
      <title>Evaluation metrics</title>
      <p>For performance evaluation, we consider two evaluation metrics that are most frequently considered in the literature [<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR11">11</xref>]: The area under the receiver operating characteristic curve (AUC ROC) measures the performance of binary classifying binders and non-binders. While AUC ROC is straightforward to evaluate, it comes with the disadvantage of having to specify a threshold to turn the targets into binary labels, which discards valuable label information during the evaluation procedure. Commonly applied threshold values exist for the datasets under consideration, as discussed in the previous section, but the simplicity of this procedure neglects a possible allele dependence of these threshold values [<xref ref-type="bibr" rid="CR22">22</xref>]. Ranking metrics such as Spearman <italic>r</italic> evaluate the correlation between the rankings of measured and predicted affinities and circumvent this issue. Spearman <italic>r</italic> can only be evaluated for quantitative measurements, which discards information on test sets that contain also qualitative measurements. For both metrics, we calculated error bars based on 95% empirical bootstrap confidence intervals. For single models, we report the mean performance across 10 runs and the maximal deviation of the point estimate compared to the lower and upper bounds provided by the respective confidence intervals as a conservative error estimate. An alternative approach, which is taken for example in [<xref ref-type="bibr" rid="CR14">14</xref>], is to frame the problem as a classification rather than a regression problem i.e. to predict the probability of binding of a peptide to a given allele and to use AUC ROC as performance metric. In principle, the raw prediction of our models before transforming back to <italic>I</italic><italic>C</italic><sub>50</sub>-values could also be interpreted as probabilities and are also available from our code repository to ensure straightforward comparability, even though the results between regression and classification models are only partially comparable due to different training objectives.</p>
      <p>The prediction performance across different alleles that make up a single MHC benchmark dataset can be quantified in different ways. <italic>Overall</italic> performance measures can be calculated across multiple alleles by concatenating all target and prediction results and evaluating the respective metrics on this set. This predominantly used but rarely discussed method has to be contrasted with reporting the <italic>mean</italic> or the median of the respective performance measures across all alleles, which is the default evaluation metric for related tasks such as remote homology detection [<xref ref-type="bibr" rid="CR23">23</xref>] or transcription factor binding site prediction [<xref ref-type="bibr" rid="CR24">24</xref>]. The difference between both evaluation approaches is related to the discussion about micro vs. macro averages for the evaluation of multi-class classification problems [<xref ref-type="bibr" rid="CR25">25</xref>]. In particular, there are two fundamental differences between both evaluation approaches: First, the datasets enter the <italic>overall</italic> score with different weights determined by the size of the respective test sets, which is a weighting based on the experimental availability of binding affinities whereas the <italic>mean</italic> score assigns equal weight to all test sets. Second, the <italic>overall</italic> performance measure implicitly assumes that prediction scores are directly comparable across different alleles, which seems slightly questionable in the light of the discussion of allele-dependent binding thresholds [<xref ref-type="bibr" rid="CR22">22</xref>]. To give the reader a complete picture of the prediction performance, we will report <italic>overall</italic> as well as <italic>mean</italic> scores. In any case, we advocate to provide individual prediction for all peptides, which allows to possibly redo the analysis using a different performance metric at a later point in time. To this end, the peptide-wise binding affinity predictions for our tools are provided in the accompanying code repository.</p>
    </sec>
  </sec>
  <sec id="Sec6" sec-type="results">
    <title>Results</title>
    <p>The results section is organized as follows: In “<xref rid="Sec7" ref-type="sec">MHC class I binding prediction</xref>” section, we present a detailed evaluation of the performance of <italic>USMPep</italic> for MHC class I binding affinity prediction. Three different benchmark datasets highlight different performance characteristics. In “<xref rid="Sec11" ref-type="sec">MHC class II binding prediction</xref>” section, we investigate the applicability of our methods for MHC class II binding affinity prediction. Finally, we discuss language modeling on peptide data and its impact on downstream performance in “<xref rid="Sec12" ref-type="sec">Language modeling on peptide data and its impact on downstream performance</xref>” section.</p>
    <sec id="Sec7">
      <title>MHC class I binding prediction</title>
      <sec id="Sec8">
        <title>IEDB16 dataset</title>
        <p>We open the assessment of MHC class I binding prediction with results on the <italic>IEDB16</italic> dataset that showcases the excellent predictive performance of <italic>USMPep</italic>. We compare to literature results that were evaluated in a recent comprehensive benchmark [<xref ref-type="bibr" rid="CR5">5</xref>] on this dataset. This benchmark includes evaluation metrics testing not only accuracy of binder classification, but also accuracy of binding affinity ranking and of direct binding affinity prediction accuracy. Covering 32 HLA alleles, the <italic>IEDB16</italic> dataset reflects a broad spectrum of MHC molecules.</p>
        <p>In Fig. <xref rid="Fig2" ref-type="fig">2</xref>, we show <italic>overall</italic> AUC ROC and <italic>overall</italic> Spearman <italic>r</italic> as reported by [<xref ref-type="bibr" rid="CR5">5</xref>] for the latest versions of the NetMHC tools, MHCFlurry, SMMPMBEC and consensus and our scores for the different versions of <italic>USMPep</italic>. This is supplemented by <italic>mean</italic> AUC ROC and <italic>mean</italic> Spearman <italic>r</italic> compared to results provided in the data repository accompanying [<xref ref-type="bibr" rid="CR5">5</xref>]. For <italic>mean</italic> AUC ROC and Spearman <italic>r</italic> error bars could not be calculated for the literature approaches due to the fact that only allele-wise scores but no peptide-wise predictions were provided. In the light of the issues discussed in “<xref rid="Sec5" ref-type="sec">Evaluation metrics</xref>” section, we advocate the use of <italic>mean</italic> scores rather than <italic>overall</italic> scores. For easy comparability, we also provide <italic>overall</italic> scores as they are used predominantly in the literature. It turns out that an ensemble of ten predictors with language model pretraining (<italic>USMPep_LM_ens</italic>), reaches the highest scores in both <italic>mean</italic> evaluation metrics. In this respect, the results of all four <italic>USMPep</italic>-variants are consistent with each other and similar (within error bars) to the result of MHCFlurry, the best-performing method in the benchmark [<xref ref-type="bibr" rid="CR5">5</xref>]. This result stresses the claims of excellent prediction performance even for a single model trained from scratch. Interestingly, the performance of all proposed prediction tools is slightly worse when considering <italic>overall</italic> scores. The error boundaries of our tools barely touch those of MHCFlurry with regard to <italic>overall</italic> Spearman <italic>r</italic>. In particular, in terms of <italic>overall</italic> AUC ROC none of our predictors is consistent with MHCFlurry within error bars. We further investigated the origin of this performance deficiency and found that it could be traced back to a single allele, HLA-B-3801, which is peculiar in the sense that 172 of the 176 test set samples fall into a single Hobohl cluster [<xref ref-type="bibr" rid="CR19">19</xref>] of sequences with more than 80% sequence similarity, i.e. show a particularly high sequence identity that is not seen in other test datasets. These 172 samples constitute a sizable amount of the overall 2827 test samples and strongly influence the predictive performance when using <italic>overall</italic> performance metrics. With the exceptions in terms of the <italic>overall</italic> metrics for the <italic>IEDB16</italic> dataset, our proposed methods are consistent with the best-performing methods for all MHC I benchmark datasets both for <italic>overall</italic> and <italic>mean</italic> performance metrics.
<fig id="Fig2"><label>Fig. 2</label><caption><p>Comparison of MHC class I predictors. AUC ROC and Spearman <italic>r</italic> are evaluated on predictions for the <italic>IEDB16_I</italic> test set (AUC ROC could not be evaluated for alleles HLA-B-2704, HLA-B-1503 and HLA-B-1501, whereas Spearman <italic>r</italic> could not be computed for alleles HLA-B-1503 and HLA-B-1501. These alleles are therefore not included in the scores.)</p></caption><graphic xlink:href="12859_2020_3631_Fig2_HTML" id="MO2"/></fig></p>
      </sec>
      <sec id="Sec9">
        <title>HPV dataset</title>
        <p>As the training data is not publicly available for some MHC I prediction tools, a possible overlap between training and test datasets and correspondingly an overestimation of the predictive performance cannot be excluded. The same applies to the most common procedure of reducing the overlap between training and test set by merely removing sequences from the test set that are also contained in the training set in identical form rather than using more elaborate measures for sequence similarity. These issues can be circumvented by a performance evaluation on a dataset of different origin that has so far not been used to train MHC prediction tools. This applies to the recently released <italic>HPV</italic> binding affinity data [<xref ref-type="bibr" rid="CR12">12</xref>]. However, in this benchmark, it is not possible to disentangle superior prediction performance due to larger amounts of training data from algorithmic advances since size and compilation of the training set of the algorithms vary.</p>
        <p>There are only quantitative measurements available for the peptides considered as binders and we therefore chose to evaluate the predictive performance with AUC ROC. We report the performance of all models considered in [<xref ref-type="bibr" rid="CR12">12</xref>] and our tools measured by AUC ROC in Table <xref rid="Tab3" ref-type="table">3</xref>, where we used the predictions provided by [<xref ref-type="bibr" rid="CR12">12</xref>]. Our <italic>USMPep</italic> tools show an excellent prediction performance. For three out of seven alleles, an <italic>USMPep</italic>-model even reaches the highest AUC ROC. All neural-network-based predictors show a similar AUC ROC evaluated across all measurements in the dataset, while the ensemble with language model pretraining (<italic>USMPep_LM_ens</italic>) shows the highest <italic>mean</italic> and <italic>overall</italic> scores among all prediction tools. As for the <italic>IEDB16</italic> dataset, even the single model <italic>USMPep</italic>-tools are very competitive.
<table-wrap id="Tab3"><label>Table 3</label><caption><p>Benchmarking MHC class I predictors on recently published binding affinity data (<italic>HPV16</italic>), see also Table <xref rid="Tab4" ref-type="table">4</xref> for allele-wise scores</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">allele</th><th align="left"><italic>mean</italic> AUC ROC</th><th align="left"><italic>overall</italic> AUC ROC</th></tr></thead><tbody><tr><td align="left">USMPep_FS_ens</td><td align="left">0.824(3)</td><td align="left">0.814(3)</td></tr><tr><td align="left">USMPep_FS_sng</td><td align="left">0.818(4)</td><td align="left">0.808(4)</td></tr><tr><td align="left">USMPep_LM_ens</td><td align="left"><bold>0.831(4)</bold></td><td align="left"><bold>0.815(3)</bold></td></tr><tr><td align="left">USMPep_LM_sng</td><td align="left">0.813(5)</td><td align="left">0.802(4)</td></tr><tr><td align="left">MHCFlurry</td><td align="left">0.817(4)</td><td align="left">0.809(4)</td></tr><tr><td align="left">NetMHC 3.4</td><td align="left"><bold>0.831(3)</bold></td><td align="left">0.794(3)</td></tr><tr><td align="left">NetMHC 4.0</td><td align="left">0.803(4)</td><td align="left">0.780(3)</td></tr><tr><td align="left">NetMHCpan 2.8</td><td align="left">0.818(4)</td><td align="left">0.792(3)</td></tr><tr><td align="left">NetMHCpan 3.0</td><td align="left">0.815(4)</td><td align="left">0.787(3)</td></tr><tr><td align="left">NetMHCpan 4.0</td><td align="left">0.820(3)</td><td align="left">0.792(4)</td></tr><tr><td align="left">SMM</td><td align="left">0.684(5)</td><td align="left">0.695(4)</td></tr><tr><td align="left">SMMPMBEC</td><td align="left">0.722(6)</td><td align="left">0.723(4)</td></tr><tr><td align="left">Pickpocket 1.1</td><td align="left">0.760(5)</td><td align="left">0.708(4)</td></tr><tr><td align="left">consensus</td><td align="left">0.751(5)</td><td align="left">0.766(4)</td></tr><tr><td align="left">IEDB recommended</td><td align="left">0.756(5)</td><td align="left">0.772(4)</td></tr><tr><td align="left">NetMHCcons 1.1</td><td align="left">0.827(4)</td><td align="left">0.799(3)</td></tr></tbody></table><table-wrap-foot><p>Predictive performance is evaluated by AUC ROC (threshold for binders &lt; 100 000nM) on single alleles and across all alleles (<italic>mean</italic> and <italic>overall</italic>). The scores for literature approaches were calculated based on peptide-wise predictions provided in [<xref ref-type="bibr" rid="CR12">12</xref>]. Numbers in brackets in the table concisely denote the corresponding bootstrap confidence intervals. For instance, 0.824(3) stands for a mean AUC ROC of 0.824 ± 0.003</p></table-wrap-foot></table-wrap><table-wrap id="Tab4"><label>Table 4</label><caption><p>Allele-wise results on (<italic>HPV16</italic>), see also Table <xref rid="Tab3" ref-type="table">3</xref> for <italic>mean</italic> and <italic>overall</italic> scores</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Allele</th><th align="left">HLAA1</th><th align="left">HLAA11</th><th align="left">HLAA2</th><th align="left">HLAA24</th><th align="left">HLAA3</th><th align="left">HLAB15</th><th align="left">HLAB7</th></tr></thead><tbody><tr><td align="left">USMPep_FS_ens</td><td align="left">0.793</td><td align="left"><bold>0.885</bold></td><td align="left">0.830</td><td align="left">0.807</td><td align="left">0.768</td><td align="left">0.803</td><td align="left">0.884</td></tr><tr><td align="left">USMPep_FS_sng</td><td align="left">0.785</td><td align="left">0.883</td><td align="left">0.822</td><td align="left">0.798</td><td align="left">0.764</td><td align="left">0.799</td><td align="left">0.883</td></tr><tr><td align="left">USMPep_LM_ens</td><td align="left"><bold>0.848</bold></td><td align="left">0.880</td><td align="left">0.809</td><td align="left"><bold>0.821</bold></td><td align="left">0.766</td><td align="left">0.824</td><td align="left">0.871</td></tr><tr><td align="left">USMPep_LM_sng</td><td align="left">0.813</td><td align="left">0.869</td><td align="left">0.805</td><td align="left">0.802</td><td align="left">0.755</td><td align="left">0.805</td><td align="left">0.854</td></tr><tr><td align="left">MHCFlurry</td><td align="left">0.816</td><td align="left">0.850</td><td align="left"><bold>0.833</bold></td><td align="left">0.755</td><td align="left">0.793</td><td align="left">0.797</td><td align="left">0.867</td></tr><tr><td align="left">NetMHC 3.4</td><td align="left">0.841</td><td align="left">0.867</td><td align="left">0.793</td><td align="left">0.765</td><td align="left"><bold>0.840</bold></td><td align="left">0.825</td><td align="left">0.884</td></tr><tr><td align="left">NetMHC 4.0</td><td align="left">0.823</td><td align="left">0.855</td><td align="left">0.792</td><td align="left">0.730</td><td align="left">0.779</td><td align="left">0.825</td><td align="left">0.801</td></tr><tr><td align="left">NetMHCpan 2.8</td><td align="left">0.756</td><td align="left">0.863</td><td align="left">0.787</td><td align="left">0.778</td><td align="left">0.794</td><td align="left">0.857</td><td align="left">0.880</td></tr><tr><td align="left">NetMHCpan 3.0</td><td align="left">0.841</td><td align="left">0.848</td><td align="left">0.781</td><td align="left">0.739</td><td align="left">0.778</td><td align="left">0.876</td><td align="left">0.825</td></tr><tr><td align="left">NetMHCpan 4.0</td><td align="left">0.839</td><td align="left">0.854</td><td align="left">0.805</td><td align="left">0.742</td><td align="left">0.784</td><td align="left"><bold>0.891</bold></td><td align="left">0.836</td></tr><tr><td align="left">SMM</td><td align="left">0.476</td><td align="left">0.828</td><td align="left">0.730</td><td align="left">0.643</td><td align="left">0.788</td><td align="left">0.704</td><td align="left">0.646</td></tr><tr><td align="left">SMMPMBEC</td><td align="left">0.593</td><td align="left">0.846</td><td align="left">0.777</td><td align="left">0.639</td><td align="left">0.799</td><td align="left">0.716</td><td align="left">0.670</td></tr><tr><td align="left">Pickpocket 1.1</td><td align="left">0.744</td><td align="left">0.773</td><td align="left">0.757</td><td align="left">0.709</td><td align="left">0.731</td><td align="left">0.808</td><td align="left">0.802</td></tr><tr><td align="left">consensus</td><td align="left">0.570</td><td align="left">0.870</td><td align="left">0.772</td><td align="left">0.687</td><td align="left">0.767</td><td align="left">0.832</td><td align="left">0.756</td></tr><tr><td align="left">IEDB recommended</td><td align="left">0.566</td><td align="left">0.877</td><td align="left">0.769</td><td align="left">0.702</td><td align="left">0.772</td><td align="left">0.852</td><td align="left">0.755</td></tr><tr><td align="left">NetMHCcons 1.1</td><td align="left">0.807</td><td align="left">0.872</td><td align="left">0.797</td><td align="left">0.777</td><td align="left">0.819</td><td align="left">0.847</td><td align="left"><bold>0.889</bold></td></tr></tbody></table></table-wrap></p>
        <p>It is instructive to investigate the performance of the different MHC prediction tools restricted to peptides of a certain length, which is only possible for the HPV dataset, where peptide-wise predictions for all literature approaches are provided. The result of such an analysis is shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. Our tools outperform the other models on only the 11mer peptides. This observation can be explained by the fact that the internal state of the recurrent neural network has to build up over the sequence. The longer the peptide, the more context is available, which is why <italic>USMPep</italic> generates comparably more accurate predictions for long sequences than for shorter ones.
<fig id="Fig3"><label>Fig. 3</label><caption><p>Evaluating MHC class I predictors on recently published binding affinity data (<italic>HPV16</italic>) grouped by peptide length. Predictive performance is evaluated by <italic>mean</italic> AUC ROC. For allele-wise and overall performance comparisons see Tables <xref rid="Tab4" ref-type="table">4</xref> and <xref rid="Tab3" ref-type="table">3</xref>. Percentages in brackets indicate the proportion of peptides of that particular length. For example, bars at 8(15<italic>%</italic>) show results for peptides that are eight amino acids long, which had a share of 15% of all peptides in total</p></caption><graphic xlink:href="12859_2020_3631_Fig3_HTML" id="MO3"/></fig></p>
      </sec>
      <sec id="Sec10">
        <title>Kim14 dataset</title>
        <p>As final benchmark dataset for MHC class I prediction, we consider the <italic>Kim14</italic> dataset that is interesting for a number of reasons. In order to investigate how the predictive power of our approach depends on the size of the training data set, we trained and tested our model on the <italic>Kim14</italic><italic>BD2009</italic> and <italic>Blind</italic> data. The authors of [<xref ref-type="bibr" rid="CR11">11</xref>] kindly provided us with the <italic>Blind</italic> predictions of their tool trained on <italic>BD2009</italic>, which allow for a direct comparison with a state-of-the-art tool. Corresponding training routines are by now also available in the code repository accompanying [<xref ref-type="bibr" rid="CR11">11</xref>].</p>
        <p>First, we compare the prediction success measured by AUC ROC and Spearman <italic>r</italic> computed across all alleles (Fig. <xref rid="Fig4" ref-type="fig">4</xref>). No MHCFlurry predictors exist for alleles HLA-B-2703, HLA-B-0803 and HLA-B3801 with rank 45, 49 and 52 due to insufficient training data. These alleles were therefore also excluded for the scores of our tools. The predictors perform very similarly with regard to all metrics. Due to the different scales of the metrics, the minor performance variations appear to be more pronounced for Spearman <italic>r</italic> in comparison to AUC ROC. Nevertheless, the ranking of the almost equally performing predictors remains consistent. Our pretrained tool <italic>USMPep_LM_ens</italic> performs only slightly better than <italic>USMPep_FS_ens</italic> trained from scratch. This also holds for the single model versions. Both <italic>USMPep</italic> ensemble predictors are compatible with MHCFlurry.
<fig id="Fig4"><label>Fig. 4</label><caption><p>Performance of <italic>USMPep</italic> and MHCFlurry on MHC class I binding prediction. Both models were trained on the <italic>Kim14</italic><italic>BD2009</italic> data AUC ROC and Spearman <italic>r</italic> were evaluated on the predictions for the <italic>Blind</italic> test set (AUC ROC could not be evaluated for allele HLA-B-4601, whereas Spearman <italic>r</italic> could not be computed for allele HLA-B-4601 and HLA-B-2703. These alleles are therefore not included in the scores.)</p></caption><graphic xlink:href="12859_2020_3631_Fig4_HTML" id="MO4"/></fig></p>
        <p>Second, to examine the impact of the training set size, we report allele-wise Spearman <italic>r</italic> scores in Fig. <xref rid="Fig5" ref-type="fig">5</xref> for our predictors and MHCFlurry. The alleles are ranked by the size of the corresponding training set. While 9528 training sequences exist for the rank 0 MHC molecule HLA-A-0201, there are only 136 training peptides for allele HLA-B-3801 with rank 52. Spearman <italic>r</italic> is only shown for alleles with more than 25 quantitative measurements. The variance of the allelwise performances of the different tools becomes more pronounced the less training data are available. However, none of the models outperforms the others for the subset of alleles with less than 1000 training data points (rank range33 to 52). When averaging over the alleles with rank 33 to 52, the mean Spearman <italic>r</italic> scores of 0.54(5), 0.57(4), 0.57(5) for <italic>USMPep_FS_ens</italic>, <italic>USMPep_LM_ens</italic> and MHCFlurry, respectively, remain consistent within error bars. This observation is interesting considering the fact that for alleles with fewer than 1000 training measurements, MHCFlurry was pretrained on an augmented training set with measurements from BLOSUM similar alleles, <italic>USMPep_LM_ens</italic> was pretrained on a large corpus of unlabeled peptides and <italic>USMPep_FS_ens</italic> in contrast only saw the training sequences corresponding to one MHC molecule. These results stress that further efforts might be required to truly leverage the potential of unlabeled peptide data in order to observe similar improvements as seen for proteins [<xref ref-type="bibr" rid="CR15">15</xref>] in particular for small datasets.
<fig id="Fig5"><label>Fig. 5</label><caption><p>Performance of our MHC I prediction tools compared to MHCFlurry on single alleles. Spearman <italic>r</italic> was calculated for predictions on the <italic>Kim14</italic><italic>Blind</italic> data for alleles with more than 25 quantitative measurements. The predictors were trained on <italic>Kim14</italic><italic>BD2009</italic>. The alleles are ranked by the size of the corresponding training set (9528 peptides for rank 0 to 136 peptides with rank 52). No MHCFlurry predictors were provided for alleles HLA-B-2703, HLA-B-0803 and HLA-B-3801 with rank 45, 49 and 52</p></caption><graphic xlink:href="12859_2020_3631_Fig5_HTML" id="MO5"/></fig></p>
      </sec>
    </sec>
    <sec id="Sec11">
      <title>MHC class II binding prediction</title>
      <p>Turning to MHC Class II binding prediction, we aim to demonstrate the universality of our approach beyond its applicability to different MHC I alleles. Here, we stress again that we use the same model architecture, the same pretrained language model in case of pretraining, and even the same set of hyperparameters for all MHC class I and class II alleles. The main difference between and MHC class I and class II binding prediction is the typically larger length of 15 amino acids for MHC class II compared to at most 11 for MHC class I. The analysis of the prediction performance in dependence of the length of the peptide in the previous section suggests that this setting is particularly suitable for the <italic>USMPep</italic> prediction tools. Unfortunately, the reported literature results vary widely concerning the selection of training data, which makes it difficult to distinguish between algorithmic improvements and improvements due to larger amounts of training data.</p>
      <p>The <italic>USMPep</italic> prediction tools, and in particular the ensemble variants, show a solid performance compared to literature results, see Fig. <xref rid="Fig6" ref-type="fig">6</xref>. Whereas the <italic>USMPep</italic>-predictors always provided the best-performing method for MHC class I prediction, it is outperformed for MHC class II by <italic>NetMHCIIpan</italic> and <italic>nn_align</italic>. We deliberately decided to train on <italic>Wang10</italic> instead of a more recent IEDB snapshot to work on a well-defined published dataset. However, this makes it hard to assess if the performance differences between our results and the best-performing methods can be attributed to the fact that the <italic>USMPep</italic>-predictors were trained using IEDB data up to 2010 whereas in particular the best-performing tools were trained on larger amounts and more recent data or if there a particular intricacies inherent to the MHC class II prediction task.
<fig id="Fig6"><label>Fig. 6</label><caption><p>Comparison of MHC class II predictors. AUC ROC and Spearman <italic>r</italic> were evaluated on predictions for the <italic>IEDB16_II</italic> test set</p></caption><graphic xlink:href="12859_2020_3631_Fig6_HTML" id="MO6"/></fig></p>
    </sec>
    <sec id="Sec12">
      <title>Language modeling on peptide data and its impact on downstream performance</title>
      <p>As final analysis, we analyze language modeling on peptide data and its impact on MHC binding affinity prediction as downstream task. To this end, we constructed a dataset of simulated proteasome-cleaved peptides to pretrain <italic>USMpep</italic> on a large corpus of unlabeled sequences. We filtered the SwissProt release 2018_10 for the human proteome and employed NetChop [<xref ref-type="bibr" rid="CR26">26</xref>] to obtain proteasome cleavage sites for these proteins. The stochastic process of protein slicing was modeled by cutting with the cleavage probability provided by NetChop. We discarded sequences of less than eight and more than 20 amino acids length and obtained 6547641 peptides. We compare the performance of a peptide language model to that of a language model trained on human protein data using prediction accuracy as metric.</p>
      <p>The results in terms of language model performance along with the corresponding downstream performance (MHC) on the regression task are compiled in Table <xref rid="Tab5" ref-type="table">5</xref> and allow a number of interesting observations: First, the language model performance increases considerably when training on (proteasome-cleaved) peptide data in accordance with expectations. It is crucial to remark, that the language modeling task on peptide data poses additional difficulties compared to language modeling on protein data as the sequences are comparably short and the model thus cannot build up a lot of context. Additionally, the model does not only have to learn the normal language model task for protein data but implicitly has to learn to stochastically predict cleavage sites. Second, even we evaluated on protein data, the protein language model only reaches an accuracy of 0.137, which is considerably lower than the accuracy of 0.41 reported in the literature [<xref ref-type="bibr" rid="CR15">15</xref>]. This effect is a direct consequence of the considerably smaller model size (1 instead of 3 layers; 64 instead of 1150 hidden units; embedding size of 50 instead of 400).
<table-wrap id="Tab5"><label>Table 5</label><caption><p>Language model and MHC class I binding affinity prediction performance</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left" colspan="2">LM</th><th align="left" colspan="2">Downstream (<italic>mean</italic>)</th></tr><tr><th align="left"/><th align="left">perpl.</th><th align="left">acc.</th><th align="left">AUC ROC</th><th align="left">Spearman <italic>r</italic></th></tr></thead><tbody><tr><td align="left">LM (protein)</td><td align="left">39.3</td><td align="left">0.083</td><td align="left">0.90(2)</td><td align="left">0.55(4)</td></tr><tr><td align="left">LM (peptide)</td><td align="left">13.4</td><td align="left">0.206</td><td align="left">0.89(2)</td><td align="left">0.57(4)</td></tr><tr><td align="left">From scratch</td><td align="left">–</td><td align="left">–</td><td align="left">0.89(2)</td><td align="left">0.55(3)</td></tr></tbody></table><table-wrap-foot><p>Language model metrics perplexity (perpl.) and accuracy (acc.) were in all cases evaluated on peptide data. The downstream performance corresponds to an ensemble of 10 predictors trained on the <italic>MHCFlurry18</italic> and evaluated on the <italic>IEDB16_I</italic> test set</p></table-wrap-foot></table-wrap></p>
      <p>The details of the language model pretraining directly impact the downstream performance and show a consistent trend across all experiments described above even though the differences in downstream performance stay small and mostly remain consistent within error bars. In line with the general trend, the most downstream-task-adapted pretraining on peptide data performs best, generally performing slightly better than the corresponding model trained from scratch. In contrast, pretraining on protein data in general even leads to a loss in performance compared to training from scratch.</p>
    </sec>
  </sec>
  <sec id="Sec13" sec-type="conclusion">
    <title>Conclusions</title>
    <p>In this work, we put forward <italic>USMPep</italic>, a recurrent neural network that consistently shows excellent performance on three popular MHC class I binding prediction datasets as well as a solid performance on MHC class II binding prediction, see Table <xref rid="Tab6" ref-type="table">6</xref> for a performance summary. Most remarkably, this is achieved with a standard training procedure without incorporating artificial negative peptides, complicated transfer learning protocols or ensembling strategies and without relying on heuristics.
<table-wrap id="Tab6"><label>Table 6</label><caption><p>Performance summary: Rank of <italic>USMPep</italic> compared to competitors across the different datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left" colspan="2"><italic>mean</italic></th><th align="left" colspan="2"><italic>overall</italic></th></tr><tr><th align="left"/><th align="left">AUC ROC</th><th align="left">Spearman <italic>r</italic></th><th align="left">AUC ROC</th><th align="left">Spearman <italic>r</italic></th></tr></thead><tbody><tr><td align="left" colspan="5">MHC class I</td></tr><tr><td align="left">IEDB16_I</td><td align="left"><bold>1</bold><sup><bold>s</bold><bold>t</bold></sup></td><td align="left"><bold>1</bold><sup><bold>s</bold><bold>t</bold></sup></td><td align="left">3 <sup>rd</sup></td><td align="left">3 <sup>rd</sup></td></tr><tr><td align="left">HPV</td><td align="left"><bold>1</bold><sup><bold>s</bold><bold>t</bold></sup></td><td align="left">–</td><td align="left"><bold>1</bold><sup><bold>s</bold><bold>t</bold></sup></td><td align="left">–</td></tr><tr><td align="left">Kim14</td><td align="left"><bold>2</bold><sup><bold>n</bold><bold>d</bold></sup></td><td align="left"><bold>2</bold><sup><bold>n</bold><bold>d</bold></sup></td><td align="left"><bold>1</bold><sup><bold>s</bold><bold>t</bold></sup></td><td align="left"><bold>2</bold><sup><bold>n</bold><bold>d</bold></sup></td></tr><tr><td align="left" colspan="5">MHC class II</td></tr><tr><td align="left">IEDB16_II</td><td align="left">4<sup>th</sup></td><td align="left">4<sup>th</sup></td><td align="left">3 <sup>rd</sup></td><td align="left">4<sup>th</sup></td></tr></tbody></table><table-wrap-foot><p>Scores marked in bold face are best-performing or consistent with the best-performing result within error bars</p></table-wrap-foot></table-wrap></p>
    <p>A central issue that prevents a true comparability of algorithmic approaches to the problem is the fact that the datasets that were used to train the prediction models differ between different literature approaches and are often not publicly available. This entangles the predictive power of a given algorithm with the data it was trained on. This urges for the creation of an appropriate benchmarking repository along with standardized evaluation procedures to allow for a structured benchmarking of MHC binding prediction algorithms. As a first step, we advocate to provide binding affinity predictions for all peptides to allow fine-grained comparisons of the overall predictive performance even at a later stage as opposed to reporting just a single score summarizing the performance across all datasets.</p>
  </sec>
  <sec id="Sec14">
    <title>Availability and requirements</title>
    <p>Project name: USMPepProject home page: <ext-link ext-link-type="uri" xlink:href="https://github.com/nstrodt/USMPep">https://github.com/nstrodt/USMPep</ext-link>Operating system(s): Platform independentProgramming language: PythonOther requirements: see project homepageLicense: BSDAny restrictions to use by non-academics: as permitted by BSD License</p>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>MHC</term>
        <def>
          <p>Major histocompatibility complex</p>
        </def>
      </def-item>
      <def-item>
        <term>AWD LSTM</term>
        <def>
          <p>Averaged stochastic gradient descent weight-dropped long short-term memory</p>
        </def>
      </def-item>
      <def-item>
        <term>IEDB</term>
        <def>
          <p>Immune Epitope Database</p>
        </def>
      </def-item>
      <def-item>
        <term>AUC ROC</term>
        <def>
          <p>Area under the receiver operating characteristic curve</p>
        </def>
      </def-item>
      <def-item>
        <term>BLOSUM</term>
        <def>
          <p>Blocks substitution matrix</p>
        </def>
      </def-item>
      <def-item>
        <term>AA</term>
        <def>
          <p>Amino acid</p>
        </def>
      </def-item>
      <def-item>
        <term>NN</term>
        <def>
          <p>Neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>PMBEC</term>
        <def>
          <p>Peptide MHC binding energy covariance</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn id="Fn1">
      <label>1</label>
      <p>
        <ext-link ext-link-type="uri" xlink:href="http://tools.iedb.org/main/datasets/">http://tools.iedb.org/main/datasets/</ext-link>
      </p>
    </fn>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>The authors thank Patrick Wagner for discussions and work on related topics. The authors thank Timothy O’Donnell for correspondence and for kindly providing MHCFlurry predictions on the <italic>Kim14</italic> dataset. <italic>USMPep</italic> was implemented using PyTorch [<xref ref-type="bibr" rid="CR27">27</xref>] and fast.ai [<xref ref-type="bibr" rid="CR28">28</xref>].</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>Conceptualization: NS, MW; Data Curation: JV, NS; Investigation: JV; Methodology: JV, NS; Software: JV, NS; Supervision: NS, WS; Validation: JV; Visualization: JV; Writing — Original Draft Preparation: JV, NS; Writing — Review &amp; Editing: JV, MW, WS, NS; Final approval of the manuscript: JV, MW, WS, NS</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work was supported by the Bundesministerium für Bildung und Forschung (BMBF) through the Berlin Big Data Center under Grant 01IS14013A and the Berlin Center for Machine Learning under Grant 01IS18037I.</p>
  </notes>
  <notes id="FPar1">
    <title>Ethics approval and consent to participate</title>
    <p>Not applicable.</p>
  </notes>
  <notes id="FPar2">
    <title>Consent for publication</title>
    <p>Not applicable.</p>
  </notes>
  <notes id="FPar3" notes-type="COI-statement">
    <title>Competing interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1</label>
      <mixed-citation publication-type="other">Scheetz L, Park KS, Li Q, Lowenstein PR, Castro MG, Schwendeman A, Moon JJ. Engineering patient-specific cancer immunotherapies. Nat Biomed Eng. 2019. 10.1038/s41551-019-0436-x.</mixed-citation>
    </ref>
    <ref id="CR2">
      <label>2</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sahin</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Türeci</surname>
            <given-names>Ö</given-names>
          </name>
        </person-group>
        <article-title>Personalized vaccines for cancer immunotherapy</article-title>
        <source>Science</source>
        <year>2018</year>
        <volume>359</volume>
        <issue>6382</issue>
        <fpage>1355</fpage>
        <lpage>60</lpage>
        <?supplied-pmid 29567706?>
        <pub-id pub-id-type="pmid">29567706</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schumacher</surname>
            <given-names>TN</given-names>
          </name>
          <name>
            <surname>Schreiber</surname>
            <given-names>RD</given-names>
          </name>
        </person-group>
        <article-title>Neoantigens in cancer immunotherapy</article-title>
        <source>Science</source>
        <year>2015</year>
        <volume>348</volume>
        <issue>6230</issue>
        <fpage>69</fpage>
        <lpage>74</lpage>
        <?supplied-pmid 25838375?>
        <pub-id pub-id-type="pmid">25838375</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Ott</surname>
            <given-names>PA</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>CJ</given-names>
          </name>
        </person-group>
        <article-title>Towards personalized, tumour-specific, therapeutic vaccines for cancer</article-title>
        <source>Nat Rev Immunol</source>
        <year>2018</year>
        <volume>18</volume>
        <issue>3</issue>
        <fpage>168</fpage>
        <pub-id pub-id-type="pmid">29226910</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Sher</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Systematically benchmarking peptide-MHC binding predictors: From synthetic to naturally processed epitopes</article-title>
        <source>PLOS Comput Biol</source>
        <year>2018</year>
        <volume>14</volume>
        <issue>11</issue>
        <fpage>1006457</fpage>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6</label>
      <mixed-citation publication-type="other">Mei S, Li F, Leier A, Marquez-Lago TT, Giam K, Croft NP, Akutsu T, Smith AI, Li J, Rossjohn J, Purcell AW, Song J. A comprehensive review and performance evaluation of bioinformatics tools for HLA class I peptide-binding prediction. Brief Bioinformatics. 2019. 10.1093/bib/bbz051.</mixed-citation>
    </ref>
    <ref id="CR7">
      <label>7</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Sidney</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Pinilla</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Sette</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Peters</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Derivation of an amino acid similarity matrix for peptide:MHC binding and its application as a Bayesian prior</article-title>
        <source>BMC Bioinformatics</source>
        <year>2009</year>
        <volume>10</volume>
        <issue>1</issue>
        <fpage>394</fpage>
        <?supplied-pmid 19948066?>
        <pub-id pub-id-type="pmid">19948066</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Moutaftsi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Peters</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Pasquetto</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Tscharke</surname>
            <given-names>DC</given-names>
          </name>
          <name>
            <surname>Sidney</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Bui</surname>
            <given-names>H-H</given-names>
          </name>
          <name>
            <surname>Grey</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Sette</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>A consensus epitope prediction approach identifies the breadth of murine TCD8+-cell responses to vaccinia virus</article-title>
        <source>Nat Biotechnol</source>
        <year>2006</year>
        <volume>24</volume>
        <issue>7</issue>
        <fpage>817</fpage>
        <lpage>9</lpage>
        <?supplied-pmid 16767078?>
        <pub-id pub-id-type="pmid">16767078</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Andreatta</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Nielsen</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Gapped sequence alignment using artificial neural networks: application to the MHC class I system</article-title>
        <source>Bioinformatics</source>
        <year>2015</year>
        <volume>32</volume>
        <issue>4</issue>
        <fpage>511</fpage>
        <lpage>7</lpage>
        <?supplied-pmid 26515819?>
        <pub-id pub-id-type="pmid">26515819</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jurtz</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Paul</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Andreatta</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Marcatili</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Peters</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Nielsen</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>NetMHCpan-4.0: Improved Peptide–MHC Class I Interaction Predictions Integrating Eluted Ligand and Peptide Binding Affinity Data</article-title>
        <source>J Immunol</source>
        <year>2017</year>
        <volume>199</volume>
        <issue>9</issue>
        <fpage>3360</fpage>
        <lpage>8</lpage>
        <?supplied-pmid 28978689?>
        <pub-id pub-id-type="pmid">28978689</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>O’Donnell</surname>
            <given-names>TJ</given-names>
          </name>
          <name>
            <surname>Rubinsteyn</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bonsack</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Riemer</surname>
            <given-names>AB</given-names>
          </name>
          <name>
            <surname>Laserson</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Hammerbacher</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>MHCflurry: Open-Source Class I MHC Binding Affinity Prediction</article-title>
        <source>Cell Syst</source>
        <year>2018</year>
        <volume>7</volume>
        <issue>1</issue>
        <fpage>129</fpage>
        <lpage>1324</lpage>
        <?supplied-pmid 29960884?>
        <pub-id pub-id-type="pmid">29960884</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bonsack</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Hoppe</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Winter</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Tichy</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Zeller</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Küpper</surname>
            <given-names>MD</given-names>
          </name>
          <name>
            <surname>Schitter</surname>
            <given-names>EC</given-names>
          </name>
          <name>
            <surname>Blatnik</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Riemer</surname>
            <given-names>AB</given-names>
          </name>
        </person-group>
        <article-title>Performance Evaluation of MHC Class-I Binding Prediction Tools Based on an Experimentally Validated MHC–Peptide Binding Data Set</article-title>
        <source>Cancer Immunol Res</source>
        <year>2019</year>
        <volume>7</volume>
        <issue>5</issue>
        <fpage>719</fpage>
        <lpage>36</lpage>
        <?supplied-pmid 30902818?>
        <pub-id pub-id-type="pmid">30902818</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13</label>
      <mixed-citation publication-type="other">Bhattacharya R, Sivakumar A, Tokheim C, Guthrie VB, Anagnostou V, Velculescu VE, Karchin R. Evaluation of machine learning methods to predict peptide binding to MHC Class I proteins. bioRxiv. 2017. 10.1101/154757.</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14</label>
      <mixed-citation publication-type="other">Phloyphisut P, Pornputtapong N, Sriswasdi S, Chuangsuwanich E. MHCSeqNet: a deep neural network model for universal MHC binding prediction. BMC Bioinformatics. 2019; 20(1). 10.1186/s12859-019-2892-4.</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Strodthoff</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Wagner</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Wenzel</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Samek</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>UDSMProt: universal deep sequence models for protein classification</article-title>
        <source>Bioinformatics</source>
        <year>2020</year>
        <volume>36</volume>
        <issue>8</issue>
        <fpage>2401</fpage>
        <lpage>9</lpage>
        <?supplied-pmid 31913448?>
        <pub-id pub-id-type="pmid">31913448</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Howard</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ruder</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Universal language model fine-tuning for text classification</article-title>
        <source>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</source>
        <year>2018</year>
        <publisher-loc>Melbourne</publisher-loc>
        <publisher-name>Association for Computational Linguistics</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17</label>
      <mixed-citation publication-type="other">Merity S, Keskar NS, Socher R. Regularizing and optimizing LSTM language models. arXiv preprint arXiv:1708.02182. 2017.</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18</label>
      <mixed-citation publication-type="other">Smith LN. A disciplined approach to neural network hyper-parameters: Part 1 - learning rate, batch size, momentum, and weight decay. arXiv preprint arXiv:1803.09820. 2018.</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Sidney</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Buus</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sette</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Nielsen</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Peters</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Dataset size and composition impact the reliability of performance benchmarks for peptide-MHC binding predictions</article-title>
        <source>BMC Bioinformatics</source>
        <year>2014</year>
        <volume>15</volume>
        <issue>1</issue>
        <fpage>241</fpage>
        <?supplied-pmid 25017736?>
        <pub-id pub-id-type="pmid">25017736</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vita</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Mahajan</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Overton</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Dhanda</surname>
            <given-names>SK</given-names>
          </name>
          <name>
            <surname>Martini</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Cantrell</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Wheeler</surname>
            <given-names>DK</given-names>
          </name>
          <name>
            <surname>Sette</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Peters</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>The Immune Epitope Database (IEDB): 2018 update</article-title>
        <source>Nucleic Acids Res</source>
        <year>2018</year>
        <volume>47</volume>
        <issue>D1</issue>
        <fpage>339</fpage>
        <lpage>43</lpage>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Sidney</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Sette</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Lund</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Nielsen</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Peters</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Peptide binding predictions for HLA DR, DP and DQ molecules</article-title>
        <source>BMC Bioinformatics</source>
        <year>2010</year>
        <volume>11</volume>
        <issue>1</issue>
        <fpage>568</fpage>
        <?supplied-pmid 21092157?>
        <pub-id pub-id-type="pmid">21092157</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Paul</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Weiskopf</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Angelo</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Sidney</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Peters</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Sette</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>HLA Class I Alleles Are Associated with Peptide-Binding Repertoires of Different Size, Affinity, and Immunogenicity</article-title>
        <source>J Immunol</source>
        <year>2013</year>
        <volume>191</volume>
        <issue>12</issue>
        <fpage>5831</fpage>
        <lpage>9</lpage>
        <?supplied-pmid 24190657?>
        <pub-id pub-id-type="pmid">24190657</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>A comprehensive review and comparison of different computational methods for protein remote homology detection</article-title>
        <source>Brief Bioinformatics</source>
        <year>2016</year>
        <volume>19</volume>
        <issue>2</issue>
        <fpage>231</fpage>
        <lpage>44</lpage>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Alipanahi</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Delong</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Weirauch</surname>
            <given-names>MT</given-names>
          </name>
          <name>
            <surname>Frey</surname>
            <given-names>BJ</given-names>
          </name>
        </person-group>
        <article-title>Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning</article-title>
        <source>Nat Biotechnol</source>
        <year>2015</year>
        <volume>33</volume>
        <issue>8</issue>
        <fpage>831</fpage>
        <lpage>8</lpage>
        <?supplied-pmid 26213851?>
        <pub-id pub-id-type="pmid">26213851</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Manning</surname>
            <given-names>CD</given-names>
          </name>
          <name>
            <surname>Raghavan</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Schutze</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <source>Introduction to Information Retrieval</source>
        <year>2008</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>Cambridge University Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nielsen</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lundegaard</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Lund</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Keşmir</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>The role of the proteasome in generating cytotoxic T-cell epitopes: insights obtained from improved predictions of proteasomal cleavage</article-title>
        <source>Immunogenetics</source>
        <year>2005</year>
        <volume>57</volume>
        <issue>1</issue>
        <fpage>33</fpage>
        <lpage>41</lpage>
        <?supplied-pmid 15744535?>
        <pub-id pub-id-type="pmid">15744535</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27</label>
      <mixed-citation publication-type="other">Paszke A, Gross S, Chintala S, Chanan G, Yang E, DeVito Z, Lin Z, Desmaison A, Antiga L, Lerer A. Automatic differentiation in PyTorch. In: 31st Conference on Neural Information Processing Systems (NIPS) Workshop Autodiff: 2017.</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28</label>
      <mixed-citation publication-type="other">Howard J, et al.fast.ai. GitHub. 2018. <ext-link ext-link-type="uri" xlink:href="https://github.com/fastai/fastai">https://github.com/fastai/fastai</ext-link>. Accessed 26 Apr 2019.</mixed-citation>
    </ref>
  </ref-list>
</back>
