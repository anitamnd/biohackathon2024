<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BioData Min</journal-id>
    <journal-id journal-id-type="iso-abbrev">BioData Min</journal-id>
    <journal-title-group>
      <journal-title>BioData Mining</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1756-0381</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">5488355</article-id>
    <article-id pub-id-type="publisher-id">142</article-id>
    <article-id pub-id-type="doi">10.1186/s13040-017-0142-8</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>EFS: an ensemble feature selection tool implemented as R-package and web-application</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Neumann</surname>
          <given-names>Ursula</given-names>
        </name>
        <address>
          <email>u.neumann@wz-straubing.de</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Genze</surname>
          <given-names>Nikita</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Heider</surname>
          <given-names>Dominik</given-names>
        </name>
        <address>
          <email>d.heider@wz-straubing.de</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <aff id="Aff1"><label>1</label>Straubing Center of Science, Schulgasse 22, Straubing, 94315 Germany </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0704 7467</institution-id><institution-id institution-id-type="GRID">grid.4819.4</institution-id><institution/><institution>University of Applied Science, </institution></institution-wrap>Weihenstephan-Triesdorf, Freising, 85354 Germany </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000000123222966</institution-id><institution-id institution-id-type="GRID">grid.6936.a</institution-id><institution>Wissenschaftszentrum Weihenstephan, </institution><institution>Technische Universität München, </institution></institution-wrap>Freising, 85354 Germany </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>27</day>
      <month>6</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>27</day>
      <month>6</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2017</year>
    </pub-date>
    <volume>10</volume>
    <elocation-id>21</elocation-id>
    <history>
      <date date-type="received">
        <day>23</day>
        <month>11</month>
        <year>2016</year>
      </date>
      <date date-type="accepted">
        <day>12</day>
        <month>6</month>
        <year>2017</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2017</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p>Feature selection methods aim at identifying a subset of features that improve the prediction performance of subsequent classification models and thereby also simplify their interpretability. Preceding studies demonstrated that single feature selection methods can have specific biases, whereas an ensemble feature selection has the advantage to alleviate and compensate for these biases.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>The software EFS (Ensemble Feature Selection) makes use of multiple feature selection methods and combines their normalized outputs to a quantitative ensemble importance. Currently, eight different feature selection methods have been integrated in EFS, which can be used separately or combined in an ensemble.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p>EFS identifies relevant features while compensating specific biases of single methods due to an ensemble approach. Thereby, EFS can improve the prediction accuracy and interpretability in subsequent binary classification models.</p>
      </sec>
      <sec>
        <title>Availability</title>
        <p>EFS can be downloaded as an R-package from CRAN or used via a web application at <ext-link ext-link-type="uri" xlink:href="http://EFS.heiderlab.de">http://EFS.heiderlab.de</ext-link>.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Machine learning</kwd>
      <kwd>Feature selection</kwd>
      <kwd>Ensemble learning</kwd>
      <kwd>R-package</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2017</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p>In the field of data mining, feature selection (FS) has become a frequently applied preprocessing step for supervised learning algorithms, thus a great variety of FS techniques already exists. They are used for reducing the dimensionality of data by ranking features in order of their importance. These orders can then be used to eliminate those features that are less relevant to the problem at hand. This improves the overall performance of the model because it addresses the problem of overfitting. But there are several reasons that can cause instability and unreliability of the feature selection, e.g., the complexity of multiple relevant features, a small-n-large-p-problem, such as in high-dimensional data [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR2">2</xref>], or when the algorithm simply ignores stability [<xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR4">4</xref>]. In former studies, it has been demonstrated that a single optimal FS method cannot be obtained [<xref ref-type="bibr" rid="CR5">5</xref>]. For example, the Gini-coefficient is widely used in predictive medicine [<xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR7">7</xref>], but it has also been demonstrated to deliver unstable results in unbalanced datasets [<xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR9">9</xref>]. To counteract instability and therewith unreliability of feature selection methods, we developed an FS procedure for binary classification, which can be used, e.g., for random clinical trials. Our new approach ensemble feature selection (EFS) [<xref ref-type="bibr" rid="CR10">10</xref>] is based on the idea of ensemble learning [<xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR12">12</xref>], and thus is based on the aggregation of multiple FS methods. Thereby a quantification of the importance scores of features can be obtained and the method-specific biases can be compensated. In the current paper we introduce an R-package and a web server based on the EFS method. The user of the R-package as well as the web application can decide which FS methods should be conducted. Therewith, the web server and the R-package can be applied to perform an ensemble of FS methods or to calculate an individual FS score.</p>
  </sec>
  <sec id="Sec2">
    <title>Implementation</title>
    <p>We used existing implementations in <bold>R</bold> (<ext-link ext-link-type="uri" xlink:href="http://www.r-project.org/">http://www.r-project.org/</ext-link>) for our package EFS. The following section will briefly introduce our methodology. For deeper insights please refer to [<xref ref-type="bibr" rid="CR10">10</xref>]. Our EFS currently incorporates eight feature selection methods for binary classifications, namely median, Pearson- and Spearman-correlation, logistic regression, and four variable importance measures embedded in two different implementations of the random forest algorithm, namely <italic>cforest</italic> [<xref ref-type="bibr" rid="CR9">9</xref>] and <italic>randomForest</italic> [<xref ref-type="bibr" rid="CR13">13</xref>].</p>
    <sec id="Sec3">
      <title>Median</title>
      <p>This method compares the positive samples (class = 1) with negative samples (class = 0) by a Mann-Whitney-U Test. The resulting <italic>p</italic>-values are used as a measure of feature importance. Thus, a smaller <italic>p</italic>-value indicates a higher importance.</p>
    </sec>
    <sec id="Sec4">
      <title>Correlation</title>
      <p>We used the idea of a fast correlation based filter of of Yu and Liu [<xref ref-type="bibr" rid="CR14">14</xref>] to select features that are highly correlated with the dependent variable, but show only low correlation with other features. The fast correlation based filter eliminates features with high correlation with other features to avoid multicollinearity. The eliminated features get an importance value of zero. Two correlation coefficients, namely the Pearson product-moment and the Spearman rank correlation coefficient were adopted and their <italic>p</italic>-values were used as importance measure.</p>
    </sec>
    <sec id="Sec5">
      <title>Logistic regression</title>
      <p>The weighting system (i.e., <italic>β</italic>-coefficients) of the logistic regression (LR) is another popular feature selection method. As preprocessing step a Z-transformation is conducted to ensure comparability between the different ranges of feature values. The <italic>β</italic>-coefficients of the resulting regression equation represent the importance measure.</p>
    </sec>
    <sec id="Sec6">
      <title>Random forest</title>
      <p>Random forests (RFs) are ensembles of multiple decision trees, which gain their randomness from the randomly chosen starting feature for each tree. There are different implementations of the RF algorithm in R available, which offer diverse feature selection methods. On the one hand we incorporated the <italic>randomForest</italic> implementation based on the classification and regression tree (CART) algorithm by Breiman [<xref ref-type="bibr" rid="CR13">13</xref>]. The <italic>cforest</italic> implementation from the party package, on the other hand, uses conditional trees for the purpose of classification and regression (cf. [<xref ref-type="bibr" rid="CR15">15</xref>]). In both implementations an error-rate-based importance measure exists. The error-rate-based methods measure the difference before and after permuting the class variable. Due to their dependency on the underlying trees, results are varying for both error-rates. The <italic>randomForest</italic> approach also provides an importance measure based on the Gini-index, which measures the node impurity in the trees. Whereas in <italic>cforest</italic> an AUC-based variable importance measure is implemented. The AUC (area under the curve) is the integral of the receiver operating characteristics (ROC) curve. The AUC-based variable importance measure works to the error-rate-based one, but instead of computing the error rate for each tree before and after permuting a feature, the AUC is computed.</p>
    </sec>
    <sec id="Sec7">
      <title>Ensemble learning</title>
      <p>The results of each individual FS methods are normalized to a common scale, an interval from 0 to <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\frac {1}{n}$\end{document}</tex-math><mml:math id="M2"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="13040_2017_142_Article_IEq1.gif"/></alternatives></inline-formula>, where <italic>n</italic> is the number of conducted FS methods chosen by the user. Thereby we ensure the comparability of all FS methods and conserve the distances between the importance of one feature to another.</p>
    </sec>
    <sec id="Sec8">
      <title>R-package</title>
      <p>The EFS package is included in the Comprehensive R Archive Network (CRAN) and can be directly downloaded and installed by using the following R command:</p>
      <p><graphic xlink:href="13040_2017_142_Figa_HTML.gif" id="MO1"/> In the following, we introduce EFS’s three functions ensemble_fs, barplot_fs and efs_eval. A summary of all commands and parameters is shown in Table <xref rid="Tab1" ref-type="table">1</xref>.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Method overview</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Command</th><th align="left">Parameters</th><th align="left">Information</th></tr></thead><tbody><tr><td align="left">ensemble_fs</td><td align="left">data</td><td align="justify">object of class data.frame</td></tr><tr><td align="left"/><td align="left">classnumber</td><td align="justify">index of variable for binary classification</td></tr><tr><td align="left"/><td align="left">NA_threshold</td><td align="justify">threshold for deletion of features with a greater proportion of NAs</td></tr><tr><td align="left"/><td align="left">cor_threshold</td><td align="justify">correlation threshold within features</td></tr><tr><td align="left"/><td align="left">runs</td><td align="justify">amount of runs for randomForest and cforest</td></tr><tr><td align="left"/><td align="left">selection</td><td align="justify">selection of feature selection methods to be conducted</td></tr><tr><td align="left">barplot_fs</td><td align="left">name</td><td align="justify">character string giving the name of the file</td></tr><tr><td align="left"/><td align="left">efs_table</td><td align="justify">table object of class matrix retrieved from ensemble_fs</td></tr><tr><td align="left">efs_eval</td><td align="left">data</td><td align="justify">object of class data.frame</td></tr><tr><td align="left"/><td align="left">efs_table</td><td align="justify">table object of class matrix retrieved from ensemble_fs</td></tr><tr><td align="left"/><td align="left">file_name</td><td align="justify">character string, name which is used for the two possible PDF files.</td></tr><tr><td align="left"/><td align="left">classnumber</td><td align="justify">index of variable for binary classification</td></tr><tr><td align="left"/><td align="left">NA_threshold</td><td align="justify">threshold for deletion of features with a greater proportion of NAs</td></tr><tr><td align="left"/><td align="left">logreg</td><td align="justify">logical value indicating whether to conduct an evaluation via logistic regression or not</td></tr><tr><td align="left"/><td align="left">permutation</td><td align="justify">logical value indicating whether to conduct a permutation of the class variable or not</td></tr><tr><td align="left"/><td align="left">p_num</td><td align="justify">number of permutations; default set to a 100</td></tr><tr><td align="left"/><td align="left">variances</td><td align="justify">logical value indicating whether to calculate the variances of importances retrieved</td></tr><tr><td align="left"/><td align="left"/><td align="justify">from bootstrapping or not</td></tr><tr><td align="left"/><td align="left">jaccard</td><td align="justify">logical value indicating whether to calculate the Jaccard-index or not</td></tr><tr><td align="left"/><td align="left">bs_num</td><td align="justify">number of bootstrap permutations of the importances</td></tr><tr><td align="left"/><td align="left">bs_percentage</td><td align="justify">proportion of randomly selected samples for bootstrapping</td></tr></tbody></table><table-wrap-foot><p>The R-package EFS provides three functions</p></table-wrap-foot></table-wrap>
</p>
    </sec>
    <sec id="Sec9">
      <title>ensemble_fs</title>
      <p>The main function is ensemble_fs. It computes all FS methods which are chosen via the selection parameter and gives back a table with all normalized FS scores in a range between 0 and <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\frac {1}{n}$\end{document}</tex-math><mml:math id="M4"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="13040_2017_142_Article_IEq2.gif"/></alternatives></inline-formula>, where <italic>n</italic> is the number of incorporated feature selection methods. Irrelevant features (e.g., those with too many missing values) can be deleted.</p>
      <p>
        <graphic xlink:href="13040_2017_142_Figb_HTML.gif" id="MO2"/>
      </p>
      <p>The parameter data is an object of class data.frame. It consists of all features and the class variables as columns. The user has to set the parameter classnumber, which represents the column number of the class variable, i.e., the dependent variable for classification. NA_threshold represents a threshold concerning the allowed proportion of missing values (NAs) in a feature column. The default value is set to 0.2, meaning that features with more than 20% of NAs are neglected by the EFS algorithm. The cor_threshold parameter is only relevant for the correlation based filter methods. It determines the threshold of feature-to-feature correlations [<xref ref-type="bibr" rid="CR14">14</xref>]. The default value of cor_threshold is 0.7. The results of RF-based FS methods vary due to the randomness of their underlying algorithms. To obtain reliable results, the RF methods are conducted several times and averaged over the number of runs. This parameter, namely runs, is set to 100 by default. The user can select the FS methods for the EFS approach by using the selection parameter. Due to the high computational costs of the RFs, the default selection is set to</p>
      <p>
        <graphic xlink:href="13040_2017_142_Figc_HTML.gif" id="MO3"/>
      </p>
      <p>meaning that the two FS methods of the conditional random forest are not used by default.</p>
    </sec>
    <sec id="Sec10">
      <title>barblot_fs</title>
      <p>The barblot_fs function sums up all individual FS scores based on the output of ensemble_fs and visualizes them in an cumulative barplot.</p>
      <p>
        <graphic xlink:href="13040_2017_142_Figd_HTML.gif" id="MO4"/>
      </p>
      <p>The barplot_fs function uses the output of the ensemble_fs function, namely the efs_table, as input. The parameter name represents the filename of the resulting PDF, which is saved in the current working directory.</p>
    </sec>
    <sec id="Sec11">
      <title>efs_eval</title>
      <p>The efs_eval function provides several tests to evaluate the performance and validity of the EFS method. The parameters data, efs_table, file_name, classnumber and NA_threshold are identical to the corresponding parameters in the ensemble_fs function: <graphic xlink:href="13040_2017_142_Fige_HTML.gif" id="MO5"/>
</p>
      <sec id="Sec12">
        <title>Performance evaluation by logistic regression</title>
        <p>The performance of the EFS method can automatically be evaluated based on a logistic regression (LR) model, by setting the parameter logreg = TRUE. efs_eval uses an LR model of the selected features with a leave-one-out cross-validation (LOOCV) scheme, and additionally trains an LR model with all available feature in order to compare the two LR models based on their ROC curves and AUC values with ROCR [<xref ref-type="bibr" rid="CR16">16</xref>] and pROC based on the method of DeLong et al. [<xref ref-type="bibr" rid="CR17">17</xref>]. A PDF with the ROC curves is automatically saved in the working directory.</p>
      </sec>
      <sec id="Sec13">
        <title>Permutation of class variable</title>
        <p>In order to estimate the robustness of the resulting LR model, permutation tests [<xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR19">19</xref>] can be automatically performed, by setting the parameter permutation = TRUE. The class variable is randomly permuted p_num times and logistic regression is conducted. The resulting AUC values are then compared with the AUC from the original LR model using a Student’s t-Test. By default, p_num is set to 100 permutations.</p>
      </sec>
      <sec id="Sec14">
        <title>Variance of feature importances</title>
        <p>If the parameter variances is TRUE an evaluation of the stability of feature importances will be conducted by a bootstrapping algorithm. The samples are permuted for bs_num times and a subset of the samples (bs_percentage) is chosen to calculate the resulting feature importances. By default, the function chooses 90% of the samples and uses 100 repetitions. Finally, the variances of the importance values are reported.</p>
      </sec>
      <sec id="Sec15">
        <title>Jaccard-index</title>
        <p>The Jaccard-index measures the similarity of the feature subsets selected by permuted EFS iterations: 
<disp-formula id="Equa"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ J\left(S_{1},\ldots,S_{n}\right)=\frac{|S_{1}\cap\ldots\cap S_{n}|}{|S_{1}\cup\ldots\cup S_{n}|}, $$ \end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mi>J</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∩</mml:mo><mml:mo>…</mml:mo><mml:mo>∩</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∪</mml:mo><mml:mo>…</mml:mo><mml:mo>∪</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="13040_2017_142_Article_Equa.gif" position="anchor"/></alternatives></disp-formula> where <italic>S</italic>
<sub><italic>i</italic></sub> is the subset of features at the <italic>i</italic>-th iteration, for <italic>i</italic>=1,…,<italic>n</italic>. The value of the Jaccard-index varies from 0 to 1, where 1 implies absolute similarity of subsets. If jaccard = TRUE is set, the Jaccard-index of the subsets retrieved from the bootstrapping algorithm is calculated.</p>
      </sec>
    </sec>
    <sec id="Sec16">
      <title>Availability and requirements</title>
      <p>The package is available for R-users under the following requirements: 
<list list-type="bullet"><list-item><p><bold>Project name:</bold> Ensemble Feature Selection</p></list-item><list-item><p><bold>Project home page (CRAN):</bold><ext-link ext-link-type="uri" xlink:href="http://cran.r-project.org/web/packages/EFS">http://cran.r-project.org/web/packages/EFS</ext-link></p></list-item><list-item><p><bold>Operating system (s):</bold> Platform independent</p></list-item><list-item><p><bold>Programming language:</bold> R (≥ 3.0.2)</p></list-item><list-item><p><bold>License:</bold> GPL (≥ 2)</p></list-item><list-item><p><bold>Any restrictions to use by non-academics:</bold> none</p></list-item></list>
</p>
      <p>Due to the high relevance of our EFS tool for researchers who are not very familiar with R (e.g., medical practitioners), we also provide a web application at <ext-link ext-link-type="uri" xlink:href="http://EFS.heiderlab.de">http://EFS.heiderlab.de</ext-link>. It contains the functions ensemble_fs and barplot_fs. Therefore no background knowledge in R is necessary to use our new EFS software.</p>
    </sec>
  </sec>
  <sec id="Sec17" sec-type="results">
    <title>Results</title>
    <p>The dataset SPECTF has been obtained from the UCI Machine Learning Repository [<xref ref-type="bibr" rid="CR20">20</xref>] and is used as an example. It describes diagnosing of cardiac Single Proton Emission Computed Tomography (SPECT) images. The class-variable represents normal (= 0) and abnormal (= 1) results and can be found in the first column of the table of the file SPECTF.csv at the UCI repository. In general, the EFS approach accepts all types of data, i.e., all types of variables, except categorical variables. These variables have to be transformed to dummy variables in advance. Data has to be combined in a single file with one column indicating the class variable with 1 and 0, e.g., representing patients and control samples, or, e.g., positive and negative samples. After loading the dataset, we compute the EFS and store it in the variable “efs”:</p>
    <p>
      <graphic xlink:href="13040_2017_142_Figf_HTML.gif" id="MO6"/>
    </p>
    <p>The results can be visualized by the barplot_fs function:</p>
    <p><graphic xlink:href="13040_2017_142_Figg_HTML.gif" id="MO7"/> The output is a PDF named “SPECTF.pdf”. Figure <xref rid="Fig1" ref-type="fig">1</xref> shows this cumulative barplot, where each FS method is given in a different color. Various methods to evaluate the stability and reliability of the EFS results are conducted by the following command:
<fig id="Fig1"><label>Fig. 1</label><caption><p>Cumulative barplot retrieved from barplot_fs function of R-package EFS</p></caption><graphic xlink:href="13040_2017_142_Fig1_HTML" id="MO8"/></fig>
</p>
    <p>
      <graphic xlink:href="13040_2017_142_Figh_HTML.gif" id="MO9"/>
    </p>
    <p>The user retrieves two PDF files. Firstly, the resulting ROC curves of the LR test (“SPECTF_ROC.pdf”) including the p-value, according to Fig. <xref rid="Fig2" ref-type="fig">2</xref>. The p-value clearly shows that there is a significant improvement in terms of AUC of the LR with features selected by the EFS method compared the LR model without feature selection. Additionally, Fig. <xref rid="Fig3" ref-type="fig">3</xref> shows the file “SPECTF_Variances.pdf”, in which boxplots of the importances retrieved from the bootstrapping approach are given. The calculated variances can be accessed in the eval_tests output. A low variance implies that the importance of a feature is stable and reliable.
<fig id="Fig2"><label>Fig. 2</label><caption><p>Performance of LR model. On the <italic>y-axis</italic> the average true positive rate (i.e., sensitivity) and on the <italic>x-axis</italic> the false positive rate (i.e., 1-specificity) is shown. Two ROC curves are shown: of all features (<italic>black</italic>) and the EFS selected features (<italic>blue</italic>). The <italic>dotted line marks</italic> the performance of random guessing</p></caption><graphic xlink:href="13040_2017_142_Fig2_HTML" id="MO10"/></fig>
<fig id="Fig3"><label>Fig. 3</label><caption><p>Boxplot of importances retrieved from the bootstrapping algorithm</p></caption><graphic xlink:href="13040_2017_142_Fig3_HTML" id="MO11"/></fig>
</p>
    <p>An additional example is provided in the documentation of the R-package on a dataset consisting of weather data from the meteorological stations in Frankfurt(Oder), Germany in February 2016.</p>
  </sec>
  <sec id="Sec18" sec-type="conclusion">
    <title>Conclusion</title>
    <p>The EFS R-package and the web-application are implementations of an ensemble feature selection method for binary classifications. We showed that this method can improve the prediction accuracy and simplifies the interpretability by feature reduction.</p>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>AUC</term>
        <def>
          <p>Area under the curve</p>
        </def>
      </def-item>
      <def-item>
        <term>CART</term>
        <def>
          <p>Classification and regression tree</p>
        </def>
      </def-item>
      <def-item>
        <term>CRAN</term>
        <def>
          <p>Comprehensive R archive network</p>
        </def>
      </def-item>
      <def-item>
        <term>EFS</term>
        <def>
          <p>Ensemble feature selection</p>
        </def>
      </def-item>
      <def-item>
        <term>FS</term>
        <def>
          <p>Feature selection</p>
        </def>
      </def-item>
      <def-item>
        <term>LR</term>
        <def>
          <p>logistic regression</p>
        </def>
      </def-item>
      <def-item>
        <term>LOOCV</term>
        <def>
          <p>leave-one-out cross validation</p>
        </def>
      </def-item>
      <def-item>
        <term>RF</term>
        <def>
          <p>random forest</p>
        </def>
      </def-item>
      <def-item>
        <term>ROC</term>
        <def>
          <p>receiver operating characteristic</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <ack>
    <p>We are grateful to the UCI Machine Learning Repository for granting access to a great variety of datasets.</p>
    <sec id="d29e1135">
      <title>Funding</title>
      <p>This work was supported by the German Research Foundation (DFG) and the Technische Universität München within the funding program Open Access Publishing and the Deichmann Foundation, which had no role in study design, collection, analysis, and interpretation of data, and in writing the manuscript.</p>
    </sec>
    <sec id="d29e1140">
      <title>Availability of data and materials</title>
      <p>The dataset <italic>SPECTF</italic> in this article is available in the UCI Machines Learning repository, http://archive.ics.uci.edu/ml.</p>
    </sec>
    <sec id="d29e1148">
      <title>Authors’ contributions</title>
      <p>UN and NG have implemented the R-package. UN has implemented the web application and drafted the manuscript. DH designed and supervised the study. DH revised the manuscript. All authors read and approved the final manuscript.</p>
    </sec>
    <sec id="d29e1153">
      <title>Competing interests</title>
      <p>The authors declare that they have no competing interests.</p>
    </sec>
    <sec id="d29e1158">
      <title>Consent for publication</title>
      <p>Not applicable.</p>
    </sec>
    <sec id="d29e1163">
      <title>Ethics approval and consent to participate</title>
      <p>Not applicable.</p>
    </sec>
    <sec id="d29e1168">
      <title>Publisher’s Note</title>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </sec>
  </ack>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dybowski</surname>
            <given-names>JN</given-names>
          </name>
          <name>
            <surname>Heider</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Hoffmann</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Structure of hiv-1 quasi-species as early indicator for switches of co-receptor tropism</article-title>
        <source>AIDS Res Ther</source>
        <year>2010</year>
        <volume>7</volume>
        <fpage>41</fpage>
        <pub-id pub-id-type="doi">10.1186/1742-6405-7-41</pub-id>
        <?supplied-pmid 21118549?>
        <pub-id pub-id-type="pmid">21118549</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pyka</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Hahn</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Heider</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Krug</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sommer</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kircher</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Jansen</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Baseline activity predicts working memory load of preceding task condition</article-title>
        <source>Hum Brain Mapp</source>
        <year>2013</year>
        <volume>34</volume>
        <issue>11</issue>
        <fpage>3010</fpage>
        <lpage>22</lpage>
        <pub-id pub-id-type="doi">10.1002/hbm.22121</pub-id>
        <?supplied-pmid 22696432?>
        <pub-id pub-id-type="pmid">22696432</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jain</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Zongker</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Feature selection: evaluation, application, and small sample performance</article-title>
        <source>IEEE Trans Pattern Anal Mach Intell</source>
        <year>1997</year>
        <volume>19</volume>
        <issue>2</issue>
        <fpage>153</fpage>
        <lpage>8</lpage>
        <pub-id pub-id-type="doi">10.1109/34.574797</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Stable feature selection for biomarker discovery</article-title>
        <source>Comput Biol Chem</source>
        <year>2010</year>
        <volume>34</volume>
        <fpage>215</fpage>
        <lpage>25</lpage>
        <pub-id pub-id-type="doi">10.1016/j.compbiolchem.2010.07.002</pub-id>
        <?supplied-pmid 20702140?>
        <pub-id pub-id-type="pmid">20702140</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>YHY</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Segal</surname>
            <given-names>MR</given-names>
          </name>
        </person-group>
        <article-title>Identifying differentially expressed genes from microarray experiments via statistic synthesis</article-title>
        <source>Bioinformatics</source>
        <year>2004</year>
        <volume>21</volume>
        <issue>7</issue>
        <fpage>1084</fpage>
        <lpage>93</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bti108</pub-id>
        <?supplied-pmid 15513985?>
        <pub-id pub-id-type="pmid">15513985</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Leclerc</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Lert</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Cecile</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Differential mortality: Some comparisons between england and wales, finland and france, based on inequality measures</article-title>
        <source>Int J Epidemiol</source>
        <year>1990</year>
        <volume>19</volume>
        <issue>4</issue>
        <fpage>1001</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="doi">10.1093/ije/19.4.1001</pub-id>
        <?supplied-pmid 2083984?>
        <pub-id pub-id-type="pmid">2083984</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Llorca</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Delgado-Rodríguez</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Visualising exposure-disease association: the lorenz curve and the gini index</article-title>
        <source>Med Sci Monit</source>
        <year>2002</year>
        <volume>8</volume>
        <issue>10</issue>
        <fpage>193</fpage>
        <lpage>7</lpage>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sandri</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Zuccolotto</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>A bias correction algorithm for the gini variable importance measure in classification trees</article-title>
        <source>J Comput Graph Stat</source>
        <year>2008</year>
        <volume>17</volume>
        <issue>3</issue>
        <fpage>611</fpage>
        <lpage>28</lpage>
        <pub-id pub-id-type="doi">10.1198/106186008X344522</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Boulesteix</surname>
            <given-names>AL</given-names>
          </name>
          <name>
            <surname>Janitza</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kruppa</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>König</surname>
            <given-names>IR</given-names>
          </name>
        </person-group>
        <article-title>Overview of random forest methodology and practical guidance with emphasis on computational biology and bioinformatics</article-title>
        <source>Wiley Interdiscip Rev Data Min Knowl Discov</source>
        <year>2012</year>
        <volume>2</volume>
        <issue>6</issue>
        <fpage>493</fpage>
        <lpage>507</lpage>
        <pub-id pub-id-type="doi">10.1002/widm.1072</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Neumann</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Riemenschneider</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Sowa</surname>
            <given-names>JP</given-names>
          </name>
          <name>
            <surname>Baars</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Kälsch</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Canbay</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Heider</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Compensation of feature selection biases accompanied with improved predictive performance for binary classification by using a novel ensemble feature selection</article-title>
        <source>BioData Min</source>
        <year>2016</year>
        <volume>9</volume>
        <issue>1</issue>
        <fpage>36</fpage>
        <pub-id pub-id-type="doi">10.1186/s13040-016-0114-4</pub-id>
        <?supplied-pmid 27891179?>
        <pub-id pub-id-type="pmid">27891179</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Saeys</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Abeel</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Van de Peer</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Robust feature selection using ensemble feature selection techniques</article-title>
        <source>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</source>
        <year>2008</year>
        <publisher-loc>Berlin</publisher-loc>
        <publisher-name>Springer-Verlag</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Abeel</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Helleputte</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Van de Peer</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Dupont</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Saeys</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Robust biomarker identification for cancer diagnosis with ensemble feature selection methods</article-title>
        <source>Bioinformatics</source>
        <year>2010</year>
        <volume>26</volume>
        <issue>3</issue>
        <fpage>392</fpage>
        <lpage>8</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btp630</pub-id>
        <?supplied-pmid 19942583?>
        <pub-id pub-id-type="pmid">19942583</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Breiman</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Random forests</article-title>
        <source>Mach Learn</source>
        <year>2001</year>
        <volume>45</volume>
        <issue>1</issue>
        <fpage>5</fpage>
        <lpage>32</lpage>
        <pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yu</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Efficient feature selection via analysis of relevance and redundancy</article-title>
        <source>J Mach Learn Res</source>
        <year>2004</year>
        <volume>5</volume>
        <fpage>1205</fpage>
        <lpage>24</lpage>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Strobl</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Boulesteix</surname>
            <given-names>AL</given-names>
          </name>
          <name>
            <surname>Kneib</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Augustin</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Zeileis</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Conditional variable importance for random forest</article-title>
        <source>BMC Bioinforma</source>
        <year>2006</year>
        <volume>9</volume>
        <issue>307</issue>
        <fpage>1</fpage>
        <lpage>11</lpage>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sing</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Beerenwinkel</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Lengauer</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Rocr: visualizing classifier performance in r</article-title>
        <source>Bioinformatics</source>
        <year>2005</year>
        <volume>21</volume>
        <issue>20</issue>
        <fpage>3940</fpage>
        <lpage>41</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bti623</pub-id>
        <?supplied-pmid 16096348?>
        <pub-id pub-id-type="pmid">16096348</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>DeLong</surname>
            <given-names>ER</given-names>
          </name>
          <name>
            <surname>DeLong</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>Clarke-Pearson</surname>
            <given-names>DL</given-names>
          </name>
        </person-group>
        <article-title>Comparing the areas under two or more correlated receiver operating characteristic curves: A nonparametric approach</article-title>
        <source>Biometrics</source>
        <year>1988</year>
        <volume>44</volume>
        <issue>3</issue>
        <fpage>837</fpage>
        <lpage>45</lpage>
        <pub-id pub-id-type="doi">10.2307/2531595</pub-id>
        <?supplied-pmid 3203132?>
        <pub-id pub-id-type="pmid">3203132</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Barbosa</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Röttger</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Hauschild</surname>
            <given-names>AC</given-names>
          </name>
          <name>
            <surname>Azevedo</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Baumbach</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>On the limits of computational functional genomics for bacterial lifestyle prediction</article-title>
        <source>Brief Funct Genomics</source>
        <year>2014</year>
        <volume>13</volume>
        <fpage>398</fpage>
        <lpage>408</lpage>
        <pub-id pub-id-type="doi">10.1093/bfgp/elu014</pub-id>
        <?supplied-pmid 24855068?>
        <pub-id pub-id-type="pmid">24855068</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sowa</surname>
            <given-names>JP</given-names>
          </name>
          <name>
            <surname>Atmaca</surname>
            <given-names>Ö</given-names>
          </name>
          <name>
            <surname>Kahraman</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Schlattjan</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lindner</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Sydor</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Scherbaum</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Lackner</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Gerken</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Heider</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Arteel</surname>
            <given-names>GE</given-names>
          </name>
          <name>
            <surname>Erim</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Canbay</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Non-invasive separation of alcoholic and non-alcoholic liver disease with predictive modeling</article-title>
        <source>PLOS ONE</source>
        <year>2014</year>
        <volume>9</volume>
        <issue>7</issue>
        <fpage>101444</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0101444</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20</label>
      <mixed-citation publication-type="other">Lichman M. UCI Machine Learning Repository. 2013. <ext-link ext-link-type="uri" xlink:href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</ext-link>.</mixed-citation>
    </ref>
  </ref-list>
</back>
