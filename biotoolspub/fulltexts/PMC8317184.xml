<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Zool Res</journal-id>
    <journal-id journal-id-type="iso-abbrev">Zool Res</journal-id>
    <journal-id journal-id-type="publisher-id">ZR</journal-id>
    <journal-title-group>
      <journal-title>Zoological Research</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">2095-8137</issn>
    <publisher>
      <publisher-name>Science Press</publisher-name>
      <publisher-loc>16 Donghuangchenggen Beijie, Beijing 100717, China</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8317184</article-id>
    <article-id pub-id-type="pmid">34235898</article-id>
    <article-id pub-id-type="publisher-id">zr-42-4-492</article-id>
    <article-id pub-id-type="doi">10.24272/j.issn.2095-8137.2021.141</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>3DPhenoFish: Application for two- and three-dimensional fish morphological phenotype extraction from point cloud analysis</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Liao</surname>
          <given-names>Yu-Hang</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">1</xref>
        <xref ref-type="author-notes" rid="fn1">#</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhou</surname>
          <given-names>Chao-Wei</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">2</xref>
        <xref ref-type="aff" rid="aff3">3</xref>
        <xref ref-type="author-notes" rid="fn1">#</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Liu</surname>
          <given-names>Wei-Zhen</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jin</surname>
          <given-names>Jing-Yi</given-names>
        </name>
        <xref ref-type="aff" rid="aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Dong-Ye</given-names>
        </name>
        <xref ref-type="aff" rid="aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Liu</surname>
          <given-names>Fei</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Fan</surname>
          <given-names>Ding-Ding</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zou</surname>
          <given-names>Yu</given-names>
        </name>
        <xref ref-type="aff" rid="aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Mu</surname>
          <given-names>Zen-Bo</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Shen</surname>
          <given-names>Jian</given-names>
        </name>
        <xref ref-type="aff" rid="aff5">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Liu</surname>
          <given-names>Chun-Na</given-names>
        </name>
        <xref ref-type="aff" rid="aff6">6</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Xiao</surname>
          <given-names>Shi-Jun</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">1</xref>
        <xref ref-type="aff" rid="aff2">2</xref>
        <xref ref-type="aff" rid="aff4">4</xref>
        <xref ref-type="corresp" rid="cor1">*</xref>
        <email>shijun_xiao@163.com</email>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yuan</surname>
          <given-names>Xiao-Hui</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">1</xref>
        <xref ref-type="corresp" rid="cor2">*</xref>
        <email>yuanxiaohui@whut.edu.cn</email>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Liu</surname>
          <given-names>Hai-Ping</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">2</xref>
        <xref ref-type="corresp" rid="cor3">*</xref>
        <email>luihappying@163.com</email>
      </contrib>
    </contrib-group>
    <aff id="aff1">
      <label>1</label>
      <addr-line>Department of Computer Science, Wuhan University of Technology, Wuhan, Hubei 430070, China</addr-line>
    </aff>
    <aff id="aff2">
      <label>2</label>
      <addr-line>Institute of Fisheries Science, Tibet Academy of Agricultural and Animal Husbandry Sciences, Lhasa, Tibet 850000, China</addr-line>
    </aff>
    <aff id="aff3">
      <label>3</label>
      <addr-line>Key Laboratory of Freshwater Fish Reproduction and Development (Ministry of Education), College of Fisheries, Southwest University, Chongqing 402460, China</addr-line>
    </aff>
    <aff id="aff4">
      <label>4</label>
      <addr-line>Jiaxing Key Laboratory for New Germplasm Breeding of Economic Mycology, Jiaxing, Zhejiang 314000, China</addr-line>
    </aff>
    <aff id="aff5">
      <label>5</label>
      <addr-line>Huadian Tibet Energy Co., Ltd, Lhasa, Tibet 851415, China</addr-line>
    </aff>
    <aff id="aff6">
      <label>6</label>
      <addr-line>China Institute of Water Resources and Hydropower Research, Beijing 100038, China</addr-line>
    </aff>
    <author-notes>
      <corresp id="cor1">E-mail:
<email>shijun_xiao@163.com</email></corresp>
      <corresp id="cor2">
        <email>yuanxiaohui@whut.edu.cn</email>
      </corresp>
      <corresp id="cor3">
        <email>luihappying@163.com</email>
      </corresp>
      <fn id="fn1">
        <p>#Authors contributed equally to this work</p>
      </fn>
    </author-notes>
    <pub-date pub-type="ppub">
      <day>18</day>
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <volume>42</volume>
    <issue>4</issue>
    <fpage>492</fpage>
    <lpage>501</lpage>
    <history>
      <date date-type="received">
        <day>24</day>
        <month>4</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>5</day>
        <month>7</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Editorial Office of Zoological Research, Kunming Institute of Zoology, Chinese Academy of Sciences</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Editorial Office of Zoological Research, Kunming Institute of Zoology, Chinese Academy of Sciences</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link xlink:href="https://creativecommons.org/licenses/by-nc/4.0/" ext-link-type="uri">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Fish morphological phenotypes are important resources in artificial breeding, functional gene mapping, and population-based studies in aquaculture and ecology. Traditional morphological measurement of phenotypes is rather expensive in terms of time and labor. More importantly, manual measurement is highly dependent on operational experience, which can lead to subjective phenotyping results. Here, we developed 3DPhenoFish software to extract fish morphological phenotypes from three-dimensional (3D) point cloud data. Algorithms for background elimination, coordinate normalization, image segmentation, key point recognition, and phenotype extraction were developed and integrated into an intuitive user interface. Furthermore, 18 key points and traditional 2D morphological traits, along with 3D phenotypes, including area and volume, can be automatically obtained in a visualized manner. Intuitive fine-tuning of key points and customized definitions of phenotypes are also allowed in the software. Using 3DPhenoFish, we performed high-throughput phenotyping for four endemic Schizothoracinae species, including <italic>Schizopygopsis younghusbandi</italic>, <italic>Oxygymnocypris stewartii</italic>, <italic>Ptychobarbus dipogon</italic>, and <italic>Schizothorax oconnori</italic>. Results indicated that the morphological phenotypes from 3DPhenoFish exhibited high linear correlation (&gt;0.94) with manual measurements and offered informative traits to discriminate samples of different species and even for different populations of the same species. In summary, we developed an efficient, accurate, and customizable tool, 3DPhenoFish, to extract morphological phenotypes from point cloud data, which should help overcome traditional challenges in manual measurements. 3DPhenoFish can be used for research on morphological phenotypes in fish, including functional gene mapping, artificial selection, and conservation studies. 3DPhenoFish is an open-source software and can be downloaded for free at https://github.com/lyh24k/3DPhenoFish/tree/master.
</p>
    </abstract>
    <kwd-group kwd-group-type="author-created">
      <kwd>Fish</kwd>
      <kwd>Phenomics</kwd>
      <kwd>Morphology</kwd>
      <kwd>Point cloud</kwd>
      <kwd>3D scanning</kwd>
    </kwd-group>
    <funding-group>
      <funding-statement>This work was supported by the National Natural Science Foundation of China (32072980) and Key Research and Development Projects in Tibet (XZ202001ZY0016N, XZ201902NB02, XZNKY-2019-C-053)</funding-statement>
    </funding-group>
  </article-meta>
</front>
<body>
  <sec id="s01">
    <title>INTRODUCTION</title>
    <p>Fish morphology provides valuable data for fishery conservation management, genome association research, and artificial breeding (<xref rid="d31e1223" ref-type="bibr">López-Fanjul &amp; Toro, 2007</xref>). In association studies, both genotypes and phenotypes are important data in functional gene identification. The emergence and development of high-throughput genomic sequencing has made it easy to obtain the genotypes of millions of fish; however, phenotyping based on traditional manual measurement is a limiting step in such studies and costly in terms of labor and time. More importantly, phenotyping results may be subjective as manual measurement requires training and experience. The lack of monitoring during and after manual measurement can also lead to potential phenotyping batch effects and biases (<xref rid="bFernandes2020" ref-type="bibr">Fernandes et al., 2020</xref>). Therefore, there is great demand for a high-throughput and intelligent technique to automatically extract morphological phenotypes for fish studies.
</p>
    <p>The advancement of computer vision technology enables morphological phenotypes to be obtained from two-dimensional (2D) images using a monocular camera (<xref rid="bHartmann2011" ref-type="bibr">Hartmann et al., 2011</xref>). Imaging techniques have been applied in fish recognition (<xref rid="bSpampinato2010" ref-type="bibr">Spampinato et al., 2010</xref>) and counting (<xref rid="bAliyu2017" ref-type="bibr">Aliyu et al., 2017</xref>; <xref rid="bSpampinato2008" ref-type="bibr">Spampinato et al., 2008</xref>) using limited 2D morphological phenotypes (mainly body length) (<xref rid="bHao2015" ref-type="bibr">Hao et al., 2015</xref>; <xref rid="bShah2019" ref-type="bibr">Shah et al., 2019</xref>). For more comprehensive morphological information, IMAFISH_ML (<xref rid="bNavarro2016" ref-type="bibr">Navarro et al., 2016</xref>) uses multiple cameras to capture images from two directions for morphological phenotype measurements. Still, this requires the fish to be frozen and ﬁns to be trimmed, causing irreversible damage to the fish. However, several non-invasive methods have been developed for living ﬁsh. For instance, <xref rid="bWang2019" ref-type="bibr">Wang et al. (2019)</xref> developed a contour-based method for measuring the length of living fish in water. <xref rid="bFernandes2020" ref-type="bibr">Fernandes et al. (2020)</xref> used a monocular camera and computer vision technique to measure body area, length, and height in tilapia. However, these studies were designed for specific fish species using 2D images, and morphological phenotypes extracted via these tools are still limited.
</p>
    <p>Although 2D features provide valuable data for phenotypes, fish are three-dimensional (3D) in nature and exhibit more elaborate and complex phenotypes, such as head volume and body conformation, which are difficult to capture by 2D imaging processes. These 3D traits provide informative phenotypes to describe conformation features (<xref rid="bZermas2020" ref-type="bibr">Zermas et al., 2020</xref>). However, 3D phenotypes, such as surface area and volume, are difficult to measure manually. With the continued development of computer vision technology, sensor-based 3D reconstruction methods have been increasingly used in life sciences and agriculture (<xref rid="bComba2018" ref-type="bibr">Comba et al., 2018</xref>), as well as in plant and livestock research. For example, mango and apple size measurement (<xref rid="bGongal2018" ref-type="bibr">Gongal et al., 2018</xref>; <xref rid="bWang2017" ref-type="bibr">Wang et al., 2017</xref>) and guava detection and pose estimation (<xref rid="bLin2019" ref-type="bibr">Lin et al., 2019</xref>) have been solved using 3D computer vision techniques. In addition, several techniques have been applied in livestock. For example, <xref rid="bLe2019" ref-type="bibr">Le Cozler et al. (2019)</xref> scanned 3D images of an entire cow’s body and estimated six phenotypes: i.e., height, heart girth, chest depth, hip width, backside width, and ischial width, which showed high correlation with manual measurements. Light detection and ranging (LiDAR) and cameras with depth information have been used to construct 3D images and calculate body conformation phenotypes in cows (<xref rid="bPezzuolo2018" ref-type="bibr">Pezzuolo et al., 2018</xref>), cattle (<xref rid="bBatanov2019" ref-type="bibr">Batanov et al., 2019</xref>; <xref rid="bHuang2018" ref-type="bibr">Huang et al., 2018</xref>), and horses (<xref rid="d31e1273" ref-type="bibr">Pérez-Ruiz et al., 2020</xref>). These studies highlight the feasibility of using 3D information to obtain more comprehensive phenotypes for animals; however, the application of 3D imaging for morphological phenotypes in fish is limited, largely due to the lack of fish-specific tools to provide comprehensive 2D and 3D phenotype data.
</p>
    <p>In the current study, we developed 3DPhenoFish software for morphological phenotype extraction using 3D point cloud information for fish species. Its structured pipeline includes background filtering, fish segmentation, key point recognition, and phenotype extraction. Both 2D and 3D morphological phenotypes represented by body length, surface area, and volume, can be automatically extracted. In addition, 3DPhenoFish also allows users to adjust key points and define new phenotypes in an intuitive visualized manner. We used endemic Schizothoracinae fish species from Tibet to validate the 3DPhenoFish phenotyping, which showed high correlation (&gt;0.94) to manual measurements. This software could be used to discriminate samples of different species and even different populations of the same species.</p>
  </sec>
  <sec id="s02">
    <title>MATERIALS AND METHODS</title>
    <p>3DPhenoFish extracts the morphological phenotypes of fish from 3D point cloud data. The following points (<xref ref-type="fig" rid="Figure1">Figure 1</xref>) describe the main data analysis pipeline steps integrated in 3DPhenoFish:
</p>
    <fig id="Figure1" orientation="portrait" position="float">
      <label>Figure 1</label>
      <caption>
        <p>Workflow scheme in 3DPhenoFish for point cloud analysis and morphological phenotype extraction</p>
      </caption>
      <abstract abstract-type="note">
        <p>Whole pipeline includes data acquisition, data pre-processing, semantic segmentation, phenotype extraction, and data management.</p>
      </abstract>
      <graphic xlink:href="zr-42-4-492-1"/>
    </fig>
    <p>1. Data acquisition: This step uses an industrial 3D scanner to obtain OBJ or PCD files as input for 3DPhenoFish.</p>
    <p>2. Data pre-processing: This step removes background and outliers.</p>
    <p>3. Semantic segmentation: This step deals with the segmentation of the fish head, body, and fins.</p>
    <p>4. Morphological phenotype extraction: This step recognizes key points and extracts morphological phenotypes.</p>
    <p>5. Phenotype management: This step involves defining custom phenotypes and key points, which are then stored in the database.</p>
    <sec id="s02.01">
      <title>Data acquisition</title>
      <p>The first step in using 3DPhenoFish involves scanning 3D point cloud data of fish samples. Many industrial 3D scanners can produce a full-color point cloud with an accuracy of 0.1 mm or higher, which satisfies the input data requirements for 3DPhenoFish. We used a GScan 3D scanner (http://en.zg-3d.com/fullcolor/131.html), which uses white-light grating stripe projection technology for surface scanning to obtain original point cloud data of fish (Supplementary Figure S1). Marking labels were applied to a yellow board that served as a scanning background. After anesthetization with MS-222, fish were placed on the board for scanning (see Supplementary Figure S1).</p>
    </sec>
    <sec id="s02.02">
      <title>Data pre-processing</title>
      <p><bold>Point cloud filtering:</bold> Point cloud filtering is the first step in 3D point cloud pre-processing and influences subsequent semantic segmentation and morphological phenotype extraction. This step involves point cloud down-sampling, planar background removal, and outlier elimination (<xref ref-type="fig" rid="Figure2">Figure 2</xref>).
</p>
      <fig id="Figure2" orientation="portrait" position="float">
        <label>Figure 2</label>
        <caption>
          <p>Point cloud filtering for fish point clouds</p>
        </caption>
        <abstract abstract-type="note">
          <p>Process includes point cloud down-sampling (A), background filtering (B), outlier filtering (C), and final fish point cloud (D).</p>
        </abstract>
        <graphic xlink:href="zr-42-4-492-2"/>
      </fig>
      <p>First, point cloud down-sampling is performed to reduce point cloud density and computational burden while maintaining conformation features. In 3DPhenoFish, down-sampling is performed with the voxel grid filter method (<xref rid="bOrts-Escolano2013" ref-type="bibr">Orts-Escolano et al., 2013</xref>). In brief, 3D voxel grids, i.e., collections of 3D cubes in space, are generated from the point cloud data, and the geometric center of the points in one voxel is used to represent the voxel overall. The default size to generate the 3D cubes is 1 mm<sup>3</sup>.
</p>
      <p>Second, sparse and discrete non-fish outliers are removed from the point cloud data. The Statistical Outlier Removal Filter (SORF) (<xref rid="bBalta2018" ref-type="bibr">Balta et al., 2018</xref>) is used to calculate the average distance from each point to its adjacent points. Assuming the average distances follow Gaussian distribution, points with an average distance beyond the threshold are regarded as outliers and eliminated.
</p>
      <p>Third, point data from the background plane are removed. As background points are in the same plane, the background can be removed using Random Sample Consensus (RANSAC) (<xref rid="bSchnabel2007" ref-type="bibr">Schnabel et al., 2007</xref>), with the number of random extractions of the point cloud sample set to 100 and the distance between the target point and plane set to 0.8. However, as the tail fin is close to the background plane, RANSAC-based fitting to the plane may result in tail fin removal. To solve this problem, the boundary of the fish point cloud is detected, and points on the boundary are used as seed points to grow outward based on constraints of normal difference, color difference, and spatial distance difference, thus making segmentation of the background and tail fin more accurate. Finally, taking advantage of the weak connections among sparse and discrete noisy points after removal of the background plane, distances between the target point and its neighbors are calculated, and Euclidean clustering (<xref rid="bWu2016" ref-type="bibr">Wu et al., 2016</xref>) is employed to eliminate these sparse and discrete point data.
</p>
      <p><bold>Coordinate normalization:</bold> After background and noisy point filtering, the point cloud coordinates of target fish are normalized. As the coordinate system may influence subsequent phenotype extraction, the normalized coordinates for the fish point cloud are defined as follows: the geometric center of the fish point cloud is the coordinate origin; and the X-, Y-, and Z-axes, conforming to the right-hand rule, are consistent with the directions of body length, body height, and body width, respectively. The Z-axis is perpendicular to the background, the Y-axis points to the dorsal fin, and the X-axis points to the fish tail, as shown in <xref ref-type="fig" rid="Figure3">Figure 3</xref>. The Z-axis is the axis with the smallest angle to the normal vector of the plane point cloud extracted above. Principal component analysis (PCA) of point coordinates is employed for automatic coordinate normalization (<xref rid="bPezzuolo2018" ref-type="bibr">Pezzuolo et al., 2018</xref>), and the X-axis is the axis represented by the feature vector with the largest eigenvalue. A transformation matrix from the local coordinate system to the global coordinate system is obtained, and initial coordinate normalization is performed (<xref ref-type="fig" rid="Figure3">Figure 3</xref>).
</p>
      <fig id="Figure3" orientation="portrait" position="float">
        <label>Figure 3</label>
        <caption>
          <p>Coordinate normalization for fish point cloud</p>
        </caption>
        <abstract abstract-type="note">
          <p>Normalization process involves two steps: transferring point cloud of fish head from positive X-axis (A and B) to negative X-axis (C and D), and then transferring point cloud of fish dorsal fin to positive Y-axis (D).</p>
        </abstract>
        <graphic xlink:href="zr-42-4-492-3"/>
      </fig>
      <p>Although the above normalization could lay fish length along the X-axis, we still need to normalize the orientation of the head and dorsal fin. First, the point cloud is transferred so that the X-axis points to the fish tail. As the tail fin is generally closer to the background plane, the point cloud for the head shows a higher standard deviation of the Z-axis coordinate compared to that for the tail fin. Therefore, we can distinguish the head and tail by comparing coordinates of cloud points within two 15% end regions along the fish’s total length; the point cloud with a larger standard deviation of the Z-axis coordinate is the fish head. In this way, the fish coordinates can be normalized so that the X-axis points to the fish tail (<xref ref-type="fig" rid="Figure3">Figure 3C</xref>, <xref ref-type="fig" rid="Figure3">D</xref>). Second, the point cloud is transferred so that the Y-axis points to the dorsal fin. Given that the fish body near to the dorsal fin is darker than the belly of the fish, body colors of the cloud points can be compared in the positive and negative Y-axis regions. The larger average gray value is in the direction of the dorsal fin, and thus the fish coordinates can finally be normalized on the Y-axis (<xref ref-type="fig" rid="Figure3">Figure 3D</xref>).
</p>
      <p>After the background is eliminated and fish coordinates are normalized, we can take advantage of symmetric features to complement the fish model. 3DPhenoFish scans the fish in planes perpendicular to the X-axis with a step size of 1. For each plane, two end points on the Y-axis are selected, and a plane that crosses two end points and is parallel to the X-axis is used as the plane of symmetry to complete the fish point cloud model. Finally, point cloud data are generated to construct a symmetric complement to the fish model.</p>
    </sec>
    <sec id="s02.03">
      <title>Semantic segmentation</title>
      <p><bold>Head segmentation based on template matching:</bold> After data pre-processing, the fish head is recognized in the point cloud. The template matching method (<xref rid="d31e1061" ref-type="bibr">Bär et al., 2012</xref>) is then applied for fish head segmentation. First, we randomly choose the head of a fish of the same species as a template. The fish head template is aligned to the target point cloud using the Sample Consensus Initial Alignment (SAC-IA) algorithm (<xref rid="bChen2017" ref-type="bibr">Chen et al., 2017</xref>) and Iterative Closest Point (ICP) algorithm (<xref rid="bBesl1992" ref-type="bibr">Besl &amp; McKay, 1992</xref>) for coarse and fine registration, respectively; thus, we obtain the approximate orientation of the head.
</p>
      <p><bold>Fish eye segmentation based on curvature:</bold> Based on the fish head segmentation above, we can calculate and obtain 200 points with the top curvature value in the fish head region. Considering the circular shape of the fish eye, RANSAC is applied to segment the circular part of the fish as the eye.
</p>
      <p><bold>Fin segmentation based on super voxel region growth:</bold> A super voxel-based region growth segmentation method is used to separate the point cloud for fish fins. As an initial step, the super voxel method (<xref rid="bLi2018" ref-type="bibr">Li &amp; Sun, 2018</xref>) is used to pre-segment the point cloud. First, the point cloud is voxelized to obtain a voxel cloud. We can then construct a mesh of the voxel space using resolution <italic>R</italic><sub>seed</sub> and select the voxel closest to the center of the mesh as the initial seed voxel. We then calculate the number of voxels in the neighborhood radius <italic>R</italic><sub>search</sub> of the seed voxel, and seed voxels with a number less than the threshold (four as default) are deleted. Distance <italic>D</italic> of a seed voxel to its neighboring voxels within the radius of <italic>R</italic><sub>seed</sub> is then calculated using equation 1, where <italic>D<sub>c</sub></italic> is the Euclidean distance in normalized RGB space, <italic>D<sub>s</sub></italic> is the Euclidean distance between two voxels, <italic>D<sub>n</sub></italic> is the normal angle between two voxels, and <italic>w<sub>c</sub></italic>, <italic>w<sub>s</sub></italic>, and <italic>w<sub>n</sub></italic> are the weights for distances. The neighboring voxel with the smallest <italic>D</italic> value is considered as the super voxel, and the neighboring voxels of this voxel are added to the search queue. All other seed voxels are grown simultaneously using the above method until the number of super voxels exceeds the threshold (1800 as default), or all neighboring points have been clustered. Finally, we obtain a pre-segmentation result for the point cloud (<xref ref-type="fig" rid="Figure4">Figure 4A</xref>).
</p>
      <fig id="Figure4" orientation="portrait" position="float">
        <label>Figure 4</label>
        <caption>
          <p>Semantic segmentation for fish point cloud</p>
        </caption>
        <abstract abstract-type="note">
          <p>A: Pre-segment point cloud using super voxel method. B: Fin segmentation is performed using adaptive weighted region growth segmentation. C: Head, eye, body, and fins are segmented from point cloud, then used for following key point recognition and morphological phenotype extraction.</p>
        </abstract>
        <graphic xlink:href="zr-42-4-492-4"/>
      </fig>
      <p>
        <disp-formula>
          <label>1</label>
          <tex-math id="M1">\begin{document}$ D=\sqrt{{w}_{c}{{D}_{c}}^{2}+\frac{{w}_{s}{{D}_{s}}^{2}}{3{{R}_{seed}}^{2}}+{w}_{n}{{D}_{n}}^{2}} $\end{document}</tex-math>
        </disp-formula>
      </p>
      <p>Based on the pre-segmentation results, the fins are segmented by an adaptive weighted region growth segmentation method (<xref rid="bVo2015" ref-type="bibr">Vo et al., 2015</xref>). First, we select the super voxel with the smallest mean curvature as the seed facet and build a seed facet queue. We then calculate the <italic>D<sub>m</sub></italic> between the seed facet and its neighboring facets according to equation 2, where <italic>G<sub>f</sub></italic> is the difference in average gray value between two super voxels, <italic>D<sub>f</sub></italic> is the difference in normal angle difference between two super voxels, and <italic>λ</italic> and <italic>μ</italic> are weights.
</p>
      <p>
        <disp-formula>
          <label>2</label>
          <tex-math id="M2">\begin{document}$ {D}_{m}=\lambda {G}_{f}+\mu {D}_{f} $\end{document}</tex-math>
        </disp-formula>
      </p>
      <p>If the <italic>D<sub>m</sub></italic> of a facet is smaller than the threshold, the facet is clustered with the seed facet, marked as used, and queued as a seed facet. The facets in the seed queue are used until the seed queue is empty and all facets are marked as used. In addition, the color weight threshold and normal angle weight threshold need to be set. If the difference in spatial features between two facets is very large, the value of <italic>μ</italic> should increase; if the difference in spatial features is small but the difference in color features is large, the value of <italic>λ</italic> should increase. The pseudocode for facet region growth is presented in Algorithm 1. The segmentation results are shown in <xref ref-type="fig" rid="Figure4">Figure 4B</xref>.
</p>
      <p>
        <bold>Algorithm 1 Adaptive weighted super voxel region growth segmentation</bold>
      </p>
      <p>
        <bold>Contents</bold>
      </p>
      <p><bold>Input:</bold> Collection of super voxels F.
</p>
      <p>
        <bold>Parameters:</bold>
        <inline-formula>
          <tex-math id="M3">\begin{document}$ \sigma,{\sigma }_{1} $\end{document}</tex-math>
        </inline-formula>
      </p>
      <p><bold>Output:</bold> Segmentation results with labels
</p>
      <p><bold>1. for</bold> each unused super voxel in F <bold>do</bold></p>
      <p><bold>2.</bold> 　seed facet
<inline-formula><tex-math id="M4">\begin{document}$ {f}_{k} $\end{document}</tex-math></inline-formula> with the smallest mean curvature in F and unused
</p>
      <p><bold>3.</bold> 　create seed queue Q
</p>
      <p><bold>4.</bold> 　Q.push_back(
<inline-formula><tex-math id="M5">\begin{document}$ {f}_{k} $\end{document}</tex-math></inline-formula>), and mark
<inline-formula><tex-math id="M6">\begin{document}$ {f}_{k} $\end{document}</tex-math></inline-formula> as used
</p>
      <p><bold>5.</bold> 　 <bold>repeat</bold></p>
      <p><bold>6.</bold> 　　
<inline-formula><tex-math id="M7">\begin{document}$ {f}_{i} $\end{document}</tex-math></inline-formula>=Q.pop_front()
</p>
      <p><bold>7.</bold> 　　 <bold>for</bold> each neighboring and unused super voxel
<inline-formula><tex-math id="M8">\begin{document}$ {f}_{j} $\end{document}</tex-math></inline-formula> of
<inline-formula><tex-math id="M9">\begin{document}$ {f}_{i} $\end{document}</tex-math></inline-formula>
<bold>do</bold></p>
      <p><bold>8.</bold> 　　　 <bold>If</bold>
<inline-formula><tex-math id="M10">\begin{document}$ {D}_{f}&lt;{\sigma }_{1} $\end{document}</tex-math></inline-formula>
<bold>and</bold>
<inline-formula><tex-math id="M11">\begin{document}$ \mu &gt;\lambda $\end{document}</tex-math></inline-formula>
<bold>then</bold></p>
      <p><bold>9.</bold> 　　　　swap(
<inline-formula><tex-math id="M12">\begin{document}$ \mu $\end{document}</tex-math></inline-formula>,
<inline-formula><tex-math id="M13">\begin{document}$ \lambda $\end{document}</tex-math></inline-formula>)
</p>
      <p><bold>10.</bold> 　　　 <bold>end if</bold></p>
      <p><bold>11.</bold> 　　 <bold>If</bold>
<inline-formula><tex-math id="M14">\begin{document}$ {D}_{m}\le \sigma $\end{document}</tex-math></inline-formula>
<bold>then</bold></p>
      <p><bold>12.</bold> 　　　　Q.push_back(
<inline-formula><tex-math id="M15">\begin{document}$ {f}_{j} $\end{document}</tex-math></inline-formula>), and mark
<inline-formula><tex-math id="M16">\begin{document}$ {f}_{j} $\end{document}</tex-math></inline-formula> as used
</p>
      <p><bold>13.</bold> 　　　　Grow
<inline-formula><tex-math id="M17">\begin{document}$ {f}_{i} $\end{document}</tex-math></inline-formula> to
<inline-formula><tex-math id="M18">\begin{document}$ {f}_{j} $\end{document}</tex-math></inline-formula>
</p>
      <p><bold>14.</bold> 　　　 <bold>end if</bold></p>
      <p><bold>15.</bold> 　　 <bold>end for</bold></p>
      <p><bold>16.</bold> 　 <bold>until</bold> Q=NULL
</p>
      <p>
        <bold>17. end for</bold>
      </p>
      <p>Based on the fish body segmentation, the semantic assignment of the tail and dorsal fin is accomplished using their relative spatial coordinates. The clusters with the largest average X-coordinate and largest average Y-coordinate are recognized as the tail and dorsal fin, respectively. The cloud points between the tail and dorsal fin are analyzed, and the cluster with the smallest average Y-coordinate in the region is recognized as the anal fin. For semantic assignment of the pectoral and ventral fin, the belly of the fish is defined as the cloud points between the head and anal fin with a negative Y-coordinate. The top 100 points with the largest curvatures are then clustered into two groups using the <italic>K</italic>-means method; these clusters are assigned as the pectoral and ventral fin, according to their relative position to the head.
</p>
    </sec>
    <sec id="s02.04">
      <title>Morphological phenotype extraction</title>
      <p>Based on the semantic segmentation results, key points on the fish body are recognized according to their relative position on the fish point cloud. Thus, we obtain 18 key points on the fish point cloud (<xref ref-type="fig" rid="Figure5">Figure 5A</xref>). Here, we summarize these key points.
</p>
      <fig id="Figure5" orientation="portrait" position="float">
        <label>Figure 5</label>
        <caption>
          <p>Key point recognition and phenotype extraction</p>
        </caption>
        <abstract abstract-type="note">
          <p>A: Main 2D phenotypes determined from distances among key points estimated directly on background plane of fish point cloud. B: Main 3D phenotypes estimated from point cloud conformation of fish, including arc length, surface, and volume. C: 3D phenotypes for head. Key points recognized in point cloud include snout point (A), front point of eye (B), back point of eye (C), external point of opercular (D), starting point of pectoral fin (E), end point of pectoral fin base (F), lowest point of ventral margin (G), starting point of ventral fin (H), end point of ventral fin base (I), starting point of anal fin (J), end point of anal fin (K), lower point of caudal peduncle (L), end point of coccyx (M), end point of tail fin (N), upper point of caudal peduncle (O), end point of dorsal fin (P), starting point of dorsal fin (Q), and highest point of dorsal margin (R).</p>
        </abstract>
        <graphic xlink:href="zr-42-4-492-5"/>
      </fig>
      <p>The snout point (A) is the point with the smallest X-coordinate of the head. The front point of the eye (B) and back point of the eye (C) are the points with the smallest and largest X-coordinates, respectively. The external point of the opercular (D) is the point closest to the right-middle point of the head bounding box. The starting point of the pectoral fin (E) is the point closest to the left-top point of the pectoral fin bounding box. The end point of the pectoral fin (F) is the point with the largest X-coordinate of the pectoral fin. The lowest point of the ventral margin (G) is the point with the smallest Y-coordinate of the fish body after the fin is removed. The starting point of the ventral fin (H) is the point closest to the left-top point of the ventral fin bounding box. The end point of the ventral fin (I) is the point closest to the right-top of the ventral fin bounding box. The starting point of the anal fin (J) is the point with the smallest X-coordinate of the anal fin. The end point of the anal fin (K) is the point closest to the right-top of the anal fin bounding box. The upper (O) and lower points of the caudal peduncle (L) are the points with the largest and smallest Y-coordinates, respectively. The end point of the coccyx (M) is the point closest to the left-middle point of the tail fin bounding box. The end point of the tail fin (N) is the point with the largest X-coordinate of the tail fin. The key points of the caudal peduncle (O and L) are the two points with the smallest difference in the Y-coordinate under the same X-coordinate of the caudal peduncle. The end point of the dorsal fin (P) is the point with the largest X-coordinate of the dorsal fin. The starting point of the dorsal fin (Q) is the point with the smallest X-coordinate of the dorsal fin. The highest point of the dorsal margin (R) is the point with the largest Y-coordinate of the fish body after the fin is removed.</p>
      <p>Traditional 2D morphological phenotypes are defined and calculated as distances among these key points. <xref ref-type="fig" rid="Figure5">Figure 5A</xref> shows the full length (SV), body length (SU), dorsal snout distance (SX), head length (ST), body height (WY), anal ventral distance (HJ), caudal peduncle height (OL), caudal peduncle length (ZU), dorsal length (QP), pectoral length (EF), ventral length (HI), anal length (JK), tail length (UV), snout length (AB), eye diameter (BC), head behind eye length (CD), and dorsal tail distance (XU). In addition, 3D phenotypes from the point cloud are also calculated, such as body surface (BS), head volume (HV), body volume (BV), height of head arc (HA<sub>H</sub>), width of head arc (HA<sub>W</sub>), length of head arc (HA<sub>L</sub>), height of dorsal arc (DA<sub>H</sub>), width of dorsal arc (DA<sub>W</sub>), length of dorsal arc (DA<sub>L</sub>), height of caudal peduncle arc (CA<sub>H</sub>), width of caudal peduncle arc (CA<sub>W</sub>), and length of caudal peduncle arc (CA<sub>L</sub>) (<xref ref-type="fig" rid="Figure5">Figure 5B</xref>). The Poisson surface reconstruction method and VTK library are used to construct the closed triangular mesh model and calculate the surface area and volume, respectively.
</p>
    </sec>
    <sec id="s02.05">
      <title>Phenotype management</title>
      <p>After calculating the phenotypes, the user can click the “phenotype management” icon in the toolbar to call up the phenotype management dialog box. This module stores the phenotype computing time, file path of the segmented point cloud, typical key points, and phenotypic information. Key point coordinates and morphological phenotypes can be downloaded as a CSV file for subsequent analysis and data sharing. (Supplementary Figure S2)</p>
    </sec>
    <sec id="s02.06">
      <title>Main interface development</title>
      <p>3DPhenoFish has an intuitive user interface embedded with the aforementioned analytical functions and enables the user to visualize the point clouds, key points, and phenotypes. The interface was written in Visual C++ in Microsoft Visual Studio 2015 and runs under Microsoft Windows 10. The core algorithm was based on the open-source Point Cloud Library (PCL) (<xref rid="bRusu2011" ref-type="bibr">Rusu &amp; Cousins, 2011</xref>) and Open-Source Computer Vision Library (OpenCV) (<xref rid="bBradski2008" ref-type="bibr">Bradski &amp; Kaehler, 2008</xref>). The graphical user interface (GUI) was created with Qt 5.12 framework (The Qt Company Ltd.). Functions of 3D visualization and view rendering were based on the open-source Visualization Toolkit (VTK) (<xref rid="bSchroeder2000" ref-type="bibr">Schroeder et al., 2000</xref>).
</p>
    </sec>
    <sec id="s02.07">
      <title>Key point and phenotype customization</title>
      <p>Although system-default key points and phenotypes are automatically recognized and extracted in 3DPhenoFish, we added functions to customize key points and phenotypes for greater flexibility. For key points, a user can adjust their precise location and even add new key points by simple mouse operations. The user can also double-click on the key point name in the key point list (<xref ref-type="fig" rid="Figure6">Figure 6</xref>; Supplementary Figure S3); the corresponding key points in the visualization window are then highlighted in green. The position of a key point can be moved by mouse while holding down “shift” on the keyboard. Modified points are colored blue. 3DPhenoFish also allows the user to customize a phenotype definition by the “custom phenotype” function listed in the toolbar (<xref ref-type="fig" rid="Figure6">Figure 6</xref>; Supplementary Figure S3). New phenotypes can be defined by assigning the system-default and user-defined key point name in the combo box. Length between two points, angle between three points, and area among three points can be defined as new phenotypes in 3DPheoFish.
</p>
      <fig id="Figure6" orientation="portrait" position="float">
        <label>Figure 6</label>
        <caption>
          <p>Main interface of 3DPhenoFish</p>
        </caption>
        <abstract abstract-type="note">
          <p>A: Main point cloud image viewer. B: List of point clouds that need to be processed. C: Properties of current point cloud. D: List of key points for fish point cloud. E: List of morphological phenotypes for fish point cloud. F: List of operation records. G: Toolbar used to open and save files, adjust visual interface of point cloud, and automatically segment fish point cloud. Morphological phenotype extraction in the software must be executed strictly by down-sampling (Ⅰ), background removal (Ⅱ), and key point recognition (Ⅲ).</p>
        </abstract>
        <graphic xlink:href="zr-42-4-492-6"/>
      </fig>
    </sec>
    <sec id="s02.08">
      <title>3DPhenoFish validation</title>
      <p>To validate the accuracy and feasibility of 3DPhenoFish, we compared phenotype automatically extracted from the software to manual measurements for identical samples. To this end, 119 fish from four major genera in Schizothoracinae, i.e., 59 <italic>Schizopygopsis younghusbandi</italic>, 11 <italic>Oxygymnocypris stewartii</italic>, 19 <italic>Ptychobarbus dipogon</italic>, and 30 <italic>Schizothorax oconnori</italic>, were collected from the Lhasa and Yarlung Zangbo rivers in Tibet. The 3D point clouds for these samples were obtained using GScan and the above 2D and 3D phenotypes were extracted using 3DPhenoFish. In addition, the 2D phenotypes were also manually measured by traditional methods. Correlation analysis for the 3DPhenoFish and manual measurements was performed to evaluate the accuracy of automatic phenotype extraction. The 2D and 3D phenotypes were used to discriminate samples in terms of species and populations using linear discriminant analysis (LDA) (<xref rid="bBalakrishnama1998" ref-type="bibr">Balakrishnama &amp; Ganapathiraju, 1998</xref>) in Python (<xref rid="bOliphant2007" ref-type="bibr">Oliphant, 2007</xref>). The significance of phenotype differences was tested using student <italic>t</italic>-test in the R package.
</p>
    </sec>
  </sec>
  <sec id="s03">
    <title>RESULTS</title>
    <p>According to the flow chart in <xref ref-type="fig" rid="Figure1">Figure 1</xref>, we collected point clouds and extracted morphological phenotypes for 119 fish from four major genera in Schizothoracinae. The 3D point clouds for the same fish samples were obtained using GScan. As a result, the data obtained for each fish consisted of about 0.7 million vertices and 1.4 million faces, which were used to verify the feasibility of 3D data model construction and automatic extraction of morphological phenotypes.
</p>
    <p>First, 30 fish samples were randomly selected to evaluate the accuracy of the extracted phenotypes. Eighteen key points were automatically recognized for all samples, and 2D morphological phenotypes were extracted and compared to traditional manual measurements (Supplementary Table S1). The Pearson correlation coefficient (<italic>r</italic>) between the calculated and measured phenotypes was higher than 0.94 (<xref ref-type="fig" rid="Figure7">Figure 7</xref>; Supplementary Table S1), thus verifying the reliability of the extracted morphological phenotypes by 3DPhenoFish.
</p>
    <fig id="Figure7" orientation="portrait" position="float">
      <label>Figure 7</label>
      <caption>
        <p>Linear correlation analysis of morphological phenotypes from 3DPhenoFish and manual measurement</p>
      </caption>
      <abstract abstract-type="note">
        <p>2D phenotypes were extracted from 3DPhenoFish and manual measurements were collected and compared for 30 randomly selected fish samples. Correlation coefficients of 17 morphological phenotypes were calculated (Supplementary Table<bold/>S1), including full length (A), body length (B), dorsal snout distance (C), body height (D), caudal peduncle height (E), and head length (F).
</p>
      </abstract>
      <graphic xlink:href="zr-42-4-492-7"/>
    </fig>
    <p>Using the 19 default morphological phenotypes extracted by 3DPhenoFish, we then performed fish sample clustering analysis. As shown in <xref ref-type="fig" rid="Figure8">Figure 8A</xref>, the <italic>O. stewartii</italic> and <italic>S. oconnori</italic> samples were obviously separated; however, the <italic>S. younghusbandi</italic> samples exhibited significant overlap with the <italic>P. dipogon</italic> samples. To improve the resolution of species separation, we added 3D phenotypes for sample clustering, which resulted in more obvious discrimination between samples from different species (<xref ref-type="fig" rid="Figure8">Figure 8B</xref>). We also observed that morphological traits related to the head and dorsal fin exhibited significant differences among species. The head height and head length ratios of <italic>O. stewartii</italic> and <italic>P. dipogon</italic> were significantly smaller than those of <italic>S. younghusbandi</italic> and <italic>S. oconnori</italic> (<italic>P</italic>&lt;2e-16 based on analysis of variance (ANOVA),<xref ref-type="fig" rid="Figure9">Figure 9A</xref>), thereby indicating that the head is relatively narrow for <italic>O. stewartii</italic> and <italic>P. dipogon</italic>. In addition, the dorsal snout distance and body length ratio for <italic>O. stewartii</italic> and <italic>S. oconnori</italic> was significantly larger (<italic>P</italic>&lt;2e-16 for ANOVA,<xref ref-type="fig" rid="Figure9">Figure 9B</xref>), indicating that the position of the dorsal fin in these two species was significantly different from that of <italic>S. younghusbandi</italic> and <italic>P. dipogon</italic>. Interestingly, we found that 2D and 3D phenotypes extracted from 3DPhenoFish could also discriminate <italic>S. younghusbandi</italic> samples from populations in the Lhasa River, Yarlung Zangbo River Saga section, and Yarlung Zangbo River Zhongba section (<xref ref-type="fig" rid="Figure8">Figure 8C,D</xref>). Remarkably, several phenotypes, e.g., dorsal arc width and caudal arc width ratio (<italic>P</italic>=3.6e-11 for ANOVA, <xref ref-type="fig" rid="Figure9">Figure 9C</xref>) and head volume and head length ratio (<italic>P</italic>=0.0026 for ANOVA, <xref ref-type="fig" rid="Figure9">Figure 9D</xref>), exhibited significant differences among populations. For instance, the dorsal and caudal arc width ratio was higher for <italic>S. younghusbandi</italic> samples from the Yarlung Zangbo River than from the Lhasa River, indicating that <italic>S. younghusbandi</italic> from the Lhasa River is slimer. Whether the phenotype differentiation is related to adaptation to the local environment requires further investigation.
</p>
    <fig id="Figure8" orientation="portrait" position="float">
      <label>Figure 8</label>
      <caption>
        <p>Phenotype-based clustering of sample classifications of species and populations using linear discriminant analysis</p>
      </caption>
      <abstract abstract-type="note">
        <p>Samples from <italic>Schizopygopsis younghusbandi</italic>, <italic>Oxygymnocypris stewartii</italic>, <italic>Ptychobarbus dipogon</italic>, and <italic>Schizothorax oconnori</italic> were used for analysis. Clustering of samples using traditional 2D morphological phenotypes (A) and 2D and 3D morphological phenotypes (B). Clustering of <italic>S. younghusbandi</italic> samples using traditional 2D morphological phenotypes (C) and 2D and 3D morphological phenotypes (D).
</p>
      </abstract>
      <graphic xlink:href="zr-42-4-492-8"/>
    </fig>
    <fig id="Figure9" orientation="portrait" position="float">
      <label>Figure 9</label>
      <caption>
        <p>Morphological phenotypes exhibited significant differences among species and <italic>Schizopygopsis younghusbandi</italic> populations
</p>
      </caption>
      <abstract abstract-type="note">
        <p>Distribution of head height/head length (A) and dorsal snout distance/body length (B) for Schizothoracinae species and dorsal arc width/caudal arc width (C) and head volume/head length (D) for <italic>S. younghusbandi</italic> populations. Significant differences are shown by labels above bars, samples sharing no label letter indicate significant difference between two groups (<italic>P</italic>≤0.05).
</p>
      </abstract>
      <graphic xlink:href="zr-42-4-492-9"/>
    </fig>
  </sec>
  <sec id="s04">
    <title>DISCUSSION</title>
    <p>In this work, we developed a practical application, 3DPhenoFish, to extract fish morphological phenotypes from point cloud data. The features of 3DPhenoFish are summarized as follows:</p>
    <p><bold>1. Easy to use:</bold> The structured pipeline for point cloud data analysis was developed and embedded in the 3DPhenoFish interface, thereby lowering the barrier to morphological phenotyping for users without programing skills or biological backgrounds.
</p>
    <p><bold>2.Objective:</bold> The automatic phenotyping ensures objective results and prevents inconsistency in phenotype definitions across users.
</p>
    <p><bold>3.Efficient:</bold> Whole scanning and phenotyping for a Schizothoracinae fish usually takes 40–50 s, which is significantly faster than the 5–6 min required for traditional manual measurements. This efficient phenotyping also alleviates stress on the fish.
</p>
    <p><bold>4.Accurate:</bold> Comparison of 3DPhenoFish and manual measurements confirmed the accuracy of the automatic phenotyping result.
</p>
    <p><bold>5.Customizable:</bold> The automatically recognized key points can be adjusted, and new key points can be added. Customized phenotypes from the relative position of existing key points can also be defined.
</p>
    <p><bold>6.Accessible:</bold> The phenotype and sample data are accessible by the phenotype management module, which can be downloaded and shared for the further analysis.
</p>
    <p>Here, 3DPhenoFish application for Tibetan Schizothoracinae fish species exhibited good performance for phenotype extraction. We showed that 2D and 3D phenotypes could be obtained in a throughput manner and could be used for species and population discrimination in wild fish resource management. The 3D phenotypes provided data for sample discrimination, and morphological traits related to the head and dorsal fin exhibited significant differences among species. However, 3DPhenoFish is still under development, and there remain several limitations in its application. Firstly, 3DPhenoFish only supports OBJ files and PCD files containing full-color information. In addition, point cloud scanning requires fish to keep still on the plane background, and therefore, anesthetization may be necessary for stress-sensitive fish species. Finally, 3DPhenoFish provides a general framework for fish species; therefore, it is necessary to validate image segmentation and phenotyping results before large-scale application to other species.</p>
  </sec>
  <sec id="s05">
    <title>CONCLUSIONS</title>
    <p>In this paper, we proposed a novel strategy to extract morphological 2D and 3D phenotypes from 3D point cloud data. To achieve intelligent phenotyping, algorithms for point cloud pre-processing, semantic segmentation, key point recognition, morphological phenotype extraction were developed. As an easy-to-use visual tool for fish phenotype analysis, 3DPhenoFish was developed by embedding those functions in a user-friendly interface. Basic phenotype extraction in 3DPhenoFish requires no user knowledge of programming or biology. Advanced functions for fine-tuning key points and defining new phenotypes are available for expert users.</p>
    <p>We employed the proposed technique for phenotype analysis of Tibetan endemic fish species: i.e., <italic>S. younghusbandi</italic>, <italic>O. stewartii</italic>, <italic>P. dipogon</italic>, and <italic>S. oconnori</italic>. Comparing our results to manual measurements, 3DPhenoFish exhibited high accuracy for traditional morphological phenotypes. The phenotypes were used in clustering analysis of the fish samples. We demonstrated that the 2D and 3D phenotypes enabled good discrimination among species and even samples for the same species but from different populations.
</p>
    <p>3DPhenoFish presents an efficient and accurate technique to obtain 2D and 3D phenotypes for fish species. The point cloud scanning is easy and fast, reducing the stress on fish during operation. More importantly, the strategy requires no knowledge of programing or biology; therefore, it could be applied in large-scale fish phenotyping surveys in aquaculture and conservation studies.</p>
  </sec>
  <sec id="s06">
    <label/>
    <title>SUPPLEMENTARY DATA</title>
    <supplementary-material content-type="local-data" id="S1">
      <caption>
        <p>Supplementary data to this article can be found online.</p>
      </caption>
      <media xlink:href="zr-42-4-492-S1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <sec id="s07">
    <title>COMPETING INTERESTS</title>
    <p content-type="COI-statement">The authors declare that they have no competing interests.</p>
  </sec>
  <sec id="s08">
    <title>AUTHORS’ CONTRIBUTIONS</title>
    <p>H.P.L., X.H.Y., and S.J.X. designed the research. Y.H.L., C.W.Z., Z.B.M., F.L., D.D.F., Y.Z., and J.S. collected samples and performed point cloud scanning. Y.H.L., D.Y.L., J.Y.J., S.J.X., W.Z.L., and C.N.L. developed the application. Y.H.L. and C.W.Z. performed manual phenotyping and comparison. Y.H.L., S.J.X, H.P.L., and X.H.Y. wrote the manuscript. All authors read and approved the final version of the manuscript.</p>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="bAliyu2017">
      <label>1</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">Aliyu I, Gana KJ, Musa AA, Agajo J, Orire AM, Abiodun FT, et al</person-group>
        <article-title>A proposed fish counting algorithm using digital image processing technique</article-title>
        <source>ATBU, Journal of Science, Technology &amp; Education</source>
        <year>2017</year>
        <volume>5</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>11</lpage>
      </element-citation>
    </ref>
    <ref id="bBalakrishnama1998">
      <label>2</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">Balakrishnama S, Ganapathiraju A</person-group>
        <article-title>Linear discriminant analysis-a brief tutorial</article-title>
        <source>Institute for Signal and information Processing</source>
        <year>1998</year>
        <volume>18</volume>
        <issue>1998</issue>
        <fpage>1</fpage>
        <lpage>8</lpage>
      </element-citation>
    </ref>
    <ref id="bBalta2018">
      <label>3</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">Balta H, Velagic J, Bosschaerts W, De Cubber G, Siciliano B</person-group>
        <article-title>Fast statistical outlier removal based method for large 3D point clouds of outdoor environments</article-title>
        <source>IFAC-PapersOnLine</source>
        <year>2018</year>
        <volume>51</volume>
        <issue>22</issue>
        <fpage>348</fpage>
        <lpage>353</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ifacol.2018.11.566</pub-id>
      </element-citation>
    </ref>
    <ref id="d31e1061">
      <label>4</label>
      <note>
        <p>Bär T, Reuter JF, Zöllner JM. 2012. Driver head pose and gaze estimation based on multi-template ICP 3-D point cloud alignment. <italic>In</italic>: Proceedings of 2012 15th International IEEE Conference on Intelligent Transportation Systems. Anchorage: IEEE, 1797–1802.
</p>
      </note>
    </ref>
    <ref id="bBatanov2019">
      <label>5</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">Batanov S D, Starostina O S, Baranova I A</person-group>
        <article-title>Non-contact methods of cattle conformation assessment using mobile measuring systems</article-title>
        <source>IOP Conference Series: Earth and Environmental Science</source>
        <year>2019</year>
        <volume>315</volume>
        <issue>3</issue>
        <fpage>032006</fpage>
      </element-citation>
    </ref>
    <ref id="bBesl1992">
      <label>6</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">Besl PJ, McKay ND</person-group>
        <article-title>A method for registration of 3-D shapes</article-title>
        <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>
        <year>1992</year>
        <volume>14</volume>
        <issue>2</issue>
        <fpage>239</fpage>
        <lpage>256</lpage>
        <pub-id pub-id-type="doi">10.1109/34.121791</pub-id>
      </element-citation>
    </ref>
    <ref id="bBradski2008">
      <label>7</label>
      <note>
        <p>Bradski G, Kaehler A. 2008. Learning OpenCV: Computer Vision with the OpenCV library. O'Reilly Media, Inc.</p>
      </note>
    </ref>
    <ref id="bChen2017">
      <label>8</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">Chen XW, Zhu YL, Wu T, Wang ZQ</person-group>
        <article-title>The point cloud registration technology based on SAC-IA and improved ICP</article-title>
        <source>J Xi’an Polytech Univ</source>
        <year>2017</year>
        <volume>31</volume>
        <issue>3</issue>
        <fpage>395</fpage>
        <lpage>401</lpage>
      </element-citation>
    </ref>
    <ref id="bComba2018">
      <label>9</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">Comba L, Biglia A, Aimonino DR, Gay P</person-group>
        <article-title>Unsupervised detection of vineyards by 3D point-cloud UAV photogrammetry for precision agriculture</article-title>
        <source>Computers and Electronics in Agriculture</source>
        <year>2018</year>
        <volume>155</volume>
        <fpage>84</fpage>
        <lpage>95</lpage>
        <pub-id pub-id-type="doi">10.1016/j.compag.2018.10.005</pub-id>
      </element-citation>
    </ref>
    <ref id="bFernandes2020">
      <label>10</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">Fernandes AFA, Turra EM, de Alvarenga ÉR, Passafaro TL, Lopes FB, Alves GFO, et al</person-group>
        <article-title>Deep Learning image segmentation for extraction of fish body measurements and prediction of body weight and carcass traits in Nile tilapia</article-title>
        <source>Computers and Electronics in Agriculture</source>
        <year>2020</year>
        <volume>170</volume>
        <fpage>105274</fpage>
        <pub-id pub-id-type="doi">10.1016/j.compag.2020.105274</pub-id>
      </element-citation>
    </ref>
    <ref id="bGongal2018">
      <label>11</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">Gongal A, Karkee M, Amatya S</person-group>
        <article-title>Apple fruit size estimation using a 3D machine vision system</article-title>
        <source>Information Processing in Agriculture</source>
        <year>2018</year>
        <volume>5</volume>
        <issue>4</issue>
        <fpage>498</fpage>
        <lpage>503</lpage>
        <pub-id pub-id-type="doi">10.1016/j.inpa.2018.06.002</pub-id>
      </element-citation>
    </ref>
    <ref id="bHao2015">
      <label>12</label>
      <note>
        <p>Hao M M, Yu H L, Li D L. 2015. The measurement of fish size by machine vision-a review. <italic>In</italic>: Proceedings of 9th IFIP WG 5.14 International Conference on Computer and Computing Technologies in Agriculture. Cham: Springer, 15–32.
</p>
      </note>
    </ref>
    <ref id="bHartmann2011">
      <label>13</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">Hartmann A, Czauderna T, Hoffmann R, Stein N, Schreiber F</person-group>
        <article-title>HTPheno: an image analysis pipeline for high-throughput plant phenotyping</article-title>
        <source>BMC Bioinformatics</source>
        <year>2011</year>
        <volume>12</volume>
        <issue>1</issue>
        <fpage>148</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-12-148</pub-id>
        <pub-id pub-id-type="pmid">21569390</pub-id>
      </element-citation>
    </ref>
    <ref id="bHuang2018">
      <label>14</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">Huang LW, Li SQ, Zhu AQ, Fan XY, Zhang CY, Wang HY</person-group>
        <article-title>Non-contact body measurement for qinchuan cattle with LiDAR sensor</article-title>
        <source>Sensors</source>
        <year>2018</year>
        <volume>18</volume>
        <issue>9</issue>
        <fpage>3014</fpage>
        <pub-id pub-id-type="doi">10.3390/s18093014</pub-id>
      </element-citation>
    </ref>
    <ref id="bLe2019">
      <label>15</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">Le Cozler Y, Allain C, Caillot A, Delouard JM, Delattre L, Luginbuhl T, et al</person-group>
        <article-title>High-precision scanning system for complete 3D cow body shape imaging and analysis of morphological traits</article-title>
        <source>Computers and Electronics in Agriculture</source>
        <year>2019</year>
        <volume>157</volume>
        <fpage>447</fpage>
        <lpage>453</lpage>
        <pub-id pub-id-type="doi">10.1016/j.compag.2019.01.019</pub-id>
      </element-citation>
    </ref>
    <ref id="bLi2018">
      <label>16</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">Li ML, Sun CM</person-group>
        <article-title>Refinement of LiDAR point clouds using a super voxel based approach</article-title>
        <source>ISPRS Journal of Photogrammetry and Remote Sensing</source>
        <year>2018</year>
        <volume>143</volume>
        <fpage>213</fpage>
        <lpage>221</lpage>
        <pub-id pub-id-type="doi">10.1016/j.isprsjprs.2018.03.010</pub-id>
      </element-citation>
    </ref>
    <ref id="bLin2019">
      <label>17</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">Lin GC, Tang YC, Zou XJ, Xiong JT, Li JH</person-group>
        <article-title>Guava detection and pose estimation using a low-cost RGB-D sensor in the field</article-title>
        <source>Sensors</source>
        <year>2019</year>
        <volume>19</volume>
        <issue>2</issue>
        <fpage>428</fpage>
        <pub-id pub-id-type="doi">10.3390/s19020428</pub-id>
      </element-citation>
    </ref>
    <ref id="d31e1223">
      <label>18</label>
      <note>
        <p>López-Fanjul C, Toro M Á. 2007. Fundamentos de la Mejora Genética en Acuicultura. Madrid: Genética y Genómica en Acuicultura, 155–182.</p>
      </note>
    </ref>
    <ref id="bNavarro2016">
      <label>19</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">Navarro A, Lee-Montero I, Santana D, Henríquez P, Ferrer MA, Morales A, et al</person-group>
        <article-title><italic>IMAFISH_ML</italic>: a fully-automated image analysis software for assessing fish morphometric traits on gilthead seabream (<italic>Sparus aurata</italic> L.), meagre (<italic>Argyrosomus regius</italic>) and red porgy (<italic>Pagrus pagrus</italic>)
</article-title>
        <source>Computers and Electronics in Agriculture</source>
        <year>2016</year>
        <volume>121</volume>
        <fpage>66</fpage>
        <lpage>73</lpage>
        <pub-id pub-id-type="doi">10.1016/j.compag.2015.11.015</pub-id>
      </element-citation>
    </ref>
    <ref id="bOliphant2007">
      <label>20</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">Oliphant TE</person-group>
        <article-title>Python for scientific computing</article-title>
        <source>Computing in Science &amp; Engineering</source>
        <year>2007</year>
        <volume>9</volume>
        <issue>3</issue>
        <fpage>10</fpage>
        <lpage>20</lpage>
      </element-citation>
    </ref>
    <ref id="bOrts-Escolano2013">
      <label>21</label>
      <note>
        <p>Orts-Escolano S, Morell V, García-Rodríguez J, Cazorla M. 2013. Point cloud data filtering and downsampling using growing neural gas. <italic>In</italic>: Proceedings of the 2013 International Joint Conference on Neural Networks. Dallas: IEEE, 1–8.
</p>
      </note>
    </ref>
    <ref id="d31e1273">
      <label>22</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">Pérez-Ruiz M, Tarrat-Martín D, Sánchez-Guerrero MJ, Valera M</person-group>
        <article-title>Advances in horse morphometric measurements using LiDAR</article-title>
        <source>Computers and Electronics in Agriculture</source>
        <year>2020</year>
        <volume>174</volume>
        <fpage>105510</fpage>
        <pub-id pub-id-type="doi">10.1016/j.compag.2020.105510</pub-id>
      </element-citation>
    </ref>
    <ref id="bPezzuolo2018">
      <label>23</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">Pezzuolo A, Guarino M, Sartori L, Marinello F</person-group>
        <article-title>A feasibility study on the use of a structured light depth-camera for three-dimensional body measurements of dairy cows in free-stall barns</article-title>
        <source>Sensors</source>
        <year>2018</year>
        <volume>18</volume>
        <issue>2</issue>
        <fpage>673</fpage>
      </element-citation>
    </ref>
    <ref id="bRusu2011">
      <label>24</label>
      <note>
        <p>Rusu RB, Cousins S. 2011. 3D is here: point cloud library (PCL). <italic>In</italic>: Proceedings of 2011 IEEE International Conference on Robotics and Automation. Shanghai: IEEE, 1–4.
</p>
      </note>
    </ref>
    <ref id="bSchnabel2007">
      <label>25</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">Schnabel R, Wahl R, Klein R</person-group>
        <article-title>Efficient RANSAC for point‐cloud shape detection</article-title>
        <source>Computer Graphics Forum</source>
        <year>2007</year>
        <volume>26</volume>
        <issue>2</issue>
        <fpage>214</fpage>
        <lpage>226</lpage>
        <pub-id pub-id-type="doi">10.1111/j.1467-8659.2007.01016.x</pub-id>
      </element-citation>
    </ref>
    <ref id="bSchroeder2000">
      <label>26</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">Schroeder WJ, Avila LS, Hoffman W</person-group>
        <article-title>Visualizing with VTK: a tutorial</article-title>
        <source>IEEE Computer Graphics and Applications</source>
        <year>2000</year>
        <volume>20</volume>
        <issue>5</issue>
        <fpage>20</fpage>
        <lpage>27</lpage>
        <pub-id pub-id-type="doi">10.1109/38.865875</pub-id>
      </element-citation>
    </ref>
    <ref id="bShah2019">
      <label>27</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">Shah SZH, Rauf HT, IkramUllah M, Khalid MS, Farooq M, Fatima M, et al</person-group>
        <article-title>Fish-pak: fish species dataset from pakistan for visual features based classification</article-title>
        <source>Data in Brief</source>
        <year>2019</year>
        <volume>27</volume>
        <fpage>104565</fpage>
        <pub-id pub-id-type="doi">10.1016/j.dib.2019.104565</pub-id>
        <pub-id pub-id-type="pmid">31656834</pub-id>
      </element-citation>
    </ref>
    <ref id="bSpampinato2008">
      <label>28</label>
      <note>
        <p>Spampinato C, Chen-Burger YH, Nadarajan G, Fisher RB. 2008. Detecting, tracking and counting fish in low quality unconstrained underwater videos. <italic>In</italic>: Proceedings of the 3rd International Conference on Computer Vision Theory and Applications. Madeira: DBLP, 514–519.
</p>
      </note>
    </ref>
    <ref id="bSpampinato2010">
      <label>29</label>
      <note>
        <p>Spampinato C, Giordano D, Di Salvo R, Chen-Burger YHJ, Fisher RB, Nadarajan G. 2010. Automatic fish classification for underwater species behavior understanding. <italic>In</italic>: Proceedings of the 1st ACM International Workshop on Analysis and Retrieval of Tracked Events and Motion in Imagery Streams. Firenze: ACM, 45–50.
</p>
      </note>
    </ref>
    <ref id="bVo2015">
      <label>30</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">Vo AV, Truong-Hong L, Laefer DF, Bertolotto M</person-group>
        <article-title>Octree-based region growing for point cloud segmentation</article-title>
        <source>ISPRS Journal of Photogrammetry and Remote Sensing</source>
        <year>2015</year>
        <volume>104</volume>
        <fpage>88</fpage>
        <lpage>100</lpage>
        <pub-id pub-id-type="doi">10.1016/j.isprsjprs.2015.01.011</pub-id>
      </element-citation>
    </ref>
    <ref id="bWang2019">
      <label>31</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">Wang G A, Hwang JN, Wallace F, Rose C</person-group>
        <article-title>Multi-scale fish segmentation refinement and missing shape recovery</article-title>
        <source>IEEE Access</source>
        <year>2019</year>
        <volume>7</volume>
        <fpage>52836</fpage>
        <lpage>52845</lpage>
        <pub-id pub-id-type="doi">10.1109/ACCESS.2019.2912612</pub-id>
      </element-citation>
    </ref>
    <ref id="bWang2017">
      <label>32</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">Wang ZL, Walsh KB, Verma B</person-group>
        <article-title>On-tree mango fruit size estimation using RGB-D images</article-title>
        <source>Sensors</source>
        <year>2017</year>
        <volume>17</volume>
        <issue>12</issue>
        <fpage>2738</fpage>
      </element-citation>
    </ref>
    <ref id="bWu2016">
      <label>33</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">Wu YX, Li F, Liu FF, Cheng LN, Guo LL</person-group>
        <article-title>Point cloud segmentation using Euclidean cluster extraction algorithm with the Smoothness</article-title>
        <source>Meas Control Technol</source>
        <year>2016</year>
        <volume>35</volume>
        <issue>3</issue>
        <fpage>36</fpage>
        <lpage>38</lpage>
      </element-citation>
    </ref>
    <ref id="bZermas2020">
      <label>34</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">Zermas D, Morellas V, Mulla D, Papanikolopoulos N</person-group>
        <article-title>3D model processing for high throughput phenotype extraction–the case of corn</article-title>
        <source>Computers and Electronics in Agriculture</source>
        <year>2020</year>
        <volume>172</volume>
        <fpage>105047</fpage>
        <pub-id pub-id-type="doi">10.1016/j.compag.2019.105047</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
