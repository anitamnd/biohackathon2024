<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sci Rep</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id>
    <journal-title-group>
      <journal-title>Scientific Reports</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2045-2322</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6778075</article-id>
    <article-id pub-id-type="pmid">31586139</article-id>
    <article-id pub-id-type="publisher-id">50587</article-id>
    <article-id pub-id-type="doi">10.1038/s41598-019-50587-1</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>ARA: accurate, reliable and active histopathological image classification framework with Bayesian deep learning</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Rączkowska</surname>
          <given-names>Alicja</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Możejko</surname>
          <given-names>Marcin</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zambonelli</surname>
          <given-names>Joanna</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1320-6695</contrib-id>
        <name>
          <surname>Szczurek</surname>
          <given-names>Ewa</given-names>
        </name>
        <address>
          <email>szczurek@mimuw.edu.pl</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1937 1290</institution-id><institution-id institution-id-type="GRID">grid.12847.38</institution-id><institution-id institution-id-type="ROR">https://ror.org/039bjqg32</institution-id><institution>Faculty of Mathematics, Informatics and Mechanics, </institution><institution>University of Warsaw, </institution></institution-wrap>Warsaw, Poland </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 1328 7408</institution-id><institution-id institution-id-type="GRID">grid.13339.3b</institution-id><institution-id institution-id-type="ROR">https://ror.org/04p2y4s44</institution-id><institution>Department of Pathology, </institution><institution>Medical University of Warsaw, </institution></institution-wrap>Warsaw, Poland </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>4</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>4</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>9</volume>
    <elocation-id>14347</elocation-id>
    <history>
      <date date-type="received">
        <day>23</day>
        <month>4</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>16</day>
        <month>9</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2019</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Machine learning algorithms hold the promise to effectively automate the analysis of histopathological images that are routinely generated in clinical practice. Any machine learning method used in the clinical diagnostic process has to be extremely accurate and, ideally, provide a measure of uncertainty for its predictions. Such accurate and reliable classifiers need enough labelled data for training, which requires time-consuming and costly manual annotation by pathologists. Thus, it is critical to minimise the amount of data needed to reach the desired accuracy by maximising the efficiency of training. We propose an accurate, reliable and active (ARA) image classification framework and introduce a new Bayesian Convolutional Neural Network (ARA-CNN) for classifying histopathological images of colorectal cancer. The model achieves exceptional classification accuracy, outperforming other models trained on the same dataset. The network outputs an uncertainty measurement for each tested image. We show that uncertainty measures can be used to detect mislabelled training samples and can be employed in an efficient active learning workflow. Using a variational dropout-based entropy measure of uncertainty in the workflow speeds up the learning process by roughly 45%. Finally, we utilise our model to segment whole-slide images of colorectal tissue and compute segmentation-based spatial statistics.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Cancer imaging</kwd>
      <kwd>Computational science</kwd>
      <kwd>Statistics</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1" sec-type="introduction">
    <title>Introduction</title>
    <p id="Par2">Histopathological images of cancer tissue samples are routinely inspected by pathologists for cancer type identification and prognosis. Hematoxylin-Eosin (H&amp;E) stained slides have been used by pathologists for over a hundred years. With such long history and proven applicability, histopathological imaging is expected to stay in common clinical practice in the coming years<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. With the advent of digital pathology, histopathological images became available for automated analysis at scale<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. To this end, a rich catalogue of machine learning approaches to image classification and whole-slide segmentation has been developed<sup><xref ref-type="bibr" rid="CR3">3</xref>,<xref ref-type="bibr" rid="CR4">4</xref></sup>, promising to aid the effort of pathologists in interpreting the images<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. Such machine learning models need to be perfectly <italic>accurate</italic>, as classification errors may result in faulty disease diagnosis and patient treatment. On top of that, we stipulate that in application to digital pathology, the models should also be <italic>reliable</italic> in their predictions. When performing the difficult task of automated classification or diagnosis based on histopathological images, they should state uncertainty in their predictions, indicating difficult cases for which human expert inspection is necessary. While accuracy is optimised by every machine learning method, reliability is another desired feature that is not delivered by many state of the art solutions.</p>
    <p id="Par3">Recent years brought particularly intensive development of deep-learning based approaches to image classification. In particular, Convolutional Neural Networks (CNNs) have served as a backbone for numerous breakthroughs in computer vision as a whole, specifically in image classification. Since 2012, when the groundbreaking AlexNet was created by Alex Krizevsky<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>, the state of the art has rapidly shifted from machine learning algorithms using manual feature engineering (henceforth referred to as ‘traditional’ machine learning approaches) to new deep learning ones<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. Medical imaging in general<sup><xref ref-type="bibr" rid="CR8">8</xref>–<xref ref-type="bibr" rid="CR12">12</xref></sup>, and histopathological image classification in particular<sup><xref ref-type="bibr" rid="CR13">13</xref>–<xref ref-type="bibr" rid="CR20">20</xref></sup>, became important applications of these methods. Multiple machine learning methods go beyond the tasks of tissue type classification and whole-slide segmentation, confirming there is more information about the patients encrypted in histopathological images than immediately visible by eye<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. For example, <italic>Wang et al</italic>.<sup><xref ref-type="bibr" rid="CR21">21</xref></sup> trained a CNN to predict survival of patients from their pathological images. Another deep learning model<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> allowed prediction of mutations in several genes from non–small cell lung cancer histopathology. Finally, Bayesian measures of measuring uncertainty for deep-learning methods have been proposed<sup><xref ref-type="bibr" rid="CR23">23</xref>,<xref ref-type="bibr" rid="CR24">24</xref></sup> and successfully applied to medical image analysis<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. Those developments open the avenue to reliable image classification, where prediction uncertainty can be reported together with the predicted class.</p>
    <p id="Par4">All of these exciting methodological inventions would not be possible without training data that is numerous enough to train accurate models. Publicly available training data with annotated images such as the Breast Cancer Histopathological Database<sup><xref ref-type="bibr" rid="CR26">26</xref></sup> allow algorithm benchmarking and evaluation, sparkling new method developments<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR28">28</xref></sup>. Similarly, <italic>Kather et al</italic>.<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> released a colorectal cancer dataset with H&amp;E tissue slides, which were cut into 5000 small tiles (or patches), each of them annotated with one of eight tissue classes. They also devised an efficient classification method, with image-derived features serving as basis for a support vector machine model. Since its publication, this dataset was utilised several times to verify the performance of an array of methods. <italic>Ribeiro et al</italic>.<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> developed a traditional method that uses multidimensional fractal techniques, curvelet transforms and Haralick descriptors. They tested its accuracy using the <italic>Kather et al</italic>. dataset in a binary classification scenario. <italic>Wang et al</italic>.<sup><xref ref-type="bibr" rid="CR31">31</xref></sup> developed a Bilinear CNN architecture that takes as input H&amp;E stained images decomposed into H and E channels and used all eight classes from the <italic>Kather et al</italic>. dataset to verify its performance. <italic>Pham</italic><sup><xref ref-type="bibr" rid="CR32">32</xref></sup> utilised this dataset to assess their autoencoder architecture. <italic>Sarkar et al</italic>.<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> created a new saliency-based dictionary learning method and used the <italic>Kather et al</italic>. dataset for both training and testing. Finally, <italic>Ciompi et al</italic>.<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> used it as an independent test set for an evaluation of two stain normalisation strategies. All traditional methods reported accuracy lower than the original classification method by <italic>Kather et al</italic>. The AUC value in the eight-class classification task obtained by <italic>Wang et al</italic>.<sup><xref ref-type="bibr" rid="CR31">31</xref></sup> was higher than the one achieved by <italic>Kather et al</italic>., confirming that a CNN is the method of choice for this dataset as well. Notably, none of these methods aimed for reliability, as they did not assess the uncertainty of their predictions.</p>
    <p id="Par5">Generation of datasets like the ones described above requires laborious workload of pathologists who process whole-slide images and assign labels to selected image regions. The requirement of meticulous pathological annotation limits the amount of data available for model training. Formally, this relates to the bias-variance trade-off in machine learning<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. The effort to minimise bias on a small training set may result in high variance and low accuracy on unseen data, an effect known as overfitting. In order to minimise the expected test error, model regularisation techniques penalising model complexity can be applied. For CNNs, a technique called dropout has been proposed as a means of regularisation<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. Another technique, called active learning, can be used to deal with the difficulty of laborious data annotation. Active learning is an iterative procedure, where in each step the model is re-trained on data expanded with new samples, which are added based on results from the previous steps in order to maximise the learning rate. Variants of active learning depend on the way the new samples are selected. One method of choice is selection which maximises the diversity of the training set<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. <italic>Gal et al</italic>.<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> proposed an active learning procedure for Bayesian deep learning models, where new samples are added in each iteration based on their uncertainty estimated using variational dropout. This technique can be conceptualised by an analogy to a diligent student, who while taking a course actively asks the teacher for more examples on topics which are hard for them to understand. There are several attempts at active learning for histopathological image classification in the literature, using both traditional machine learning<sup><xref ref-type="bibr" rid="CR37">37</xref>,<xref ref-type="bibr" rid="CR39">39</xref>–<xref ref-type="bibr" rid="CR42">42</xref></sup> and deep learning<sup><xref ref-type="bibr" rid="CR43">43</xref>–<xref ref-type="bibr" rid="CR46">46</xref></sup>. However, none of these approaches utilised uncertainty for selection of new samples in active training.</p>
    <p id="Par6">In this work, we introduce an accurate, reliable and active (shortly, ARA) learning framework for classification of histopathological images of colorectal cancer. To this end, we develop a new CNN model (called ARA-CNN) for classification of colorectal cancer tissues, trained on the <italic>Kather et al</italic>. dataset (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). The model achieves stellar accuracy, higher than reported in the original publication of <italic>Kather et al</italic>.<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> and in later studies. The key contribution of this work is an extensive analysis of the utility of two variational-dropout based uncertainty measures, Entropy <italic>H</italic> and BALD (Bayesian Active Learning by Disagreement; introduced by <italic>Houlsby et al</italic>.<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>), in their application to histopathological image classification. We demonstrate that the distribution of uncertainty is increased for tissue classes that are the most difficult to learn for the model. Moreover, images that are misclassified tend to have the highest uncertainties. We propose an active learning framework, where the model suggests the most uncertain classes for annotation by a pathologist and identifies the most certain misclassified images as potentially incorrectly annotated (Fig. <xref rid="Fig1" ref-type="fig">1A</xref>). We show that <italic>H</italic> outperforms random selection of images and BALD when applied to select samples in an active learning procedure, speeding up the learning process by roughly 45%. In-depth inspection indicates that correctly classified images with very low Entropy <italic>H</italic> are highly characteristic of each tissue class. We show that low Entropy <italic>H</italic> for misclassified images correctly identifies mislabelled data in the training dataset and that ARA-CNN is highly robust to such noise in the data. On the other hand, images with very high uncertainty <italic>H</italic> are atypical or show pathological features that could be shared by other classes, which makes them pathologically difficult to categorise. In addition to image classification and uncertainty estimation, the framework is successfully applied to image segmentation and provides segmentation-based statistics of tissue class abundance in whole tissue slides (Fig. <xref rid="Fig1" ref-type="fig">1B</xref>).<fig id="Fig1"><label>Figure 1</label><caption><p>Overview of the proposed ARA framework. (<bold>A</bold>) Active histopathology workflow. Annotated whole-slide images (WSIs) are split into small image patches, which constitute a dataset. ARA-CNN is trained on that dataset. After the first round of training, the pathologist should be informed about i) which classes are the most uncertain and ii) which image patches are misclassified and highly certain, and thus identified as potentially mislabelled. The former should inform the pathologist about which classes to prioritise in the next round of annotation. The latter should inform about which image patches should be re-annotated with correct labels. We then take new annotated whole-slide images and continue the workflow until we reach a satisfying level of classification accuracy. (<bold>B</bold>) Segmentation workflow. Whole slide images are split into small image patches. Each of these is classified by trained ARA-CNN and is assigned a colour based on its classification result. These coloured tiles are merged together to form a segmented whole slide image and can be analysed in terms of their spatial relationships. Each resulting tile has a measured uncertainty value as well, so pathologists can make an informed decision whether to take the automated segmentation as-is or to inspect it manually.</p></caption><graphic xlink:href="41598_2019_50587_Fig1_HTML" id="d32e513"/></fig></p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <sec id="Sec3">
      <title>Analysed data</title>
      <p id="Par7">The analysed dataset holds 5000 image patches belonging to eight balanced classes of histopathologically recognisable tissues<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. The patches were pulled from ten anonymised and digitised tissue slides, stained with the H&amp;E technique. After initial coarse-grained annotation, 625 non-overlapping tiles were extracted from contiguous tissue areas for each class. Each tile has the same size of 150 × 150 pixels (equivalent to 74 <italic>μm</italic> × 74 <italic>μm</italic>). The eight tissue classes are: tumour epithelium, simple stroma (homogeneous composition, includes tumour stroma, extra-tumoural stroma and smooth muscle), complex stroma (containing single tumour cells and/or few immune cells), immune cells (including immune-cell conglomerates and sub-mucosal lymphoid follicles), debris (including necrosis, haemorrhage and mucus), normal mucosal glands, adipose tissue, background (no tissue). Here, for the sake of brevity, these classes are labelled as: Tumour, Stroma, Complex, Lympho, Debris, Mucosa, Adipose and Empty. In addition, one tissue slide denoted by <italic>Kather et al</italic>.<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> as a test image was used for the purpose of segmentation (see below).</p>
    </sec>
    <sec id="Sec4">
      <title>ARA-CNN model</title>
      <p id="Par8">To automatically classify the images from the analysed dataset into their corresponding classes, we developed and trained a Convolutional Neural Network (CNN) model. The architecture of the model, called ARA-CNN, was inspired by many state of the art solutions, including Microsoft ResNet<sup><xref ref-type="bibr" rid="CR48">48</xref></sup> and DarkNet 19<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>. For normalisation and to reduce overfitting, we used a popular technique called Batch Normalisation<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>. In ARA-CNN, overfitting is also reduced by using dropout<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. This in turn allowed us to apply variational dropout<sup><xref ref-type="bibr" rid="CR51">51</xref></sup> during testing. For every tested image, the model provides not only its predicted class, but also a measure of uncertainty estimated using variational dropout. The architecture of ARA-CNN is discussed in depth in Supplementary Information and presented in Fig. <xref rid="MOESM1" ref-type="media">S1</xref>.</p>
      <sec id="Sec5">
        <title>Dropout</title>
        <p id="Par9">Due to their size, deep learning models are especially prone to overfitting - they can inadvertently learn from sampling noise instead of actual non-linearities in the training data. One of the more popular and successful methods of combating this problem is dropout<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. It works on the basis of randomly removing units in a neural network during training in order to simulate a committee of multiple different architectures. In our model, dropout is applied to two fully connected layers with 32 units preceding auxiliary and final output. Its rate is equal to 0.5, which means that during both inference and training approximately half of all units are turned off and set to 0.</p>
      </sec>
      <sec id="Sec6">
        <title>Model training</title>
        <p id="Par10">The whole dataset of 5000 images was split into a training dataset and a test dataset used for evaluation. Their sizes varied depending on the experiment. In the 8-class case, we randomly divided the dataset into the training set with 4496 images (562 images per class) and the test set with 504 images (63 images per class). In the case of binary classification, the training set contained 1124 images, while the test set was comprised of 126 images (divided in half between Tumour and Stroma). These divisions were repeated ten times in the process of 10-fold cross-validation. Moreover, we also performed 5-fold cross-validation and 2-fold cross-validation, with the dataset split according to the number of folds.</p>
        <p id="Par11">Additionally, in each training epoch the training data was split into two datasets: the actual training data and a validation dataset. The latter was used for informing the learning rate reducer - we monitored the accuracy on the validation set and if it stopped improving, the learning rate was reduced by a factor of 0.1. In the task of evaluating model performance, this split was in proportion 90% to 10% between actual training data and the validation set, respectively, while in active learning the split was in proportion 70% to 30%. This is due to the fact that in active learning we start from a very small dataset and 10% was too small of a proportion to provide enough validation samples.</p>
        <p id="Par12">For parameter optimisation, we used the Adam<sup><xref ref-type="bibr" rid="CR52">52</xref></sup> optimiser. The training time differed depending on the experiment. In the cross-validation and mislabelled sample identification experiments we used 200 epochs, but in the active learning experiments it was 100 epochs instead (due to limited computational resources). In all cases, the training data was passed to the network in batches of 32, while the validation and test data was split into batches of 128 images.</p>
      </sec>
      <sec id="Sec7">
        <title>Loss function</title>
        <p id="Par13">During training, the categorical cross-entropy loss function was applied to both (auxiliary and final) outputs. The final loss is a weighted sum of these two losses with weight 0.9 for the final output and 0.1 for the auxiliary output. For observation <italic>o</italic>, a set of <italic>M</italic> classes and class <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${y}^{\ast }\in \{1\ldots M\}$$\end{document}</tex-math><mml:math id="M2"><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>⁎</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mi>…</mml:mi><mml:mi>M</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:math><inline-graphic xlink:href="41598_2019_50587_Article_IEq1.gif"/></alternatives></inline-formula>, we denote the probability of assigning the observation to that class as <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P({y}^{\ast }|o,\hat{\omega })$$\end{document}</tex-math><mml:math id="M4"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>⁎</mml:mo></mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>ω</mml:mi><mml:mo>ˆ</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:math><inline-graphic xlink:href="41598_2019_50587_Article_IEq2.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{\omega }$$\end{document}</tex-math><mml:math id="M6"><mml:mover accent="true"><mml:mi>ω</mml:mi><mml:mo>ˆ</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41598_2019_50587_Article_IEq3.gif"/></alternatives></inline-formula> represents the estimated parameters of the model. Categorical cross-entropy can then be defined as:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-\,\mathop{\sum }\limits_{{y}^{\ast }=1}^{M}\,\delta ({y}_{o}={y}^{\ast })\,\log (P({y}^{\ast }|o,\hat{\omega })),$$\end{document}</tex-math><mml:math id="M8" display="block"><mml:mo>−</mml:mo><mml:mspace width="-.25em"/><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>⁎</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mspace width=".1em"/><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>⁎</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width=".1em"/><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>⁎</mml:mo></mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>ω</mml:mi><mml:mo>ˆ</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41598_2019_50587_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <italic>δ</italic> is the Dirac function and <italic>y</italic><sub><italic>o</italic></sub> is the correct class for observation <italic>o</italic>.</p>
      </sec>
    </sec>
    <sec id="Sec8">
      <title>Variational dropout for inference and uncertainty estimation</title>
      <p id="Par14">In order to provide more accurate classification as well as uncertainty prediction, we adopted a popular method called variational dropout<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>. The central idea of this technique is to keep dropout enabled by performing multiple model calls during prediction. Thanks to the fact that different units are dropped across different model calls, it might be considered as Bayesian sampling from a variational distribution of models<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. In a Bayesian setting, the parameters (i.e. weights) <inline-formula id="IEq4"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\omega $$\end{document}</tex-math><mml:math id="M10"><mml:mi>ω</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_50587_Article_IEq4.gif"/></alternatives></inline-formula> of a CNN model are treated as random variables. In variational inference, we approximate the posterior distribution <italic>P</italic>(<italic>ω</italic>|D) by a simpler (variational) distribution <inline-formula id="IEq5"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$q(\omega )$$\end{document}</tex-math><mml:math id="M12"><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math><inline-graphic xlink:href="41598_2019_50587_Article_IEq5.gif"/></alternatives></inline-formula>, where D is the training dataset. Thus, we assume that <inline-formula id="IEq6"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{\omega }}_{t}\sim q(\omega )$$\end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>ω</mml:mi><mml:mo>ˆ</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math><inline-graphic xlink:href="41598_2019_50587_Article_IEq6.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq7"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{\omega }}_{t}$$\end{document}</tex-math><mml:math id="M16"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>ω</mml:mi><mml:mo>ˆ</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2019_50587_Article_IEq7.gif"/></alternatives></inline-formula> is an estimation of <inline-formula id="IEq8"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\omega $$\end{document}</tex-math><mml:math id="M18"><mml:mi>ω</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_50587_Article_IEq8.gif"/></alternatives></inline-formula> resulting from a variatonal dropout call <italic>t</italic>. With these assumptions, the following approximations can be derived<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P({y}^{\ast }|o,D)={\int }^{}\,P({y}^{\ast }|o,\omega )P(\omega /D)d\omega \approx {\int }^{}\,P({y}^{\ast }|o,\omega )q(\omega )d\omega \approx \frac{1}{T}\,\mathop{\sum }\limits_{t=1}^{T}\,P({y}^{\ast }|o,{\hat{\omega }}_{t}),$$\end{document}</tex-math><mml:math id="M20" display="block"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>⁎</mml:mo></mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mo>∫</mml:mo><mml:mspace width="-.25em"/></mml:msup><mml:mspace width=".1em"/><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>⁎</mml:mo></mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo>/</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>ω</mml:mi><mml:mo>≈</mml:mo><mml:msup><mml:mo>∫</mml:mo><mml:mspace width="-.25em"/></mml:msup><mml:mspace width=".1em"/><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>⁎</mml:mo></mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>ω</mml:mi><mml:mo>≈</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:mspace width=".1em"/><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mspace width=".1em"/><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>⁎</mml:mo></mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>ω</mml:mi><mml:mo>ˆ</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41598_2019_50587_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where <italic>T</italic> is the number of variational samples. In our model we used <inline-formula id="IEq10"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$T=50$$\end{document}</tex-math><mml:math id="M22"><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:math><inline-graphic xlink:href="41598_2019_50587_Article_IEq10.gif"/></alternatives></inline-formula>.</p>
      <p id="Par15">Variational dropout has enabled us to measure the uncertainty of predictions. We implemented two uncertainty measures: Entropy <italic>H</italic> and BALD<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>. If the output of the model is a conditional probability distribution <inline-formula id="IEq11"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P({y}^{\ast }|o,D)$$\end{document}</tex-math><mml:math id="M24"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>⁎</mml:mo></mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math><inline-graphic xlink:href="41598_2019_50587_Article_IEq11.gif"/></alternatives></inline-formula>, then the measure <italic>H</italic> can be defined as entropy of the predictive distribution:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H[P({y}^{\ast }|o,D)]=-\,\,\,\sum _{{y}^{\ast }\in \{1\ldots M\}}\,P({y}^{\ast }|o,D)\,\log \,P({y}^{\ast }|o,D)$$\end{document}</tex-math><mml:math id="M26" display="block"><mml:mi>H</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>⁎</mml:mo></mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mspace width="-.25em"/><mml:mspace width="-.25em"/><mml:mspace width="-.25em"/><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>⁎</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mi>…</mml:mi><mml:mi>M</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:munder><mml:mspace width=".1em"/><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>⁎</mml:mo></mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width=".1em"/><mml:mi>log</mml:mi><mml:mspace width=".1em"/><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>⁎</mml:mo></mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math><graphic xlink:href="41598_2019_50587_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par16">The second uncertainty measure, BALD, is based on mutual information and measures the information gain about the model parameters <inline-formula id="IEq12"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\omega $$\end{document}</tex-math><mml:math id="M28"><mml:mi>ω</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_50587_Article_IEq12.gif"/></alternatives></inline-formula> obtained from classifying observation <italic>o</italic> with label <italic>y</italic>*. In the case of variational dropout, this can be expressed as the difference between entropy of the predictive distribution and the mean entropy of predictions across multiple model calls:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{rcl}I(\omega ,{y}^{\ast }|o,D) &amp; = &amp; H[P({y}^{\ast }|o,D)]-{{\mathbb{E}}}_{P(\omega /D)}[H[P({y}^{\ast }|o,\omega )]]\\  &amp; \simeq  &amp; H[P({y}^{\ast }|o,D)]-\frac{1}{T}\,\mathop{\sum }\limits_{t=1}^{T}\,H[P({y}^{\ast }|o,{\hat{\omega }}_{t})].\end{array}$$\end{document}</tex-math><mml:math id="M30" display="block"><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="right"><mml:mrow><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>⁎</mml:mo></mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>H</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>⁎</mml:mo></mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo>/</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>⁎</mml:mo></mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="right"/><mml:mtd columnalign="center"><mml:mo>≃</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>H</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>⁎</mml:mo></mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:mspace width=".1em"/><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mspace width=".1em"/><mml:mi>H</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>⁎</mml:mo></mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>ω</mml:mi><mml:mo>ˆ</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="41598_2019_50587_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par17">The difference between these two measures pertains to how they react to two different types of uncertainty in the data: epistemic and aleatoric<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>. The former type is caused by a lack of knowledge - in terms of machine learning, this is analogous to a lack of data, so the posterior probability over model parameters is broad. The latter uncertainty is a result of noise in the data - no matter how much data the model has seen, if there is inherent noise then the best possible prediction may be highly uncertain<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>. In general, the Entropy <italic>H</italic> measure cannot distinguish these two types of uncertainty. If uncertainty of a new observation is measured by <italic>H</italic>, then the value would not depend on the underlying uncertainty type. On the other hand, it is believed that BALD measures epistemic uncertainty of the model<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>, so it would not return a high value if there is only aleoratic uncertainty present. Depending on the dataset, one of these measures might work better than the other at catching and describing the uncertainty.</p>
    </sec>
    <sec id="Sec9">
      <title>Image segmentation</title>
      <p id="Par18">To perform segmentation of test tissue slides from the <italic>Kather et al</italic>.<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> dataset, each of these 5000 × 5000 px images was split into 10000 non-overlapping test samples with resolution of 50 × 50 pixels. These test images were then supplied as input to our model (by being upscaled to 128 × 128 pixels), which returned a classification into one of eight classes of colorectal tissue. Since the output of the model is a probability distribution, we selected the class with the highest value as the prediction for a given test image patch. We did not consider the measured uncertainty in this process. To get the final segmentation, we assigned a colour to each predicted class and generated a 50 × 50 pixels single-coloured patch for each test image. These patches were then stitched together to form the final images. Lastly, we applied a blurring Gaussian filter to smooth out the edges of tissue regions.</p>
      <p id="Par19">Finally, we performed a simple spatial analysis for each slide by counting the percentage of surface area taken by each class.</p>
    </sec>
    <sec id="Sec10">
      <title>Active learning</title>
      <p id="Par20">Active learning is an iterative procedure, where the initial model is trained on a small dataset and in consecutive iterations it is re-trained on a dataset extended by new samples. At each step, the new samples are added according to some acquisition function evaluated using the current model. Intuitively, the uncertainty measures described above are a good basis for an acquisition function in deep learning. In a given iteration, the model should first choose the samples it is most uncertain of<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>.</p>
      <p id="Par21">In this work we implemented and compared effectively three different acquisition functions. Two were based on uncertainty measures H and BALD, whereas the third was a random selection and served as a baseline. We performed a series of experiments in order to determine if uncertainty-based active learning can speed-up training with the colorectal cancer dataset. To this end, we emulated the proposed active learning workflow (Fig. <xref rid="Fig1" ref-type="fig">1A</xref>) utilising the available data. We started from generating three random splits of the full dataset - this gave us three test sets of 504 images and three training sets of 4496 images. Then for each of these test-train pairs, we performed the active learning procedure for both uncertainty measures plus a baseline training process based on random selection of images. In each case, we started from selecting 40 images per class (so 320 in total) from the training dataset. We trained the model on that small dataset and then, based on a given acquisition function, we chose 160 images to add to the previous 320. This slightly larger set became a new training dataset. We repeated this process, adding 160 images in each step, until there were no more images to draw from the initial full training dataset, giving us 28 training steps in total. Additionally, in order to eliminate the effects of random weight initialisation, we pre-initialised the model 8 times for each step and used these initialisations for each of the 3 dataset splits. Thus, for each of the 28 steps we had to train the model 24 times.</p>
      <p id="Par22">For the random selection, the 160 new images in each step were sampled uniformly at random from the full training dataset. For the uncertainty-based functions, we performed inference on remaining images from the full training set in each step. We evaluated the uncertainty for each image using the H and BALD measures, according to Eqs (<xref rid="Equ3" ref-type="disp-formula">3</xref>) and (<xref rid="Equ4" ref-type="disp-formula">4</xref>). We sorted the results by uncertainty in descending order and selected the top 160. The results for each active learning step were averaged between initialisations and dataset splits.</p>
    </sec>
    <sec id="Sec11">
      <title>Identification of mislabelled training samples</title>
      <p id="Par23">Identification of mislabelled training samples is an important problem in supervised learning<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>. Mislabelled training data is particularly likely to occur in our classification problem, where the training image patches are cut out from relatively large regions of WSIs annotated by the pathologist. In such a setup, the entire region labelled with a particular class may coincidentally include patches which in fact belong to a different class. Here, we show that such mislabelled training patches can be identified as those that were misclassified by ARA-CNN with low entropy <italic>H</italic> (i.e. with high certainty). Specifically, the identification of candidate mislabelled images using ARA-CNN proceeds as follows. Given the expected percentage of such mislabelled training images <italic>p</italic><sub><italic>m</italic></sub>, separately for each class <italic>c</italic> we identify images misclassified by the model with uncertainty below a threshold <inline-formula id="IEq13"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${H}_{t}^{c}={q}^{c}({p}_{m})$$\end{document}</tex-math><mml:math id="M32"><mml:msubsup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:math><inline-graphic xlink:href="41598_2019_50587_Article_IEq13.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq14"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${q}^{c}({p}_{m})$$\end{document}</tex-math><mml:math id="M34"><mml:msup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:math><inline-graphic xlink:href="41598_2019_50587_Article_IEq14.gif"/></alternatives></inline-formula> is the <italic>p</italic><sub><italic>m</italic></sub>-th percentile of the empirical distribution of <italic>H</italic> in class <italic>c</italic>. The final set of candidate mislabelled images is the union of the identified images across all classes.</p>
    </sec>
  </sec>
  <sec id="Sec12" sec-type="results">
    <title>Results</title>
    <sec id="Sec13">
      <title>Model performance</title>
      <p id="Par24">To evaluate the performance of ARA-CNN, similarly to previous models trained on the same dataset, we measured its receiver operating characteristic (ROC) curves, area under the ROC curves (AUC) and error rates in 10-fold cross-validation for both 8-class and 2-class (Tumour vs Stroma) classification tasks. In addition, we also evaluated precision-recall curves. We used images with all colour information preserved. The results were compared to those of the original model by <italic>Kather et al</italic>. (Fig. <xref rid="Fig2" ref-type="fig">2</xref>), as well as to other methods that used the same dataset. Where necessary, we performed 5-fold or 2-fold cross-validation and used the results as a comparison point. In their work, <italic>Kather et al</italic>.<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> tested the performance of several low-level image features in combination with four classification algorithms, applied to grayscale images from their dataset. Their approach is an example of a ‘traditional’ procedure, where image features have to be hand-crafted and chosen appropriately depending on the dataset. The best results were reported for a combination of features containing: pixel value histograms, local binary patterns, gray-level co-occurence matrix and perception-like features. The best performing classifier was a support vector machine (SVM) algorithm with the radial basis function (RBF) kernel.<fig id="Fig2"><label>Figure 2</label><caption><p>Model performance in 10-fold cross-validation. (<bold>A</bold>) ROC and area under the ROC curve (AUC) for classification into eight tissue types. The model presented in this work achieved an average AUC of 0.995 (a mean was taken across all eight classes), (<bold>B</bold>) ROC and AUC for binary classification between Tumour and Stroma. ARA-CNN achieves AUC of 0.998. (<bold>C</bold>) Precision-recall curves for ARA-CNN in a multiclass classification setting. The mean AUC for these curves is 0.972. (<bold>D</bold>) Error comparison to previous work. With error rate 7.56% for eight class classification, our model substantially reduces the error (by 5.04%) compared to error rate 12.6% of the best model assessed by <italic>Kather et al</italic>.<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. For binary (Tumour versus Stroma) classification, our model has error rate 0.89%, which is also lower than the 1.4% error rate of the Kather <italic>et al</italic>. model.</p></caption><graphic xlink:href="41598_2019_50587_Fig2_HTML" id="d32e1649"/></fig></p>
      <p id="Par25">The ROC curves (generated with a one-vs-all method) for the 8-class experiment show excellent performance of ARA-CNN (Fig. <xref rid="Fig2" ref-type="fig">2A</xref>). The AUC values for the Tumour, Mucosa, Lympho, Adipose and Empty classes range from 0.997 to 0.999. Values for the Stroma, Complex and Debris classes are a little lower (from 0.988 to 0.992), which indicates that the model cannot always distinguish them from other classes. Still, the mean AUC value is 0.995, which is higher than the value of 0.976 obtained by <italic>Kather et al</italic>.<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. The ROC curve for the 2-class problem (Fig. <xref rid="Fig2" ref-type="fig">2B</xref>) and its corresponding AUC value of 0.998 also illustrate near-perfect performance of ARA-CNN. It is important to note that performance evaluation using ROC curves for the multiclass classification task in a one-vs-all setting may be biased due to the fact that the classes are unbalanced. In such a setting, it is better to use precision-recall curves (Fig. <xref rid="Fig2" ref-type="fig">2C</xref>). The AUC values for these curves, as obtained by ARA-CNN, are a bit lower than for the ROC curves, but with the mean AUC of 0.972 are still indicative of excellent performance. The lowest AUC value (0.924) is obtained for the Complex versus all classification task. This indicates that the Complex class is the most difficult one to classify correctly for the model. We do not compare these results to other methods, as we are not aware of any other approaches that used precision-recall curves for performance evaluation on this dataset.</p>
      <p id="Par26">In terms of error rates, for the 8-class problem the ARA-CNN model reached an average rate of 7.56%, which is substantially lower, by 5.04%, than the best result reported by <italic>Kather et al</italic>.<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> (Fig. <xref rid="Fig2" ref-type="fig">2D</xref>). Similarly, in the binary classification task, we obtained an error rate of 0.89%, lower than 1.4% obtained by <italic>Kather et al</italic>. Thus, our model is better than the best of standard approaches presented by <italic>Kather et al</italic>.<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>, especially in the multiclass classification scenario. One of the differences between deep learning and the standard approaches is that the former construct the features on the fly based on the data itself. Here, the features identified by ARA-CNN as part of the learning process outperform the set of features that were engineered by <italic>Kather et al</italic>.<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> in the difficult task of decisively describing all classes in a multiclass image classification problem.</p>
      <p id="Par27">The classification performance of ARA-CNN is also superior or comparable to other published models that used the <italic>Kather et al</italic>. dataset, including both traditional and deep learning approaches that utilise CNNs (Table <xref rid="Tab1" ref-type="table">1</xref>). ARA-CNN outperforms the traditional methods by a significant margin both in terms of AUC and accuracy. When it comes to CNN methods, <italic>Wang et al</italic>.<sup><xref ref-type="bibr" rid="CR31">31</xref></sup> performed 5-fold cross-validation and reported a mean AUC value of 0.985 (lower by 0.01 than ARA-CNN) and 92.6% accuracy (higher by 0.36% than ARA-CNN) for their BCNN in the multiclass task. Although BCNN and ARA-CNN achieve similarly high performance results, their architectures are very different. BCNN depends on an external method to perform stain decomposition of H&amp;E images and is composed of two simple feed-forward CNNs, which take as input separate signals from the Eosin and Hematoxylin components and whose outputs are combined by bilinear pooling. We took a more typical deep-learning approach, with a deeper network with residual connections, where no independent feature extraction nor decomposition is needed, and the network itself is responsible for extracting important signals from raw image data. <italic>Pham</italic><sup><xref ref-type="bibr" rid="CR32">32</xref></sup> used an autoencoder architecture to re-sample the images from the <italic>Kather et al</italic>. dataset and trained a small supervised network for different re-sampling factors. They reported at best an accuracy of 84.00% for binary classification, which is lower by 14.88% in comparison to our result. <italic>Ciompi et al</italic>.<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> used the <italic>Kather et al</italic>. dataset for testing their model trained on an independent colorectal cancer dataset and reported relatively small accuracies of 50.96% and 75.55%, where the former was achieved without stain normalisation and the latter was an improvement resulting from having stain normalisation applied. However, since this model was trained on a different dataset, we do not directly compare our result to theirs. Overall, ARA-CNN’s achieves excellent performance on the <italic>Kather et al</italic>. dataset, and scores better than most other published methods that utilised the same data for training. Exceptional performance of our approach indicates that it successfully combines the flexibility typical for deep neural networks with strong regularisation resulting from dropout and Batch Normalisation.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Comparison of different methods that used the <italic>Kather et al</italic>. dataset for training.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>Method type</th><th>Problem type</th><th>Max. reported 10-fold ACC</th><th>Max. reported 5-fold ACC</th><th>Max. reported 2-fold ACC</th><th>10-fold AUC</th><th>5-fold AUC</th></tr></thead><tbody><tr><td rowspan="2"><italic>Kather et al</italic>.</td><td rowspan="2">Traditional</td><td>Binary</td><td>98.6%</td><td>—</td><td>—</td><td>—</td><td>—</td></tr><tr><td>Multiclass</td><td>87.4%</td><td>—</td><td>—</td><td>0.976</td><td>—</td></tr><tr><td><italic>Ribeiro et al</italic>.*</td><td>Traditional</td><td>Binary</td><td>97.68%</td><td>—</td><td>—</td><td>—</td><td>—</td></tr><tr><td><italic>Sarkar et al</italic>.</td><td>Traditional</td><td>Multiclass</td><td>73.66%</td><td>—</td><td>—</td><td>—</td><td>—</td></tr><tr><td><italic>Wang et al</italic>.</td><td>CNN</td><td>Multiclass</td><td>—</td><td><bold>92.6</bold> ± <bold>1.2%</bold></td><td>—</td><td>—</td><td>0.985</td></tr><tr><td><italic>Pham</italic></td><td>CNN</td><td>Binary</td><td>—</td><td>—</td><td>84.00%</td><td>—</td><td>—</td></tr><tr><td rowspan="2">ARA-CNN</td><td rowspan="2">CNN</td><td>Binary</td><td><bold>99.11</bold> ± <bold>0.97%</bold></td><td><bold>98.88</bold> ± <bold>0.52%</bold></td><td><bold>98.88%</bold></td><td><bold>0.998</bold></td><td><bold>0.999</bold></td></tr><tr><td>Multiclass</td><td><bold>92.44</bold> ± <bold>0.81%</bold></td><td>92.24 ± 0.82%</td><td><bold>88.92</bold> ± <bold>1.95%</bold></td><td><bold>0.995</bold></td><td><bold>0.995</bold></td></tr></tbody></table><table-wrap-foot><p>ACC– accuracy. We summarise performance measures of compared methods as reported by the authors. Results in bold are the best in their category.</p><p>*The authors do not explicitly state the number of folds. Since in other reported results the number of folds they used is 10, we assume 10-fold cross-validation here as well.</p></table-wrap-foot></table-wrap></p>
      <p id="Par28">Finally, we present the excellent results of segmentation of whole slide images in Supplementary Information and in Fig. <xref rid="MOESM1" ref-type="media">S2</xref>.</p>
    </sec>
    <sec id="Sec14">
      <title>Uncertainty, active learning and identification of mislabelled images</title>
      <p id="Par29">Deep learning models are often criticised for being so-called black-boxes. Due to their complexity, it can be very hard to tell why a given test sample is classified to a certain class. The model presented in this work, thanks to its implementation of dropout and variational inference, has a few ways to measure the uncertainty of each prediction. These uncertainty measures allow the model predictions to be reliable. Consider an example image, which is classified by the model as Tumour with high probability 0.95, but the measured uncertainty is also high. This can mean that the prediction cannot be taken for granted and needs to be double-checked by a human. This additional indication of prediction uncertainty brings us one step closer to alleviating the problem of the black-box nature of deep learning and increases model-based understanding of the data. Here, we evaluated two uncertainty measures, Entropy <italic>H</italic> and BALD (see <italic>Uncertainty estimation</italic>), checking their distribution in each class and their performance as acquisition functions in active learning on the <italic>Kather et al</italic>.<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> dataset.</p>
      <p id="Par30">First, we applied the trained model to 504 test images. For each image, we recorded the classification and the measured uncertainty. The results for Entropy <italic>H</italic> are presented in Fig. <xref rid="Fig3" ref-type="fig">3A</xref>. On average, the highest uncertainty values were reported for images from the Stroma and Complex classes. The biggest variance in uncertainty was measured for the Debris class. These three classes were also misclassified as each other, which indicates that they are similar in appearance and the model has a hard time differentiating them. This is in agreement with the precision-recall curves in Fig. <xref rid="Fig2" ref-type="fig">2D</xref> and with the analysis described below in <italic>Understanding uncertainty</italic>. In addition, it can be observed that misclassification occurred almost exclusively when the uncertainty was high. Thus, a high uncertainty is indeed a good indicator that the prediction may be faulty. The results for BALD are shown in Fig. <xref rid="Fig3" ref-type="fig">3B</xref>. On average, the most uncertain classes according to that measure are Stroma and Complex, in agreement with Entropy <italic>H</italic>. Interestingly, BALD measured much less variance in the Debris class, which makes Lympho the most variable class in this case. Moreover, the Empty class is relatively more certain according to BALD than in the Entropy <italic>H</italic> experiment. These differences may be a result of epistemic and aleatoric uncertainties present in the data, which are measured differently by BALD and Entropy <italic>H</italic> (see <italic>Active learning</italic>). Nevertheless, the BALD measure still captures the fact that misclassifications take place mainly for highly uncertain predictions.<fig id="Fig3"><label>Figure 3</label><caption><p>The uncertainty of image classification. (<bold>A</bold>,<bold>B</bold>) Distribution of uncertainty for the colorectal cancer images used to train our model. The horizontal axis shows the actual class of these images, whereas the classification of each image is represented with coloured jitter. The y-axis value represents the amount of uncertainty. Our model is on average most uncertain when it comes to the Stroma and Complex classes. It also makes mistakes in classification mostly when it is uncertain. (<bold>A</bold>) Distribution of uncertainty for the Entropy <italic>H</italic> measure. (<bold>B</bold>) Distribution of uncertainty for the BALD measure. (<bold>C</bold>) Results of active learning experiments. Starting from a small training dataset with 320 images in total (step number 0, 40 images per class), the model was re-trained on the dataset increased in every iteration by 160 additional images. Three distinct acquisition functions were tested: Random, Entropy <italic>H</italic> and BALD. At each step, the average classification accuracy was measured (y-axis). (<bold>D</bold>,<bold>E</bold>) Microscopic images of tissues composing colorectal cancer. Samples were categorised by uncertainty measured with Entropy <italic>H</italic>. Columns correspond to different tissue classes. (<bold>D</bold>) Images with low uncertainty <italic>H</italic>. (<bold>E</bold>) Images with high uncertainty <italic>H</italic>.</p></caption><graphic xlink:href="41598_2019_50587_Fig3_HTML" id="d32e2126"/></fig></p>
      <sec id="Sec15">
        <title>Active learning</title>
        <p id="Par31">Active learning is a set of methods that try to minimise the amount of labelled data needed to fully train a classifier. They start from a small dataset and, as the training goes on, add new training samples according to some kind of acquisition function. We tested the effectiveness of using uncertainty measures as this function, effectively choosing most uncertain images as the ones the model should learn first. The idea here is that if the model learns first what it has the most trouble with, then it should achieve high accuracy at an earlier stage in the active learning process.</p>
        <p id="Par32">Here, we designed an active learning process with either the Entropy <italic>H</italic> or BALD measures acting as acquisition functions (see <italic>Active learning</italic>). We evaluated its efficiency on the <italic>Kather et al</italic>.<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> dataset by analysing the resulting model accuracy as a function of the number of training samples (Fig. <xref rid="Fig3" ref-type="fig">3C</xref>). The Random acquisition function serves as a baseline. In initial active learning iterations the Entropy <italic>H</italic> measure performs very similarly to random selection, but from step 7 (which contained 1440 images) Entropy <italic>H</italic> achieves consistently higher accuracy (with on average 2% improvement in classification accuracy) until the very end of the process. The accuracy of the model trained on samples selected using the BALD measure is worse than the random one from the start of active learning until step 12. From step 13 (which contained 2400 images) it gets slightly better, but never eclipses the accuracy received using the Entropy <italic>H</italic> measure. This proves that the Entropy <italic>H</italic> uncertainty measure can be successfully used as an acquisition function in active learning scenarios utilising our ARA-CNN model. It can speed up the learning process by roughly 45%. The model reaches the classification accuracy equivalent to the full dataset already at step 15, in which the training set contained 2720 images. Thus, the fraction of images required for obtaining the full accuracy is only 2720 out of 5000 (54.4%), and the fraction of steps required is only 15 out of 27 (55.56%), both amounting to around 45% reduction. It means that this subset of images, chosen based on the Entropy <italic>H</italic> uncertainty measure, is large enough to accurately train the model.</p>
      </sec>
      <sec id="Sec16">
        <title>Identification of mislabelled images</title>
        <p id="Par33">We propose that images that are misclassified by ARA-CNN with high certainty (i.e., low <italic>H</italic>) are good candidates for identifying mislabelled training samples (see <italic>Identification of mislabelled training samples</italic>). To demonstrate the performance of our identification approach, we artificially introduced increasing percentage of mislabelled images into the training set and measured sensitivity and specificity, while recording the overall model performance.</p>
        <p id="Par34">To this end, we randomly divided the dataset into a training set and a test set, with the the same proportions as during the model training (see <italic>Model training</italic>). Next, we took the training set, randomly sampled a given percentage <italic>p</italic><sub><italic>m</italic></sub> of images and changed their assigned class at random. Finally, we trained the model on a training dataset with these mislabelled images reintroduced. We defined the set of positives <italic>P</italic> as candidate mislabelled images identified by our approach. The set of true positives <italic>TP</italic> is defined as all of the artificially mislabelled images. Sensitivity was evaluated as <inline-formula id="IEq15"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$|TP|/|P|$$\end{document}</tex-math><mml:math id="M36"><mml:mo stretchy="false">|</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mo>/</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:math><inline-graphic xlink:href="41598_2019_50587_Article_IEq15.gif"/></alternatives></inline-formula> and specificity as <inline-formula id="IEq16"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$|TN|/|N|$$\end{document}</tex-math><mml:math id="M38"><mml:mo stretchy="false">|</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mo>/</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:math><inline-graphic xlink:href="41598_2019_50587_Article_IEq16.gif"/></alternatives></inline-formula>, with <italic>TN</italic> and <italic>N</italic> being the complement sets for <italic>TP</italic> and <italic>P</italic>, respectively.</p>
        <p id="Par35">Sensitivity of mislabelled image identification is overall very high, and is only slightly affected by the growing percentage of mislabelled samples (Fig. <xref rid="Fig4" ref-type="fig">4A</xref>). For <inline-formula id="IEq17"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${p}_{m}\in \{0.1,0.5,1\}$$\end{document}</tex-math><mml:math id="M40"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:math><inline-graphic xlink:href="41598_2019_50587_Article_IEq17.gif"/></alternatives></inline-formula>, sensitivity is at 100%, meaning that all misclassified images with uncertainty below <italic>H</italic><sub><italic>t</italic></sub> are in fact mislabelled. For higher <italic>p</italic><sub><italic>m</italic></sub> values, sensitivity is slightly lower, but it never dips below 88.82% (for <inline-formula id="IEq18"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${p}_{m}=40$$\end{document}</tex-math><mml:math id="M42"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>40</mml:mn></mml:math><inline-graphic xlink:href="41598_2019_50587_Article_IEq18.gif"/></alternatives></inline-formula>). Specificity decreases with the increase of the percentage of mislabelled training samples, but remains at very high level even for substantial percentage <italic>p</italic><sub><italic>m</italic></sub>, dropping below 80% only at <italic>p</italic><sub><italic>m</italic></sub> around 20%. This demonstrates that uncertainty <italic>H</italic> can be used to find mislabelled training samples even when the noise in the training data is extremely high.<fig id="Fig4"><label>Figure 4</label><caption><p>Identification of mislabelled images as a function of their percentage in the training set. (<bold>A</bold>) Sensitivity and specificity for the proposed mislabelled sample identification strategy. (<bold>B</bold>) Classification accuracy of ARA-CNN decreases only for very high fraction of mislabelled training images.</p></caption><graphic xlink:href="41598_2019_50587_Fig4_HTML" id="d32e2352"/></fig></p>
        <p id="Par36">We also measured what effect an increasing <italic>p</italic><sub><italic>m</italic></sub> has on the classification accuracy of ARA-CNN on the test set (Fig. <xref rid="Fig4" ref-type="fig">4B</xref>). Remarkably, up to and including 5% of artificially mislabelled training samples, the performance is not affected. From 10% up to and including 70%, it decreases, but only slightly. From 80%, the amount of mislabelled images is too large and the model cannot be trained properly, which results in a substantial drop in accuracy. Such a good classification performance even in the case when the majority of the training samples are mislabelled at random indicates that ARA-CNN is highly robust to noise in the training data.</p>
      </sec>
      <sec id="Sec17">
        <title>Understanding the uncertainty of image classification</title>
        <p id="Par37">To investigate what pathological features of images are determinant for assigning specific uncertainty values measured by Entropy <italic>H</italic>, we selected test images with very low (<inline-formula id="IEq19"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H\le 0.2$$\end{document}</tex-math><mml:math id="M44"><mml:mi>H</mml:mi><mml:mo>≤</mml:mo><mml:mn>0.2</mml:mn></mml:math><inline-graphic xlink:href="41598_2019_50587_Article_IEq19.gif"/></alternatives></inline-formula>) and very high (<inline-formula id="IEq20"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H\ge 0.8$$\end{document}</tex-math><mml:math id="M46"><mml:mi>H</mml:mi><mml:mo>≥</mml:mo><mml:mn>0.8</mml:mn></mml:math><inline-graphic xlink:href="41598_2019_50587_Article_IEq20.gif"/></alternatives></inline-formula>) uncertainty and inspected them by eye. We focused on Entopy <italic>H</italic> due to its superior performance in active learning. There were no examples of the Empty class with high uncertainty, indicating this class is easy for the algorithm to recognise and classify properly. For each of the remaining seven tissue classes, images of lowest uncertainty display characteristic pathological features (Fig. <xref rid="Fig3" ref-type="fig">3D</xref>). Images of the Tumour class with low <italic>H</italic> display cells that have distinct changes in their nuclei: enlargement, hiperchromasia (dark violet colour), improper chromatin distribution (i.e. spots with higher and lower density) accompanied by multiplication of nucleoli, increased nuclear to cytoplasmic ratio, nuclear fusions and cell overlapping. The images of the Stroma class with lowest uncertainty display typical uniformly stained pink, eosinophilic fibres with elongated nuclei, and low nuclear to cytoplasmic ratio. For the images of the Complex class with low assigned <italic>H</italic>, the stroma is infiltrated by lymphatic or neoplastic cells with addition of erythrocytes. The highly certain images of the Lympho class show features typical for areas of lymphocytic dense infiltration - lymphocytes are intensively stained, monomorphic cells with round nucleus and very scarce thin, basophilic cytoplasm rim. Nucleoli are not visible. Images of the Debris class with low uncertainty <italic>H</italic> values are composed of various tissue samples. First, they contain a mucous, amorphic substance creating multiple, fine vesicles, white in the centre with violet contours. On top of that, features characteristic of the Debris class are mostly extravasated erythrocytes – red, round cell conglomerates presenting very dense collocation with blurred cell contours. Images of the Mucosa class with very low assigned uncertainty show typical features of mucosal glands in large intestine. They are composed of visible characteristic goblet cells that are cylindrical in shape and contain big, round areas filled with mucous - white with violet margin. Small, regular, dark nuclei are visible at the cell periphery. Goblet cells lay in linear or rosette–like formations. Finally, images of the Adipose class with low uncertainty show pathological features typical of the adipose tissue. They are composed of big, white polygonal areas with violet, wide contour, adhering to each other tightly. No nuclei are visible.</p>
        <p id="Par38">In contrast to low uncertainty images, the images with the highest uncertainty show features that are pathologically difficult to categorise (Fig. <xref rid="Fig3" ref-type="fig">3E</xref>). For very uncertain images of the Tumour class, the sparse cells visible within the stroma show fewer features of malignancy – most of them are small, regular in shape, with no visible nucleoli. No nuclear fusions or cell overlapping are observed. The pictures could be mistaken with complex stroma. For the images of the Stroma class that were assigned very high uncertainty <italic>H</italic>, the tissue has irregular structure without typical linear fibres and elongated nuclei. Empty spaces in both example images and very low colour intensity in the top one may be artefacts, although whole samples could be categorised as complex stroma or perchance debris because of listed alternations. Out of the two complex stroma example images with very high uncertainty, in the top image (Fig. <xref rid="Fig3" ref-type="fig">3E</xref> third column) there are no visible fibres. At the same time, the image contains many pale vesicular areas slightly similar to mucous. The bottom image could be interpreted as a normal stroma sample, because of its colour, fibrotic structure and shape of the nuclei. In the top image representative of very high uncertainty images of the Lympho class, cell arrangement is not very dense and there is a lot of stroma visible between nuclei – this could be categorised as complex stroma instead. The bottom picture shows many features of malignancy that should suggest diagnosis of tumour cells. From the two uncertain example images from the Debris class, the top consists of tissue residues with no particular structures visible. The bottom image shows structures very similar to mucosal glands – areas of mucous are bigger and well margined in comparison to amorphic mucous specific for this category. From the two high uncertainty images of the Mucosa class, the top image has heterogeneous composition. In the right part of the image, goblet cells with their nuclei can be seen. The left part is full of amorphic substance and could be categorised as debris. In the bottom example, only the lower left corner looks like mucosal glands forming rosette. The rest of the image contains stroma with lymphatic infiltration, thus pathologically could be categorised as complex stroma. In the top uncertain example of the Adipose class, although white, empty spaces are clearly visible and cell walls have more irregular margin than normally. In the bottom example, the characteristic polygonal shapes are not visible. The images do not suit any other category more than adipose tissue, however they do not share its typical features.</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec18" sec-type="discussion">
    <title>Discussion</title>
    <p id="Par39">In this article, we stipulated the necessity of an accurate, reliable and active (ARA) machine learning framework for histopathological image classifiction. We implemented this framework with a new Bayesian deep learning CNN model, called ARA-CNN. ARA-CNN was applied to the task of colorectal tissue classification and incorporated it into an uncertainty-based active pathology workflow. The classification accuracy achieved by our model exceeds the results reported by authors of the training dataset <italic>Kather et al</italic>.<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> used in this work. The proposed CNN architecture shows outstanding performance in both binary and multiclass classification scenarios, reaching almost perfect accuracy (error rate of 0.89%) in the former case and best in class (error rate of 7.56%) in the latter. It also surpasses the classification performance of other methods that were trained with the same dataset by up to 18.78%.</p>
    <p id="Par40">To achieve reliability, the model measures the uncertainty of each prediction. As demonstrated by our active learning results (Fig. <xref rid="Fig3" ref-type="fig">3</xref>), it can be used to largely reduce the labour that trained pathologists need to put into image labelling and increase the efficiency of model training. In an active learning workflow involving interaction with a pathologist responsible for annotating whole slide images, the pathologist should be informed which classes are the most uncertain and prioritise them in subsequent annotation iterations (Fig. <xref rid="Fig1" ref-type="fig">1A</xref>). Our analysis involved a comparison of two different uncertainty measures, Entropy <italic>H</italic>, and BALD. The two measures agreed on which classes are most uncertain on average, pointing to classes which were most often misclassified by the model. The Entropy <italic>H</italic>, however, outperformed BALD as an acquisition function in the active learning workflow. Compared to random selection, <italic>H</italic> was able to speed-up the training process by a significant margin, while BALD performed only slightly better. Using <italic>H</italic>, the classification accuracy equal to that of the model trained with the full dataset was reached 45% faster. On top of that, we proposed a highly sensitive and specific approach for identification of mislabelled images in the training data as those which were misclassified by ARA-CNN with low uncertainty <italic>H</italic>. We showed that ARA-CNN is highly robust to such mislabelled training samples. To investigate how the pathological characteristics of images relate to their uncertainty measure <italic>H</italic>, we analysed pathological features of examples of highly certain and highly uncertain images. We observed that highly certain images are very good representatives of their class, while the highly uncertain ones are inconclusive and could have been annotated incorrectly when the dataset was constructed. This shows that measuring uncertainty is a good indicator of how well the model is trained and whether its predictions should be trusted without verification.</p>
    <p id="Par41">The excellent performance of ARA-CNN indicates that it is a step forward in establishing accurate and reliable machine learning models for histopathology. Based on such a model, further exciting avenues of research can be followed. As future work, we plan to apply our model to other histopathological tissue datasets. Due to its deep learning nature, our architecture should easily handle tissue types other than colorectal (potentially with the help of transfer learning). Furthermore, we plan more involved spatial analysis of segmented whole-slide images, especially in conjunction with clinical data. Our segmentation could facilitate application of methods that quantify spatial heterogeneity<sup><xref ref-type="bibr" rid="CR55">55</xref></sup> in histological samples of colorectal cancer, and improve our understanding of how tumour microenvironment influences the development of this disease. To this end, we plan to work on more precise segmentation algorithms, which will allow better understanding of spatial relations in analysed tissues.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <sec id="Sec19">
      <title>Supplementary information</title>
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="41598_2019_50587_MOESM1_ESM.pdf">
            <caption>
              <p>Supplementary Information</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p><bold>Supplementary information</bold> accompanies this paper at 10.1038/s41598-019-50587-1.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>We thank Łukasz Koperski for guidelines in interpreting the histopatological images.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author Contributions</title>
    <p>A.R. performed all experiments and data analysis and contributed to the model. M.M. developed the model architecture. M.M. and A.R. developed the implementation of the approach and prepared the visualisations. J.Z. performed the inspection of images with low and high uncertainty. E.S. supervised the research. M.M. and E.S. conceptualised the project. A.R. and E.S. wrote the manuscript. All authors reviewed the manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data Availability</title>
    <p>The model definition is available as open-source Python code on GitHub: <ext-link ext-link-type="uri" xlink:href="https://github.com/animgoeth/ARA-CNN">https://github.com/animgoeth/ARA-CNN</ext-link>.</p>
  </notes>
  <notes id="FPar1" notes-type="COI-statement">
    <title>Competing Interests</title>
    <p id="Par42">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fox</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Is H&amp;E morphology coming to an end?</article-title>
        <source>J. Clin. Pathol.</source>
        <year>2000</year>
        <volume>53</volume>
        <fpage>38</fpage>
        <lpage>40</lpage>
        <pub-id pub-id-type="doi">10.1136/jcp.53.1.38</pub-id>
        <?supplied-pmid 10767854?>
        <pub-id pub-id-type="pmid">10767854</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gurcan</surname>
            <given-names>MN</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Histopathological image analysis: a review</article-title>
        <source>IEEE Rev Biomed Eng</source>
        <year>2009</year>
        <volume>2</volume>
        <fpage>147</fpage>
        <lpage>171</lpage>
        <pub-id pub-id-type="doi">10.1109/RBME.2009.2034865</pub-id>
        <?supplied-pmid 20671804?>
        <pub-id pub-id-type="pmid">20671804</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Komura</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Ishikawa</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Machine Learning Methods for Histopathological Image Analysis</article-title>
        <source>Comput Struct Biotechnol J</source>
        <year>2018</year>
        <volume>16</volume>
        <fpage>34</fpage>
        <lpage>42</lpage>
        <pub-id pub-id-type="doi">10.1016/j.csbj.2018.01.001</pub-id>
        <?supplied-pmid 30275936?>
        <pub-id pub-id-type="pmid">30275936</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Madabhushi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Image analysis and machine learning in digital pathology: Challenges and opportunities</article-title>
        <source>Med Image Anal</source>
        <year>2016</year>
        <volume>33</volume>
        <fpage>170</fpage>
        <lpage>175</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2016.06.037</pub-id>
        <?supplied-pmid 27423409?>
        <pub-id pub-id-type="pmid">27423409</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Djuric</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Zadeh</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Aldape</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Diamandis</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Precision histology: how deep learning is poised to revitalize histomorphology for personalized cancer care</article-title>
        <source>NPJ Precis Oncol</source>
        <year>2017</year>
        <volume>1</volume>
        <fpage>22</fpage>
        <pub-id pub-id-type="doi">10.1038/s41698-017-0022-1</pub-id>
        <?supplied-pmid 29872706?>
        <pub-id pub-id-type="pmid">29872706</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Krizhevsky</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>GE</given-names>
          </name>
        </person-group>
        <article-title>Imagenet classification with deep convolutional neural network</article-title>
        <source>NIPS’12 Proceedings of the 25th International Conference on Neural Information Processing Systems</source>
        <year>2012</year>
        <volume>1</volume>
        <fpage>1097</fpage>
        <lpage>1105</lpage>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <mixed-citation publication-type="other">Russakovsky, O. <italic>et al</italic>. Imagenet large scale visual recognition challenge. <italic>International Journal of Computer Vision</italic> (2015).</mixed-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">Ciresan, D. C., Giusti, A., Gambardella, L. M. &amp; Schmidhuber, J. Deep neural networks segment neuronal membranes in electron microscopy images. <italic>NIPS 2012</italic> (2012).</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ciresan</surname>
            <given-names>DC</given-names>
          </name>
          <name>
            <surname>Giusti</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gambardella</surname>
            <given-names>LM</given-names>
          </name>
          <name>
            <surname>Schmidhuber</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Mitosis detection in breast cancer histology images with deep neural networks</article-title>
        <source>MICCAI LNCS</source>
        <year>2013</year>
        <volume>16</volume>
        <issue>Pt 2</issue>
        <fpage>411</fpage>
        <lpage>8</lpage>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liao</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Oto</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Representation learning: A unified deep learning framework for automatic prostate mr segmentation</article-title>
        <source>MICCAI LNCS</source>
        <year>2013</year>
        <volume>16</volume>
        <issue>Pt 2</issue>
        <fpage>254</fpage>
        <lpage>61</lpage>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cruz-Roa</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Arevalo</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Madabhushi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gonzalez</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>A deep learning architecture for image representation, visual interpretability and automated basal-cell carcinoma cancer detection</article-title>
        <source>MICCAI LNCS</source>
        <year>2013</year>
        <volume>16</volume>
        <issue>Pt 2</issue>
        <fpage>403</fpage>
        <lpage>10</lpage>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep learning based imaging data completion for improved brain disease diagnosis</article-title>
        <source>MICCAI LNCS</source>
        <year>2014</year>
        <volume>17</volume>
        <issue>Pt 3</issue>
        <fpage>305</fpage>
        <lpage>12</lpage>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Xie, Y., Xing, F., Kong, X., Su, H. &amp; Yang, L. Beyond classification: Structured regression for robust cell detection using convolutional neural network. <italic>MICCAI LNCS</italic> (2015).</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Gilmore</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Madabhushi</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>A deep convolutional neural network for segmenting and classifying epithelial and stromal regions in histopathological images</article-title>
        <source>Neurocomputing</source>
        <year>2016</year>
        <volume>191</volume>
        <fpage>214</fpage>
        <lpage>223</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neucom.2016.01.034</pub-id>
        <?supplied-pmid 28154470?>
        <pub-id pub-id-type="pmid">28154470</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Xu, J., Zhou, C., Lang, B. &amp; Liu, Q. Deep learning for histopathological image analysis: Towards computerized diagnosis on cancers. <italic>Advances in Computer Vision and Pattern Recognition</italic> (2017).</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sharma</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Zerbe</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Klempert</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Hellwich</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Hufnagl</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Deep convolutional neural networks for automatic classification of gastric carcinoma using whole slide images in digital histopathology</article-title>
        <source>Computerized Medical Imaging and Graphics</source>
        <year>2017</year>
        <volume>61</volume>
        <fpage>2</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="doi">10.1016/j.compmedimag.2017.06.001</pub-id>
        <?supplied-pmid 28676295?>
        <pub-id pub-id-type="pmid">28676295</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <mixed-citation publication-type="other">Qu, J. <italic>et al</italic>. Gastric pathology image classification using stepwise fine-tuning for deep neural networks. <italic>Journal of Healthcare Engineering</italic> (2018).</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Large scale tissue histopathology image classification, segmentation, and visualization via deep convolutional activation features</article-title>
        <source>BMC Bioinformatics</source>
        <year>2017</year>
        <volume>18</volume>
        <fpage>281</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-017-1685-x</pub-id>
        <?supplied-pmid 28549410?>
        <pub-id pub-id-type="pmid">28549410</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xing</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>An automatic learning-based framework for robust nucleus segmentation</article-title>
        <source>IEEE Transactions on Medical Imaging</source>
        <year>2016</year>
        <volume>35</volume>
        <fpage>550</fpage>
        <lpage>566</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2015.2481436</pub-id>
        <?supplied-pmid 26415167?>
        <pub-id pub-id-type="pmid">26415167</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sirinukunwattana</surname>
            <given-names>K</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Locality sensitive deep learning for detection and classification of nuclei in routine colon cancer histology images</article-title>
        <source>IEEE Transactions on Medical Imaging</source>
        <year>2016</year>
        <volume>35</volume>
        <fpage>1196</fpage>
        <lpage>1206</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2016.2525803</pub-id>
        <?supplied-pmid 26863654?>
        <pub-id pub-id-type="pmid">26863654</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Comprehensive analysis of lung cancer pathology images to discover tumor shape and boundary features that predict survival outcome</article-title>
        <source>Sci Rep</source>
        <year>2018</year>
        <volume>8</volume>
        <fpage>10393</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-018-27707-4</pub-id>
        <?supplied-pmid 29991684?>
        <pub-id pub-id-type="pmid">29991684</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Coudray</surname>
            <given-names>N</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Classification and mutation prediction from non-small cell lung cancer histopathology images using deep learning</article-title>
        <source>Nat. Med.</source>
        <year>2018</year>
        <volume>24</volume>
        <fpage>1559</fpage>
        <lpage>1567</lpage>
        <pub-id pub-id-type="doi">10.1038/s41591-018-0177-5</pub-id>
        <?supplied-pmid 30224757?>
        <pub-id pub-id-type="pmid">30224757</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Smith, L. &amp; Gal, Y. Understanding measures of uncertainty for adversarial example detection. <italic>CoRR</italic> abs/1803.08533 (2018).</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Gal, Y. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. <italic>Proceedings of the 33rd International Conference on Machine Learning</italic> (2016).</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Leibig</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Allken</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Ayhan</surname>
            <given-names>MS</given-names>
          </name>
          <name>
            <surname>Berens</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Wahl</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Leveraging uncertainty information from deep neural networks for disease detection</article-title>
        <source>Sci Rep</source>
        <year>2017</year>
        <volume>7</volume>
        <fpage>17816</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-017-17876-z</pub-id>
        <?supplied-pmid 29259224?>
        <pub-id pub-id-type="pmid">29259224</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Spanhol</surname>
            <given-names>FA</given-names>
          </name>
          <name>
            <surname>Oliveira</surname>
            <given-names>LS</given-names>
          </name>
          <name>
            <surname>Petitjean</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Heutte</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>A dataset for breast cancer histopathological image classification</article-title>
        <source>IEEE Transactions on Biomedical Engineering</source>
        <year>2016</year>
        <volume>63</volume>
        <fpage>1455</fpage>
        <lpage>1462</lpage>
        <pub-id pub-id-type="doi">10.1109/TBME.2015.2496264</pub-id>
        <?supplied-pmid 26540668?>
        <pub-id pub-id-type="pmid">26540668</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Han</surname>
            <given-names>Z</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Breast Cancer Multi-classification from Histopathological Images with Structured Deep Learning Model</article-title>
        <source>Sci Rep</source>
        <year>2017</year>
        <volume>7</volume>
        <fpage>4172</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-017-04075-z</pub-id>
        <?supplied-pmid 28646155?>
        <pub-id pub-id-type="pmid">28646155</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Bayramoglu, N., Kannala, J. &amp; Heikkilä, J. Deep learning for magnification independent breast cancer histopathology image classification. In <italic>2016 23rd International Conference on Pattern Recognition (ICPR)</italic>, 2440–2445, 10.1109/ICPR.2016.7900002 (2016).</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Kather, J. N. <italic>et al</italic>. Multi-class texture analysis in colorectal cancer histology. <italic>Scientific Reports</italic> (2016).</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">Ribeiro, M. G. <italic>et al</italic>. Classification of colorectal cancer based on the association of multidimensional and multiresolution features. <italic>Expert Systems With Applications</italic> (2019).</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Wang, C., Shi, J., Zhang, Q. &amp; Ying, S. Histopathological image classification with bilinear convolutional neural networks. <italic>2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</italic> 4050–4053 (2017).</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Pham, T. D. Scaling of texture in training autoencoders for classification of histological images of colorectal cancer. <italic>International Symposium on Neural Networks</italic> (2017).</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sarkar</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Acton</surname>
            <given-names>ST</given-names>
          </name>
        </person-group>
        <article-title>Sdl: Saliency-based dictionary learning framework for image similarity</article-title>
        <source>IEEE Transactions on Image Processing</source>
        <year>2018</year>
        <volume>27</volume>
        <fpage>749</fpage>
        <lpage>763</lpage>
        <pub-id pub-id-type="doi">10.1109/TIP.2017.2763829</pub-id>
        <?supplied-pmid 29053460?>
        <pub-id pub-id-type="pmid">29053460</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Ciompi, F. <italic>et al</italic>. The importance of stain normalization in colorectal tissue classification with convolutional networks. <italic>CoRR</italic> abs/1702.05931 (2017).</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Bishop</surname>
            <given-names>CM</given-names>
          </name>
        </person-group>
        <source>Pattern Recognition and Machine Learning (Information Science and Statistics)</source>
        <year>2006</year>
        <publisher-loc>Secaucus, NJ, USA</publisher-loc>
        <publisher-name>Springer-Verlag New York, Inc.</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Srivastava</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Krizhevsky</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Salakhutdinov</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Dropout: A simple way to prevent neural networks from overfitting</article-title>
        <source>Journal of Machine Learning Research</source>
        <year>2014</year>
        <volume>15</volume>
        <fpage>1929</fpage>
        <lpage>1958</lpage>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Nalisnik, M. <italic>et al</italic>. Interactive phenotyping of large-scale histology imaging data with HistomicsML. <italic>Scientific Reports</italic> (2017).</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Gal, Y., Islam, R. &amp; Ghahramani, Z. Deep bayesian active learning with image data. In <italic>ICML</italic> (2017).</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Doyle, S., Monaco, J., Feldman, M., Tomaszewski, J. &amp; Madabhushi, A. An active learning based classification strategy for the minority class problem: application to histopathology annotation. <italic>BMC Bioinformatics</italic> (2011).</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">Padmanabhan, R. K. <italic>et al</italic>. An active learning approach for rapid characterization of endothelial cells in human tumors. In <italic>PLoS One</italic> (2014).</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Metaxas</surname>
            <given-names>DN</given-names>
          </name>
        </person-group>
        <article-title>Scalable histopathological image analysis via active learning</article-title>
        <source>MICCAI LNCS</source>
        <year>2014</year>
        <volume>17</volume>
        <issue>Pt 3</issue>
        <fpage>369</fpage>
        <lpage>76</lpage>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>J-Y</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>EI-C</given-names>
          </name>
          <name>
            <surname>Lai</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Tu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Weakly supervised histopathology cancer image segmentation and classification</article-title>
        <source>Medical image analysis</source>
        <year>2014</year>
        <volume>18</volume>
        <issue>3</issue>
        <fpage>591</fpage>
        <lpage>604</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2014.01.010</pub-id>
        <?supplied-pmid 24637156?>
        <pub-id pub-id-type="pmid">24637156</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Shao, W., Sun, L. &amp; Zhang, D. Deep active learning for nucleus classification in pathology images. <italic>2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)</italic> 199–202 (2018).</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <mixed-citation publication-type="other">Du, B., Qi, Q., Zheng, H., Huang, Y. &amp; Ding, X. Breast cancer histopathological image classification via deep active learning and confidence boosting. <italic>Artificial Neural Networks and Machine Learning - ICANN 2018</italic> (2018).</mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <mixed-citation publication-type="other">Smailagic, A. <italic>et al</italic>. Medal: Deep active learning sampling method for medical image analysis. <italic>CoRR</italic> abs/1809.09287 (2018).</mixed-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <mixed-citation publication-type="other">Hou, L. <italic>et al</italic>. Patch-based convolutional neural network for whole slide tissue image classification. <italic>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic> 2424–2433 (2016).</mixed-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <mixed-citation publication-type="other">Houlsby, N., Huszár, F., Ghahramani, Z. &amp; Lengyel, M. Bayesian active learning for classification and preference learning. <italic>arXiv:1112</italic>.<italic>5745</italic> (2011).</mixed-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <mixed-citation publication-type="other">He, K., Xiangyu Zhang, S. R. &amp; Sun, J. Deep residual learning for image recognition. <italic>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic> 770–778 (2016).</mixed-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <mixed-citation publication-type="other">Redmon, J. &amp; Farhadi, A. Yolo9000: Better, faster, stronger. <italic>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic> 6517–6525 (2017).</mixed-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <mixed-citation publication-type="other">Ioffe, S. &amp; Szegedy, C. Batch normalization: accelerating deep network training by reducing internal covariate shift. <italic>ICML</italic>’<italic>15 Proceedings of the 32nd International Conference on International Conference on Machine Learning</italic> Volume 37 (2015).</mixed-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <mixed-citation publication-type="other">Gal, Y. &amp; Ghahramani, Z. Bayesian convolutional neural networks with Bernoulli approximate variational inference. <italic>CoRR</italic> abs/1506.02158 (2016).</mixed-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <mixed-citation publication-type="other">Kingma, D. P. &amp; Ba, J. Adam: A method for stochastic optimization. <italic>arXiv:1412</italic>.<italic>6980v9</italic> (2014).</mixed-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <mixed-citation publication-type="other">Kiureghian, A. D. &amp; Ditlevsen, O. Aleatory or epistemic? Does it matter? <italic>Structural Safety</italic> (2009).</mixed-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Brodley</surname>
            <given-names>CE</given-names>
          </name>
          <name>
            <surname>Friedl</surname>
            <given-names>MA</given-names>
          </name>
        </person-group>
        <article-title>Identifying mislabeled training data</article-title>
        <source>Journal Of Artificial Intelligence Research</source>
        <year>1999</year>
        <volume>11</volume>
        <fpage>131</fpage>
        <lpage>167</lpage>
        <pub-id pub-id-type="doi">10.1613/jair.606</pub-id>
      </element-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <mixed-citation publication-type="other">Yuan, Y. Spatial Heterogeneity in the Tumor Microenvironment. <italic>Cold Spring Harb Perspect Med</italic><bold>6</bold> (2016).</mixed-citation>
    </ref>
  </ref-list>
</back>
