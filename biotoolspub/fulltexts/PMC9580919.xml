<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Bioinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Bioinform</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Bioinform.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2673-7647</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9580919</article-id>
    <article-id pub-id-type="publisher-id">857577</article-id>
    <article-id pub-id-type="doi">10.3389/fbinf.2022.857577</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Bioinformatics</subject>
        <subj-group>
          <subject>Technology and Code</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>N-Tools-Browser: Web-Based Visualization of Electrocorticography Data for Epilepsy Surgery</article-title>
      <alt-title alt-title-type="left-running-head">Burkhardt et al.</alt-title>
      <alt-title alt-title-type="right-running-head">N-Tools-Browser for Epilepsy Surgery</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Burkhardt</surname>
          <given-names>Jay</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="fn1" ref-type="author-notes">
          <sup>†</sup>
        </xref>
        <uri xlink:href="https://loop.frontiersin.org/people/1633746/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Sharma</surname>
          <given-names>Aaryaman</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="fn1" ref-type="author-notes">
          <sup>†</sup>
        </xref>
        <uri xlink:href="https://loop.frontiersin.org/people/1641638/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Tan</surname>
          <given-names>Jack</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="fn1" ref-type="author-notes">
          <sup>†</sup>
        </xref>
        <uri xlink:href="https://loop.frontiersin.org/people/1746720/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Franke</surname>
          <given-names>Loraine</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <uri xlink:href="https://loop.frontiersin.org/people/1691120/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Leburu</surname>
          <given-names>Jahnavi</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <uri xlink:href="https://loop.frontiersin.org/people/1748325/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jeschke</surname>
          <given-names>Jay</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Devore</surname>
          <given-names>Sasha</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <uri xlink:href="https://loop.frontiersin.org/people/54859/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Friedman</surname>
          <given-names>Daniel</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <uri xlink:href="https://loop.frontiersin.org/people/1125238/overview"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Chen</surname>
          <given-names>Jingyun</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="c001" ref-type="corresp">*</xref>
        <uri xlink:href="https://loop.frontiersin.org/people/610762/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Haehn</surname>
          <given-names>Daniel</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <uri xlink:href="https://loop.frontiersin.org/people/1137150/overview"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Machine Psychology Lab</institution>, <institution>Department of Computer Science</institution>, <institution>University of Massachusetts Boston</institution>, <addr-line>Boston</addr-line>, <addr-line>MA</addr-line>, <country>United States</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Department of Neurology</institution>, <institution>New York University</institution>, <institution>Grossman School of Medicine</institution>, <addr-line>New York</addr-line>, <addr-line>NY</addr-line>, <country>United States</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p><bold>Edited by:</bold><ext-link xlink:href="https://loop.frontiersin.org/people/1083578/overview" ext-link-type="uri">Sean O'Donoghue</ext-link>, Garvan Institute of Medical Research, Australia</p>
      </fn>
      <fn fn-type="edited-by">
        <p><bold>Reviewed by:</bold><ext-link xlink:href="https://loop.frontiersin.org/people/307438/overview" ext-link-type="uri">Jan Egger</ext-link>, University Hospital Essen, Germany</p>
        <p><ext-link xlink:href="https://loop.frontiersin.org/people/1078714/overview" ext-link-type="uri">Rachel Sparks</ext-link>, King’s College London, United Kingdom</p>
      </fn>
      <corresp id="c001">*Correspondence: Jingyun Chen, <email>jingyun.chen@nyulangone.org</email>
</corresp>
      <fn fn-type="equal" id="fn1">
        <label>
          <sup>†</sup>
        </label>
        <p>These authors have contributed equally to this work</p>
      </fn>
      <fn fn-type="other">
        <p>This article was submitted to Data Visualization, a section of the journal Frontiers in Bioinformatics</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>21</day>
      <month>4</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>2</volume>
    <elocation-id>857577</elocation-id>
    <history>
      <date date-type="received">
        <day>18</day>
        <month>1</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>11</day>
        <month>3</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2022 Burkhardt, Sharma, Tan, Franke, Leburu, Jeschke, Devore, Friedman, Chen and Haehn.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Burkhardt, Sharma, Tan, Franke, Leburu, Jeschke, Devore, Friedman, Chen and Haehn</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Epilepsy affects more than three million people in the United States. In approximately one-third of this population, anti-seizure medications do not control seizures. Many patients pursue surgical treatment that can include a procedure involving the implantation of electrodes for intracranial monitoring of seizure activity. For these cases, accurate mapping of the implanted electrodes on a patient’s brain is crucial in planning the ultimate surgical treatment. Traditionally, electrode mapping results are presented in static figures that do not allow for dynamic interactions and visualizations. In collaboration with a clinical research team at a Level 4 Epilepsy Center, we developed N-Tools-Browser, a web-based software using WebGL and the X-Toolkit (XTK), to help clinicians interactively visualize the location and functional properties of implanted intracranial electrodes in 3D. Our software allows the user to visualize the seizure focus location accurately and simultaneously display functional characteristics (e.g., results from electrical stimulation mapping). Different visualization modes enable the analysis of multiple electrode groups or individual anatomical locations. We deployed a prototype of N-Tools-Browser for our collaborators at the New York University Grossman School of Medicine Comprehensive Epilepsy Center. Then, we evaluated its usefulness with domain experts on clinical cases.</p>
    </abstract>
    <kwd-group>
      <kwd>epilepsy</kwd>
      <kwd>electrode</kwd>
      <kwd>ECOG</kwd>
      <kwd>visualization</kwd>
      <kwd>electrocorticography</kwd>
      <kwd>seizure</kwd>
      <kwd>surgery</kwd>
      <kwd>webgl</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="s1">
    <title>1 Introduction</title>
    <p>In the United States, around one million people have epilepsy that cannot be controlled with anti-seizure medications (<xref rid="B5" ref-type="bibr">Engel, 2016</xref>; <xref rid="B24" ref-type="bibr">Zack and Kobau, 2017</xref>). Surgical intervention is often necessary for these patients and becomes the main path to treatment. Some surgical epilepsy patients undergo a two-stage surgical procedure during which 1) electrodes are implanted in the brain to help clinicians identify the location(s) of the seizure focus followed by 2) the resection or ablation of the pathological tissue. During the first stage of the surgery, electrical activity across broad brain regions is recorded through the implanted surface with depth electrodes. The seizure focus location is estimated based on patterns of abnormal brain activity during seizures. It is used to determine the resection or ablation boundaries in the second stage. Before resection, electrical stimulation mapping is typically performed to determine whether the identified areas are eloquent, i.e., functionally critical for language, memory, sensation, or motor responses, which would preclude them from a resective procedure. Thus, pre-surgical planning for the second stage of epilepsy surgery requires registration of epileptiform activity and functional mapping results on the patient’s brain.</p>
    <p>Planning the surgical approach to the treatment of drug-resistant focal epilepsy involves the integration of multiple sources of data to determine the brain areas to target for resection in order to maximize the chance of cure while minimizing the risks of neurological deficits. In many cases, neurologists and neurosurgeons employ intracranial electrodes to precisely identify sites of seizure initiation and spread as well as identify eloquent brain areas to avoid. In clinical practice, the spatial distribution of electrodes is first recorded manually by neurosurgeons, either via hand drawings or schematic plots (see example on the left of <xref rid="F1" ref-type="fig">Figure 1</xref>). Although these raw materials serve as essential references for surgical planning, they are not intended to convey the precise anatomical locations of these electrodes. The clinicians must then mentally translate this two-dimensional data onto the complex three-dimensional structure of the patient’s own brain, in order to determine if the observed data supports their localization hypothesis (based the presence of brain lesions or the anatomical-clinical manifestation of the seizure) and then plan the best surgical approach (<xref rid="B14" ref-type="bibr">Jobst et al., 2020</xref>). There are limited visualization approaches for this data and most require specialized software for viewing and annotation. Using post-operative Computed Tomography (CT) or magnetic resonance imaging (MRI) scans coregistered to the pre-operative MRI, the physical location of electrodes can be registered directly to patient brains. The visual results of electrode localization procedures typically contain schematics of electrodes overlaid on an individual patient’s brain (for subject-level analyses) or a template brain (for group-level analyses). The right panel of <xref rid="F1" ref-type="fig">Figure 1</xref> shows an example electrode localization that was generated by the open-source toolkit, N-Tools-Elec (<xref rid="B23" ref-type="bibr">Yang et al., 2012</xref>).</p>
    <fig position="float" id="F1">
      <label>FIGURE 1</label>
      <caption>
        <p>Different visualization methods for the same ECoG data set. Left: a schematic drawing. Right: a static 2D figure from N-Tools-elec.</p>
      </caption>
      <graphic xlink:href="fbinf-02-857577-g001" position="float"/>
    </fig>
    <p>N-Tools-Elec is a semi-automated Matlab toolkit developed by New York University (NYU) ‘s Comprehensive Epilepsy Center (CEC). It can accurately map electrode coordinates onto a 3D brain surface reconstructed from an MRI scan and determine the type of brain tissue where the electrodes are located. The N-Tools-Elec mapping algorithms were also implemented in the open-source toolkit iELVis, by Groppe et al. at the University of Toronto (<xref rid="B10" ref-type="bibr">Groppe et al., 2017</xref>).</p>
    <p>Traditionally, the visualization outputs of N-Tools-Elec are stored as static pictures, with a pre-set camera configuration without any functionality enabling the user to change the view or dynamically interact with the output. A web-based, easy to use visualization tool that can be deployed anywhere in the hospital–not only in the operating room to guide surgery but also at the patient bedside to facilitate education and counseling - would be extremely useful to clinicians. In this paper, we will introduce such a web-based visualization tool, N-Tools-Browser, that extends N-Tools-Elec and allows for real-time camera interaction in a web browser.</p>
    <p>N-Tools-Browser is designed to be a fully interactive and easy-to-use ECoG tool that runs directly in the browser and streamlines the electrode visualization process for clinicians involved in pre-surgical planning. N-Tools-Browser maps and displays electrodes (represented as spheres) on both a 3D brain surface that is reconstructed from an individual patient’s brain scan as well as 2D brain slices in three different anatomical positions: coronal, sagittal, and axial. By selecting a specific electrode in the 3D map, the 2D images automatically update to the correct anatomical position and all the relevant attributes for that electrode are displayed in the user interface. The workflow for N-Tools-Browser is divided into three steps.</p>
    <p>As shown in <xref rid="F2" ref-type="fig">Figure 2</xref>, the first step is the reconstruction, where a user runs N-Tools-Elec scripts in MATLAB and produces a brain surface mesh file (reconstructed from T1-weighted MRI via Freesurfer), and a JSON file containing electrode attributes and stimulation mapping. Second, the user runs a Python script on the patient’s JSON and NIfTI file to generate a label map for the different seizure types. Finally, the JSON, NIfTI, mesh files, and the label map are used to generate the 2D and 3D visualizations in N-Tools-Browser. We evaluated the satisfaction of clients and the impact of our tool through an expert study. Through a structured list of tasks and the NASA-TLX standard survey, two expert clinicians from the New York University School of Medicine provided us with qualitative feedback.</p>
    <fig position="float" id="F2">
      <label>FIGURE 2</label>
      <caption>
        <p>Workflow for N-Tools: In the first step, the user runs a patient’s electrode data through a MATLAB preprocessing pipeline to generate a T1 MRI NIfTI file, a JSON file, and a FreeSurfer Mesh file. In the second step, a Python script takes the patient’s JSON and NIfTI files and generates a label map of the different seizure types. Finally, the T1 MRI NIfTI file, the JSON file, and the label map are loaded into N-Tools-Browser for visualization.</p>
      </caption>
      <graphic xlink:href="fbinf-02-857577-g002" position="float"/>
    </fig>
  </sec>
  <sec id="s2">
    <title>2 Related Work</title>
    <p>Electrocorticography (ECoG) measures the electrical activity in the brain through electrodes that are placed directly on the cortical surface or in the brain parenchyma (<xref rid="B2" ref-type="bibr">Amaral et al., 2007</xref>). Visualization is a crucial element for processing ECoG data. While some stand-alone software tools are primarily used for visualization purposes (e.g., Visbrain (<xref rid="B3" ref-type="bibr">Combrisson et al., 2019</xref>)), the visualization modules are generally integrated into more comprehensive analysis packages, such as Brainstorm (<xref rid="B22" ref-type="bibr">Tadel et al., 2011</xref>), MNE (<xref rid="B9" ref-type="bibr">Gramfort et al., 2013</xref>), NeuralAct (<xref rid="B15" ref-type="bibr">Kubanek and Schalk, 2015</xref>), N-Tools-Elec (<xref rid="B23" ref-type="bibr">Yang et al., 2012</xref>), iELVis (<xref rid="B10" ref-type="bibr">Groppe et al., 2017</xref>), and ECoG ClusterFlow (<xref rid="B19" ref-type="bibr">Murugesan et al., 2017</xref>), etc. These conventional ECoG visualization software tools often require a local download and installation of a software package onto a workstation. While visualization on local computers has advantages in data safeguard and maintenance, it has limitations when multiple users, such as a team of neurosurgeons and epileptologists, need joint access.</p>
    <p>In order to address some of these limitations of traditional software, several web-based medical imaging tools have been proposed. Biomedisa, developed at Heidelberg University, is an example of an open-source medical imaging library which has been shown to handle a wide variety of volumetric data (<xref rid="B16" ref-type="bibr">Lösel et al., 2020</xref>). Another online tool, GradioHub, has been proposed as a collaborative way for clinicians and biomedical researchers to share and study data (<xref rid="B1" ref-type="bibr">Abid et al., 2020</xref>). Finally, the open-source tool Studierfenster has been recently been created for the purpose of biomedical data visualization, enabling rendering of both 3D and 2D data, as well as letting the user annotate the data they are examining (<xref rid="B4" ref-type="bibr">Egger et al., 2022</xref>). These tools represent a small fraction of the many web-based medical imaging libraries available.</p>
    <p>Recent efforts aimed to develop web-based visualization of neuroimaging data to enable real-time sharing between multiple users. For example, Mislap et al. at Johns Hopkins University (JHU) developed WebFM, a browser-based toolkit for brain-computer interfaces and functional brain mapping (<xref rid="B18" ref-type="bibr">Milsap et al., 2019</xref>). Also, at JHU, the Cognitive Neurophysiology and Brain-Machine Interface Lab developed an ECOG reconstruction suite (<ext-link xlink:href="https://github.com/cronelab/ReconstructionSuite" ext-link-type="uri">https://github.com/cronelab/ReconstructionSuite</ext-link>) that supports web-based visualization. Other tools such as RAVE (<xref rid="B17" ref-type="bibr">Magnotti et al., 2020</xref>) have been used for EEG data and some types of intracranial electrodes with a web interface.</p>
    <p>To enable web-based interfaces, the use of open-source Javascript libraries for 3D web-based visualization of medical imaging data is often required. An emerging number of visualization techniques and software tools have been developed for neuroscientific purposes, which may be one of the largest fields in medical imaging (<xref rid="B7" ref-type="bibr">Franke and Haehn, 2020</xref>). For example, the framework Brainbrowser (<ext-link xlink:href="https://brainbrowser.cbrain.mcgill.ca" ext-link-type="uri">https://brainbrowser.cbrain.mcgill.ca</ext-link>) is a well-established WebGL library with powerful graphical features (<xref rid="B21" ref-type="bibr">Sherif et al., 2015</xref>). However, it has restrictive requirements for input data, such as format and naming schemes. Compared to Brainbrowser, the A* Medical Imaging (AMI) toolkit (<ext-link xlink:href="https://github.com/FNNDSC/ami" ext-link-type="uri">https://github.com/FNNDSC/ami</ext-link>) (<xref rid="B20" ref-type="bibr">Rannou et al., 2017</xref>) is more flexible and easier to deploy, but development seems stalled based on Github commit activity.</p>
    <p>XTK was the first library for web-based medical image visualization and had a straightforward, open API (<ext-link xlink:href="https://github.com/xtk/X" ext-link-type="uri">https://github.com/xtk/X</ext-link>) (<xref rid="B11" ref-type="bibr">Haehn et al., 2014</xref>). Our N-Tools-Browser (which extends the NYU N-Tools-Elec toolkit) is a novel web-based visualization tool based on XTK. N-Tools-Browser is easily accessible via a web browser and contains a responsive user interface that is inspired by SliceDrop, a web-based viewer for medical imaging data (<xref rid="B12" ref-type="bibr">Haehn, 2013</xref>). Users can hover over and click on electrodes in 3D space. The UI responds by displaying relevant information about the particular electrode on a panel and moving the 2D slices to the selected location.</p>
  </sec>
  <sec id="s3">
    <title>3 Design Objectives</title>
    <p>We consulted with researchers and clinicians at the NYU CEC to develop the specific requirements for our development efforts. Core to our design principles of N-Tools-Browser is a simple, straightforward, and easy-to-use graphical user interface (GUI) for clinicians to access the information they need during pre-surgical planning quickly. We want clinicians to be able to look at the attributes of each electrode, such as the electrode ID, electrode type, and seizure type, to aid them in pinpointing the seizure focus. In semi-structured interviews with domain experts from the NYU CEC, we defined the following design objectives of our tool to create a useful visualization tool.<list list-type="simple"><list-item><p>D1) Create a 3D display of a specific patient, populating a brain mesh with sphere objects to represent electrodes on the patient’s brain and displaying colorful electrodes on 2D slices with a label map overlay.</p></list-item><list-item><p>D2) Provide functionality for switching between seizure types and functional mappings.</p></list-item><list-item><p>D3) Enable clinicians to correctly identify the anatomical region of a particular electrode along with other relevant information (ID, coordinates, interictal population, seizure type, functional mapping, etc.).</p></list-item></list>
</p>
  </sec>
  <sec id="s4">
    <title>4 Implementation</title>
    <p>To be consistent with our design objectives, we aimed for a simple and user-friendly implementation that facilitates the tasks of domain experts, surgeons, and clinicians.</p>
    <sec id="s4-1">
      <title>4.1 Electrode Visualization on 3D Brain Surface</title>
      <p>N-Tools-Browser uses the X-Toolkit (XTK) API(<xref rid="B11" ref-type="bibr">Haehn et al., 2014</xref>) for 3D visualization as it offers many built-in features for working with medical imaging data. XTK can load and parse NIfTI images and mesh surfaces and then render them in 3D space. XTK also supports basic geometric shapes, such as spheres and cylinders, which we use to represent electrodes and functional mappings, respectively. The rendering process begins by using XTK’s volume and mesh objects which can render NIfTI and mesh surface files from FreeSurfer (<xref rid="B6" ref-type="bibr">Fischl, 2012</xref>). The files can be stored either locally or remotely on a server. These objects are then added to four different renderers. The first renderer handles the displaying of 3D objects on the scene, whereas the remaining renderers handle the 2D slices of the NIfTI file in the saggital, horizontal, and coronal planes.</p>
      <p>A JSON file containing a series of arrays is loaded after initializing all the renderers. The arrays all have the same length as the number of electrodes in the data set. Each array index corresponds to a different electrode, i.e., the 0th index in each array corresponds with the first electrode in the data-set. Each array also stores additional electrode attributes, such as the electrodes identification number, coordinates, group type (grid, strip, depth, etc.), associated seizure type (1, 2a, 2b, etc.), and attribute (onset, early spread, etc.), interictal population, as well as different functional mappings (language, motor, etc.). The JSON is then parsed, and each electrode is mapped to an XTK sphere object and rendered onto the scene (D1).</p>
      <p>The JSON file also contains an array for each seizure type of a particular subject. Each type of seizure activity is mapped onto a specific color, and a legend showing this mapping is displayed in the UI. <xref rid="F3" ref-type="fig">Figure 3</xref> provides an example of this legend for a particular seizure type, and <xref rid="F4" ref-type="fig">Figure 4</xref> shows electrodes of type “onset” mapped to a brain surface after loading.</p>
      <fig position="float" id="F3">
        <label>FIGURE 3</label>
        <caption>
          <p>Full view of the user interface after loading the data set of a patient. Detailed view of the GUI control panel to adjust different visualization properties of the brain mesh and 2D slices.</p>
        </caption>
        <graphic xlink:href="fbinf-02-857577-g003" position="float"/>
      </fig>
      <fig position="float" id="F4">
        <label>FIGURE 4</label>
        <caption>
          <p>Population of electrodes on the brain in 3D space (left) and on a 2D slice (right) in the <italic>x</italic>-direction. The red electrodes represent the “onset” attributes of associated seizure type, while white indicates no seizure type is associated to the electrode.</p>
        </caption>
        <graphic xlink:href="fbinf-02-857577-g004" position="float"/>
      </fig>
    </sec>
    <sec id="s4-2">
      <title>4.2 Electrode Visualization on 2D Slices</title>
      <p>To implement the two-dimensional visualization of electrodes on brain slices, we used a Python script that reads the JSON data and creates a label map for each seizure type. Using the Python script to generate label maps enables the user to switch between them more efficiently. Since the label map is essentially a static overlay to the original scan, the label maps and original scan can exist in the same renderer, eliminating the need for multiple renderers. Generating the label map data also ensures the data will be ready to use from launch, and not slowly re-generated during each rendering. Each dimension of the array represents a two-dimensional slice. The coordinates, which appear on an interval from -127.5 to 127.5, must be placed in a slice indexed from 0 to total number of slices in the data. We do this with the following equation that takes as input an input range [<italic>a</italic>, <italic>b</italic>], an output range [<italic>c</italic>, <italic>d</italic>], and a coordinate:</p>
      <p>
        <inline-formula id="inf1">
          <mml:math id="m1" overflow="scroll">
            <mml:msub>
              <mml:mrow>
                <mml:mi>s</mml:mi>
              </mml:mrow>
              <mml:mrow>
                <mml:mi>i</mml:mi>
              </mml:mrow>
            </mml:msub>
            <mml:mo>=</mml:mo>
            <mml:mfrac>
              <mml:mrow>
                <mml:mi>b</mml:mi>
                <mml:mo>−</mml:mo>
                <mml:mi>a</mml:mi>
              </mml:mrow>
              <mml:mrow>
                <mml:mi>d</mml:mi>
                <mml:mo>−</mml:mo>
                <mml:mi>c</mml:mi>
              </mml:mrow>
            </mml:mfrac>
            <mml:mrow>
              <mml:mo stretchy="false">(</mml:mo>
              <mml:mrow>
                <mml:mi>c</mml:mi>
                <mml:mi>o</mml:mi>
                <mml:mi>o</mml:mi>
                <mml:mi>r</mml:mi>
                <mml:mi>d</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>n</mml:mi>
                <mml:mi>a</mml:mi>
                <mml:mi>t</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mo>−</mml:mo>
                <mml:mi>c</mml:mi>
              </mml:mrow>
              <mml:mo stretchy="false">)</mml:mo>
            </mml:mrow>
            <mml:mo>+</mml:mo>
            <mml:mi>a</mml:mi>
          </mml:math>
        </inline-formula>
      </p>
      <p>In this equation, <italic>s</italic>
<sub><italic>i</italic></sub> refers to the section of the MRI scan on which a 3D coordinate is mapped. In our case, the input contains the possible range for a coordinate, and the output includes the number of slices. For example, a coordinate with value -32.7 on the interval [ − 127.5, 127.5] will be matched with slice number 95 on the interval [0, 255] when rounded to the nearest integer. After calculating an index for each coordinate, we change the value of the array at that index to a predetermined constant matching a specific electrode color, and the array is saved as a NIfTI file. The volume parser in XTK already has functionality for adding label maps, so this new NIfTI file is loaded on top of the already existing file. This allows for electrodes to appear on slices without any additional rendering as they now appear as one image (D2/D3).</p>
    </sec>
    <sec id="s4-3">
      <title>4.3 User Interface</title>
      <p>For best usability, we designed our user interface to show all the relevant information of an electrode in one panel. <xref rid="F5" ref-type="fig">Figure 5</xref> shows an example where a user has highlighted electrode G27, as well as a functional map in the category of <italic>“motor”</italic>. The information regarding the seizure type of an electrode is displayed, and the note for the selected functional map. Since G27 has no seizure type, no caption is displayed for seizure type or interictal population. New connections will be drawn if the user selects an alternative functional map. The user can also press the <italic>“Show All Tags”</italic> button to display the caption of each electrode, as seen in <xref rid="F6" ref-type="fig">Figure 6</xref>. Finally, <xref rid="F7" ref-type="fig">Figure 7</xref> shows the complete finished rendering of three subjects.</p>
      <fig position="float" id="F5">
        <label>FIGURE 5</label>
        <caption>
          <p>Information displayed about an electrode (G27) and functional mapping when clicked by a user. Selecting a functional map from the drop down menu will display the connected electrode pairs involved.</p>
        </caption>
        <graphic xlink:href="fbinf-02-857577-g005" position="float"/>
      </fig>
      <fig position="float" id="F6">
        <label>FIGURE 6</label>
        <caption>
          <p>Brain surface mesh with all electrode IDs displayed simultaneously.</p>
        </caption>
        <graphic xlink:href="fbinf-02-857577-g006" position="float"/>
      </fig>
      <fig position="float" id="F7">
        <label>FIGURE 7</label>
        <caption>
          <p>Final browser-based visualization of three different subjects NY704, NY758, NY836 including localization of electrodes on the brain surface. The color of each electrode represents the different attribute (onset, early spread, etc.) for the corresponding seizure type.</p>
        </caption>
        <graphic xlink:href="fbinf-02-857577-g007" position="float"/>
      </fig>
      <p>We added WebGL’s dat. GUI for direct visibility and to enable interactive manipulation for the user. We separate the electrode and functional map data from the slice and mesh controllers by using two panels. <xref rid="F4" ref-type="fig">Figure 4</xref> shows the one panel in two segments responsible for controlling the slices and hemisphere meshes. Users can use this panel to select a specific slice number, control the opacity of the slices and hemisphere, and toggle visibility of the slices. This makes it easier to find needed information more quickly.</p>
    </sec>
    <sec id="s4-4">
      <title>4.4 Setup, Installation, and Deployment</title>
      <p>N-Tools-Browser is designed to run client-side only and can be used with any web server. The installation and setup of N-Tools-Browser involve specific steps associated with the overall workflow established in <xref rid="F2" ref-type="fig">Figure 2</xref>. A user first runs the N-Tools-Elec pipeline on a local workstation, generating a brain surface file (reconstructed from an input T1-weighted MRI <italic>via</italic> Freesurfer) and a JSON file containing electrode information (see section Electrode Visualization on 3D brain surface). All three, the T1 MRI, Freesurfer mesh file, and JSON file, are loaded into then uploaded to the patient’s directory on a webserver. For web security and health information protection, the T1 MRI is defaced, and the patient’s directory contains no personal information other than the patient ID. In the case of NYU, the webserver is also protected by campus firewall, accessible to authorized users only. The user should browse to the Github repository for N-Tools-Browser, which can be found at <ext-link xlink:href="https://github.com/ntoolsbrowser/ntoolsbrowser.github.io/" ext-link-type="uri">https://github.com/ntoolsbrowser/ntoolsbrowser.github.io/</ext-link>, then make a copy of the source code by forking the repository.</p>
      <p>{</p>
      <p>“subjID”:“fsMNI”,</p>
      <p>“totalSeizType”:2, “SeizDisplay”[“SeizureType3″,“SeizureType4″,“intPopulation”,“funMapping”], </p>
      <p>“elecID” [“G01″,“G02″,“G03″,“G04″,“G05″,“G06″,“G07″,“G08″,“G09″, ..., “G64”],</p>
      <p>“coorX” [−37.5, −47.5, −55.83333, −59.3, −62.5, −65.3, −65.5, −65, −40.5, ..., −59.9],</p>
      <p>“coorY” [−41.5, −41, −39.16667, −38.9, −37.16667, −35.3,−34.5, −33,−31.5, ..., 41.5],</p>
      <p>“coorZ” [47.5, 38, 26.83333, 16.3, 2.83333, −8.1, −19.5, −33, 49.5, 38.83333, ..., −22.5],</p>
      <p>“elecType” [“G″,“G″,“G″,“G″,“G″,“G″,“G″,“G″,“G″, ..., “G”],</p>
      <p>“intPopulation” [2,2,0,0,0,0,6,0,0, ..., 0],</p>
      <p>...</p>
      <p>“fmapMotor”[”“,”“,”“,“Right facial (chin, lip)deviation”,”“,”“, ...,””],</p>
      <p>...</p>
      <p>}</p>
      <p>
        <statement content-type="listing" id="Listing_1">
          <label>Listing 1</label>
          <p>A small segment of what the generated JSON file looks like. The “...” in the arrays indicate additional values not shown, as the arrays in the actual JSON can contain hundreds of elements.</p>
          <p>A core intermediate step in the workflow is generating a label map containing patients’ electrode locations mapped onto 2D brain slices. This step is achieved via a Python script, which takes the electrode’s location in 3D, calculates the 2D position, and maps the electrodes onto brain slices in the coronal, sagittal, and axial positions. This step must be completed before visualization in N-Tools-Browser. N-Tools-Browser also retrieves its inputs from the same directory. Therefore, we set the script to output the label map to the patient’s subject folder located in the parent directory stored the script on the webserver. To run the script, a user would first navigate to the umb_ntools directory where the Python script, electrode_slice.py, is located using the cd command. The user would then run the script by providing the following three arguments in the command-line: 1) SubjectID, 2) Path to subject’s.json file, and 3) Path to subject’s T1. nii file. Both pathnames can either be an absolute pathname or a relative pathname.</p>
          <p>$ cd p r e p r o c e s i n g.</p>
          <p>$./electrode_slice.py fsMNI .. /fsMNI/fsMNI.json .. /fsMNI/fsMNI_T1.nii</p>
        </statement>
      </p>
      <p>
        <statement content-type="listing" id="Listing_2">
          <label>Listing 2</label>
          <p>An example of running the script for subject ID <italic>“fsMNI.”</italic>
</p>
          <p>After running the Python script, depending on the subject’s seizure types, one or more NIfTI files are generated in the subject’s directory. By default, if the seizure type is labeled “funMapping,” then a file with the name <italic>subject_default_labels.nii</italic> would be saved. For all other seizure types, a file with the name <italic>subject_seizType_labels.nii</italic> would be saved. Following the label map generation, a user at NYU could then proceed to N-Tools-Browser, select NYU mode, and enter the subject ID to load all the relevant files for visualization.</p>
          <p>A demo of N-Tools can be found at the following link: <ext-link xlink:href="https://ntoolsbrowser.github.io/" ext-link-type="uri">https://ntoolsbrowser.github.io/</ext-link>. Two sample datasets are available for the demo: <italic>fsaverage</italic> and <italic>fsMNI</italic>. Users can enter either dataset into the subject_ID box, select UMB mode, and click on <italic>“Load Data!”</italic> to load the sample files. Although N-Tools currently only supports structural MRI data at this time, the open source nature of the tool allows future support of additional formats and modalities.</p>
        </statement>
      </p>
    </sec>
  </sec>
  <sec id="s5">
    <title>5 Evaluation</title>
    <p>Based on our design objectives defined in <xref rid="s3" ref-type="sec">Section 3</xref>, we evaluated the usability of the user interface with two domain experts. We selected two expert clinicians from the NYU School of Medicine who are familiar with working with various software tools for epilepsy research. Expert one is a 46-year-old male, and expert two is a 45-year-old female, both of whom are experienced neuroscientists performing tasks involving the extraction or analysis of data from visualization. From the design objectives, we derived a structured list of tasks for our experts to evaluate our tool, which is described in <xref rid="s5-1" ref-type="sec">Section 5.1</xref>.</p>
    <p>Besides the tasks, we additionally used the standard NASA-TLX survey (<xref rid="B13" ref-type="bibr">Hart and Staveland, 1988</xref>) to assess the workload with tasks concerning mental demand, frustration, etc. We also asked some general usability questions on how the users perceived the overall usability and some open-ended questions to collect user comments as qualitative feedback.</p>
    <sec id="s5-1">
      <title>5.1 Tasks</title>
      <p>After the introduction, we asked our domain experts to perform the following list of tasks during the user study in two rounds with two different subjects (full instructions are provided as Supplemental Material):</p>
      <p>
        <statement content-type="task" id="Task_1">
          <label>Task 1</label>
          <p>Search and load subject NY704 (training) and subject NY836.</p>
        </statement>
      </p>
      <p>
        <statement content-type="task" id="Task_2">
          <label>Task 2</label>
          <p>Identify which electrodes are connected in the functional mapping group named “motor”.</p>
        </statement>
      </p>
      <p>
        <statement content-type="task" id="Task_3">
          <label>Task 3</label>
          <p>Find the coordinates of electrode <italic>“G56”</italic> (training) and <italic>“G008”</italic>, and identify the anatomical region.</p>
          <p>Our tasks were structured with increasing difficulty. Same as in a real-world scenario, Task one had the goal to search and load specific data in which a domain scientist might be interested. Next, Task two was concerned with identifying the functional mapping. Task three was particularly for experienced neuroscientists, finding an anatomical region.</p>
        </statement>
      </p>
    </sec>
    <sec id="s5-2">
      <title>5.2 User Study and Results</title>
      <p>We performed our user study virtually in separate Zoom meetings with our domain experts. The meetings took around 15 min per expert. We began by asking the experts to open the tool in the web browser and explaining the main functionalities. Then, we gave them 5 min to explore the tool to become familiar with its functionality and ask any questions. Once this short period was complete, we performed two rounds of three tasks.</p>
      <p>Task one had the quickest completion time since it simply involved searching and loading the different subjects. The subject used after the training round contained significantly more data than the first, which accounts for the time difference for the two rounds. We observed that Task two took most of the time since the experts occasionally had to change camera angles or make particular 3D objects invisible to obtain a better view. Both experts had a much easier time with Task 3, which involved finding an electrode and identifying its anatomical region. They quickly found this information, such as the electrode coordinates, by selecting an electrode from the drop-down menu and then using their anatomical knowledge to identify the appropriate anatomical region A task summary along with completion times is shown in <xref rid="T1" ref-type="table">Table 1</xref>.</p>
      <table-wrap position="float" id="T1">
        <label>TABLE 1</label>
        <caption>
          <p>We measured how fast two experts can perform common visual exploration tasks with N-Tools-Browser. After a 5 min period of voice-guided training, the experts were asked to perform the tasks on another previously unseen dataset without help. Both experts were able to successfully complete the tasks.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead valign="top">
            <tr>
              <th align="left" rowspan="1" colspan="1">Task</th>
              <th align="center" rowspan="1" colspan="1">Expert 1 Time [s]</th>
              <th align="center" rowspan="1" colspan="1">Expert 2 Time [s]</th>
              <th align="center" rowspan="1" colspan="1">Average Total <bold>[s]</bold>
</th>
            </tr>
          </thead>
          <tbody valign="top">
            <tr>
              <td align="left" rowspan="1" colspan="1">Task 1 (Search and Load), <italic>training</italic>
</td>
              <td align="char" char="." rowspan="1" colspan="1">3</td>
              <td align="char" char="." rowspan="1" colspan="1">5</td>
              <td align="char" char="." rowspan="1" colspan="1">4</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Task 2 (Identify Functional Mapping), <italic>training</italic>
</td>
              <td align="char" char="." rowspan="1" colspan="1">88</td>
              <td align="char" char="." rowspan="1" colspan="1">25</td>
              <td align="char" char="." rowspan="1" colspan="1">56.5</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Task 3 (Find Anatomical Region), <italic>training</italic>
</td>
              <td align="char" char="." rowspan="1" colspan="1">10</td>
              <td align="char" char="." rowspan="1" colspan="1">36</td>
              <td align="char" char="." rowspan="1" colspan="1">23</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Task 1 (Search and Load)</td>
              <td align="char" char="." rowspan="1" colspan="1">13</td>
              <td align="char" char="." rowspan="1" colspan="1">10</td>
              <td align="char" char="." rowspan="1" colspan="1">11.5</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Task 2 (Identify Functional Mapping)</td>
              <td align="char" char="." rowspan="1" colspan="1">43</td>
              <td align="char" char="." rowspan="1" colspan="1">60</td>
              <td align="char" char="." rowspan="1" colspan="1">51.5</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Task 3 (Find Anatomical Region)</td>
              <td align="char" char="." rowspan="1" colspan="1">13</td>
              <td align="char" char="." rowspan="1" colspan="1">20</td>
              <td align="char" char="." rowspan="1" colspan="1">16.5</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>Our supplemental questionnaire had our experts rate their experience with different features on a Likert scale from one to 7. The 7-point ordinal scale allows the respondents to rate the degree to which they agree or disagree with a statement with one being <italic>“totally disagree”</italic> and seven being <italic>“totally agree.“</italic>. Overall, we found that their responses indicated that they found it easy to fulfill their tasks and were confident with having the correct results, both experts rating either a six or 7 (out of 7) for each. They also strongly or totally agreed that both the 2D and 3D visualizations were helpful and understandable.</p>
      <p>The qualitative feedback from the experts was positive, mentioning <italic>“we eagerly anticipate using this program!“</italic>, <italic>“this is very cool”</italic>, and <italic>“I like the tool.“</italic>. One expert, when commenting on the functional mapping visualization and captioning, said <italic>“I really like that you can add the actual findings.“</italic>. The user also liked the hovering option on the electrodes to show the motor functions that are then visible in the labels. They also commented on possible additional features or improvements. For example, since both experts started Task two and Task three by making the rendered 3D slices invisible, they suggested that this should be the default setting after loading. Other suggestions included <italic>“a cursor or other marker should identify the selected electrode in the 2D slices,”</italic> and using less significant digits when displaying the 3D coordinates of an electrode.</p>
      <p>The NASA TLX workload assessment showed that experts found it overall easy to fulfill the tasks. The scale is ranked from 0 to 21 where 0 depicts a lower perceived workload, and 21 a higher perceived workload. Both experts rated the mental, physical, and temporal demand as low, with the highest being a report of eight out of the 21-point scale for cognitive demand. When assessing the performance of the tool, the question <italic>“How successful were you in accomplishing what you were asked to do?”</italic> was asked, and both experts responded in the high to very high range, with scores of 16 and 20 (out of 21) The complete feedback from both questionnaires can be seen in <xref rid="F8" ref-type="fig">Figure 8</xref>.</p>
      <fig position="float" id="F8">
        <label>FIGURE 8</label>
        <caption>
          <p>Left: We collected the qualitative feedback from neuroscientists after evaluating the N-Tool-Browser. The experts gave positive feedback on the scale range from one to seven, and were satisfied with their results. Right: We also performed the standardized NASA-TLX questionnaire and the experts reported low frustration and high performance on the scale range from 1–21.</p>
        </caption>
        <graphic xlink:href="fbinf-02-857577-g008" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec id="s6">
    <title>6 Conclusion and Outlook</title>
    <p>We present N-Tools-Browser as an extension of the pre-existing N-Tools-Elec ECoG toolkit. As web-based medical imaging software, N-Tools-Browser allows neuroscientists to explore the toolkit output without any additional software installation. Our application also allows quick access to electrodes’ positions, functional units, and anatomical information.</p>
    <p>Being a prototype, N-Tools-Browser also has some limitations. First, the pre-processing pipeline that generates input data for N-Tools-Browser contains both MATLAB and Python scripts (see <xref rid="s4-4" ref-type="sec">Section 4.4</xref>). These extra required pre-processing steps may present challenges to clinicians who may not be experienced with the how to run the MATLAB pipeline or Pythons script to generate the necessary outputs As Python packages for ECoG data processing are emerging (<xref rid="B3" ref-type="bibr">Combrisson et al. (2019)</xref>; <xref rid="B9" ref-type="bibr">Gramfort et al. (2013)</xref>; <xref rid="B19" ref-type="bibr">Murugesan et al. (2017)</xref>), we believe that a pure Python pipeline would be easier to maintain and update. Second, while the input NIfTI and Freesurfer surface files are in a standard format, the format of the JSON electrode profile is specific to the installation at NYU CEC. Therefore, data reformatting may be required to use this toolkit at other centers. Future work involves implementing the standardized BIDS format (<xref rid="B8" ref-type="bibr">Gorgolewski et al., 2016</xref>) to interface other neuroimaging tools with N-Tools-Browser as well as converting the custom MATLAB pipeline entirely to Python code which runs directly in N-Tools-Browser as the data is loaded. N-Tools-Browser is not certified for clinical use and should be used for research purposes only.</p>
  </sec>
</body>
<back>
  <ack>
    <p>Portions of this research were supported by Finding a Cure for Epilepsy and Seizures (NYU FACES). The authors would also like to thank Xiuyuan Wang at Weill Cornell Medicine for his suggestions and comments on software design.</p>
  </ack>
  <sec sec-type="data-availability" id="s7">
    <title>Data Availability Statement</title>
    <p>Publicly available datasets were analyzed in this study. This data can be found here: fsaverage and fMNI datasets used for training only <ext-link xlink:href="https://github.com/ntoolsbrowser/ntoolsbrowser.github.io/tree/main/data" ext-link-type="uri">https://github.com/ntoolsbrowser/ntoolsbrowser.github.io/tree/main/data</ext-link>.</p>
  </sec>
  <sec id="s8">
    <title>Author Contributions</title>
    <p>JB, AS, and JT developed the prototype. LF, JL, and SD helped with the manuscript, DF is our clinical collaborator, JC and DH supervised the research.</p>
  </sec>
  <sec sec-type="COI-statement" id="s9">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec sec-type="disclaimer" id="s10">
    <title>Publisher’s Note</title>
    <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
  </sec>
  <sec id="s11">
    <title>Supplementary Material</title>
    <p>The Supplementary Material for this article can be found online at: <ext-link xlink:href="https://www.frontiersin.org/articles/10.3389/fbinf.2022.857577/full#supplementary-material" ext-link-type="uri">https://www.frontiersin.org/articles/10.3389/fbinf.2022.857577/full#supplementary-material</ext-link>
</p>
    <supplementary-material id="SM1" position="float" content-type="local-data">
      <media xlink:href="DataSheet2.PDF">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="SM2" position="float" content-type="local-data">
      <media xlink:href="DataSheet3.PDF">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="SM3" position="float" content-type="local-data">
      <media xlink:href="DataSheet1.PDF">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="SM4" position="float" content-type="local-data">
      <media xlink:href="Video1.MP4">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abid</surname><given-names>A.</given-names></name><name><surname>Abdalla</surname><given-names>A.</given-names></name><name><surname>Abid</surname><given-names>A.</given-names></name><name><surname>Khan</surname><given-names>D.</given-names></name><name><surname>Alfozan</surname><given-names>A.</given-names></name><name><surname>Zou</surname><given-names>J.</given-names></name></person-group> (<year>2020</year>). <article-title>An Online Platform for Interactive Feedback in Biomedical Machine Learning</article-title>. <source>Nat. Mach Intell.</source>
<volume>2</volume>, <fpage>86</fpage>–<lpage>88</lpage>. <pub-id pub-id-type="doi">10.1038/s42256-020-0147-8</pub-id>
</mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Amaral</surname><given-names>P.</given-names></name><name><surname>Paulo</surname><given-names>J.</given-names></name><name><surname>Cunha</surname><given-names>S.</given-names></name><name><surname>Dias</surname><given-names>P.</given-names></name><name><surname>Maria</surname><given-names>J.</given-names></name></person-group> (<year>2007</year>). <source>Multimodal Application for Visualization and Manipulation of Electrocorticography Data</source>. <publisher-loc>Portugal</publisher-loc>: <publisher-name>Porto Salvo</publisher-name>. </mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Combrisson</surname><given-names>E.</given-names></name><name><surname>Vallat</surname><given-names>R.</given-names></name><name><surname>O'Reilly</surname><given-names>C.</given-names></name><name><surname>Jas</surname><given-names>M.</given-names></name><name><surname>Pascarella</surname><given-names>A.</given-names></name><name><surname>Saive</surname><given-names>A. L.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>Visbrain: a Multi-Purpose Gpu-Accelerated Open-Source Suite for Multimodal Brain Data Visualization</article-title>. <source>Front. Neuroinformatics</source>
<volume>13</volume>, <fpage>14</fpage>. <pub-id pub-id-type="doi">10.3389/fninf.2019.00014</pub-id>
</mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Egger</surname><given-names>J.</given-names></name><name><surname>Wild</surname><given-names>D.</given-names></name><name><surname>Weber</surname><given-names>M.</given-names></name><name><surname>Bedoya</surname><given-names>C. A. R.</given-names></name><name><surname>Karner</surname><given-names>F.</given-names></name><name><surname>Prutsch</surname><given-names>A.</given-names></name><etal/></person-group> (<year>2022</year>). <article-title>Studierfenster: an Open Science Cloud-Based Medical Imaging Analysis Platform</article-title>. <source>J. Digital Imaging</source>
<volume>2022</volume>, <fpage>1</fpage>–<lpage>16</lpage>. <pub-id pub-id-type="doi">10.1007/s10278-021-00574-8</pub-id>
</mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engel</surname><given-names>J.</given-names></name></person-group> (<year>2016</year>). <article-title>What Can We Do for People with Drug-Resistant Epilepsy?: The 2016 Wartenberg Lecture</article-title>. <source>Neurology</source>
<volume>87</volume>, <fpage>2483</fpage>–<lpage>2489</lpage>. <pub-id pub-id-type="doi">10.1212/WNL.0000000000003407</pub-id>
<pub-id pub-id-type="pmid">27920283</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname><given-names>B.</given-names></name></person-group> (<year>2012</year>). <article-title>Freesurfer</article-title>. <source>Neuroimage</source>
<volume>62</volume>, <fpage>774</fpage>–<lpage>781</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.021</pub-id>
<pub-id pub-id-type="pmid">22248573</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franke</surname><given-names>L.</given-names></name><name><surname>Haehn</surname><given-names>D.</given-names></name></person-group> (<year>2020</year>). <article-title>Modern Scientific Visualizations on the Web</article-title>. <source>Informatics</source>
<volume>7</volume>, <fpage>37</fpage>. <pub-id pub-id-type="doi">10.3390/informatics7040037</pub-id>
</mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>K. J.</given-names></name><name><surname>Auer</surname><given-names>T.</given-names></name><name><surname>Calhoun</surname><given-names>V. D.</given-names></name><name><surname>Craddock</surname><given-names>R. C.</given-names></name><name><surname>Das</surname><given-names>S.</given-names></name><name><surname>Duff</surname><given-names>E. P.</given-names></name><etal/></person-group> (<year>2016</year>). <article-title>The Brain Imaging Data Structure, a Format for Organizing and Describing Outputs of Neuroimaging Experiments</article-title>. <source>Scientific data</source>
<volume>3</volume>, <fpage>1</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1038/sdata.2016.44</pub-id>
</mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gramfort</surname><given-names>A.</given-names></name><name><surname>Luessi</surname><given-names>M.</given-names></name><name><surname>Larson</surname><given-names>E.</given-names></name><name><surname>Engemann</surname><given-names>D. A.</given-names></name><name><surname>Strohmeier</surname><given-names>D.</given-names></name><name><surname>Brodbeck</surname><given-names>C.</given-names></name><etal/></person-group> (<year>2013</year>). <article-title>Meg and Eeg Data Analysis with Mne-python</article-title>. <source>Front. Neurosci.</source>
<volume>7</volume>, <fpage>267</fpage>. <pub-id pub-id-type="doi">10.3389/fnins.2013.00267</pub-id>
<pub-id pub-id-type="pmid">24431986</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groppe</surname><given-names>D. M.</given-names></name><name><surname>Bickel</surname><given-names>S.</given-names></name><name><surname>Dykstra</surname><given-names>A. R.</given-names></name><name><surname>Wang</surname><given-names>X.</given-names></name><name><surname>Mégevand</surname><given-names>P.</given-names></name><name><surname>Mercier</surname><given-names>M. R.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>Ielvis: An Open Source Matlab Toolbox for Localizing and Visualizing Human Intracranial Electrode Data</article-title>. <source>J. Neurosci. Methods</source>
<volume>281</volume>, <fpage>40</fpage>–<lpage>48</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2017.01.022</pub-id>
<pub-id pub-id-type="pmid">28192130</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haehn</surname><given-names>D.</given-names></name><name><surname>Rannou</surname><given-names>N.</given-names></name><name><surname>Ahtam</surname><given-names>B.</given-names></name><name><surname>Grant</surname><given-names>E.</given-names></name><name><surname>Pienaar</surname><given-names>R.</given-names></name></person-group> (<year>2014</year>). <article-title>Neuroimaging in the Browser Using the X Toolkit</article-title>. <source>Front. Neuroinformatics</source>
<volume>101</volume>. </mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Haehn</surname><given-names>D.</given-names></name></person-group> (<year>2013</year>). “<article-title>Slice: Drop: Collaborative Medical Imaging in the Browser</article-title>,” in <source>ACM SIGGRAPH 2013 Computer Animation Festival (ACM SIGGRAPH)</source> (<publisher-loc>New York, US</publisher-loc>: <publisher-name>Association for Computing Machinery</publisher-name>), <fpage>1</fpage>. </mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hart</surname><given-names>S. G.</given-names></name><name><surname>Staveland</surname><given-names>L. E.</given-names></name></person-group> (<year>1988</year>). <article-title>Development of Nasa-Tlx (Task Load index): Results of Empirical and Theoretical Research</article-title>. <source>Adv. Psychol.</source>
<volume>52</volume>, <fpage>139</fpage>–<lpage>183</lpage>. <pub-id pub-id-type="doi">10.1016/s0166-4115(08)62386-9</pub-id>
</mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jobst</surname><given-names>B. C.</given-names></name><name><surname>Bartolomei</surname><given-names>F.</given-names></name><name><surname>Diehl</surname><given-names>B.</given-names></name><name><surname>Frauscher</surname><given-names>B.</given-names></name><name><surname>Kahane</surname><given-names>P.</given-names></name><name><surname>Minotti</surname><given-names>L.</given-names></name><etal/></person-group> (<year>2020</year>). <article-title>Intracranial EEG in the 21st century</article-title>. <source>Epilepsy Currents</source>
<volume>20</volume>, <fpage>180</fpage>–<lpage>188</lpage>. <pub-id pub-id-type="doi">10.1177/1535759720934852</pub-id>
<pub-id pub-id-type="pmid">32677484</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kubanek</surname><given-names>J.</given-names></name><name><surname>Schalk</surname><given-names>G.</given-names></name></person-group> (<year>2015</year>). <article-title>Neuralact: a Tool to Visualize Electrocortical (Ecog) Activity on a Three-Dimensional Model of the Cortex</article-title>. <source>Neuroinformatics</source>
<volume>13</volume>, <fpage>167</fpage>–<lpage>174</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-014-9252-3</pub-id>
<pub-id pub-id-type="pmid">25381641</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lösel</surname><given-names>P. D.</given-names></name><name><surname>van de Kamp</surname><given-names>T.</given-names></name><name><surname>Jayme</surname><given-names>A.</given-names></name><name><surname>Ershov</surname><given-names>A.</given-names></name><name><surname>Faragó</surname><given-names>T.</given-names></name><name><surname>Pichler</surname><given-names>O.</given-names></name><etal/></person-group> (<year>2020</year>). <article-title>Introducing Biomedisa as an Open-Source Online Platform for Biomedical Image Segmentation</article-title>. <source>Nat. Commun.</source>
<volume>11</volume>, <fpage>1</fpage>–<lpage>14</lpage>. <pub-id pub-id-type="doi">10.1038/s41467-020-19303-w</pub-id>
<pub-id pub-id-type="pmid">31911652</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magnotti</surname><given-names>J. F.</given-names></name><name><surname>Wang</surname><given-names>Z.</given-names></name><name><surname>Beauchamp</surname><given-names>M. S.</given-names></name></person-group> (<year>2020</year>). <article-title>Rave: Comprehensive Open-Source Software for Reproducible Analysis and Visualization of Intracranial Eeg Data</article-title>. <source>NeuroImage</source>
<volume>223</volume>, <fpage>117341</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117341</pub-id>
<pub-id pub-id-type="pmid">32920161</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milsap</surname><given-names>G.</given-names></name><name><surname>Collard</surname><given-names>M.</given-names></name><name><surname>Coogan</surname><given-names>C.</given-names></name><name><surname>Crone</surname><given-names>N. E.</given-names></name></person-group> (<year>2019</year>). <article-title>Bci2000web and Webfm: Browser-Based Tools for Brain Computer Interfaces and Functional Brain Mapping</article-title>. <source>Front. Neurosci.</source>
<volume>12</volume>, <fpage>1030</fpage>. <pub-id pub-id-type="doi">10.3389/fnins.2018.01030</pub-id>
<pub-id pub-id-type="pmid">30814923</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murugesan</surname><given-names>S.</given-names></name><name><surname>Bouchard</surname><given-names>K.</given-names></name><name><surname>Chang</surname><given-names>E.</given-names></name><name><surname>Dougherty</surname><given-names>M.</given-names></name><name><surname>Hamann</surname><given-names>B.</given-names></name><name><surname>Weber</surname><given-names>G. H.</given-names></name></person-group> (<year>2017</year>). <article-title>Multi-scale Visual Analysis of Time-Varying Electrocorticography Data via Clustering of Brain Regions</article-title>. <source>BMC bioinformatics</source>
<volume>18</volume>, <fpage>1</fpage>–<lpage>15</lpage>. <pub-id pub-id-type="doi">10.1186/s12859-017-1633-9</pub-id>
<pub-id pub-id-type="pmid">28049414</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rannou</surname><given-names>N.</given-names></name><name><surname>Bernal-Rusiel</surname><given-names>J.</given-names></name><name><surname>Haehn</surname><given-names>D.</given-names></name><name><surname>Grant</surname><given-names>P.</given-names></name><name><surname>Pienaar</surname><given-names>R.</given-names></name></person-group> (<year>2017</year>). “<article-title>Medical Imaging in the Browser with the a* Medical Imaging (Ami) Toolkit</article-title>,” in <conf-name>34th Annual Scientific Meeting European Society for Magnetic Resonance in Medicine and Biology</conf-name>, <conf-loc>Barcelona, Spain</conf-loc>, <conf-date>19-21 Oct. 2017</conf-date>. </mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sherif</surname><given-names>T.</given-names></name><name><surname>Kassis</surname><given-names>N.</given-names></name><name><surname>Rousseau</surname><given-names>M.-É.</given-names></name><name><surname>Adalat</surname><given-names>R.</given-names></name><name><surname>Evans</surname><given-names>A. C.</given-names></name></person-group> (<year>2015</year>). <article-title>Brainbrowser: Distributed, Web-Based Neurological Data Visualization</article-title>. <source>Front. Neuroinformatics</source>
<volume>8</volume>, <fpage>89</fpage>. <pub-id pub-id-type="doi">10.3389/fninf.2014.00089</pub-id>
</mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tadel</surname><given-names>F.</given-names></name><name><surname>Baillet</surname><given-names>S.</given-names></name><name><surname>Mosher</surname><given-names>J. C.</given-names></name><name><surname>Pantazis</surname><given-names>D.</given-names></name><name><surname>Leahy</surname><given-names>R. M.</given-names></name></person-group> (<year>2011</year>). <article-title>Brainstorm: a User-Friendly Application for Meg/eeg Analysis</article-title>. <source>Comput. intelligence Neurosci.</source>
<volume>2011</volume>, <fpage>879716</fpage>. <pub-id pub-id-type="doi">10.1155/2011/879716</pub-id>
</mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>A. I.</given-names></name><name><surname>Wang</surname><given-names>X. H.</given-names></name><name><surname>Doyle</surname><given-names>W. K.</given-names></name><name><surname>Halgren</surname><given-names>E.</given-names></name><name><surname>Carlson</surname><given-names>C.</given-names></name><name><surname>Belcher</surname><given-names>T. L.</given-names></name><etal/></person-group> (<year>2012</year>). <article-title>Localization of Dense Intracranial Electrode Arrays Using Magnetic Resonance Imaging</article-title>. <source>NeuroImage</source>
<volume>63</volume>, <fpage>157</fpage>–<lpage>165</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.06.039</pub-id>
<pub-id pub-id-type="pmid">22759995</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zack</surname><given-names>M. M.</given-names></name><name><surname>Kobau</surname><given-names>R.</given-names></name></person-group> (<year>2017</year>). <article-title>National and State Estimates of the Numbers of Adults and Children with Active epilepsy—united states, 2015</article-title>. <source>MMWR. Morbidity Mortality Weekly Report</source>
<volume>66</volume>, <fpage>821</fpage>. <pub-id pub-id-type="doi">10.15585/mmwr.mm6631a1</pub-id>
<pub-id pub-id-type="pmid">28796763</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
