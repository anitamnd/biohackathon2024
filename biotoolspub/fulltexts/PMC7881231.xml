<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_HLY6236 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr001 jpg ?>
<?FILEgr002 jpg ?>
<?FILEgr003 tif ?>
<?FILEgr004 jpg ?>
<?FILEfx001 jpg ?>
<?FILEfx002 jpg ?>
<?FILEfx003 jpg ?>
<?FILEsi1 svg ?>
<?FILEsi2 svg ?>
<?FILEsi3 svg ?>
<?FILEsi4 svg ?>
<?FILEsi5 svg ?>
<?FILEsi6 svg ?>
<?FILEsi7 svg ?>
<?FILEsi8 svg ?>
<?FILEsi9 svg ?>
<?properties open_access?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Heliyon</journal-id>
    <journal-id journal-id-type="iso-abbrev">Heliyon</journal-id>
    <journal-title-group>
      <journal-title>Heliyon</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2405-8440</issn>
    <publisher>
      <publisher-name>Elsevier</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7881231</article-id>
    <article-id pub-id-type="pii">S2405-8440(21)00341-8</article-id>
    <article-id pub-id-type="doi">10.1016/j.heliyon.2021.e06236</article-id>
    <article-id pub-id-type="publisher-id">e06236</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title><bold>audiomath</bold>: A neuroscientist's sound toolkit</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au0010">
        <name>
          <surname>Hill</surname>
          <given-names>N. Jeremy</given-names>
        </name>
        <email>jezhill@gmail.com</email>
        <xref rid="aff0010" ref-type="aff">a</xref>
        <xref rid="cr0010" ref-type="corresp">⁎</xref>
      </contrib>
      <contrib contrib-type="author" id="au0020">
        <name>
          <surname>Mooney</surname>
          <given-names>Scott W.J.</given-names>
        </name>
        <xref rid="aff0020" ref-type="aff">b</xref>
        <xref rid="aff0030" ref-type="aff">c</xref>
      </contrib>
      <contrib contrib-type="author" id="au0030">
        <name>
          <surname>Prusky</surname>
          <given-names>Glen T.</given-names>
        </name>
        <xref rid="aff0020" ref-type="aff">b</xref>
        <xref rid="aff0030" ref-type="aff">c</xref>
        <xref rid="aff0040" ref-type="aff">d</xref>
      </contrib>
      <aff id="aff0010"><label>a</label>Stratton VA Medical Center, Albany, NY, USA</aff>
      <aff id="aff0020"><label>b</label>Burke Neurological Institute, White Plains, NY, USA</aff>
      <aff id="aff0030"><label>c</label>Blythedale Children's Hospital, Valhalla, NY, USA</aff>
      <aff id="aff0040"><label>d</label>Department of Physiology and Biophysics, Weill Cornell Medicine, New York, NY, USA</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cr0010"><label>⁎</label>Corresponding author. <email>jezhill@gmail.com</email></corresp>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>10</day>
      <month>2</month>
      <year>2021</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="collection">
      <month>2</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>10</day>
      <month>2</month>
      <year>2021</year>
    </pub-date>
    <volume>7</volume>
    <issue>2</issue>
    <elocation-id>e06236</elocation-id>
    <history>
      <date date-type="received">
        <day>29</day>
        <month>4</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>16</day>
        <month>10</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>5</day>
        <month>2</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <license license-type="CC BY" xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).</license-p>
      </license>
    </permissions>
    <abstract id="ab0010">
      <p>In neuroscientific experiments and applications, working with auditory stimuli demands software tools for generation and acquisition of raw audio, for composition and tailoring of that material into finished stimuli, for precisely timed presentation of the stimuli, and for experimental session recording. Numerous programming tools exist to approach these tasks, but their differing specializations and conventions demand extra time and effort for integration. In particular, verifying stimulus timing requires extensive engineering effort when developing new applications.</p>
      <p>This paper has two purposes. The first is to present <bold>audiomath</bold> (<ext-link ext-link-type="uri" xlink:href="https://pypi.org/project/audiomath" id="inf0010">https://pypi.org/project/audiomath</ext-link>), a sound software library for Python that prioritizes the needs of neuroscientists. It minimizes programming effort by providing a simple object-oriented interface that unifies functionality for audio generation, manipulation, visualization, decoding, encoding, recording, and playback. It also incorporates specialized tools for measuring and optimizing stimulus timing.</p>
      <p>The second purpose is to relay what we have learned, during development and application of the software, about the twin challenges of delivering stimuli precisely at a certain time, and of precisely measuring the time at which stimuli were delivered. We provide a primer on these problems and the possible approaches to them. We then report audio latency measurements across a range of hardware, operating systems and settings, to illustrate the ways in which hardware and software factors interact to affect stimulus presentation performance, and the resulting pitfalls for the programmer and experimenter. In particular, we highlight the potential conflict between demands for low latency, low variability in latency (“jitter”), cooperativeness, and robustness. We report the ways in which <bold>audiomath</bold> can help to map this territory and provide a simplified path toward each application's particular priority.</p>
      <p>By unifying audio-related functionality and providing specialized diagnostic tools, <bold>audiomath</bold> both simplifies and potentiates the development of neuroscientific applications in Python.</p>
    </abstract>
    <abstract abstract-type="teaser" id="ab0020">
      <p>Auditory stimuli; Python; Software library; Audio latency; Audio jitter</p>
    </abstract>
    <kwd-group id="kws0010">
      <title>Keywords</title>
      <kwd>Auditory stimuli</kwd>
      <kwd>Python</kwd>
      <kwd>Software library</kwd>
      <kwd>Audio latency</kwd>
      <kwd>Audio jitter</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="se0010">
    <label>1</label>
    <title>Introduction</title>
    <p id="pr0010">The open-source software library <bold>audiomath</bold> makes it easy for Python programmers to synthesize, record, manipulate, edit, visualize or play sound waveforms. Since these functions are integral to a wide variety of use cases in science, engineering, art and entertainment, <bold>audiomath</bold>'s potential applications are many and diverse. Its development was motivated by the need for tools for designing and presenting auditory stimuli in neuroscientific research, so we will describe <bold>audiomath</bold> from the neuroscientist's perspective. The software is available at <ext-link ext-link-type="uri" xlink:href="https://pypi.org/project/audiomath" id="inf0020">https://pypi.org/project/audiomath</ext-link>.</p>
    <p id="pr0020">We created <bold>audiomath</bold> as part of the Burke-Blythedale Pediatric Neuroscience Research Collaboration, to support field applications of neurotechnology—specifically, EEG-based cognitive assessments in children with brain injuries, performed at the bedside. This is one component of our broader vision of a “scalable neurological assessment platform” (SNAP) which consists of multiple reusable software modules. Another published component, <bold>Shady</bold>, allows rendering and real-time manipulation of research-quality visual stimuli even on sub-optimal hardware <xref rid="br0010" ref-type="bibr">[1]</xref>. Providing full documentation and support to external users is part of our strategy to ensure that modules such as <bold>Shady</bold> and <bold>audiomath</bold> remain usable and future-proof as the platform evolves. Python was chosen as the basis for this platform due to the maturity and modularity of the language, the power and minimalism of its syntax, the comprehensiveness of both its standard library and the range of high-quality packages available for free from third parties, and its consequent high prevalence in scientific communities.</p>
    <p id="pr0030">We identified the need for software tools that facilitate—and, to the greatest extent possible, enable us to automate—the four main tasks enumerated below with minimal effort on the part of the programmer. In outlining these tasks, we have underlined the recurring lower-level functions that these tasks require:<list list-type="simple" id="ls0010"><list-item id="li0010"><label>1.</label><p id="pr0040">Gathering raw material to make auditory stimuli—depending on the experiment, this might include <underline>decoding</underline> content from existing audio files, <underline>generating</underline> waveforms from simple mathematical specifications, <underline>recording</underline> sounds from a microphone, or a combination of these.</p></list-item><list-item id="li0020"><label>2.</label><p id="pr0050">Composing stimuli by <underline>manipulating</underline> the sound data in memory—for example, extracting segments of interest, trimming or padding to the required length, aligning, windowing, rescaling, resampling, cleaning up, multiplexing and splicing. As part of this process <underline>visualization</underline> and <underline>playback</underline> are valuable tools for previewing the stimulus under construction. Typically, the end-point of this pipeline entails <underline>encoding</underline> the result in some recognized audio file format and saving it as a file.</p></list-item><list-item id="li0030"><label>3.</label><p id="pr0060">Presenting stimuli to the subject. For this, we needed our <underline>playback</underline> functionality to meet certain criteria:<list list-type="simple" id="ls0020"><list-item id="li0040"><label>•</label><p id="pr0070">ease of controlling multiple overlapping stimuli independently;</p></list-item><list-item id="li0050"><label>•</label><p id="pr0080">the capability of playing multi-channel sounds (more than 2 channels);</p></list-item><list-item id="li0060"><label>•</label><p id="pr0090">programmatic control of the operating-system's overall volume (because we found that standardizing the system volume by hand, as part of an experimenter's standard operating procedure, is overly prone to omission due to human error);</p></list-item><list-item id="li0070"><label>•</label><p id="pr0100">in some applications, we need to achieve low latency, and/or minimal <italic>variability</italic> in latency (“jitter”)—see Section <xref rid="se0100" ref-type="sec">3</xref>;</p></list-item><list-item id="li0080"><label>•</label><p id="pr0110">in other applications, we might be willing to sacrifice latency but require playback to be robust (able to tolerate the use of processing resources by other threads or processes without the sound stuttering, skipping or slowing down) and/or cooperative (i.e. to allow other processes to play sound at the same time).</p></list-item></list></p></list-item><list-item id="li0090"><label>4.</label><p id="pr0120">Capturing sound during the experimental session: some experimental designs require continuous <underline>recording</underline> of sound data and <underline>encoding</underline> it to file, for example to capture verbal responses by the subject, notes dictated by the experimenter, and other events that may be relevant to subsequent data analysis.</p></list-item></list></p>
    <p id="pr0130">In summary, the lower-level functions (underlined above) are: <underline>manipulation</underline>, <underline>generation</underline>, <underline>visualization</underline>, <underline>decoding</underline>, <underline>encoding</underline>, <underline>recording</underline>, and <underline>playback</underline>. In Section <xref rid="se0020" ref-type="sec">2</xref>, we explain how <bold>audiomath</bold> enables each of these functions. In Section <xref rid="se0100" ref-type="sec">3</xref> we examine the issue of performance, defining criteria by which audio presentation performance can be judged, the factors that affect it, and outlining various approaches to the problem of recording stimulus timing. In Section <xref rid="se0240" ref-type="sec">4</xref> we describe the methods, included in the <bold>audiomath</bold> codebase, that we used to gather illustrative performance data from a range of hardware and operating systems, which we then report in Section <xref rid="se0250" ref-type="sec">5</xref> before concluding.</p>
  </sec>
  <sec id="se0020">
    <label>2</label>
    <title>Design</title>
    <p id="pr0140">The goal of any programmer's toolbox is to provide functionality at a certain level while saving the programmer from having to worry about implementation details at lower levels. This can be achieved both by writing original code and by wrapping third-party code (the purpose of wrapping being to ensure that code from different sources operates smoothly together, while exposing a consistent level of detail to the programmer who uses it). In <bold>audiomath</bold>, we aim at the level of detail exemplified in <xref rid="fg0010" ref-type="fig">Listing 1</xref>. It allows the Python implementation of an idea at this level to be roughly as concise as the English-language expression of the same idea, while still being highly readable and maintainable by programmers.<fig id="fg0010"><label>Listing 1</label><caption><p>Example Python code, using <bold>audiomath</bold> to accomplish the goal, <italic>“Create and present a four-channel stimulus that has the content of the file</italic><bold>myStimulus.wav</bold><italic>in the first two channels (left and right), simultaneous with a tone burst for synchronization in a third channel, and silence in a fourth.”</italic>. This goal is expressed in 230 characters or 39 words of English. Disregarding the comments and an initial <bold>import</bold> statement, the listing uses 270 characters or 34 words/numbers.</p></caption><alt-text id="at0010">Listing 1</alt-text><graphic xlink:href="gr001"/></fig></p>
    <p id="pr0150">The following subsections describe the way in which <bold>audiomath</bold> uses original code and unites the functionality of various third-party toolboxes, to implement each of its essential functions. Finally, Section <xref rid="se0090" ref-type="sec">2.7</xref> summarizes the third-party dependencies entailed in these solutions, and how <bold>audiomath</bold> manages them.</p>
    <sec id="se0030">
      <label>2.1</label>
      <title>Manipulation</title>
      <p id="pr0160">At its core, <bold>audiomath</bold> represents sounds as numeric arrays, via the (required) third-party Python package <bold>numpy</bold>. <bold>numpy</bold> is well-established, well-supported, and ubiquitous in scientific Python applications <xref rid="br0020" ref-type="bibr">[2]</xref>, <xref rid="br0030" ref-type="bibr">[3]</xref>, <xref rid="br0040" ref-type="bibr">[4]</xref>. In <bold>audiomath</bold>, sound arrays are embedded in an object-oriented interface, bundling together the raw data and the associated meta-information into <bold>Sound</bold> objects. The name <bold>audiomath</bold> is inspired primarily by the ability to perform arithmetic operations on these objects using elementary syntax—for example, <bold>x = 2 * y + z</bold> makes a linear superposition of sounds. Similarly minimal and intuitive notation can be used for slicing and splicing along the time dimension, selection and multiplexing of channels, resampling, mixing, trimming, padding, rescaling, and amplitude modulation.</p>
      <p id="pr0170">For more sophisticated types of manipulation, <bold>audiomath</bold> also provides wrappers that interface with some optional third-party audio processing utilities. For example, it can:<list list-type="simple" id="ls0030"><list-item id="li0100"><label>•</label><p id="pr0180">Time-stretch or pitch-shift a sound (i.e. make it last longer while holding pitch constant, or change its pitch while holding duration constant). To do this, <bold>audiomath</bold> simply provides a convenience wrapper around the phase-vocoder implementation provided by the (optional) third-party Python package <bold>librosa</bold>
<xref rid="br0050" ref-type="bibr">[5]</xref>.</p></list-item><list-item id="li0110"><label>•</label><p id="pr0190">Process a sound via the SoX utility. As part of <bold>audiomath</bold>, we provide a convenience wrapper around <bold>sox</bold>, a third-party cross-platform command-line tool that can perform a range of different processing and editing functions on sounds. One of these, of particular interest to neuroscientists, is the “loudness” effect, which can standardize the expected perceived intensity of a sound according to international standard ISO 226 <xref rid="br0060" ref-type="bibr">[6]</xref>.</p></list-item></list></p>
      <p id="pr0200">Furthermore, the easy accessibility of the raw <bold>numpy</bold> array inside each <bold>Sound</bold> object, and the ubiquity of <bold>numpy</bold>, makes it relatively easy for users to implement any sound data transformation or analysis method that is not already implemented in <bold>audiomath</bold>, either from scratch or using third-party audio processing packages such as <bold>librosa</bold> and <bold>scipy.signal</bold>.</p>
    </sec>
    <sec id="se0040">
      <label>2.2</label>
      <title>Generation</title>
      <p id="pr0210">In its <bold>Signal</bold> sub-module, <bold>audiomath</bold> provides routines for synthesizing simple stimulus waveforms such as clicks, sine-waves, square-waves, sawtooth-waves and triangle-waves, with or without antialiasing, and for modulating their amplitude with other waveforms, including windowing/tapering functions. There is also support for playback of sounds that are functionally generated on-the-fly (i.e. while playing).</p>
    </sec>
    <sec id="se0050">
      <label>2.3</label>
      <title>Visualization</title>
      <p id="pr0220">Sound waveforms, and/or their amplitude or power spectra, can be plotted provided the (optional) third-party Python package <bold>matplotlib</bold> is installed. Like <bold>numpy</bold>, <bold>matplotlib</bold> is an almost-ubiquitous standard, used throughout the world by scientists of all fields <xref rid="br0040" ref-type="bibr">[4]</xref>, <xref rid="br0070" ref-type="bibr">[7]</xref>, <xref rid="br0080" ref-type="bibr">[8]</xref>.</p>
    </sec>
    <sec id="se0060">
      <label>2.4</label>
      <title>Decoding and encoding</title>
      <p id="pr0230">Using routines that are already part of Python's standard library, <bold>audiomath</bold> can read uncompressed <bold>.wav</bold> files into memory, and write them back out to file. It can also decode a wide variety of other audio formats, or extract the sound track from a variety of video formats, using the third-party C library <bold>AVbin</bold>
<xref rid="br0090" ref-type="bibr">[9]</xref>—<bold>AVbin</bold> binaries are included in the package, for the most common platforms. Non-<bold>.wav</bold> formats can also be encoded and written to file—under the hood, this uses <bold>audiomath</bold>'s own wrapper around the popular third-party command-line utility <bold>ffmpeg</bold>, which must be installed separately. (Alternatively, <bold>sox</bold> may be used—again, this must be installed separately.)</p>
    </sec>
    <sec id="se0070">
      <label>2.5</label>
      <title>Recording</title>
      <p id="pr0240">To access the audio input and output hardware, <bold>audiomath</bold> ships with a back-end implementation based on the third-party cross-platform C library <bold>PortAudio</bold>
<xref rid="br0100" ref-type="bibr">[10]</xref>, binaries for which are included for a range of common platforms. This allows sound to be recorded either into memory or direct to file. Recording direct to file additionally requires a separate installation of the <bold>ffmpeg</bold> or <bold>sox</bold> command-line utilities. The <bold>PortAudio</bold> library also enables flexible and precise playback (see next section).</p>
    </sec>
    <sec id="se0080">
      <label>2.6</label>
      <title>Playback</title>
      <p id="pr0250">The default back-end for playback is the <bold>PortAudio</bold> library (also used for recording—see previous section). Across a range of devices, we have found that this provides flexibility and reasonable performance (see the results of Section <xref rid="se0250" ref-type="sec">5</xref>). An alternative back-end, based on the optional third-party package <bold>psychtoolbox</bold>, is also included—this enables access to <bold>PsychPortAudio</bold>, a customized version of the <bold>PortAudio</bold> library tailored for precise timing <xref rid="br0110" ref-type="bibr">[11]</xref>, <xref rid="br0120" ref-type="bibr">[12]</xref>. Users can switch to the <bold>PsychPortAudio</bold> implementation if they want to reduce jitter even further.</p>
      <p id="pr0260">Programmers may also use <bold>audiomath</bold> to control the operating-system's overall volume. This is accomplished via bindings to Applescript on macOS, to PulseAudio command-line utilities on Linux, or to Microsoft's Component Object Model on Windows (which requires the third-party Python packages <bold>comtypes</bold> and <bold>psutil</bold>).</p>
    </sec>
    <sec id="se0090">
      <label>2.7</label>
      <title>Summary of <bold>audiomath</bold>'s third-party dependencies</title>
      <p id="pr0270">To implement each of the required functions described in the foregoing sections, <bold>audiomath</bold> brings together functionality already implemented by various third-party developers. Some of these dependencies are included in the package when it is installed, some others are installed automatically when the user installs <bold>audiomath</bold> via a package manager such as the ubiquitous <bold>pip</bold>, and some others are left to the user's discretion. For reference, they are summarized in <xref rid="tbl0010" ref-type="table">Table 1</xref>.<table-wrap position="float" id="tbl0010"><label>Table 1</label><caption><p>This table lists the third-party dependencies of <bold>audiomath</bold>. The recommended way of installing <bold>audiomath</bold> is to use the command <bold>python -m pip install audiomath</bold> and this will handle the dependencies listed as “automatically installed” and “included”. Optional dependencies can be installed at the user's discretion, using <bold>pip</bold> for Python packages (for example with <bold>python -m pip install psychtoolbox</bold>) and separate methods (independent of Python) for the command-line utilities.</p></caption><alt-text id="at0070">Table 1</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Name</th><th>Type</th><th>Role</th></tr></thead><tbody><tr><td colspan="3" align="left"><bold>Automatically installed alongside audiomath on all platforms:</bold></td></tr><tr><td><bold>numpy</bold></td><td>Python package</td><td>required for everything <bold>audiomath</bold> does</td></tr><tr><td colspan="3" align="left"><bold>Automatically installed alongside audiomath on Windows:</bold></td></tr><tr><td><bold>psutil</bold></td><td>Python package</td><td>required for controlling the system volume on Windows</td></tr><tr><td><bold>comtypes</bold></td><td>Python package</td><td>required for controlling the system volume on Windows</td></tr><tr><td colspan="3" align="left"><bold>Included in the audiomath package, ready-compiled for common platforms:</bold></td></tr><tr><td><bold>PortAudio</bold></td><td>binary (C library)</td><td>enables recording and playback</td></tr><tr><td><bold>AVBin</bold></td><td>binary (C library)</td><td>enables sounds to be decoded from common compressed audio and video file formats</td></tr><tr><td colspan="3" align="left"><bold>Optional add-ons that can enhance audiomath's functionality:</bold></td></tr><tr><td><bold>matplotlib</bold></td><td>Python package</td><td>enables plotting of waveforms and spectra</td></tr><tr><td><bold>psychtoolbox</bold></td><td>Python package</td><td>enables playback with very low jitter</td></tr><tr><td><bold>librosa</bold></td><td>Python package</td><td>enables pitch-shifting and time-stretching; (as well as providing a library of other sophisticated analysis and processing functions)</td></tr><tr><td><bold>ffmpeg</bold></td><td>command-line utility</td><td>enables <bold>audiomath</bold> sounds to be written to file in a wide range of encoded formats</td></tr><tr><td><bold>sox</bold></td><td>command-line utility</td><td>allows various effects to be applied to sound; enables <bold>audiomath</bold> sounds to be written to file in some encoded formats</td></tr></tbody></table></table-wrap></p>
      <p id="pr0280">With the exception of <bold>numpy</bold>, <bold>audiomath</bold>'s functionality degrades gracefully in the absence of these dependencies. For example, if the user's operating system is not among the platforms already supported by the included <bold>AVBin</bold> binaries (currently: 64- and 32-bit Windows, 64-bit macOS and Linux), then <bold>audiomath</bold> will only be able to read audio data from uncompressed <bold>.wav</bold> files, but other functions will still work; or, if the user chooses not to install <bold>matplotlib</bold>, plotting will be disabled but no other functions will be compromised.</p>
    </sec>
  </sec>
  <sec id="se0100">
    <label>3</label>
    <title>Understanding playback performance: a primer for neuroscientists</title>
    <p id="pr0730">When a computer program issues the command to play a sound (or modulate it, or stop it), the command may return and allow the program to proceed within microseconds; however, the physical sound will not start (or change, or stop) until much later. This delay, referred to as audio latency, reflects the time taken for the operating system's sound libraries, drivers and hardware to process the command. Latencies are highly variable between hardware/driver/operating-system configurations—from a few milliseconds to a few hundred milliseconds. They are also somewhat variable from one repetition to another on the same setup—we define “jitter” as the within-setup standard deviation of latencies measured under uniform conditions.</p>
    <p id="pr0740">Latency and jitter both matter, for different reasons, to neuroscientists. Additional performance criteria may also interact with one's latency settings. We outline multiple performance criteria and their significance in Section <xref rid="se0110" ref-type="sec">3.1</xref> below. Of these criteria, uncertainty in stimulus timing, in the form of jitter, tends to demand the largest amount of energy when one is engineering neuroscientific experiments and applications. In Section <xref rid="se0190" ref-type="sec">3.2</xref> we outline different methods for recording the timing of auditory stimuli for the purpose of reducing uncertainty. In Section <xref rid="se0240" ref-type="sec">4</xref> we describe how <bold>audiomath</bold> unifies these methods to provide a measurement tool for latency and jitter. In Section <xref rid="se0250" ref-type="sec">5</xref> we report results of such measurements, illustrating some of the factors that affect performance.</p>
    <sec id="se0110">
      <label>3.1</label>
      <title>Performance criteria and their significance in neuroscience</title>
      <p id="pr0310">In the following subsections we define multiple performance criteria—latency, jitter, robustness, cooperativeness, multi-channel capability and programmability—and briefly highlight the significance of each. These criteria may matter to differing extents, and they may be more or less easy to achieve, depending on whether the application is deployed in a laboratory or “in the field” (for example, as a bedside test in a clinical setting). In field applications there may be less scope for dedicating specialized hardware to each separate task, higher likelihood of having to use sub-optimal hardware (often chosen for portability rather than performance), and greater imperative to multi-task (for example, in our pediatric applications we often present critical stimuli opportunistically, interleaved with entertainment that usually requires a browser-based video player).</p>
      <p id="pr0320">Often it is necessary to accept a trade-off between criteria. Most obviously, low latency and robustness pull in opposite directions. On the Windows platform there are further complications, as the lowest latencies are typically only available at the expense of cooperativeness. This depends on what the <bold>PortAudio</bold> library refers to as the host application programmer's interface or “host API”. Host APIs are platform-specific libraries that allow programmers to interface with sound drivers without writing driver-specific code.<xref rid="fn0010" ref-type="fn">1</xref>
<bold>PortAudio</bold>, as a cross-platform library, offers an additional level of abstraction so that the programmer does not need to write host-API-specific code. It does this by rolling the selection of host API into the user's selection of sound device: for example, on a given Windows computer, device #4 might be a pair of headphones as addressed via the “DirectSound” host API, whereas device #6 might be the same pair of headphones as addressed through the “WASAPI” host API.<xref rid="fn0110" ref-type="fn">2</xref> This selection affects performance in multiple ways, especially under Windows, as we will discuss below and illustrate in Section <xref rid="se0250" ref-type="sec">5</xref>. Therefore, it is valuable to be aware of the available choices, which are illustrated in <xref rid="fg0020" ref-type="fig">Fig. 1</xref>. One motivation in making <bold>PortAudio</bold> the default back-end for <bold>audiomath</bold> was that it provides a simple way for the user to cut through the complexity of <xref rid="fg0020" ref-type="fig">Fig. 1</xref> and choose a host API with minimal fuss.<fig id="fg0020"><label>Figure 1</label><caption><p>This figure shows software components that provide Python programmers with access to sound input and output hardware. We show the components that are relevant to <bold>audiomath</bold> and, for comparison, the audio components related to <bold>psychopy</bold>, which is a separate high-level package also aimed at neuroscientists. Both of them make use (directly or indirectly) of cross-platform libraries such as <bold>PortAudio</bold> which, in turn, wrap the various platform-specific “host APIs” (darker boxes). Each host API has advantages and disadvantages relative to the others, including the fact that the driver for a given device may support features of different host APIs to differing extents. Therefore, <bold>PortAudio</bold> (and the Python packages that use it, such as <bold>audiomath</bold>) provide users with commands by which the desired host API can be chosen and configured. The choices available to the <bold>audiomath</bold> user have been highlighted: blue (mid-intensity) arrows indicate options that use the standard <bold>PortAudio</bold> library, and red (darkest) arrows indicate options that take advantage of <bold>PsychPortAudio</bold>, a performance-optimized version of <bold>PortAudio</bold> that comes from the Psychophysics Toolbox project. Gray (lightest) arrows show connections that are either unrelated to <bold>audiomath</bold> or outside of the <bold>audiomath</bold> user's control. The figure is incomplete, in the sense that there are many other sound libraries available, unrelated to either <bold>audiomath</bold> or <bold>psychopy</bold>, and even additional host APIs that <bold>PortAudio</bold> does not support.</p></caption><alt-text id="at0020">Figure 1</alt-text><graphic xlink:href="gr002"/></fig></p>
      <sec id="se0120">
        <label>3.1.1</label>
        <title>Low latency</title>
        <p id="pr0330">Some applications require absolute latency to be as low as possible. In neuroscience these are typically closed-loop interactive applications that are intended to mimic real-world interaction. Such applications need low latency for the same reasons musicians need it: to ensure sound perception is sufficiently tightly bound to other sensory inputs and to efference copy information from the motor system. This concern was highlighted in a study by Scarpaci et al. <xref rid="br0140" ref-type="bibr">[14]</xref>, who manipulated the latency of real-time updates to the virtual head-centric position of a sound stimulus, which changed as a function of subjects' head movements while tracking the stimulus. They measured subjects' errors in tracking, reporting equal (baseline) error rates at latencies of 4 and 18 ms, with increasing errors as latency increased further (32 ms or more). Some hardware and software configurations are capable of producing latencies below 20–30 ms and some are not. Therefore it is clear that neuroscientists would need to select and configure their computers carefully and validate their performance for this application—and indeed for applications involving <italic>other</italic> response modalities that might have even narrower latency tolerances.</p>
        <p id="pr0340">Another advantage of lower latency is that it tends to entail lower <italic>uncertainty</italic> in stimulus onset time (although there are exceptions, such as the one noted in the next subsection). In other words, as absolute expected latency decreases, absolute jitter usually also decreases.</p>
      </sec>
      <sec id="se0130">
        <label>3.1.2</label>
        <title>Low jitter</title>
        <p id="pr0350">Post-hoc analyses of the brain's responses to sound are anchored first and foremost in knowing <italic>when</italic> the sounds occurred, relative to the response data that are being collected (brain signal data, eye movements, button-presses, etc). This is where jitter matters: even if the absolute latency were high, zero jitter would mean that expected latency could be measured once and then compensated-for precisely in every analysis thereafter; conversely, non-zero jitter entails uncertainty in the timing of each individual stimulus, degrading the power of the analysis. Various approaches for recording stimulus timing, and the way in which they deal with jitter, are described in Section <xref rid="se0190" ref-type="sec">3.2</xref>. Usually, jitter decreases as the absolute expected latency decreases, but there is one notable exception: when using <bold>PsychPortAudio</bold>, a performance-optimized version of <bold>PortAudio</bold> that comes from the Psychophysics Toolbox project, it is possible to <italic>pre-schedule</italic> a sound. Relative to the time the command is issued, the scheduled time should be completely outside the usual distribution of latencies (so, the effective latency is longer than usual) but the resulting jitter is very low, regardless of absolute latency.</p>
      </sec>
      <sec id="se0140">
        <label>3.1.3</label>
        <title>Robustness</title>
        <p id="pr0360">We use the term “robust” to describe a system that plays and records clean audio without skipping, stuttering, crackling or slowing, despite concurrent use of CPU or memory resources by one's own program,<xref rid="fn0020" ref-type="fn">3</xref> or by other processes on the computer. In traditional laboratory settings, an experimenter may be able to dedicate and configure a computer (or even a more-expensive specialized audio processor) exclusively for the task of auditory stimulus presentation. In this case, the experimenter typically does not have to worry about robustness thereafter. In field applications, by contrast, robustness may be of greater concern. The lower the latency, the higher the risk of disruption—therefore, in some situations it may be necessary to run at higher latency for the sake of robustness.</p>
      </sec>
      <sec id="se0150">
        <label>3.1.4</label>
        <title>Cooperativeness</title>
        <p id="pr0370">We call a process “cooperative” if, while using a given sound device, it allows other processes to use the same sound device concurrently. On Windows, the programmer must unfortunately choose: the lowest-latency host APIs (WASAPI with aggressively-configured latency settings, or ASIO,<xref rid="fn0030" ref-type="fn">4</xref> or WDM/KS<xref rid="fn0040" ref-type="fn">5</xref>) are non-cooperative, whereas selecting a host API capable of cooperation (WASAPI with cooperative settings, or MME,<xref rid="fn0050" ref-type="fn">6</xref> or DirectSound) results in considerably higher latencies. In many neuroscience experiments it may actually be desirable for the main process to use the sound hardware uncooperatively, since this will prevent incidental interruptions by sounds from other processes. However, it is undesirable in more-integrated applications—consider, for example, a brain-computer interface designed purely as an access method through which users might instruct their computers to perform any normal task, including playing music or making calls.</p>
      </sec>
      <sec id="se0160">
        <label>3.1.5</label>
        <title>Multi-channel capability</title>
        <p id="pr0380">Nearly all sound software, and nearly all sound output hardware, is capable of delivering two precisely-synchronized channels simultaneously: left and right. We use the term “multi-channel” to refer to anything that delivers <italic>more than two</italic> channels, such as a surround-sound system. Even when we need only two channels for stimulus presentation, it is sometimes critical to have more available—for example, when implementing some types of hardware-triggered pulse synchronization, as described in Section <xref rid="se0220" ref-type="sec">3.2.3</xref>. In addition to demanding suitably specialized hardware, this requirement may constrain one's choice of software (before adding <bold>PortAudio</bold> support to <bold>audiomath</bold>, we tried a number of other audio toolboxes for Python, and found that many of them were not multi-channel capable) as well as one's choice of host API (on Windows, the “MME” host API is not multi-channel capable). Luckily, the market for surround-sound entertainment applications has ensured that suitably specialized portable hardware is available and well-supported—we have had good results from 8-channel dedicated USB sound adapters that cost around 30 US Dollars, in both performance tests and in our auditory brain-computer-interfacing studies <xref rid="br0160" ref-type="bibr">[16]</xref>.</p>
      </sec>
      <sec id="se0170">
        <label>3.1.6</label>
        <title>Programmability</title>
        <p id="pr0390">In designing <bold>audiomath</bold>, we aimed to allow programmers to automate as much functionality as possible, reducing potential sources of error by cutting out manual intervention by the experimenter. However, some software configurations can undermine this. For example, <xref rid="fg0020" ref-type="fig">Fig. 1</xref> shows that, under Windows, sound hardware can be addressed via the ASIO host API and the third-party <bold>ASIO4ALL</bold> driver. This offers flexibility in configuring buffering and latency settings and can be used to achieve very low latencies. However, some of its parameters must be accessed by the end-user manually in a graphical user interface (GUI): <bold>PortAudio</bold> can programmatically increase buffer size relative to the GUI-configured setting, but cannot decrease it, and has no access to the influential “buffer offset” setting. Note that <bold>ASIO4ALL</bold> is itself built on top of the WDM/KS host API—accordingly, we have found that comparably low latencies are achievable by having <bold>PortAudio</bold> address WDM/KS directly, so we have eventually concluded that there is no need to use <bold>ASIO4ALL</bold>.</p>
      </sec>
      <sec id="se0180">
        <label>3.1.7</label>
        <title>Hardware-invariance in latency</title>
        <p id="pr0400">Software intended for distribution to collaborators or customers may end up running on a wide variety of hardware. It may be desirable for performance to vary as little as possible across different hardware setups. We find that the property of invariance is itself affected by one's configuration—particularly, once again, the choice of host API on Windows (DirectSound latencies being highly variable across the setups we have measured, and WDM/KS latencies being the least variable).</p>
      </sec>
    </sec>
    <sec id="se0190">
      <label>3.2</label>
      <title>Synchronization methods</title>
      <p id="pr0410">In the following subsections, we describe four approaches to recording the timing of auditory stimuli: software logging, software-triggered pulse generation, hardware-triggered pulse generation and hardware-synchronized envelope extraction. The <bold>audiomath</bold> source code repository contains microcontroller code that can be used to implement software-triggered or hardware-triggered pulse synchronization, with the option to use both methods simultaneously for measuring audio latencies (see Section <xref rid="se0240" ref-type="sec">4</xref>).</p>
      <sec id="se0200">
        <label>3.2.1</label>
        <title>Software logging</title>
        <p id="pr0420">In the software logging approach, the computer merely saves a record of the times at which sound commands were issued. It requires a common clock that can be read by the stimulus presentation program and also by whatever system provides timestamps for the response data. This approach is used by most applications in the popular brain-computer interfacing platform BCI2000 <xref rid="br0170" ref-type="bibr">[17]</xref>, <xref rid="br0180" ref-type="bibr">[18]</xref>. It does not account for audio latency or jitter: when these are critical, the best one can do is to perform a separate validation using hardware triggering to verify that jitter is low enough.<xref rid="fn0060" ref-type="fn">7</xref> Validation methods might include, for example, the method developed and reported by Wilson et al. <xref rid="br0190" ref-type="bibr">[19]</xref> for BCI2000, or <bold>audiomath</bold>'s own method described below in Section <xref rid="se0240" ref-type="sec">4</xref>. The assumption is that the latency and jitter are the same during validation as they will be during subsequent use without the hardware trigger. This assumption leaves us vulnerable to non-stationarity of the system's audio latency—either transient variation due to contention over processing resources with some background system task, or lasting changes due to updates in the drivers or other operating-system components (the latter means that the validation procedure must be repeated after any system update or reconfiguration).</p>
      </sec>
      <sec id="se0210">
        <label>3.2.2</label>
        <title>Software-triggered pulse synchronization</title>
        <p id="pr0430">When there is no common clock that both the stimulus presentation program and the data acquisition system can read, software logging cannot be used. Instead, it may be possible to use software-triggered pulse synchronization. This relies on the fact that many biosignal recording devices can record auxiliary information time-locked to the primary signal. One or more auxiliary channels can be used to record stimulus timing—typically by marking stimulus onset with the rising edge of a 5-volt pulse. Immediately before playing a sound, the computer sends a message over a serial, parallel or network interface to a specialized synchronization device (“widget”) that generates such a pulse as soon as it receives the message. The widget is typically a simple processor that performs little or no multi-tasking, so its timing is very precise relative to that of the computer. However, the computer's half of the communication link will inevitably add a small extra amount of latency and jitter to the whole system. Other than this, the software-triggered pulse synchronization approach is no different from the software logging method, and suffers all the same problems.</p>
      </sec>
      <sec id="se0220">
        <label>3.2.3</label>
        <title>Hardware-triggered pulse synchronization</title>
        <p id="pr0440">Like the software-triggered approach, hardware-triggered pulse synchronization also uses a pulse-generating widget. Instead of triggering the pulse via digital communication, the computer's audio output cable is split so that identical signals run both to the subject's headphones or speakers, and also to an analog input on the widget. The widget monitors the sound signal and generates a pulse as soon as it detects that the amplitude exceeds some threshold. This is the most accurate and precise way to localize stimulus onsets in time, provided the sound amplitude exceeds the threshold at the right moment. A potential complication arises when working with natural sounds, which may have gradual onsets to differing extents: the relative onset time of two natural sounds may then appear to change according to the (arbitrarily-chosen) threshold parameter value.<xref rid="fn0070" ref-type="fn">8</xref> This problem can be circumvented if the sound software and hardware are capable of delivering more channels than are needed for actual speakers and headphones—each stimulus can incorporate an artificial, abrupt-onset sound in the extra audio channels, whose corresponding cables are connected directly to the widget and not to the speakers.<xref rid="fn0120" ref-type="fn">9</xref> If the stimuli are monophonic, one can use inexpensive audio cable adapters to make this work even with simple stereo hardware (one channel for sound, one for synchronization). For stimuli that require two or more audible channels, one must take steps to ensure that one's hardware, drivers and host API all have multi-channel capability.</p>
      </sec>
      <sec id="se0230">
        <label>3.2.4</label>
        <title>Hardware-synchronized envelope extraction</title>
        <p id="pr0450">Another approach is to abandon the use of digital pulses and instead record sound intensity directly in an <italic>analog</italic> auxiliary channel of the data recording system, if available. (Note that if this involves plugging the audio signal into an input normally intended for sensors that contact the subject, then an optical isolator is required between the audio cable and the auxiliary input, for safety.) Usually the sampling rate of the data recorder is too low to capture the full audible frequency spectrum—if so, one can instead opt to extract and record the <italic>envelope</italic> of the sound, using a digital audio processing widget or even a passive circuit such as the one shown in <xref rid="fg0030" ref-type="fig">Fig. 2</xref>. Either way, this approach allows subsequent analysis to use the envelope information in its entirety—or, if it becomes necessary to define onset as a single instant, it can be decided post-hoc how that instant is determined. We used this approach in an EEG brain-computer interfacing study <xref rid="br0200" ref-type="bibr">[20]</xref>, employing the custom-made passive circuit shown in <xref rid="fg0030" ref-type="fig">Fig. 2</xref> to extract the sound signal's envelope.<fig id="fg0030"><label>Figure 2</label><caption><p>This circuit extracts the envelope of an audio signal by high-pass-filtering, then rectifying, then smoothing, so that it is suitable for recording in an auxiliary channel of a recording device with a low (200 Hz or more) sampling frequency. The circuit was designed to capture sound energy anywhere above 10 Hz, and to capture amplitude fluctuations up to 100 Hz. The resistor value R2 was chosen in the context of a 100 kΩ load on the output (from an optical isolation unit used to isolate the circuit from the EEG recording device, for safety reasons). Smaller loads (larger load resistances) lead to longer decay times of the circuit's impulse response—to bring this back into the low-double-digit millisecond range, R2 might have to be increased (and C2 reduced accordingly so that the product R2⋅C2 remains constant).</p></caption><alt-text id="at0030">Figure 2</alt-text><graphic xlink:href="gr003"/></fig></p>
      </sec>
    </sec>
  </sec>
  <sec id="se0240">
    <label>4</label>
    <title>Materials and methods</title>
    <p id="pr0460">We will report audio latency results measured using a widget that performs both software- and hardware-triggered pulse synchronization, in combination with a way of measuring the time between the two triggers. We used a PJRC Teensy <xref rid="br0210" ref-type="bibr">[21]</xref> version 3.1 microcontroller as the basis for the widget, and added an audio jack. We loaded it with the <bold>PlayerLatency-TeensySketch</bold> microcontroller program provided in <bold>audiomath</bold>'s code repository, and used it in conjunction with the accompanying Python script <bold>PlayerLatency.py</bold>.</p>
    <p id="pr0470"><xref rid="fg0040" ref-type="fig">Fig. 3</xref> shows the system schematically. Immediately before issuing the command to play a sound, the computer sends a message over a serial connection to the microcontroller (first black arrow). When it receives this message, the microcontroller reads its internal clock and then begins monitoring its audio input, which is connected to the computer's analog sound output. As soon as it has detected the onset of the physical sound signal, the microcontroller reads the clock again, and sends its estimate of the elapsed time <inline-formula><mml:math id="M1" altimg="si1.svg"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mtext>int</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> back to the computer over the serial port (second black arrow). The computer, meanwhile, measures the time <inline-formula><mml:math id="M2" altimg="si2.svg"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mtext>ext</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> taken for the whole round trip, including the two unknown (not directly measurable) serial communication latencies <inline-formula><mml:math id="M3" altimg="si3.svg"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="M4" altimg="si4.svg"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>. Starting from the sound's actual physical onset, it takes some small time <italic>d</italic> to gather evidence for detecting that the sound is present. The size of <italic>d</italic> can be estimated from separate measurements of <inline-formula><mml:math id="M5" altimg="si1.svg"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mtext>int</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>, taken while the sound is <italic>already</italic> playing on an indefinite seamless loop (in which context <bold>Play()</bold> commands have no additional effect on the audio output). We have <inline-formula><mml:math id="M6" altimg="si5.svg"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mtext>ext</mml:mtext></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">+</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mtext>int</mml:mtext></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">+</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="M7" altimg="si6.svg"><mml:mi>s</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">+</mml:mo><mml:mi>a</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">+</mml:mo><mml:mi>d</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">+</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mtext>int</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>, where <italic>s</italic> is the time that it takes for the initial <bold>Send()</bold> command to return (depending on the computer, we found <italic>s</italic> to be between 0.1 to 0.4 ms, with jitter typically around one third of its magnitude). If we make the assumption that <inline-formula><mml:math id="M8" altimg="si7.svg"><mml:mi>u</mml:mi><mml:mn>1</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:math></inline-formula>, then we can estimate <inline-formula><mml:math id="M9" altimg="si8.svg"><mml:mi>a</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mtext>int</mml:mtext></mml:mrow></mml:msub><mml:mo linebreak="badbreak" linebreakstyle="after">+</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mtext>ext</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">/</mml:mo><mml:mn>2</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mi>s</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mi>d</mml:mi></mml:math></inline-formula>. In our implementation we found <italic>d</italic> to be less than <inline-formula><mml:math id="M10" altimg="si9.svg"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mfrac></mml:math></inline-formula> of a millisecond (mean 0.02 ms, SD 0.04 ms) and so we disregard it in our estimates.<fig id="fg0040"><label>Figure 3</label><caption><p>This schematic shows our system for estimating the audio latency <italic>a</italic> of stimulus presentation (or the overall latency <italic>s</italic> + <italic>a</italic> of a software-triggered stimulus logging and presentation system) by implementing both software- and hardware-timed triggering in a microcontroller-based timing widget. Time is on the horizontal axis. Black arrows indicate serial-port communication. The analog output of the computer's sound-card is connected to an analog input of the microcontroller. Gray arrows indicate the audio pathway: processing of the <bold>Play()</bold> command by the host API, driver and hardware, followed by the microcontroller's detection of the physical sound signal (which takes time <italic>d</italic>). Time durations <italic>t</italic><sub>ext</sub> and <italic>t</italic><sub>int</sub> are measured by the computer and the microcontroller, respectively. <italic>u</italic><sub>1</sub> and <italic>u</italic><sub>2</sub> are unknown (not directly measurable) latencies of serial-port communication. See text for further details.</p></caption><alt-text id="at0060">Figure 3</alt-text><graphic xlink:href="gr004"/></fig></p>
  </sec>
  <sec id="se0250">
    <label>5</label>
    <title>Results</title>
    <p id="pr0480">Performance may be affected by many factors—for example:<list list-type="simple" id="ls0040"><list-item id="li0120"><label>•</label><p id="pr0490">the quality of one's chosen hardware;</p></list-item><list-item id="li0130"><label>•</label><p id="pr0500">the way in which the sound hardware interfaces with the computer (it may be “on-board”, i.e. integrated into the computer's motherboard, or it may be a dedicated device attached either via the computer's PCI bus or USB bus);</p></list-item><list-item id="li0140"><label>•</label><p id="pr0510">the quality of its driver, and which features of which host APIs the driver supports;</p></list-item><list-item id="li0150"><label>•</label><p id="pr0520">the programmer's choice of host API through which to address the driver;</p></list-item><list-item id="li0160"><label>•</label><p id="pr0530">the programmer's choice of latency settings (PortAudio offers the opportunity to configure a “suggested latency” setting and a buffer size, both of which will affect latency and robustness);</p></list-item><list-item id="li0170"><label>•</label><p id="pr0540">the amount of resources demanded both by other threads of the sound-generating process, and by other processes.</p></list-item></list></p>
    <p id="pr0550">The results of our latency measurements (using the method described in Section <xref rid="se0240" ref-type="sec">4</xref>) illustrate how some of these factors affect performance. <xref rid="tbl0020" ref-type="table">Table 2</xref> shows the profound effect of one's choice of host API when running on Windows. The measurements were taken using one of our preferred platforms for field applications, an all-in-one computer whose sound hardware is not particularly well optimized for performance. The results are heavily influenced by one's chosen “suggested latency” setting, which is an input parameter exposed by both <bold>PortAudio</bold> and <bold>PsychPortAudio</bold>, and accordingly by <bold>audiomath</bold>, during the initialization of the software connection to the sound driver. (Note that the “suggestion” is not followed faithfully on Windows—the true latency is significantly higher.) It was clearly possible to set this setting <italic>too</italic> low, producing audible artifacts (rows 2A and 2J). Nonetheless, it was possible to achieve very low latency (&lt;10 ms) and jitter (&lt;1 ms) without artifacts, either using our default <bold>PortAudio</bold> back-end (row 2B) or using <bold>PsychPortAudio</bold> (row 2D). Furthermore, by increasing latency slightly, it was possible to use <bold>PsychPortAudio</bold>'s pre-scheduling functionality to reduce jitter even further (±0.3 ms, row 2E).<table-wrap position="float" id="tbl0020"><label>Table 2</label><caption><p>This table shows observed audio latency and jitter (mean and standard deviation of latency across 100 repetitions) measured via the headphone socket of one particular all-in-one Windows computer using the method described in Section <xref rid="se0240" ref-type="sec">4</xref>. Performance varies greatly depending on the chosen host API and latency settings. “Suggested latency” refers to a configurable parameter exposed by the <bold>PortAudio</bold> library and by <bold>audiomath</bold> when initializing sound hardware, except in the case marked <sup>⁎</sup> (row 2E): this measurement was taken by “suggesting” a latency of 1 ms as in the previous condition, but then using <bold>PsychPortAudio</bold>'s unique pre-scheduling functionality to configure a longer latency of 10 ms, resulting in reduced jitter.</p></caption><alt-text id="at0080">Table 2</alt-text><graphic xlink:href="fx001"/></table-wrap></p>
    <p id="pr0560">These results, and others like them from other computers, have led us to offer two sets of default settings in <bold>audiomath</bold>. If a program starts by announcing itself as a low-latency application by calling the <bold>audiomath</bold> function <bold>LowLatencyMode(True)</bold>, a suggested latency of 4 ms will be used by default, and on Windows the WDM/KS host API will be preferred. Without this declaration, <bold>audiomath</bold> defaults to more robust, cooperative settings: on non-Windows platforms, suggested latency is at least 10 ms (typically leading to actual measured latencies of around 20 ms), and on Windows, DirectSound is preferred, with a suggested latency of 60 ms (we found this setting to be more robust than lower values in the context of moderate to high concurrent CPU use).<xref rid="fn0080" ref-type="fn">10</xref> From here, therefore, we narrow our focus to consider just WDM/KS and DirectSound.</p>
    <p id="pr0570">All results are subject to variability depending on one's hardware, as illustrated in <xref rid="tbl0030" ref-type="table">Table 3</xref>. Here we compare a variety of Windows computers and sound cards, under WDM/KS and DirectSound. The former shows much better hardware-invariance than the latter: the standard deviation across the five different onboard sound-cards' mean latencies is a mere 1.3 ms under WDM/KS, but 12.4 ms under DirectSound. The results also illustrate that a dedicated sound-card may give slight advantages (compare 3F vs. 3G), but this advantage may be undone if one chooses the wrong host API for the hardware's driver (the same comparison backfires under DirectSound: 3M vs. 3N). We can also see that some low-cost multi-channel USB devices can be used with little disadvantage (3D vs 3E, 3K vs. 3L). Our results from the same device under macOS and Linux, (not shown) also suggest that the USB latencies differ only slightly from onboard latencies.<table-wrap position="float" id="tbl0030"><label>Table 3</label><caption><p>This table shows observed audio latency and jitter (mean and standard deviation of latency across 100 repetitions) measured via the headphone sockets of various Windows computers with various different sound cards, using the method described in Section <xref rid="se0240" ref-type="sec">4</xref>. Two host APIs are compared: WDM/KS, our choice for low-latency applications, shows little variability between setups; by contrast DirectSound, our choice for cooperative applications, has much higher variability. For each host API, a suggested-latency value was chosen to be as low as possible, but high enough to avoid quality problems on <italic>all</italic> of the hardware setups. The sound-card marked <sup>†</sup> was a StarTech ICUSBAUDIO7D. The sound-card marked <sup>‡</sup> was a Creative Sound Blaster Audigy 5/Rx—before measuring it, we entered the computer's firmware settings and disabled the onboard sound-card so that the Sound Blaster was the only recognized sound output device.</p></caption><alt-text id="at0090">Table 3</alt-text><graphic xlink:href="fx002"/></table-wrap></p>
    <p id="pr0580">Finally, <xref rid="tbl0040" ref-type="table">Table 4</xref> shows the results across three different operating systems. We note that, given the right choice of settings, it is possible to achieve low latency without artifacts on all of them (4C, 4F, 4N, 4S, 4X). On macOS, low latencies can even be achieved without sacrificing cooperativeness.<xref rid="fn0090" ref-type="fn">11</xref> One other observation arising from this table is that <bold>PsychPortAudio</bold> may lack hardware-invariance under Windows: recall that on our Dell computer, its best performance (2D) came reasonably close to the overall best (2B) but now we see that on the Surface Pro it did not (4J and 4K are nowhere near 4N, 4O and 4P). Under Linux and macOS, its performance is indistinguishable from plain <bold>PortAudio</bold>. However, given a sufficiently high target latency for the particular hardware, <bold>PsychPortAudio</bold>'s outstanding contribution lies in its pre-scheduling function, which achieves very low jitter on all platforms (4D, 4L, 4U).<table-wrap position="float" id="tbl0040"><label>Table 4</label><caption><p>This table shows observed audio latency and jitter (mean and standard deviation of latency across 100 repetitions) measured via the headphone sockets of two portable computers running three different operating systems, using the method described in Section <xref rid="se0240" ref-type="sec">4</xref>. In all configurations, it was possible to achieve low latencies (under 10 ms) without audible artifacts. “Suggested latency” refers to a configurable parameter exposed by the <bold>PortAudio</bold> library and by <bold>audiomath</bold> when initializing sound hardware, except where marked with <sup>⁎</sup>: these entries were measured by adopting the best previous suggested-latency setting, but then using <bold>PsychPortAudio</bold>'s unique pre-scheduling functionality to configure a longer latency of 10 ms (or 25 ms for the Surface Pro under Windows, where <bold>PsychPortAudio</bold> could not achieve latencies under 10 ms). For each platform, the <bold>LowLatencyMode(True)</bold> default configuration is marked <sup>†</sup>, and the more-robust <bold>LowLatencyMode(False)</bold> default configuration is marked <sup>‡</sup>.</p></caption><alt-text id="at0100">Table 4</alt-text><graphic xlink:href="fx003"/></table-wrap></p>
  </sec>
  <sec id="se0260">
    <label>6</label>
    <title>Discussion</title>
    <p id="pr0590">We have documented the ways in which <bold>audiomath</bold> enables Python programmers to gather raw material to make auditory stimuli, to compose and tailor stimuli in memory, to present stimuli, and to record experimental sessions. These tasks entail manipulation, generation, visualization, decoding, encoding, recording and playback of audio data. It is possible to find many existing software tools—some programmable, some not—that offer one or more of these functions. The main advantage of <bold>audiomath</bold> is that it provides a single programmer's interface that unifies them all, with minimal third-party dependencies.</p>
    <p id="pr0600">Among these tasks, the one that tends to consume the greatest amount of time and energy during development is stimulus presentation—especially the question of precise timing. The <bold>audiomath</bold> package provides new tools for addressing this challenge, and for measuring performance. We used these tools to measure audio latency and jitter across a variety of hardware, operating systems, and settings. The results show that, given the right choices, low latency (less than 10 ms) and low jitter (less than 1 ms) can be achieved even with non-specialized off-the-shelf computer hardware. Nonetheless these good results are part of a complex landscape containing many traps for the unwary. While retaining flexibility in case the programmer should require it, <bold>audiomath</bold> simplifies the problem by facilitating the choice of priorities between low latency, low jitter, or cooperativeness and robustness.</p>
    <p id="pr0610">Note that the tests reported here were limited to consumer-grade sound processing cards—most of them “on-board”, i.e. integrated into the computer's main system board. It is encouraging to find that even these inexpensive cards can achieve low enough latencies for most neuroscientific intents and purposes. However, note that we have not yet been able to report any results representing the de-facto standard for low-latency audio, which is to use a professional-grade sound processor with a driver that supports the ASIO host API natively (i.e. not via ASIO4ALL) on Windows. So far, budget limitations have prevented us from testing hardware of high enough quality to assess the difference this might make. It would be valuable to extend our tests to more-expensive higher-quality sound processors, to assess the extent to which they might ease or remove some of the performance trade-off dilemmas.</p>
    <p id="pr0620">For some of the core audio functions—specifically visualization, decoding, encoding, recording and playback—the bulk of the code that <bold>audiomath</bold> uses “under the hood” is not our own. The tools we wrote for these purposes, while often being non-trivial works in their own right, are really wrappers around functions from trusted third-party libraries. We have described the way in which these dependencies are managed in as conservative and modular a way as possible—for example, if the third-party visualization library <bold>matplotlib</bold> has or develops some incompatibility with your system, your <bold>audiomath</bold> applications that do <italic>not</italic> perform visualization (let's say they only need to perform playback) will still work. Such behavior cannot generally be assumed with scientific Python packages, which tend to sit on top of deep hierarchies of dependencies and be catastrophically vulnerable to weaknesses in any one of them.</p>
    <p id="pr0630">By contrast, when it comes to generation and manipulation of audio content, <bold>audiomath</bold> provides original implementations. The central goal in their design was simplicity of use—hence, adding two sound objects' signals together is as simple as writing <bold>x + y</bold>, doubling a sound's amplitude is as simple as <bold>x *= 2</bold>, etc. The common operations of splicing (concatenating sounds together in time), and multiplexing (stacking channels to create multi-channel sounds) can also be performed with minimal typing, using the <bold>%</bold> and <bold>&amp;</bold> operators, respectively.<xref rid="fn0100" ref-type="fn">12</xref> In implementing these operations, an important design goal was to minimize fuss. For example, when adding two sounds to superimpose them, or multiplying them to perform amplitude modulation, or multiplexing them together, <bold>audiomath</bold> does not complain about sounds being of unequal duration—it automatically pads shorter sounds to the required length. Similarly, when adding sounds, multiplying sounds, or splicing sounds along the time axis together, <bold>audiomath</bold> does not complain if one of the arguments is monophonic and the other has multiple channels—instead, it automatically replicates the monophonic content up to the required number of channels. This tolerant approach greatly reduces the complexity of the code that users have to write.</p>
    <p id="pr0640">It is worth noting where the scope of <bold>audiomath</bold>'s functionality stops, relative to some other packages. We do not aim to perform sophisticated signal-processing in <bold>audiomath</bold>, preferring to leave the implementation details to more specialized packages such as <bold>scipy.signal</bold>, <bold>librosa</bold>, or dedicated speech-processing toolboxes. It also currently requires sound files to be completely loaded into memory before playback, lacking the functionality for streaming sounds from a file or network in the way offered by (for example) the <bold>pyglet</bold> package. This may be added in future versions.</p>
  </sec>
  <sec id="se0270">
    <title>Declarations</title>
    <sec id="se0280">
      <title>Author contribution statement</title>
      <p id="pr0660">N. Jeremy Hill: Conceived and designed the experiments; Performed the experiments; Analyzed and interpreted the data; Contributed reagents, materials, analysis tools or data; Wrote the paper.</p>
      <p id="pr0670">Scott W.J. Mooney: Performed the experiments; Contributed reagents, materials, analysis tools or data; Wrote the paper.</p>
      <p id="pr0680">Glen T. Prusky: Contributed reagents, materials, analysis tools or data; Wrote the paper.</p>
    </sec>
    <sec id="se0290">
      <title>Funding statement</title>
      <p id="pr0690">This work was supported by institutional funding from <funding-source id="gsp0020">Blythedale Children's Hospital</funding-source>, and by the National Center for Adaptive Neurotechnologies (<funding-source id="gsp0010"><institution-wrap><institution-id institution-id-type="doi">10.13039/100000002</institution-id><institution>NIH</institution></institution-wrap></funding-source> grant 7-P41-EB018783-07, National Institute for Biomedical Imaging and Bioengineering).</p>
    </sec>
    <sec sec-type="data-availability" id="se0300">
      <title>Data availability statement</title>
      <p id="pr0700">Data associated with this study has been deposited at <ext-link ext-link-type="uri" xlink:href="https://osf.io/v6dwh/" id="inf0060">https://osf.io/v6dwh/</ext-link>.</p>
    </sec>
    <sec sec-type="COI-statement" id="se0310">
      <title>Declaration of interests statement</title>
      <p id="pr0710">The authors declare no conflict of interest.</p>
    </sec>
    <sec id="se0320">
      <title>Additional information</title>
      <p id="pr0720">No additional information is available for this paper.</p>
    </sec>
  </sec>
</body>
<back>
  <ref-list id="bl0010">
    <title>References</title>
    <ref id="br0010">
      <label>1</label>
      <element-citation publication-type="journal" id="bibB665F00496EB42E3797B0721C8EE12C0s1">
        <person-group person-group-type="author">
          <name>
            <surname>Hill</surname>
            <given-names>N.J.</given-names>
          </name>
          <name>
            <surname>Mooney</surname>
            <given-names>S.W.J.</given-names>
          </name>
          <name>
            <surname>Ryklin</surname>
            <given-names>E.B.</given-names>
          </name>
          <name>
            <surname>Prusky</surname>
            <given-names>G.T.</given-names>
          </name>
        </person-group>
        <article-title>Shady: a software engine for real-time visual stimulus manipulation</article-title>
        <source>J. Neurosci. Methods</source>
        <volume>320</volume>
        <issue>C</issue>
        <year>2019</year>
        <fpage>79</fpage>
        <lpage>86</lpage>
        <pub-id pub-id-type="pmid">30946876</pub-id>
      </element-citation>
    </ref>
    <ref id="br0020">
      <label>2</label>
      <element-citation publication-type="book" id="bibF20AE8BE892358ECCC15D904A7331895s1">
        <person-group person-group-type="author">
          <name>
            <surname>Oliphant</surname>
            <given-names>T.E.</given-names>
          </name>
        </person-group>
        <chapter-title>A Guide to NumPy, vol. 1</chapter-title>
        <year>2006</year>
        <publisher-name>Trelgol Publishing</publisher-name>
        <publisher-loc>USA</publisher-loc>
      </element-citation>
    </ref>
    <ref id="br0030">
      <label>3</label>
      <element-citation publication-type="journal" id="bib16FE5DDBC337D5EA3FB2635CF44A17D2s1">
        <person-group person-group-type="author">
          <name>
            <surname>van der Walt</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Colbert</surname>
            <given-names>S.C.</given-names>
          </name>
          <name>
            <surname>Varoquaux</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>The NumPy array: a structure for efficient numerical computation</article-title>
        <source>Comput. Sci. Eng.</source>
        <volume>13</volume>
        <issue>2</issue>
        <year>2011</year>
        <fpage>22</fpage>
        <lpage>30</lpage>
      </element-citation>
    </ref>
    <ref id="br0040">
      <label>4</label>
      <element-citation publication-type="book" id="bibC89F4E6078AB8E5101B8084EF2D14F4As1">
        <person-group person-group-type="author">
          <name>
            <surname>Nelli</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <chapter-title>Python Data Analytics: With Pandas, Numpy, and Matplotlib</chapter-title>
        <year>2018</year>
        <publisher-name>Apress</publisher-name>
      </element-citation>
    </ref>
    <ref id="br0050">
      <label>5</label>
      <element-citation publication-type="book" id="bib9CC931BB9F2FE1343D5B5B742BB2E38Es1">
        <person-group person-group-type="author">
          <name>
            <surname>McFee</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Raffel</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Ellis</surname>
            <given-names>D.P.</given-names>
          </name>
          <name>
            <surname>McVicar</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Battenberg</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Nieto</surname>
            <given-names>O.</given-names>
          </name>
        </person-group>
        <chapter-title>librosa: audio and music signal analysis in Python</chapter-title>
        <source>Proceedings of the 14th Python in Science Conference</source>
        <volume>vol. 8</volume>
        <year>2015</year>
      </element-citation>
    </ref>
    <ref id="br0060">
      <label>6</label>
      <element-citation publication-type="book" id="bib7342F08B24B2BEC57C467E10960F9748s1">
        <person-group person-group-type="author">
          <name>
            <surname>ISO 226</surname>
          </name>
        </person-group>
        <chapter-title>Acoustics—normal equal-loudness-level contours</chapter-title>
        <comment>Technical committee 43</comment>
        <year>2003</year>
        <publisher-name>International Organization for Standardization</publisher-name>
        <publisher-loc>Geneva, CH</publisher-loc>
      </element-citation>
    </ref>
    <ref id="br0070">
      <label>7</label>
      <element-citation publication-type="journal" id="bibEB1A38E2E0722F8A37FBC12B5F1DB7B7s1">
        <person-group person-group-type="author">
          <name>
            <surname>Hunter</surname>
            <given-names>J.D.</given-names>
          </name>
        </person-group>
        <article-title>matplotlib: a 2D graphics environment</article-title>
        <source>Comput. Sci. Eng.</source>
        <volume>9</volume>
        <issue>3</issue>
        <year>2007</year>
        <fpage>90</fpage>
        <lpage>95</lpage>
      </element-citation>
    </ref>
    <ref id="br0080">
      <label>8</label>
      <element-citation publication-type="book" id="bibCC4E1CCE3E7A98E30986887A479E18E2s1">
        <person-group person-group-type="author">
          <name>
            <surname>McGreggor</surname>
            <given-names>D.M.</given-names>
          </name>
        </person-group>
        <chapter-title>Mastering Matplotlib</chapter-title>
        <year>2015</year>
        <publisher-name>Packt Publishing Ltd.</publisher-name>
      </element-citation>
    </ref>
    <ref id="br0090">
      <label>9</label>
      <element-citation publication-type="other" id="bib44980E759C09773AABAE7FD1473AF619s1">
        <person-group person-group-type="author">
          <name>
            <surname>Stocks</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Holkner</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Richert</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Techtonik</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Virag</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Lujan</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Sheerman-Chase</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>AVBin</article-title>
        <comment>Github project</comment>
        <ext-link ext-link-type="uri" xlink:href="https://avbin.github.io" id="inf0070">https://avbin.github.io</ext-link>
      </element-citation>
    </ref>
    <ref id="br0100">
      <label>10</label>
      <element-citation publication-type="other" id="bibBF88F318CD37E226260EFE0D0FAD2435s1">
        <person-group person-group-type="author">
          <name>
            <surname>Bencina</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Burk</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>PortAudio</article-title>
        <comment>Assembla git project</comment>
        <ext-link ext-link-type="uri" xlink:href="http://portaudio.com" id="inf0080">http://portaudio.com</ext-link>
      </element-citation>
    </ref>
    <ref id="br0110">
      <label>11</label>
      <element-citation publication-type="book" id="bib25370F942AE06259969376C16114E25Cs1">
        <person-group person-group-type="author">
          <name>
            <surname>Kleiner</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Brainard</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Pelli</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <chapter-title>What's New in Psychtoolbox-3?</chapter-title>
        <year>2007</year>
        <publisher-name>Pion Ltd.</publisher-name>
      </element-citation>
    </ref>
    <ref id="br0120">
      <label>12</label>
      <element-citation publication-type="book" id="bib65247A4D57C3C75D0056BB9B1694EA9Ds1">
        <person-group person-group-type="author">
          <name>
            <surname>Borgo</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Soranzo</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Grassi</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <chapter-title>Psychtoolbox: sound, keyboard and mouse</chapter-title>
        <source>MATLAB for Psychologists</source>
        <year>2012</year>
        <publisher-name>Springer</publisher-name>
        <fpage>249</fpage>
        <lpage>273</lpage>
      </element-citation>
    </ref>
    <ref id="br0130">
      <label>13</label>
      <element-citation publication-type="other" id="bib42A414FA116519B08182E191D6EBE76As1">
        <person-group person-group-type="author">
          <name>
            <surname>Kirk</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>A brief history of Windows audio APIs</article-title>
        <comment>Personal blog</comment>
        <ext-link ext-link-type="uri" xlink:href="http://shanekirk.com/2015/10/a-brief-history-of-windows-audio-apis" id="inf0090">http://shanekirk.com/2015/10/a-brief-history-of-windows-audio-apis</ext-link>
      </element-citation>
    </ref>
    <ref id="br0140">
      <label>14</label>
      <element-citation publication-type="book" id="bibDB2C83F4D66BB5FC6F7A76C91BB3539Cs1">
        <person-group person-group-type="author">
          <name>
            <surname>Scarpaci</surname>
            <given-names>J.W.</given-names>
          </name>
          <name>
            <surname>Colburn</surname>
            <given-names>H.S.</given-names>
          </name>
          <name>
            <surname>White</surname>
            <given-names>J.A.</given-names>
          </name>
        </person-group>
        <chapter-title>A system for real-time virtual auditory space</chapter-title>
        <source>Proceedings of ICAD 05—Eleventh Meeting of the International Conference on Auditory Display</source>
        <year>2005</year>
      </element-citation>
    </ref>
    <ref id="br0150">
      <label>15</label>
      <element-citation publication-type="book" id="bibAFAC29A3EC64227B20BB0F43345C61E0s1">
        <person-group person-group-type="author">
          <name>
            <surname>Beazley</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <chapter-title>Understanding the Python GIL</chapter-title>
        <source>Proceedings of the PyCON Python Conference</source>
        <conf-name>Atlanta, Georgia</conf-name>
        <year>2010</year>
      </element-citation>
    </ref>
    <ref id="br0160">
      <label>16</label>
      <element-citation publication-type="journal" id="bibDB6577DE55D59008510C2CAC1FC40447s1">
        <person-group person-group-type="author">
          <name>
            <surname>Hill</surname>
            <given-names>N.J.</given-names>
          </name>
          <name>
            <surname>Moinuddin</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Kienzle</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Häuser</surname>
            <given-names>A.-K.</given-names>
          </name>
          <name>
            <surname>Schalk</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Communication and control by listening: toward optimal design of a two-class auditory streaming brain-computer interface</article-title>
        <source>Front. Neurosci.</source>
        <volume>6</volume>
        <year>2012</year>
        <fpage>181</fpage>
        <pub-id pub-id-type="pmid">23267312</pub-id>
      </element-citation>
    </ref>
    <ref id="br0170">
      <label>17</label>
      <element-citation publication-type="journal" id="bib8C0D683BC3D9D4686BADC3180821C8C5s1">
        <person-group person-group-type="author">
          <name>
            <surname>Schalk</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>McFarland</surname>
            <given-names>D.J.</given-names>
          </name>
          <name>
            <surname>Hinterberger</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Birbaumer</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Wolpaw</surname>
            <given-names>J.R.</given-names>
          </name>
        </person-group>
        <article-title>BCI2000: a general-purpose brain-computer interface (BCI) system</article-title>
        <source>IEEE Trans. Biomed. Eng.</source>
        <volume>51</volume>
        <issue>6</issue>
        <year>2004</year>
        <fpage>1034</fpage>
        <lpage>1043</lpage>
        <pub-id pub-id-type="pmid">15188875</pub-id>
      </element-citation>
    </ref>
    <ref id="br0180">
      <label>18</label>
      <element-citation publication-type="book" id="bib51C5C0593BB10615256EAAB256FCA22Es1">
        <person-group person-group-type="author">
          <name>
            <surname>Schalk</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Mellinger</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <chapter-title>A Practical Guide to Brain-Computer Interfacing with BCI2000: General-Purpose Software for Brain-Computer Interface Research, Data Acquisition, Stimulus Presentation, and Brain Monitoring</chapter-title>
        <year>2010</year>
        <publisher-name>Springer Science &amp; Business Media</publisher-name>
      </element-citation>
    </ref>
    <ref id="br0190">
      <label>19</label>
      <element-citation publication-type="journal" id="bibAD28D3FA7D4956C9E015C88DD406328Ds1">
        <person-group person-group-type="author">
          <name>
            <surname>Wilson</surname>
            <given-names>J.A.</given-names>
          </name>
          <name>
            <surname>Mellinger</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Schalk</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Williams</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>A procedure for measuring latencies in brain-computer interfaces</article-title>
        <source>IEEE Trans. Biomed. Eng.</source>
        <volume>57</volume>
        <issue>7</issue>
        <year>2010</year>
        <fpage>1785</fpage>
        <lpage>1797</lpage>
        <pub-id pub-id-type="pmid">20403781</pub-id>
      </element-citation>
    </ref>
    <ref id="br0200">
      <label>20</label>
      <element-citation publication-type="journal" id="bibC7019B7184B0E0B43ACB767AABD315CBs1">
        <person-group person-group-type="author">
          <name>
            <surname>Hill</surname>
            <given-names>N.J.</given-names>
          </name>
          <name>
            <surname>Schölkopf</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>An online brain-computer interface based on shifting attention to concurrent streams of auditory stimuli</article-title>
        <source>J. Neural Eng.</source>
        <volume>9</volume>
        <issue>2</issue>
        <year>2012</year>
        <object-id pub-id-type="publisher-id">026011</object-id>
      </element-citation>
    </ref>
    <ref id="br0210">
      <label>21</label>
      <element-citation publication-type="book" id="bib3E5B3DD45F0C07BEA5BBBA82FCDC605Es1">
        <person-group person-group-type="author">
          <name>
            <surname>Edstrom</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <chapter-title>Arduino for Musicians: A Complete Guide to Arduino and Teensy Microcontrollers</chapter-title>
        <year>2016</year>
        <publisher-name>Oxford University Press</publisher-name>
      </element-citation>
    </ref>
  </ref-list>
  <ack id="ac0010">
    <title>Acknowledgements</title>
    <p id="pr0650">We thank the editor and anonymous reviewer for their helpful comments and tips. We would also like to thank Bernd Battes at the Max Planck Institute for Intelligent Systems, who designed, built and tested the circuit shown in <xref rid="fg0030" ref-type="fig">Fig. 2</xref>.</p>
  </ack>
  <fn-group>
    <fn id="fn0010">
      <label>1</label>
      <p id="np0010">For background details regarding Windows audio APIs, see Kirk <xref rid="br0130" ref-type="bibr">[13]</xref>.</p>
    </fn>
    <fn id="fn0110">
      <label>2</label>
      <p id="np0110">WASAPI stands for Windows Audio Session Application Programmer's Interface.</p>
    </fn>
    <fn id="fn0020">
      <label>3</label>
      <p id="np0020">Python in particular may exacerbate the effects of resource contention between threads of the same process, because of its so-called Global Interpreter Lock <xref rid="br0150" ref-type="bibr">[15]</xref>.</p>
    </fn>
    <fn id="fn0030">
      <label>4</label>
      <p id="np0030">ASIO stands for Audio Stream Input/Output.</p>
    </fn>
    <fn id="fn0040">
      <label>5</label>
      <p id="np0040">WDM/KS stands for Windows Driver Model/Kernel Streaming.</p>
    </fn>
    <fn id="fn0050">
      <label>6</label>
      <p id="np0050">MME stands for Multi-Media Extensions and is sometimes also known as WinMM.</p>
    </fn>
    <fn id="fn0060">
      <label>7</label>
      <p id="np0060">The definition of “low enough” may differ in different applications—for example, ±5 ms may be acceptable when analyzing late or even mid-latency event-related potentials in EEG, but not when attempting to distinguish components of early brainstem evoked responses that are only one or two milliseconds apart.</p>
    </fn>
    <fn id="fn0070">
      <label>8</label>
      <p id="np0070">Our shorthand for describing this effect is the “Pink Floyd problem”, referring to fact that many of that band's albums start with such a gradual fade-in that they cause a period of uncertainty as to whether or not one actually pressed “play”.</p>
    </fn>
    <fn id="fn0120">
      <label>9</label>
      <p id="np0120">Luckily, it is generally safe to assume precise time-locking between the audible channels and the artificial sounds—precise inter-channel timing is a mission-critical feature of even the cheapest soundcards.</p>
    </fn>
    <fn id="fn0080">
      <label>10</label>
      <p id="np0080">Naturally the user still has the option to override either set of defaults, as well as the choice of the third path of loading the <bold>PsychPortAudio</bold> back-end (which defaults to using the WASAPI host API on Windows) if low jitter is the overriding priority.</p>
    </fn>
    <fn id="fn0090">
      <label>11</label>
      <p id="np0090">On Linux, the Advanced Linux Sound Architecture (ALSA) host API can also be used in a cooperative mode, in which we have found latency to be very high (ca. 150 ms) on our Surface Pro, but encouragingly lower on other hardware (ca. 8 ms on our Lenovo Horizon II).</p>
    </fn>
    <fn id="fn0100">
      <label>12</label>
      <p id="np0100">When dealing with sound arrays, we have never come across meaningful applications of the modulo division operator (the conventional meaning of <bold>%</bold> in Python) or bitwise logical AND (the conventional meaning of <bold>&amp;</bold>), so confusion is very unlikely in typical use-cases provided one is already familiar with <bold>audiomath</bold>'s conventions. However, in situations where familiarity cannot be assumed, use of such redefined operators is likely to reduce code readability. For this reason, functional forms <bold>Concatenate()</bold> and <bold>Stack()</bold> are also available, either as global functions (equivalent to <bold>%</bold> and <bold>&amp;</bold> respectively) or as object methods (equivalent to <bold>%=</bold> and <bold>&amp;=</bold>).</p>
    </fn>
  </fn-group>
</back>
