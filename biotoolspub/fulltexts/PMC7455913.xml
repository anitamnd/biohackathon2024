<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Genomics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Genomics</journal-id>
    <journal-title-group>
      <journal-title>BMC Genomics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2164</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7455913</article-id>
    <article-id pub-id-type="publisher-id">6978</article-id>
    <article-id pub-id-type="doi">10.1186/s12864-020-06978-0</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>ACEP: improving antimicrobial peptides recognition through automatic feature fusion and amino acid embedding</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Fu</surname>
          <given-names>Haoyi</given-names>
        </name>
        <address>
          <email>fhy11235813@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Cao</surname>
          <given-names>Zicheng</given-names>
        </name>
        <address>
          <email>caozichengtom@foxmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Mingyuan</given-names>
        </name>
        <address>
          <email>limingyuan0430@163.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Wang</surname>
          <given-names>Shunfang</given-names>
        </name>
        <address>
          <email>sfwang_66@ynu.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.440773.3</institution-id><institution-id institution-id-type="ISNI">0000 0000 9342 2456</institution-id><institution>School of Information Science and Engineering, Yunnan University, </institution></institution-wrap>Kunming, 650500 China </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.12981.33</institution-id><institution-id institution-id-type="ISNI">0000 0001 2360 039X</institution-id><institution>School of Public Health (Shenzhen), Sun Yat-sen University, </institution></institution-wrap>Guangzhou, 510006 China </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>28</day>
      <month>8</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>28</day>
      <month>8</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2020</year>
    </pub-date>
    <volume>21</volume>
    <elocation-id>597</elocation-id>
    <history>
      <date date-type="received">
        <day>31</day>
        <month>1</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>11</day>
        <month>8</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Antimicrobial resistance is one of our most serious health threats. Antimicrobial peptides (AMPs), effecter molecules of innate immune system, can defend host organisms against microbes and most have shown a lowered likelihood for bacteria to form resistance compared to many conventional drugs. Thus, AMPs are gaining popularity as better substitute to antibiotics. To aid researchers in novel AMPs discovery, we design computational approaches to screen promising candidates.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">In this work, we design a deep learning model that can learn amino acid embedding patterns, automatically extract sequence features, and fuse heterogeneous information. Results show that the proposed model outperforms state-of-the-art methods on recognition of AMPs. By visualizing data in some layers of the model, we overcome the black-box nature of deep learning, explain the working mechanism of the model, and find some import motifs in sequences.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">ACEP model can capture similarity between amino acids, calculate attention scores for different parts of a peptide sequence in order to spot important parts that significantly contribute to final predictions, and automatically fuse a variety of heterogeneous information or features. For high-throughput AMPs recognition, open source software and datasets are made freely available at <ext-link ext-link-type="uri" xlink:href="https://github.com/Fuhaoyi/ACEP">https://github.com/Fuhaoyi/ACEP</ext-link>.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Antimicrobial resistance</kwd>
      <kwd>Antimicrobial peptide</kwd>
      <kwd>Deep learning</kwd>
      <kwd>Feature fusion</kwd>
      <kwd>Visualization</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2020</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p>Antimicrobial resistance is one of our most serious health threats. Infections from resistant bacteria are now too common, and some pathogens have even become resistant to the multiple types of antibiotics [<xref ref-type="bibr" rid="CR1">1</xref>]. Natural antimicrobials, known as host defense peptides or antimicrobial peptides (AMPs), defend host organisms against microbes, and most have shown a lowered likelihood for bacteria to form resistance compared to many conventional drugs [<xref ref-type="bibr" rid="CR2">2</xref>]. AMPs have been demonstrated to kill Gram-negative and Gram-positive bacteria, enveloped viruses, fungi and even transformed or cancerous cells; thus, AMPs are considered as potential novel antimicrobial compounds [<xref ref-type="bibr" rid="CR3">3</xref>]. Unlike the majority of conventional antibiotics, AMPs frequently destabilize biological membranes, form transmembrane channels and may also have the ability to enhance immunity by functioning as immunomodulators [<xref ref-type="bibr" rid="CR4">4</xref>].</p>
    <p>Over the last few decades, several AMPs have successfully been approved as drugs by FDA, which has prompted an interest in these AMPs. To aid researchers in novel AMP discovery, a variety of computational approaches are proposed for AMP recognition. Many incorporate machine learning algorithms or statistical analysis techniques, such as artificial neural networks (ANN) [<xref ref-type="bibr" rid="CR5">5</xref>], discriminant analysis (DA) [<xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR7">7</xref>], fuzzy k-nearest neighbors (KNN) [<xref ref-type="bibr" rid="CR8">8</xref>], hidden Markov models (HM) [<xref ref-type="bibr" rid="CR9">9</xref>], logistic regression (LR) [<xref ref-type="bibr" rid="CR10">10</xref>, <xref ref-type="bibr" rid="CR11">11</xref>], random forests (RF) [<xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR10">10</xref>], support vector machines (SVM) [<xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR12">12</xref>] and deep neural network (DNN) [<xref ref-type="bibr" rid="CR13">13</xref>].</p>
    <p>To improve the recognition performance of AMPs, many popular feature extraction methods have been proposed. Basic amino acid counts over the N- and C-termini or the full peptide are used by the AntiBP2 methods [<xref ref-type="bibr" rid="CR14">14</xref>]. The compositional, physicochemical and structural features are incorporated into the Pseudo-amino acid composition method [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR16">16</xref>]. Constructing and selecting complex sequence-based features that capture information about distal patterns within a peptide are used in the evolutionary feature construction method [<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR18">18</xref>]. Physicochemical properties, such as charge, hydrophobicity, isoelectric point, aggregation propensity and more, are also used to encode sequences as numerical vectors [<xref ref-type="bibr" rid="CR19">19</xref>].</p>
    <p>In this paper, we improve existing AMP recognition technology. First, we introduce an amino acid embedding tensor that can map amino acids to tensors of real numbers automatically, which allows neural networks to discover similarity between amino acids. We use position-specific scoring matrices (PSSM) and these tensors to encode peptide sequences. The PSSMs contain the evolutionary information of sequences, which contributes to reducing the impact of amino acid variations in peptide sequences. Second, we design a new deep neural network, which has better performance on AMP recognition than existing methods. By applying the convolutional (Conv) layer and the ‘long short-term memory’ (LSTM) layer to our DNN, the model can effectively capture sequence features. Third, we discover some important motifs in sequences and build a ‘convolution and concatenation’ (CVCA) layer to fuse features by using the attention mechanism of natural language processing. Fourth, we ‘open’ the black box of ACEP model and explain the relationship between the patterns in deep neural network and the characteristics of sequences itself. Finally, we provide the source code and data on GitHub. The methods can be used to encode other types of protein sequences and improve the performance of sequence pattern recognition.</p>
    <p>The superiority of DNN has been proven in many problems of bioinformatics, such as protein secondary structure prediction [<xref ref-type="bibr" rid="CR20">20</xref>], protein folding recognition [<xref ref-type="bibr" rid="CR21">21</xref>], membrane protein types prediction [<xref ref-type="bibr" rid="CR22">22</xref>], drug discovery [<xref ref-type="bibr" rid="CR23">23</xref>, <xref ref-type="bibr" rid="CR24">24</xref>], brain disease detection [<xref ref-type="bibr" rid="CR25">25</xref>], etc. The tensor technique has been used for neural network data representation. A tensor is a container which can house data in <italic>N</italic> dimensions, along with its linear operations. In the paper, we refer to the data in neural network as tensors, which are generated during training neural network.</p>
    <p>The attention mechanism has been used in natural language processing [<xref ref-type="bibr" rid="CR26">26</xref>, <xref ref-type="bibr" rid="CR27">27</xref>], computer vision [<xref ref-type="bibr" rid="CR28">28</xref>] and bioinformatics [<xref ref-type="bibr" rid="CR29">29</xref>] to produce interpretable results for deep learning models. This strategy assigns different weights to each input feature, so that the model can focus on the most crucial features to perform better prediction.</p>
  </sec>
  <sec id="Sec2">
    <title>Results and discussion</title>
    <sec id="Sec3">
      <title>Model evaluation</title>
      <p>We evaluate classification performance in terms of sensitivity (SENS), specificity (SPEC), accuracy (ACC) and Matthews Correlation Coefficient (MCC), which are defined using the number of true positive (TP), true negative (TN), false positive (FP) and false negative (FN) predictions.
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ Sensitivity = \frac{{TP}}{{TP + FN}} \times 100\%  $$ \end{document}</tex-math><mml:math id="M2"><mml:mtext mathvariant="italic">Sensitivity</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext mathvariant="italic">TP</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">FN</mml:mtext></mml:mrow></mml:mfrac><mml:mo>×</mml:mo><mml:mn>100</mml:mn><mml:mi>%</mml:mi></mml:math><graphic xlink:href="12864_2020_6978_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>
        <disp-formula id="Equ2">
          <label>2</label>
          <alternatives>
            <tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ Specificity = \frac{{TN}}{{TN + FP}} \times 100\%  $$ \end{document}</tex-math>
            <mml:math id="M4">
              <mml:mtext mathvariant="italic">Specificity</mml:mtext>
              <mml:mo>=</mml:mo>
              <mml:mfrac>
                <mml:mrow>
                  <mml:mtext mathvariant="italic">TN</mml:mtext>
                </mml:mrow>
                <mml:mrow>
                  <mml:mtext mathvariant="italic">TN</mml:mtext>
                  <mml:mo>+</mml:mo>
                  <mml:mtext mathvariant="italic">FP</mml:mtext>
                </mml:mrow>
              </mml:mfrac>
              <mml:mo>×</mml:mo>
              <mml:mn>100</mml:mn>
              <mml:mi>%</mml:mi>
            </mml:math>
            <graphic xlink:href="12864_2020_6978_Article_Equ2.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>
        <disp-formula id="Equ3">
          <label>3</label>
          <alternatives>
            <tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ Accuracy = \frac{{TP + TN}}{{TP + FP + TN + FN}} \times 100\%  $$ \end{document}</tex-math>
            <mml:math id="M6">
              <mml:mtext mathvariant="italic">Accuracy</mml:mtext>
              <mml:mo>=</mml:mo>
              <mml:mfrac>
                <mml:mrow>
                  <mml:mtext mathvariant="italic">TP</mml:mtext>
                  <mml:mo>+</mml:mo>
                  <mml:mtext mathvariant="italic">TN</mml:mtext>
                </mml:mrow>
                <mml:mrow>
                  <mml:mtext mathvariant="italic">TP</mml:mtext>
                  <mml:mo>+</mml:mo>
                  <mml:mtext mathvariant="italic">FP</mml:mtext>
                  <mml:mo>+</mml:mo>
                  <mml:mtext mathvariant="italic">TN</mml:mtext>
                  <mml:mo>+</mml:mo>
                  <mml:mtext mathvariant="italic">FN</mml:mtext>
                </mml:mrow>
              </mml:mfrac>
              <mml:mo>×</mml:mo>
              <mml:mn>100</mml:mn>
              <mml:mi>%</mml:mi>
            </mml:math>
            <graphic xlink:href="12864_2020_6978_Article_Equ3.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>
        <disp-formula id="Equ4">
          <label>4</label>
          <alternatives>
            <tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \begin{aligned} &amp; MCC = \\ &amp; \frac{{TP \times TN - FN \times FP}}{{\sqrt {(TP + FN)(TN + FP)(TP + FP)(TN + FN)}}} \end{aligned}  $$ \end{document}</tex-math>
            <mml:math id="M8">
              <mml:mtable>
                <mml:mtr>
                  <mml:mtd>
                    <mml:mtext mathvariant="italic">MCC</mml:mtext>
                    <mml:mo>=</mml:mo>
                  </mml:mtd>
                </mml:mtr>
                <mml:mtr>
                  <mml:mtd>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:mtext mathvariant="italic">TP</mml:mtext>
                        <mml:mo>×</mml:mo>
                        <mml:mtext mathvariant="italic">TN</mml:mtext>
                        <mml:mo>−</mml:mo>
                        <mml:mtext mathvariant="italic">FN</mml:mtext>
                        <mml:mo>×</mml:mo>
                        <mml:mtext mathvariant="italic">FP</mml:mtext>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:msqrt>
                          <mml:mrow>
                            <mml:mo>(</mml:mo>
                            <mml:mtext mathvariant="italic">TP</mml:mtext>
                            <mml:mo>+</mml:mo>
                            <mml:mtext mathvariant="italic">FN</mml:mtext>
                            <mml:mo>)</mml:mo>
                            <mml:mo>(</mml:mo>
                            <mml:mtext mathvariant="italic">TN</mml:mtext>
                            <mml:mo>+</mml:mo>
                            <mml:mtext mathvariant="italic">FP</mml:mtext>
                            <mml:mo>)</mml:mo>
                            <mml:mo>(</mml:mo>
                            <mml:mtext mathvariant="italic">TP</mml:mtext>
                            <mml:mo>+</mml:mo>
                            <mml:mtext mathvariant="italic">FP</mml:mtext>
                            <mml:mo>)</mml:mo>
                            <mml:mo>(</mml:mo>
                            <mml:mtext mathvariant="italic">TN</mml:mtext>
                            <mml:mo>+</mml:mo>
                            <mml:mtext mathvariant="italic">FN</mml:mtext>
                            <mml:mo>)</mml:mo>
                          </mml:mrow>
                        </mml:msqrt>
                      </mml:mrow>
                    </mml:mfrac>
                  </mml:mtd>
                </mml:mtr>
              </mml:mtable>
            </mml:math>
            <graphic xlink:href="12864_2020_6978_Article_Equ4.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>We also make use of the receiver-operating characteristic (ROC) curve [<xref ref-type="bibr" rid="CR30">30</xref>] to compare the performance of various methods. The ROC curve shows the performance of a classifier as the discrimination threshold is varied. In ROC curve figure, the x-axis represents the false positive rate and the y-axis represents the true positive rate. We calculate the area under the ROC curve (AUC) using the scikit-learn package in Python to evaluate performance in a quantitative, comparative setting. AUC ranges from 0.5 (corresponding to a random guess) to 1 (corresponding to the case when all predictions are correct).</p>
    </sec>
    <sec id="Sec4">
      <title>Model performance</title>
      <p>Table <xref rid="Tab1" ref-type="table">1</xref> shows the classification performance, where columns 1 and 2 list training set and testing set, and columns 3-7 list SENS, SPEC, ACC, MCC and AUC. In particular, row 3 shows the performance of ACEP on the independent testing dataset, and the accuracy exceeds 93%, indicating that the model has a good generalization ability. The SENS, SPEC, ACC, and AUC values are all over 90%, and the MCC score is over 0.85. This row is used to compare with other AMP recognition methods. Row 5 shows recognition performance in a 10-fold cross-validation (CV) setting, where each of 10-folds is used once as a testing data with the model trained on the remaining 9-folds. The CV results represents the average performance of ACEP for out-of-sample data, and the relatively low standard deviation of ACC, MCC, and AUC indicates that our model has strong performance on approximately 90% of the data. Examining FN sequences from ACEP model on the testing set reveals 54 AMPs are missed (the detailed list is available in the Additional file Table).
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Model performance on different training and evaluation data partitions</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Training set</th><th align="left">Evaluation set</th><th align="left">SENS(%)</th><th align="left">SPEC(%)</th><th align="left">ACC(%)</th><th align="left">MCC</th><th align="left">AUC (%)</th></tr></thead><tbody><tr><td align="left">Train</td><td align="left">Tune</td><td align="left">95.76</td><td align="left">83.85</td><td align="left">87.80</td><td align="left">0.7582</td><td align="left">96.67</td></tr><tr><td align="left">Train</td><td align="left">Test</td><td align="left">93.39</td><td align="left">90.44</td><td align="left">91.92</td><td align="left">0.8388</td><td align="left">97.22</td></tr><tr><td align="left">Train+Tune</td><td align="left">Test</td><td align="left">92.42</td><td align="left">94.17</td><td align="left">93.39</td><td align="left">0.8663</td><td align="left">97.63</td></tr><tr><td align="left">All Data</td><td align="left">All Data</td><td align="left">98.26</td><td align="left">99.66</td><td align="left">98.96</td><td align="left">0.9793</td><td align="left">99.94</td></tr><tr><td align="left">All Data</td><td align="left">10-fold CV</td><td align="left">91.37(± 1.05)</td><td align="left">93.32(± 1.73)</td><td align="left">92.46(± 0.87)</td><td align="left">0.8474(± 0.01)</td><td align="left">96.79(± 0.47)</td></tr></tbody></table><table-wrap-foot><p>Note: Performance is shown for ACEP model built and evaluated on the datasets listed in columns 1 and 2, respectively, on metrics listed in columns 3-7. The bottom line shows 10-fold CV performance, and the standard deviation is shown in parentheses</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec5">
      <title>Comparison with state-of-the-art methods</title>
      <p>We compare ACEP model with 9 state-of-the-art machine learning methods proposed for AMP recognition between 2010 and 2018. In these methods, AntiBP2 used some new features based on terminal sequence composition; CAMP tried several common machine learning classifiers and a simple artificial neural network; iAMPpred introduced physicochemical characteristics and PseAAC; AMPScanner used deep learning technology. In Table <xref rid="Tab2" ref-type="table">2</xref>, we list these methods chronologically, and line 10 shows our model. The bold in table 2 represent the best performance for a given metric.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Performance comparison on the AMP dataset testing partition</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">SENS(%)</th><th align="left">SPEC(%)</th><th align="left">ACC(%)</th><th align="left">MCC</th><th align="left">AUC(%)</th></tr></thead><tbody><tr><td align="left">AntiBP2</td><td align="left">87.91</td><td align="left">90.80</td><td align="left">89.37</td><td align="left">0.7876</td><td align="left">89.36</td></tr><tr><td align="left">CAMPr3-ANN</td><td align="left">83.00</td><td align="left">85.11</td><td align="left">84.05</td><td align="left">0.6813</td><td align="left">84.05</td></tr><tr><td align="left">CAMPr3-DA</td><td align="left">87.07</td><td align="left">80.75</td><td align="left">83.91</td><td align="left">0.6797</td><td align="left">89.97</td></tr><tr><td align="left">CAMPr3-RF</td><td align="left"><bold>92.69</bold></td><td align="left">82.44</td><td align="left">87.57</td><td align="left">0.7553</td><td align="left">93.63</td></tr><tr><td align="left">CAMPr3-SVM</td><td align="left">88.62</td><td align="left">80.47</td><td align="left">84.55</td><td align="left">0.6933</td><td align="left">90.62</td></tr><tr><td align="left">iAMP-2L</td><td align="left">83.99</td><td align="left">85.86</td><td align="left">84.90</td><td align="left">0.6983</td><td align="left">84.90</td></tr><tr><td align="left">iAMPpred</td><td align="left">89.33</td><td align="left">87.22</td><td align="left">88.27</td><td align="left">0.7656</td><td align="left">94.44</td></tr><tr><td align="left">gkmSVM</td><td align="left">88.34</td><td align="left">90.59</td><td align="left">89.46</td><td align="left">0.7895</td><td align="left">94.98</td></tr><tr><td align="left">AMPScanner</td><td align="left">89.88</td><td align="left">92.69</td><td align="left">91.29</td><td align="left">0.8261</td><td align="left">96.30</td></tr><tr><td align="left">ACEP</td><td align="left">92.41</td><td align="left"><bold>93.67</bold></td><td align="left"><bold>93.04</bold></td><td align="left"><bold>0.8610</bold></td><td align="left"><bold>97.78</bold></td></tr></tbody></table><table-wrap-foot><p>Note: Recognition performance on the testing dataset is shown for state-of-the-art methods (listed in column 1) on the metrics listed in columns 2-6. The best performance on a metric is marked in bold. Our deep neural network is shown in row 10</p></table-wrap-foot></table-wrap></p>
      <p>From Table <xref rid="Tab2" ref-type="table">2</xref>, we can see that our method has the best performance in terms of SPEC, ACC, MCC and AUC. The random forests CAMPr3-RF achieves the highest SENS score (0.29% higher than our method). AMPScanner attains a similar performance (ACC and MCC values are reduced by approximately 2% and 0.04, respectively, compared with our method) due to using a Convolutional LSTM neural networks. The overall performance of the latest version of iAMPpred is also very good (ACC and MCC values are reduced by 4.7% and 0.1, respectively compared with our method). In addition, the AntiBP2 method limits the length of input sequences, so 211 test sequences are excluded when using this method for testing.</p>
      <p>Figure <xref rid="Fig1" ref-type="fig">1</xref>(a) compares the performance of various methods intuitively by plotting ROC curves. As shown in Table <xref rid="Tab2" ref-type="table">2</xref>, AUC ranges from 84.05% to 97.78%. The AUC of ACEP (blue ROC curve) is approximately 1% higher than the suboptimal AMPScanner (orange ROC curve). The ROC curves of these methods are sorted in descending order according to AUC. CAMPr3-ANN does not provide the probability value of prediction results, so straight line is used to approximate the ROC curve.
<fig id="Fig1"><label>Fig. 1</label><caption><p><bold>a</bold> ROC curves for the various methods compared, ordered by AUC. <bold>b</bold> Training history curves</p></caption><graphic xlink:href="12864_2020_6978_Fig1_HTML" id="MO1"/></fig></p>
      <p>To assess the stability of ACEP model in training process, we use training history data recorded by Keras to plot the curve of accuracy and training epochs, as shown in the Fig. <xref rid="Fig1" ref-type="fig">1</xref>(b). The red line is the accuracy of training data, and the green line is the accuracy of testing data. During training, the accuracy of training data and testing data increased steadily with the number of training epochs.</p>
    </sec>
    <sec id="Sec6">
      <title>Model visualization and analysis</title>
      <p>To overcome the black-box nature of deep learning and enhance the interpretability of ACEP, here we visualize four important tensors in the neural networks, including the amino acid embedding tensor, the attention scores in the CVCA layer, the fusion tensor and the attention scores in the LSTM layer.</p>
      <p>First, we extract the embedding tensor (<italic>E</italic>) carrying evolutionary information from the deep neural network. We use scikit-learn’s k-means algorithm [<xref ref-type="bibr" rid="CR31">31</xref>] to cluster these amino acids into 5 clusters. Then, we use t-SNE algorithm [<xref ref-type="bibr" rid="CR32">32</xref>] to reduce the dimensions of each amino acid tensor to 2D. Figure <xref rid="Fig2" ref-type="fig">2</xref>a shows the clustering results of 20 amino acids in <italic>E</italic> after dimension reduction. In this case, the distance indicates the similarity of amino acids. The amino acids with a shorter projection distance have more similar activation patterns, and these with longer projection distance have more differences.
<fig id="Fig2"><label>Fig. 2</label><caption><p><bold>a</bold> The clustering of amino acid embedding tensors with evolutionary information. <bold>b</bold> The attention scores of evolutionary information, raw sequence information and supplementary information. <bold>c</bold> The raw sequences before being processed. <bold>d</bold> The fusion tensors after being processed</p></caption><graphic xlink:href="12864_2020_6978_Fig2_HTML" id="MO2"/></fig></p>
      <p>In Fig. <xref rid="Fig2" ref-type="fig">2</xref>a, the negative amino acids aspartic acid (D) and glutamic acid (E) are clustered together because they both contain a negatively charged side chain. The amino acids with uncharged side chains, such as serine (S), threonine (T), asparagine (N) and glutamine (Q), are also close together in the cluster C1. Cysteine (C), which forms disulfide bonds, stands on its own in the top right, because it plays a unique role in structure formation or ligand interactions. It is perhaps unsurprising to see proline (P) is slightly isolated and distant. Proline-rich AMPs are shown to inactivate an intracellular biopolymer in bacteria without destroying or remaining attached to the bacterial cell membrane, and as such emerged as viable candidates for the treatment of mammalian infections [<xref ref-type="bibr" rid="CR33">33</xref>]. These amino acid tensors are automatically generated by ACEP model during training.</p>
      <p>ACEP model can fuse evolutionary information (EI), raw sequence information (RI) and supplementary information (SI) into a fusion tensor. The attention score (<bold><italic>μ</italic></bold><sup><bold><italic>′</italic></bold></sup>) in CVCA layer can indicate which information ACEP model tends to pay attention to. We collect attention scores of 1424 sequences and plot the statistical graph of attention scores vs. sequence length, as Fig. <xref rid="Fig2" ref-type="fig">2</xref>b shows. We find that the attention score of EI increases with the sequence length because the phylogenetic information is more abundant for long sequences. And the attention score of SI decreases as the sequence length increases because amino acid composition only becomes available at short sequences; the attention score of RI remains almost unchanged because it’s not related to length. These attention scores are predicted by ACEP model, thus we speculate that the DNN model has learned a concept consistent with our cognition.</p>
      <p>The fusion tensor (<italic>F</italic><sub><italic>meg</italic></sub>) is a new representation of sequences generated by ACEP after integrating EI, RI and SI. In order to evaluate the quality of fusion tensors into which three types of information are fused, the 1424 sequences in testing dataset are projected onto a 2D space by using t-SNE. Figure <xref rid="Fig2" ref-type="fig">2</xref>c shows the raw sequences, and Fig. <xref rid="Fig2" ref-type="fig">2</xref>d shows the fusion tensors. The AMPs are represented by blue dots, and the non-AMPs are represented by orange dots. The fusion tensor forms two clusters in space, thus they are very effective to distinguish AMPs and non-AMPs.</p>
      <p>The attention scores (<bold><italic>β</italic></bold><sup><bold><italic>′</italic></bold></sup>) in the LSTM layer indicates which parts of a sequence are the most important (the length of sequences changes from 200 to 40 after passing through the pooling layer with a window of length 5). We calculate the average attention scores of 712 AMP sequences at 40 different positions and plot Fig. <xref rid="Fig3" ref-type="fig">3</xref>a. In Fig. <xref rid="Fig3" ref-type="fig">3</xref>a, the average attention score increases from 0 to 0.3 along sequence direction because the first half is invalid padding parts and the last half is the real sequence. The attention scores of the padding are close to 0s, indicating ACEP model can automatically ignore the padding and can effectively process variable-length sequences. We only list the results of P21 to P40, and the attention scores of P1 to P20 are all close to 0 (the complete results are shown in the Additional file Figure).
<fig id="Fig3"><label>Fig. 3</label><caption><p><bold>a</bold> The average attention scores calculated from 712 AMPs. <bold>b</bold> The heat map that visualizes the attention scores of different parts of 10 sequences. <bold>c</bold> The attention scores in sequences 1-3. <bold>d</bold> The similar clips found by the attention mechanism</p></caption><graphic xlink:href="12864_2020_6978_Fig3_HTML" id="MO3"/></fig></p>
      <p>Next, we randomly select 10 AMP sequences and use ACEP model to calculate the attention scores of each part in the sequences. Through the attention mechanism, we can discover some important motifs in AMP sequences. Figure <xref rid="Fig3" ref-type="fig">3</xref>b shows the attention scores of 10 AMP sequences, and the brightness of blocks correspond to the attention scores. In subsequent calculations, the DNN will pay more attention to the parts with higher scores and ignore the parts with lower scores. Fig. <xref rid="Fig3" ref-type="fig">3</xref>c corresponds to the first three sequences in Fig. <xref rid="Fig3" ref-type="fig">3</xref>b, and the relatively high attention scores of some clips in the sequences imply that these parts may be active motifs and functional parts. For instance, the attention scores of sequence 1 have four peaks, which are the P27, P31, P36 and P39, and they exactly correspond to four <italic>α</italic>-<italic>helix</italic> structures in the sequence shown in PDB database (PDB ID: 1L9L) [<xref ref-type="bibr" rid="CR34">34</xref>]. The subsequence ‘QRSVSNAATRVCRTGRSRW’ has some consecutive relatively high attention scores because it’s an active motif against <italic>B. subtilis</italic> and <italic>E. coli</italic> [<xref ref-type="bibr" rid="CR35">35</xref>]. In sequence 2, the part P33-P35 with the highest attention scores corresponds to the structure of two <italic>β</italic>-<italic>strands</italic> connected by a <italic>turn</italic> (PDB ID: 2RNG) [<xref ref-type="bibr" rid="CR36">36</xref>]. In sequence 3, the attention score of P30 is almost 0 due to no secondary structure, and the P36 and P38 are higher attention scores due to corresponding to two <italic>α</italic>-<italic>helix</italic> (PDB ID: 2N8P) [<xref ref-type="bibr" rid="CR37">37</xref>]. In addition, we analyze the clips of AMP sequences with attention scores over 0.2 and find 67 clusters of similar clips. Taking cluster 1 for instance, the DNN model pays more attention to these clips that are quite similar, which implies that these clips may have some potential patterns contribute to peptide design. Based on the number of similar clips in each cluster, we ordered these clusters. In Fig. <xref rid="Fig3" ref-type="fig">3</xref>d, we show the first five clusters (the complete data is shown in the Additional Data).</p>
    </sec>
    <sec id="Sec7">
      <title>Comparison of modules</title>
      <p>In ACEP model, the modules R1, R2 and R3 are used to process EI, RI and SI, respectively. We list all the combinations of these modules to compare the impact of each module on the overall performance of the system. In these combinations, when only one module is used, we disabled R4 (fusion module); when more than two modules are used, we integrate the output tensors of each module through R4. Table <xref rid="Tab3" ref-type="table">3</xref> shows the performance of the system in each case. In lines 1 to 3, the predicted performance in a single module is shown. Due to carrying EI, the R1 performs well in long sequences, with ACC exceeding 93%, but has poor performance for short sequences, with ACC about 89%. In lines 4 to 6, the performance integrated with two modules is shown. Because the amino acid composition contributes to the recognition of short sequences, the R1 + R3 is very effective for both long sequences and short sequences, with overall ACC exceeding 92%. The performance of the R2 + R3 is the worst for long sequences due to the lack of EI, with ACC about 88%.
<table-wrap id="Tab3"><label>Table 3</label><caption><p>The performance of different modules</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Module</th><th align="left" colspan="2">Sequence length &lt; 30</th><th align="left"/><th align="left" colspan="2">Sequence length ≥ 30</th><th align="left"/><th align="left" colspan="2">All sequences</th></tr><tr><th align="left"/><th align="left">ACC(%)</th><th align="left">MCC</th><th align="left"/><th align="left">ACC(%)</th><th align="left">MCC</th><th align="left"/><th align="left">ACC(%)</th><th align="left">MCC</th></tr></thead><tbody><tr><td align="left">R1</td><td align="left">89.74</td><td align="left">0.7946</td><td align="left"/><td align="left">93.11</td><td align="left">0.8623</td><td align="left"/><td align="left">91.29</td><td align="left">0.8258</td></tr><tr><td align="left">R2</td><td align="left">91.03</td><td align="left">0.8214</td><td align="left"/><td align="left">90.06</td><td align="left">0.8013</td><td align="left"/><td align="left">90.58</td><td align="left">0.8124</td></tr><tr><td align="left">R3</td><td align="left">89.09</td><td align="left">0.7816</td><td align="left"/><td align="left">90.36</td><td align="left">0.8077</td><td align="left"/><td align="left">89.67</td><td align="left">0.7938</td></tr><tr><td align="left">R1+R2</td><td align="left">89.61</td><td align="left">0.7926</td><td align="left"/><td align="left">92.50</td><td align="left">0.8500</td><td align="left"/><td align="left">90.94</td><td align="left">0.8188</td></tr><tr><td align="left">R1+R3</td><td align="left">91.03</td><td align="left">0.8206</td><td align="left"/><td align="left">94.18</td><td align="left">0.8846</td><td align="left"/><td align="left">92.48</td><td align="left">0.8500</td></tr><tr><td align="left">R2+R3</td><td align="left">91.42</td><td align="left">0.8284</td><td align="left"/><td align="left">88.37</td><td align="left">0.7716</td><td align="left"/><td align="left">90.02</td><td align="left">0.8018</td></tr><tr><td align="left">R1+R2+R3</td><td align="left">91.16</td><td align="left">0.8236</td><td align="left"/><td align="left">94.34</td><td align="left">0.8867</td><td align="left"/><td align="left">92.62</td><td align="left">0.8527</td></tr></tbody></table></table-wrap></p>
    </sec>
  </sec>
  <sec id="Sec8" sec-type="conclusion">
    <title>Conclusions</title>
    <p>In this study, we developed a new protein classification algorithm for AMP recognition. In the encoding part, we use embedding tensor to capture hidden patterns between amino acids and integrate EI into sequence encoding. In the modeling part, the convolutional layer and LSTM layer are used to generate feature tensor; the attention mechanism is used to calculate the scores of each part in a sequence; the CVCA layer designed by us is used to fuse three types of feature tensors.</p>
    <p>The latest comprehensive AMP data from APD database are used in training and testing of our DNN model. The results show that the performance on ACEP is better than the state-of-the-art methods. In addition, we overcome the black-box nature of deep learning and visualize some tensors of ACEP model, thereby discovering some similar amino acids and some meaningful motifs and explaining the working mechanism of the model. We offer all open source code of ACEP, including data preprocessing, model training and visualization. By loading pretrained weights, high-throughput AMP recognition can be easily performed on ordinary computers.</p>
    <p>There are still several directions that can be further explored to advance this topic. ACEP model can accept and integrate a variety of heterogeneous information or features. At present, only EI and AAC are used in the research. In following research, some physicochemical features that are helpful to measure the antibacterial activity of sequences will be added to the model. The information of these biological processes will further expand the potential of ACEP model. It is also significant to integrate constantly updated AMP database. Relying on rich data, we can build a special predictor for AMPs with different activities and functions, and large-scale data are helpful to develop better algorithms.</p>
    <p>In addition, ACEP model can encode sequences into a very effective multi-dimensional representation. If we use the DNN with SVM or Random Forest, it is likely to further improve recognition performance [<xref ref-type="bibr" rid="CR38">38</xref>]. And it is also interesting to explore whether the model can identify the entire sequence of long AMP or the regions of sequences, and we will carry out this work in the future.</p>
    <p>In conclusion, we hope that our method can help to find more AMPs and accelerate the research and development of AMP drugs. We also hope that ACEP model can be applied to a wider range of protein sequences analysis tasks.</p>
  </sec>
  <sec id="Sec9">
    <title>Methods</title>
    <sec id="Sec10">
      <title>Datasets</title>
      <p>In this study, we hope that the training data can cover widespread AMPs and some newly discovered AMPs. In 2018, Daniel Veltri et al. [<xref ref-type="bibr" rid="CR13">13</xref>] constructed a benchmark dataset of experimentally validated AMPs (released on AMPScanner website [<xref ref-type="bibr" rid="CR39">39</xref>]). In the benchmark dataset, the positive samples consisted of 1778 AMPs that were active against Gram-positive and Gram-negative bacteria, which were screened from the largest comprehensive AMP repository APD [<xref ref-type="bibr" rid="CR40">40</xref>]; the negative samples consisted of 1778 peptide sequences in cytoplasm, which were screened from UniProt [<xref ref-type="bibr" rid="CR41">41</xref>] and filtered out antibiotic, antiviral, antifungal, effector or excreted characteristics. In addition, about 97.5% of the sequences were between 11AA and 100AA in length, and about 2.5% of the sequences were between 101AA and 200AA in length (some yet important antimicrobial proteins). The average length of all data is 34AA with a standard deviation of 22AA. The detailed sequence length distribution was shown in Figure in the Additional file. The dataset of 3556 peptide sequences was divided into three parts: 1424 for training, 708 for tuning and 1424 for testing.</p>
    </sec>
    <sec id="Sec11">
      <title>Encoding</title>
      <sec id="Sec12">
        <title>Amino acid embedding</title>
        <p>The 20 canonical amino acids can be classified according to their properties, and some important factors are charge, hydrophilicity or hydrophobicity, size, aggregation propensity and functional groups [<xref ref-type="bibr" rid="CR42">42</xref>]. These properties can affect the function of amino acids, thus some amino acids with similar properties may also have similar functions. According to their side chains' <italic>p</italic><italic>K</italic><sub><italic>a</italic></sub> values and charges carried at physiological pH (7.4), 20 standard amino acids can be divided into five groups, as shown in the Table <xref rid="Tab4" ref-type="table">4</xref>.
<table-wrap id="Tab4"><label>Table 4</label><caption><p>Groups of amino acids according to their properties</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Groups</th><th align="left">Amino Acids</th></tr></thead><tbody><tr><td align="left">Electrically Charged Side Chains(Positive)</td><td align="left">R,H,K</td></tr><tr><td align="left">Electrically Charged Side Chains(Negative)</td><td align="left">D,E</td></tr><tr><td align="left">Polar Uncharged Side Chains</td><td align="left">S,T,N,Q</td></tr><tr><td align="left">Hydrophobic Side Chains</td><td align="left">A,I,L,M,F,W,Y,V</td></tr><tr><td align="left">Special Cases</td><td align="left">C,G,P</td></tr></tbody></table></table-wrap></p>
        <p>A single value to encode amino acids can’t reflect the similarity (distance) between amino acids [<xref ref-type="bibr" rid="CR43">43</xref>]. To enable the DNN to automatically capture the hidden pattern of amino acids, we propose to use trainable tensors to represent individual amino acids. For each amino acid <italic>u</italic><sub><italic>k</italic></sub>, we use a 64-dimensional embedding tensor to encode it, called the <italic>E</italic><sub><italic>k</italic></sub>, and 20 amino acids are mapped to 20 embedding tensors as follows:
<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ {{u_{k}}} \to {{\boldsymbol{E}}_{k}}\;\;\;k =1,2,...,20  $$ \end{document}</tex-math><mml:math id="M10"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">E</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace width="2.77626pt"/><mml:mspace width="2.77626pt"/><mml:mspace width="2.77626pt"/><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>...</mml:mi><mml:mo>,</mml:mo><mml:mn>20</mml:mn></mml:math><graphic xlink:href="12864_2020_6978_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>As Fig. <xref rid="Fig4" ref-type="fig">4</xref>a shows, we vertically stack the <italic>E</italic><sub>1</sub>,<italic>E</italic><sub>2</sub>,...,<italic>E</italic><sub>20</sub> into a trainable embedding tensor <italic>E</italic> and initialize it with a uniform distribution. During training, the <italic>E</italic> is updated constantly with the back-propagation algorithm. The advantage of using embedding tensor is that the similarity of amino acids can be measured by geometric distance between tensors.
<fig id="Fig4"><label>Fig. 4</label><caption><p><bold>a</bold> The embedding tensor of 20 types of amino acids. <bold>b</bold> The <italic>P</italic> is the weight matrix of a sequence, where a row corresponds to a position in the sequence, columns represent 20 types of amino acids, and <italic>n</italic> denotes sequence length. <bold>c</bold> The <italic>O</italic> is a one-hot matrix, and each row in <italic>O</italic> is a one-hot vector converted from the corresponding numerical index of amino acids. <bold>d</bold> The <italic>L</italic><sub><italic>p</italic></sub> is the sequence tensor with evolutionary information</p></caption><graphic xlink:href="12864_2020_6978_Fig4_HTML" id="MO4"/></fig></p>
      </sec>
      <sec id="Sec13">
        <title>Sequence tensor constructing</title>
        <p>Due to the heredity and mutation of sequences in the process of evolution, the amino acid at each position of sequences may mutate into other amino acids. We obtain EI from the position-specific scoring matrix (PSSM) that contained the probability of occurrence of each type of amino acid at each position along with insertion or deletion. Hence, PSSM is considered as a measure of residue conservation in a given location [<xref ref-type="bibr" rid="CR44">44</xref>].</p>
        <p>We treat PSSM as the weight matrix of the sequence and rename it as <italic>P</italic>. The row corresponds to the position in the sequence and the column corresponds to 20 types of amino acids. The value <italic>p</italic><sub><italic>i</italic>,<italic>k</italic></sub> in <italic>P</italic> represents the weight of <italic>k</italic> amino acid in <italic>i</italic> position in the sequence. Thus, the EI for each amino acid is encapsulated in a vector of 20 dimensions, and the size of the <italic>P</italic> matrix of a peptide with <italic>n</italic> residues is <italic>n</italic>×20 as Fig. <xref rid="Fig4" ref-type="fig">4</xref>b shows.</p>
        <p>For each sequence, the <italic>P</italic> matrix can be obtained during PSI-BLAST [<xref ref-type="bibr" rid="CR45">45</xref>] search against Uniref50 database of protein sequences at online server POSSUM [<xref ref-type="bibr" rid="CR46">46</xref>], and three iterations of searching at threshold e-value of 0.001 are set. Next we calculate the weighted sum of amino acids at each position in the sequence, called <inline-formula id="IEq1"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}${\bar X}_{i}$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12864_2020_6978_Article_IEq1.gif"/></alternatives></inline-formula>, the specific definition is as follows.</p>
        <sec id="d30e1835">
          <title>
            <bold>Definition 1</bold>
          </title>
          <p>The <italic>E</italic> is the embedding tensor of 20 types of amino acid, and <italic>E</italic><sub><italic>k</italic></sub> is one of the <italic>E</italic>. The <italic>p</italic><sub><italic>i</italic>,<italic>k</italic></sub> is the weight of amino acid <italic>k</italic> at position <italic>i</italic>. The tensor <inline-formula id="IEq2"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}${\bar X}_{i}$\end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12864_2020_6978_Article_IEq2.gif"/></alternatives></inline-formula> at each position of sequence <italic>L</italic> satisfies formula (6). A sequence with <italic>n</italic> residues can be encoded as <inline-formula id="IEq3"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}${{L}_{p}} = [{\bar X}_{1},{\bar X}_{2}, \ldots,{\bar X}_{n}]$\end{document}</tex-math><mml:math id="M16"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:math><inline-graphic xlink:href="12864_2020_6978_Article_IEq3.gif"/></alternatives></inline-formula> as follows:</p>
          <p>
            <disp-formula id="Equ6">
              <label>6</label>
              <alternatives>
                <tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ {{\bar X_{i}} = \sum\limits_{k = 1}^{20} {{p_{i,k}} \cdot {E_{k}}} \;\;\;k = 1,2, \ldots,20}  $$ \end{document}</tex-math>
                <mml:math id="M18">
                  <mml:msub>
                    <mml:mrow>
                      <mml:mover accent="true">
                        <mml:mrow>
                          <mml:mi>X</mml:mi>
                        </mml:mrow>
                        <mml:mo>¯</mml:mo>
                      </mml:mover>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>=</mml:mo>
                  <mml:munderover accent="false" accentunder="false">
                    <mml:mrow>
                      <mml:mo>∑</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>k</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mn>20</mml:mn>
                    </mml:mrow>
                  </mml:munderover>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>p</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:mo>,</mml:mo>
                      <mml:mi>k</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>·</mml:mo>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>E</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>k</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mspace width="2.77626pt"/>
                  <mml:mspace width="2.77626pt"/>
                  <mml:mspace width="2.77626pt"/>
                  <mml:mi>k</mml:mi>
                  <mml:mo>=</mml:mo>
                  <mml:mn>1</mml:mn>
                  <mml:mo>,</mml:mo>
                  <mml:mn>2</mml:mn>
                  <mml:mo>,</mml:mo>
                  <mml:mo>…</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mn>20</mml:mn>
                </mml:math>
                <graphic xlink:href="12864_2020_6978_Article_Equ6.gif" position="anchor"/>
              </alternatives>
            </disp-formula>
          </p>
          <p>where <italic>i</italic> represents the position index of the sequence, and subscript <italic>k</italic>=1,2,…,20 represents the numerical index of amino acids.</p>
          <p>We call <italic>L</italic><sub><italic>p</italic></sub> the sequence tensor with EI. To facilitate the calculation of the DNN model, we define <italic>L</italic><sub><italic>p</italic></sub> as a <italic>n</italic>×64 tensor of <inline-formula id="IEq4"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}${\bar X}_{1},{\bar X}_{2}, \ldots,{\bar X}_{n}$\end{document}</tex-math><mml:math id="M20"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12864_2020_6978_Article_IEq4.gif"/></alternatives></inline-formula> stacked vertically in order, as shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>d. In this case, the <italic>L</italic><sub><italic>p</italic></sub> can be quickly calculated using the formula (7), as shown below:
<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ {{\boldsymbol{L}}_{p}} = {{\boldsymbol{P}}_{n \times 20}} \cdot {{\boldsymbol{E}}_{20 \times 64}}  $$ \end{document}</tex-math><mml:math id="M22"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">E</mml:mi></mml:mrow><mml:mrow><mml:mn>20</mml:mn><mml:mo>×</mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:msub></mml:math><graphic xlink:href="12864_2020_6978_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula></p>
          <p>Compared with the use of some fixed numbers encoding sequences, the use of the weighted sum of amino acid embedding tensors encoding sequences can solve the problem of residue variation in sequences. At the same time, the embedding tensor can find the similarity between amino acids, thus the DNN can have better generalization ability and help to develop more abundant patterns of sequences.</p>
          <p>Next, we encode the raw sequences to the one-hot vectors as the second input of ACEP. The amino acids in each position of a sequence are represented by a 20-dimensional one-hot vector. For a sequence of length <italic>n</italic>, we construct an <italic>n</italic>×20 matrix <italic>O</italic>, as shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>c. Through replacing the <italic>P</italic> with the <italic>O</italic>, we can obtain <italic>L</italic><sub><italic>o</italic></sub>, we call <italic>L</italic><sub><italic>o</italic></sub> the sequence tensor with RI, as shown in formula (8). The <italic>O</italic> matrix only carry the raw information of the sequence.
<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ {{\boldsymbol{L}}_{o}} = {{\boldsymbol{O}}_{n \times 20}} \cdot {{\boldsymbol{E}}_{20 \times 64}}  $$ \end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">O</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">E</mml:mi></mml:mrow><mml:mrow><mml:mn>20</mml:mn><mml:mo>×</mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:msub></mml:math><graphic xlink:href="12864_2020_6978_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula></p>
          <p>In the above two methods of encoding sequences, the dimensions of tensors depend on the length of sequences. And the short sequences encoded as lower-dimensional tensors are easily ignored by DNN. Therefore, we add amino acid composition (AAC) as a supplementary information to improve the sensitivity of DNN to short sequences. AAC is a most frequently used feature descriptor that can capture global compositional information of peptides [<xref ref-type="bibr" rid="CR47">47</xref>]. We compute the occurrences of 20 types of amino acids in the sequence, and the feature vector for the AAC descriptor is as follows:
<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ {{\boldsymbol{AAC}} = ({V_{1}},{V_{2}}, \cdots,{V_{a} }, \cdots {V_{20}})}  $$ \end{document}</tex-math><mml:math id="M26"><mml:mi mathvariant="bold-italic">AAC</mml:mi><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mspace width="0.3em"/><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>20</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math><graphic xlink:href="12864_2020_6978_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula></p>
          <p>where <italic>V</italic><sub><italic>a</italic></sub> denotes the occurrence number of the amino acid type <italic>a</italic>.</p>
          <p>As shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>, each sequence is encoded as the <italic>L</italic><sub><italic>p</italic></sub>, the <italic>L</italic><sub><italic>o</italic></sub> and the <italic>A</italic><italic>A</italic><italic>C</italic>. Then, they are sent to ACEP model to generate the feature tensors, and these feature tenors carry the EI, RI and SI. Finally, ACEP model integrates these feature tensors to predict results.
<fig id="Fig5"><label>Fig. 5</label><caption><p>The flowchart of encoding sequences. The subsequent neural networks use the encoded tensors to generate the feature tensors with evolutionary information, raw sequence information and supplementary information</p></caption><graphic xlink:href="12864_2020_6978_Fig5_HTML" id="MO5"/></fig></p>
          <p>Although the PSSM profiles of short sequences contain almost no EI, we still convert short sequences into PSSMs in order that all length sequences can be represented by consistent descriptors, which makes the subsequent neural network easier to be trained. In addition, the neural network we designed can automatically select suitable descriptors, and these descriptors with insufficient information can be filtered by the attention mechanism in module R4.</p>
        </sec>
      </sec>
    </sec>
    <sec id="Sec14">
      <title>Architecture of proposed dNN</title>
      <p>We design a new model based on deep learning called ACEP (Attention mechanism, Convolutional neural networks and Embedding tensor for antimicrobial Peptide recognition) to enhance the recognition of AMPs. We build ACEP model with the Keras framework [<xref ref-type="bibr" rid="CR48">48</xref>] running on the TensorFlow [<xref ref-type="bibr" rid="CR49">49</xref>] deep learning library (Detailed architecture of the model and the setting of parameters are shown in Figure in the Additional file).</p>
      <p>ACEP model consists of four main functional modules. Module R1 and R2 are used to generate the feature tensor of <italic>L</italic><sub><italic>p</italic></sub> and <italic>L</italic><sub><italic>o</italic></sub>, and module R3 is used to adjust the dimension of <italic>A</italic><italic>A</italic><italic>C</italic>, and module R4 is used to fuse the feature tensors generated by the first three modules, as Fig. <xref rid="Fig6" ref-type="fig">6</xref> shows.
<fig id="Fig6"><label>Fig. 6</label><caption><p>ACEP architecture consists of four module R1-R4. The R1-R3 are used to process different sequence information, and the R4 is used to fuse the feature tensors generated by the first three regions. In module R1, the Conv layer and the LSTM layer extract sequence features, and the attention layer predicts scores for different parts in sequences. In module R3, two fully connected layers are used to map 20-dimensional <italic>A</italic><italic>A</italic><italic>C</italic> vectors to 64-dimensional feature tensors. In module R4, we use the CVCA layer and the attention mechanism to fuse <italic>F</italic><sub>1</sub>- <italic>F</italic><sub>3</sub> into <italic>F</italic><sub><italic>meg</italic></sub>, then <italic>F</italic><sub><italic>meg</italic></sub> is passed to a Sigmoid function to predict results</p></caption><graphic xlink:href="12864_2020_6978_Fig6_HTML" id="MO6"/></fig></p>
      <sec id="Sec15">
        <title>Feature extraction</title>
        <p>In module R1, we use 1D convolution to automatically extract local features of <italic>L</italic><sub><italic>p</italic></sub>, the Conv1D layer has 64 convolution kernels of size 16. And the max pooling layer downsamples sequences by sliding a non-overlapping window of length 5 and selecting the largest value. This layer prevents overfitting and speeds up calculations. Next, the LSTM layer with 64 units is applied to identify sequential patterns along the sequence direction. The LSTM is set to return a complete sequence, and the feature tensor at each time step is passed to next layer. The Dropout [<xref ref-type="bibr" rid="CR50">50</xref>] in the LSTM layer helps prevent overfitting by randomly ignoring 30% of inputs. Each LSTM unit contains the input gate, output gate, hidden gate, forget gate, candidate cell gate and cell activation gate. These gates enable the model to remember or ignore the old information passed along the time step and prevent the gradient vanishing. In particular, the R1 and R2 are two independent modules with the same structure but have different parameters and inputs.</p>
        <p>A growing number of biological studies point that different parts of an AMP sequence may be used for different purposes. Flexible termini may be important to disrupt membranes, and specific hydrophobic regions may serve as anchors to initiate interactions [<xref ref-type="bibr" rid="CR51">51</xref>]. The attention scores (which can also be considered as weights) for different parts of a peptide sequence derived by the attention mechanism allow one to spot those important parts that significantly contribute to the final predictions [<xref ref-type="bibr" rid="CR52">52</xref>]. Hence, the attention mechanism is a suitable technique to aid the discovery of the functional patterns of AMP sequences. In module R1, the attention layer and the merge layer work together to give different attention to different parts in the sequence. The attention layer predicts scores for each position in a sequence, and the merge layer merges the out of each position to form a new feature tensor using weighted sum. Finally, by training the model, the attention layer has ability to assign high scores to those parts that are more useful for recognition, and quickly ignores the padding characters.</p>
        <p>As Fig. <xref rid="Fig7" ref-type="fig">7</xref>a shows, we simplify the attention mechanism in natural language processing. The tensors returned in each time step in the LSTM layer are stacked vertically to form the tensor <italic>A</italic>. The attention layer calculates the score <bold><italic>β</italic></bold> for <italic>A</italic>, and these scores measure the importance of each position in a sequence, as follows:
<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ {\boldsymbol{\beta}} = {\boldsymbol{b}} + {{\boldsymbol{A}}}\boldsymbol{\omega}  $$ \end{document}</tex-math><mml:math id="M28"><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi mathvariant="bold-italic">ω</mml:mi></mml:math><graphic xlink:href="12864_2020_6978_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula><fig id="Fig7"><label>Fig. 7</label><caption><p><bold>a</bold> The attention mechanism on the LSTM layer. <bold>b</bold> The convolution and concatenation layer</p></caption><graphic xlink:href="12864_2020_6978_Fig7_HTML" id="MO7"/></fig></p>
        <p>where <bold><italic>ω</italic></bold> is the weight of the fully connected layer in the attention module, <italic>b</italic> is the bias, and each fully connected layer shares the same parameters. Then, feed <bold><italic>β</italic></bold> into the Softmax layer and normalize it to <bold><italic>β</italic></bold><sup><bold><italic>′</italic></bold></sup>, as follows:
<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \boldsymbol{\beta'} = {\mathop{Softmax}\nolimits}(\boldsymbol{\beta})  $$ \end{document}</tex-math><mml:math id="M30"><mml:mstyle mathvariant="bold-italic"><mml:msup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mstyle><mml:mo>=</mml:mo><mml:mo>Softmax</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>)</mml:mo></mml:math><graphic xlink:href="12864_2020_6978_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>The merge layer receives <italic>A</italic> and <bold><italic>β</italic></bold><sup><bold><italic>′</italic></bold></sup> from the LSTM layer and the attention layer, then calculates the feature tensor <italic>F</italic><sub>1</sub> by weighted sum, as follows:
<disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ {{\boldsymbol{F}}_{1}} = \boldsymbol{\beta'}{\boldsymbol{A}}  $$ \end{document}</tex-math><mml:math id="M32"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold-italic"><mml:msup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mstyle><mml:mi mathvariant="bold-italic">A</mml:mi></mml:math><graphic xlink:href="12864_2020_6978_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>In module R2, the feature tensor <italic>F</italic><sub>2</sub> is calculated in the same way as above. In addition, module 3 contains the fully connected layer with 64 units and the dropout layer, which is used to process <italic>A</italic><italic>A</italic><italic>C</italic>. The fully connected layer can map a 20-dimensional <italic>A</italic><italic>A</italic><italic>C</italic> vector to a 64-dimensional feature tensor <italic>F</italic><sub>3</sub> in order to fusing with other two feature tensors.</p>
      </sec>
      <sec id="Sec16">
        <title>Feature fusion</title>
        <p>In module R4, for the purpose of fusing the feature tensor <italic>F</italic><sub>1</sub>- <italic>F</italic><sub>3</sub>, we designed a ‘Convolution and Concatenation’ layer.
<disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \begin{array}{l} {\boldsymbol{F}} = {\mathop{Concat}\nolimits} ([{{\boldsymbol{F}}_{1}},{{\boldsymbol{F}}_{2}},{{\boldsymbol{F}}_{3}}])\\ {{\boldsymbol{C}}_{F}} = {\mathop{Conv}\nolimits} ({\boldsymbol{F}})\\ {{\boldsymbol{F}}_{i}}' = Concat([{{\boldsymbol{F}}_{i}},{{\boldsymbol{C}}_{F}}])\;\;\;\;\;i = 1,2,3\\ {\boldsymbol{\mu}} = {\boldsymbol{b}}+{\boldsymbol{F}}'\boldsymbol{\omega}\\ \boldsymbol{\mu}' = {\mathop{Softmax}\nolimits}(\boldsymbol{\mu})\\ {{\boldsymbol{F}}_{meg}} = \boldsymbol{\mu}' {\boldsymbol{F}} \end{array}  $$ \end{document}</tex-math><mml:math id="M34"><mml:mtable class="array" columnalign="left"><mml:mtr><mml:mtd><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mo>=</mml:mo><mml:mo>Concat</mml:mo><mml:mo>(</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>]</mml:mo><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>Conv</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mtext mathvariant="italic">Concat</mml:mtext><mml:mo>(</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo><mml:mo>)</mml:mo><mml:mspace width="2.77626pt"/><mml:mspace width="2.77626pt"/><mml:mspace width="2.77626pt"/><mml:mspace width="2.77626pt"/><mml:mspace width="2.77626pt"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">ω</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">μ</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>Softmax</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">meg</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">μ</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12864_2020_6978_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>As shown in the CVCA layer in Fig. <xref rid="Fig7" ref-type="fig">7</xref>b, <italic>F</italic><sub>1</sub>- <italic>F</italic><sub>3</sub> constitute a tensor <italic>F</italic>, and a 1D convolution is used to convolve <italic>F</italic> to generate the convolutional tensor <italic>C</italic><sub><italic>F</italic></sub>. Next, we respectively concatenate <italic>C</italic><sub><italic>F</italic></sub> with <italic>F</italic><sub>1</sub>- <italic>F</italic><sub>3</sub> to form <italic>F</italic>1′- <italic>F</italic>3′, and vertically stack them into a tensor <italic>F</italic><sup>′</sup>. Concatenating <italic>C</italic><sub><italic>F</italic></sub> with other three feature tensors makes attention scores more stable and effective. Then <italic>F</italic><sup>′</sup> is fed into the attention layer in order to generate the fusion score <bold><italic>μ</italic></bold>, and <bold><italic>μ</italic></bold> is normalized to <bold><italic>μ</italic></bold><sup>′</sup> by the Softmax function. Finally, we use attention scores to calculate the weighted sum of the three feature tensors (<italic>F</italic><sub>1</sub>- <italic>F</italic><sub>3</sub>). And <italic>F</italic><sub><italic>meg</italic></sub> stands for the fusion tensor containing the EI, RI and SI. In formula (13), <bold><italic>ω</italic></bold> is the weights of the fully connected layer, <italic>b</italic> is the bias, and each fully connected layer shares the same parameters.</p>
      </sec>
      <sec id="Sec17">
        <title>Prediction</title>
        <p><italic>F</italic><sub><italic>meg</italic></sub> is fed into a Sigmoid function to predict classification results. We train ACEP model with 30 epochs and set the maximum length of the input sequences to 200AA, which can accept the longest sequence (183AA) in our dataset. For the sequences less than 200 in length, we fill <italic>L</italic><sub><italic>p</italic></sub> and <italic>L</italic><sub><italic>o</italic></sub> with 0s, making their dimensions 200×20. During training, the parameters in <italic>E</italic> are updated together with other parameters in ACEP model. The threshold value of the prediction probability &gt; 0.5 is identified as AMP, and the probability ≤ 0.5 is identified as non-AMP.</p>
      </sec>
    </sec>
    <sec id="Sec18">
      <title>Amino acid clustering</title>
      <p>We extract the <italic>E</italic> from ACEP model, and cluster the embedding tensor of 20 types of amino acids using the k-means algorithm [<xref ref-type="bibr" rid="CR31">31</xref>] in scikit-learn. To find the natural number of clusters, we calculate the average silhouette and the sum of squared distances under different <italic>k</italic> values, as shown in Fig. <xref rid="Fig8" ref-type="fig">8</xref>. The silhouette of a instance is a measure of how closely it is matched to data within its cluster and how loosely it is matched to data of the neighboring cluster, i.e., the cluster whose average distance from the datum is lowest [<xref ref-type="bibr" rid="CR53">53</xref>]. The sum of squared distances measures the distance between the sample and the cluster center.
<fig id="Fig8"><label>Fig. 8</label><caption><p><bold>a</bold> The sum of squared distances under different <italic>k</italic> values. <bold>b</bold> The average silhouette under different <italic>k</italic> values</p></caption><graphic xlink:href="12864_2020_6978_Fig8_HTML" id="MO8"/></fig></p>
      <p>We draw the within-cluster sum-of-squares curve and the silhouette curve to determine the real <italic>k</italic> value. We expect the silhouette value of <italic>k</italic> to be as large as possible on the premise that the sum of squared distances is as small as possible. In Fig. <xref rid="Fig8" ref-type="fig">8</xref>, it can be noticed that the value of the silhouette is the largest when <italic>k</italic>=2, but the sum of squared distances is also very large, approximately 6.7, so the datum instances are far from the cluster center. As the trade-off between the silhouette and the sum of squared distances, we choose <italic>k</italic>=5 as the cluster numbers.</p>
    </sec>
    <sec id="Sec19">
      <title>Model tuning and cross-validation</title>
      <p>By using the scikit-learn API provided by Keras, we package ACEP as a scikit-learn model to optimize hyperparameters. The RandomizedSearchCV of scikit-learn [<xref ref-type="bibr" rid="CR54">54</xref>] is used to search optimal hyperparameters. The tuning step uses only the training dataset and the tuning dataset. After the hyperparameters are selected, the training model is established by combining the training dataset and the tuning dataset, and the performance is evaluated on the testing dataset.</p>
      <p>We use CV to estimate how accurately our predictive model will perform in practice. Specifically, we split all the data (training, tuning and testing dataset) into <italic>k</italic> folds (<italic>k</italic>=10), a single fold is retained as the validation data for testing the model, and the remaining <italic>k</italic>−1 folds are used as training data. The CV process is then repeated <italic>k</italic> times, with each of the <italic>k</italic> folds used exactly once as the validation data. The <italic>k</italic> results can be averaged to produce a single estimation. During CV, the hyperparameters that were selected via model tuning are not changed. In summary, CV averages the measures of fitness in prediction to derive a more accurate estimate of model prediction performance.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec20">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="12864_2020_6978_MOESM1_ESM.pdf">
            <caption>
              <p><bold>Additional file 1</bold><bold>Figure S1.</bold> Sequence length distributions of AMPs and non-AMPs. <bold>Figure S2.</bold> The shapes and connections of each layer in ACEP model. <bold>Figure S3.</bold> The attention scores of different parts of the sequences. <bold>Table S1.</bold> False negative AMP sequences.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>AAC</term>
        <def>
          <p>Amino acid composition</p>
        </def>
      </def-item>
      <def-item>
        <term>ACC</term>
        <def>
          <p>Accuracy</p>
        </def>
      </def-item>
      <def-item>
        <term>AMP</term>
        <def>
          <p>Antimicrobial peptide</p>
        </def>
      </def-item>
      <def-item>
        <term>AUC</term>
        <def>
          <p>Area under the ROC curve</p>
        </def>
      </def-item>
      <def-item>
        <term>Conv</term>
        <def>
          <p>Convolution</p>
        </def>
      </def-item>
      <def-item>
        <term>CV</term>
        <def>
          <p>Cross validation</p>
        </def>
      </def-item>
      <def-item>
        <term>CVCA</term>
        <def>
          <p>Convolution and Concatenation</p>
        </def>
      </def-item>
      <def-item>
        <term>DNN</term>
        <def>
          <p>Deep neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>EI</term>
        <def>
          <p>Evolutionary information</p>
        </def>
      </def-item>
      <def-item>
        <term>LSTM</term>
        <def>
          <p>Long short-term memory</p>
        </def>
      </def-item>
      <def-item>
        <term>MCC</term>
        <def>
          <p>Matthews Correlation Coefficient</p>
        </def>
      </def-item>
      <def-item>
        <term>PSSM</term>
        <def>
          <p>Position-specific scoring matrix</p>
        </def>
      </def-item>
      <def-item>
        <term>ROC</term>
        <def>
          <p>Receiver operating characteristic</p>
        </def>
      </def-item>
      <def-item>
        <term>RI</term>
        <def>
          <p>Raw sequence information</p>
        </def>
      </def-item>
      <def-item>
        <term>SENS</term>
        <def>
          <p>Sensitivity</p>
        </def>
      </def-item>
      <def-item>
        <term>SPEC</term>
        <def>
          <p>Specificity</p>
        </def>
      </def-item>
      <def-item>
        <term>SI</term>
        <def>
          <p>Supplementary information</p>
        </def>
      </def-item>
      <def-item>
        <term>t-SNE</term>
        <def>
          <p>t-distributed stochastic neighbor embedding</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p><bold>Supplementary information</bold> accompanies this paper at 10.1186/s12864-020-06978-0.</p>
  </sec>
  <ack>
    <p>We thank all the members of the Prof. Shunfang Wang laboratory for their helpful suggestions to improve this research. The comments of the anonymous referees, as well as the Associated Editors, are gratefully acknowledged.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>HF and SW designed the research and wrote the manuscript; HF performed the experiments and analyses; ZC and ML assisted in data preparation and code development. SW and ML performed English editing. All authors read and approved the manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work was supported by the National Natural Science Foundation of China [ 11661081] ; the Natural Science Foundation of Yunnan Province [ 2017FA032] ; and the Training Plan for Young and Middle-aged Academic Leaders of Yunnan Province [ 2018HB031]. The funding bodies played no role in the design of the study and collection, analysis, and interpretation of data and in writing the manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>Open source software and datasets are made freely available at <ext-link ext-link-type="uri" xlink:href="https://github.com/Fuhaoyi/ACEP">https://github.com/Fuhaoyi/ACEP</ext-link>. Using these codes, you can perform high-throughput AMP predictions, reproduce paper experiments and visualize experimental results.</p>
  </notes>
  <notes id="FPar1">
    <title>Ethics approval and consent to participate</title>
    <p>Not applicable.</p>
  </notes>
  <notes id="FPar2">
    <title>Consent for publication</title>
    <p>Not applicable.</p>
  </notes>
  <notes id="FPar3" notes-type="COI-statement">
    <title>Competing interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <collab>US Department of Health and Human Services</collab>
        </person-group>
        <source>Antibiotic Resistance Threats in the United States</source>
        <year>2013</year>
        <publisher-loc>Atlanta</publisher-loc>
        <publisher-name>US Department of Health and Human Services</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fjell</surname>
            <given-names>CD</given-names>
          </name>
          <name>
            <surname>Hiss</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Hancock</surname>
            <given-names>RE</given-names>
          </name>
          <name>
            <surname>Schneider</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Designing antimicrobial peptides: form follows function</article-title>
        <source>Nat Rev Drug Discovery</source>
        <year>2012</year>
        <volume>11</volume>
        <fpage>37</fpage>
        <lpage>51</lpage>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Reddy</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Yedery</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Aranha</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Antimicrobial peptides: premises and promises</article-title>
        <source>Int J Antimicrob Agents</source>
        <year>2004</year>
        <volume>24</volume>
        <issue>6</issue>
        <fpage>536</fpage>
        <lpage>547</lpage>
        <pub-id pub-id-type="pmid">15555874</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Van t Hof</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Veerman</surname>
            <given-names>EC</given-names>
          </name>
          <name>
            <surname>Helmerhorst</surname>
            <given-names>EJ</given-names>
          </name>
          <name>
            <surname>Amerongen</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Antimicrobial peptides: properties and applicability</article-title>
        <source>Biol Chem</source>
        <year>2001</year>
        <volume>382</volume>
        <issue>4</issue>
        <fpage>597</fpage>
        <lpage>619</lpage>
        <pub-id pub-id-type="pmid">11405223</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Torrent</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Andreu</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Nogués</surname>
            <given-names>VM</given-names>
          </name>
          <name>
            <surname>Boix</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Connecting peptide physicochemical and antimicrobial properties by a rational prediction model</article-title>
        <source>PloS ONE</source>
        <year>2011</year>
        <volume>6</volume>
        <issue>2</issue>
        <fpage>e16968</fpage>
        <?supplied-pmid 21347392?>
        <pub-id pub-id-type="pmid">21347392</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Thomas</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Karnik</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Barai</surname>
            <given-names>RS</given-names>
          </name>
          <name>
            <surname>Jayaraman</surname>
            <given-names>VK</given-names>
          </name>
          <name>
            <surname>Idicula-Thomas</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>CAMP: a useful resource for research on antimicrobial peptides</article-title>
        <source>Nucleic Acids Res</source>
        <year>2009</year>
        <volume>38</volume>
        <fpage>D774</fpage>
        <lpage>D780</lpage>
        <?supplied-pmid 19923233?>
        <pub-id pub-id-type="pmid">19923233</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Yue</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>G-DipC: An Improved Feature Representation Method for Short Sequences to Predict the Type of Cargo in Cell-Penetrating Peptides</article-title>
        <source>IEEE/ACM Trans Comput Biol Bioinf</source>
        <year>2020</year>
        <volume>17</volume>
        <issue>3</issue>
        <fpage>739</fpage>
        <lpage>747</lpage>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xiao</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Jia</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Chou</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>iAMP-2L: a two-level multi-label classifier for identifying antimicrobial peptides and their functional types</article-title>
        <source>Anal Biochem</source>
        <year>2013</year>
        <volume>436</volume>
        <issue>2</issue>
        <fpage>168</fpage>
        <lpage>177</lpage>
        <?supplied-pmid 23395824?>
        <pub-id pub-id-type="pmid">23395824</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fjell</surname>
            <given-names>CD</given-names>
          </name>
          <name>
            <surname>Jenssen</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Hilpert</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Cheung</surname>
            <given-names>WA</given-names>
          </name>
          <name>
            <surname>Pante</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Hancock</surname>
            <given-names>RE</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Identification of novel antibacterial peptides by chemoinformatics and machine learning</article-title>
        <source>J Med Chem</source>
        <year>2009</year>
        <volume>52</volume>
        <issue>7</issue>
        <fpage>2006</fpage>
        <lpage>2015</lpage>
        <?supplied-pmid 19296598?>
        <pub-id pub-id-type="pmid">19296598</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Veltri</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kamath</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Shehu</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Improving recognition of antimicrobial peptides and target selectivity through machine learning and genetic programming</article-title>
        <source>IEEE/ACM Trans Comput Biol Bioinf</source>
        <year>2015</year>
        <volume>14</volume>
        <issue>2</issue>
        <fpage>300</fpage>
        <lpage>313</lpage>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Randou</surname>
            <given-names>EG</given-names>
          </name>
          <name>
            <surname>Veltri</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Shehu</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Binary response models for recognition of antimicrobial peptides</article-title>
        <source>Proceedings of the International Conference on Bioinformatics, Computational Biology and Biomedical Informatics</source>
        <year>2013</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>ACM</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>EY</given-names>
          </name>
          <name>
            <surname>Fulan</surname>
            <given-names>BM</given-names>
          </name>
          <name>
            <surname>Wong</surname>
            <given-names>GC</given-names>
          </name>
          <name>
            <surname>Ferguson</surname>
            <given-names>AL</given-names>
          </name>
        </person-group>
        <article-title>Mapping membrane activity in undiscovered peptide sequence space using machine learning</article-title>
        <source>PNAS</source>
        <year>2016</year>
        <volume>113</volume>
        <issue>48</issue>
        <fpage>13588</fpage>
        <lpage>13593</lpage>
        <?supplied-pmid 27849600?>
        <pub-id pub-id-type="pmid">27849600</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Veltri</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kamath</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Shehu</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Deep learning improves antimicrobial peptide recognition</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>34</volume>
        <issue>16</issue>
        <fpage>2740</fpage>
        <lpage>2747</lpage>
        <?supplied-pmid 29590297?>
        <pub-id pub-id-type="pmid">29590297</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lata</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Mishra</surname>
            <given-names>NK</given-names>
          </name>
          <name>
            <surname>Raghava</surname>
            <given-names>GP</given-names>
          </name>
        </person-group>
        <article-title>AntiBP2: improved version of antibacterial peptide prediction</article-title>
        <source>BMC Bioinf</source>
        <year>2010</year>
        <volume>11</volume>
        <issue>1</issue>
        <fpage>S19</fpage>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Meher</surname>
            <given-names>PK</given-names>
          </name>
          <name>
            <surname>Sahu</surname>
            <given-names>TK</given-names>
          </name>
          <name>
            <surname>Saini</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Rao</surname>
            <given-names>AR</given-names>
          </name>
        </person-group>
        <article-title>Predicting antimicrobial peptides with improved accuracy by incorporating the compositional, physico-chemical and structural features into Chou’s general PseAAC</article-title>
        <source>Sci Rep</source>
        <year>2017</year>
        <volume>7</volume>
        <fpage>42362</fpage>
        <?supplied-pmid 28205576?>
        <pub-id pub-id-type="pmid">28205576</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chou</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Prediction of protein cellular attributes using pseudo-amino acid composition</article-title>
        <source>Proteins Struct Funct Bioinf</source>
        <year>2001</year>
        <volume>43</volume>
        <issue>3</issue>
        <fpage>246</fpage>
        <lpage>255</lpage>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kamath</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>De Jong</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Shehu</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Effective automated feature construction and selection for classification of biological sequences</article-title>
        <source>PloS ONE</source>
        <year>2014</year>
        <volume>9</volume>
        <issue>7</issue>
        <fpage>e99982</fpage>
        <?supplied-pmid 25033270?>
        <pub-id pub-id-type="pmid">25033270</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Veltri</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kamath</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Shehu</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Improving recognition of antimicrobial peptides and target selectivity through machine learning and genetic programming</article-title>
        <source>IEEE/ACM Trans Comput Biol Bioinf</source>
        <year>2017</year>
        <volume>14</volume>
        <issue>2</issue>
        <fpage>300</fpage>
        <lpage>313</lpage>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fernandes</surname>
            <given-names>FC</given-names>
          </name>
          <name>
            <surname>Rigden</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>Franco</surname>
            <given-names>OL</given-names>
          </name>
        </person-group>
        <article-title>Prediction of antimicrobial peptides based on the adaptive neuro-fuzzy inference system application</article-title>
        <source>Pept Sci</source>
        <year>2012</year>
        <volume>98</volume>
        <issue>4</issue>
        <fpage>280</fpage>
        <lpage>287</lpage>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Spencer</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Eickholt</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>A deep learning network approach to ab initio protein secondary structure prediction</article-title>
        <source>IEEE/ACM Trans Comput Biol Bioinf</source>
        <year>2015</year>
        <volume>12</volume>
        <issue>1</issue>
        <fpage>103</fpage>
        <lpage>112</lpage>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jo</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Hou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Eickholt</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Improving protein fold recognition by deep learning networks</article-title>
        <source>Sci Rep</source>
        <year>2015</year>
        <volume>5</volume>
        <fpage>17573</fpage>
        <?supplied-pmid 26634993?>
        <pub-id pub-id-type="pmid">26634993</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Fei</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Efficient utilization on PSSM combining with recurrent neural network for membrane protein types prediction</article-title>
        <source>Comput Biol Chem</source>
        <year>2019</year>
        <volume>81</volume>
        <fpage>9</fpage>
        <lpage>15</lpage>
        <?supplied-pmid 31472418?>
        <pub-id pub-id-type="pmid">31472418</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>LeCun</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Deep learning</article-title>
        <source>Nature</source>
        <year>2015</year>
        <volume>521</volume>
        <issue>7553</issue>
        <fpage>436</fpage>
        <lpage>444</lpage>
        <pub-id pub-id-type="pmid">26017442</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Drug repositioning based on bounded nuclear norm regularization</article-title>
        <source>Bioinformatics</source>
        <year>2019</year>
        <volume>35</volume>
        <issue>14</issue>
        <fpage>i455</fpage>
        <lpage>i463</lpage>
        <?supplied-pmid 31510658?>
        <pub-id pub-id-type="pmid">31510658</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bi</surname>
            <given-names>Xa</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <article-title>Morbigenous brain region and gene detection with a genetically evolved random neural network cluster approach in late mild cognitive impairment</article-title>
        <source>Bioinformatics</source>
        <year>2020</year>
        <volume>36</volume>
        <issue>8</issue>
        <fpage>2561</fpage>
        <lpage>2568</lpage>
        <?supplied-pmid 31971559?>
        <pub-id pub-id-type="pmid">31971559</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26</label>
      <mixed-citation publication-type="other">Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate. CoRR. 2015;:abs/1409.0473.</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27</label>
      <mixed-citation publication-type="other">Li J, Luong MT, Jurafsky D. A hierarchical neural autoencoder for paragraphs and documents. Association for Computational Linguistics; 2015. pp. 1106–1115.</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Mnih</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Heess</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Graves</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Kavukcuoglu</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Ghahramani</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Welling</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Cortes</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Lawrence</surname>
            <given-names>ND</given-names>
          </name>
          <name>
            <surname>Weinberger</surname>
            <given-names>KQ</given-names>
          </name>
        </person-group>
        <article-title>Recurrent Models of Visual Attention</article-title>
        <source>Advances in Neural Information Processing Systems, vol 27</source>
        <year>2014</year>
        <publisher-loc>Montréal</publisher-loc>
        <publisher-name>Curran Associates, Inc.</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>DeepHINT: understanding HIV-1 integration via deep learning with attention</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>35</volume>
        <issue>10</issue>
        <fpage>1660</fpage>
        <lpage>1667</lpage>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hanley</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>McNeil</surname>
            <given-names>BJ</given-names>
          </name>
        </person-group>
        <article-title>The meaning and use of the area under a receiver operating characteristic ROC curve</article-title>
        <source>Radiology</source>
        <year>1982</year>
        <volume>143</volume>
        <issue>1</issue>
        <fpage>29</fpage>
        <lpage>36</lpage>
        <pub-id pub-id-type="pmid">7063747</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lloyd</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Least squares quantization in PCM</article-title>
        <source>IEEE Trans Inf Theory</source>
        <year>1982</year>
        <volume>28</volume>
        <issue>2</issue>
        <fpage>129</fpage>
        <lpage>137</lpage>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Van der Maaten</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Visualizing data using t-SNE</article-title>
        <source>J Mach Learn Res</source>
        <year>2008</year>
        <volume>9</volume>
        <issue>Nov</issue>
        <fpage>2579</fpage>
        <lpage>2605</lpage>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Otvos</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>The short proline-rich antibacterial peptide family</article-title>
        <source>Cell Mol Life Sci</source>
        <year>2002</year>
        <volume>59</volume>
        <issue>7</issue>
        <fpage>1138</fpage>
        <lpage>1150</lpage>
        <?supplied-pmid 12222961?>
        <pub-id pub-id-type="pmid">12222961</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Anderson</surname>
            <given-names>DH</given-names>
          </name>
          <name>
            <surname>Sawaya</surname>
            <given-names>MR</given-names>
          </name>
          <name>
            <surname>Cascio</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Ernst</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Modlin</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Krensky</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Granulysin crystal structure and a structure-derived lytic mechanism</article-title>
        <source>J Mol Biol</source>
        <year>2003</year>
        <volume>325</volume>
        <issue>2</issue>
        <fpage>355</fpage>
        <lpage>365</lpage>
        <?supplied-pmid 12488100?>
        <pub-id pub-id-type="pmid">12488100</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Che</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zha</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>L</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Higher efficiency soluble prokaryotic expression, purification, and structural analysis of antimicrobial peptide G13</article-title>
        <source>Protein Expr Purif</source>
        <year>2016</year>
        <volume>119</volume>
        <fpage>45</fpage>
        <lpage>50</lpage>
        <?supplied-pmid 26581777?>
        <pub-id pub-id-type="pmid">26581777</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kouno</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Fujitani</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Mizuguchi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Osaki</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Nishimura</surname>
            <given-names>Si</given-names>
          </name>
          <name>
            <surname>Kawabata</surname>
            <given-names>Si</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A novel <italic>β</italic>-defensin structure: a potential strategy of big defensin for overcoming resistance by Gram-positive bacteria</article-title>
        <source>Biochemistry</source>
        <year>2008</year>
        <volume>47</volume>
        <issue>40</issue>
        <fpage>10611</fpage>
        <lpage>10619</lpage>
        <?supplied-pmid 18785751?>
        <pub-id pub-id-type="pmid">18785751</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Acedo</surname>
            <given-names>JZ</given-names>
          </name>
          <name>
            <surname>van Belkum</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Lohans</surname>
            <given-names>CT</given-names>
          </name>
          <name>
            <surname>Towle</surname>
            <given-names>KM</given-names>
          </name>
          <name>
            <surname>Miskolzie</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Vederas</surname>
            <given-names>JC</given-names>
          </name>
        </person-group>
        <article-title>Nuclear magnetic resonance solution structures of lacticin Q and aureocin A53 reveal a structural motif conserved among leaderless bacteriocins with broad-spectrum activity</article-title>
        <source>Biochemistry</source>
        <year>2016</year>
        <volume>55</volume>
        <issue>4</issue>
        <fpage>733</fpage>
        <lpage>742</lpage>
        <?supplied-pmid 26771761?>
        <pub-id pub-id-type="pmid">26771761</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Girshick</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Donahue</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Darrell</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Malik</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</article-title>
        <source>2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>
        <year>2014</year>
        <publisher-loc>Los Alamitos</publisher-loc>
        <publisher-name>IEEE Computer Society</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39</label>
      <mixed-citation publication-type="other">Antimicrobial Peptide Scanner vr.2 web server. <ext-link ext-link-type="uri" xlink:href="http://www.ampscanner.com">http://www.ampscanner.com</ext-link>. Accessed 15 Jan 2020.</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>APD3: the antimicrobial peptide database as a tool for research and education</article-title>
        <source>Nucleic Acids Res</source>
        <year>2015</year>
        <volume>44</volume>
        <issue>D1</issue>
        <fpage>D1087—D1093</fpage>
        <pub-id pub-id-type="pmid">26602694</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41</label>
      <mixed-citation publication-type="other">Magrane M, UniProt consortium. UniProt Knowledgebase: a hub of integrated protein data. Database. 2011;2011. Bar009.</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Creighton</surname>
            <given-names>TE</given-names>
          </name>
        </person-group>
        <source>Proteins: structures and molecular properties</source>
        <year>1993</year>
        <publisher-loc>Ann Arbor</publisher-loc>
        <publisher-name>Macmillan</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yu</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lai</surname>
            <given-names>KR</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Refining Word Embeddings Using Intensity Scores for Sentiment Analysis</article-title>
        <source>IEEE-ACM Trans Audio Speech Lang</source>
        <year>2018</year>
        <volume>26</volume>
        <issue>3</issue>
        <fpage>671</fpage>
        <lpage>681</lpage>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kumar</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Gromiha</surname>
            <given-names>MM</given-names>
          </name>
          <name>
            <surname>Raghava</surname>
            <given-names>GPS</given-names>
          </name>
        </person-group>
        <article-title>Prediction of RNA binding sites in a protein using SVM and PSSM profile</article-title>
        <source>Proteins Struct Funct Bioinf</source>
        <year>2008</year>
        <volume>71</volume>
        <issue>1</issue>
        <fpage>189</fpage>
        <lpage>194</lpage>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Altschul</surname>
            <given-names>SF</given-names>
          </name>
          <name>
            <surname>Madden</surname>
            <given-names>TL</given-names>
          </name>
          <name>
            <surname>Schäffer</surname>
            <given-names>AA</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Miller</surname>
            <given-names>W</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Gapped BLAST and PSI-BLAST: a new generation of protein database search programs</article-title>
        <source>Nucleic Acids Res</source>
        <year>1997</year>
        <volume>25</volume>
        <issue>17</issue>
        <fpage>3389</fpage>
        <lpage>3402</lpage>
        <?supplied-pmid 9254694?>
        <pub-id pub-id-type="pmid">9254694</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Revote</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Leier</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Marquez-Lago</surname>
            <given-names>TT</given-names>
          </name>
          <name>
            <surname>Webb</surname>
            <given-names>G</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>POSSUM: a bioinformatics toolkit for generating numerical sequence feature descriptors based on PSSM profiles</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>33</volume>
        <issue>17</issue>
        <fpage>2756</fpage>
        <lpage>2758</lpage>
        <?supplied-pmid 28903538?>
        <pub-id pub-id-type="pmid">28903538</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Qiang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Ye</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Du</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Su</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>CPPred-FL: a sequence-based predictor for large-scale identification of cell-penetrating peptides by feature representation learning</article-title>
        <source>Briefings Bioinf</source>
        <year>2018</year>
        <volume>09;21</volume>
        <issue>1</issue>
        <fpage>11</fpage>
        <lpage>23</lpage>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48</label>
      <mixed-citation publication-type="other">Keras: The Python Deep Learning library. <ext-link ext-link-type="uri" xlink:href="http://www.keras.io">http://www.keras.io</ext-link>. Accessed 15 Jan 2020.</mixed-citation>
    </ref>
    <ref id="CR49">
      <label>49</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Abadi</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Tensorflow: A system for large-scale machine learning</article-title>
        <source>12th USENIX Symposium on Operating Systems Design and Implementation (OSDI), vol 16</source>
        <year>2016</year>
        <publisher-loc>Savannah</publisher-loc>
        <publisher-name>USENIX Association</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Srivastava</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Krizhevsky</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Salakhutdinov</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title>
        <source>J Mach Learn Res</source>
        <year>2014</year>
        <volume>15</volume>
        <issue>1</issue>
        <fpage>1929</fpage>
        <lpage>1958</lpage>
      </element-citation>
    </ref>
    <ref id="CR51">
      <label>51</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tossi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sandri</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Giangaspero</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Amphipathic, <italic>α</italic>-helical antimicrobial peptides</article-title>
        <source>Pept Sci</source>
        <year>2000</year>
        <volume>55</volume>
        <issue>1</issue>
        <fpage>4</fpage>
        <lpage>30</lpage>
      </element-citation>
    </ref>
    <ref id="CR52">
      <label>52</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Wan</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Xiong</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>ACME: pan-specific peptide–MHC class I binding prediction through attention-based deep neural networks</article-title>
        <source>Bioinformatics</source>
        <year>2019</year>
        <volume>35</volume>
        <issue>23</issue>
        <fpage>4946</fpage>
        <lpage>4954</lpage>
        <?supplied-pmid 31120490?>
        <pub-id pub-id-type="pmid">31120490</pub-id>
      </element-citation>
    </ref>
    <ref id="CR53">
      <label>53</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rousseeuw</surname>
            <given-names>PJ</given-names>
          </name>
        </person-group>
        <article-title>Silhouettes: a graphical aid to the interpretation and validation of cluster analysis</article-title>
        <source>J Comput Appl Math</source>
        <year>1987</year>
        <volume>20</volume>
        <fpage>53</fpage>
        <lpage>65</lpage>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pedregosa</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Varoquaux</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Gramfort</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Michel</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Thirion</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Grisel</surname>
            <given-names>O</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Scikit-learn: Machine learning in Python</article-title>
        <source>J Mach Learn Res</source>
        <year>2011</year>
        <volume>12</volume>
        <issue>Oct</issue>
        <fpage>2825</fpage>
        <lpage>2830</lpage>
      </element-citation>
    </ref>
  </ref-list>
</back>
