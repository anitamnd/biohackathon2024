<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7750944</article-id>
    <article-id pub-id-type="pmid">32589734</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btaa594</article-id>
    <article-id pub-id-type="publisher-id">btaa594</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Applications Notes</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Bioimage Informatics</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>BeadNet: deep learning-based bead detection and counting in low-resolution microscopy images</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-8755-2825</contrib-id>
        <name>
          <surname>Scherr</surname>
          <given-names>Tim</given-names>
        </name>
        <aff><institution>Institute for Automation and Applied Informatics</institution>, Eggenstein-Leopoldshafen 76344, <country country="DE">Germany</country></aff>
        <xref rid="btaa594-cor1" ref-type="corresp"/>
        <!--tim.scherr@kit.edu-->
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Streule</surname>
          <given-names>Karolin</given-names>
        </name>
        <aff><institution>Institute of Biological and Chemical Systems – Functional Molecular Systems, Karlsruhe Institute of Technology</institution>, Eggenstein-Leopoldshafen 76344, <country country="DE">Germany</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Bartschat</surname>
          <given-names>Andreas</given-names>
        </name>
        <aff><institution>Institute for Automation and Applied Informatics</institution>, Eggenstein-Leopoldshafen 76344, <country country="DE">Germany</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Böhland</surname>
          <given-names>Moritz</given-names>
        </name>
        <aff><institution>Institute for Automation and Applied Informatics</institution>, Eggenstein-Leopoldshafen 76344, <country country="DE">Germany</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Stegmaier</surname>
          <given-names>Johannes</given-names>
        </name>
        <aff><institution>Institute of Imaging and Computer Vision, RWTH Aachen University</institution>, Aachen 52074, <country country="DE">Germany</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Reischl</surname>
          <given-names>Markus</given-names>
        </name>
        <aff><institution>Institute for Automation and Applied Informatics</institution>, Eggenstein-Leopoldshafen 76344, <country country="DE">Germany</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Orian-Rousseau</surname>
          <given-names>Véronique</given-names>
        </name>
        <aff><institution>Institute of Biological and Chemical Systems – Functional Molecular Systems, Karlsruhe Institute of Technology</institution>, Eggenstein-Leopoldshafen 76344, <country country="DE">Germany</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Mikut</surname>
          <given-names>Ralf</given-names>
        </name>
        <aff><institution>Institute for Automation and Applied Informatics</institution>, Eggenstein-Leopoldshafen 76344, <country country="DE">Germany</country></aff>
        <xref rid="btaa594-cor1" ref-type="corresp"/>
        <!--ralf.mikut@kit.edu-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Xu</surname>
          <given-names>Jinbo</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btaa594-cor1">To whom correspondence should be addressed. E-mail: <email>tim.scherr@kit.edu</email> or <email>ralf.mikut@kit.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <day>01</day>
      <month>9</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2020-06-26">
      <day>26</day>
      <month>6</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>26</day>
      <month>6</month>
      <year>2020</year>
    </pub-date>
    <volume>36</volume>
    <issue>17</issue>
    <fpage>4668</fpage>
    <lpage>4670</lpage>
    <history>
      <date date-type="received">
        <day>20</day>
        <month>1</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>20</day>
        <month>5</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>18</day>
        <month>6</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btaa594.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>An automated counting of beads is required for many high-throughput experiments such as studying mimicked bacterial invasion processes. However, state-of-the-art algorithms under- or overestimate the number of beads in low-resolution images. In addition, expert knowledge is needed to adjust parameters.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>In combination with our image labeling tool, BeadNet enables biologists to easily annotate and process their data reducing the expertise required in many existing image analysis pipelines. BeadNet outperforms state-of-the-art-algorithms in terms of missing, added and total amount of beads.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>BeadNet (software, code and dataset) is available at <ext-link xlink:href="https://bitbucket.org/t_scherr/beadnet" ext-link-type="uri">https://bitbucket.org/t_scherr/beadnet</ext-link>. The image labeling tool is available at https://bitbucket.org/abartschat/imagelabelingtool.</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Helmholtz Association</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001656</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>BioInterfaces in Technology and Medicine</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Deutsche Forschungsgemeinschaft</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001659</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>OR124/16-1</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="3"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Ligand-coupled beads are often used in <italic toggle="yes">in vitro</italic> experiments to mimic bacterial invasion processes (<xref rid="btaa594-B2" ref-type="bibr">Braun et al., 1998</xref>; <xref rid="btaa594-B3" ref-type="bibr">Hebert et al., 2017</xref>; <xref rid="btaa594-B4" ref-type="bibr">Jung et al., 2009</xref>). In the experiments that led to the development of BeadNet, red fluorescent latex beads of 1 µm in size were chemically coupled with a bacterial surface ligand, and their internalization into human cells was investigated. To distinguish internalized beads and beads that remained outside of cells, the cells were fixed without permeabilization, and external beads were recognized using a ligand-specific antibody coupled to a green fluorophore (<xref rid="btaa594-F1" ref-type="fig">Fig. 1a</xref>).
</p>
    <fig position="float" id="btaa594-F1">
      <label>Fig. 1.</label>
      <caption>
        <p>Exemplary application case and detection results of BeadNet. (<bold>a</bold>) A maximum intensity projection of red fluorescent beads (additional green fluorophore for beads outside of cells) and blue fluorescent cell nuclei. The 32 × 32 px test image in (<bold>b</bold>) is taken from the red fluorescent channel of (a). The upsampled test image is shown in (<bold>c</bold>). The white 2 × 2 px seeds in (<bold>d</bold>) are enlarged (gray) for the calculation of the evaluation metrics. The predictions of BeadNet are shown in (<bold>e</bold>) (white in gray ground truth)</p>
      </caption>
      <graphic xlink:href="btaa594f1" position="float"/>
    </fig>
    <p>The complexity of bead detection in low-resolution images may lead biologists to either use an inefficient and user-biased manual quantification or to use a user-friendly but inaccurate method. In addition, the parameters of sophisticated analysis pipelines need to be adjusted in case of changes in the experimental design. Even for experts, it is often easier to annotate new images instead of adjusting parameters for low-resolution and low signal-to-noise-ratio data. Thus, we designed the bead detection and counting software BeadNet to avoid parameter adjustments. For domain adaptation, biologists only need to create new training images using a built-in functionality and annotate them using our labeling tool. Despite its simple use, BeadNet enables a nearly error-free bead detection on a manually annotated low-resolution bead dataset. To our knowledge, this is the first annotated 2D dataset and software tackling the problem of counting beads in low-resolution images with high accuracy. The software combines a crucial upsampling preprocessing with a deep learning-based detection. In addition, BeadNet provides a graphical user interface for easy use, e.g. for generating new training data. The upsampling step and the training data generation are often missing in toolboxes, leaving users alone with the upsampling, the normalization and the creation of new training samples. BeadNet also provides the automatic calculation of evaluation measures and overlays of the detection results.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <p>BeadNet consists of an upsampling preprocessing step for low-resolution images and a deep learning-based bead detection with subsequent counting. The bilinear upsampling, hereby, enables distinguishing touching beads (<xref rid="btaa594-F1" ref-type="fig">Fig. 1b and c</xref>). Furthermore, it eases the annotation of a low-resolution dataset that is required for supervised learning (<xref rid="btaa594-F1" ref-type="fig">Fig. 1d</xref>). Due to the upsampling preprocessing, centers of touching beads do not share an edge anymore, and the bead detection can be treated as a semantic segmentation task. Thus, for the bead detection, an adapted U-Net trained with a combination of binary cross-entropy and Dice loss is used (<xref rid="btaa594-B5" ref-type="bibr">Milletari et al., 2016</xref>; <xref rid="btaa594-B8" ref-type="bibr">Ronneberger et al., 2015</xref>). Thereby, the traditional upsampling utilizes prior knowledge enforcing the network to work at reasonable scales. To improve the generalization ability of the trained model, specific training data augmentations are applied online during training. To get a more robust seed detection, a morphological dilation with a cross-like mask can be applied to the upsampled training label images. This enlarges the seeds and reduces the number of missing seed detections. However, depending on the resolution and the seeds’ distances the number of missing detections may also increase.</p>
    <p>In the post-processing, the raw predictions are binarized using a global threshold of 0.5, and the centroids of predicted beads are calculated. Finally, the detected beads are counted. Additional information concerning the processing steps and the U-Net model used can be found in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>.</p>
    <p>To use our BeadNet software, a user needs to select a predefined bead diameter range required for the upsampling preprocessing. If a retraining for domain adaptation is needed, i.e. the resulting seed detection overlays are erroneous, new training data crops can be generated and annotated. Then, multiple models with and without the dilation step are trained and evaluated. For inference, the on the test dataset best performing model is selected automatically.</p>
  </sec>
  <sec>
    <title>3 Low-resolution bead dataset</title>
    <p>We annotated a low-resolution bead dataset consisting of 60 training, 15 validation and 25 test images with 2587 beads in total. The bead diameters range from about 2 px to 4 px (<xref rid="btaa594-F1" ref-type="fig">Fig. 1b</xref>). Neighboring seeds can touch each other and the center of beads is not well-defined. This makes manual annotation of seeds in the original images very difficult. The application of a fourfold bilinear upsampling and the use of 2 × 2 px seeds enable to annotate beads quickly, to avoid touching seeds and to hit the center easily (<xref rid="btaa594-F1" ref-type="fig">Fig. 1c and d</xref>).</p>
  </sec>
  <sec>
    <title>4 Results</title>
    <p>For performance measurements, the ground truth seeds are enlarged (<xref rid="btaa594-F1" ref-type="fig">Fig. 1d</xref>). Then, the normalized amount of missing (no predicted seed in a ground truth bead), split (multiple predicted seeds in a ground truth bead) and added beads (predicted seed in the ground truth background) can be counted. All shown metrics do not count added seeds at the border area (only fully visible beads are annotated, see <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref> for the applied border correction).</p>
    <p><xref rid="btaa594-T1" ref-type="table">Table 1</xref> shows quantitative results of BeadNet, of the Laplacian of Gaussian-based seed detection of TWANG (<xref rid="btaa594-B1" ref-type="bibr">Bartschat et al., 2016</xref>; <xref rid="btaa594-B9" ref-type="bibr">Stegmaier et al., 2014</xref>), of a Hough-transform-based detection using MATLABs imfindcircles (<xref rid="btaa594-B11" ref-type="bibr">Yuen et al., 1990</xref>) and of a simple Otsu thresholding (<xref rid="btaa594-B7" ref-type="bibr">Otsu, 1979</xref>) with Euclidean distance transform on the (upsampled) test dataset. The median BeadNet prediction outperforms the other methods in nearly every metric. More detailed results including the single initializations, and a qualitative comparison with the FISH-quant software for the automatic counting of transcripts in FISH images (<xref rid="btaa594-B6" ref-type="bibr">Mueller et al., 2013</xref>) is provided in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>.
</p>
    <table-wrap position="float" id="btaa594-T1">
      <label>Table 1.</label>
      <caption>
        <p>Results on the 25 test images of the bead dataset (670 beads)</p>
      </caption>
      <table frame="hsides" rules="groups">
        <colgroup span="1">
          <col valign="top" align="left" span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="center" span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
        </colgroup>
        <thead>
          <tr>
            <th rowspan="1" colspan="1">Method</th>
            <th rowspan="1" colspan="1"><italic toggle="yes">F</italic>-Score <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="normal">F</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></th>
            <th rowspan="1" colspan="1">Precision <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></th>
            <th rowspan="1" colspan="1">Recall <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="normal">R</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></th>
            <th rowspan="1" colspan="1">Split <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>split</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (%)</th>
            <th rowspan="1" colspan="1">Missing <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>miss</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (%)</th>
            <th rowspan="1" colspan="1">Added <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>add</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (%)</th>
            <th rowspan="1" colspan="1">Detections (%)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td rowspan="1" colspan="1">BeadNet</td>
            <td rowspan="1" colspan="1">0.971</td>
            <td rowspan="1" colspan="1">0.977</td>
            <td rowspan="1" colspan="1">0.954</td>
            <td rowspan="1" colspan="1">0.45</td>
            <td rowspan="1" colspan="1">4.63</td>
            <td rowspan="1" colspan="1">1.79</td>
            <td rowspan="1" colspan="1">97.76</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">BeadNet (w. dilation)</td>
            <td rowspan="1" colspan="1">0.977</td>
            <td rowspan="1" colspan="1">0.979</td>
            <td rowspan="1" colspan="1">0.976</td>
            <td rowspan="1" colspan="1">0.15</td>
            <td rowspan="1" colspan="1">2.39</td>
            <td rowspan="1" colspan="1">2.09</td>
            <td rowspan="1" colspan="1">99.70</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">TWANG</td>
            <td rowspan="1" colspan="1">0.939</td>
            <td rowspan="1" colspan="1">0.960</td>
            <td rowspan="1" colspan="1">0.919</td>
            <td rowspan="1" colspan="1">0</td>
            <td rowspan="1" colspan="1">8.06</td>
            <td rowspan="1" colspan="1">3.88</td>
            <td rowspan="1" colspan="1">95.82</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Hough transform</td>
            <td rowspan="1" colspan="1">0.927</td>
            <td rowspan="1" colspan="1">0.977</td>
            <td rowspan="1" colspan="1">0.882</td>
            <td rowspan="1" colspan="1">0</td>
            <td rowspan="1" colspan="1">11.79</td>
            <td rowspan="1" colspan="1">2.09</td>
            <td rowspan="1" colspan="1">90.30</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Otsu</td>
            <td rowspan="1" colspan="1">0.651</td>
            <td rowspan="1" colspan="1">0.766</td>
            <td rowspan="1" colspan="1">0.567</td>
            <td rowspan="1" colspan="1">2.09</td>
            <td rowspan="1" colspan="1">43.28</td>
            <td rowspan="1" colspan="1">15.22</td>
            <td rowspan="1" colspan="1">74.03</td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn id="tblfn1">
          <p><italic toggle="yes">Note</italic>: For BeadNet, the median of five trained models is shown. The other methods are deterministic.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
  </sec>
  <sec>
    <title>5 Conclusion</title>
    <p>BeadNet outperforms traditional bead detection methods, which need expert knowledge to adjust them. The high detection rate shows that no additional preprocessing toolbox is needed, e.g. a 2D adaptation of the image restoration toolbox CSBDeep for the denoising and isotropic recovery of 3D microscopy data (<xref rid="btaa594-B10" ref-type="bibr">Weigert et al., 2018</xref>). Combined with the integrated training data generation enables the high detection rate a user-friendly end-to-end use of BeadNet for the detection of poorly resolved objects, e.g. of ligand-coupled beads or other spherical or non-spherical objects.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was supported by the Helmholtz Association in the program BioInterfaces in Technology and Medicine (BIFTM), and by a grant from the Deutsche Forschungsgemeinschaft [OR124/16-1 to V.O.-R. and K.S.].</p>
    <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btaa594_Supplementary_Information</label>
      <media xlink:href="btaa594_supplementary_information.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btaa594-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bartschat</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) 
<article-title>XPIWIT – an XML pipeline wrapper for the insight toolkit</article-title>. <source>Bioinformatics</source>, <volume>32</volume>, <fpage>315</fpage>–<lpage>317</lpage>.<pub-id pub-id-type="pmid">26415725</pub-id></mixed-citation>
    </ref>
    <ref id="btaa594-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Braun</surname><given-names>L.</given-names></string-name></person-group>  <etal>et al</etal> (<year>1998</year>) 
<article-title>The InlB protein of <italic toggle="yes">Listeria monocytogenes</italic> is sufficient to promote entry into mammalian cells</article-title>. <source>Mol. Microbiol</source>., <volume>27</volume>, <fpage>1077</fpage>–<lpage>1087</lpage>.<pub-id pub-id-type="pmid">9535096</pub-id></mixed-citation>
    </ref>
    <ref id="btaa594-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hebert</surname><given-names>K.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) 
<article-title><italic toggle="yes">Anaplasma marginale</italic> outer membrane protein A is an adhesin that recognizes sialylated and fucosylated glycans and functionally depends on an essential binding domain</article-title>. <source>Infect. Immun</source>., <volume>85</volume>, 1–16.</mixed-citation>
    </ref>
    <ref id="btaa594-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jung</surname><given-names>C.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2009</year>) 
<article-title>Involvement of CD44v6 in InlB-dependent <italic toggle="yes">Listeria</italic> invasion</article-title>. <source>Mol. Microbiol</source>., <volume>72</volume>, <fpage>1196</fpage>–<lpage>1207</lpage>.<pub-id pub-id-type="pmid">19432801</pub-id></mixed-citation>
    </ref>
    <ref id="btaa594-B5">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Milletari</surname><given-names>F.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) V-Net: fully convolutional neural networks for volumetric medical image segmentation. In: <italic toggle="yes">2016 Fourth International Conference on 3D Vision</italic>, IEEE, Stanford, CA, USA. pp. <fpage>565</fpage>–<lpage>571</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa594-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mueller</surname><given-names>F.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2013</year>) 
<article-title>FISH-quant: automatic counting of transcripts in 3D FISH images</article-title>. <source>Nat. Methods</source>, <volume>10</volume>, <fpage>277</fpage>–<lpage>278</lpage>.<pub-id pub-id-type="pmid">23538861</pub-id></mixed-citation>
    </ref>
    <ref id="btaa594-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Otsu</surname><given-names>N.</given-names></string-name></person-group> (<year>1979</year>) 
<article-title>A threshold selection method from gray-level histograms</article-title>. <source>IEEE Trans. Syst. Man Cybern</source>., <volume>9</volume>, <fpage>62</fpage>–<lpage>66</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa594-B8">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Ronneberger</surname><given-names>O.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2015</year>) <part-title>U-Net: convolutional networks for biomedical image segmentation</part-title>. In: Navab,N. <italic toggle="yes">et al</italic>. (eds) <source>Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015</source>. 
<publisher-name>Springer International Publishing</publisher-name>, Cham, pp. <fpage>234</fpage>–<lpage>241</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa594-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stegmaier</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2014</year>) 
<article-title>Fast segmentation of stained nuclei in terabyte-scale, time resolved 3D microscopy image stacks</article-title>. <source>PLoS One</source>, <volume>9</volume>, <fpage>e90036</fpage>.<pub-id pub-id-type="pmid">24587204</pub-id></mixed-citation>
    </ref>
    <ref id="btaa594-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weigert</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>Content-aware image restoration: pushing the limits of fluorescence microscopy</article-title>. <source>Nat. Methods</source>, <volume>15</volume>, <fpage>1090</fpage>–<lpage>1097</lpage>.<pub-id pub-id-type="pmid">30478326</pub-id></mixed-citation>
    </ref>
    <ref id="btaa594-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yuen</surname><given-names>H.K.</given-names></string-name></person-group>  <etal>et al</etal> (<year>1990</year>) 
<article-title>Comparative study of Hough transform methods for circle finding</article-title>. <source>Image Vis. Comput</source>., <volume>8</volume>, <fpage>71</fpage>–<lpage>77</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
