<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">5408773</article-id>
    <article-id pub-id-type="pmid">27591081</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btw570</article-id>
    <article-id pub-id-type="publisher-id">btw570</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Applications Notes</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Data and Text Mining</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Precrec: fast and accurate precision–recall and ROC curve calculations in R</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Saito</surname>
          <given-names>Takaya</given-names>
        </name>
        <xref ref-type="aff" rid="btw570-aff1">1</xref>
        <xref ref-type="corresp" rid="btw570-cor1"/>
        <!--<email>takaya.saito@ii.uib.no</email>-->
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Rehmsmeier</surname>
          <given-names>Marc</given-names>
        </name>
        <xref ref-type="aff" rid="btw570-aff1">1</xref>
        <xref ref-type="aff" rid="btw570-aff2">2</xref>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Wren</surname>
          <given-names>Jonathan</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <aff id="btw570-aff1"><label>1</label>Computational Biology Unit, Department of Informatics, University of Bergen, Bergen, Norway</aff>
    <aff id="btw570-aff2"><label>2</label>Integrated Research Institute (IRI) for the Life Sciences and Department of Biology, Humboldt-Universität zu Berlin, Berlin, Germany</aff>
    <author-notes>
      <corresp id="btw570-cor1">To whom correspondence should be addressed. Email: <email>takaya.saito@ii.uib.no</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <day>01</day>
      <month>1</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2016-09-01">
      <day>01</day>
      <month>9</month>
      <year>2016</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>01</day>
      <month>9</month>
      <year>2016</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>33</volume>
    <issue>1</issue>
    <fpage>145</fpage>
    <lpage>147</lpage>
    <history>
      <date date-type="received">
        <day>11</day>
        <month>4</month>
        <year>2016</year>
      </date>
      <date date-type="rev-recd">
        <day>24</day>
        <month>8</month>
        <year>2016</year>
      </date>
      <date date-type="accepted">
        <day>26</day>
        <month>8</month>
        <year>2016</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author 2016. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2016</copyright-year>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/" license-type="cc-by">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btw570.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="sa1">
        <title>Summary</title>
        <p>The precision–recall plot is more informative than the ROC plot when evaluating classifiers on imbalanced datasets, but fast and accurate curve calculation tools for precision–recall plots are currently not available. We have developed Precrec, an R library that aims to overcome this limitation of the plot. Our tool provides fast and accurate precision–recall calculations together with multiple functionalities that work efficiently under different conditions.</p>
      </sec>
      <sec id="sa2">
        <title>Availability and Implementation</title>
        <p>Precrec is licensed under GPL-3 and freely available from CRAN (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=precrec">https://cran.r-project.org/package=precrec</ext-link>). It is implemented in R with C ++.</p>
      </sec>
      <sec id="sa3">
        <title>Supplementary information</title>
        <p><xref ref-type="supplementary-material" rid="sup1">Supplementary data</xref> are available at <italic>Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <counts>
      <page-count count="3"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec id="SEC1">
    <title>1 Introduction</title>
    <p>The recent rapid advances of molecular technologies have increased the importance of developing efficient and robust algorithms to handle large amounts of data in various fields of bioinformatics. Binary classifiers are mathematical and computational models that have successfully solved a wide range of life science problems with huge volumes of data produced from high-throughput experiments (<xref rid="btw570-B8" ref-type="bibr">Saito and Rehmsmeier, 2015</xref>). The Receiver Operating Characteristics (ROC) plot is the most popular performance measure for the evaluation of binary classification models. Its popularity comes from several well-studied characteristics, such as intuitive visual interpretation of the curve, easy comparisons of multiple models, and the Area Under the Curve (AUC) as a single-value quantity (<xref rid="btw570-B3" ref-type="bibr">Fawcett, 2006</xref>). Nonetheless, the intuitive visual interpretation can be misleading and potentially result in inaccurate conclusions caused by a wrong interpretation of specificity when the datasets are imbalanced. Imbalanced data naturally occur in life sciences. For instance, the majority of the datasets from genome-wide studies, such as microRNA gene discovery, are heavily imbalanced (<xref rid="btw570-B8" ref-type="bibr">Saito and Rehmsmeier, 2015</xref>). The precision–recall plot is an ROC alternative and can be used to avoid this potential pitfall of the ROC plot (<xref rid="btw570-B5" ref-type="bibr">He and Garcia, 2009</xref>; <xref rid="btw570-B8" ref-type="bibr">Saito and Rehmsmeier, 2015</xref>).</p>
    <p>Although some performance evaluation tools offer the calculation of precision–recall curves, they tend to underestimate several important aspects. One of these aspects is that any point on an ROC curve has a one-to-one relationship with a corresponding point on a precision–recall curve. To satisfy this relationship, precision–recall curves require non-linear interpolations to connect two adjacent points, unlike the simple linear interpolations of ROC curves (<xref rid="btw570-B2" ref-type="bibr">Davis and Goadrich, 2006</xref>). This non-linear interpolation is further developed in closely connected areas, such as calculations of AUC scores and confidence interval bands (<xref rid="btw570-B1" ref-type="bibr">Boyd <italic>et al.</italic>, 2013</xref>; <xref rid="btw570-B6" ref-type="bibr">Keilwagen <italic>et al.</italic>, 2014</xref>). Nonetheless, only a limited number of tools can produce non-linear interpolations of precision–recall curves (<xref rid="btw570-B2" ref-type="bibr">Davis and Goadrich, 2006</xref>; <xref rid="btw570-B4" ref-type="bibr">Grau <italic>et al.</italic>, 2015</xref>), and they usually come with high computational demands. Moreover, tools that are specific to precision–recall calculations tend to lack support for pre- and post-processing such as handling tied scores and calculating confidence interval bands, whereas some ROC-specific tools provide multiple functionalities (<xref rid="btw570-B7" ref-type="bibr">Robin <italic>et al.</italic>, 2011</xref>). We have developed Precrec, a tool that offers fast and accurate precision–recall calculations with several additional functionalities. Our comparison tests show that Precrec is the only tool that performs fast and accurate precision–recall calculations under various conditions.</p>
  </sec>
  <sec id="SEC2">
    <title>2 Implementation</title>
    <p>We separated Precrec into several modules according to their functions, and optimized each module with respect to processing time and accuracy. Specifically, we focused on the following six aspects to achieve high accuracy and multiple functionalities:
<list list-type="order"><list-item><p>Calculation of correct non-linear interpolations.</p></list-item><list-item><p>Estimation of the first point, which is necessary when the precision value becomes undefined due to no positive predictions.</p></list-item><list-item><p>Use of score-wise threshold values instead of fixed bins.</p></list-item><list-item><p>Integration of other evaluation measures, such as ROC and basic measures from the confusion matrix.</p></list-item><list-item><p>Handling of multiple models and multiple test sets.</p></list-item><list-item><p>Addition of pre- and post-process functions for simple data preparation and curve analysis.</p></list-item></list></p>
    <p>The aspects 1–3 are related to correct curve calculations. The remaining aspects pertain to the other evaluation measures and features that Precrec offers. Precrec concurrently calculates ROC and precision–recall curves together with their AUCs. It can also calculate several basic evaluation measures, such as error rate, accuracy, specificity, sensitivity and positive predictive value. Moreover, Precrec can directly accept multiple models and multiple test sets. For instance, it automatically calculates the average curve and the confidence interval bands when multiple test sets are specified. Precrec also has powerful features for data preparation. For instance, it offers several options for handling tied scores and missing values.</p>
    <p>To speed up calculations in the Precrec modules, we first tried to optimize only in R. We replaced some R code with C ++ code when it was difficult to solve low-performance issues in R.</p>
  </sec>
  <sec id="SEC3">
    <title>3 Results</title>
    <p>For the evaluation of Precrec, we have developed prcbench, an R library that serves as a compact testing workbench for the evaluation of precision–recall curves (available on CRAN). We have also compared our tool with four other tools that can calculate precision–recall curves: ROCR (<xref rid="btw570-B9" ref-type="bibr">Sing <italic>et al.</italic>, 2005</xref>), AUCCalculator (<xref rid="btw570-B2" ref-type="bibr">Davis and Goadrich, 2006</xref>), PerfMeas (available on CRAN) and PRROC (<xref rid="btw570-B4" ref-type="bibr">Grau <italic>et al.</italic>, 2015</xref>). The workbench provides two types of test results: the accuracy of the curves (<xref ref-type="fig" rid="btw570-F1">Fig. 1</xref>) and the benchmarking of processing time (<xref rid="btw570-T1" ref-type="table">Table 1</xref>).</p>
    <fig id="btw570-F1" orientation="portrait" position="float">
      <label>Fig. 1</label>
      <caption>
        <p>Results of evaluating precision–recall curves calculated by five different tools for three test sets – C1, C2 and C3. (<bold>A</bold>) The plot shows manually calculated points for C1 (red), C2 (green) and C3 (blue). Each test set contains three different test categories: SE (start and end positions), Ip (intermediate position and interpolation) and Rg (x and y ranges). In addition, each category has 3–5 individual test items. The remaining plots show the calculated curves with successes/total per category for (<bold>B</bold>) ROCR, (<bold>C</bold>) AUCCalculator, (<bold>D</bold>) PerfMeas, (<bold>E</bold>) PRROC and (<bold>F</bold>) Precrec</p>
      </caption>
      <graphic xlink:href="btw570f1p"/>
    </fig>
    <p>
      <table-wrap id="btw570-T1" orientation="portrait" position="float">
        <label>Table 1</label>
        <caption>
          <p>Benchmarking results of the five tools in millisecond</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Tool</th>
              <th rowspan="1" colspan="1">Curve</th>
              <th rowspan="1" colspan="1">AUC</th>
              <th rowspan="1" colspan="1">NL</th>
              <th rowspan="1" colspan="1">100</th>
              <th rowspan="1" colspan="1">1000</th>
              <th rowspan="1" colspan="1">1 million</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" rowspan="1" colspan="1">ROCR</td>
              <td align="left" rowspan="1" colspan="1">Yes</td>
              <td align="left" rowspan="1" colspan="1">No</td>
              <td align="left" rowspan="1" colspan="1">No</td>
              <td align="left" rowspan="1" colspan="1">5.4</td>
              <td align="left" rowspan="1" colspan="1">6.8</td>
              <td align="left" rowspan="1" colspan="1">(2.6 s)</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">AUCCalculator</td>
              <td align="left" rowspan="1" colspan="1">Yes</td>
              <td align="left" rowspan="1" colspan="1">Yes</td>
              <td align="left" rowspan="1" colspan="1">Yes</td>
              <td align="left" rowspan="1" colspan="1">105</td>
              <td align="left" rowspan="1" colspan="1">216</td>
              <td align="left" rowspan="1" colspan="1">(33 min)</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">PerfMeas</td>
              <td align="left" rowspan="1" colspan="1">Yes</td>
              <td align="left" rowspan="1" colspan="1">Yes</td>
              <td align="left" rowspan="1" colspan="1">No</td>
              <td align="left" rowspan="1" colspan="1">0.2</td>
              <td align="left" rowspan="1" colspan="1">0.4</td>
              <td align="left" rowspan="1" colspan="1">763</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">PRROC</td>
              <td align="left" rowspan="1" colspan="1">Yes</td>
              <td align="left" rowspan="1" colspan="1">Yes</td>
              <td align="left" rowspan="1" colspan="1">Yes</td>
              <td align="left" rowspan="1" colspan="1">348</td>
              <td align="left" rowspan="1" colspan="1">(74 sec)</td>
              <td align="left" rowspan="1" colspan="1">(123 days)<sup>a</sup></td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">PRROC (step=1)</td>
              <td align="left" rowspan="1" colspan="1">Yes</td>
              <td align="left" rowspan="1" colspan="1">Yes</td>
              <td align="left" rowspan="1" colspan="1">No</td>
              <td align="left" rowspan="1" colspan="1">7.9</td>
              <td align="left" rowspan="1" colspan="1">96</td>
              <td align="left" rowspan="1" colspan="1">(6.3 hrs)<sup>a</sup></td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">PRROC (AUC)</td>
              <td align="left" rowspan="1" colspan="1">No</td>
              <td align="left" rowspan="1" colspan="1">Yes</td>
              <td align="left" rowspan="1" colspan="1">Yes</td>
              <td align="left" rowspan="1" colspan="1">23.7</td>
              <td align="left" rowspan="1" colspan="1">236</td>
              <td align="left" rowspan="1" colspan="1">(4 min)</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Precrec</td>
              <td align="left" rowspan="1" colspan="1">Yes</td>
              <td align="left" rowspan="1" colspan="1">Yes</td>
              <td align="left" rowspan="1" colspan="1">Yes</td>
              <td align="left" rowspan="1" colspan="1">6.4</td>
              <td align="left" rowspan="1" colspan="1">6.8</td>
              <td align="left" rowspan="1" colspan="1">463</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="btw570-TF1">
            <p><bold>Tool</bold>: We performed PRROC (step = 1) with minStepSize = 1 and PRROC (AUC) without curve calculation. <bold>Curve:</bold> curve calculation. <bold>AUC:</bold> AUC calculation. <bold>NL:</bold> non-linear interpolation. <bold>100</bold>, <bold>1000</bold>, <bold>1 million:</bold> test dataset size. We tested each case 10 times and calculated the average (mean) processing time. The measurement unit is millisecond unless indicated otherwise.</p>
          </fn>
          <fn id="btw570-TF2">
            <label>a</label>
            <p>We tested only once for these cases.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </p>
    <sec id="SEC3.1">
      <title>3.1 Precrec calculates accurate precision–recall curves</title>
      <p><xref ref-type="fig" rid="btw570-F1">Figure 1A</xref> shows the base points of three tests sets – C1, C2 and C3. The tests are based on these pre-calculated points through which correctly calculated curves must pass. Each test set contains three categories. SE is for checking the correct curve elongation to the start and the end points. Ip is for correct curve calculations both on the intermediate points and interpolations. Rg is for <italic>x</italic> and <italic>y</italic> ranges; it is less important than the other two categories, but incorrect ranges may cause graph plotting issues. The results show that ROCR, AUCCalculator and PerfMeas (<xref ref-type="fig" rid="btw570-F1">Fig. 1B–D</xref>) have inconsistent starting points. Of these three, only AUCCalculator applies non-linear interpolations. Both PRROC and Precrec (<xref ref-type="fig" rid="btw570-F1">Fig. 1E, F</xref>) calculate correct curves on C2 and C3, but only Precrec calculates a correct curve for C1, whereas PRROC fails on this set by providing several precision values that are larger than 1 by around 1E-15 in our test environment (indicated by a dotted curve in <xref ref-type="fig" rid="btw570-F1">Figure 1E;</xref> see <xref ref-type="supplementary-material" rid="sup1">Supplementary methods</xref> and results for details).</p>
    </sec>
    <sec id="SEC3.2">
      <title>3.2 Precrec uses additional support points for non-linear interpolation and confidence intervals</title>
      <p>Precrec relies on additional support points for non-linear interpolation between two adjacent points and offers an option (x_bins) that associates with the number of support points for the whole curve, with the default value being 1000. For instance, the distances between two support points are consistent and respectively 0.5 and 0.001 when x_bins are 2 and 1000. Precrec performs linear interpolation when x_bins is 1. Moreover, this approach enables us to calculate the average curve with confidence interval bands when multiple test datasets are specified.</p>
    </sec>
    <sec id="SEC3.3">
      <title>3.3 Precrec provides fast calculations regardless of dataset sizes</title>
      <p><xref rid="btw570-T1" ref-type="table">Table 1</xref> shows the benchmarking result of processing time for the five tools. All tools perform reasonably well on small (100 items) and medium (1000 items) datasets, but only Precrec appears to be practically useful for calculating accurate non-linear interpolations (NL:Yes) on large (1 million items) datasets (see <xref ref-type="supplementary-material" rid="sup1">Supplementary methods</xref> and results for details).</p>
    </sec>
    <sec id="SEC3.4">
      <title>3.4 Precrec calculates AUCs with high accuracy</title>
      <p>Precrec uses the trapezoidal rule to calculate AUC scores. If a different number of support points is specified, the score changes accordingly. We also analyzed the accuracy of AUC scores by using randomly generated datasets. AUC scores appear to be very similar across the tools especially for large datasets. PerfMeas calculates AUC scores that are slightly different from the others, but the differences are small (see <xref ref-type="supplementary-material" rid="sup1">Supplementary methods</xref> and results for details). The results also show that there are only small differences between linear and non-linear AUCs. Nonetheless, correct non-linear interpolation can be useful when a dataset contains distantly separated adjacent points.</p>
    </sec>
    <sec id="SEC3.5">
      <title>3.5 Datasets with class imbalance and tied scores may require non-linear interpolation</title>
      <p>Non-linear interpolation is important when two adjacent points are distantly separated. Such a separation usually occurs when the dataset size is small. Nonetheless, it may even occur for large datasets, for instance, if a dataset is heavily imbalanced or contains a number of tied scores (see <xref ref-type="supplementary-material" rid="sup1">Supplementary methods</xref> and results for details). Hence, it is useful to provide non-linear calculations regardless of the dataset size.</p>
    </sec>
    <sec id="SEC3.6">
      <title>3.6 Precrec concurrently calculates ROC curve</title>
      <p>ROC and precision–recall curves have a number of aspects in common, and it is sometimes demanded to analyze both curves. Precrec calculates both curves and their AUCs by default.</p>
    </sec>
  </sec>
  <sec id="SEC4">
    <title>4 Summary</title>
    <p>The precision–recall plot is more informative than the ROC plot when evaluating classifiers on imbalanced datasets. Nevertheless, most performance evaluation tools focus mainly on the ROC plot. We have developed a performance evaluation library that works efficiently with various types of datasets and evaluation measures. In summary, Precrec is a powerful tool which provides fast and accurate precision–recall and ROC calculations with various functionalities.</p>
    <p><italic>Conflict of Interest</italic>: none declared.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>Supplementary Data</label>
      <media xlink:href="btw570_supp.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="btw570-B1">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Boyd</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) <chapter-title>Area under the precision–recall curve: point estimates and confidence intervals</chapter-title> In: <person-group person-group-type="editor"><name name-style="western"><surname>Blockeel</surname><given-names>H.</given-names></name></person-group>, <etal>et al</etal> (eds) <source>Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2013, Prague, Czech Republic, September 23–27, 2013, Proceedings, Part III</source>. <publisher-name>Springer</publisher-name>, <publisher-loc>Berlin, Heidelberg</publisher-loc>, p. <fpage>451</fpage>–<lpage>466</lpage>.</mixed-citation>
    </ref>
    <ref id="btw570-B2">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Davis</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Goadrich</surname><given-names>M.</given-names></name></person-group> (<year>2006</year>) The relationship between precision–recall and ROC curves. In: <italic>Proceedings of the 23rd international conference on Machine Learning</italic>, pp. <fpage>233</fpage>–<lpage>240</lpage>.</mixed-citation>
    </ref>
    <ref id="btw570-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fawcett</surname><given-names>T.</given-names></name></person-group> (<year>2006</year>) <article-title>An introduction to ROC analysis</article-title>. <source>Pattern Recognit. Lett</source>., <volume>27</volume>, <fpage>861</fpage>–<lpage>874</lpage>.</mixed-citation>
    </ref>
    <ref id="btw570-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Grau</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) <article-title>PRROC: computing and visualizing precision–recall and receiver operating characteristic curves in R</article-title>. <source>Bioinformatics</source>, <volume>31</volume>, <fpage>2595</fpage>–<lpage>2597</lpage>.<pub-id pub-id-type="pmid">25810428</pub-id></mixed-citation>
    </ref>
    <ref id="btw570-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Garcia</surname><given-names>E.</given-names></name></person-group> (<year>2009</year>) <article-title>Learning from imbalanced data</article-title>. <source>IEEE Trans. Knowl. Data Eng</source>., <volume>21</volume>, <fpage>1263</fpage>–<lpage>1284</lpage>.</mixed-citation>
    </ref>
    <ref id="btw570-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Keilwagen</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) <article-title>Area under precision–recall curves for weighted and unweighted data</article-title>. <source>PLoS One</source>, <volume>9</volume>, <fpage>e92209</fpage>.<pub-id pub-id-type="pmid">24651729</pub-id></mixed-citation>
    </ref>
    <ref id="btw570-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Robin</surname><given-names>X.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) <article-title>pROC: an open-source package for R and S+ to analyze and compare ROC curves</article-title>. <source>BMC Bioinformatics</source>, <volume>12</volume>, <fpage>77</fpage>.<pub-id pub-id-type="pmid">21414208</pub-id></mixed-citation>
    </ref>
    <ref id="btw570-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Saito</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Rehmsmeier</surname><given-names>M.</given-names></name></person-group> (<year>2015</year>) <article-title>The precision–recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets</article-title>. <source>PLoS One</source>, <volume>10</volume>, <fpage>e0118432</fpage>.<pub-id pub-id-type="pmid">25738806</pub-id></mixed-citation>
    </ref>
    <ref id="btw570-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sing</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2005</year>) <article-title>ROCR: visualizing classifier performance in R</article-title>. <source>Bioinformatics</source>, <volume>21</volume>, <fpage>3940</fpage>–<lpage>3941</lpage>.<pub-id pub-id-type="pmid">16096348</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
