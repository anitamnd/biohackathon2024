<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Neurosci</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Neurosci</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Neurosci.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Neuroscience</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1662-4548</issn>
    <issn pub-type="epub">1662-453X</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9125180</article-id>
    <article-id pub-id-type="doi">10.3389/fnins.2022.779106</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Neuroscience</subject>
        <subj-group>
          <subject>Brief Research Report</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>PyRAT: An Open-Source Python Library for Animal Behavior Analysis</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>De Almeida</surname>
          <given-names>Tulio Fernandes</given-names>
        </name>
        <uri xlink:href="http://loop.frontiersin.org/people/935820/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Spinelli</surname>
          <given-names>Bruno Guedes</given-names>
        </name>
        <uri xlink:href="http://loop.frontiersin.org/people/1622231/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hypolito Lima</surname>
          <given-names>Ramón</given-names>
        </name>
        <xref rid="fn002" ref-type="author-notes">
          <sup>†</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/480161/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Gonzalez</surname>
          <given-names>Maria Carolina</given-names>
        </name>
        <xref rid="fn002" ref-type="author-notes">
          <sup>†</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/156693/overview"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Rodrigues</surname>
          <given-names>Abner Cardoso</given-names>
        </name>
        <xref rid="c001" ref-type="corresp">
          <sup>*</sup>
        </xref>
        <xref rid="fn002" ref-type="author-notes">
          <sup>†</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/588622/overview"/>
      </contrib>
    </contrib-group>
    <aff><institution>Post Graduation Program in Neuroengineering, Santos Dumont Institute, Edmond and Lily Safra International Institute of Neuroscience</institution>, <addr-line>Macaíba</addr-line>, <country>Brazil</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: William T. Katz, Howard Hughes Medical Institute, United States</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Brent Winslow, Design Interactive, United States; Jesse Marshall, Harvard University, United States</p>
      </fn>
      <corresp id="c001">*Correspondence: Abner Cardoso Rodrigues <email>abner.neto@isd.org.br</email></corresp>
      <fn fn-type="other" id="fn001">
        <p>This article was submitted to Neural Technology, a section of the journal Frontiers in Neuroscience</p>
      </fn>
      <fn fn-type="equal" id="fn002">
        <p>†These authors have contributed equally to this work</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>09</day>
      <month>5</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>16</volume>
    <elocation-id>779106</elocation-id>
    <history>
      <date date-type="received">
        <day>17</day>
        <month>9</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>21</day>
        <month>3</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2022 De Almeida, Spinelli, Hypolito Lima, Gonzalez and Rodrigues.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>De Almeida, Spinelli, Hypolito Lima, Gonzalez and Rodrigues</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Here we developed an open-source Python-based library called Python rodent Analysis and Tracking (PyRAT). Our library analyzes tracking data to classify distinct behaviors, estimate traveled distance, speed and area occupancy. To classify and cluster behaviors, we used two unsupervised algorithms: hierarchical agglomerative clustering and t-distributed stochastic neighbor embedding (t-SNE). Finally, we built algorithms that associate the detected behaviors with synchronized neural data and facilitate the visualization of this association in the pixel space. PyRAT is fully available on GitHub: <ext-link xlink:href="https://github.com/pyratlib/pyrat" ext-link-type="uri">https://github.com/pyratlib/pyrat</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>deep learning</kwd>
      <kwd>unsupervised learning</kwd>
      <kwd>behavioral analysis</kwd>
      <kwd>animal tracking</kwd>
      <kwd>electrophysiology</kwd>
      <kwd>neuroscience method</kwd>
    </kwd-group>
    <counts>
      <fig-count count="4"/>
      <table-count count="0"/>
      <equation-count count="0"/>
      <ref-count count="30"/>
      <page-count count="9"/>
      <word-count count="5083"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>1. Introduction</title>
    <p>Deep learning (DL) and computer vision research fields are improving the performance of image, video and audio data processing (Krizhevsky et al., <xref rid="B14" ref-type="bibr">2012</xref>). The use of these approaches to estimate human and animal pose is increasing rapidly. This new direction stems from several factors, including improved feature extraction, high scalability to data, availability of low-cost hardware designed for DL, and pre-trained models ready for deployment (Toshev and Szegedy, <xref rid="B28" ref-type="bibr">2014</xref>; Redmon et al., <xref rid="B24" ref-type="bibr">2016</xref>; Ilg et al., <xref rid="B10" ref-type="bibr">2017</xref>; Levine et al., <xref rid="B15" ref-type="bibr">2018</xref>; Nath et al., <xref rid="B20" ref-type="bibr">2019</xref>).</p>
    <p>Evaluation of animal behavior by human assessment is commonly subjected to inter-rater variability and requires several hours of manual video data evaluation (Spink et al., <xref rid="B26" ref-type="bibr">2001</xref>). Commercial automation software for animal behavior assessment is expensive and rarely provides complex behavioral information. This software uses classical approaches of image processing to track animals' position using contrast or shape data, but they are less reliable to extract detailed information from images (Geuther et al., <xref rid="B5" ref-type="bibr">2019</xref>). In contrast, DL models identify patterns in image data allowing to track the complex movement of specific body parts. Also, DL models allow 3D reconstruction of subjects using single or multiple camera setups instead of complex body markers or light sources to track positions (Nath et al., <xref rid="B20" ref-type="bibr">2019</xref>; Nourizonoz et al., <xref rid="B22" ref-type="bibr">2020</xref>; Dunn et al., <xref rid="B2" ref-type="bibr">2021</xref>).</p>
    <p>In the last decade, the scientific community has been incorporating DL algorithms to analyze complex behavior (Gris et al., <xref rid="B7" ref-type="bibr">2017</xref>; Mathis et al., <xref rid="B18" ref-type="bibr">2018</xref>; Jin et al., <xref rid="B12" ref-type="bibr">2020</xref>). Usually, tracking body parts is the first step to classify and/or predict animal behavior. There are several open-source software based on DL to extract body coordinates from videos. However, they only provide the coordinate position for body parts and researchers must implement routines to infer these metrics.</p>
    <p>Here, we present a toolbox called Python in Rodent Analysis and Tracking (PyRAT), which is a Python library capable of performing the most common analysis of animal behavior from tracking data. Our user-friendly library can integrate neural data with kinematic metrics, such as velocity, acceleration, presence in areas, and object exploration. We also implemented an unsupervised algorithm to identify and cluster distinct animal behaviors. PyRAT is available in a public repository and can be found at: <ext-link xlink:href="https://github.com/pyratlib/pyrat" ext-link-type="uri">https://github.com/pyratlib/pyrat</ext-link>.</p>
    <p>We believe PyRAT is a useful tool because it can be easily employed to infer some of the most common video analysis metrics through a collection of Python scripts. We developed the library to address real use cases of video analysis, frequently performed in the behavioral field. The outputs of our functions are designed to produce graphics and tables, allowing the selection of subjects and/or time window in each experiment or trial to compare groups. Other open-source libraries presents similar features, however, the behavioral community can benefit from PyRAT simpler and direct approach. We documented the library features with Jupyter notebooks in our repository to guide users to apply our code to their data.</p>
  </sec>
  <sec sec-type="materials and methods" id="s2">
    <title>2. Materials and Methods</title>
    <sec>
      <title>2.1. Data</title>
      <p>To develop the PyRAT, we used datasets from the Edmond and Lily Safra International Institute of Neuroscience. Adult male Wistar rats (<italic>n</italic> = 12) were placed in an open field arena (59x59 cm with 45 cm tall walls) for 20 min per day for 3 consecutive days. Twenty-four hours later, animals were exposed to two identical objects presented in the open field arena for 5 min. We analyzed 48 videos recorded from a top-down view perspective with a Microsoft LifeCam camera at a resolution of 640 x 480 pixels at 30 frames per second (FPS). Alongside these experiments, neural data from the dorsal hippocampus were collected. All procedures were in accordance with the National Institutes of Health Guide for the Care and Use of Laboratory Animals and approved by a local Animal Care and Use Committee.</p>
      <p>Furthermore, we used datasets provided by Sturman et al. (<xref rid="B27" ref-type="bibr">2020</xref>) and Fujisawa et al. (<xref rid="B3" ref-type="bibr">2008</xref>, <xref rid="B4" ref-type="bibr">2015</xref>) to develop and test PyRAT functions in different scenarios. Sturman et al. (<xref rid="B27" ref-type="bibr">2020</xref>) used DeepLabCut to extract poses from mice in an elevated plus maze and an open field arena and provided the videos and the tracking data. Fujisawa et al. (<xref rid="B3" ref-type="bibr">2008</xref>) recorded single unit activity in rats performing a working memory task. The dataset is composed of extracellular recordings from the medial prefrontal cortex (64 channels) and dorsal CA1 (a subdivision of the hippocampus, 32 channels) in three rats.</p>
    </sec>
    <sec>
      <title>2.2. Video Analysis</title>
      <p>For body part tracking, we used DeepLabCut (DLC, version 2.2rc3) (Mathis et al., <xref rid="B18" ref-type="bibr">2018</xref>; Nath et al., <xref rid="B20" ref-type="bibr">2019</xref>). Specifically, we labeled 200 frames (<xref rid="F1" ref-type="fig">Figure 1A</xref>) taken from 5 videos for each scenario (then 95% was used for training). We used a ResNet-50 neural network (Insafutdinov et al., <xref rid="B11" ref-type="bibr">2016</xref>) with default parameters for 3,20,000 training iterations. We validated with 1 number of shuffles and found the test error was: 4.32 pixels, train: 2.69 pixels (image size was 640 by 480). We then used a p-cutoff of 0.9 to condition the X, Y coordinates for future analysis. This network was then used to analyze videos from similar experimental settings.</p>
      <fig position="float" id="F1">
        <label>Figure 1</label>
        <caption>
          <p><bold>(A)</bold> Representative image showing the marks of body parts used to train the network and the rat skeleton generated based on these marks. <bold>(B)</bold> Representative trajectory plots of a rat during the exploration sessions of an open field arena carried out on 3 consecutive days. Color variation indicates the moment in time at the rat's location. <bold>(C)</bold> Heatmaps of average trajectories during each exploration session. <bold>(D)</bold> Average distance traveled during each exploration session. <bold>(E)</bold> Average distance traveled during each exploration session is shown in blocks of 5 min per day. Data are expressed as mean ± SD.</p>
        </caption>
        <graphic xlink:href="fnins-16-779106-g0001" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>2.3. Library Design and Implementation</title>
      <p>Our library is designed to receive as input the DLC tracking data. However, the functions work on pixel space and then can receive any tracking data after applying a few adjustments such as removing the file header, if present and renaming the columns. We developed an example using tracking data from Plexon - available on GitHUb. PyRAT was implemented using Python 3 and the following libraries: NumPy, pandas, scikit-learn, and matplotlib, and hosted in Anaconda and Python Package Index (PyPi).</p>
    </sec>
    <sec>
      <title>2.4. Unsupervised Behavior Classification</title>
      <p>A common task in animal behavior analysis is the identification of distinct behaviors, such as rearing, grooming, nesting, immobility, and left and right turns. To automatically classify behaviors, we used a combination of two unsupervised approaches on each video frame. We used the hierarchical agglomerative clustering algorithm to label the clusters (Lukasová, <xref rid="B17" ref-type="bibr">1979</xref>) and a non-linear technique for dimensionality reduction called t-distributed stochastic neighbor embedding (t-SNE) to visualize the result (Van der Maaten and Hinton, <xref rid="B29" ref-type="bibr">2008</xref>). The input of both algorithms is the distances between labeled body parts. This approach was chosen because the relative distance between body parts is invariant to the animal position in the pixel space. Combining these techniques, we created a map where the distances between the body parts of each frame are transformed into 2D space using t-SNE and the color of each point is determined by the label from hierarchical agglomerative clustering (<bold>Figure 3A</bold>).</p>
      <p>To enhance cluster visualization, we optimize the t-SNE hyperparameters according to the heuristics reported in Kobak and Berens (<xref rid="B13" ref-type="bibr">2019</xref>). Their approach is based on three steps, (1) the use of Principal Component Analysis (PCA) in t-SNE initialization to preserve the data structure in lower dimensions; (2) set the <monospace>learning rate</monospace> as η = <italic>n</italic>/12, where <italic>n</italic> is the number of data points (frames); and (3) set the <monospace>perplexity</monospace> hyperparameter, which controls the similarity between points and governs their attraction, as <italic>n</italic>/100. In addition, we implemented three metrics to quantify the quality of the t-SNE output (Kobak and Berens, <xref rid="B13" ref-type="bibr">2019</xref>), (1) the KNN (<monospace>k-nearest neighbors</monospace>), which quantifies the preservation of the local structure; (2) the KNC (<monospace>k-nearest class</monospace>), which quantifies the preservation of the mesoscale structure; and (3) the CPD (<monospace>Spearman correlation</monospace>
<monospace>between pairwise distances</monospace>), which quantifies the preservation of the global structure.</p>
      <p>Since the hyperparameters are not optimized by the learning algorithm, they must be defined <italic>a priori</italic> and selected by trial and error or searching approaches. However, it must be noted that these heuristics have been proven to be useful in empirical tests (Kobak and Berens, <xref rid="B13" ref-type="bibr">2019</xref>).</p>
    </sec>
  </sec>
  <sec sec-type="results" id="s3">
    <title>3. Results</title>
    <sec>
      <title>3.1. Library Features</title>
      <p>Python in Rodent Analysis and Tracking is a Python toolbox for the analysis of animal tracking data that is easily accessible by new programmers, entirely developed in Python due to its popularity in the scientific community. The only prerequisite for using our toolbox is having minimal to moderate skills in Python and pandas library. We implemented the functions in a procedural approach instead of using the object-oriented features from Python as we believe that the procedural approach is more user friendly to non-programmers. Moreover, each function encapsulates an analysis, returning all inferred information and graphics. As we employed well-known Python libraries such as pandas, PyRAT can be used with other Python data science libraries such as scipy, sklearn, seaborn, matplotlib, and others.</p>
      <p>Python in Rodent Analysis and Tracking functions receive as input a pandas DataFrame with cartesian coordinates of labeled body parts to plot the graphics (data example available on GitHub). The input format is based on the DLC output, which consists of two columns in pixel space (x and y) for each tracked body part. However, any coordinate data organized in DataFrame format can be loaded in PyRAT if it follows the structure of x and y columns for each body part.</p>
      <p>To visualize the animal trajectory, we developed two functions. The function <monospace>Trajectory()</monospace> plots the body part coordinates across time using a matplotlib colormap (<xref rid="F1" ref-type="fig">Figure 1B</xref>). Here, we use a scatter plot of x and y points and add a third dimension to represent time to facilitate trajectory dynamics. The other function, <monospace>Heatmap()</monospace>), generates a heatmap of the animal occupancy in the arena (<xref rid="F1" ref-type="fig">Figure 1C</xref>). The occupancy plot is a 2D histogram that shows the body part occurrence in each spatial bin. We also evaluated the functions in a public dataset of mice performing the open field and the elevated plus maze tasks (Sturman et al., <xref rid="B27" ref-type="bibr">2020</xref>).</p>
      <p>To perform quantitative analyses, we developed the function <monospace>MotionMetrics()</monospace>, which estimates speed, acceleration, and traveled distance for each animal (<xref rid="F1" ref-type="fig">Figure 1D</xref> and <xref rid="SM1" ref-type="supplementary-material">Supplementary Material</xref>). To estimate these metrics, we transform the data from the pixel space to the centimeters space, using a known physical reference, applying the function <monospace>pixel2centimeters()</monospace>. Also, the user can define a time interval as an input parameter to calculate the metrics (<xref rid="F1" ref-type="fig">Figure 1E</xref>) and plot trajectory (<xref rid="F2" ref-type="fig">Figure 2A</xref>). To test the accuracy of PyRAT functions, we used a public dataset previously analyzed with EthoVision software (Sturman et al., <xref rid="B27" ref-type="bibr">2020</xref>), and we found equivalent results (data available on PyRAT's GitHub).</p>
      <fig position="float" id="F2">
        <label>Figure 2</label>
        <caption>
          <p><bold>(A)</bold> Image showing the trajectory of one rat for 120 s based on the snout coordinates. <bold>(B)</bold> Image showing rat body orientation during the entire object exploration session. <bold>(C)</bold> Average heatmap during the entire object exploration session. <bold>(D)</bold> Top: Object interaction across the entire object exploration session; Bottom left: Bar plot showing interaction time with objects A and A'; Bottom right: Bar plot showing the number of interactions with object A and A'. Data are expressed as mean ± SD.</p>
        </caption>
        <graphic xlink:href="fnins-16-779106-g0002" position="float"/>
      </fig>
      <p>Experimental designs that access pathological states or drug effects can use PyRAT to extract head orientation and locomotor activity to compare treatment or conditions (Gulley et al., <xref rid="B8" ref-type="bibr">2003</xref>; Aonuma et al., <xref rid="B1" ref-type="bibr">2020</xref>). The function <monospace>HeadOrientation()</monospace> returns head position and orientation in each frame using two points to calculate the element-wise arc tangent between them. The head orientation must be estimated using the neck and snout; however, the same function can estimate body orientation as shown in <xref rid="F2" ref-type="fig">Figure 2B</xref>, using the tail base and snout.</p>
      <p>To represent the pattern of object interaction among animal groups, the <monospace>Heatmap()</monospace> function can also be used to plot concatenated data, facilitating visual comparison between days, groups, or trials (<xref rid="F2" ref-type="fig">Figure 2C</xref>).</p>
      <p>In addition, we developed the <monospace>FieldDetermination()</monospace> and <monospace>Interaction()</monospace> functions to evaluate the interaction of the animal with defined areas in the pixel space. For this feature, the user must first use the function <monospace>FieldDetermination()</monospace> to create circular or a rectangular area. Once the bounding areas are determined, the user must call the function <monospace>Interaction()</monospace>, which estimates animal interaction with the areas and returns a DataFrame that reports the beginning and end of each interaction in chronological order. To visualize these outputs, we developed the function <monospace>PlotInteraction()</monospace> (<xref rid="F2" ref-type="fig">Figure 2D</xref>).</p>
      <p>To summarize data from several subjects and facilitate visualization of behavioral metrics, we included the function <monospace>Reports()</monospace>, which combines <monospace>MotionMetrics()</monospace> and <monospace>PlotInteraction()</monospace> and creates a unified report. The input of this function is a list of the tracking data from each animal and the output is a single DataFrame (examples in <xref rid="SM1" ref-type="supplementary-material">Supplementary Material</xref>).</p>
      <p>The function <monospace>ClassifyBehavior()</monospace> was developed to identify and classify different behaviors. We test this function in two different animal models in the open field task. In rats, 12 clusters were found automatically. The function returns a 2-dimensional color map, a histogram, and a dendrogram to better visualize the results (<xref rid="F3" ref-type="fig">Figure 3</xref>). In addition, the histogram helps to detect mislabeled behaviors considering the number of frames in a cluster. For example, Clusters 7 and 8 presented a small number of frames, and after visual inspection, we confirmed that they were miss-classified samples (<xref rid="F3" ref-type="fig">Figure 3B</xref>). Then, an experienced researcher must inspect the clusters to determine the type of behavior. The dendrogram shows the proximity between clusters and helps to identify the ramifications that represent a class of behavior (<xref rid="F3" ref-type="fig">Figure 3C</xref>). In mice, 5 behavioral clusters were identified (locomotion, left/right turns, sniffing, rearing, and exploration), suggesting that PyRAT is easily generalizable to different experimental setups (data available on PyRAT's GitHub).</p>
      <fig position="float" id="F3">
        <label>Figure 3</label>
        <caption>
          <p><bold>(A)</bold> Bidimensional projection representing each cluster found by the unsupervised algorithm of behavior classification. <bold>(B)</bold> Histogram showing the number of frames in each cluster. <bold>(C)</bold> Top left: Dendrogram presenting the proximity of the clusters. Clusters with similar behaviors were grouped after visual inspection. We identified five behavioral clusters: immobility, sniffing, locomotion, rearing, and nesting/sleeping. The other images are representative frames showing some of the behavioral clusters identified.</p>
        </caption>
        <graphic xlink:href="fnins-16-779106-g0003" position="float"/>
      </fig>
      <p>We developed a function to facilitate coupling the tracking data with the analysis of neural signals, in this way, we implemented the <monospace>SignalSubset()</monospace> function to extract time windows of defined events based on the interactions (function <monospace>Interaction()</monospace> output), the behavioral clusters or even from a list of timestamps (<xref rid="F4" ref-type="fig">Figure 4A</xref>).</p>
      <fig position="float" id="F4">
        <label>Figure 4</label>
        <caption>
          <p><bold>(A)</bold> Overview of SignalSubset function. Left column: The SignalSubset function receives as input neural data (e.g., raw LFP) and the clustermap produced by ClassifyBehavior. Right column: SignalSubset function returns a list of extracted neural data corresponding to time windows of a determined behavior (e.g., Cluster 11). We also show a representative spectrogram of extracted data. <bold>(B)</bold> Overview of SpatialNeuralActivity function. Left column: The SpatialNeuralActivity function receives as input the neural data (e.g., single unit spike rasterplot) to be shown in pixel space, and the tracking as spatial data. Right column: The SpatialNeuralActivity function returns the quantification of neural activity (spike firing) in each part of the pixel space.</p>
        </caption>
        <graphic xlink:href="fnins-16-779106-g0004" position="float"/>
      </fig>
      <p>The function <monospace>SpatialNeuralActivity</monospace> can be used to create a map associating a neural activity to the pixel space. The input of this function is a Dataframe with the x and y of each frame together with a third column with the neural activity to be visualized. The output is a 2D NumPy array with the mean activity in each discrete space of the map. We used neural data published in Fujisawa et al. (<xref rid="B3" ref-type="bibr">2008</xref>) to develop an example of spike triggered activity for some units in a T-maze (<xref rid="F4" ref-type="fig">Figure 4B</xref>). We are still developing this function to add more features, e.g., to plot the mean band of an LFP channel in the map instead of the spike data. The results and the code are available on PyRAT's GitHub.</p>
    </sec>
    <sec>
      <title>3.2. User Guide</title>
      <p>Python in Rodent Analysis and Tracking is a user-friendly Python toolbox to automate the analysis of animal tracking and neural data. Toolbox functions are documented, and here, we describe how to use the key features. PyRAT can be installed using <monospace>pip install pyratlib</monospace>. Then, it is necessary to import the following libraries:</p>
      <preformat position="float" xml:space="preserve">
      import pyratlib as rat
      import pandas as pd
</preformat>
      <p>Subsequently, the user must read tracking data as a DataFrame, e.g., using the <monospace>read_csv()</monospace> function from pandas. This Dataframe will be used as input on the majority of PyRAT functions. Here, we show how to plot the trajectories and the heatmap:</p>
      <preformat position="float" xml:space="preserve">
      data = pd.read_csv('your_data_path.csv')
  
      rat.Trajectory(data, bodyPart = 'tail', bodyPartBox = 'tail')
      rat.Heatmap(data, bodyPart = 'tail', bins = 10, vmax = 50)
</preformat>
      <p>To plot the trajectory, the user must define a body part in the function <monospace>Trajectory</monospace> using the <monospace>bodyPart</monospace> parameter which is the column name of the chosen body part. The function <monospace>Heatmap()</monospace> uses the <monospace>bodyPart</monospace> and the parameters <monospace>bins</monospace> and <monospace>vmax</monospace>, which determine the resolution and color scale of the plot.</p>
      <p>Another PyRAT feature is the quantification of the interaction between a body part and an area. This interaction can be calculated with the function <monospace>Interaction()</monospace> and defining a bounding area by passing the size and coordinates of the vertices. The function <monospace>FieldDetermination()</monospace> allows the visualization of areas in the pixel space, according to the tracking data. Also, we developed the function <monospace>PlotInteraction()</monospace> to plot the beginning, end, and duration of interactions with each bounding area across time:</p>
      <preformat position="float" xml:space="preserve">
     obj_dict = {'Obj_1': [1,0,0,0,430, 35,90,75],
                 'Obj_2': [1,0,0,0,430,380,90,75]}
  
     objects = rat.FieldDetermination(posit = obj_dict)
     interactions = rat.Interaction(data,'snout',objects)
     rat.PlotInteraction(interactions)
</preformat>
      <p>In the example above, two areas representing objects in distinct positions were passed as input, and the output is a DataFrame with the timestamps of each object interaction. The function <monospace>PlotInteraction()</monospace> plots object interactions across time (<xref rid="F2" ref-type="fig">Figure 2D</xref>).</p>
      <p>The function <monospace>ClassifyBehavior()</monospace> is a behavioral classifier and receives as parameters the tracking DataFrame, the video directory, the selected body parts, and the distance:</p>
      <preformat position="float" xml:space="preserve">
      rat.ClassifyBehavior(df,
                           video = 'path',
                           bp_list = ['snout', 'ear_R', 'ear_L', 'tail'],
                           distance = 28)
</preformat>
      <p>The distance metric passed in this function is Ward's distance and defines the threshold above which the clusters will not be merged.</p>
      <p>To facilitate the analysis of neural signals recorded during behavioral tasks, we developed the function <monospace>Interaction()</monospace> to extract timestamps of events of interest and the function <monospace>SignalSubset()</monospace> to extract epochs of the neural signal. An example of neural data input is available in <xref rid="SM1" ref-type="supplementary-material">Supplementary Material</xref>. We used files from Plexon and Blackrock Neurotech, but data from other acquisition systems can be used.</p>
      <preformat position="float" xml:space="preserve">
      subsets = rat.SignalSubset(signal, freq = 1000,
                                 fields = interactions)
</preformat>
      <p><monospace>SignalSubset()</monospace> returns the extracted data organized in a dictionary with the number of the epoch as the key. In addition, it can extract the time of a selected behavioral cluster. For this, it is necessary to use the cluster output from <monospace>ClassifyBehavior()</monospace> as input to the <monospace>IntervalBehaviors()</monospace> function, which will return a dictionary with the time windows when each behavior was manifested (documented in GitHub). This function facilitates data processing and allows saving the dictionary, speeding up data loading.</p>
      <p>The function <monospace>Reports()</monospace>, which summarizes data from several animals, receives as input the lists with DataFrames and the file names, as well as the body part of interest to extract the metrics and, if necessary, an area to calculate interactions:</p>
      <preformat position="float" xml:space="preserve">
      list_df = [df01,df02,df03,df04,df05,df06,
                 df07,df08,df09,df10,df11,df12]
      names = ['RAT01','RAT02','RAT03','RAT04','RAT05','RAT06',
               'RAT07','RAT08','RAT09','RAT10','RAT11','RAT12']
      report = rat.Reports(df_list = list_df,list_name = names,
                           bodypart = 'snout',fields = objects)
</preformat>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s4">
    <title>4. Discussion</title>
    <p>We presented the PyRAT, a library for animal tracking data analysis developed to be accessible to less experienced programmers. We implemented functions to infer common animal behavioral metrics used in the literature, such as object interaction (duration and number of interactions), traveled distance, speed, and time spent in different areas (Lima et al., <xref rid="B16" ref-type="bibr">2009</xref>; Gonzalez et al., <xref rid="B6" ref-type="bibr">2019</xref>; Rossato et al., <xref rid="B25" ref-type="bibr">2019</xref>; Moura et al., <xref rid="B19" ref-type="bibr">2020</xref>). Also, we implemented functions to infer animal behavior from tracked body parts in each frame using unsupervised approaches. If video recordings are synchronized with neural data, PyRAT can be used to extract epochs based on specific behaviors or metrics. Finally, our results indicate that PyRAT analyzes tracking data from different animal models if videos were acquired from a top-down perspective.</p>
    <p>There is similar software that can analyze tracking data as PyRAT, such as Traja, DLCAnalyzer, SimBA, and B-SOiD. Traja is a Python library that can analyze tracking data from coordinate data from any setup but does not infer behavioral metrics. DLCAnalyzer is a collection of R scripts that processes DLC files and quantifies motion metrics and behavior using supervised algorithms (Sturman et al., <xref rid="B27" ref-type="bibr">2020</xref>). Simple Behavioral Analysis (SimBA) is software with an easy-to-use interface that analyzes video or tracking data and applies a pre-trained supervised classifier to cluster behaviors (Nilsson et al., <xref rid="B21" ref-type="bibr">2020</xref>). However, the SimBA interface only works in Windows, limiting its usability on other platforms. B-SOiD is an open-source package that identifies behavior by combining supervised and unsupervised algorithms (Hsu and Yttri, <xref rid="B9" ref-type="bibr">2021</xref>) and works in mice, rats, and humans. B-SOiD analyzes videos acquired from different perspectives, showing the best results from bottom-up recordings. For further discussion and comparison between these tools refer to Panadeiro et al. (<xref rid="B23" ref-type="bibr">2021</xref>); von Ziegler et al. (<xref rid="B30" ref-type="bibr">2021</xref>). In contrast with other tools, PyRAT can be used in any operational system, does not need pre-trained classifiers, works without a graphic interface, and provides interactive documentation using Jupyter notebooks.</p>
    <p>Python in Rodent Analysis and Tracking is easier to use than other alternatives as it is a collection of functions, and the user just needs to input the tracking data to get the results and graphics following the step-by-step tutorial included in the documentation. In addition, PyRAT has a low learning curve, as its implementation is based on procedural programming. We designed the library to display metrics and graphics for all recorded sessions with a few lines of code. It does not have software requirements besides Python and widely used libraries, such as sklearn, pandas, and matplotlib. In summary, we present an open-source Python library to process tracking data, extract behavior and associate this information with neural data in a user-friendly approach.</p>
  </sec>
  <sec sec-type="data-availability" id="s5">
    <title>Data Availability Statement</title>
    <p>The original contributions presented in the study are publicly available. This data can be found here: GitHub: <ext-link xlink:href="https://github.com/pyratlib/pyrat" ext-link-type="uri">https://github.com/pyratlib/pyrat</ext-link>; Zenodo: <ext-link xlink:href="https://zenodo.org/record/5883277" ext-link-type="uri">https://zenodo.org/record/5883277</ext-link>.</p>
  </sec>
  <sec id="s6">
    <title>Ethics Statement</title>
    <p>The animal study was reviewed and approved by Animal Research Ethics Committee of Santos Dumont Institute.</p>
  </sec>
  <sec id="s7">
    <title>Author Contributions</title>
    <p>TD, BS, and AR designed, wrote, tested the library, and performed the analysis of the examples. RH and MG evaluated the algorithms. TD documented the library. TD, RH, MG, and AR wrote the manuscript. All authors contributed to the article and approved the submitted version.</p>
  </sec>
  <sec sec-type="funding-information" id="s8">
    <title>Funding</title>
    <p>“This study was financed in part by the Coordenação de Aperfeiçoamento de Pessoal de Nível” “Superior – Brasil (CAPES) – Finance Code 001.”, Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq), Ministério da Educação (MEC), and Instituto Santos Dumont (ISD).</p>
  </sec>
  <sec sec-type="COI-statement" id="conf1">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec sec-type="disclaimer" id="s9">
    <title>Publisher's Note</title>
    <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
  </sec>
</body>
<back>
  <sec sec-type="supplementary-material" id="s10">
    <title>Supplementary Material</title>
    <p>The Supplementary Material for this article can be found online at: <ext-link xlink:href="https://www.frontiersin.org/articles/10.3389/fnins.2022.779106/full#supplementary-material" ext-link-type="uri">https://www.frontiersin.org/articles/10.3389/fnins.2022.779106/full#supplementary-material</ext-link></p>
    <supplementary-material id="SM1" position="float" content-type="local-data">
      <media xlink:href="Data_Sheet_1.CSV">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="SM2" position="float" content-type="local-data">
      <media xlink:href="Data_Sheet_2.CSV">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aonuma</surname><given-names>H.</given-names></name><name><surname>Mezheritskiy</surname><given-names>M.</given-names></name><name><surname>Boldyshev</surname><given-names>B.</given-names></name><name><surname>Totani</surname><given-names>Y.</given-names></name><name><surname>Vorontsov</surname><given-names>D.</given-names></name><name><surname>Zakharov</surname><given-names>I.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>The role of serotonin in the influence of intense locomotion on the behavior under uncertainty in the mollusk lymnaea stagnalis</article-title>. <source>Front. Physiol</source>. <volume>11</volume>, <fpage>221</fpage>. <pub-id pub-id-type="doi">10.3389/fphys.2020.00221</pub-id><?supplied-pmid 32256385?><pub-id pub-id-type="pmid">32256385</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunn</surname><given-names>T. W.</given-names></name><name><surname>Marshall</surname><given-names>J. D.</given-names></name><name><surname>Severson</surname><given-names>K. S.</given-names></name><name><surname>Aldarondo</surname><given-names>D. E.</given-names></name><name><surname>Hildebrand</surname><given-names>D. G.</given-names></name><name><surname>Chettih</surname><given-names>S. N.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>Geometric deep learning enables 3d kinematic profiling across species and environments</article-title>. <source>Nat. Methods</source><volume>18</volume>, <fpage>564</fpage>–<lpage>573</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-021-01106-6</pub-id><?supplied-pmid 33875887?><pub-id pub-id-type="pmid">33875887</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fujisawa</surname><given-names>S.</given-names></name><name><surname>Amarasingham</surname><given-names>A.</given-names></name><name><surname>Harrison</surname><given-names>M. T.</given-names></name><name><surname>Buzsáki</surname><given-names>G.</given-names></name></person-group> (<year>2008</year>). <article-title>Behavior-dependent short-term assembly dynamics in the medial prefrontal cortex</article-title>. <source>Nat. Neurosci</source>. <volume>11</volume>, <fpage>823</fpage>–<lpage>833</lpage>. <pub-id pub-id-type="doi">10.1038/nn.2134</pub-id><?supplied-pmid 18516033?><pub-id pub-id-type="pmid">18516033</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fujisawa</surname><given-names>S.</given-names></name><name><surname>Amarasingham</surname><given-names>A.</given-names></name><name><surname>Harrison</surname><given-names>M. T.</given-names></name><name><surname>Buzsáki</surname><given-names>G.</given-names></name></person-group> (<year>2015</year>). <article-title>Simultaneous electrophysiological recordings of ensembles of isolated neurons in rat medial prefrontal cortex and intermediate ca1 area of the hippocampus during a working memory task</article-title>. <source>Dataset</source>
<volume>1</volume>, <fpage>1</fpage>–<lpage>6</lpage>. <pub-id pub-id-type="doi">10.6080/K01V5BWK</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geuther</surname><given-names>B. Q.</given-names></name><name><surname>Deats</surname><given-names>S. P.</given-names></name><name><surname>Fox</surname><given-names>K. J.</given-names></name><name><surname>Murray</surname><given-names>S. A.</given-names></name><name><surname>Braun</surname><given-names>R. E.</given-names></name><name><surname>White</surname><given-names>J. K.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Robust mouse tracking in complex environments using neural networks</article-title>. <source>Commun. Biol</source>. <volume>2</volume>, <fpage>1</fpage>–<lpage>11</lpage>. <pub-id pub-id-type="doi">10.1038/s42003-019-0362-1</pub-id><?supplied-pmid 30937403?><pub-id pub-id-type="pmid">30740537</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gonzalez</surname><given-names>M. C.</given-names></name><name><surname>Rossato</surname><given-names>J. I.</given-names></name><name><surname>Radiske</surname><given-names>A.</given-names></name><name><surname>Reis</surname><given-names>M. P.</given-names></name><name><surname>Cammarota</surname><given-names>M.</given-names></name></person-group> (<year>2019</year>). <article-title>Recognition memory reconsolidation requires hippocampal zif268</article-title>. <source>Sci. Rep</source>. <volume>9</volume>, <fpage>1</fpage>–<lpage>11</lpage>. <pub-id pub-id-type="doi">10.1038/s41598-019-53005-8</pub-id><?supplied-pmid 31719567?><pub-id pub-id-type="pmid">30626917</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gris</surname><given-names>K. V.</given-names></name><name><surname>Coutu</surname><given-names>J.-P.</given-names></name><name><surname>Gris</surname><given-names>D.</given-names></name></person-group> (<year>2017</year>). <article-title>Supervised and unsupervised learning technology in the study of rodent behavior</article-title>. <source>Front. Behav. Neurosci</source>. <volume>11</volume>, <fpage>141</fpage>. <pub-id pub-id-type="doi">10.3389/fnbeh.2017.00141</pub-id><?supplied-pmid 28804452?><pub-id pub-id-type="pmid">28804452</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gulley</surname><given-names>J. M.</given-names></name><name><surname>Hoover</surname><given-names>B. R.</given-names></name><name><surname>Larson</surname><given-names>G. A.</given-names></name><name><surname>Zahniser</surname><given-names>N. R.</given-names></name></person-group> (<year>2003</year>). <article-title>Individual differences in cocaine-induced locomotor activity in rats: behavioral characteristics, cocaine pharmacokinetics, and the dopamine transporter</article-title>. <source>Neuropsychopharmacology</source>
<volume>28</volume>, <fpage>2089</fpage>–<lpage>2101</lpage>. <pub-id pub-id-type="doi">10.1038/sj.npp.1300279</pub-id><?supplied-pmid 12902997?><pub-id pub-id-type="pmid">12902997</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>A. I.</given-names></name><name><surname>Yttri</surname><given-names>E. A.</given-names></name></person-group> (<year>2021</year>). <article-title>B-soid, an open-source unsupervised algorithm for identification and fast prediction of behaviors</article-title>. <source>Nat. Commun</source>. <volume>12</volume>, <fpage>1</fpage>–<lpage>13</lpage>. <pub-id pub-id-type="doi">10.1038/s41467-021-25420-x</pub-id><?supplied-pmid 34465784?><pub-id pub-id-type="pmid">33397941</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ilg</surname><given-names>E.</given-names></name><name><surname>Mayer</surname><given-names>N.</given-names></name><name><surname>Saikia</surname><given-names>T.</given-names></name><name><surname>Keuper</surname><given-names>M.</given-names></name><name><surname>Dosovitskiy</surname><given-names>A.</given-names></name><name><surname>Brox</surname><given-names>T.</given-names></name></person-group> (<year>2017</year>). <article-title>“Flownet 2.0: Evolution of optical flow estimation with deep networks,”</article-title> in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Honolulu, HI</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>2462</fpage>–<lpage>2470</lpage>.</mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Insafutdinov</surname><given-names>E.</given-names></name><name><surname>Pishchulin</surname><given-names>L.</given-names></name><name><surname>Andres</surname><given-names>B.</given-names></name><name><surname>Andriluka</surname><given-names>M.</given-names></name><name><surname>Schiele</surname><given-names>B.</given-names></name></person-group> (<year>2016</year>). <article-title>“Deepercut: a deeper, stronger, and faster multi-person pose estimation model,”</article-title> in <source>European Conference on Computer Vision</source> (<publisher-loc>Amsterdam</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>34</fpage>–<lpage>50</lpage>.</mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname><given-names>T.</given-names></name><name><surname>Duan</surname><given-names>F.</given-names></name><name><surname>Yang</surname><given-names>Z.</given-names></name><name><surname>Yin</surname><given-names>S.</given-names></name><name><surname>Chen</surname><given-names>X.</given-names></name><name><surname>Liu</surname><given-names>Y.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>Markerless rat behavior quantification with cascade neural network</article-title>. <source>Front. Neurorobot</source>. <volume>14</volume>, <fpage>570313</fpage>. <pub-id pub-id-type="doi">10.3389/fnbot.2020.570313</pub-id><?supplied-pmid 33192436?><pub-id pub-id-type="pmid">33192436</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kobak</surname><given-names>D.</given-names></name><name><surname>Berens</surname><given-names>P.</given-names></name></person-group> (<year>2019</year>). <article-title>The art of using t-sne for single-cell transcriptomics</article-title>. <source>Nat. Commun</source>. <volume>10</volume>, <fpage>1</fpage>–<lpage>14</lpage>. <pub-id pub-id-type="doi">10.1038/s41467-019-13056-x</pub-id><?supplied-pmid 31780648?><pub-id pub-id-type="pmid">30602773</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A.</given-names></name><name><surname>Sutskever</surname><given-names>I.</given-names></name><name><surname>Hinton</surname><given-names>G. E.</given-names></name></person-group> (<year>2012</year>). <article-title>Imagenet classification with deep convolutional neural networks</article-title>. <source>Adv. Neural Inf. Process. Syst</source>. <volume>25</volume>, <fpage>1097</fpage>–<lpage>1105</lpage>.</mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levine</surname><given-names>S.</given-names></name><name><surname>Pastor</surname><given-names>P.</given-names></name><name><surname>Krizhevsky</surname><given-names>A.</given-names></name><name><surname>Ibarz</surname><given-names>J.</given-names></name><name><surname>Quillen</surname><given-names>D.</given-names></name></person-group> (<year>2018</year>). <article-title>Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection</article-title>. <source>Int. J. Rob. Res</source>. <volume>37</volume>, <fpage>421</fpage>–<lpage>436</lpage>. <pub-id pub-id-type="doi">10.1177/0278364917710318</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lima</surname><given-names>R. H.</given-names></name><name><surname>Rossato</surname><given-names>J. I.</given-names></name><name><surname>Furini</surname><given-names>C. R.</given-names></name><name><surname>Bevilaqua</surname><given-names>L. R.</given-names></name><name><surname>Izquierdo</surname><given-names>I.</given-names></name><name><surname>Cammarota</surname><given-names>M.</given-names></name></person-group> (<year>2009</year>). <article-title>Infusion of protein synthesis inhibitors in the entorhinal cortex blocks consolidation but not reconsolidation of object recognition memory</article-title>. <source>Neurobiol. Learn. Mem</source>. <volume>91</volume>, <fpage>466</fpage>–<lpage>472</lpage>. <pub-id pub-id-type="doi">10.1016/j.nlm.2008.12.009</pub-id><?supplied-pmid 19141326?><pub-id pub-id-type="pmid">19141326</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lukasová</surname><given-names>A.</given-names></name></person-group> (<year>1979</year>). <article-title>Hierarchical agglomerative clustering procedure</article-title>. <source>Pattern Recognit</source>. <volume>11</volume>, <fpage>365</fpage>–<lpage>381</lpage>. <pub-id pub-id-type="doi">10.1016/0031-3203(79)90049-9</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>A.</given-names></name><name><surname>Mamidanna</surname><given-names>P.</given-names></name><name><surname>Cury</surname><given-names>K. M.</given-names></name><name><surname>Abe</surname><given-names>T.</given-names></name><name><surname>Murthy</surname><given-names>V. N.</given-names></name><name><surname>Mathis</surname><given-names>M. W.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>Deeplabcut: markerless pose estimation of user-defined body parts with deep learning</article-title>. <source>Nat. Neurosci</source>. <volume>21</volume>, <fpage>1281</fpage>–<lpage>1289</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id><?supplied-pmid 30127430?><pub-id pub-id-type="pmid">30127430</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moura</surname><given-names>C. A.</given-names></name><name><surname>Oliveira</surname><given-names>M. C.</given-names></name><name><surname>Costa</surname><given-names>L. F.</given-names></name><name><surname>Tiago</surname><given-names>P. R.</given-names></name><name><surname>Holanda</surname><given-names>V. A.</given-names></name><name><surname>Lima</surname><given-names>R. H.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>Prenatal restraint stress impairs recognition memory in adult male and female offspring</article-title>. <source>Acta Neuropsychiatr</source>. <volume>32</volume>, <fpage>122</fpage>–<lpage>127</lpage>. <pub-id pub-id-type="doi">10.1017/neu.2020.3</pub-id><?supplied-pmid 31992385?><pub-id pub-id-type="pmid">31992385</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nath</surname><given-names>T.</given-names></name><name><surname>Mathis</surname><given-names>A.</given-names></name><name><surname>Chen</surname><given-names>A. C.</given-names></name><name><surname>Patel</surname><given-names>A.</given-names></name><name><surname>Bethge</surname><given-names>M.</given-names></name><name><surname>Mathis</surname><given-names>M. W.</given-names></name></person-group> (<year>2019</year>). <article-title>Using deeplabcut for 3d markerless pose estimation across species and behaviors</article-title>. <source>Nat. Protoc</source>. <volume>14</volume>, <fpage>2152</fpage>–<lpage>2176</lpage>. <pub-id pub-id-type="doi">10.1038/s41596-019-0176-0</pub-id><?supplied-pmid 31227823?><pub-id pub-id-type="pmid">31227823</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Nilsson</surname><given-names>S. R.</given-names></name><name><surname>Goodwin</surname><given-names>N. L.</given-names></name><name><surname>Choong</surname><given-names>J. J.</given-names></name><name><surname>Hwang</surname><given-names>S.</given-names></name><name><surname>Wright</surname><given-names>H. R.</given-names></name><name><surname>Norville</surname><given-names>Z.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>Simple behavioral analysis (simba): an open source toolkit for computer classification of complex social behaviors in experimental animals</article-title>. <source>BioRxiv</source>. <pub-id pub-id-type="doi">10.1101/2020.04.19.049452</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nourizonoz</surname><given-names>A.</given-names></name><name><surname>Zimmermann</surname><given-names>R.</given-names></name><name><surname>Ho</surname><given-names>C. L. A.</given-names></name><name><surname>Pellat</surname><given-names>S.</given-names></name><name><surname>Ormen</surname><given-names>Y.</given-names></name><name><surname>Prévost-Solié</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>Etholoop: automated closed-loop neuroethology in naturalistic environments</article-title>. <source>Nat. Methods</source><volume>17</volume>, <fpage>1052</fpage>–<lpage>1059</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-020-0961-2</pub-id><?supplied-pmid 32994566?><pub-id pub-id-type="pmid">32994566</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panadeiro</surname><given-names>V.</given-names></name><name><surname>Rodriguez</surname><given-names>A.</given-names></name><name><surname>Henry</surname><given-names>J.</given-names></name><name><surname>Wlodkowic</surname><given-names>D.</given-names></name><name><surname>Andersson</surname><given-names>M.</given-names></name></person-group> (<year>2021</year>). <article-title>A review of 28 free animal-tracking software applications: current features and limitations</article-title>. <source>Lab. Anim</source>. <volume>50</volume>, <fpage>246</fpage>–<lpage>254</lpage>. <pub-id pub-id-type="doi">10.1038/s41684-021-00811-1</pub-id><?supplied-pmid 34326537?><pub-id pub-id-type="pmid">34326537</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Redmon</surname><given-names>J.</given-names></name><name><surname>Divvala</surname><given-names>S.</given-names></name><name><surname>Girshick</surname><given-names>R.</given-names></name><name><surname>Farhadi</surname><given-names>A.</given-names></name></person-group> (<year>2016</year>). <article-title>“You only look once: unified, real-time object detection,”</article-title> in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern recognition</source> (<publisher-loc>Las Vegas, NV</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>779</fpage>–<lpage>788</lpage>.</mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossato</surname><given-names>J. I.</given-names></name><name><surname>Gonzalez</surname><given-names>M. C.</given-names></name><name><surname>Radiske</surname><given-names>A.</given-names></name><name><surname>Apolinário</surname><given-names>G.</given-names></name><name><surname>Conde-Ocazionez</surname><given-names>S.</given-names></name><name><surname>Bevilaqua</surname><given-names>L. R.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Pkmζ inhibition disrupts reconsolidation and erases object recognition memory</article-title>. <source>J. Neurosci</source>. <volume>39</volume>, <fpage>1828</fpage>–<lpage>1841</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2270-18.2018</pub-id><?supplied-pmid 30622166?><pub-id pub-id-type="pmid">30622166</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spink</surname><given-names>A.</given-names></name><name><surname>Tegelenbosch</surname><given-names>R.</given-names></name><name><surname>Buma</surname><given-names>M.</given-names></name><name><surname>Noldus</surname><given-names>L.</given-names></name></person-group> (<year>2001</year>). <article-title>The ethovision video tracking system—a tool for behavioral phenotyping of transgenic mice</article-title>. <source>Physiol. Behav</source>. <volume>73</volume>, <fpage>731</fpage>–<lpage>744</lpage>. <pub-id pub-id-type="doi">10.1016/S0031-9384(01)00530-3</pub-id><?supplied-pmid 11566207?><pub-id pub-id-type="pmid">11566207</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sturman</surname><given-names>O.</given-names></name><name><surname>von Ziegler</surname><given-names>L.</given-names></name><name><surname>Schläppi</surname><given-names>C.</given-names></name><name><surname>Akyol</surname><given-names>F.</given-names></name><name><surname>Privitera</surname><given-names>M.</given-names></name><name><surname>Slominski</surname><given-names>D.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>Deep learning-based behavioral analysis reaches human accuracy and is capable of outperforming commercial solutions</article-title>. <source>Neuropsychopharmacology</source><volume>45</volume>, <fpage>1942</fpage>–<lpage>1952</lpage>. <pub-id pub-id-type="doi">10.1038/s41386-020-0776-y</pub-id><?supplied-pmid 32711402?><pub-id pub-id-type="pmid">32711402</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Toshev</surname><given-names>A.</given-names></name><name><surname>Szegedy</surname><given-names>C.</given-names></name></person-group> (<year>2014</year>). <article-title>“Deeppose: human pose estimation via deep neural networks,”</article-title> in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Columbus, OH</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>1653</fpage>–<lpage>1660</lpage>.</mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van der Maaten</surname><given-names>L.</given-names></name><name><surname>Hinton</surname><given-names>G.</given-names></name></person-group> (<year>2008</year>). <article-title>Visualizing data using t-sne</article-title>. <source>J. Mach. Learn. Res</source>. <volume>9</volume>, <fpage>2579</fpage>–<lpage>2605</lpage>.</mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>von Ziegler</surname><given-names>L.</given-names></name><name><surname>Sturman</surname><given-names>O.</given-names></name><name><surname>Bohacek</surname><given-names>J.</given-names></name></person-group> (<year>2021</year>). <article-title>Big behavior: challenges and opportunities in a new era of deep behavior profiling</article-title>. <source>Neuropsychopharmacology</source>
<volume>46</volume>, <fpage>33</fpage>–<lpage>44</lpage>. <pub-id pub-id-type="doi">10.1038/s41386-020-0751-7</pub-id><?supplied-pmid 32599604?><pub-id pub-id-type="pmid">32599604</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
