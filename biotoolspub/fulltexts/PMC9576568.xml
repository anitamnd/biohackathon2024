<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_ISCI105169 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEgr4 jpg ?>
<?FILEgr5 jpg ?>
<?FILEgr6 jpg ?>
<?FILEgr7 jpg ?>
<?FILEgr8 jpg ?>
<?FILEgr9 jpg ?>
<?FILEgr10 jpg ?>
<?FILEgr11 jpg ?>
<?FILEgr12 jpg ?>
<?FILEfx1 jpg ?>
<?FILEsi1 gif ?>
<?FILEsi2 gif ?>
<?FILEsi3 gif ?>
<?FILEsi4 gif ?>
<?FILEsi5 gif ?>
<?FILEsi6 gif ?>
<?FILEsi7 gif ?>
<?FILEsi8 gif ?>
<?FILEsi9 gif ?>
<?FILEsi10 gif ?>
<?FILEsi11 gif ?>
<?FILEsi12 gif ?>
<?FILEsi13 gif ?>
<?FILEsi14 gif ?>
<?FILEsi15 gif ?>
<?FILEsi16 gif ?>
<?FILEsi17 gif ?>
<?FILEsi18 gif ?>
<?FILEsi19 gif ?>
<?FILEsi20 gif ?>
<?FILEsi21 gif ?>
<?FILEsi22 gif ?>
<?FILEsi23 gif ?>
<?FILEsi24 gif ?>
<?FILEsi25 gif ?>
<?FILEsi26 gif ?>
<?FILEsi27 gif ?>
<?FILEsi28 gif ?>
<?FILEsi29 gif ?>
<?FILEsi30 gif ?>
<?FILEsi31 gif ?>
<?FILEsi32 gif ?>
<?FILEsi33 gif ?>
<?FILEsi34 gif ?>
<?FILEsi35 gif ?>
<?FILEsi36 gif ?>
<?FILEsi37 gif ?>
<?FILEsi38 gif ?>
<?FILEsi39 gif ?>
<?FILEsi40 gif ?>
<?FILEsi41 gif ?>
<?FILEsi42 gif ?>
<?FILEsi43 gif ?>
<?FILEsi44 gif ?>
<?FILEsi45 gif ?>
<?FILEsi46 gif ?>
<?FILEsi47 gif ?>
<?FILEsi48 gif ?>
<?FILEsi49 gif ?>
<?FILEsi50 gif ?>
<?FILEsi51 gif ?>
<?FILEsi52 gif ?>
<?FILEsi53 gif ?>
<?FILEsi54 gif ?>
<?FILEsi55 gif ?>
<?FILEsi56 gif ?>
<?FILEsi57 gif ?>
<?FILEsi58 gif ?>
<?FILEsi59 gif ?>
<?FILEsi60 gif ?>
<?FILEsi61 gif ?>
<?FILEsi62 gif ?>
<?FILEsi63 gif ?>
<?FILEsi64 gif ?>
<?FILEsi65 gif ?>
<?FILEsi66 gif ?>
<?FILEsi67 gif ?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">iScience</journal-id>
    <journal-id journal-id-type="iso-abbrev">iScience</journal-id>
    <journal-title-group>
      <journal-title>iScience</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2589-0042</issn>
    <publisher>
      <publisher-name>Elsevier</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9576568</article-id>
    <article-id pub-id-type="pii">S2589-0042(22)01441-9</article-id>
    <article-id pub-id-type="doi">10.1016/j.isci.2022.105169</article-id>
    <article-id pub-id-type="publisher-id">105169</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>ADH-PPI: An attention-based deep hybrid model for protein-protein interaction prediction</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au1">
        <name>
          <surname>Asim</surname>
          <given-names>Muhammad Nabeel</given-names>
        </name>
        <email>muhammad_nabeel.asim@dfki.de</email>
        <xref rid="aff1" ref-type="aff">1</xref>
        <xref rid="aff2" ref-type="aff">2</xref>
        <xref rid="fn1" ref-type="fn">4</xref>
        <xref rid="cor1" ref-type="corresp">∗</xref>
      </contrib>
      <contrib contrib-type="author" id="au2">
        <name>
          <surname>Ibrahim</surname>
          <given-names>Muhammad Ali</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">1</xref>
        <xref rid="aff2" ref-type="aff">2</xref>
      </contrib>
      <contrib contrib-type="author" id="au3">
        <name>
          <surname>Malik</surname>
          <given-names>Muhammad Imran</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">3</xref>
      </contrib>
      <contrib contrib-type="author" id="au4">
        <name>
          <surname>Dengel</surname>
          <given-names>Andreas</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">1</xref>
        <xref rid="aff2" ref-type="aff">2</xref>
      </contrib>
      <contrib contrib-type="author" id="au5">
        <name>
          <surname>Ahmed</surname>
          <given-names>Sheraz</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">2</xref>
      </contrib>
      <aff id="aff1"><label>1</label>Department of Computer Science, Technical University of Kaiserslautern, 67663 Kaiserslautern, Germany</aff>
      <aff id="aff2"><label>2</label>German Research Center for Artificial Intelligence GmbH, 67663 Kaiserslautern, Germany</aff>
      <aff id="aff3"><label>3</label>National Center of Artificial Intelligence, National University of Sciences and Technology, Islamabad 44000, Pakistan</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>∗</label>Corresponding author <email>muhammad_nabeel.asim@dfki.de</email></corresp>
      <fn id="fn1">
        <label>4</label>
        <p id="ntpara0010">Lead contact</p>
      </fn>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>21</day>
      <month>9</month>
      <year>2022</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="collection">
      <day>21</day>
      <month>10</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>21</day>
      <month>9</month>
      <year>2022</year>
    </pub-date>
    <volume>25</volume>
    <issue>10</issue>
    <elocation-id>105169</elocation-id>
    <history>
      <date date-type="received">
        <day>2</day>
        <month>5</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>30</day>
        <month>6</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>16</day>
        <month>9</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022 The Authors</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).</license-p>
      </license>
    </permissions>
    <abstract id="abs0010">
      <title>Summary</title>
      <p>Protein-protein interaction (PPI) prediction is essential to understand the functions of proteins in various biological processes and their roles in the development, progression, and treatment of different diseases. To perform economical large-scale PPI analysis, several artificial intelligence-based approaches have been proposed. However, these approaches have limited predictive performance due to the use of in-effective statistical representation learning methods and predictors that lack the ability to extract comprehensive discriminative features. The paper in hand generates statistical representation of protein sequences by applying transfer learning in an unsupervised manner using FastText embedding generation approach. Furthermore, it presents “ADH-PPI” classifier which reaps the benefits of three different neural layers, long short-term memory, convolutional, and self-attention layers. Over two different species benchmark datasets, proposed ADH-PPI predictor outperforms existing approaches by an overall accuracy of 4%, and matthews correlation coefficient of 6%. In addition, it achieves an overall accuracy increment of 7% on four independent test sets. <bold>Availability</bold>: ADH-PPI web server is publicly available at <ext-link ext-link-type="uri" xlink:href="https://sds_genetic_analysis.opendfki.de/PPI/" id="intref0010">https://sds_genetic_analysis.opendfki.de/PPI/</ext-link></p>
    </abstract>
    <abstract abstract-type="graphical" id="abs0015">
      <title>Graphical abstract</title>
      <fig id="undfig1" position="anchor">
        <graphic xlink:href="fx1"/>
      </fig>
    </abstract>
    <abstract abstract-type="author-highlights" id="abs0020">
      <title>Highlights</title>
      <p>
        <list list-type="simple" id="ulist0010">
          <list-item id="u0010">
            <label>•</label>
            <p id="p0010">Protein sequences representation generation through unsupervised transfer learning</p>
          </list-item>
          <list-item id="u0015">
            <label>•</label>
            <p id="p0015">A unique paradigm for sequence fixed length generation</p>
          </list-item>
          <list-item id="u0020">
            <label>•</label>
            <p id="p0020">Development of a robust, precise, and interpretable classifier</p>
          </list-item>
          <list-item id="u0025">
            <label>•</label>
            <p id="p0025">Development of a public web server to predict protein-protein interactions on the go</p>
          </list-item>
        </list>
      </p>
    </abstract>
    <abstract abstract-type="teaser" id="abs0025">
      <p>Association analysis; Bioinformatics; Computational bioinformatics</p>
    </abstract>
    <kwd-group id="kwrds0010">
      <title>Subject areas</title>
      <kwd>Association analysis</kwd>
      <kwd>Bioinformatics</kwd>
      <kwd>Computational bioinformatics</kwd>
    </kwd-group>
  </article-meta>
  <notes>
    <p id="misc0010">Published: October 21, 2022</p>
  </notes>
</front>
<body>
  <sec id="sec1">
    <title>Introduction</title>
    <p id="p0055">Proteins are large and complex biomolecules that perform a multitude of crucial functions within living organisms mostly by interacting with other proteins (<xref rid="bib8" ref-type="bibr">Berggård et al., 2007</xref>). Protein-protein interaction (PPI) analysis is important to understand diverse biological processes including cell proliferation (<xref rid="bib52" ref-type="bibr">Nooren and Thornton, 2003</xref>), signal transduction (<xref rid="bib56" ref-type="bibr">Pawson and Nash, 2000</xref>), DNA transcription, replication (<xref rid="bib80" ref-type="bibr">Zhang et al., 2012</xref>; <xref rid="bib67" ref-type="bibr">Vickers, 2017</xref>), hormone regulation (<xref rid="bib81" ref-type="bibr">Zhao, 2015</xref>), cycle control (<xref rid="bib40" ref-type="bibr">Kulminskaya and Oberer, 2020</xref>), and neuro-transmission (<xref rid="bib66" ref-type="bibr">Südhof, 1995</xref>). It also helps to identify disease-related signaling pathways and symbolize unfamiliar targets for therapeutic intervention (<xref rid="bib76" ref-type="bibr">You et al., 2010</xref>). In-depth exploration of PPIs is critical for a thorough understanding of protein functionalities, genetic mechanisms (<xref rid="bib70" ref-type="bibr">Wang et al., 2007</xref>; <xref rid="bib1" ref-type="bibr">Alberts, 1998</xref>), discovery of new drug targets (<xref rid="bib3" ref-type="bibr">Andrei et al., 2017</xref>), and development of effective preventive or therapeutic strategies to combat diseases (<xref rid="bib58" ref-type="bibr">Petta et al., 2016</xref>).</p>
    <p id="p0060">A number of experimental approaches including tandem affinity purification (<xref rid="bib18" ref-type="bibr">Gavin et al., 2002</xref>), mass spectrometric protein complex identification (<xref rid="bib23" ref-type="bibr">Ho et al., 2002</xref>), protein chips (<xref rid="bib83" ref-type="bibr">Zhu, 2003</xref>), and yeast two-hybrid (Y2H) (<xref rid="bib33" ref-type="bibr">Ito et al., 2000</xref>; <xref rid="bib39" ref-type="bibr">Krogan et al., 2006</xref>) have been utilized to infer PPIs. However, these experimental methods are expensive and time-consuming (<xref rid="bib62" ref-type="bibr">Schoenrock et al., 2014</xref>). Furthermore, because of high specificity between proteins, these experimental approaches produce significant false positive results which marks the need of additional methodologies to cross-check the obtained results. Due to slow sequence analysis process, these approaches have been typically applied to identify intra-species PPIs, whereas inter-species interactome remained comparatively understudied (<xref rid="bib62" ref-type="bibr">Schoenrock et al., 2014</xref>). Advancements in high-throughput approaches and the influx of PPI data related to different species have given rise to many databases including the Database of Interacting Proteins (DIP): RRID:<ext-link ext-link-type="uri" xlink:href="rridsoftware:SCR_003167" id="interref110">SCR_003167</ext-link>; <ext-link ext-link-type="uri" xlink:href="https://dip.doe-mbi.ucla.edu/dip/Main.cgi" id="interref0115">https://dip.doe-mbi.ucla.edu/dip/Main.cgi</ext-link> (<xref rid="bib60" ref-type="bibr">Salwinski et al., 2004</xref>), the Molecular Interaction Database (MINT): RRID:<ext-link ext-link-type="uri" xlink:href="rridsoftware:SCR_003546" id="interref120">SCR_003546</ext-link>; <ext-link ext-link-type="uri" xlink:href="http://integrativebiology.org" id="interref520">http://integrativebiology.org</ext-link> (<xref rid="bib44" ref-type="bibr">Licata et al., 2012</xref>), and the Human Protein References Database (HPRD): RRID:<ext-link ext-link-type="uri" xlink:href="rridsoftware:SCR_007027" id="interref125">SCR_007027</ext-link>; <ext-link ext-link-type="uri" xlink:href="http://www.hprd.org" id="interref0130">http://www.hprd.org</ext-link> (<xref rid="bib57" ref-type="bibr">Peri et al., 2004</xref>). The public availability of such humongous annotated data has opened new horizons for the development of computational approaches for economical, fast, and more accurate analysis of PPIs.</p>
    <p id="p0065">In order to predict PPIs, to date, a plethora of computational approaches have been developed (<xref rid="bib36" ref-type="bibr">Joshi et al., 2004</xref>; <xref rid="bib59" ref-type="bibr">Qi et al., 2005</xref>) which can be broadly segregated into three classes: 1) Structure based, 2) Network based, and 2) Sequence based. Structure-based approaches estimate the likelihood of PPIs by leveraging primary and higher level spatial structures like secondary, tertiary, or quarternary structures (<xref rid="bib53" ref-type="bibr">Northey et al., 2018</xref>). Those proteins are more likely to interact in which compatibility levels of interacting regions are high or in which spatial structures more oftenly appear on protein-protein binding-motif regions (<xref rid="bib53" ref-type="bibr">Northey et al., 2018</xref>; <xref rid="bib14" ref-type="bibr">Espadaler et al., 2005</xref>; <xref rid="bib64" ref-type="bibr">Singh et al., 2010</xref>). Following this principle, (<xref rid="bib30" ref-type="bibr">Hue et al., 2010</xref>) performed the pioneer work to predict PPIs in which they fed structural information of protein pairs to support vector machine (SVM) classifier. (<xref rid="bib80" ref-type="bibr">Zhang et al., 2012</xref>) performed similar work by using protein structural information and Bayesian classifier for <funding-source id="gs1">PPI</funding-source> interaction prediction. (<xref rid="bib24" ref-type="bibr">Hosur et al., 2012</xref>) utilized protein structural information to compute the interaction confidence score for each protein pair using a boosting classifier. Structural information-based PPI predictors neglect the mutual influence of local structures (<xref rid="bib16" ref-type="bibr">Fu et al., 2019</xref>; <xref rid="bib4" ref-type="bibr">Asim et al., 2020a</xref>). Such approaches are more vulnerable to overlook important information for accurate PPI prediction which might be present in primary sequences and likely to get lost while extracting structural information (<xref rid="bib16" ref-type="bibr">Fu et al., 2019</xref>; <xref rid="bib4" ref-type="bibr">Asim et al., 2020a</xref>).</p>
    <p id="p0070">Network-based PPI prediction approaches utilize the link information present in existing PPI networks. PPI networks are hierarchical illustrations of interacting proteins and exist in form of ontologies where each node represents a particular protein and interaction of two different proteins is represented by an association link. Proteins resided in upper hierarchy act as parents and their attached interacting partners of lower hierarchy act as child’s. Network-based PPI prediction approaches extract the names of proteins from existing ontologies to find their biological characteristics in other resources and heterogeneous relations between proteins in order to predict interactions between new proteins on the basis of prior learning. Initial network-based approaches considered that those proteins are more likely to interact which share more common interacting partners in PPI network (<xref rid="bib38" ref-type="bibr">Kovács et al., 2019</xref>). However, these approaches have become obsolete after the discovery of (<xref rid="bib38" ref-type="bibr">Kovács et al., 2019</xref>) that two proteins are more likely to interact if at least one of them is very similar to other’s interacting partners. But (<xref rid="bib38" ref-type="bibr">Kovács et al., 2019</xref>) approach has limited practical significance as it lacks to determine the interactions between the long distant proteins. To address this problem, (<xref rid="bib71" ref-type="bibr">Wang et al., 2020b</xref>) predicted PPIs without defining the length of different network paths in advance; however, their approach heavily relies on the quality of PPI network. Most recent paradigm of network-based approaches considers that proteins of same functional module are more likely to interact as compared to the proteins of different functional module (<xref rid="bib26" ref-type="bibr">Hu et al., 2021b</xref>). Using the already known information of the functional modules, (<xref rid="bib25" ref-type="bibr">Hu et al., 2021a</xref>) integrated biological information of proteins, particularly Gene Ontology into PPI network to predict PPIs. Likewise, Ioan et el. (<xref rid="bib31" ref-type="bibr">Ieremie et al., 2022</xref>) proposed attention-based deep learning model which used graph-based embeddings to learn deep semantic relations of Gene Ontology to distinguish interactive and non-interactive protein sequence pairs. A closer look at different network-based PPI prediction approaches reveals that these approaches completely rely on pre-computed PPI networks and biological information, both of which need periodic updates to cater huge proteins-related data produced by high-throughput technologies. Furthermore, such resources are characterized by high false-positive as well as false negative rates which eventually hamper the performance of PPI predictors. Therefore, raw sequence-based PPI prediction approaches are widely considered more appropriate to perform large-scale PPI analysis.</p>
    <p id="p0075">To date, several raw sequence-based machine and deep learning-based approaches have also been proposed (<xref rid="bib79" ref-type="bibr">Yu et al., 2021</xref>; <xref rid="bib35" ref-type="bibr">Jiang et al., 2020</xref>; <xref rid="bib69" ref-type="bibr">Wang et al., 2020a</xref>) for PPI prediction. For example, most recently, (<xref rid="bib79" ref-type="bibr">Yu et al., 2021</xref>) proposed a machine learning-based PPI predictor GcForest-PPI. It utilized residues composition information and physicochemical characteristics to generate statistical representation of protein sequences. It used elastic net (<xref rid="bib84" ref-type="bibr">Zou and Hastie, 2005</xref>) to extract discriminative set of features that were passed to an ensemble classifier based on three different models namely XGBoost, Random Forest, and Extra-Tree. GcForest-PPI achieved the accuracy of 95.44% and 89.26% on benchmark <italic>Saccharomyces cerevisiae</italic> (S.cervisiae) and <italic>Helicobacter pylori</italic> (H.pylori) datasets. (<xref rid="bib37" ref-type="bibr">Kong et al., 2020</xref>) presented another machine learning approach namely FCTP-WSRC. They utilized amino acid physicochemical properties, composition, and transition information to generate statistical representations of protein sequences. They utilized principal component analysis to reduce redundant features and generate better feature space. Using reduced statistical representations and WSRC (<xref rid="bib37" ref-type="bibr">Kong et al., 2020</xref>) classifier, they managed to achieve the accuracy of 86.73% and 78.70% on 2 benchmark S.cervisiae and H.pylori datasets. (<xref rid="bib34" ref-type="bibr">Jia et al., 2015</xref>) also proposed a machine learning-based PPI predictor namely “iPPI-Esml”. They combined residue composition information, physicochemical characteristics, and protein chain-specific wavelet transform information to generate statistical representations of protein sequences which were passed to a deep forest classifier. The iPPI-Esml approach achieved the accuracy of 95% on benchmark S.cervisiae and 90% on H.pylori datasets.</p>
    <p id="p0080">Apart from machine learning-based PPI predictors, (<xref rid="bib73" ref-type="bibr">Yao et al., 2019</xref>) proposed a deep learning-based predictor namely DeepFE-PPI. They utilized Word2vec-based embedding generation approach (<xref rid="bib49" ref-type="bibr">Mikolov et al., 2013</xref>) to generate statistical representations of protein sequences which were passed to a multilayer perceptron model for PPI prediction. DeepFE-PPI achieved the accuracy of 95% on benchmark S.cerevisiae dataset. (<xref rid="bib13" ref-type="bibr">Du et al., 2017</xref>) presented DeepPPI which utilized residues physicochemical properties to generate statistical representations of protein sequences. They utilized a multilayer perceptron model which extracted the high-level discriminative features from statistical vectors to make accurate PPI prediction. DeepPPI achieved the accuracy of 94% and 86% on 2 benchmark S.cervisiae and H.pylori datasets.</p>
    <p id="p0085">Critical analysis of machine and deep learning-based PPI predictors (i.e GcForest-PPI (<xref rid="bib79" ref-type="bibr">Yu et al., 2021</xref>) WSRC (<xref rid="bib37" ref-type="bibr">Kong et al., 2020</xref>), DeepFE-PPI (<xref rid="bib73" ref-type="bibr">Yao et al., 2019</xref>)) reveals that residue composition or physicochemical properties-based protein sequence encoding methods overlook the relationships that exist between different amino acid segments as a function of context of long protein sequences (<xref rid="bib34" ref-type="bibr">Jia et al., 2015</xref>; <xref rid="bib79" ref-type="bibr">Yu et al., 2021</xref>). Furthermore, selecting an optimal set of physicochemical properties from a huge available collection requires extensive empirical evaluation (<xref rid="bib34" ref-type="bibr">Jia et al., 2015</xref>; <xref rid="bib79" ref-type="bibr">Yu et al., 2021</xref>). Besides, concatenation of statistical representations generated through different types of encoding methods also gives birth to redundant features. To remove redundant features, existing PPI predictors (<xref rid="bib37" ref-type="bibr">Kong et al., 2020</xref>; <xref rid="bib79" ref-type="bibr">Yu et al., 2021</xref>) utilize dimensionality reduction or feature selection approaches to generate an effective feature space. However, dimensionality reduction approaches generally prove in-efficient for large and weakly nonlinear data (<xref rid="bib65" ref-type="bibr">Sorzano et al., 2014</xref>; <xref rid="bib11" ref-type="bibr">Chao et al., 2019</xref>). Also, determining the number of principal components for the generation of compressed representation varies across different datasets, indicating that optimal principal components are found through comprehensive empirical evaluation. Similarly, major disadvantage of using elastic net as a feature selection approach (<xref rid="bib79" ref-type="bibr">Yu et al., 2021</xref>) is the high computational cost as one needs to cross validate the relative weights of L1 and L2 penalty. Elastic-net leverages a combination of L1 and L2 penalty in order to shrink coefficient of un-important features to near zero, which is a computationally expensive and a time-consuming process (<xref rid="bib61" ref-type="bibr">Sanchez-Pinto et al., 2018</xref>).</p>
    <p id="p0090">Furthermore, Word2vec (<xref rid="bib49" ref-type="bibr">Mikolov et al., 2013</xref>)-based PPI prediction approaches (<xref rid="bib73" ref-type="bibr">Yao et al., 2019</xref>) also lack to generate effective statistical representation of protein sequences. Because, Word2vec (<xref rid="bib49" ref-type="bibr">Mikolov et al., 2013</xref>) treats k-mers as atomic entities to generate their distinct vectors in which it neglects the distribution of amino acids within each k-mer. FastText (<xref rid="bib10" ref-type="bibr">Bojanowski et al., 2017</xref>) is an extension of Word2vec (<xref rid="bib49" ref-type="bibr">Mikolov et al., 2013</xref>) where vector of each k-mer is computed by considering the distribution of k-mers and distribution of amino acids inside the k-mers. Also, our previous work (<xref rid="bib5" ref-type="bibr">Asim et al., 2020b</xref>) found that among three different neural embedding generation approaches namely Word2Vec, FastText, and Glove, FastText approach most effectively captures semantic information of k-mers.</p>
    <p id="p0095">We use FastText approach to generate comprehensive contextual information aware statistical vectors for k-mers present in protein sequences. Furthermore, we generate fixed length protein sequences using six traditional and four robust fixed length generation approaches. We propose a robust attention-based deep hybrid model namely ADH-PPI which makes best use of different neural network layers and optimization strategies for accurate PPI prediction. ADH-PPI makes use of long short-term memory, convolutional, and attention layers to find most discriminative features along with their short- and long-range dependencies important to effectively distinguish interactive protein sequence pairs from non-interactive protein sequence pairs. To avoid under-fitting and over-fitting, training of the ADH-PPI is optimized using different kinds of dropout, normalization, and learning rate decay strategies.</p>
    <p id="p0100">A comprehensive empirical evaluation indicates that proposed ADH-PPI approach outperforms several machine and deep learning-based PPI predictors across 6 different species benchmark datasets with a decent margin. To better describe the decisions of proposed ADH-PPI approach, we map the weights of statistical feature space to potential k-mer distributions which contribute the most for accurate PPI prediction through the reverse engineering strategy.</p>
  </sec>
  <sec id="sec2">
    <title>Results</title>
    <sec id="sec2.1">
      <title>Key idea</title>
      <p id="p0105">We generate an effective statistical representation of protein sequences which is based on comprehensive local and global contextual information of sequence residues captured by applying transfer learning in an unsupervised manner using FastText-based embedding generation approach. To generate fixed length proteins sequences without losing residues distributions important for protein-protein interaction prediction, we use six traditional and four robust fixed length generation methods. Using optimized fixed length protein sequences, we develop a robust classifier which makes best use of heterogeneous neural layers such as long short-term memory layer, convolutional layer, and attention layer to capture most informative hidden features and their long-range dependencies essential to effectively distinguish interactive proteins from non-interactive proteins.</p>
    </sec>
    <sec id="sec2.2">
      <title>Summary of results</title>
      <p id="p0110">We comprehensively describe the performance produced by 6 traditional pre-processing strategies used to generate fixed length sequences. Furthermore, we compare the performance of 4 distinct settings based on subsequences to showcase which region of protein sequences contains most crucial information about PPI prediction. We perform a performance comparison of proposed predictor using traditional pre-processing strategies and proposed subsequence generation settings. We quantify the performance impact of CNN layer in proposed predictor. Finally, we compare the performance of proposed ADH-PPI approach with existing PPI predictors using two core datasets and four independent test sets.</p>
    </sec>
    <sec id="sec2.3">
      <title>A comprehensive performance analysis of traditional sequence pre-processing strategies</title>
      <p id="p0115"><xref rid="fig1" ref-type="fig">Figure 1</xref> illustrates the performance values produced by proposed ADH-PPI predictor under the hood of 6 traditional copy padding and sequence truncation approaches used to generate fixed length sequences across two benchmark core datasets.<fig id="fig1"><label>Figure 1</label><caption><p>Comparison of the performance of proposed ADH-PPI approach across 2 different S.cerevisiae and H.pyloir datasets using 6 traditional sequence fixed length generation approaches</p></caption><graphic xlink:href="gr1"/></fig></p>
      <p id="p0120">Performance analysis of 6 commonly used pre-processing strategies over <italic>Saccharomyces cerevisiae</italic> (S.cerevisiae) dataset indicates that mapping protein sequence to maximum possible length and applying copy padding at the end of protein sequences marks best performance of 92% in terms of accuracy and F1-score. Mapping protein sequences to average length and applying padding or sequence truncation trick at the end of protein sequences achieve second best performance. Among all 3 settings which applies copy padding or sequence truncation at the end of protein sequences, mapping protein sequences to minimum possible length attains lowest performance across both evaluation measures. On the other hand, from 3 settings where copy padding or sequence truncation trick is applied at the start of protein sequences, once again mapping protein sequences to maximum possible length achieves overall third best and slightly better performance than other 2 settings based on average and minimum length.</p>
      <p id="p0125">Contrarily, over <italic>Helicobacter pylori</italic> (H.pylori) dataset, mapping protein sequences to average sequence length and applying copy padding or sequence truncation at the ending region of protein sequence marks best performance followed by the performance produced by maximum sequence length setting where copy padding is applied at the starting region of protein sequences across both evaluation metrics. Setting based on maximum length where copy padding is applied at the end of protein sequence and setting based on minimum length where sequence truncation is applied at the end of sequence length achieve almost similar performance of around 87% in terms of accuracy and F1-score. Whereas, average sequence length-based setting where copy padding or sequence truncation is applied at starting region of protein sequences marks slightly better performance than minimum sequence length-based setting.</p>
      <p id="p0130">Among all 6 traditional copy padding or sequence truncation approaches, sequence fixed length generation approaches which apply copy padding or sequence truncation at the ending regions of protein sequences using average or maximum sequence length mark better performance across both core datasets.</p>
    </sec>
    <sec id="sec2.4">
      <title>Performance analysis of proposed subsequence-based pre-processing approaches</title>
      <p id="p0135">To showcase the impact of 4 different subsequence-based fixed length generation strategies on the performance of proposed classifier, <xref rid="fig2" ref-type="fig">Figure 2</xref> illustrates which protein regions contain most informative distribution of residues for PPI predictions across core datasets of 2 distinct species.<fig id="fig2"><label>Figure 2</label><caption><p>Proposed classifier performance analysis under the hood of 4 different subsequences-based strategies used to generate fixed length sequences</p><p>Here PA represents protein A and PB refers to protein B, whereas S indicates the starting amino acids of protein sequence and E represents the ending amino acids of protein sequence.</p></caption><graphic xlink:href="gr2"/></fig></p>
      <p id="p0140">A critical analysis indicates that over S.cerevisiae dataset, performance of 2 settings where amino acids taken from the start of protein A are combined with the amino acids taken from the end of protein B, and amino acids of starting region of protein A are combined with amino acids of starting region of protein B marks similar performance trends across different thresholds of residues. While former setting achieves the performance of 95.5%, latter setting attains the performance of 94% until 20 residues. With the increase of amino acids, performance of both settings slightly fluctuates before finishing at 95% and 93.5%, respectively, at 70 residues across both evaluation metrics. Former setting achieves the peak performance using 40 amino acids whereas latter setting marks the best performance with 10 amino acids. Performance of setting-5 which explores the start-end regions of protein A and protein B almost gradually declines until 30 amino acids, increases up to 94% with 40 amino acids before flattening off across rest of amino acids thresholds. Likewise, performance of setting-3 which selects amino acids from the ending regions of protein A and protein B also progressively decreases from the peak of 93.5% until 30 amino acids before leveling off until 50 amino acids and finishing at 93% at 70 amino acids in terms of accuracy and F1-score. Among all 4 settings, setting-4 which selects amino acids from starting region of protein A and ending region of protein B marks best performance followed by setting-5 which explores the start-end region of both proteins. Whereas, setting-3 marks the lowest performance among all settings based on protein discriminative subsequences.</p>
      <p id="p0145">Over H.pylori dataset (<xref rid="fig2" ref-type="fig">Figure 2</xref>), performance of setting-4 remains around 86% until 30 residues before jumping to the peak of 91% with 40 amino acids which declines afterward and finished at 87% with 70 amino acids. Here, performance of setting-2 slightly fluctuates until 40 amino acids before declining and leveling off at 87%. Performance of setting-2 almost gradually decreases from 86% to 84% until 30 amino acids, jumps to the peak of 88.5% until 50 amino acids before decreasing and ending around 86%. Setting-5 performance shows upward trend at most amino acids thresholds and finishes around 86% across both evaluation metrics. Like S.cerevisiae dataset, once again, setting-4 which explores the starting region of protein A and ending region of protein B marks best performance in terms of accuracy and F1-score. However, for H.pylori dataset, peak performances of all 4 setting are comparatively lower than the figures achieved over S.cerevisiae dataset across both evaluation metrics.</p>
      <p id="p0150">Overall, among all protein subsequence-based settings, setting-4 which selects amino acids from starting region of protein A and ending region of protein B marks best performance across both core PPI datasets in terms of accuracy and F1-score.</p>
    </sec>
    <sec id="sec2.5">
      <title>Performance comparison of proposed subsequence approaches with traditional sequence fixed length generation approaches</title>
      <p id="p0155">In order to compare the performance of traditional copy padding or sequence truncation-based settings with 4 other settings which explores the performance potential of distinct regions of protein sequences by selecting different number of residues, <xref rid="fig3" ref-type="fig">Figure 3</xref> indicates area under receiver operating characteristics (AU-ROC) produced by 5 different settings over S.cerevisiae and H.pylori datasets. As is indicated by the <xref rid="fig3" ref-type="fig">Figure 3</xref>, over S.cerevisiae dataset, in setting-1, applying traditional copy padding or sequence truncation approaches at the ending region of protein sequence slightly achieve better degree of separability as compared to those approaches which pad or truncate starting region of protein sequence. Former approaches attain the peak of 95% and latter approaches acquire the peak of 94%. Among all 6 approaches, mapping protein sequences to average length and applying copy padding or sequence truncation at the end of protein sequences mark best performance followed by another ending region-based setting which maps protein sequences to minimum length.<fig id="fig3"><label>Figure 3</label><caption><p>Impact of 5 different settings on the performance of proposed ADH-PPI approach across 2 different S.cerevisiae and H.pyloir datasets for the task of PPI prediction in terms of area under receiver operating characteristics</p><p>Setting 1 is based on traditional copy padding, sequence truncation, and hybrid approaches. Setting 2, 3, 4, and 5 are based on subsequences criteria where x number of amino acids from staring and ending regions of Protein A and Protein B are taken. The value of x varies from 10 to 70 amino acids with the difference of 10 amino acids. Setting 2 takes x number of amino acids from starting region of Protein A and ending region of Protein B. Setting 3 takes x number of amino acids from ending region of Protein A and Protein B. Setting 4 takes x number of amino acids from starting region of Protein A and Protein B. Setting 5 takes x number of amino acids from starting and ending region of Protein A and starting and ending region of Protein B.</p></caption><graphic xlink:href="gr3"/></fig></p>
      <p id="p0160">Furthermore, in setting-2 based on partial protein sequences, with the influx of residues, ADH-PPI degree of separability gets improved up to the peak of 98% until 30. Afterward, ADH-PPI performance fluctuates across different residue thresholds before finishing at 97%. However, all setting-2 residue variants achieve better performance than traditional copy padding or sequence truncation approaches (setting-1), indicating the prime performance potential of protein subsequences.</p>
      <p id="p0165">In setting-3 which explores the performance potential of merely ending region of protein pairs, varying the residues from 10 to 70, performance of ADH-PPI remains almost constant at 96% which is still better than the performance attained by most commonly used sequence fixed length generation approaches (setting-1). Likewise, in setting-4 which selects different residues from starting region of protein A and ending region of protein B, ADH-PPI achieves the degree of separability of 98% across 7 different residue thresholds, showing best AU-ROC among all 5 settings. Whereas setting-5 based on start-end region of protein pairs attains the performance of 97% across all 7 residue thresholds, indicating degree of separability comparable to setting-2.</p>
      <p id="p0170">On the other hand, over H.pylori dataset, applying copy padding or sequence truncation approaches at the starting region of protein pairs attain slightly superior degree of separability as compared to approaches based on ending region of protein sequences. However, unlike S.cerevisiae dataset, here, both kind of approaches mark better performance by mapping the protein sequences to minimum length. In setting-2, performance of ADH-PPI declines from 93% to 91% until 30 residues however jumps to 95% until 50 residues before finishing at 93%. Overall, it outperforms traditional copy padding or sequence truncation approaches by 2% in terms of AU-ROC. In setting-3 which merely selects residues from ending region of protein pairs, performance of ADH-PPI fluctuates by the figure of 1%. ADH-PPI attains the degree of separability of 93% with 40 residues, indicating overall better performance than setting-1 but slightly lower performance than setting-2. Like S.cerevisiae dataset, here once again, setting-4 based on starting region of protein A and ending region of protein B achieves best degree of separability among all 5 settings. With the influx of residues, ADH-PPI performance jumps to 96% until 40 residues before slightly fluctuating and ending at 93%. Whereas, performance of setting-5 based on start-end region of protein pairs increases upto 92% until 30 residues and gets flatten afterward across rest of the residue thresholds.</p>
      <p id="p0175">In a nutshell, prime objective of developing artificial intelligence-based predictors is to make best use of raw protein sequences, and extract distinct distribution of amino acids in the sequences in order to discriminate interactive protein sequences from non-interactive protein sequences. However, protein sequences are highly variable in length and deep learning models require fixed length input sequences. Commonly used sequence fixed length generation approaches are copy padding and sequence truncation. In copy padding approach, all sequences are mapped to maximum sequence length by padding certain letter to shorter sequences, whereas in sequence truncation approach, all sequences are mapped to minimum sequence length by eliminating extra amino acids from longer sequences. Distribution of amino acids varies in different sub regions of sequences and the performance of deep learning algorithms primarily relies on the extraction of discriminative distribution of amino acids. Copy padding approach creates unnecessary bias through the repetition of same padding letter which make sequences quite similar to each other; similarly, sequence truncation approach is vulnerable to loose most informative distribution of amino acids. Subsequences-based fixed length generation is more effective as it does not insert any hypothetical letter. Furthermore, it skips constant regions that usually lie in center of the sequences and does not loose informative distribution because it takes both starting and ending regions of the sequences into account. Experimental results reveal that most discriminative distribution of amino acids lies in first 40 amino acids of protein A and last 40 amino acids of protein B, indicating the success of subsequence-based setting for capturing the informative and discriminative essence of protein sequences.</p>
    </sec>
    <sec id="sec2.6">
      <title>Performance impact of CNN layer</title>
      <p id="p0180">To better illustrate the necessity of convolutional (CNN) layer in proposed ADH-PPI predictor, we have performed experimentation on H.pylori dataset under the hood of two different settings. In first setting, we take long short-term memory (LSTM), CNN, and Attention layers, whereas in second setting, we only take LSTM and attention layers. <xref rid="tbl1" ref-type="table">Table 1</xref> illustrates the predictive performance of both settings in terms of five different evaluation measures namely accuracy, precision, recall, F1-score, and MCC.<table-wrap position="float" id="tbl1"><label>Table 1</label><caption><p>Performance analysis of proposed model using LSTM, CNN, and attention layers and only LSTM and attention layers over H.pylori species dataset to quantify the impact of CNN layer</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Evaluation measures</th><th>Proposed model with LSTM, CNN, and attention layers</th><th>Proposed model with only LSTM and attention layers</th><th>Performance difference</th></tr></thead><tbody><tr><td><bold>Accuracy</bold></td><td>0.926</td><td>0.919</td><td>Around 1%</td></tr><tr><td><bold>Precision</bold></td><td>0.928</td><td>0.921</td><td>Around 1%</td></tr><tr><td><bold>Recall</bold></td><td>0.961</td><td>0.945</td><td>Around 2%</td></tr><tr><td><bold>F1-score</bold></td><td>0.944</td><td>0.912</td><td>Around 3%</td></tr><tr><td><bold>MCC</bold></td><td>0.855</td><td>0.848</td><td>Around 1%</td></tr></tbody></table></table-wrap></p>
      <p id="p0185">Among different subsequence-based settings, using 40 residues from starting region of protein A and 40 residues from ending region of protein B, proposed model with LSTM and attention layers achieves the accuracy of 0.919, recall of 0.945, precision of 0.921, F1-score of 0.912, and MCC of 0.848. However, this performance is less than the performance achieved using LSTM, CNN, and attention layers in proposed predictor by the F1-score of 3%, accuracy of 2%, precision, recall, and MCC of 1%. Overall, exclusion of CNN layer slightly drops the predictive performance, and better performance is achieved when LSTM, CNN, and attention layers are used in proposed predictor. This proves the necessity of CNN layer in proposed predictor that essentially captures local dependencies and translational invariance of amino acids present in protein subsequences which complement predictive performance.</p>
    </sec>
    <sec id="sec2.7">
      <title>Performance assessment of ADH-PPI robustness for different order protein sequence pairs</title>
      <p id="p0190">Empirical evaluation reveals that proposed ADH-PPI achieves the highest performance on 2 core benchmark datasets and 4 independent test sets on account of protein sequence pairs generated by combining the subsequence of protein A with subsequence of protein B. Among different subsequence generation settings, setting-4 (<xref rid="fig11" ref-type="graphic">Figure 11</xref>) which focuses on the starting region of protein A and ending region of protein B develops most informative residue distribution-based protein sequence pairs. However, it is important to note that we have randomly chosen one protein as protein A and other protein as protein B. Building on the equal possibility of generating conversely ordered protein sequence pairs, here we validate the idea that regardless of protein sequence order, starting region of one protein and ending region of other protein contains the most informative residue distribution for PPI prediction.</p>
      <p id="p0195">Mainly, experimentation is performed with optimal subsequence generation setting across 2 core datasets and 4 independent test sets by treating one protein subsequence as protein A, other protein subsequence as protein B, and exchanging the the order of protein subsequences. Furthermore, we use same parameters (e.g subsequence window size, model parameters (<xref rid="tbl1" ref-type="table">Table 1</xref>) values described in previous sections <xref rid="sec2.5" ref-type="sec">2.5</xref> for each dataset across both kind of paradigms in order to accurately reveal the robustness of ADH-PPI approach.</p>
      <p id="p0200"><xref rid="fig4" ref-type="fig">Figures 4</xref> and <xref rid="fig5" ref-type="fig">5</xref> illustrate the performance produced by setting-4 using protein sequence pairs generated by treating one protein as protein A and other protein as protein B as well as reversing the order on 2 core benchmark datasets and 4 independent test sets, respectively. As is indicated by the <xref rid="fig4" ref-type="fig">Figures 4</xref> and <xref rid="fig5" ref-type="fig">5</xref>, ADH-PPI achieves almost same performance across all datasets with protein sequence pairs generated using 2 different protein subsequence orders. This indicates that although changing the combination order of protein-subsequences changes the characteristic of protein-sequence pairs, however, proposed ADH-PPI is robust enough to capture most informative distribution of protein sequences important for PPI prediction.<fig id="fig4"><label>Figure 4</label><caption><p>Performance assessment of optimal most informative subsequence generation setting using 2 differently ordered protein sequence pairs over S.cerevisiae and H.pyloir core datasets</p><p>Here P_A represents the Protein A and P_B refers to Protein B, whereas start and end represent the starting and ending region of respective protein.</p></caption><graphic xlink:href="gr4"/></fig><fig id="fig5"><label>Figure 5</label><caption><p>Performance assessment of optimal most informative subsequence generation setting using 2 differently ordered protein sequence pairs over C.elegans, H.sapiens, M.musculus, and E.coli independent test sets after training the model on core S.cerevisiae dataset</p></caption><graphic xlink:href="gr5"/></fig></p>
    </sec>
    <sec id="sec2.8">
      <title>Performance comparison of proposed ADH-PPI predictor with existing PPI predictors using two benchmark core datasets</title>
      <p id="p0205">In order to prove the integrity of proposed ADH-PPI predictor, rich performance comparison with existing PPI predictors is performance using two core datasets in terms of 4 different evaluation metrics.</p>
      <p id="p0210"><xref rid="tbl2" ref-type="table">Table 2</xref> compares the performance of proposed ADH-PPI predictor with 12 machine and deep learning-based predictors over S.cerevisiae dataset. As indicated by the <xref rid="tbl2" ref-type="table">Table 2</xref>, proposed ADH-PPI predictor outperforms auto co-variance and SVM-based PPI prediction methodology (<xref rid="bib20" ref-type="bibr">Guo et al., 2008<italic>a</italic></xref>) by 7%, 5%, and 7%, and KNN-based methodology (<xref rid="bib20" ref-type="bibr">Guo et al., 2008<italic>a</italic></xref>) by 10%, 13%, and 6% in terms of accuracy, recall, and precision, respectively. It outperforms WSRC classifier (<xref rid="bib37" ref-type="bibr">Kong et al., 2020</xref>) by 9%, 4%, and 14% in terms of accuracy, recall, and MCC and ippi-esml (<xref rid="bib34" ref-type="bibr">Jia et al., 2015</xref>) approach by 3%, 5%, 3%, and 4% in terms of accuracy, recall, precision, and MCC, respectively. Multi-scale continuous and discontinuous feature representation and SVM classifier-based approach (<xref rid="bib78" ref-type="bibr">You et al., 2014</xref>) takes the previous best accuracy of 89%–91%, amino acid substitution matrix-based feature representation and RF classifier-based approach (<xref rid="bib77" ref-type="bibr">You et al., 2017</xref>) attains the accuracy of 94%. RF classifier achieves the accuracy of 95% using multivariate mutual information of protein feature representation (<xref rid="bib12" ref-type="bibr">Ding et al., 2016</xref>) and 94% using multiscale local descriptor (MLD)-based feature representation (<xref rid="bib74" ref-type="bibr">You et al., 2015<italic>a</italic></xref>). Proposed ADH-PPI predictor outperforms SVM and random forest-based PPI prediction methodologies by the comparable margin. From existing machine learning-based PPI predictors, GcForest-PPI (<xref rid="bib79" ref-type="bibr">Yu et al., 2021</xref>) achieves top performance in terms of most evaluation metrics. Proposed ADH-PPI predictor surpasses the performance of GcForest-PPI (<xref rid="bib79" ref-type="bibr">Yu et al., 2021</xref>) by 1% in terms of accuracy and recall and equalizes the MCC performance value.<table-wrap position="float" id="tbl2"><label>Table 2</label><caption><p>Performance comparison of proposed ADH-PPI predictor with 12 existing PPI predictors on benchmark <italic>S. cerevisiae</italic> dataset, where results of existing PPI predictors are taken from (Yu et al., 2021) paper</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>ACC (%)</th><th>Recall (%)</th><th>Precision (%)</th><th>MCC</th></tr></thead><tbody><tr><td><bold>ACC+SVM</bold> (<xref rid="bib20" ref-type="bibr">Guo et al., 2008a</xref>)</td><td>0.8933 ± 2.67</td><td>0.8993 ± 3.68</td><td>0.8887 ± 6.16</td><td>N/A</td></tr><tr><td><bold>Code4+KNN</bold> (<xref rid="bib20" ref-type="bibr">Guo et al., 2008a</xref>)</td><td>0.8615 ± 1.17</td><td>0.8103 ± 1.74</td><td>0.9024 ± 1.34</td><td>N/A</td></tr><tr><td><bold>MCD+SVM</bold> (<xref rid="bib78" ref-type="bibr">You et al., 2014</xref>)</td><td>0.9136 ± 0.36</td><td>0.9067 ± 0.69</td><td>0.9194 ± 0.62</td><td>0.8421 ± 0.0059</td></tr><tr><td><bold>MLD+RF</bold> (<xref rid="bib74" ref-type="bibr">You et al., 2015a</xref>)</td><td>0.9472 ± 0.43</td><td>0.9434 ± 0.49</td><td>0.9891 ± 0.33</td><td>0.8599 ± 0.0089</td></tr><tr><td><bold>PR-LPQ+RF</bold> (<xref rid="bib75" ref-type="bibr">You et al., 2015b</xref>)</td><td>0.9392 ± 0.36</td><td>0.9110 ± 0.31</td><td>0.9645 ± 0.45</td><td>0.8856 ± 0.0063</td></tr><tr><td>MIMI + NMBAC+<break/><bold>RF (</bold><xref rid="bib12" ref-type="bibr">Ding et al., 2016</xref><bold>)</bold></td><td>0.9501 ± 0.46</td><td>0.9267 ± 0.50</td><td>0.9716 ± 0.55</td><td>0.9010 ± 0.0092</td></tr><tr><td><bold>LRA+RF</bold> (<xref rid="bib77" ref-type="bibr">You et al., 2017</xref>)</td><td>0.9414 ± 1.8</td><td>0.9122 ± 1.6</td><td>0.9710 ± 2.1</td><td>0.8896 ± 0.026</td></tr><tr><td><bold>DeepPPI</bold> (<xref rid="bib13" ref-type="bibr">Du et al., 2017</xref>)</td><td>0.9443 ± 0.30</td><td>0.9206 ± 0.36</td><td>0.9665 ± 0.59</td><td>0.8897 ± 0.0062</td></tr><tr><td><bold>ippi-esml</bold> (<xref rid="bib34" ref-type="bibr">Jia et al., 2015</xref>)</td><td>0.9515 ± 0.25</td><td>0.9221 ± 0.36</td><td>0.9797 ± 0.60</td><td>0.9045 ± 0.0053</td></tr><tr><td><bold>WSRC</bold> (<xref rid="bib37" ref-type="bibr">Kong et al., 2020</xref>)</td><td>0.8673</td><td>0.8993</td><td>NA</td><td>0.7693</td></tr><tr><td><bold>DeepFE-PPI</bold> (<xref rid="bib73" ref-type="bibr">Yao et al., 2019</xref>)</td><td>0.9478</td><td>0.9299</td><td>0.9645</td><td>0.8962</td></tr><tr><td><bold>GcForest-PPI</bold> (<xref rid="bib79" ref-type="bibr">Yu et al., 2021</xref>)</td><td>0.9544</td><td>0.9272</td><td>0.9805</td><td>0.9102</td></tr><tr><td><bold>ADH-PPI</bold></td><td><bold>0.9573</bold></td><td><bold>0.9394</bold></td><td><bold>0.9575</bold></td><td><bold>0.9144</bold></td></tr></tbody></table></table-wrap></p>
      <p id="p0215">Turning toward existing deep learning-based PPI prediction methodologies, DeepPPI (<xref rid="bib13" ref-type="bibr">Du et al., 2017</xref>) and DeepFE-PPI (<xref rid="bib73" ref-type="bibr">Yao et al., 2019</xref>) achieve almost similar performance on S.cerevisiae dataset in terms of four different evaluation metrics. Proposed ADH-PPI predictor outperforms deep learning-based PPI predictors by 1% in terms of accuracy, recall, and MCC.</p>
      <p id="p0220">Moreover, performance produced by proposed ADH-PPI predictor and ten existing PPI predictors on H.pylori dataset is shown in <xref rid="tbl3" ref-type="table">Table 3</xref>. Analysis of <xref rid="tbl3" ref-type="table">Table 3</xref> reveals that proposed ADH-PPI predictor achieves even more promising figures than existing PPI predictors across all evaluation metrics. Proposed ADH-PPI predictor outshines best performing machine learning-based PPI predictor namely GcForest-PPI (<xref rid="bib34" ref-type="bibr">Jia et al., 2015</xref>) by 7%, 7%, 4%, and 4% in terms of recall, MCC, precision, and accuracy, respectively. It outperforms another top performing MIMI and Random forest-based PPI predictor (<xref rid="bib12" ref-type="bibr">Ding et al., 2016</xref>) by Matthews correlation coefficient of 13%, recall of 10%, precision of 5%, and accuracy of 6%. In comparison to deep learning-based PPI predictors, proposed ADH-PPI predictor outperforms DeepPPI (<xref rid="bib13" ref-type="bibr">Du et al., 2017</xref>) predictor by 7%, 7%, 9%, and 12% in terms of accuracy, recall, precision, and MCC.<table-wrap position="float" id="tbl3"><label>Table 3</label><caption><p>Performance comparison of proposed ADH-PPI predictor with 10 existing predictors on benchmark <italic>H. pylori</italic> dataset, where results of existing PPI predictors are taken from (Yu et al., 2021) paper</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>ACC (%)</th><th>Recall (%)</th><th>Precision (%)</th><th>MCC</th></tr></thead><tbody><tr><td><bold>SVM [6]</bold></td><td>0.8340</td><td>0.7990</td><td>0.8570</td><td>N/A</td></tr><tr><td><bold>WSR</bold> (<xref rid="bib50" ref-type="bibr">Nanni, 2005</xref>)</td><td>0.8370</td><td>0.7900</td><td>0.8700</td><td>N/A</td></tr><tr><td>Ensemble of<break/><bold>HKNN (</bold><xref rid="bib51" ref-type="bibr">Nanni and Lumini, 2006</xref><bold>)</bold></td><td>0.8660</td><td>0.8670</td><td>0.8500</td><td>N/A</td></tr><tr><td><bold>DCT+WSRC</bold> (<xref rid="bib27" ref-type="bibr">Huang et al., 2016</xref>)</td><td>0.8674</td><td>0.8643</td><td>0.8701</td><td>0.7699</td></tr><tr><td><bold>MCD+SVM</bold> (<xref rid="bib78" ref-type="bibr">You et al., 2014</xref>)</td><td>0.8491</td><td>0.8324</td><td>0.8612</td><td>0.7440</td></tr><tr><td>MIMI+<break/><bold>NMBAC+RF (</bold><xref rid="bib12" ref-type="bibr">Ding et al., 2016</xref><bold>)</bold></td><td>0.8759</td><td>0.8681</td><td>0.8823</td><td>0.7524</td></tr><tr><td><bold>DeepPPI</bold> (<xref rid="bib13" ref-type="bibr">Du et al., 2017</xref>)</td><td>0.8623</td><td>0.8944</td><td>0.8432</td><td>0.7263</td></tr><tr><td><bold>ippi-esml</bold> (<xref rid="bib34" ref-type="bibr">Jia et al., 2015</xref>)</td><td>0.9047 ± 0.84</td><td>0.9115 ± 1.42</td><td>0.8999 ± 2.06</td><td>0.8100 ± 0.0163</td></tr><tr><td><bold>WSRC</bold> (<xref rid="bib37" ref-type="bibr">Kong et al., 2020</xref>)</td><td>0.7870</td><td>0.7321</td><td>NA</td><td>0.7693</td></tr><tr><td><bold>GcForest-PPI</bold> (<xref rid="bib79" ref-type="bibr">Yu et al., 2021</xref>)</td><td>0.8926</td><td>0.8971</td><td>0.8895</td><td>0.7857</td></tr><tr><td><bold>ADH-PPI</bold></td><td><bold>0.9263</bold></td><td><bold>0.9609</bold></td><td><bold>0.9284</bold></td><td><bold>0.8547</bold></td></tr></tbody></table></table-wrap></p>
      <p id="p0225">To summarize, proposed ADH-PPI predictor outperforms both machine and deep learning-based PPI prediction methodologies by decent margin for S.cerevisiae and by significant margin for H.pylori dataset. It is important to mention that (<xref rid="bib37" ref-type="bibr">Kong et al., 2020</xref>) proposed FCTP-WSRC predictor results are not comparable to proposed ADH-PPI predictor. Generally, dimensionality reduction approaches such as principal components analysis (PCA) is applied on training data to learn the reduced matrix and the transformation is applied on testing data where test data is projected to reduce feature space. However, (<xref rid="bib37" ref-type="bibr">Kong et al., 2020</xref>) applied PCA on training and testing data separately which introduces biaseness. In our experimentation, to find the valid performance figures of FCTP-WSRC predictor (<xref rid="bib37" ref-type="bibr">Kong et al., 2020</xref>), we have applied the PCA in correct manner and reported the valid results on 2 core benchmark datasets (<xref rid="tbl2" ref-type="table">Tables 2</xref> and <xref rid="tbl3" ref-type="table">3</xref>) and independent test sets (<xref rid="fig6" ref-type="fig">Figure 6</xref>).<fig id="fig6"><label>Figure 6</label><caption><p>Accuracy comparison of ADH-PPI and recent PPI predictors on four independent test sets</p></caption><graphic xlink:href="gr6"/></fig></p>
    </sec>
    <sec id="sec2.9">
      <title>Performance comparison of ADH-PPI with existing PPI predictors using four independent test sets</title>
      <p id="p0230">To further prove the effectiveness of proposed ADH-PPI predictor, comparison between 6 existing PPI predictors and proposed ADH-PPI predictor is performed. Following experimentation criteria of existing predictors, we train the proposed predictor over core S.cerevisiae dataset and perform evaluation over 4 different independent test sets belonging to C.elegans, E.coli, H.sapiens, and M.musculus species (<xref rid="bib79" ref-type="bibr">Yu et al., 2021</xref>; <xref rid="bib28" ref-type="bibr">Huang et al., 2015a</xref>). <xref rid="fig6" ref-type="fig">Figure 6</xref> compares the accuracy of proposed ADH-PPI predictor with existing predictors. As shown by the <xref rid="fig6" ref-type="fig">Figure 6</xref>, proposed ADH-PPI predictor achieves best performance across all four independent test sets which is higher than the peak performance achieved by deep learning-based PPI predictor DeepPPI (<xref rid="bib13" ref-type="bibr">Du et al., 2017</xref>) by 4% for C.elegans, 6% for E.coli, 5% for H.sapiens, and 9% for M.musculus species test sets.</p>
      <p id="p0235">Furthermore, proposed ADH-PPI predictor outperforms machine learning-based state-of-the-art PPI predictor namely GCForest-PPI (<xref rid="bib79" ref-type="bibr">Yu et al., 2021</xref>) by 3%, 2%, 1%, and 1%, achieving more than 98% performance over all four different species independent test sets.</p>
      <p id="p0240">In a nutshell, over two core datasets and four independent test sets, among all existing PPIs predictors, machine learning-based PPI predictors perform better than deep learning-based predictors. Proposed ADH-PPI predictor outshines state-of-the-art PPI predictor across all 6 datasets of different species including humans, Drosophila, Yeast, Bacterium, <italic>Caenorhabditis elegans</italic>, and <italic>Escherichia coli</italic> in terms of most evaluation metrics. The paradigm of considering both k-mer distribution as well as amino acid distributions within k-mer best characterizes the protein sequences. Furthermore, the utilization of LSTM, CNN, and attention ensures the extraction of comprehensive discriminative features along with long-range dependencies which are essential to accurately predict PPIs across different species. The best utilization of multiple strategies not only enhances the predictive power of proposed ADH-PPI approach but also makes the decisions of proposed ADH-PPI predictor interpretable. Therefore, we believe ADH-PPI will prove a great computational asset for biological researchers and practitioners which can be used to find protein-protein interactions, protein non-coding RNA interactions, or even interaction between different biomolecules.</p>
    </sec>
    <sec id="sec2.10">
      <title>Explainability of the proposed ADH-PPI model</title>
      <p id="p0245">With an aim to overcome a very common black box modeling issue of deep neural networks by decoding the importance of individual amino acids and k-mers, we analyze the attention weights associated with different k-mers to illustrate which k-mers contribute the most in making accurate PPI predictions.</p>
      <p id="p0250">To more precisely demonstrate the explainability of the proposed ADH-PPI approach, we arbitrarily take two protein sequence pairs from test sets of benchmark S.cerevisiae and H.pylori datasets. Following the working paradigm of proposed ADH-PPI predictor, we generate 5-mers of both test protein sequence pairs and feed both test protein sequence pairs 5-mers along with pre-trained embeddings to two different classifiers trained on S.cerevisiae and H.pylori training sets. These classifiers decide whether given protein sequence pairs are interactive or non-interactive. Classifiers make decisions based on the attention weights associated with 5-mers. We extract attention weights and feed these attention weighs to decision explainable module which performs reverse engineering to map these attention weights to different 5-mers. To illustrate better, decision explainable module categorizes different 5-mers into five different groups based on different thresholds applied at attention weights ranging from 0 to 1. Each group is represented with a unique shade of red color, the higher the intensity of red color is the higher the attention weight is for particular k-mer, indicating darkest red color 5-mers and their inherent amino acids contribute the most in making accurate PPI predictions and lightest red color 5-mers and their inherent amino acids make least contributions in making accurate PPI predictions. The attention weights range of five different groups of 5-mers is shown on the x axis of the bar graph (<xref rid="fig7" ref-type="fig">Figure 7</xref>) whereas y axis of the bar graph shows the count of 5-mers in each group.<fig id="fig7"><label>Figure 7</label><caption><p>Most informative and least informative amino acid and K-mers patterns identified by attention layer of proposed ADH-PPI predictor in two test protein sequences belonging to benchmark S.cerevisiae and H.pylori Datasets</p></caption><graphic xlink:href="gr7"/></fig></p>
      <p id="p0255">It is evident in the <xref rid="fig7" ref-type="fig">Figure 7</xref>, for S.cerevisiae dataset, only two 5-mers GGKAG and SAAKA fall in first group which has best range of attention weights 0.90–1.0. Two 5-mers fall in second group which has second best range of attention weights 0.70–0.89. Similarly, one 5-mer falls in third group and three k-mers fall in fourth group which have attention weight ranges of 0.50–0.69 and 0.20–0.49, respectively. Among all groups, fifth group has most eight 5-mers, attention weights of which falls in range of 0.1–0.20. Furthermore, it can be seen that starting 5-mers distribution has the top attention weights where amino acids G and A are most frequent, which contribute the most in making accurate PPI predictions on S.cerevisiae dataset.</p>
      <p id="p0260">Unlike S.cerevisiae dataset, on H.pylori dataset, eighteen 5-mers fall in fifth group, fourteen 5-mers in second group, and four 5-mers in third group. Once again, very few 5-mers fall in first and second group. More specifically, three 5-mers LIFYYF, IFYFL, LDFKG, individual amino acids F and L of central regions of subsequence contribute the most in making accurate PPI predictions on H.pylori dataset.</p>
      <p id="p0265">These attention weight distribution patterns at the k-mer level and amino acid level are quite consistent across most sequences. Furthermore, this is quite consistent with our unique hypothesis of predicting PPIs using only most discriminative subsequences. The starting or central k-mers distribution within subsequences gets the higher attention weight and serves as most influential regions, and the surrounding k-mers distribution gets lower attention weights and exists as supportive and auxiliary information regions. In a nutshell, we validate the ADH-PPI suitability to discover useful patterns in protein sequences, their dependencies, and explainable associations for PPI prediction.</p>
    </sec>
    <sec id="sec2.11">
      <title>An interactive and user-friendly ADH-PPI web server</title>
      <p id="p0270">We have developed a user-friendly web server for ADH-PPI approach which can be accessed at <ext-link ext-link-type="uri" xlink:href="https://sds_genetic_analysis.opendfki.de/PPI/" id="intref0035">https://sds_genetic_analysis.opendfki.de/PPI/</ext-link>. Proteomics researchers and practitioners can leverage this web server to determine interactions between proteins solely using raw sequences related to human and mouse species. Researchers can also use this web server to validate experimentally identified PPIs using raw sequences, train and optimize the model from scratch for new species, and perform inference on new sequences belonging to existing or new species.</p>
    </sec>
    <sec id="sec2.12">
      <title>Conclusion</title>
      <p id="p0275">This paper can be considered a huge milestone toward the accurate prediction of PPIs for a variety of species solely using raw sequences. First, unlike previous methods, it captures comprehensive amino acids order, occurrence, and contextual information by generating k-mer of protein sequences, distributed representations of which are computed as the sum of their embeddings and the embeddings of their inherent amino acid sub-mers using FastText (<xref rid="bib10" ref-type="bibr">Bojanowski et al., 2017</xref>) approach. Second, instead of feeding entire protein sequences to deep learning models, it explores the discriminative aptitude of multifarious regions of protein sequences to obtain highly informative amino acid distribution-based subsequences. Third, it develops an attention-based deep hybrid neural network which makes best use of heterogeneous layers (LSTM, CNN, and Attention) to make accurate and interpretable PPI predictions. A stringent benchmarking performance comparison of ADH-PPI with existing computational predictors proves that ADH-PPI outperforms existing machine and deep learning-based PPI predictors by decent margin. A compelling future line of current would be to assess the performance potential of ADH-PPI approach for interaction prediction tasks related to other biomolecules.</p>
    </sec>
    <sec id="sec2.13">
      <title>Limitations of the study</title>
      <p id="p0280">The limitation of current work is that it only identifies key k-mers which contributes the most in making accurate PPI prediction. However, to more comprehensively explain the decisions of proposed ADH-PPI approach, identifying which k-mer distributions map to which hidden features as well as which hidden unit gets activated at what point in time and input scenarios, would further fine-grained the explainability and interpretability of proposed ADH-PPI predictor for biomedical researchers and practitioners.</p>
    </sec>
  </sec>
  <sec id="sec4">
    <title>STAR★Methods</title>
    <sec id="sec4.1">
      <title>Key resources table</title>
      <p id="p0290">
        <table-wrap position="float" id="undtbl1">
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th>REAGENT or RESOURCE</th>
                <th>SOURCE</th>
                <th>IDENTIFIER</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td colspan="3">
                  <bold>Desposited data</bold>
                </td>
              </tr>
              <tr>
                <td colspan="3">
                  <hr/>
                </td>
              </tr>
              <tr>
                <td>S.cerevisiae Dataset</td>
                <td>(<xref rid="bib21" ref-type="bibr">Guo et al., 2008<italic>b</italic></xref>)</td>
                <td>Database of Interacting Proteins (DIP): RRID:<ext-link ext-link-type="uri" xlink:href="rridsoftware:SCR_003167" id="interref135">SCR_003167</ext-link>; <ext-link ext-link-type="uri" xlink:href="https://dip.doe-mbi.ucla.edu/dip/Main.cgi" id="intref0040">https://dip.doe-mbi.ucla.edu/dip/Main.cgi</ext-link></td>
              </tr>
              <tr>
                <td>H.pylori Dataset</td>
                <td>(<xref rid="bib46" ref-type="bibr">Martin et al., 2005</xref>)</td>
                <td>Database of Interacting Proteins (DIP): RRID:<ext-link ext-link-type="uri" xlink:href="rridsoftware:SCR_003167" id="interref140">SCR_003167</ext-link>; <ext-link ext-link-type="uri" xlink:href="https://dip.doe-mbi.ucla.edu/dip/Main.cgi" id="intref0045">https://dip.doe-mbi.ucla.edu/dip/Main.cgi</ext-link></td>
              </tr>
              <tr>
                <td>E.coli Test Set</td>
                <td>(<xref rid="bib82" ref-type="bibr">Zhou et al., 2011</xref>)</td>
                <td>Database of Interacting Proteins (DIP): RRID:<ext-link ext-link-type="uri" xlink:href="rridsoftware:SCR_003167" id="interref145">SCR_003167</ext-link>; <ext-link ext-link-type="uri" xlink:href="https://dip.doe-mbi.ucla.edu/dip/Main.cgi" id="intref0050">https://dip.doe-mbi.ucla.edu/dip/Main.cgi</ext-link></td>
              </tr>
              <tr>
                <td>C.elegans Test Setdata</td>
                <td>(<xref rid="bib82" ref-type="bibr">Zhou et al., 2011</xref>)</td>
                <td>Database of Interacting Proteins (DIP): RRID:<ext-link ext-link-type="uri" xlink:href="rridsoftware:SCR_003167" id="interref150">SCR_003167</ext-link>; <ext-link ext-link-type="uri" xlink:href="https://dip.doe-mbi.ucla.edu/dip/Main.cgi" id="intref0055">https://dip.doe-mbi.ucla.edu/dip/Main.cgi</ext-link></td>
              </tr>
              <tr>
                <td>Homo Sapiens Test Set</td>
                <td>(<xref rid="bib82" ref-type="bibr">Zhou et al., 2011</xref>)</td>
                <td>Database of Interacting Proteins (DIP): RRID:<ext-link ext-link-type="uri" xlink:href="rridsoftware:SCR_003167" id="interref155">SCR_003167</ext-link>; <ext-link ext-link-type="uri" xlink:href="https://dip.doe-mbi.ucla.edu/dip/Main.cgi" id="intref0060">https://dip.doe-mbi.ucla.edu/dip/Main.cgi</ext-link></td>
              </tr>
              <tr>
                <td>Mus Musculus Test Set</td>
                <td>(<xref rid="bib82" ref-type="bibr">Zhou et al., 2011</xref>)</td>
                <td>Database of Interacting Proteins (DIP): RRID:<ext-link ext-link-type="uri" xlink:href="rridsoftware:SCR_003167" id="interref160">SCR_003167</ext-link>; <ext-link ext-link-type="uri" xlink:href="https://dip.doe-mbi.ucla.edu/dip/Main.cgi" id="intref0065">https://dip.doe-mbi.ucla.edu/dip/Main.cgi</ext-link></td>
              </tr>
              <tr>
                <td colspan="3">
                  <hr/>
                </td>
              </tr>
              <tr>
                <td colspan="3">
                  <bold>Software and algorithms</bold>
                </td>
              </tr>
              <tr>
                <td colspan="3">
                  <hr/>
                </td>
              </tr>
              <tr>
                <td>Sub-Sequence Generation<break/>Paradigm</td>
                <td>This Study</td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="https://sds_genetic_analysis.opendfki.de/PPI/" id="intref0070">https://sds_genetic_analysis.opendfki.de/PPI/</ext-link>
                </td>
              </tr>
              <tr>
                <td>Protein Sequence<break/>Embeddings (FastText)</td>
                <td>Open Source</td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="https://fasttext.cc/" id="intref0075">https://fasttext.cc/</ext-link>
                </td>
              </tr>
              <tr>
                <td>Python</td>
                <td>Open Source</td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="https://www.python.org/" id="intref0080">https://www.python.org/</ext-link>
                </td>
              </tr>
              <tr>
                <td>Pytorch</td>
                <td>Open Source</td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="https://pytorch.org/" id="intref0085">https://pytorch.org/</ext-link>
                </td>
              </tr>
              <tr>
                <td>Grid Search</td>
                <td>(<xref rid="bib9" ref-type="bibr">Bergstra et al., 2011</xref>)</td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="http://tinyurl.com/54phrd33" id="intref0090">http://tinyurl.com/54phrd33</ext-link>
                </td>
              </tr>
              <tr>
                <td>CD-HIT</td>
                <td>(<xref rid="bib15" ref-type="bibr">Fu et al., 2012</xref>)</td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="http://weizhong-lab.ucsd.edu/cd-hit/" id="intref0095">http://weizhong-lab.ucsd.edu/cd-hit/</ext-link>
                </td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </p>
    </sec>
    <sec id="sec4.2">
      <title>Resource availability</title>
      <sec id="sec4.2.1">
        <title>Lead contact</title>
        <p id="p0295">For more information as well as requests for the materials and code shall be directed to and will be fulfilled by lead contact, Muhammad Nabeel Asim (<ext-link ext-link-type="uri" xlink:href="mailto:Muhammad_Nabeel.Asim@dfki.de" id="intref0100">Muhammad_Nabeel.Asim@dfki.de</ext-link>).</p>
      </sec>
      <sec id="sec4.2.2">
        <title>Materials availability</title>
        <p id="p0300">In this study, no new materials are generated.</p>
      </sec>
    </sec>
    <sec id="sec4.3">
      <title>Method details</title>
      <sec id="sec4.3.1">
        <title>Data sources benchmark protein-protein interaction prediction datasets</title>
        <p id="p0305">In order to prove the integrity of proposed ADH-PPI approach and to perform a fair comparison with existing PPIs prediction approaches, we evaluate ADH-PPI performance over PPIs datasets of 6 different species including humans, Drosophila, Yeast, Bacterium, <italic>Caenorhabditis elegans</italic>, and <italic>Escherichia coli</italic>.</p>
        <p id="p0310">From Yeast specie, performance of ADH-PPI is evaluated on a well known public benchmark dataset namely <italic>Saccharomyces cerevisiae</italic> (S.cerevisiae), which is extensively utilized by several researchers for PPI prediction (<xref rid="bib79" ref-type="bibr">Yu et al., 2021</xref>; <xref rid="bib35" ref-type="bibr">Jiang et al., 2020</xref>; <xref rid="bib69" ref-type="bibr">Wang et al., 2020a</xref>). PPIs of <italic>Saccharomyces cerevisiae</italic> (S.cerevisiae) were first extracted by (<xref rid="bib21" ref-type="bibr">Guo et al., 2008b</xref>) from Database of Interacting Proteins (DIP): RRID:<ext-link ext-link-type="uri" xlink:href="rridsoftware:SCR_003167" id="interref165">SCR_003167</ext-link>; <ext-link ext-link-type="uri" xlink:href="https://dip.doe-mbi.ucla.edu/dip/Main.cgi" id="interref0170">https://dip.doe-mbi.ucla.edu/dip/Main.cgi</ext-link> (<xref rid="bib72" ref-type="bibr">Xenarios et al., 2002</xref>). Authors eliminated those protein pairs where any one of the protein was comprised of less than 50 residues and obtained a dataset of 5,943 protein pairs with positive interactions. To eliminate redundancy, researchers utilized a renowned program CD-HIT (<xref rid="bib15" ref-type="bibr">Fu et al., 2012</xref>). From 11,188 PPIs, a total of 5,594 PPIs were retained considering that they had less than 40% pairwise sequence similarity with each other. An equal number of negative PPIs were generated using 3 different approaches. In first approach, non-interacting protein pairs were generated by random pairing of proteins which were not present in the positive dataset. In second approach, negative dataset was generated by combining proteins having similar sub-cellular localization patterns extracted from Swiss-Prot database (<xref rid="bib7" ref-type="bibr">Bairoch and Apweiler, 1996</xref>). In third approach, negative dataset was generated using data augmentation approach. Another widely used PPI prediction dataset (<xref rid="bib79" ref-type="bibr">Yu et al., 2021</xref>; <xref rid="bib35" ref-type="bibr">Jiang et al., 2020</xref>; <xref rid="bib69" ref-type="bibr">Wang et al., 2020a</xref>) <italic>Helicobacter pylori</italic> belongs to Bacterium specie, which was compiled by (<xref rid="bib46" ref-type="bibr">Martin et al., 2005</xref>). It contained 2,916 protein pairs out of which 1,458 protein pairs were positive and 1,458 protein pairs were negative. From a collection of protein pairs which were not explicitly declared as interactive, a bunch of protein pairs were selected as non-interacting proteins. Statistics of both core datasets S.cerevisiae and H.pylori are described in Figure.<fig id="fig8"><caption><p>Statistics of 2 core protein-protein interaction prediction datasets</p></caption><graphic xlink:href="gr8"/></fig></p>
        <p id="p0315">In order to perform a fair performance comparison with existing PPI predictors and to further prove the versatility of proposed methodology ADH-PPI, we also evaluate ADH-PPI over 4 independent test sets developed by (<xref rid="bib82" ref-type="bibr">Zhou et al., 2011</xref>). These datasets have been extensively used in literature (<xref rid="bib79" ref-type="bibr">Yu et al., 2021</xref>; <xref rid="bib29" ref-type="bibr">Huang et al., 2015b</xref>; <xref rid="bib22" ref-type="bibr">Hashemifar et al., 2018</xref>). As the procedure used to develop 4 different independent test sets has been described in existing studies (<xref rid="bib82" ref-type="bibr">Zhou et al., 2011</xref>; <xref rid="bib79" ref-type="bibr">Yu et al., 2021</xref>; <xref rid="bib28" ref-type="bibr">Huang et al., 2015a</xref>; <xref rid="bib22" ref-type="bibr">Hashemifar et al., 2018</xref>), here we only shed light on the statistics of 4 independent test sets. E.coli consists of 6,954 protein pairs with positive interactions, C.elegans contains 4,103, <italic>Homo sapiens</italic> (<italic>H. sapiens</italic>) consists of 1,412, and <italic>Mus musculus</italic> (<italic>M. musculus</italic>) is composed of 313 protein pairs with positive interactions.</p>
      </sec>
      <sec id="sec4.3.2">
        <title>Model architecture</title>
        <sec id="sec4.3.2.1">
          <title>ADH-PPI: An Attention based Deep Hybrid Model for Protein Protein Interaction Prediction</title>
          <p id="p0320">The working of the proposed ADH-PPI predictor can be categorized into three different modules. First module generates effective statistical representations of k-mers present in protein sequences by applying transfer learning in an unsupervised manner. Second module generates fixed length protein sequences using traditional and robust sequence fixed length generation methods. Using fixed length protein sequences and k-mer embeddings, third module trains a robust attention based deep hybrid neural network for PPI prediction. A brief description of each module is provided in the following sub-sections.</p>
        </sec>
        <sec id="sec4.3.2.2">
          <title>K-mer embedding generation using unsupervised transfer learning</title>
          <p id="p0325">To generate k-mer embeddings, first step is to divide the protein sequences into k-mers. Overlapping k-mers are generated by rotating a fixed-size window over a protein sequence where the stride size is always less than the size of window. On the other hand, non-overlapping k-mers are generated by rotating a window with a stride size equal to window size. Figure illustrates the process of generating overlapping and non-overlapping k-mers of a hypothetical protein sequence.<fig id="fig9"><caption><p>Overlapping and non-overlapping K-mers generation for protein sequences</p></caption><graphic xlink:href="gr9"/></fig></p>
          <p id="p0330">Protein sequences are made up of 20 distinct amino acids. Hence, in both overlapping or non-overlapping k-mer generation, the unique vocabulary size is equal to <inline-formula><mml:math id="M1" altimg="si1.gif"><mml:mrow><mml:msup><mml:mn>20</mml:mn><mml:mi>k</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. The value of <italic>k</italic> determines the size of vocabulary which impacts model complexity, memory cost, run time cost, as well as up to what extent amino acid contextual information is taken into account, hence the choice of <italic>k</italic> is very crucial. Following the work of (<xref rid="bib41" ref-type="bibr">Le et al., 2019</xref>) and (<xref rid="bib4" ref-type="bibr">Asim et al., 2020a</xref>), we generate different overlapping and non-overlapping k-mers by varying the window size from 2-to-7 and stride size from 1-to-7.</p>
          <p id="p0335">For different sizes overlapping and non-overlapping k-mers, we generate k-mers embeddings of different dimensions using FastText embedding generation model, working of which is graphically illustrated in Figure.<fig id="fig10"><caption><p>Workflow of unsupervised transfer learning applied using 6 datasets of distinct species to learn distributed representation of higher order sequence residues</p></caption><graphic xlink:href="gr10"/></fig></p>
          <p id="p0340">With an aim to capture comprehensive information of amino acids distributions, we take two benchmark S.cerevisiae, H.pylori datasets and four independent test sets in order to most effectively train the FastText model over large dataset of 26,886 protein sequences. For all 26,886 protein sequences, we generate overlapping and non-overlapping k-mers by varying the window size from 2-to-7 and stride size from 1-to-7. This produces number of different k-mers=6 <inline-formula><mml:math id="M2" altimg="si2.gif"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo></mml:mrow></mml:math></inline-formula> maximum possible different stride size =6 equal to 36 different versions of protein sequences corpus based on different overlapping k-mers and 6 versions of protein sequences corpus based on different non-overlapping k-mers. For each version of protein sequences corpus, we train the FastText model to generate k-mer embeddings of different dimensions <italic>d</italic> ranging from 100, 120, 240, to 300. For example, by considering non-overlapping 3-mers, 26,886 protein sequences are divided in 3-mers which generates a vocabulary of <inline-formula><mml:math id="M3" altimg="si3.gif"><mml:mrow><mml:msup><mml:mn>20</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> unique 3-mers and FastText generates <italic>d</italic>-dimensional statistical vectors for each 3-mer. FastText embedding generation model is an extension of Skipgram model (<xref rid="bib49" ref-type="bibr">Mikolov et al., 2013</xref>). Given a training k-mer sequence <inline-formula><mml:math id="M4" altimg="si4.gif"><mml:mrow><mml:mi>k</mml:mi><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>.</mml:mo><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>, objective function of Skipgram model can be defined as follows:<disp-formula id="fd1"><label>(Equation 1)</label><mml:math id="M5" altimg="si5.gif"><mml:mrow><mml:mi>J</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:munderover><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo linebreak="badbreak">∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>Where <inline-formula><mml:math id="M6" altimg="si6.gif"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the collection of surrounding k-mers of current k-mer <inline-formula><mml:math id="M7" altimg="si7.gif"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, given current k-mer <inline-formula><mml:math id="M8" altimg="si7.gif"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M9" altimg="si8.gif"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the probability of observing its surrounding k-mer <inline-formula><mml:math id="M10" altimg="si9.gif"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.<disp-formula id="fd2"><label>(Equation 2)</label><mml:math id="M11" altimg="si10.gif"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mtext>,</mml:mtext><mml:msub><mml:mi>k</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:msubsup><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>W</mml:mi></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mtext>,</mml:mtext><mml:msub><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
          <p id="p0345">Here <inline-formula><mml:math id="M12" altimg="si11.gif"><mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mtext>,</mml:mtext><mml:msub><mml:mi>k</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the scoring function. Skipgram model considers the scoring function as scalar product <inline-formula><mml:math id="M13" altimg="si12.gif"><mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mtext>,</mml:mtext><mml:msub><mml:mi>k</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M14" altimg="si13.gif"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M15" altimg="si14.gif"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represent the vectors of two k-mers <inline-formula><mml:math id="M16" altimg="si7.gif"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M17" altimg="si9.gif"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> respectively. However, SkipGram can only generate a distinct vector for each k-mer without exploiting their sub-kmer information. To overcome this problem, FastText represents a k-mer as a bag of sub-kmers. For instance, k-mer “HGDTP” will be represented by sub-kmers such as <inline-formula><mml:math id="M18" altimg="si15.gif"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">&lt;</mml:mo></mml:mrow></mml:math></inline-formula>#HGD, HGDT, GDTP, DTP# <inline-formula><mml:math id="M19" altimg="si16.gif"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">&gt;</mml:mo></mml:mrow></mml:math></inline-formula> and k-mer itself <inline-formula><mml:math id="M20" altimg="si15.gif"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">&lt;</mml:mo></mml:mrow></mml:math></inline-formula> HGDTP<inline-formula><mml:math id="M21" altimg="si16.gif"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">&gt;</mml:mo></mml:mrow></mml:math></inline-formula>. Unlike Skigpgram model, FastText defines the scoring function <inline-formula><mml:math id="M22" altimg="si11.gif"><mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mtext>,</mml:mtext><mml:msub><mml:mi>k</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as the <inline-formula><mml:math id="M23" altimg="si17.gif"><mml:mrow><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>.</mml:mo><mml:mi>G</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>z</mml:mi><mml:mi>g</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>v</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:munder></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="M24" altimg="si18.gif"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>.</mml:mo><mml:mi>G</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the sub-kmers collection of <inline-formula><mml:math id="M25" altimg="si7.gif"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M26" altimg="si19.gif"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the vector of sub-kmer, and <inline-formula><mml:math id="M27" altimg="si20.gif"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the vector of k-mer <inline-formula><mml:math id="M28" altimg="si9.gif"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. In this manner, FastText learns the embeddings of sub-kmers. Using sub-kmers embeddings, a k-mer embedding is learned as the sum of distributed representations of its sub-kmers. Major advantage of FastText embedding generation model is that it takes k-mer distributions as well as distributions of amino acids within k-mers into account to generate effective distributed representation of k-mers. Another advantage is that it shares the distributed representation of sub-kmers across all the k-mers which is extremely useful to generate optimal embeddings for less frequent k-mers. FastText embedding generation model is trained with an objective to maximize the probability of target k-mer over all k-mers present in the vocabulary using a softmax layer. Embedding matrix along with output layer parameters are learned by back propagating the error using stochastic gradient descent and negative sampling approach. Using FastText, we generate effective <italic>d</italic>-dimensional vectors for k-mers where the value of <italic>d</italic> is varied from 100, 120, 240, to 300.</p>
        </sec>
      </sec>
      <sec id="sec4.3.3">
        <title>Fixed length generation of Protein sequences</title>
        <p id="p0350">Exploratory analysis of 2 core PPI datasets (Figure) indicates that minimum sequence length for both S.cerevisiae and H.pylori datasets is 10 residues, average protein sequence length for S.cerevisiae dataset is around 1100 residues and for H.pylori dataset, average sequence length is around 734 residues. It is evident that protein sequences have high length variability. Considering machine learning approaches require fixed length protein sequences, existing PPI prediction approaches transform variable length protein sequences into fixed length sequences using traditional copy padding or sequence truncation approaches (<xref rid="bib79" ref-type="bibr">Yu et al., 2021</xref>; <xref rid="bib35" ref-type="bibr">Jiang et al., 2020</xref>; <xref rid="bib69" ref-type="bibr">Wang et al., 2020a</xref>). We perform experimentation with 6 different variations of copy padding and sequence truncation approaches in order to quantify their efficacy for PPI prediction. Furthermore, we present a unique way to generate fixed length protein sequences by finding and retaining only most informative amino acids distributions. This section briefly summarizes five different settings to generate fixed length sequences.</p>
        <p id="p0355">In <inline-formula><mml:math id="M29" altimg="si21.gif"><mml:mrow><mml:mn>1</mml:mn><mml:mtext>st</mml:mtext></mml:mrow></mml:math></inline-formula> setting, performance of 6 traditional copy padding and sequence truncation is evaluated. In copy padding approach, first maximum length of sequence is computed by comparing corpus sequences. Then, all the sequences having length less than maximum length are extended to make them equal to maximum length by adding certain constant. Sequence truncation is another way to make fixed length sequences where minimum sequence length is computed by comparing corpus sequences. Residues from all those sequences whose length is larger than minimum length are truncated to make them equal to minimum sequence length. Another trend is to utilise both copy padding and truncation approaches where average length is computed by comparing corpus sequences. Certain constant is added in those sequences which are shorter than the average length, whereas, sequences that are larger than the average length are truncated.</p>
        <p id="p0360">In copy padding trick, it is an important question whether the start of the sequences is an ideal location for the addition of constant or the end of the sequences. Likewise, in the sequence truncation approach, it is questionable whether extra residues need to be truncated from start of the sequences or end of the sequences. For copy padding trick, we first add constant at the start of sequences, and in another variation, we add constant at the end of the sequence to find out which approach is more appropriate. Similarly, for the sequence truncation approach, we truncate sequences from the start of corpus sequences and from the end of sequences in other variation. In hybrid sequence fixed length generation paradigm based on average length, we also extend or truncate corpus sequences from the start of sequences or end of the sequences. A graphical representation of all 6 strategies is presented in Figure under the the hood of setting-1.<fig id="fig11"><caption><p>A variety of settings based on traditional copy padding or sequence truncation, and proposed bag of most informative amino acids distribution-based tricks used to generate fixed length sequences</p></caption><graphic xlink:href="gr11"/></fig></p>
        <p id="p0365">Considering the vulnerability of traditional copy padding approach to create unnecessary bias through the addition of too many constants and sequence truncation to loose important k-mer distribution while handling flexible protein sequences. Here we propose a unique idea to optimize fixed length sequence generation process where fixed length sequences are generated using only the few residues from different regions of protein sequences which contains the most informative distribution of amino acids for the task of PPI prediction. More specifically, in Figure, under the hood of <inline-formula><mml:math id="M30" altimg="si22.gif"><mml:mrow><mml:mn>2</mml:mn><mml:mtext>nd</mml:mtext></mml:mrow></mml:math></inline-formula> setting, ADH-PPI selects X residues solely from the starting region of one protein A and Y residues solely from starting region of Protein B. In <inline-formula><mml:math id="M31" altimg="si23.gif"><mml:mrow><mml:mn>3</mml:mn><mml:mtext>rd</mml:mtext></mml:mrow></mml:math></inline-formula> setting, performance of X residues taken only from the ending region of protein A and Y residues taken merely from the ending region of protein B is evaluated. Whereas, in setting <inline-formula><mml:math id="M32" altimg="si24.gif"><mml:mrow><mml:mn>4</mml:mn><mml:mtext>th</mml:mtext></mml:mrow></mml:math></inline-formula>, X residues of protein A taken from the starting region of protein sequence are combined with Y residue of protein B taken from the ending region of protein sequence to assess the discriminative aptitude of start-end region. In last setting, performance is assessed by combining X residues taken from start-end regions of protein A with Y residue taken from start-end region of protein B. To identify up to what number of residues can capture the discriminative essence of protein sequences, in all 4 proposed sub-sequence based fixed length generation settings, we select as minimum number of residues as possible (e.g 10, depending on the minimum sequence length of the benchmark dataset) and iteratively increments this number with a step size of 10 residues up to 50% of average sequence length of benchmark core PPI prediction datasets. In all 4 settings, X and Y range from 10-to-70 residues taken with the difference of 10 residues. By fusing protein A sequences with protein B sequences, the fixed length protein sequence generated through traditional and robust pre-processing strategies are passed to an attention based deep hybrid neural network for PPI prediction.</p>
      </sec>
      <sec id="sec4.3.4">
        <title>An attention based deep hybrid neural network (ADH-PPI)</title>
        <p id="p0370">In the marathon of developing robust and precise deep learning based end-to-end frameworks for diverse Genomics and Proteomics sequence analysis tasks, we are witnessing the explosion of deep learning approaches, core architectures of which are mainly formed by deep feed forward neural networks (<xref rid="bib42" ref-type="bibr">Li et al., 2019</xref>), deep belief networks (<xref rid="bib85" ref-type="bibr">Zou et al., 2019</xref>), convolutional neural networks (<xref rid="bib42" ref-type="bibr">Li et al., 2019</xref>), autoencoders (<xref rid="bib48" ref-type="bibr">Meyer, 2021</xref>), and long short-term memory networks (<xref rid="bib48" ref-type="bibr">Meyer, 2021</xref>). Predominantly, efforts are being made under the hood of two different paradigms to develop more efficient deep learning models for diverse sequence analysis tasks (<xref rid="bib85" ref-type="bibr">Zou et al., 2019</xref>; <xref rid="bib42" ref-type="bibr">Li et al., 2019</xref>; <xref rid="bib48" ref-type="bibr">Meyer, 2021</xref>). The main focus of one paradigm is to develop deep neural networks based on series of neural layers (i.e convolutional layer, recurrent layer) to effectively capture the non-linearity of genomic and proteomic sequences (<xref rid="bib85" ref-type="bibr">Zou et al., 2019</xref>; <xref rid="bib42" ref-type="bibr">Li et al., 2019</xref>; <xref rid="bib48" ref-type="bibr">Meyer, 2021</xref>; <xref rid="bib54" ref-type="bibr">Nusrat and Jang, 2018</xref>; <xref rid="bib45" ref-type="bibr">Lydia and Francis, 2019</xref>). Whereas, other paradigm pays more attention to develop shallow or ensemble neural networks which utilize neural layers (i.e convolutional layer, recurrent layer) in different parallel channels and combine the features extracted by different channels to perform target prediction. The paper in hand develops an attention based deep hybrid model (ADH-PPI) for PPI prediction following the structure of first paradigm. Workflow of proposed ADH-PPI approach is illustrated in Figure, a brief description of different components of ADH-PPI approach is given in the following subsections.<fig id="fig12"><caption><p>Workflow of proposed attention-based deep hybrid methodology ADH-PPI for protein-protein interaction prediction</p></caption><graphic xlink:href="gr12"/></fig></p>
      </sec>
      <sec id="sec4.3.5">
        <title>Stochastic embedding layer</title>
        <p id="p0375">Stochastic embedding layer takes k-mers of protein sequences and unsupervisedly learned k-mer embeddings (generation of which is explained in section ADH-PPI: An Attention based Deep Hybrid Model for Protein Protein Interaction Prediction) to generate an embedding weight matrix <inline-formula><mml:math id="M33" altimg="si25.gif"><mml:mrow><mml:mi>E</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>q</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>k</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mo>|</mml:mo><mml:mo linebreak="badbreak">×</mml:mo><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mo>_</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, where the number of rows are equal to unique k-mers and number of columns are equal to k-mers embedding size. To fine-tune embedding matrix in a more generic way, we apply two different kinds of dropouts on the embedding matrix, where there is a probability <inline-formula><mml:math id="M34" altimg="si26.gif"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> to fully replace k-mer embedding vectors with zero and probability <inline-formula><mml:math id="M35" altimg="si27.gif"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>s</mml:mi><mml:mo>_</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> to replace individual continuous values with zero in remaining k-mer embedding vectors. First kind of dropout drops few k-mer embedding vectors whereas second kind of dropout drops few continuous values of remaining k-mer embedding vectors (<xref rid="bib17" ref-type="bibr">Gal and Ghahramani, 2016</xref>; <xref rid="bib47" ref-type="bibr">Merity et al., 2017</xref>). This regularization avoids model over-fitting by ensuring that model does not over-specialize certain k-mers to extract most informative features for various classes. The k-mer embedding vector <inline-formula><mml:math id="M36" altimg="si26.gif"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and dimension <inline-formula><mml:math id="M37" altimg="si27.gif"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>s</mml:mi><mml:mo>_</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> dropout probabilities are varied from 0.002-to-0.008 where we find that <inline-formula><mml:math id="M38" altimg="si26.gif"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> of 0.004 and <inline-formula><mml:math id="M39" altimg="si27.gif"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>s</mml:mi><mml:mo>_</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> of 0.005 performs better.</p>
        <p id="p0380">The optimized embedding matrix containing 120-dimensional embedding vectors for unique k-mers is passed to a Long Short Term Memory layer.</p>
      </sec>
      <sec id="sec4.3.6">
        <title>Optimized long short term memory layer</title>
        <p id="p0385">Long short-term memory (LSTM) layer is a special kind of recurrent layer which avoids gradient explosion and gradient disappearance issues faced by the neural network during the modeling of long sequences (<xref rid="bib28" ref-type="bibr">Huang et al., 2015a</xref>). Furthermore, LSTM is really effective for the extraction of long dependencies of features which is very critical for accurate PPI prediction (<xref rid="bib29" ref-type="bibr">Huang et al., 2015b</xref>). Unlike a traditional recurrent neural network, LSTM makes use of multiple gates described in following equations to control the flow of comprehensive k-mers information provided by optimized embedding layer.<disp-formula id="fd3"><label>(Equation 3)</label><mml:math id="M40" altimg="si28.gif"><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.25em"/><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>u</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>.</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msup><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>.</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="fd4"><label>(Equation 4)</label><mml:math id="M41" altimg="si29.gif"><mml:mrow><mml:mi>F</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.25em"/><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>f</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mi>f</mml:mi></mml:msup><mml:mo>.</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msup><mml:mi>U</mml:mi><mml:mi>f</mml:mi></mml:msup><mml:mo>.</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="fd5"><label>(Equation 5)</label><mml:math id="M42" altimg="si30.gif"><mml:mrow><mml:mi>O</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.25em"/><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>o</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mi>o</mml:mi></mml:msup><mml:mo>.</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msup><mml:mi>U</mml:mi><mml:mi>o</mml:mi></mml:msup><mml:mo>.</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="fd6"><label>(Equation 6)</label><mml:math id="M43" altimg="si31.gif"><mml:mrow><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mo>.</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msup><mml:mi>U</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mo>.</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="fd7"><label>(Equation 7)</label><mml:math id="M44" altimg="si32.gif"><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi><mml:mspace width="0.25em"/><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.25em"/><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mrow><mml:mo>(</mml:mo></mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>u</mml:mi></mml:msub><mml:mo linebreak="goodbreak">⊙</mml:mo><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo linebreak="goodbreak">+</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>f</mml:mi></mml:msub><mml:mo linebreak="goodbreak">⊙</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="fd8"><label>(Equation 8)</label><mml:math id="M45" altimg="si33.gif"><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mspace width="0.25em"/><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>o</mml:mi></mml:msub><mml:mo linebreak="badbreak">⊙</mml:mo><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p>
        <p id="p0390">The input, forget, and output gates get activated or deactivated mainly on the basis of their weigh matrices and biases, and work on the basis of their activation functions (sigmoid, tanh) to determine which information need to be retained or discarded from memory cell states (<inline-formula><mml:math id="M46" altimg="si34.gif"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M47" altimg="si35.gif"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>). In order to preserve k-mers information for a longer period of time, hidden state <italic>h</italic> of each cell is saved at every time step <italic>t</italic>.</p>
        <p id="p0395">To most effectively regularize recurrent layer, unlike existing approaches which arbitrarily drop the hidden states during the update to the memory state <inline-formula><mml:math id="M48" altimg="si34.gif"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, we use DropConnect (<xref rid="bib68" ref-type="bibr">Wan et al., 2013</xref>) which applies dropout with the probability of 0.4 on the recurrent [<inline-formula><mml:math id="M49" altimg="si36.gif"><mml:mrow><mml:msup><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M50" altimg="si37.gif"><mml:mrow><mml:msup><mml:mi>U</mml:mi><mml:mi>f</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M51" altimg="si38.gif"><mml:mrow><mml:msup><mml:mi>U</mml:mi><mml:mi>o</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>] and non-recurrent weight matrices [<inline-formula><mml:math id="M52" altimg="si39.gif"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M53" altimg="si40.gif"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mi>f</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M54" altimg="si41.gif"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mi>o</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>] of the LSTM layer before the forward and the backward pass to enhance LSTM aptitude to extract informative features along with their long range dependencies. Using LSTM with 120 hidden units, ADH-PPI model extracts the short and long range dependencies of features which are important to distinguish interactive protein sequence pairs from non-interactive protein sequence pairs. The 120-dimensional feature vectors produced by the LSTM layer are passed to the convolutional layer.</p>
      </sec>
      <sec id="sec4.3.7">
        <title>Convolutional layer</title>
        <p id="p0400">The convolutional layer has been widely applied in Natural Language Processing and Bioinformatics tasks (<xref rid="bib2" ref-type="bibr">Alzubaidi et al., 2021</xref>) due to two unique charateristics, local perception as well as parameter sharing (<xref rid="bib19" ref-type="bibr">Goodfellow et al., 2016</xref>). Like simulating the cells with local receptive fields within human brain, the convolutional layer performs an operation known as convolution which uses local connection and shared weights to extract hidden informative features and reduce the overall complexity of neural network (<xref rid="bib19" ref-type="bibr">Goodfellow et al., 2016</xref>). Convolution operation applied at a particular <inline-formula><mml:math id="M55" altimg="si42.gif"><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> layer produces a feature map <inline-formula><mml:math id="M56" altimg="si43.gif"><mml:mrow><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>l</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> that can be mathematically expressed as:<disp-formula id="fd9"><label>(Equation 9)</label><mml:math id="M57" altimg="si44.gif"><mml:mrow><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>l</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msup><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>l</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msup><mml:mspace width="1em"/><mml:mo linebreak="badbreak">⊗</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>l</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msup><mml:mo linebreak="badbreak">+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>l</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>Where <inline-formula><mml:math id="M58" altimg="si45.gif"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>l</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> represents the weight matrix of convolutional kernel of the <inline-formula><mml:math id="M59" altimg="si42.gif"><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> layer, symbol <inline-formula><mml:math id="M60" altimg="si46.gif"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">⊗</mml:mo></mml:mrow></mml:math></inline-formula> denotes the convolutional operation, <inline-formula><mml:math id="M61" altimg="si47.gif"><mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>l</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> represents the off-set vector, and f(x) denotes the activation function. We use ReLu as an activation function to sparse the final output of convolutional layer which leads to speed up the training process and maintain the steady convergence rate to prevent vanishing gradient issue. CNN layer use 50 kernels of size 3 to produce 50-dimensional feature vectors which are passed to an attention layer.</p>
      </sec>
      <sec id="sec4.3.8">
        <title>Attention layer</title>
        <p id="p0405">Attention layer is widely used to adjust the weights of feature vectors in such a manner that most crucial features are emphasized and less important features are penalized (<xref rid="bib19" ref-type="bibr">Goodfellow et al., 2016</xref>). Attention function can be considered a mapping from a Query vector (Q), and Key-Value vectors (K-V) to an output vector. Here Q, K, and V are linear projection of given protein sequence statistical representation and output is the new protein sequence statistical representation of same dimensions incorporating comprehensive mutual association of k-mers present in protein sequences. The entire process involves three steps: acquiring Query, Key, and Value linear projections, estimating the weight through placing Query and Key into a certain compatibility function, and obtaining the output by estimating the weighted sum of Value using the pre-computed weight. There are many types of compatibility functions which produces many flavors of attention mechanism. Considering the long length of protein sequences (thousands of k-mers), we use the least space and time efficient version of the compatibility function namely Scaled Dot-Product Attention (SDPA). The SDPA computes the dot product of Query and Key which is divided by <inline-formula><mml:math id="M62" altimg="si48.gif"><mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="M63" altimg="si49.gif"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the Key dimension, and finally applies the softmax over it to obtain the weight.<disp-formula id="fd10"><label>(Equation 10)</label><mml:math id="M64" altimg="si50.gif"><mml:mrow><mml:mi>W</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:msup><mml:mi>K</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
        <p id="p0410">In <xref rid="fd10" ref-type="disp-formula">Equation 10</xref>, Weight represents a square matrix having number of rows/columns equivalent to length of protein sequences calculated in terms of number of k-mers. Each i<sup><italic>th</italic></sup> row j<sup><italic>th</italic></sup> column value denotes the interaction intensiveness among i<sup><italic>th</italic></sup> k-mer and j<sup><italic>th</italic></sup> k-mer. After computing weight, every row of output that represents the statistical vector of a k-mer, can be estimated as the weighted sum of all k-mers. This is primarily implemented through a single-matrix multiplication which can be mathematically expressed as follows:<disp-formula id="fd11"><label>(Equation 11)</label><mml:math id="M65" altimg="si51.gif"><mml:mrow><mml:mi>O</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>W</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mtext>∗</mml:mtext><mml:mi>V</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:msup><mml:mi>K</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt></mml:mfrac><mml:mi>V</mml:mi></mml:mrow></mml:math></disp-formula></p>
        <p id="p0415">Given, 50-dimensional statistical vectors of protein sequences, attention layer updates the values of statistical features on the basis of their usefulness for PPI prediction.</p>
      </sec>
      <sec id="sec4.3.9">
        <title>Normalization layer</title>
        <p id="p0420">Neural network faces the issue of internal co-variance shift which de-stabilizes the neural network due to change in input distribution to hidden layers of neural network when model weights are updated after the execution of every batch (<xref rid="bib32" ref-type="bibr">Ioffe and Szegedy, 2015</xref>). Internal co-variance shift makes the optimal weights learned by the network during previous iterations obsolete (<xref rid="bib32" ref-type="bibr">Ioffe and Szegedy, 2015</xref>), disturbs the convergence and generalizability of the model (<xref rid="bib32" ref-type="bibr">Ioffe and Szegedy, 2015</xref>).</p>
        <p id="p0425">Normalization addresses this issue by standardizing the input before feeding to a hidden layer for every batch. It ensures that input-to-output mapping of a neural network does not over-specialize one particular region of protein sequences, resulting in faster training, convergence and improved generalizability (<xref rid="bib32" ref-type="bibr">Ioffe and Szegedy, 2015</xref>).</p>
        <p id="p0430"><xref rid="fd12" ref-type="disp-formula">Equation 12</xref> describes the overall paradigm of normalization which normalizes each sequence <inline-formula><mml:math id="M66" altimg="si52.gif"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> by tuning 2 parameters γ and β.<disp-formula id="fd12"><label>(Equation 12)</label><mml:math id="M67" altimg="si53.gif"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>B</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>γ</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p>
        <p id="p0435"><xref rid="fd13" ref-type="disp-formula">Equation 13</xref> illustrates the way mean of a given batch is computed where <inline-formula><mml:math id="M68" altimg="si52.gif"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the current sequence from <italic>m</italic> sequences present in a given batch <italic>b</italic>.<disp-formula id="fd13"><label>(Equation 13)</label><mml:math id="M69" altimg="si54.gif"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn><mml:mo linebreak="goodbreak">/</mml:mo><mml:mi>m</mml:mi><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p>
        <p id="p0440"><xref rid="fd14" ref-type="disp-formula">Equation 14</xref> describes the way variance of every batch <italic>b</italic> is computed where each sequence <inline-formula><mml:math id="M70" altimg="si52.gif"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is subtracted from the mean of entire batch (<inline-formula><mml:math id="M71" altimg="si55.gif"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) before aggregating and computing average using <italic>m</italic> number of sequences present in given batch <italic>b</italic>.<disp-formula id="fd14"><label>(Equation 14)</label><mml:math id="M72" altimg="si56.gif"><mml:mrow><mml:msubsup><mml:mn>0</mml:mn><mml:mi>b</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn><mml:mo linebreak="goodbreak">/</mml:mo><mml:mi>m</mml:mi><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>u</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula></p>
        <p id="p0445"><xref rid="fd15" ref-type="disp-formula">Equation 15</xref> subtracts each sequence <inline-formula><mml:math id="M73" altimg="si52.gif"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> from mean of the batch <inline-formula><mml:math id="M74" altimg="si55.gif"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and takes fraction by standard deviation to normalize the values between 0 and 1, which is represented with <inline-formula><mml:math id="M75" altimg="si57.gif"><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true">ˆ</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>.<disp-formula id="fd15"><label>(Equation 15)</label><mml:math id="M76" altimg="si58.gif"><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true">ˆ</mml:mo></mml:mover><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak="badbreak">−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow><mml:msqrt><mml:mrow><mml:msubsup><mml:mn>0</mml:mn><mml:mi>b</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
        <p id="p0450">In order to enable the network to adapt mean and variance of distribution, 2 parameters γ and β are learned and updated along with biases and weights during training. Final, normalized, scaled, and shifted version of hidden distribution can be represented using <xref rid="fd16" ref-type="disp-formula">Equation 16</xref>.<disp-formula id="fd16"><label>(Equation 16)</label><mml:math id="M77" altimg="si59.gif"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>γ</mml:mi><mml:mo linebreak="badbreak">∗</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true">ˆ</mml:mo></mml:mover><mml:mo linebreak="goodbreak">+</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:math></disp-formula></p>
      </sec>
      <sec id="sec4.3.10">
        <title>Standard dropout layer</title>
        <p id="p0455">Dropout is a de-facto standard to regularize neural networks, which generally improves the quality of the hidden features by alleviating the likelihood of hidden units co-adaptations problem. More specifically, for every hidden unit, dropout avoids co-adaptation by iteratively tweaking the presence and absence of other hidden units to ensure that a hidden unit can not rely on other hidden units to fix its mistakes.</p>
        <p id="p0460">In proposed ADH-PPI methodology, each hidden unit has the probability <italic>p</italic> to be dropped where the value of <italic>p</italic> falls in range of of 0.01-to-0.4. Mathematically (<xref rid="fd17" ref-type="disp-formula">Equation 17</xref>), likelihood of omitting a hidden unit is done according to the Bernoulli distribution with probability p. Through an element wise product of hidden unit vector with a mask where each element is randomly sampled from Bernoulli distribution, hidden units are dropped during training. Whereas, for testing (<xref rid="fd18" ref-type="disp-formula">Equation 18</xref>), instead of dropping the hidden unit, probability for a hidden unit not to be dropped <inline-formula><mml:math id="M78" altimg="si60.gif"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula>% is estimated.<disp-formula id="fd17"><label>(Equation 17)</label><mml:math id="M79" altimg="si61.gif"><mml:mrow><mml:mi>y</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>W</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">·</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak="goodbreak">∼</mml:mo><mml:mi>B</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="fd18"><label>(Equation 18)</label><mml:math id="M80" altimg="si62.gif"><mml:mrow><mml:mi>y</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>W</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p>
      </sec>
      <sec id="sec4.3.11">
        <title>Softmax layer</title>
        <p id="p0465">Using dense 50-dimensional representation of protein sequences, softmax layer discriminates interactive protein pairs from non-interactive protein pairs. Categorical cross-entropy also known as softmax loss is used as a loss function which is a simple softmax activation plus a cross entropy loss. Working of softmax activation and categorical cross entropy are described in <xref rid="fd19" ref-type="disp-formula">Equation 19</xref> and <xref rid="fd20" ref-type="disp-formula">Equation 20</xref> respectively.<disp-formula id="fd19"><label>(Equation 19)</label><mml:math id="M81" altimg="si63.gif"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:msubsup><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mi>j</mml:mi><mml:mi>C</mml:mi></mml:msubsup><mml:msubsup><mml:mi>e</mml:mi><mml:mi>j</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="fd20"><label>(Equation 20)</label><mml:math id="M82" altimg="si64.gif"><mml:mrow><mml:mi>C</mml:mi><mml:mi>E</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mo linebreak="goodbreak">−</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mi>i</mml:mi><mml:mi>C</mml:mi></mml:munderover><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p>
        <p id="p0470">In these equations, <italic>t</italic> represents one-hot encoded ground truth, <inline-formula><mml:math id="M83" altimg="si65.gif"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents probability score for each class in C and <inline-formula><mml:math id="M84" altimg="si66.gif"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> refers to softmax activation applied before the computation of cross-entropy loss.</p>
      </sec>
      <sec id="sec4.3.12">
        <title>Model training optimization</title>
        <p id="p0475">Proposed ADH-PPI model is implemented using Pytorch (<xref rid="bib55" ref-type="bibr">Paszke et al., 2019</xref>). To perform a fair performance comparison of proposed ADH-PPI approach with existing PPI predictors on two benchmark core S.cerevisiae and H.pylori datasets, 10-fold cross validation is performed. ADH-PPI is trained on complete core S.cerevisiae dataset to evaluate its performance on four independent test sets belonging to E.coli, C.elegans, H.sapiens, and <italic>M. musculus</italic> species. To optimize the training of ADH-PPI model, learning rate is the most crucial hyperparameter which controls how much model weights need to be updated in response to error estimated after the execution of every batch. Choosing a learning rate is really challenging as very small value may lead to longer training, a very large value may destabilize the training process or learn a sub-optimal collection of weights quickly, and constant learning rate leads to saturation where validation loss of the model stops improving. To find an optimal learning rate, we use a learning rate or weight decay strategy which iteratively changes the learning rate if validation loss does not improve for specific number of epochs. In this manner, model converges to optimal weights which largely reduce the generalization error. The ADH-PPI approach is trained using the batch size of 64 and ADAMW tweaks initial learning rate defined as 0.01-to-0.07 using decay rate defined as 0.00001-to-0.01 if categorical cross entropy loss on 1% validation sequences stops improving. To facilitate the reproduceability of experimental results, best values of different hyperparameters found through Grid search (<xref rid="bib63" ref-type="bibr">Shekar and Dagnew, 2019</xref>; <xref rid="bib43" ref-type="bibr">Liashchynskyi and Liashchynskyi, 2019</xref>) with respect to two core benchmark datasets and 4 independent test sets are provided in Table.<table-wrap position="float" id="tbl4"><caption><p>Optimal values of different hyperparameters of proposed ADH-PPI methodology for 2 core datasets and 4 independent test sets for the task of PPI prediction</p></caption><table frame="hsides" rules="groups"><thead><tr><th>PPI Dataset</th><th>Degree of higher order<break/>residue (K-mer)</th><th>Stride size</th><th>Sequence embedding<break/>dimension</th><th>Learning rate</th><th>Weight decay</th><th>Dropout rate</th><th>Subsequence regions</th></tr></thead><tbody><tr><td>S.Cerevisiave</td><td>5</td><td>5</td><td>FastText-120</td><td>0.03</td><td>0.1</td><td>0.3</td><td>P-A_S-40, P-B_E-40</td></tr><tr><td>H.Pyloir</td><td>5</td><td>1</td><td>FastText-120</td><td>0.05</td><td>0.01</td><td>0.01</td><td>P-A_S-40, P-B_E-40</td></tr><tr><td>C.elegans</td><td>5</td><td>5</td><td>FastText-120</td><td>0.05</td><td>1.00 × 10<sup>−5</sup></td><td>0.1</td><td>P-A_S-40, P-B_E-40</td></tr><tr><td>H.sapiens</td><td>5</td><td>5</td><td>FastText-120</td><td>0.05</td><td>1.00 × 10<sup>−5</sup></td><td>0.1</td><td>P-A_S-40, P-B_E-40</td></tr><tr><td>M.musculus</td><td>5</td><td>5</td><td>FastText-120</td><td>0.05</td><td>1.00 × 10<sup>−5</sup></td><td>0.1</td><td>P-A_S-40, P-B_E-40</td></tr><tr><td>E.coli</td><td>5</td><td>5</td><td>FastText-120</td><td>0.05</td><td>1.00 × 10<sup>−5</sup></td><td>0.1</td><td>P-A_S-40, P-B_E-40</td></tr></tbody></table></table-wrap></p>
      </sec>
      <sec id="sec4.3.13">
        <title>Model evaluation criterion</title>
        <p id="p0480">Following the evaluation criterion of previous PPI predictors (<xref rid="bib79" ref-type="bibr">Yu et al., 2021</xref>; <xref rid="bib35" ref-type="bibr">Jiang et al., 2020</xref>; <xref rid="bib69" ref-type="bibr">Wang et al., 2020a</xref>), to perform a fair performance comparison of proposed ADH-PPI approach with existing PPI predictors, we use 6 different evaluation measures namely accuracy (ACC), precision, recall, matthews correlation coefficient (MCC), F1-score, and area under receiver operating characteristics (AU-ROC).<disp-formula id="fd21"><label>(Equation 21)</label><mml:math id="M85" altimg="si67.gif"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mtext>Accuracy </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mtext>ACC</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext>Precision </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mtext>PRE</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext>Recall </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mtext>REC</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext>MCC</mml:mtext><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">×</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">×</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">×</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext>F</mml:mtext><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:mtext>score</mml:mtext><mml:mo linebreak="badbreak">=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo linebreak="badbreak">×</mml:mo><mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>E</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mi>R</mml:mi><mml:mi>E</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>E</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>R</mml:mi><mml:mi>E</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula></p>
        <p id="p0485">In <xref rid="fd21" ref-type="disp-formula">Equation 21</xref>, TP, FP, TN, FN indicate the true positives, false positives, true negatives, and false negatives.</p>
        <p id="p0490">Accuracy (ACC) is the ratio of correctly predicted interactive and non-interactive instances with total predictions. It does not accurately measure the performance of the classifier if the dataset has unbalanced classes. Recall true positive rate, whereas precision computes upto what extent positive predictions are fully correct. F1-score takes the harmonic mean of precision and recall. Evaluation criterion such as (PRE, REC, F1-score) do not take true negatives into account. Whereas, matthews correlation coefficient (MCC) takes all four TP, FP TN, FN into account to compute the performance. High value of MCC shows that classifier is effectively distinguishing all corpus classes even when a class is under or over represented. Area under receiver operating characteristic curve (AU-ROC) measures degree of separability of the model at different thresholds. A high degree of separability indicates that model accurately distinguishes interactive protein sequence pairs from non-interactive protein sequence pairs.</p>
      </sec>
    </sec>
    <sec id="sec4.4">
      <title>Quantification and statistical analysis</title>
      <p id="p0495">The statistical tests are performed by making use of a python package namely Scipy (version: 1.8.0).</p>
    </sec>
  </sec>
</body>
<back>
  <ref-list id="cebib0010">
    <title>References</title>
    <ref id="bib1">
      <element-citation publication-type="journal" id="sref1">
        <person-group person-group-type="author">
          <name>
            <surname>Alberts</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>The cell as a collection of protein machines: preparing the next generation of molecular biologists</article-title>
        <source>cell</source>
        <volume>92</volume>
        <year>1998</year>
        <fpage>291</fpage>
        <lpage>294</lpage>
        <pub-id pub-id-type="pmid">9476889</pub-id>
      </element-citation>
    </ref>
    <ref id="bib2">
      <element-citation publication-type="journal" id="sref2">
        <person-group person-group-type="author">
          <name>
            <surname>Alzubaidi</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Humaidi</surname>
            <given-names>A.J.</given-names>
          </name>
          <name>
            <surname>Al-Dujaili</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Duan</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Al-Shamma</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Santamaría</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Fadhel</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Al-Amidie</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Farhan</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Review of deep learning: concepts, cnn architectures, challenges, applications, future directions</article-title>
        <source>J. Big Data</source>
        <volume>8</volume>
        <year>2021</year>
        <fpage>53</fpage>
        <lpage>74</lpage>
        <pub-id pub-id-type="pmid">33816053</pub-id>
      </element-citation>
    </ref>
    <ref id="bib3">
      <element-citation publication-type="journal" id="sref3">
        <person-group person-group-type="author">
          <name>
            <surname>Andrei</surname>
            <given-names>S.A.</given-names>
          </name>
          <name>
            <surname>Sijbesma</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Hann</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Davis</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>O’Mahony</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Perry</surname>
            <given-names>M.W.D.</given-names>
          </name>
          <name>
            <surname>Karawajczyk</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Eickhoff</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Brunsveld</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Doveston</surname>
            <given-names>R.G.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Stabilization of protein-protein interactions in drug discovery</article-title>
        <source>Expert Opin. Drug Discov.</source>
        <volume>12</volume>
        <year>2017</year>
        <fpage>925</fpage>
        <lpage>940</lpage>
        <pub-id pub-id-type="pmid">28695752</pub-id>
      </element-citation>
    </ref>
    <ref id="bib4">
      <element-citation publication-type="book" id="sref4">
        <person-group person-group-type="author">
          <name>
            <surname>Asim</surname>
            <given-names>M.N.</given-names>
          </name>
          <name>
            <surname>Ibrahim</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Malik</surname>
            <given-names>M.I.</given-names>
          </name>
          <name>
            <surname>Dengel</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Ahmed</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <part-title>Enhancer-dsnet: a supervisedly prepared enriched sequence representation for the identification of enhancers and their strength</part-title>
        <source>International Conference on Neural Information Processing</source>
        <year>2020</year>
        <publisher-name>Springer</publisher-name>
        <fpage>38</fpage>
        <lpage>48</lpage>
      </element-citation>
    </ref>
    <ref id="bib5">
      <element-citation publication-type="book" id="sref5">
        <person-group person-group-type="author">
          <name>
            <surname>Asim</surname>
            <given-names>M.N.</given-names>
          </name>
          <name>
            <surname>Malik</surname>
            <given-names>M.I.</given-names>
          </name>
          <name>
            <surname>Dengel</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Ahmed</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <part-title>K-mer neural embedding performance analysis using amino acid codons</part-title>
        <source>2020 International Joint Conference on Neural Networks (IJCNN)</source>
        <year>2020</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>1</fpage>
        <lpage>8</lpage>
      </element-citation>
    </ref>
    <ref id="bib7">
      <element-citation publication-type="journal" id="sref7">
        <person-group person-group-type="author">
          <name>
            <surname>Bairoch</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Apweiler</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>The swiss-prot protein sequence data bank and its new supplement trembl</article-title>
        <source>Nucleic Acids Res.</source>
        <volume>24</volume>
        <year>1996</year>
        <fpage>21</fpage>
        <lpage>25</lpage>
        <pub-id pub-id-type="pmid">8594581</pub-id>
      </element-citation>
    </ref>
    <ref id="bib8">
      <element-citation publication-type="journal" id="sref8">
        <person-group person-group-type="author">
          <name>
            <surname>Berggård</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Linse</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>James</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Methods for the detection and analysis of protein–protein interactions</article-title>
        <source>Proteomics</source>
        <volume>7</volume>
        <year>2007</year>
        <fpage>2833</fpage>
        <lpage>2842</lpage>
        <pub-id pub-id-type="pmid">17640003</pub-id>
      </element-citation>
    </ref>
    <ref id="bib9">
      <element-citation publication-type="journal" id="sref9">
        <person-group person-group-type="author">
          <name>
            <surname>Bergstra</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Bardenet</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Kégl</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>Algorithms for hyper-parameter optimization</article-title>
        <source>Adv. Neural Inf. Process. Syst.</source>
        <volume>24</volume>
        <year>2011</year>
      </element-citation>
    </ref>
    <ref id="bib10">
      <element-citation publication-type="journal" id="sref10">
        <person-group person-group-type="author">
          <name>
            <surname>Bojanowski</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Grave</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Joulin</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Mikolov</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Enriching word vectors with subword information</article-title>
        <source>Trans. Assoc. Comput. Linguist.</source>
        <volume>5</volume>
        <year>2017</year>
        <fpage>135</fpage>
        <lpage>146</lpage>
      </element-citation>
    </ref>
    <ref id="bib11">
      <element-citation publication-type="journal" id="sref11">
        <person-group person-group-type="author">
          <name>
            <surname>Chao</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <article-title>Recent advances in supervised dimension reduction: a survey</article-title>
        <source>Mach. Learn. Knowl. Extr.</source>
        <volume>1</volume>
        <year>2019</year>
        <fpage>341</fpage>
        <lpage>358</lpage>
      </element-citation>
    </ref>
    <ref id="bib12">
      <element-citation publication-type="journal" id="sref12">
        <person-group person-group-type="author">
          <name>
            <surname>Ding</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>Predicting protein-protein interactions via multivariate mutual information of protein sequences</article-title>
        <source>BMC Bioinf.</source>
        <volume>17</volume>
        <year>2016</year>
        <fpage>398</fpage>
        <lpage>413</lpage>
      </element-citation>
    </ref>
    <ref id="bib13">
      <element-citation publication-type="journal" id="sref13">
        <person-group person-group-type="author">
          <name>
            <surname>Du</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Yao</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Deepppi: boosting prediction of protein–protein interactions with deep neural networks</article-title>
        <source>J. Chem. Inf. Model.</source>
        <volume>57</volume>
        <year>2017</year>
        <fpage>1499</fpage>
        <lpage>1510</lpage>
        <pub-id pub-id-type="pmid">28514151</pub-id>
      </element-citation>
    </ref>
    <ref id="bib14">
      <element-citation publication-type="journal" id="sref14">
        <person-group person-group-type="author">
          <name>
            <surname>Espadaler</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Romero-Isart</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Jackson</surname>
            <given-names>R.M.</given-names>
          </name>
          <name>
            <surname>Oliva</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>Prediction of protein–protein interactions using distant conservation of sequence patterns and structure relationships</article-title>
        <source>Bioinformatics</source>
        <volume>21</volume>
        <year>2005</year>
        <fpage>3360</fpage>
        <lpage>3368</lpage>
        <pub-id pub-id-type="pmid">15961445</pub-id>
      </element-citation>
    </ref>
    <ref id="bib15">
      <element-citation publication-type="journal" id="sref15">
        <person-group person-group-type="author">
          <name>
            <surname>Fu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Niu</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <article-title>Cd-hit: accelerated for clustering the next-generation sequencing data</article-title>
        <source>Bioinformatics</source>
        <volume>28</volume>
        <year>2012</year>
        <fpage>3150</fpage>
        <lpage>3152</lpage>
        <pub-id pub-id-type="pmid">23060610</pub-id>
      </element-citation>
    </ref>
    <ref id="bib16">
      <element-citation publication-type="journal" id="sref16">
        <person-group person-group-type="author">
          <name>
            <surname>Fu</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Cai</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Liao</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Improved pre-mirnas identification through mutual information of pre-mirna sequences and structures</article-title>
        <source>Front. Genet.</source>
        <volume>10</volume>
        <year>2019</year>
        <fpage>119</fpage>
        <pub-id pub-id-type="pmid">30858864</pub-id>
      </element-citation>
    </ref>
    <ref id="bib17">
      <element-citation publication-type="book" id="sref17">
        <person-group person-group-type="author">
          <name>
            <surname>Gal</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Ghahramani</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <part-title>A theoretically grounded application of dropout in recurrent neural networks</part-title>
        <source>Advances in neural information processing systems</source>
        <year>2016</year>
        <fpage>1019</fpage>
        <lpage>1027</lpage>
      </element-citation>
    </ref>
    <ref id="bib18">
      <element-citation publication-type="journal" id="sref18">
        <person-group person-group-type="author">
          <name>
            <surname>Gavin</surname>
            <given-names>A.C.</given-names>
          </name>
          <name>
            <surname>Bösche</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Krause</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Grandi</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Marzioch</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Bauer</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Schultz</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Rick</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Michon</surname>
            <given-names>A.M.</given-names>
          </name>
          <name>
            <surname>Cruciat</surname>
            <given-names>C.M.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Functional organization of the yeast proteome by systematic analysis of protein complexes.</article-title>
        <source>Nature</source>
        <volume>415</volume>
        <year>2002</year>
        <fpage>141</fpage>
        <lpage>147</lpage>
        <pub-id pub-id-type="pmid">11805826</pub-id>
      </element-citation>
    </ref>
    <ref id="bib19">
      <element-citation publication-type="book" id="sref19">
        <person-group person-group-type="author">
          <name>
            <surname>Goodfellow</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Courville</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <part-title>Deep learning mit press</part-title>
        <source>Conference on information and communication systems (ICICS)</source>
        <year>2016</year>
        <fpage>151</fpage>
        <lpage>156</lpage>
        <ext-link ext-link-type="uri" xlink:href="http://www.deeplearningbook.org" id="intref0105">http://www.deeplearningbook.org</ext-link>
      </element-citation>
    </ref>
    <ref id="bib20">
      <element-citation publication-type="journal" id="sref20">
        <person-group person-group-type="author">
          <name>
            <surname>Guo</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Wen</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Using support vector machine combined with auto covariance to predict protein–protein interactions from protein sequences</article-title>
        <source>Nucleic Acids Res.</source>
        <volume>36</volume>
        <year>2008</year>
        <fpage>3025</fpage>
        <lpage>3030</lpage>
        <pub-id pub-id-type="pmid">18390576</pub-id>
      </element-citation>
    </ref>
    <ref id="bib21">
      <element-citation publication-type="journal" id="sref21">
        <person-group person-group-type="author">
          <name>
            <surname>Guo</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Wen</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Using support vector machine combined with auto covariance to predict protein–protein interactions from protein sequences</article-title>
        <source>Nucleic Acids Res.</source>
        <volume>36</volume>
        <year>2008</year>
        <fpage>3025</fpage>
        <lpage>3030</lpage>
        <pub-id pub-id-type="pmid">18390576</pub-id>
      </element-citation>
    </ref>
    <ref id="bib22">
      <element-citation publication-type="journal" id="sref22">
        <person-group person-group-type="author">
          <name>
            <surname>Hashemifar</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Neyshabur</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Khan</surname>
            <given-names>A.A.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Predicting protein–protein interactions through sequence-based deep learning</article-title>
        <source>Bioinformatics</source>
        <volume>34</volume>
        <year>2018</year>
        <fpage>i802</fpage>
        <lpage>i810</lpage>
        <pub-id pub-id-type="pmid">30423091</pub-id>
      </element-citation>
    </ref>
    <ref id="bib23">
      <element-citation publication-type="journal" id="sref23">
        <person-group person-group-type="author">
          <name>
            <surname>Ho</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Gruhler</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Heilbut</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Bader</surname>
            <given-names>G.D.</given-names>
          </name>
          <name>
            <surname>Moore</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Adams</surname>
            <given-names>S.-L.</given-names>
          </name>
          <name>
            <surname>Millar</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Taylor</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Bennett</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Boutilier</surname>
            <given-names>K.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Systematic identification of protein complexes in saccharomyces cerevisiae by mass spectrometry</article-title>
        <source>Nature</source>
        <volume>415</volume>
        <year>2002</year>
        <fpage>180</fpage>
        <lpage>183</lpage>
        <pub-id pub-id-type="pmid">11805837</pub-id>
      </element-citation>
    </ref>
    <ref id="bib24">
      <element-citation publication-type="journal" id="sref24">
        <person-group person-group-type="author">
          <name>
            <surname>Hosur</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Vinayagam</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Stelzl</surname>
            <given-names>U.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Perrimon</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Bienkowska</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Berger</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>A computational framework for boosting confidence in high-throughput protein-protein interaction datasets</article-title>
        <source>Genome biology</source>
        <volume>13</volume>
        <year>2012</year>
        <fpage>1</fpage>
        <lpage>14</lpage>
      </element-citation>
    </ref>
    <ref id="bib25">
      <element-citation publication-type="journal" id="sref25">
        <person-group person-group-type="author">
          <name>
            <surname>Hu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>Y.A.</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>You</surname>
            <given-names>Z.-H.</given-names>
          </name>
        </person-group>
        <article-title>A novel network-based algorithm for predicting protein-protein interactions using gene ontology</article-title>
        <source>Front. Microbiol.</source>
        <volume>12</volume>
        <year>2021</year>
        <fpage>735329</fpage>
        <pub-id pub-id-type="pmid">34512614</pub-id>
      </element-citation>
    </ref>
    <ref id="bib26">
      <element-citation publication-type="journal" id="sref26">
        <person-group person-group-type="author">
          <name>
            <surname>Hu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Pan</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>You</surname>
            <given-names>Z.-H.</given-names>
          </name>
        </person-group>
        <article-title>Hiscf: leveraging higher-order structures for clustering analysis in biological networks</article-title>
        <source>Bioinformatics</source>
        <volume>37</volume>
        <year>2021</year>
        <fpage>542</fpage>
        <lpage>550</lpage>
        <pub-id pub-id-type="pmid">32931549</pub-id>
      </element-citation>
    </ref>
    <ref id="bib27">
      <element-citation publication-type="journal" id="sref27">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>Y.-A.</given-names>
          </name>
          <name>
            <surname>You</surname>
            <given-names>Z.-H.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Chan</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>Sequence-based prediction of protein-protein interactions using weighted sparse representation model combined with global encoding</article-title>
        <source>BMC Bioinf.</source>
        <volume>17</volume>
        <year>2016</year>
        <fpage>184</fpage>
        <lpage>211</lpage>
      </element-citation>
    </ref>
    <ref id="bib28">
      <element-citation publication-type="journal" id="sref28">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>Y.-A.</given-names>
          </name>
          <name>
            <surname>You</surname>
            <given-names>Z.-H.</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Wong</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Using weighted sparse representation model combined with discrete cosine transformation to predict protein-protein interactions from protein sequence</article-title>
        <source>BioMed Res. Int.</source>
        <volume>2015</volume>
        <year>2015</year>
        <fpage>902198</fpage>
        <pub-id pub-id-type="pmid">26634213</pub-id>
      </element-citation>
    </ref>
    <ref id="bib29">
      <element-citation publication-type="journal" id="sref29">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Bidirectional lstm-crf models for sequence tagging</article-title>
        <comment>Preprint at</comment>
        <source>arXiv</source>
        <year>2015</year>
        <pub-id pub-id-type="doi">10.48550/arXiv.1508.01991</pub-id>
      </element-citation>
    </ref>
    <ref id="bib30">
      <element-citation publication-type="journal" id="sref30">
        <person-group person-group-type="author">
          <name>
            <surname>Hue</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Riffle</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Vert</surname>
            <given-names>J.-P.</given-names>
          </name>
          <name>
            <surname>Noble</surname>
            <given-names>W.S.</given-names>
          </name>
        </person-group>
        <article-title>Large-scale prediction of protein-protein interactions from structures</article-title>
        <source>BMC Bioinf.</source>
        <volume>11</volume>
        <year>2010</year>
        <fpage>144</fpage>
        <lpage>149</lpage>
      </element-citation>
    </ref>
    <ref id="bib31">
      <element-citation publication-type="journal" id="sref31">
        <person-group person-group-type="author">
          <name>
            <surname>Ieremie</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Ewing</surname>
            <given-names>R.M.</given-names>
          </name>
          <name>
            <surname>Niranjan</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Transformergo: predicting protein–protein interactions by modelling the attention between sets of gene ontology terms</article-title>
        <source>Bioinformatics</source>
        <volume>38</volume>
        <year>2022</year>
        <fpage>2269</fpage>
        <lpage>2277</lpage>
      </element-citation>
    </ref>
    <ref id="bib32">
      <element-citation publication-type="book" id="sref32">
        <person-group person-group-type="author">
          <name>
            <surname>Ioffe</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Szegedy</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <part-title>Batch normalization: accelerating deep network training by reducing internal covariate shift</part-title>
        <source>International conference on machine learning</source>
        <year>2015</year>
        <publisher-name>PMLR</publisher-name>
        <fpage>448</fpage>
        <lpage>456</lpage>
      </element-citation>
    </ref>
    <ref id="bib33">
      <element-citation publication-type="journal" id="sref33">
        <person-group person-group-type="author">
          <name>
            <surname>Ito</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Tashiro</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Muta</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Ozawa</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Chiba</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Nishizawa</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Yamamoto</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Kuhara</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Sakaki</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Toward a protein–protein interaction map of the budding yeast: a comprehensive system to examine two-hybrid interactions in all possible combinations between the yeast proteins</article-title>
        <source>Proc. Natl. Acad. Sci. USA</source>
        <volume>97</volume>
        <year>2000</year>
        <fpage>1143</fpage>
        <lpage>1147</lpage>
        <pub-id pub-id-type="pmid">10655498</pub-id>
      </element-citation>
    </ref>
    <ref id="bib34">
      <element-citation publication-type="journal" id="sref34">
        <person-group person-group-type="author">
          <name>
            <surname>Jia</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Chou</surname>
            <given-names>K.-C.</given-names>
          </name>
        </person-group>
        <article-title>ippi-esml: an ensemble classifier for identifying the interactions of proteins by incorporating their physicochemical properties and wavelet transforms into pseaac</article-title>
        <source>J. Theor. Biol.</source>
        <volume>377</volume>
        <year>2015</year>
        <fpage>47</fpage>
        <lpage>56</lpage>
        <pub-id pub-id-type="pmid">25908206</pub-id>
      </element-citation>
    </ref>
    <ref id="bib35">
      <element-citation publication-type="book" id="sref35">
        <person-group person-group-type="author">
          <name>
            <surname>Jiang</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Qiu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Fu</surname>
            <given-names>Q.</given-names>
          </name>
        </person-group>
        <part-title>Prediction of membrane protein interaction based on deep residual learning</part-title>
        <source>International Conference on Intelligent Computing</source>
        <year>2020</year>
        <publisher-name>Springer</publisher-name>
        <fpage>103</fpage>
        <lpage>108</lpage>
      </element-citation>
    </ref>
    <ref id="bib36">
      <element-citation publication-type="journal" id="sref36">
        <person-group person-group-type="author">
          <name>
            <surname>Joshi</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Becker</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Alexandrov</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>Genome-scale gene function prediction using multiple sources of high-throughput data in yeast saccharomyces cerevisiae</article-title>
        <source>OMICS A J. Integr. Biol.</source>
        <volume>8</volume>
        <year>2004</year>
        <fpage>322</fpage>
        <lpage>333</lpage>
      </element-citation>
    </ref>
    <ref id="bib37">
      <element-citation publication-type="journal" id="sref37">
        <person-group person-group-type="author">
          <name>
            <surname>Kong</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Dehmer</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Fctp-wsrc: protein–protein interactions prediction via weighted sparse representation based classification</article-title>
        <source>Front. Genet.</source>
        <volume>11</volume>
        <year>2020</year>
        <fpage>18</fpage>
        <pub-id pub-id-type="pmid">32117437</pub-id>
      </element-citation>
    </ref>
    <ref id="bib38">
      <element-citation publication-type="journal" id="sref38">
        <person-group person-group-type="author">
          <name>
            <surname>Kovács</surname>
            <given-names>I.A.</given-names>
          </name>
          <name>
            <surname>Luck</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Spirohn</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Pollis</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Schlabach</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Bian</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>D.-K.</given-names>
          </name>
          <name>
            <surname>Kishore</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Hao</surname>
            <given-names>T.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Network-based prediction of protein interactions</article-title>
        <source>Nat. Commun.</source>
        <volume>10</volume>
        <year>2019</year>
        <fpage>1240</fpage>
        <lpage>1248</lpage>
        <pub-id pub-id-type="pmid">30886144</pub-id>
      </element-citation>
    </ref>
    <ref id="bib39">
      <element-citation publication-type="journal" id="sref39">
        <person-group person-group-type="author">
          <name>
            <surname>Krogan</surname>
            <given-names>N.J.</given-names>
          </name>
          <name>
            <surname>Cagney</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Zhong</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Ignatchenko</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Pu</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Datta</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Tikuisis</surname>
            <given-names>A.P.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Global landscape of protein complexes in the yeast saccharomyces cerevisiae</article-title>
        <source>Nature</source>
        <volume>440</volume>
        <year>2006</year>
        <fpage>637</fpage>
        <lpage>643</lpage>
        <pub-id pub-id-type="pmid">16554755</pub-id>
      </element-citation>
    </ref>
    <ref id="bib40">
      <element-citation publication-type="journal" id="sref40">
        <person-group person-group-type="author">
          <name>
            <surname>Kulminskaya</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Oberer</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Protein-protein interactions regulate the activity of adipose triglyceride lipase in intracellular lipolysis</article-title>
        <source>Biochimie</source>
        <volume>169</volume>
        <year>2020</year>
        <fpage>62</fpage>
        <lpage>68</lpage>
        <pub-id pub-id-type="pmid">31404588</pub-id>
      </element-citation>
    </ref>
    <ref id="bib41">
      <element-citation publication-type="journal" id="sref41">
        <person-group person-group-type="author">
          <name>
            <surname>Le</surname>
            <given-names>N.Q.K.</given-names>
          </name>
          <name>
            <surname>Yapp</surname>
            <given-names>E.K.Y.</given-names>
          </name>
          <name>
            <surname>Ho</surname>
            <given-names>Q.-T.</given-names>
          </name>
          <name>
            <surname>Nagasundaram</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Ou</surname>
            <given-names>Y.-Y.</given-names>
          </name>
          <name>
            <surname>Yeh</surname>
            <given-names>H.-Y.</given-names>
          </name>
        </person-group>
        <article-title>ienhancer-5step: identifying enhancers using hidden information of dna sequences via chou’s 5-step rule and word embedding</article-title>
        <source>Anal. Biochem.</source>
        <volume>571</volume>
        <year>2019</year>
        <fpage>53</fpage>
        <lpage>61</lpage>
        <pub-id pub-id-type="pmid">30822398</pub-id>
      </element-citation>
    </ref>
    <ref id="bib42">
      <element-citation publication-type="journal" id="sref42">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Pan</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>Deep learning in bioinformatics: introduction, application, and perspective in the big data era</article-title>
        <source>Methods</source>
        <volume>166</volume>
        <year>2019</year>
        <fpage>4</fpage>
        <lpage>21</lpage>
        <pub-id pub-id-type="pmid">31022451</pub-id>
      </element-citation>
    </ref>
    <ref id="bib43">
      <element-citation publication-type="journal" id="sref43">
        <person-group person-group-type="author">
          <name>
            <surname>Liashchynskyi</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Liashchynskyi</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Grid Search, Random Search, Genetic Algorithm: A Big Comparison for Nas</article-title>
        <comment>Preprint at</comment>
        <source>arXiv</source>
        <year>2019</year>
        <pub-id pub-id-type="doi">10.48550/arXiv.1912.06059</pub-id>
      </element-citation>
    </ref>
    <ref id="bib44">
      <element-citation publication-type="journal" id="sref44">
        <person-group person-group-type="author">
          <name>
            <surname>Licata</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Briganti</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Peluso</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Perfetto</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Iannuccelli</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Galeota</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Sacco</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Palma</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Nardozza</surname>
            <given-names>A.P.</given-names>
          </name>
          <name>
            <surname>Santonico</surname>
            <given-names>E.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Mint, the molecular interaction database: 2012 update</article-title>
        <source>Nucleic Acids Res.</source>
        <volume>40</volume>
        <year>2012</year>
        <fpage>D857</fpage>
        <lpage>D861</lpage>
        <pub-id pub-id-type="pmid">22096227</pub-id>
      </element-citation>
    </ref>
    <ref id="bib45">
      <element-citation publication-type="book" id="sref45">
        <person-group person-group-type="author">
          <name>
            <surname>Lydia</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Francis</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <part-title>A Survey of Optimization Techniques for Deep Learning Networks</part-title>
        <year>2019</year>
        <fpage>2454</fpage>
        <lpage>9150</lpage>
      </element-citation>
    </ref>
    <ref id="bib46">
      <element-citation publication-type="journal" id="sref46">
        <person-group person-group-type="author">
          <name>
            <surname>Martin</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Roe</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Faulon</surname>
            <given-names>J.-L.</given-names>
          </name>
        </person-group>
        <article-title>Predicting protein–protein interactions using signature products</article-title>
        <source>Bioinformatics</source>
        <volume>21</volume>
        <year>2005</year>
        <fpage>218</fpage>
        <lpage>226</lpage>
        <pub-id pub-id-type="pmid">15319262</pub-id>
      </element-citation>
    </ref>
    <ref id="bib47">
      <element-citation publication-type="journal" id="sref47">
        <person-group person-group-type="author">
          <name>
            <surname>Merity</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Keskar</surname>
            <given-names>N.S.</given-names>
          </name>
          <name>
            <surname>Socher</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Regularizing and optimizing lstm language models</article-title>
        <comment>Preprint at</comment>
        <source>arXiv</source>
        <year>2017</year>
        <pub-id pub-id-type="doi">10.48550/arXiv.1708.02182</pub-id>
      </element-citation>
    </ref>
    <ref id="bib48">
      <element-citation publication-type="book" id="sref48">
        <person-group person-group-type="author">
          <name>
            <surname>Meyer</surname>
            <given-names>J.G.</given-names>
          </name>
        </person-group>
        <part-title>‘Deep Learning Neural Network Tools for Proteomics’, <italic>Cell Reports Methods</italic></part-title>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">100003</object-id>
      </element-citation>
    </ref>
    <ref id="bib49">
      <element-citation publication-type="journal" id="sref49">
        <person-group person-group-type="author">
          <name>
            <surname>Mikolov</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Corrado</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Dean</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Efficient estimation of word representations in vector space</article-title>
        <comment>Preprint at</comment>
        <source>arXiv</source>
        <year>2013</year>
        <pub-id pub-id-type="doi">10.48550/arXiv.1301.3781</pub-id>
      </element-citation>
    </ref>
    <ref id="bib50">
      <element-citation publication-type="journal" id="sref50">
        <person-group person-group-type="author">
          <name>
            <surname>Nanni</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Fusion of classifiers for predicting protein–protein interactions</article-title>
        <source>Neurocomputing</source>
        <volume>68</volume>
        <year>2005</year>
        <fpage>289</fpage>
        <lpage>296</lpage>
      </element-citation>
    </ref>
    <ref id="bib51">
      <element-citation publication-type="journal" id="sref51">
        <person-group person-group-type="author">
          <name>
            <surname>Nanni</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Lumini</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>An ensemble of k-local hyperplanes for predicting protein–protein interactions</article-title>
        <source>Bioinformatics</source>
        <volume>22</volume>
        <year>2006</year>
        <fpage>1207</fpage>
        <lpage>1210</lpage>
        <pub-id pub-id-type="pmid">16481334</pub-id>
      </element-citation>
    </ref>
    <ref id="bib52">
      <element-citation publication-type="journal" id="sref52">
        <person-group person-group-type="author">
          <name>
            <surname>Nooren</surname>
            <given-names>I.M.A.</given-names>
          </name>
          <name>
            <surname>Thornton</surname>
            <given-names>J.M.</given-names>
          </name>
        </person-group>
        <article-title>Structural characterisation and functional significance of transient protein–protein interactions</article-title>
        <source>J. Mol. Biol.</source>
        <volume>325</volume>
        <year>2003</year>
        <fpage>991</fpage>
        <lpage>1018</lpage>
        <pub-id pub-id-type="pmid">12527304</pub-id>
      </element-citation>
    </ref>
    <ref id="bib53">
      <element-citation publication-type="journal" id="sref53">
        <person-group person-group-type="author">
          <name>
            <surname>Northey</surname>
            <given-names>T.C.</given-names>
          </name>
          <name>
            <surname>Barešić</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>A.C.R.</given-names>
          </name>
        </person-group>
        <article-title>Intpred: a structure-based predictor of protein–protein interaction sites</article-title>
        <source>Bioinformatics</source>
        <volume>34</volume>
        <year>2018</year>
        <fpage>223</fpage>
        <lpage>229</lpage>
        <pub-id pub-id-type="pmid">28968673</pub-id>
      </element-citation>
    </ref>
    <ref id="bib54">
      <element-citation publication-type="journal" id="sref54">
        <person-group person-group-type="author">
          <name>
            <surname>Nusrat</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Jang</surname>
            <given-names>S.-B.</given-names>
          </name>
        </person-group>
        <article-title>A comparison of regularization techniques in deep neural networks</article-title>
        <source>Symmetry</source>
        <volume>10</volume>
        <year>2018</year>
        <fpage>648</fpage>
      </element-citation>
    </ref>
    <ref id="bib55">
      <element-citation publication-type="journal" id="sref55">
        <person-group person-group-type="author">
          <name>
            <surname>Paszke</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Gross</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Massa</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Lerer</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Bradbury</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Chanan</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Killeen</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Gimelshein</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Antiga</surname>
            <given-names>L.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Pytorch: An imperative style, high-performance deep learning library</article-title>
        <comment>Preprint at</comment>
        <source>arXiv</source>
        <year>2019</year>
        <comment>
          <italic>arXiv:1912.01703</italic>
        </comment>
      </element-citation>
    </ref>
    <ref id="bib56">
      <element-citation publication-type="journal" id="sref56">
        <person-group person-group-type="author">
          <name>
            <surname>Pawson</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Nash</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Protein–protein interactions define specificity in signal transduction</article-title>
        <source>Genes Dev.</source>
        <volume>14</volume>
        <year>2000</year>
        <fpage>1027</fpage>
        <lpage>1047</lpage>
        <pub-id pub-id-type="pmid">10809663</pub-id>
      </element-citation>
    </ref>
    <ref id="bib57">
      <element-citation publication-type="journal" id="sref57">
        <person-group person-group-type="author">
          <name>
            <surname>Peri</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Navarro</surname>
            <given-names>J.D.</given-names>
          </name>
          <name>
            <surname>Kristiansen</surname>
            <given-names>T.Z.</given-names>
          </name>
          <name>
            <surname>Amanchy</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Surendranath</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Muthusamy</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Gandhi</surname>
            <given-names>T.K.B.</given-names>
          </name>
          <name>
            <surname>Chandrika</surname>
            <given-names>K.N.</given-names>
          </name>
          <name>
            <surname>Deshpande</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Suresh</surname>
            <given-names>S.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Human protein reference database as a discovery resource for proteomics</article-title>
        <source>Nucleic Acids Res.</source>
        <volume>32</volume>
        <year>2004</year>
        <fpage>D497</fpage>
        <lpage>D501</lpage>
        <pub-id pub-id-type="pmid">14681466</pub-id>
      </element-citation>
    </ref>
    <ref id="bib58">
      <element-citation publication-type="journal" id="sref58">
        <person-group person-group-type="author">
          <name>
            <surname>Petta</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Lievens</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Libert</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Tavernier</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>De Bosscher</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Modulation of protein–protein interactions for the development of novel therapeutics</article-title>
        <source>Mol. Ther.</source>
        <volume>24</volume>
        <year>2016</year>
        <fpage>707</fpage>
        <lpage>718</lpage>
        <pub-id pub-id-type="pmid">26675501</pub-id>
      </element-citation>
    </ref>
    <ref id="bib59">
      <element-citation publication-type="book" id="sref59">
        <person-group person-group-type="author">
          <name>
            <surname>Qi</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Klein-Seetharaman</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Bar-Joseph</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <part-title>Random forest similarity for protein-protein interaction prediction from multiple sources</part-title>
        <source>Biocomputing 2005</source>
        <year>2005</year>
        <publisher-name>World Scientific</publisher-name>
        <fpage>531</fpage>
        <lpage>542</lpage>
      </element-citation>
    </ref>
    <ref id="bib60">
      <element-citation publication-type="journal" id="sref60">
        <person-group person-group-type="author">
          <name>
            <surname>Salwinski</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Miller</surname>
            <given-names>C.S.</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>A.J.</given-names>
          </name>
          <name>
            <surname>Pettit</surname>
            <given-names>F.K.</given-names>
          </name>
          <name>
            <surname>Bowie</surname>
            <given-names>J.U.</given-names>
          </name>
          <name>
            <surname>Eisenberg</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>The database of interacting proteins: 2004 update</article-title>
        <source>Nucleic Acids Res.</source>
        <volume>32</volume>
        <year>2004</year>
        <fpage>D449</fpage>
        <lpage>D451</lpage>
        <pub-id pub-id-type="pmid">14681454</pub-id>
      </element-citation>
    </ref>
    <ref id="bib61">
      <element-citation publication-type="journal" id="sref61">
        <person-group person-group-type="author">
          <name>
            <surname>Sanchez-Pinto</surname>
            <given-names>L.N.</given-names>
          </name>
          <name>
            <surname>Venable</surname>
            <given-names>L.R.</given-names>
          </name>
          <name>
            <surname>Fahrenbach</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Churpek</surname>
            <given-names>M.M.</given-names>
          </name>
        </person-group>
        <article-title>Comparison of variable selection methods for clinical predictive modeling</article-title>
        <source>Int. J. Med. Inform.</source>
        <volume>116</volume>
        <year>2018</year>
        <fpage>10</fpage>
        <lpage>17</lpage>
        <pub-id pub-id-type="pmid">29887230</pub-id>
      </element-citation>
    </ref>
    <ref id="bib62">
      <element-citation publication-type="journal" id="sref62">
        <person-group person-group-type="author">
          <name>
            <surname>Schoenrock</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Samanfar</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Pitre</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Hooshyar</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Jin</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Phillips</surname>
            <given-names>C.A.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Phanse</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Omidi</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Gui</surname>
            <given-names>Y.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Efficient prediction of human protein-protein interactions at a global scale</article-title>
        <source>BMC Bioinf.</source>
        <volume>15</volume>
        <year>2014</year>
        <fpage>383</fpage>
      </element-citation>
    </ref>
    <ref id="bib63">
      <element-citation publication-type="book" id="sref63">
        <person-group person-group-type="author">
          <name>
            <surname>Shekar</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Dagnew</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <part-title>Grid search-based hyperparameter tuning and classification of microarray cancer data</part-title>
        <source>2019 Second International Conference on Advanced Computational and Communication Paradigms (ICACCP)</source>
        <year>2019</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>1</fpage>
        <lpage>8</lpage>
      </element-citation>
    </ref>
    <ref id="bib64">
      <element-citation publication-type="journal" id="sref64">
        <person-group person-group-type="author">
          <name>
            <surname>Singh</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Hosur</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Berger</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>Struct2net: a web service to predict protein–protein interactions using a structure-based approach</article-title>
        <source>Nucleic Acids Res.</source>
        <volume>38</volume>
        <year>2010</year>
        <fpage>W508</fpage>
        <lpage>W515</lpage>
        <pub-id pub-id-type="pmid">20513650</pub-id>
      </element-citation>
    </ref>
    <ref id="bib65">
      <element-citation publication-type="journal" id="sref65">
        <person-group person-group-type="author">
          <name>
            <surname>Sorzano</surname>
            <given-names>C.O.S.</given-names>
          </name>
          <name>
            <surname>Vargas</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Montano</surname>
            <given-names>A.P.</given-names>
          </name>
        </person-group>
        <article-title>A survey of dimensionality reduction techniques</article-title>
        <comment>Preprint at</comment>
        <source>arXiv</source>
        <year>2014</year>
        <comment>
          <italic>arXiv:1403.2877</italic>
        </comment>
      </element-citation>
    </ref>
    <ref id="bib66">
      <element-citation publication-type="journal" id="sref66">
        <person-group person-group-type="author">
          <name>
            <surname>Südhof</surname>
            <given-names>T.C.</given-names>
          </name>
        </person-group>
        <article-title>The synaptic vesicle cycle: a cascade of protein–protein interactions</article-title>
        <source>Nature</source>
        <volume>375</volume>
        <year>1995</year>
        <fpage>645</fpage>
        <lpage>653</lpage>
        <pub-id pub-id-type="pmid">7791897</pub-id>
      </element-citation>
    </ref>
    <ref id="bib67">
      <element-citation publication-type="journal" id="sref67">
        <person-group person-group-type="author">
          <name>
            <surname>Vickers</surname>
            <given-names>N.J.</given-names>
          </name>
        </person-group>
        <article-title>Animal communication: when i’m calling you, will you answer too?</article-title>
        <source>Curr. Biol.</source>
        <volume>27</volume>
        <year>2017</year>
        <fpage>R713</fpage>
        <lpage>R715</lpage>
        <pub-id pub-id-type="pmid">28743020</pub-id>
      </element-citation>
    </ref>
    <ref id="bib68">
      <element-citation publication-type="book" id="sref68">
        <person-group person-group-type="author">
          <name>
            <surname>Wan</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Zeiler</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Le Cun</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Fergus</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <part-title>Regularization of neural networks using dropconnect</part-title>
        <source>International conference on machine learning</source>
        <year>2013</year>
        <publisher-name>PMLR</publisher-name>
        <fpage>1058</fpage>
        <lpage>1066</lpage>
      </element-citation>
    </ref>
    <ref id="bib69">
      <element-citation publication-type="book" id="sref69">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>You</surname>
            <given-names>Z.-H.</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Z.-W.</given-names>
          </name>
        </person-group>
        <part-title>Gcnsp: a novel prediction method of self-interacting proteins based on graph convolutional networks</part-title>
        <source>International Conference on Intelligent Computing</source>
        <year>2020</year>
        <publisher-name>Springer</publisher-name>
        <fpage>109</fpage>
        <lpage>120</lpage>
      </element-citation>
    </ref>
    <ref id="bib70">
      <element-citation publication-type="journal" id="sref70">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>R.-S.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>L.-Y.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X.-S.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Analysis on multi-domain cooperation for predicting protein-protein interactions</article-title>
        <source>BMC Bioinf.</source>
        <volume>8</volume>
        <year>2007</year>
        <fpage>391</fpage>
      </element-citation>
    </ref>
    <ref id="bib71">
      <element-citation publication-type="book" id="sref71">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <part-title>A novel stochastic block model for network-based prediction of protein-protein interactions</part-title>
        <source>International Conference on Intelligent Computing</source>
        <year>2020</year>
        <publisher-name>Springer</publisher-name>
        <fpage>621</fpage>
        <lpage>632</lpage>
      </element-citation>
    </ref>
    <ref id="bib72">
      <element-citation publication-type="journal" id="sref72">
        <person-group person-group-type="author">
          <name>
            <surname>Xenarios</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Salwínski</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Duan</surname>
            <given-names>X.J.</given-names>
          </name>
          <name>
            <surname>Higney</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>S.-M.</given-names>
          </name>
          <name>
            <surname>Eisenberg</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>Dip, the database of interacting proteins: a research tool for studying cellular networks of protein interactions</article-title>
        <source>Nucleic Acids Res.</source>
        <volume>30</volume>
        <year>2002</year>
        <fpage>303</fpage>
        <lpage>305</lpage>
        <pub-id pub-id-type="pmid">11752321</pub-id>
      </element-citation>
    </ref>
    <ref id="bib73">
      <element-citation publication-type="journal" id="sref73">
        <person-group person-group-type="author">
          <name>
            <surname>Yao</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Du</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Diao</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>An integration of deep learning with feature embedding for protein–protein interaction prediction</article-title>
        <source>PeerJ</source>
        <volume>7</volume>
        <year>2019</year>
        <object-id pub-id-type="publisher-id">e7126</object-id>
      </element-citation>
    </ref>
    <ref id="bib74">
      <element-citation publication-type="journal" id="sref74">
        <person-group person-group-type="author">
          <name>
            <surname>You</surname>
            <given-names>Z.-H.</given-names>
          </name>
          <name>
            <surname>Chan</surname>
            <given-names>K.C.C.</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Predicting protein-protein interactions from primary protein sequences using a novel multi-scale local feature representation scheme and the random forest</article-title>
        <source>PLoS One</source>
        <volume>10</volume>
        <year>2015</year>
        <object-id pub-id-type="publisher-id">e0125811</object-id>
      </element-citation>
    </ref>
    <ref id="bib75">
      <element-citation publication-type="journal" id="sref75">
        <person-group person-group-type="author">
          <name>
            <surname>You</surname>
            <given-names>Z.-H.</given-names>
          </name>
          <name>
            <surname>Chan</surname>
            <given-names>K.C.C.</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Predicting protein-protein interactions from primary protein sequences using a novel multi-scale local feature representation scheme and the random forest</article-title>
        <source>PLoS One</source>
        <volume>10</volume>
        <year>2015</year>
        <object-id pub-id-type="publisher-id">e0125811</object-id>
      </element-citation>
    </ref>
    <ref id="bib76">
      <element-citation publication-type="journal" id="sref76">
        <person-group person-group-type="author">
          <name>
            <surname>You</surname>
            <given-names>Z.-H.</given-names>
          </name>
          <name>
            <surname>Lei</surname>
            <given-names>Y.-K.</given-names>
          </name>
          <name>
            <surname>Gui</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>D.-S.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>Using manifold embedding for assessing and predicting protein interactions from high-throughput experimental data</article-title>
        <source>Bioinformatics</source>
        <volume>26</volume>
        <year>2010</year>
        <fpage>2744</fpage>
        <lpage>2751</lpage>
        <pub-id pub-id-type="pmid">20817744</pub-id>
      </element-citation>
    </ref>
    <ref id="bib77">
      <element-citation publication-type="journal" id="sref77">
        <person-group person-group-type="author">
          <name>
            <surname>You</surname>
            <given-names>Z.-H.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Chan</surname>
            <given-names>K.C.</given-names>
          </name>
        </person-group>
        <article-title>An improved sequence-based prediction protocol for protein-protein interactions using amino acids substitution matrix and rotation forest ensemble classifiers</article-title>
        <source>Neurocomputing</source>
        <volume>228</volume>
        <year>2017</year>
        <fpage>277</fpage>
        <lpage>282</lpage>
      </element-citation>
    </ref>
    <ref id="bib78">
      <element-citation publication-type="journal" id="sref78">
        <person-group person-group-type="author">
          <name>
            <surname>You</surname>
            <given-names>Z.-H.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>C.-H.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>H.-J.</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>S.-P.</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>Prediction of protein-protein interactions from amino acid sequences using a novel multi-scale continuous and discontinuous feature set</article-title>
        <source>BMC Bioinf.</source>
        <volume>15</volume>
        <year>2014</year>
        <fpage>1</fpage>
        <lpage>9</lpage>
      </element-citation>
    </ref>
    <ref id="bib79">
      <element-citation publication-type="journal" id="sref79">
        <person-group person-group-type="author">
          <name>
            <surname>Yu</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>Prediction of protein–protein interactions based on elastic net and deep forest</article-title>
        <source>Expert Syst. Appl.</source>
        <volume>176</volume>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">114876</object-id>
      </element-citation>
    </ref>
    <ref id="bib80">
      <element-citation publication-type="journal" id="sref80">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Q.C.</given-names>
          </name>
          <name>
            <surname>Petrey</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Qiang</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Thu</surname>
            <given-names>C.A.</given-names>
          </name>
          <name>
            <surname>Bisikirska</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Lefebvre</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Accili</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Hunter</surname>
            <given-names>T.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Structure-based prediction of protein–protein interactions on a genome-wide scale</article-title>
        <source>Nature</source>
        <volume>490</volume>
        <year>2012</year>
        <fpage>556</fpage>
        <lpage>560</lpage>
        <pub-id pub-id-type="pmid">23023127</pub-id>
      </element-citation>
    </ref>
    <ref id="bib81">
      <element-citation publication-type="journal" id="sref81">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Phospholipase d and phosphatidic acid in plant defence response: from protein–protein and lipid–protein interactions to hormone signalling</article-title>
        <source>J. Exp. Bot.</source>
        <volume>66</volume>
        <year>2015</year>
        <fpage>1721</fpage>
        <lpage>1736</lpage>
        <pub-id pub-id-type="pmid">25680793</pub-id>
      </element-citation>
    </ref>
    <ref id="bib82">
      <element-citation publication-type="book" id="sref82">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>Y.Z.</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>Y.Y.</given-names>
          </name>
        </person-group>
        <part-title>Prediction of protein-protein interactions using local description of amino acid sequence</part-title>
        <source>Advances in computer science and education applications</source>
        <year>2011</year>
        <publisher-name>Springer</publisher-name>
        <fpage>254</fpage>
        <lpage>262</lpage>
      </element-citation>
    </ref>
    <ref id="bib83">
      <element-citation publication-type="journal" id="sref83">
        <person-group person-group-type="author">
          <name>
            <surname>Zhu</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Snyder</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Snyder m</article-title>
        <source>Curr. Opin. Chem. Biol.</source>
        <volume>7</volume>
        <year>2003</year>
        <fpage>55</fpage>
        <lpage>63</lpage>
        <pub-id pub-id-type="pmid">12547427</pub-id>
      </element-citation>
    </ref>
    <ref id="bib84">
      <element-citation publication-type="journal" id="sref84">
        <person-group person-group-type="author">
          <name>
            <surname>Zou</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Hastie</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Regularization and variable selection via the elastic net</article-title>
        <source>J. Royal Statistical Soc. B</source>
        <volume>67</volume>
        <year>2005</year>
        <fpage>301</fpage>
        <lpage>320</lpage>
      </element-citation>
    </ref>
    <ref id="bib85">
      <element-citation publication-type="journal" id="sref85">
        <person-group person-group-type="author">
          <name>
            <surname>Zou</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Huss</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Abid</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Mohammadi</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Torkamani</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Telenti</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>A primer on deep learning in genomics</article-title>
        <source>Nat. Genet.</source>
        <volume>51</volume>
        <year>2019</year>
        <fpage>12</fpage>
        <lpage>18</lpage>
        <pub-id pub-id-type="pmid">30478442</pub-id>
      </element-citation>
    </ref>
  </ref-list>
  <sec sec-type="data-availability" id="da0010">
    <title>Data and code availability</title>
    <p id="p0030">
      <list list-type="simple" id="ulist0015">
        <list-item id="u0030">
          <label>•</label>
          <p id="p0035">This paper performs PPI prediction over existing benchmark datasets and independent test sets given by (<xref rid="bib21" ref-type="bibr">Guo et al., 2008<italic>b</italic></xref>), (<xref rid="bib46" ref-type="bibr">Martin et al., 2005</xref>), and (<xref rid="bib82" ref-type="bibr">Zhou et al., 2011</xref>). All benchmark protein sequences datasets used to predict PPIs are available at <ext-link ext-link-type="uri" xlink:href="https://sds_genetic_analysis.opendfki.de/PPI/Download_PPI/" id="intref0015">https://sds_genetic_analysis.opendfki.de/PPI/Download_PPI/</ext-link>. These datasets are also available on a general-purpose open repository Zenodo <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.6973538" id="intref0020">https://doi.org/10.5281/zenodo.6973538</ext-link>.</p>
        </list-item>
        <list-item id="u0035">
          <label>•</label>
          <p id="p0040">A Public web server that allows the users to train predictive pipeline from scratch as well as utilize pre-trained predictive pipeline can be publicly accessed at <ext-link ext-link-type="uri" xlink:href="https://sds_genetic_analysis.opendfki.de/PPI/" id="intref0025">https://sds_genetic_analysis.opendfki.de/PPI/</ext-link>.</p>
        </list-item>
        <list-item id="u0040">
          <label>•</label>
          <p id="p0045">The complete source code of proposed ADH-PPI predictor is available at <ext-link ext-link-type="uri" xlink:href="https://sds_genetic_analysis.opendfki.de/PPI/Download_PPI/" id="intref0030">https://sds_genetic_analysis.opendfki.de/PPI/Download_PPI/</ext-link>.</p>
        </list-item>
        <list-item id="u0045">
          <label>•</label>
          <p id="p0050">Any further information needed to reanalyze the data reported in current study is available from <xref rid="sec4.2.1" ref-type="sec">lead contact</xref> upon the request.</p>
        </list-item>
      </list>
    </p>
  </sec>
  <ack id="ack0010">
    <title>Acknowledgments</title>
    <p id="p0500">We acknowledge that this work was supported by <funding-source id="gs2">Sartorius Artificial Intelligence Lab</funding-source>.</p>
    <sec id="sec15">
      <title>Author contributions</title>
      <p id="p0805">Conceptualization: M.N.A.; S. A.; Data curation: M. N. A., M. A. I.; Formal analysis: M. N. A., M. A. I.; Investigation: M. N. A., M. A. I.; Methodology: M. N. A., M. A. I.; Software: M. N. A., M. A. I.; Supervision: A. D., S. A., M. I. M.; Validation: M. N. A., M. A. I.; Visualization: M. N. A., M. A. I.; Writing – original draft: M. N. A., M. A. I.; Writing – review &amp; editing: A. D., S. A.</p>
    </sec>
    <sec sec-type="COI-statement" id="sec5">
      <title>Declaration of interests</title>
      <p id="p0505">The authors declare no competing interests.</p>
    </sec>
  </ack>
</back>
