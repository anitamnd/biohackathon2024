<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with OASIS Tables v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-archive-oasis-article1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jpoasis-nisons2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Comput Biol</journal-id>
    <journal-id journal-id-type="iso-abbrev">J. Comput. Biol</journal-id>
    <journal-id journal-id-type="publisher-id">cmb</journal-id>
    <journal-title-group>
      <journal-title>Journal of Computational Biology</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1066-5277</issn>
    <issn pub-type="epub">1557-8666</issn>
    <publisher>
      <publisher-name>Mary Ann Liebert, Inc., publishers</publisher-name>
      <publisher-loc>140 Huguenot Street, 3rd FloorNew Rochelle, NY 10801USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7232674</article-id>
    <article-id pub-id-type="pmid">31486672</article-id>
    <article-id pub-id-type="publisher-id">10.1089/cmb.2019.0210</article-id>
    <article-id pub-id-type="doi">10.1089/cmb.2019.0210</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Articles</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title><italic>bAIcis</italic>: A Novel Bayesian Network Structural Learning Algorithm and Its Comprehensive Performance Evaluation Against Open-Source Software</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Zhang</surname>
          <given-names>Lixia</given-names>
        </name>
        <xref ref-type="aff" rid="aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Rodrigues</surname>
          <given-names>Leonardo O.</given-names>
        </name>
        <xref ref-type="aff" rid="aff1"/>
        <xref ref-type="corresp" rid="corr1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Narain</surname>
          <given-names>Niven R.</given-names>
        </name>
        <xref ref-type="aff" rid="aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Akmaev</surname>
          <given-names>Viatcheslav R.</given-names>
        </name>
        <xref ref-type="aff" rid="aff1"/>
      </contrib>
      <aff id="aff1">BERG Health, Framingham, Massachusetts, USA.</aff>
    </contrib-group>
    <author-notes>
      <corresp id="corr1">Address correspondence to: Dr. Leonardo O. Rodrigues, BERG Health, 500 Old Connecticut Path, Building B, Framingham, MA 01701, USA <email>leonardo.rodrigues@berghealth.com</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <day>01</day>
      <month>5</month>
      <year>2020</year>
      <string-date>May 2020</string-date>
    </pub-date>
    <pub-date pub-type="epub">
      <day>07</day>
      <month>5</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>07</day>
      <month>5</month>
      <year>2020</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>27</volume>
    <issue>5</issue>
    <fpage>698</fpage>
    <lpage>708</lpage>
    <permissions>
      <copyright-statement>© Lixia Zhang, et al., 2020. Published by Mary Ann Liebert, Inc.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Lixia Zhang, et al.</copyright-holder>
      <license license-type="open-access">
        <license-p>This Open Access article is distributed under the terms of the Creative Commons License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0">http://creativecommons.org/licenses/by/4.0</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="cmb.2019.0210.pdf"/>
    <abstract>
      <p>Structural learning of Bayesian networks (BNs) from observational data has gained increasing applied use and attention from various scientific and industrial areas. The mathematical theory of BNs and their optimization is well developed. Although there are several open-source BN learners in the public domain, none of them are able to handle both small and large feature space data and recover network structures with acceptable accuracy. <italic>bAIcis</italic><sup>®</sup> is a novel BN learning and simulation software from BERG. It was developed with the goal of learning BNs from “Big Data” in health care, often exceeding hundreds of thousands features when research is conducted in genomics or multi-omics. This article provides a comprehensive performance evaluation of <italic>bAIcis</italic> and its comparison with the open-source BN learners. The study investigated synthetic datasets of discrete, continuous, and mixed data in small and large feature space, respectively. The results demonstrated that <italic>bAIcis</italic> outperformed the publicly available algorithms in structure recovery precision in almost all of the evaluated settings, achieving the true positive rates of 0.9 and precision of 0.8. In addition, <italic>bAIcis</italic> supports all data types, including continuous, discrete, and mixed variables. It is effectively parallelized on a distributed system and can work with datasets of thousands of features that are infeasible for any of the publicly available tools with a desired level of recovery accuracy.</p>
    </abstract>
    <kwd-group kwd-group-type="author">
      <kwd>Bayesian network</kwd>
      <kwd>causal inference</kwd>
      <kwd>structural learning</kwd>
    </kwd-group>
    <counts>
      <fig-count count="4"/>
      <table-count count="2"/>
      <ref-count count="35"/>
      <page-count count="11"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s001">
    <title>1. Introduction</title>
    <p>Causal inference, the process of finding relationships that describe cause-and-effect events, involves inferring the consequences in a counterfactual reality where an alternative potential cause occurred (Pearl, <xref rid="B18" ref-type="bibr">2010</xref>; Morgan and Winship, <xref rid="B14" ref-type="bibr">2014</xref>). As Pearl pointed out, causal and statistical inferences have fundamental differences since they focus on causation and association, respectively (Pearl, <xref rid="B16" ref-type="bibr">2009a</xref>). Moreover, when compared with statistical inference, causation requires one step further to investigate the outcomes by changing their conditions. Identifying causal relationships generally requires three levels of empirical evidence: temporal precedence, empirical association, and nonspurious relationships (Chambliss and Schutt, <xref rid="B5" ref-type="bibr">2018</xref>). One traditional approach for testing causal hypotheses is to conduct a well-designed experiment, where it is possible to control and intervene the condition, monitor the outcome change, and finally reach the causal conclusion. A clinical trial is a typical example that aims at demonstrating that one drug is the cause of improved outcomes. However, in certain scientific fields, such as epidemiology and social science, most studies are, by nature, observational rather than experimental (Rothman et al., <xref rid="B23" ref-type="bibr">2008</xref>); in addition, in new domains such as climate research (Von Storch, <xref rid="B30" ref-type="bibr">1999</xref>) and microarray measurements of gene expression (Nelson et al., <xref rid="B15" ref-type="bibr">2004</xref>), where the number of measured variables can be up to tens of thousands, even when experimental interventions are available, performing such a number of experiments is costly, time-consuming and takes extensive resources.</p>
    <p>Aiming at detecting causal relationships in observational data, Pearl debated that genuine causal inferences are possible from passive observations, introduced a minimal-model semantics of causation, and developed the Inductive Causation algorithm to identify causal relations rather than spurious covariations (Pearl and Verma, <xref rid="B22" ref-type="bibr">1995</xref>). Moreover, the theory of causal transportability discussed that causal relations learned from experiments can be transferred to a different environment where only observational data are available (Pearl and Bareinboim, <xref rid="B21" ref-type="bibr">2011</xref>). Chickering discussed learning causation structure by a scoring metric and advantages taken from score-equivalent evaluation criterion in identifying high-scoring structures (Chickering, <xref rid="B6" ref-type="bibr">1996</xref>, <xref rid="B7" ref-type="bibr">2002</xref>). These altogether led to the development of graphical causal modeling, a methodology widely used to describe the conditional independence relationships among a set of random variables based on the probability theory (Pearl, <xref rid="B16" ref-type="bibr">2009b</xref>).</p>
    <p>In a graphical model, nodes represent variables of interest, edges connecting nodes represent dependencies among the variables, and arrows, if they exist, refer to directionalities of the dependencies, for example, causal relationship. Bayesian networks (BNs) (Pearl, <xref rid="B19" ref-type="bibr">2011</xref>) are a specific type of graphical models that are directed acyclic graphs (DAGs), thus all of the edges are directed with no cycles existing in the model. As a marriage of causality and probability theories, BNs convey knowledge of data-generating process and are capable of identifying and inferring causation in both experimental and observational data. In this regard, BNs received a great amount of attention from various scientific fields such as reverse engineering of gene regulatory network (Baldi and Long, <xref rid="B2" ref-type="bibr">2001</xref>; Hartemink et al., <xref rid="B11" ref-type="bibr">2001</xref>; Xiao et al., <xref rid="B33" ref-type="bibr">2015</xref>) and explanations of social phenomena (Whitney et al., <xref rid="B31" ref-type="bibr">2011</xref>; Farasat et al., <xref rid="B9" ref-type="bibr">2015</xref>).</p>
    <p>In recent years, many algorithms have been developed for learning causal relationships among a set of variables under the BN framework. <italic>Rimbanet</italic> is a software package focusing on reconstructing integrative molecular BNs to understand biological systems (Zhu et al., <xref rid="B35" ref-type="bibr">2004</xref>); <italic>deal</italic> is an R package that provides algorithms for analyzing data by using BNs restricted to conditionally Gaussian networks (Boettcher and Dethlefsen, <xref rid="B3" ref-type="bibr">2003</xref>); <italic>bnFinder</italic>, scripted in Python, implements an exact learning algorithm for BNs reconstruction with parallel computing for multicore and distributed systems (Frolova and Wilczyński, <xref rid="B10" ref-type="bibr">2018</xref>); and <italic>sparsebn</italic>, an R package, is designed to deal with large feature space data (Aragam et al., <xref rid="B1" ref-type="bibr">2017</xref>). Most of these algorithms are only able to handle either small or large feature space data effectively, with only a few being able to deal with both regimes effectively.</p>
    <p>In this article, we introduce <italic>bAIcis</italic><sup>®</sup>, a BN structure learning algorithm developed and implemented by BERG LLC. It was developed with the goal of learning BNs from “Big Data” in health care, which often exceeds hundreds of thousands features when the research is conducted in genomics or multi-omics. Thus, the algorithm is capable of handling data in both small and large feature spaces effectively, and it has built-in capability to run in both multicore desktops and distributed systems, making the algorithm efficient for datasets in any scale. The purpose of this article is to benchmark the statistical performance of <italic>bAIcis</italic> and a number of open-source BN learners with regard to the accuracy of network recovery, scalability, and computation time.</p>
  </sec>
  <sec sec-type="methods" id="s002">
    <title>2. Methods</title>
    <sec id="s003">
      <title>2.1. Bayesian networks</title>
      <p>BNs, which encode the conditional independencies among a set of variables in a DAG, are usually used as a presentation of cause-effect relationships (Pearl, <xref rid="B19" ref-type="bibr">2011</xref>). Each directed edge indicates a direct causal relationship, whereas the absence of an edge refers to no direct causal impact. Hence, it is easy to borrow kinship relation terms to describe the relationships in a graph, such as parent, child, ancestor, and descendent. For example, an arrow <italic>X</italic> → <italic>Y</italic> refers to <italic>X</italic> as a parent of <italic>Y</italic> and <italic>Y</italic> as a child of <italic>X</italic> (<xref ref-type="fig" rid="f1">Fig. 1</xref>). As described in <xref ref-type="fig" rid="f1">Figure 1</xref>, both sprinkler and rain can directly influence whether the grass is wet; whereas the influence of seasonal variations on the wetness of grass is mediated by other conditions. Further, if it is raining, the grass is wet regardless of the season condition. This statement satisfies the Markov condition, which states that every node in a BN is conditionally independent of its nondescendent nodes, given its parent nodes. In this example, the joint distribution of all four variables can be factorized by this BN as</p>
      <fig id="f1" fig-type="figure" orientation="portrait" position="float">
        <label>FIG. 1.</label>
        <caption>
          <p>A BN representing the causal relationships among four variables: the season of the year (<italic>X<sub>1</sub></italic>), whether rain falls (<italic>X<sub>2</sub></italic>), whether the sprinkler is on (<italic>X<sub>3</sub></italic>), and whether the grass gets wet (<italic>X<sub>4</sub></italic>). BN, Bayesian network.</p>
        </caption>
        <graphic xlink:href="cmb.2019.0210_figure1"/>
      </fig>
      <disp-formula>
        <mml:math id="M1">
          <mml:mi>P</mml:mi>
          <mml:mrow>
            <mml:mo class="MathClass-open">(</mml:mo>
            <mml:mrow>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>X</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mn>1</mml:mn>
                </mml:mrow>
              </mml:msub>
              <mml:mo class="MathClass-punc">,</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>X</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mn>2</mml:mn>
                </mml:mrow>
              </mml:msub>
              <mml:mo class="MathClass-punc">,</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>X</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mn>3</mml:mn>
                </mml:mrow>
              </mml:msub>
              <mml:mo class="MathClass-punc">,</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>X</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mn>4</mml:mn>
                </mml:mrow>
              </mml:msub>
            </mml:mrow>
            <mml:mo class="MathClass-close">)</mml:mo>
          </mml:mrow>
          <mml:mo class="MathClass-rel">=</mml:mo>
          <mml:mi>P</mml:mi>
          <mml:mrow>
            <mml:mo class="MathClass-open">(</mml:mo>
            <mml:mrow>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>X</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mn>1</mml:mn>
                </mml:mrow>
              </mml:msub>
            </mml:mrow>
            <mml:mo class="MathClass-close">)</mml:mo>
          </mml:mrow>
          <mml:mi>P</mml:mi>
          <mml:mrow>
            <mml:mo class="MathClass-open">(</mml:mo>
            <mml:mrow>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>X</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mn>2</mml:mn>
                </mml:mrow>
              </mml:msub>
              <mml:mo class="MathClass-rel">|</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>X</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mn>1</mml:mn>
                </mml:mrow>
              </mml:msub>
            </mml:mrow>
            <mml:mo class="MathClass-close">)</mml:mo>
          </mml:mrow>
          <mml:mi>P</mml:mi>
          <mml:mrow>
            <mml:mo class="MathClass-open">(</mml:mo>
            <mml:mrow>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>X</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mn>3</mml:mn>
                </mml:mrow>
              </mml:msub>
              <mml:mo class="MathClass-rel">|</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>X</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mn>1</mml:mn>
                </mml:mrow>
              </mml:msub>
            </mml:mrow>
            <mml:mo class="MathClass-close">)</mml:mo>
          </mml:mrow>
          <mml:mi>P</mml:mi>
          <mml:mrow>
            <mml:mo class="MathClass-open">(</mml:mo>
            <mml:mrow>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>X</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mn>4</mml:mn>
                </mml:mrow>
              </mml:msub>
              <mml:mo class="MathClass-rel">|</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>X</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mn>2</mml:mn>
                </mml:mrow>
              </mml:msub>
              <mml:mo class="MathClass-punc">,</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>X</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mn>3</mml:mn>
                </mml:mrow>
              </mml:msub>
            </mml:mrow>
            <mml:mo class="MathClass-close">)</mml:mo>
          </mml:mrow>
          <mml:mo class="MathClass-punc">.</mml:mo>
        </mml:math>
      </disp-formula>
      <p>In general, given nodes <bold>X</bold> = (<italic>X</italic><sub>1</sub>, <italic>X</italic><sub>2</sub>…, <italic>X<sub>n</sub></italic>), the joint probability function for any BN is
<disp-formula><mml:math id="M2"><mml:mi>P</mml:mi><mml:mrow><mml:mo class="MathClass-open">(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle></mml:mrow><mml:mo class="MathClass-close">)</mml:mo></mml:mrow><mml:mo class="MathClass-rel">=</mml:mo><mml:msubsup><mml:mrow><mml:mo class="MathClass-op">∏</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo class="MathClass-rel">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mo class="MathClass-open">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo class="MathClass-rel">|</mml:mo><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo class="MathClass-open">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo class="MathClass-close">)</mml:mo></mml:mrow></mml:mrow><mml:mo class="MathClass-close">)</mml:mo></mml:mrow><mml:mo class="MathClass-punc">.</mml:mo></mml:math></disp-formula></p>
      <p>Hence, a BN factorizes a global full joint distribution of all variables to a set of local conditional distributions for each variable given its parents depending on the model structure.</p>
    </sec>
    <sec id="s004">
      <title>2.2. Bayesian network tools and packages</title>
      <p>The prevalent techniques of learning BNs can be grouped into two broad categories: score-based algorithms and constraint-based algorithms (Yu et al., <xref rid="B34" ref-type="bibr">2016</xref>). Score-based algorithms assign a score to each candidate BN on measuring goodness of fit and attempt to return a causal structure that maximizes the score, for example, Bayesian information criterion (BIC) (Chickering, <xref rid="B7" ref-type="bibr">2002</xref>; Tsamardinos et al., <xref rid="B29" ref-type="bibr">2006</xref>; Carvalho, <xref rid="B4" ref-type="bibr">2009</xref>); whereas constraint-based algorithms learn the BN structure based on Markov condition by a series of local conditional independence constraints and construct a graph that meets the independent relationships (Spirtes and Glymour, <xref rid="B27" ref-type="bibr">1991</xref>; Pearl and Verma, <xref rid="B22" ref-type="bibr">1995</xref>; Claassen and Heskes, <xref rid="B8" ref-type="bibr">2012</xref>). The advantages and disadvantages of the two algorithms were discussed elsewhere (Spirtes, <xref rid="B26" ref-type="bibr">2010</xref>; Triantafillou and Tsamardinos, <xref rid="B28" ref-type="bibr">2016</xref>; Scutari et al., <xref rid="B25" ref-type="bibr">2018</xref>).</p>
      <p><italic>bAIcis</italic> is a score-based BN learning algorithm with a BIC score criterion. <italic>bAIcis</italic> is a proprietary model-search algorithm that learns the network structure from the data by maximizing the BIC score in two phases. In the first phase, <italic>bAIcis</italic> generates optimal combinations of parents for each individual node by local BIC, and in the latter phase <italic>bAIcis</italic> incorporates those families to construct a final optimal network by global BIC. Bayesian methods are applied by using prior distributions to estimate the parameters (Heckerman, <xref rid="B12" ref-type="bibr">1998</xref>).</p>
      <p>There are several BN learners scripted in either R or Python available in the public domain. The tools <italic>bnFinder</italic> (Frolova and Wilczyński, <xref rid="B10" ref-type="bibr">2018</xref>), <italic>bnlearn</italic> (Scutari, <xref rid="B24" ref-type="bibr">2010</xref>), <italic>deal</italic> (Scutari, <xref rid="B24" ref-type="bibr">2010</xref>), <italic>pcalg</italic> (Kalisch et al., <xref rid="B13" ref-type="bibr">2012</xref>), <italic>Rimbanet</italic> (Zhu et al., <xref rid="B35" ref-type="bibr">2004</xref>), and <italic>sparsebn</italic> (Aragam et al., <xref rid="B1" ref-type="bibr">2017</xref>) were selected to benchmark the causal structure recovery.</p>
      <p><xref rid="tb1" ref-type="table">Table 1</xref> displays a summary of these BN tools, including <italic>bAIcis</italic>, in regard to network type, structure learning, and implemented learning algorithm. The <italic>Network Type</italic> section represents the ability of the method to work with continuous, discrete, or mixed variables; the <italic>Structure Learning</italic> section provides the level of flexibility of the obtained network solution, where weighted edges indicate the strength of dependencies connecting two nodes and enable the flexibility to scale down or up the networks; and <italic>Learning Algorithm</italic> indicates whether the tool is score-based or constraint-based. The majority of the selected tools implemented score-based learning algorithms and are consistent with <italic>bAIcis</italic>, except the R package <italic>pcalg</italic> where a PC algorithm is used for comparison. In the R package <italic>bnlearn</italic>, which implements both algorithms, the score-based hill-climbing greedy search algorithm is utilized.</p>
      <table-wrap id="tb1" orientation="portrait" position="float">
        <label>Table 1.</label>
        <caption>
          <p>Summary of the Evaluated Bayesian Network Tools, Along with the Capability of Dealing with Different Data Types in the Network, the Solution Format from Network Structure Learning, and the Implemented Learning Algorithm</p>
        </caption>
        <!--OASIS TABLE HERE-->
        <table frame="hsides" rules="groups">
          <colgroup>
            <col align="left"/>
            <col align="left"/>
            <col align="left"/>
            <col align="left"/>
            <col align="left"/>
            <col align="left"/>
            <col align="left"/>
            <col align="left"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="2" align="center" valign="bottom"> </th>
              <th colspan="3" align="center" valign="bottom">Network type<hr/></th>
              <th colspan="2" align="center" valign="bottom">Structure learning solution<hr/></th>
              <th colspan="2" align="center" valign="bottom">Learning algorithm<hr/></th>
            </tr>
            <tr>
              <th align="center" valign="bottom">Continuous</th>
              <th align="center" valign="bottom">Discrete</th>
              <th align="center" valign="bottom">Mixed</th>
              <th align="center" valign="bottom">One solution</th>
              <th align="center" valign="bottom">Weighted edges</th>
              <th align="center" valign="bottom">Score based</th>
              <th align="center" valign="bottom">Constraint based</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" valign="bottom">bAIcis<sup>®</sup></td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✗</td>
            </tr>
            <tr>
              <td align="left" valign="bottom">Rimbanet</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✗</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✗</td>
            </tr>
            <tr>
              <td align="left" valign="bottom">bnlearn</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✗</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✓</td>
            </tr>
            <tr>
              <td align="left" valign="bottom">deal</td>
              <td align="center" valign="bottom">✗</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✗</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✗</td>
            </tr>
            <tr>
              <td align="left" valign="bottom">sparsebn</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✗</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✗</td>
            </tr>
            <tr>
              <td align="left" valign="bottom">pcalg</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✗</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✗</td>
              <td align="center" valign="bottom">✗</td>
              <td align="center" valign="bottom">✓</td>
            </tr>
            <tr>
              <td align="left" valign="bottom">bnFinder</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✗</td>
              <td align="center" valign="bottom">✓</td>
              <td align="center" valign="bottom">✗</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
  </sec>
  <sec id="s005">
    <title>3. Benchmarking Study</title>
    <sec id="s006">
      <title>3.1. Synthetic networks</title>
      <p>A set of synthetic networks and datasets were generated to evaluate and compare the performance of the BN learners in both small and large feature spaces, where the number of nodes is below 50 and beyond 500, respectively. Since structural equation models (SEMs) are considered as a language for causality (Wright, <xref rid="B32" ref-type="bibr">1921</xref>; Pearl, <xref rid="B20" ref-type="bibr">2013</xref>), linear SEMs is utilized to simulate the synthetic data for network reconstruction.</p>
      <p>Networks were generated by predefining the network type, topology structure, node size, and sample size. Network type represents the data type for nodes, which could be continuous, discrete, and mixed networks (mixed with continuous and discrete nodes with a condition only allowing discrete variables to be the parents of discrete child nodes). Topology structure, the overall nodes’ degree distribution, is predefined as either random network or scale-free network. In the discrete-only networks, the number of parents for a discrete child has to be constrained, otherwise the discrete level of the child would inflate. Thus, scale-free topology was only implemented in continuous networks. Node size constrains the number of involved variables, and sample size sets contain the number of observations.</p>
      <p>A total of 36 datasets were produced for the experimental design configurations. Under each setting, generalized linear SEMs were utilized to generate synthetic data as follows:
<list list-type="bullet"><list-item><p>A continuous child <italic>X<sub>i</sub></italic> with all continuous parents was simulated from a Gaussian distribution <inline-formula><mml:math id="M3"><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo class="MathClass-rel">∼</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo class="MathClass-open">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo class="MathClass-op">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo class="MathClass-rel">∈</mml:mo><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo class="MathClass-punc">,</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo class="MathClass-close">)</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M4"><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> refers to a set of parents of node <italic>X<sub>i</sub></italic>, <italic>β<sub>ji</sub></italic> is the structural parameter associated with parent <italic>X<sub>j</sub></italic> generated from <italic>N</italic>(2, 0.8) with the sign (positive or negative regulation) simulated from a binomial distribution with probability 0.5, and <inline-formula><mml:math id="M5"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> is the error term generated from <italic>N</italic>(1, 0.01);</p></list-item><list-item><p>A discrete child was simulated by a multinomial distribution, ensuring conditional dependencies between each parent and the child;</p></list-item><list-item><p>A continuous child with discrete or mixed parents was simulated from a mixture Gaussian distribution.</p></list-item></list></p>
      <p>Since some BN tools learn network structures by the order of variables presented in datasets, for example, <italic>bnlearn</italic>, the order was shuffled to eliminate the impact. In addition, 20 replications were conducted for each configuration to capture the variation. The open-source BN learners were run with their default settings. Regarding <italic>bnFinder</italic>, the search space for each node was limited to six parent nodes to expedite algorithm running.</p>
      <p>In addition to benchmarking performance in small feature space with node size at 10, 20, and 50, a study in large feature space was also conducted, where <italic>bnlearn</italic> and <italic>sparsebn</italic> were selected since both claim to be efficient for data in large feature space.</p>
    </sec>
    <sec id="s007">
      <title>3.2. Network learning evaluation</title>
      <p>Performance was evaluated and benchmarked across the selected BN tools, regarding structure learning accuracy and computation time. Structure learning was evaluated on three metrics: true positive rate (TPR), precision, and false positive rate (FPR). TPR, also called as recall, measures the capability of detecting a true edge; precision, true discovery rate, is the ratio of true edges among all detected ones; and FPR refers to the type I error, that is, the ratio falsely detected edges among all nondirected relations. Two-sided paired sample <italic>t</italic>-test was assessed to compare <italic>bAIcis</italic> with all the other BN learners on the three metrics. Regarding running time, for small feature space, the number of running CPU was restricted to be 1 for all algorithms, except <italic>bnFinder</italic>, which was allocated a total of 32 CPUs.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="s008">
    <title>4. Results</title>
    <sec id="s009">
      <title>4.1. Small feature space</title>
      <sec id="s010">
        <title>4.1.1. Structure learning</title>
        <p>Networks with 50, 200, and 1000 observations were generated, submitted to the BN learners’ analysis, and evaluated for TPR, precision, and FPR. In the continuous network with 20 nodes (<xref ref-type="fig" rid="f2">Fig. 2a</xref>), <italic>deal</italic> was not evaluated due to its inability of handling continuous networks. Regarding TPR, all analyzed BN learners, except <italic>Rimbanet</italic>, were able to recover a comparable number of true edges, even in the 50-observations networks. However, <italic>bAIcis</italic> was able to significantly recover more true edges, compared with most BN learners. For FPR and sample size of 50 and 200, all the values were dense within a relatively low range, except <italic>sparsebn</italic> with a median-FPR higher than 0.25. In the precision, <italic>bAIcis</italic> outperformed all BN learners across all observation sizes and the superiority was statistically significant at a level of 0.05 by the two-sided paired sample <italic>t</italic>-test. As sample size increased to 1000, <italic>bAIcis</italic> identified less false edges in the learned network structure with increasing precision and decreasing FPR; whereas <italic>bnFinder</italic> and <italic>pcalg</italic> showed an opposite trend. <italic>Rimbanet</italic> was less impacted by sample size, with a comparatively flat trend in all three metrics. We evaluated the same metrics in the discrete network (<xref ref-type="fig" rid="f2">Fig. 2b</xref>).</p>
        <fig id="f2" fig-type="figure" orientation="portrait" position="float">
          <label>FIG. 2.</label>
          <caption>
            <p>Evaluation and comparison in metrics of edge detection among BN tools for synthetic networks of random topology with 20 nodes across different sample sizes in continuous network (left) and discrete network (right). Each panel comprises three metrics plots for TPR, precision, and FPR shown from top to bottom. In each metric plot, <italic>y</italic>-axis presents the metric value ranging from 0 to 1; <italic>x</italic>-axis shows the compared BN tools; and the distributions of metric rate across 20 replicates are summarized in boxplots stratified by sample size. An asterisk below a boxplot indicates that <italic>bAIcis</italic><sup>®</sup> performs statistically significantly better than the corresponding tool from a two-sided paired sample <italic>t</italic>-test at significant level 0.5. FPR, false positive rate; TPR, true positive rate.</p>
          </caption>
          <graphic xlink:href="cmb.2019.0210_figure2"/>
        </fig>
        <p>The difference in performance was slightly less dramatic compared with the continuous network. <italic>Rimbanet</italic> overall showed a better performance in the discrete network. <italic>bAIcis</italic> was not top ranked in TPR and precision under the 50-observations scenario; however, it shone and outperformed the competitors in precision when sample size increased to 200 and beyond. In the metric of TPR, overall <italic>bnFinder</italic> performed the best whereas <italic>Rimbanet</italic> was left behind; regarding precision and FPR, all values were relatively close, except <italic>deal</italic> whose values were below 0.2 and above 0.3, respectively. The trends by the sample size shared the pattern with the continuous network. In addition to <italic>Rimbanet</italic>, <italic>deal</italic> presented as another member in the unimpacted group.</p>
        <p>The results of all 36 analyzed settings can be seen in <xref ref-type="fig" rid="f4">Figure 4a–c</xref>. Each figure exhibits one evaluation metric and is summarized in a plot integrating the outcomes throughout all 36 settings. Nonsupported outcomes were left blank in figures, for example, <italic>deal</italic> in continuous networks. Besides, the results for <italic>deal</italic> and <italic>bnFinder</italic> with 50 nodes were not applicable because <italic>deal</italic> failed to learn a network with 50 nodes due to its memory limit, and <italic>bnFinder</italic> required an unexpectedly long computation time (more than 1 day on 32 CPUs). For all three metrics, the figure patterns preserve the same, with slight differences across different node sizes given the network type and topology structure. Overall, the boxplots got compressed as the node size increased, indicating that the variance among replicates was reduced. The performance of <italic>bAIcis</italic> was quite stable and not largely affected by the node size. Comparing figures vertically, we can see that patterns change mostly due to the network type rather than topology structure, although the average performance goes down on a small scale in scale-free topology. <italic>bAIcis</italic> still emerges as the winner in the mixed network for all three metrics.</p>
      </sec>
      <sec id="s011">
        <title>4.1.2. Computation time</title>
        <p>Computation time was calculated by using the mean and standard deviation across 20 replications as summarized statistics (<xref rid="tb2" ref-type="table">Table 2</xref> and <xref ref-type="fig" rid="f4">Fig. 4d</xref>). No dramatic difference was observed in running time between continuous and discrete networks when sample size was 50 and 200, except <italic>bnFinder</italic> whose running time was considerably reduced in handling discrete networks (<xref rid="tb2" ref-type="table">Table 2</xref>). Among all, <italic>bnlearn</italic> and <italic>pcalg</italic> held the fastest completion times that were in milliseconds; whereas <italic>bnFinder</italic> and <italic>deal</italic> were the most time-consuming algorithms. As expected, the computation cost was higher as the sample size or the node number increased. The increase varied depending on the different algorithms and network types (<xref ref-type="fig" rid="f4">Fig. 4d</xref>). <italic>Rimbanet</italic> and <italic>bnlearn</italic> were unaffected by the sample size and both display a plateau in each subfigure of <xref ref-type="fig" rid="f4">Figure 4d</xref>. Regarding <italic>bAIcis</italic>, it took from 30 seconds to learn a 50-samples network to 5 minutes to learn a 1000-samples network, with 20 nodes.</p>
        <table-wrap id="tb2" orientation="portrait" position="float">
          <label>Table 2.</label>
          <caption>
            <p>Summary for Computation Time (in Seconds) Among Bayesian Network Tools for Synthetic Networks of Random Topology Structure with 20 Nodes in Continuous and Discrete Networks</p>
          </caption>
          <!--OASIS TABLE HERE-->
          <table frame="hsides" rules="groups">
            <colgroup>
              <col align="left"/>
              <col align="left"/>
              <col align="left"/>
              <col align="left"/>
              <col align="left"/>
              <col align="left"/>
              <col align="left"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="3" align="left" valign="bottom">Sample size<italic toggle="yes"><break/></italic>Bayesian tool</th>
                <th colspan="6" align="center" valign="bottom">Network type<hr/></th>
              </tr>
              <tr>
                <th colspan="3" align="center" valign="bottom">Continuous<hr/></th>
                <th colspan="3" align="center" valign="bottom">Discrete<hr/></th>
              </tr>
              <tr>
                <th align="center" valign="bottom">50</th>
                <th align="center" valign="bottom">200</th>
                <th align="center" valign="bottom">1000</th>
                <th align="center" valign="bottom">50</th>
                <th align="center" valign="bottom">200</th>
                <th align="center" valign="bottom">1000</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" valign="bottom">bAIcis</td>
                <td align="center" valign="bottom">34.27 (3.839)</td>
                <td align="center" valign="bottom">44.72 14.856</td>
                <td align="center" valign="bottom">61.07 (30.979)</td>
                <td align="center" valign="bottom">35.4 (7.674)</td>
                <td align="center" valign="bottom">48.26 (8.208)</td>
                <td align="center" valign="bottom">319.14 (61.843)</td>
              </tr>
              <tr>
                <td align="left" valign="bottom">bnFinder</td>
                <td align="center" valign="bottom">8326.7 (1223.209)</td>
                <td align="center" valign="bottom">30891.56 (2269.565)</td>
                <td align="center" valign="bottom">158702.03 (10325.297)</td>
                <td align="center" valign="bottom">248.1 (44.935)</td>
                <td align="center" valign="bottom">767.12 (131.194)</td>
                <td align="center" valign="bottom">3213.06 (465.16)</td>
              </tr>
              <tr>
                <td align="left" valign="bottom">bnlearn</td>
                <td align="center" valign="bottom">0.02 (0.006)</td>
                <td align="center" valign="bottom">0.03 (0.012)</td>
                <td align="center" valign="bottom">0.06 (0.022)</td>
                <td align="center" valign="bottom">0.02 (0.006)</td>
                <td align="center" valign="bottom">0.02 (0.005)</td>
                <td align="center" valign="bottom">0.03 (0.01)</td>
              </tr>
              <tr>
                <td align="left" valign="bottom">deal</td>
                <td align="center" valign="bottom">NA</td>
                <td align="center" valign="bottom">NA</td>
                <td align="center" valign="bottom">NA</td>
                <td align="center" valign="bottom">4616.59 (1074.237)</td>
                <td align="center" valign="bottom">5747.56 (1511.478)</td>
                <td align="center" valign="bottom">5644.55 (1339.394)</td>
              </tr>
              <tr>
                <td align="left" valign="bottom">pcalg</td>
                <td align="center" valign="bottom">0.02 (0.005)</td>
                <td align="center" valign="bottom">0.02 (0.008)</td>
                <td align="center" valign="bottom">0.03 (0.018)</td>
                <td align="center" valign="bottom">0.05 (0.007)</td>
                <td align="center" valign="bottom">0.11 (0.03)</td>
                <td align="center" valign="bottom">0.34 (0.169)</td>
              </tr>
              <tr>
                <td align="left" valign="bottom">Rimbanet</td>
                <td align="center" valign="bottom">9.4 (1.845)</td>
                <td align="center" valign="bottom">9.89 (1.367)</td>
                <td align="center" valign="bottom">10.85 (1.583)</td>
                <td align="center" valign="bottom">11.29 (0.947)</td>
                <td align="center" valign="bottom">11.22 (0.776)</td>
                <td align="center" valign="bottom">12.82 (0.578)</td>
              </tr>
              <tr>
                <td align="left" valign="bottom">sparsebn</td>
                <td align="center" valign="bottom">3.37 (0.258)</td>
                <td align="center" valign="bottom">3.33 (0.197)</td>
                <td align="center" valign="bottom">3.57 (0.551)</td>
                <td align="center" valign="bottom">3.04 (0.304)</td>
                <td align="center" valign="bottom">7.12 (0.925)</td>
                <td align="center" valign="bottom">30.88 (4.102)</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tf1">
              <p>The computation time is displayed as mean (stardard deviation) across 20 replicates.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
      </sec>
    </sec>
    <sec id="s012">
      <title>4.2. Large feature space</title>
      <p>The benchmarking study under large feature space with a node size at 500 and 2000 was conducted on <italic>bAIcis</italic>, <italic>bnlearn</italic>, and <italic>sparsebn</italic>. <xref ref-type="fig" rid="f3">Figure 3</xref> displays the integrated plot for metrics (in median) stratified by network type, topology structure, node number, and sample size. Because of the extremely large base of nonconnected relations, the FPR values for the three algorithms mostly all reached the bottom. Comparing algorithms on the other two metrics, <italic>bAIcis</italic> recovered more true edges and less false edges when compared with <italic>bnlearn</italic> and <italic>sparsebn</italic>; <italic>bnlearn</italic> failed to persist the good performance in large feature space; whereas <italic>sparsebn</italic> climbed steadily as more nodes were involved in the network. Computation time wise, <italic>bAIcis</italic> enabled its parallel functionality by running on distributed systems, and it was able to finish the network learning within 45 minutes; whereas the other two were running on one single CPU, and <italic>sparsebn</italic> turned to be more efficient than <italic>bnlearn</italic> on average. It took <italic>bnlearn</italic> more than 1 day to tackle the continuous networks with 2000 nodes, and thus the corresponding outcomes were left blank.</p>
      <fig id="f3" fig-type="figure" orientation="portrait" position="float">
        <label>FIG. 3.</label>
        <caption>
          <p>Median points of TPR (in square), precision (in triangle), and FPR (in cross) under large feature space data for three BN leaners: <italic>bAIcis</italic>, <italic>bnlearn</italic>, and <italic>sparsebn</italic>. It is an integrated figure composed of eight subfigures stratified by network type and topology structure (continuous random topology network, continuous scale-free topology network, discrete random topology network, and mixed random topology network) vertically and node size (500 and 2000) horizontally; in each subfigure, <italic>y</italic>-axis presents the metric value ranging from 0 to 1, <italic>x</italic>-axis shows the compared BN tools, and median points are further stratified by sample size (50 in brown, 200 in green and 1000 in yellow). The variance among the replicates was extremely small and hence was not shown. The outcomes for <italic>bnlearn</italic> on the continuous networks with 2000 nodes are left blank due to its unexpected long running time.</p>
        </caption>
        <graphic xlink:href="cmb.2019.0210_figure3"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s013">
    <title>5. Discussion</title>
    <p>We have conducted a benchmark study to evaluate and compare the performance of the BN learners to recover causal structure under synthetic data against various settings of network types, topology structures, numbers of nodes, and sample size. <italic>bAIcis</italic> was capable of handling networks in both small and large feature regimes effectively and supported discrete, continuous, and mixed networks. Further, the complexity of the network structure impacted <italic>bAIcis</italic> little as it had stable performance in the accuracy of structure recovery (<xref ref-type="fig" rid="f4">Fig. 4</xref>). Although the performance in scale-free structure was slightly lower, it is mostly due to the unexpectedly large number of parents for certain nodes.</p>
    <fig id="f4" fig-type="figure" orientation="portrait" position="float">
      <label>FIG. 4.</label>
      <caption>
        <p>Boxplots of metrics under small feature space data for all BN learners. Panels represent the outputs from TPR, precision, FPR, and computation time in log scale (in seconds). Each panel is an integrated figure composed of 12 subfigures stratified by network type and topology structure (continuous random topology network, continuous scalefree topology network, discrete random topology network, and mixed random topology network) vertically and node size (10, 20 and 50) horizontally; in each subfigure, boxplots are further stratified by sample size (50 in brown, 200 in green, and 1000 in yellow).</p>
      </caption>
      <graphic xlink:href="cmb.2019.0210_figure4"/>
    </fig>
    <p>The majority of the BN algorithms, including <italic>bAIcis</italic>, gained power to identify edge connections without sacrificing precision as the sample size increased, whereas <italic>bnFinder</italic> and <italic>pcalg</italic> failed to differentiate between true and false connections when more observations were available, as observed with a decrease in precision and an increase in FPR.</p>
    <p><italic>bAIcis</italic> performed superior to the other open-source BN tools under the majority of simulation settings by identifying directed edges with high TPR and precision and low FPR values. The superiority was stronger in large feature spaces. Among three metrics, <italic>bAIcis</italic> outperformed the most in precision, which is usually considered as the major feature in real-life applications where the ground truth is unknown. High precision achieved by <italic>bAIcis</italic> indicates that the networks recovered by it are of high fidelity and have meaningful links between the variables.</p>
    <p>Algorithm computation time in both <italic>bnlearn</italic> and <italic>pcalg</italic> was quite superior to the others, whereas the running time for <italic>bnFinder</italic> gained dramatically as the node size increased. Although <italic>bAIcis</italic> was not the fastest algorithm in running time for small networks, the time spent, less than 5 minutes, is still considered as reasonable and acceptable. If the <italic>bAIcis</italic> parallelization functionality is enabled, the software can run on a distributed system and, hence, the spent time would be dramatically reduced.</p>
    <p>Although not evaluated in this study, <italic>bAIcis</italic> possesses several other differentiating features that enlarge its real application and give the flexibility on the postnetwork analysis and network representation. For example, <italic>bAIcis</italic> provides comprehensive output containing the estimated parameters of all predicted dependencies. The edge matrix can be easily converted to a topologic graph. Moreover, <italic>bAIcis</italic> can be run from an R-library wrapper that automates the run as well as post-BN data processing, for example, network visualization in Cystoscape or igraph.</p>
    <p>This benchmarking study gives a comprehensive evaluation on the performance of BN structure learners and demonstrates the advantages of <italic>bAIcis</italic> compared with open-source BN learners. Since the synthetic data were generated by linear SEMs, it is hard to generalize the results on nonlinear relationships. But based on the study results, we can conclude that <italic>bAIcis</italic> (1) supports continuous, discrete, and mixed networks and it performs stably and effectively for both small and large feature space; (2) achieves high precision; and (3) is implemented with parallel-computing capability that allows it to run in both multicore desktops and distributed systems.</p>
  </sec>
  <sec sec-type="conclusion" id="s014">
    <title>6. Conclusion</title>
    <p>BNs provide an alternative paradigm for statistical learning where causal relationships connecting various data features may open up possibilities for either hypothetical or real-life interventions into the studied system. BNs also allow for evidential reasoning by simulation or propagation through the network connections. BNs have wide real-world application across a variety of scientific and industrial areas, for example, learning gene regulation in life sciences, understanding most optimal strategies in patient care, and identifying root causes of poor clinical outcomes and phenotypes. From the benchmarking study, we can conclude that <italic>bAIcis</italic> is one of the most accurate BN structural learners compared with a number of publicly available algorithms. Moreover, the <italic>bAIcis</italic> software is effectively parallelized on a distributed system and hence can manage extremely large datasets that are currently common in life sciences, including genomics and multi-omics data.</p>
  </sec>
</body>
<back>
  <sec id="s015" sec-type="COI-statement">
    <title>Author Disclosure Statement</title>
    <p>The authors declare there are no competing financial interests.</p>
  </sec>
  <ref-list content-type="parsed">
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Aragam</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Gu</surname><given-names>J.</given-names></name>, and <name name-style="western"><surname>Zhou</surname><given-names>Q.</given-names></name></person-group><year>2017</year> Learning large-scale Bayesian networks with the sparsebn package. arXiv preprint arXiv:1703.04025</mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Baldi</surname><given-names>P.</given-names></name>, and <name name-style="western"><surname>Long</surname><given-names>A.D.</given-names></name></person-group><year>2001</year><article-title>A Bayesian framework for the analysis of microarray expression data: Regularized t-test and statistical inferences of gene changes</article-title>. <source>Bioinformatics</source><volume>17</volume>, <fpage>509</fpage>–<lpage>519</lpage><pub-id pub-id-type="pmid">11395427</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Boettcher</surname><given-names>S.</given-names></name>, and <name name-style="western"><surname>Dethlefsen</surname><given-names>C.</given-names></name></person-group><year>2003</year><article-title>deal: A package for learning Bayesian networks</article-title>. <source>J. Stat. Softw. Articles</source><volume>8</volume>, <fpage>1</fpage>–<lpage>40</lpage></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Carvalho</surname><given-names>A.M.</given-names></name></person-group><year>2009</year> Scoring functions for learning Bayesian networks. <italic>Inesc-id Tec. Rep</italic> 12</mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Chambliss</surname><given-names>D.F.</given-names></name>, and <name name-style="western"><surname>Schutt</surname><given-names>R.K.</given-names></name></person-group><year>2018</year><source>Making Sense of the Social World: Methods of Investigation</source>. Sage Publications, Incorporated</mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Chickering</surname><given-names>D.M.</given-names></name></person-group><year>1996</year><article-title>Learning Bayesian networks is NP-complete,</article-title><fpage>121</fpage>–<lpage>130</lpage>. <italic>In</italic> Fisher, D., and Lenz, H.J. eds. <source>Learning from Data: Artificial Intelligence and Statistics V</source>. <publisher-name>Springer, New York</publisher-name>, <publisher-loc>NY</publisher-loc></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chickering</surname><given-names>D.M.</given-names></name></person-group><year>2002</year><article-title>Learning equivalence classes of Bayesian-network structures</article-title>. <source>J. Mach. Learn. Res</source>. <volume>2</volume>, <fpage>445</fpage>–<lpage>498</lpage></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Claassen</surname><given-names>T.</given-names></name>, and <name name-style="western"><surname>Heskes</surname><given-names>T.</given-names></name></person-group><year>2012</year> A Bayesian approach to constraint based causal inference. arXiv preprint arXiv:1210.4866</mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Farasat</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Nikolaev</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Srihari</surname><given-names>S.N.</given-names></name>, <etal>et al.</etal></person-group><year>2015</year><article-title>Probabilistic graphical models in modern social network analysis. <italic>Soc. Netw. Anal</italic></article-title>. <source>Mining</source><volume>5</volume>, <fpage>62</fpage></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Frolova</surname><given-names>A.</given-names></name>, and <name name-style="western"><surname>Wilczyński</surname><given-names>B.</given-names></name></person-group><year>2018</year><article-title>Distributed Bayesian networks reconstruction on the whole genome scale</article-title>. <source>PeerJ</source><volume>6</volume>, <fpage>e5692</fpage><pub-id pub-id-type="pmid">30364537</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hartemink</surname><given-names>A.J.</given-names></name>, <name name-style="western"><surname>Gifford</surname><given-names>D.K.</given-names></name>, <name name-style="western"><surname>Jaakkola</surname><given-names>T.S.</given-names></name>, <etal>et al.</etal></person-group><year>2001</year><article-title>Using graphical models and genomic expression to statistically validate models of genetic regulatory networks</article-title>. <source>Pac. Symp. Biocomput</source>. <volume>6</volume>, <fpage>422</fpage>–<lpage>433</lpage></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Heckerman</surname><given-names>D.</given-names></name></person-group><year>1998</year><article-title>A tutorial on learning with Bayesian networks,</article-title><fpage>301</fpage>–<lpage>354</lpage>. <source>In Learning in Graphical Models</source>. <publisher-name>Springer</publisher-name>, <publisher-loc>Dordrecht</publisher-loc></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kalisch</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Mächler</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Colombo</surname><given-names>D.</given-names></name>, <etal>et al.</etal></person-group><year>2012</year><article-title>Causal inference using graphical models with the R package pcalg</article-title>. <source>J. Stat. Softw</source>. <volume>47</volume>, <fpage>1</fpage>–<lpage>26</lpage></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Morgan</surname><given-names>S.L.</given-names></name>, and <name name-style="western"><surname>Winship</surname><given-names>C.</given-names></name></person-group><year>2014</year><italic>Counterfactuals and Causal Inference</italic> Cambridge University Press</mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nelson</surname><given-names>P.T.</given-names></name>, <name name-style="western"><surname>Baldwin</surname><given-names>D.A.</given-names></name>, <name name-style="western"><surname>Scearce</surname><given-names>L.M.</given-names></name>, <etal>et al.</etal></person-group><year>2004</year><article-title>Microarray-based, high-throughput gene expression profiling of microRNAs</article-title>. <source>Nat. Methods</source><volume>1</volume>, <fpage>155</fpage>–<lpage>161</lpage><pub-id pub-id-type="pmid">15782179</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pearl</surname><given-names>J.</given-names></name></person-group><year>2009a</year><article-title>Causal inference in statistics: An overview</article-title>. <source>Stat. Surv</source>. <volume>3</volume>, <fpage>96</fpage>–<lpage>146</lpage></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Pearl</surname><given-names>J.</given-names></name></person-group><year>2009b</year><source>Causality</source>. <publisher-name>Cambridge University Press</publisher-name></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pearl</surname><given-names>J.</given-names></name></person-group><year>2010</year><article-title>An introduction to causal inference</article-title>. <source>Int. J. Biostat</source>. <volume>6</volume>, Article 7</mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Pearl</surname><given-names>J.</given-names></name></person-group><year>2011</year> Bayesian networks. UCLA: Department of Statistics</mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pearl</surname><given-names>J.</given-names></name></person-group><year>2013</year><article-title>Linear models: A useful “microscope” for causal analysis</article-title>. <source>Journal of Causal Inference</source>. <volume>1</volume>, <fpage>155</fpage>–<lpage>170</lpage></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Pearl</surname><given-names>J.</given-names></name>, and <name name-style="western"><surname>Bareinboim</surname><given-names>E.</given-names></name></person-group><year>2011</year> Transportability of causal and statistical relations: A formal approach, 540–547. <italic>In 2011 IEEE 11th International Conference on Data Mining Workshops</italic></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Pearl</surname><given-names>J.</given-names></name>, and <name name-style="western"><surname>Verma</surname><given-names>T.S.</given-names></name></person-group><year>1995</year> A theory of inferred causation, 789–811. <italic>In Logic, Methodology and Philosophy of Science IX</italic>, <italic>Studies in Logic and the Foundations of Mathematics</italic>, vol. 134. Elsevier</mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Rothman</surname><given-names>K.J.</given-names></name>, <name name-style="western"><surname>Greenland</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Lash</surname><given-names>T.L.</given-names></name>, <etal>et al.</etal></person-group><year>2008</year><source>Modern Epidemiology</source>, <edition>3rd</edition> ed., <publisher-name>Wolters Kluwer Health/Lippincott Williams &amp; Wilkins Philadelphia</publisher-name></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Scutari</surname><given-names>M.</given-names></name></person-group><year>2010</year><article-title>Learning Bayesian networks with the bnlearn R package</article-title>. <source>J. Stat. Softw. Articles</source><volume>35</volume>, <fpage>1</fpage>–<lpage>22</lpage></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Scutari</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Graafland</surname><given-names>C.E.</given-names></name>, and <name name-style="western"><surname>Gutiérrez</surname><given-names>J.M.</given-names></name></person-group><year>2018</year> Who learns better Bayesian network structures: Constraint-based, score-based or hybrid algorithms? arXiv preprint arXiv:1805.11908</mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Spirtes</surname><given-names>P.</given-names></name></person-group><year>2010</year><article-title>Introduction to causal inference</article-title>. <source>J. Mach. Learn. Res</source>. <volume>11</volume>, <fpage>1643</fpage>–<lpage>1662</lpage></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Spirtes</surname><given-names>P.</given-names></name>, and <name name-style="western"><surname>Glymour</surname><given-names>C.</given-names></name></person-group><year>1991</year><article-title>An algorithm for fast recovery of sparse causal graphs</article-title>. <source>Soc. Sci. Comput. Rev</source>. <volume>9</volume>, <fpage>62</fpage>–<lpage>72</lpage></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Triantafillou</surname><given-names>S.</given-names></name>, and <name name-style="western"><surname>Tsamardinos</surname><given-names>I.</given-names></name></person-group><year>2016</year> Score-based vs constraint-based causal learning in the presence of confounders, 59–67. <italic>In CFA@ UAI</italic></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tsamardinos</surname><given-names>I.</given-names></name>, <name name-style="western"><surname>Brown</surname><given-names>L.E.</given-names></name>, and <name name-style="western"><surname>Aliferis</surname><given-names>C.F.</given-names></name></person-group><year>2006</year><article-title>The max-min hill-climbing Bayesian network structure learning algorithm</article-title>. <source>Mach. learn</source>. <volume>65</volume>, <fpage>31</fpage>–<lpage>78</lpage></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Von Storch</surname><given-names>H.</given-names></name></person-group><year>1999</year> Misuses of statistical analysis in climate research, 11–26. In Von Storch, H., and Havarra A., eds. <italic>Analysis of Climate Variability</italic>, Springer, Berlin, Heidelberg</mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Whitney</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>White</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Walsh</surname><given-names>S.</given-names></name>, <etal>et al.</etal></person-group><year>2011</year><article-title> Bayesian networks for social modeling, 227–235</article-title>. <conf-name><italic>In International Conference on Social Computing, Behavioral-Cultural Modeling, and Prediction</italic></conf-name><publisher-name>Springer</publisher-name>, <publisher-loc>Berlin, Heidelberg</publisher-loc></mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wright</surname><given-names>S.</given-names></name></person-group><year>1921</year><article-title>Systems of mating. I. the biometric relations between parent and offspring</article-title>. <source>Genetics</source><volume>6</volume>, <fpage>111</fpage><pub-id pub-id-type="pmid">17245958</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xiao</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Lv</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Zhao</surname><given-names>H.</given-names></name>, <etal>et al.</etal></person-group><year>2015</year><article-title>Predicting the functions of long noncoding RNAs using RNA-seq based on Bayesian network. <italic>BioMed Res</italic></article-title>. <source>Int</source>. <volume>2015</volume>, <fpage>839590</fpage></mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>J.</given-names></name>, and <name name-style="western"><surname>Liu</surname><given-names>L.</given-names></name></person-group><year>2016</year> A review on algorithms for constraint-based causal discovery. arXiv preprint arXiv:1611.03977</mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Lum</surname><given-names>P.Y.</given-names></name>, <name name-style="western"><surname>Lamb</surname><given-names>J.</given-names></name>, <etal>et al.</etal></person-group><year>2004</year><article-title>An integrative genomics approach to the reconstruction of gene networks in segregating populations</article-title>. <source>Cytogenet. Genome Res</source>. <volume>105</volume>, <fpage>363</fpage>–<lpage>374</lpage><pub-id pub-id-type="pmid">15237224</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
