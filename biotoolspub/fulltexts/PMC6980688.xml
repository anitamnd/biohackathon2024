<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS Comput. Biol</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-id journal-id-type="pmc">ploscomp</journal-id>
    <journal-title-group>
      <journal-title>PLoS Computational Biology</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1553-734X</issn>
    <issn pub-type="epub">1553-7358</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6980688</article-id>
    <article-id pub-id-type="pmid">31929520</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pcbi.1007598</article-id>
    <article-id pub-id-type="publisher-id">PCOMPBIOL-D-19-00708</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Physics</subject>
          <subj-group>
            <subject>Acoustics</subject>
            <subj-group>
              <subject>Bioacoustics</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Bioacoustics</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Physics</subject>
          <subj-group>
            <subject>Acoustics</subject>
            <subj-group>
              <subject>Acoustic Signals</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Physics</subject>
          <subj-group>
            <subject>Acoustics</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Data Visualization</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Human Factors Engineering</subject>
          <subj-group>
            <subject>Man-Computer Interface</subject>
            <subj-group>
              <subject>Graphical User Interfaces</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Computer Architecture</subject>
          <subj-group>
            <subject>User Interfaces</subject>
            <subj-group>
              <subject>Graphical User Interfaces</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Psychology</subject>
          <subj-group>
            <subject>Behavior</subject>
            <subj-group>
              <subject>Animal Behavior</subject>
              <subj-group>
                <subject>Animal Communication</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Social Sciences</subject>
        <subj-group>
          <subject>Psychology</subject>
          <subj-group>
            <subject>Behavior</subject>
            <subj-group>
              <subject>Animal Behavior</subject>
              <subj-group>
                <subject>Animal Communication</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Zoology</subject>
          <subj-group>
            <subject>Animal Behavior</subject>
            <subj-group>
              <subject>Animal Communication</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Organisms</subject>
          <subj-group>
            <subject>Eukaryota</subject>
            <subj-group>
              <subject>Animals</subject>
              <subj-group>
                <subject>Vertebrates</subject>
                <subj-group>
                  <subject>Amniotes</subject>
                  <subj-group>
                    <subject>Mammals</subject>
                    <subj-group>
                      <subject>Marine Mammals</subject>
                      <subj-group>
                        <subject>Whales</subject>
                        <subj-group>
                          <subject>Sperm Whales</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Marine Biology</subject>
          <subj-group>
            <subject>Marine Mammals</subject>
            <subj-group>
              <subject>Whales</subject>
              <subj-group>
                <subject>Sperm Whales</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Earth Sciences</subject>
        <subj-group>
          <subject>Marine and Aquatic Sciences</subject>
          <subj-group>
            <subject>Marine Biology</subject>
            <subj-group>
              <subject>Marine Mammals</subject>
              <subj-group>
                <subject>Whales</subject>
                <subj-group>
                  <subject>Sperm Whales</subject>
                </subj-group>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Psychology</subject>
          <subj-group>
            <subject>Behavior</subject>
            <subj-group>
              <subject>Animal Behavior</subject>
              <subj-group>
                <subject>Echolocation</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Social Sciences</subject>
        <subj-group>
          <subject>Psychology</subject>
          <subj-group>
            <subject>Behavior</subject>
            <subj-group>
              <subject>Animal Behavior</subject>
              <subj-group>
                <subject>Echolocation</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Zoology</subject>
          <subj-group>
            <subject>Animal Behavior</subject>
            <subj-group>
              <subject>Echolocation</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DetEdit: A graphical user interface for annotating and editing events detected in long-term acoustic monitoring data</article-title>
      <alt-title alt-title-type="running-head">DetEdit: A graphical user interface for annotating and editing acoustic data events</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-4096-742X</contrib-id>
        <name>
          <surname>Solsona-Berga</surname>
          <given-names>Alba</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Conceptualization</role>
        <role content-type="http://credit.casrai.org/">Data curation</role>
        <role content-type="http://credit.casrai.org/">Methodology</role>
        <role content-type="http://credit.casrai.org/">Software</role>
        <role content-type="http://credit.casrai.org/">Writing – original draft</role>
        <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
        <xref ref-type="corresp" rid="cor001">*</xref>
        <xref ref-type="aff" rid="aff001"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Frasier</surname>
          <given-names>Kaitlin E.</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Conceptualization</role>
        <role content-type="http://credit.casrai.org/">Data curation</role>
        <role content-type="http://credit.casrai.org/">Funding acquisition</role>
        <role content-type="http://credit.casrai.org/">Methodology</role>
        <role content-type="http://credit.casrai.org/">Software</role>
        <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff001"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Baumann-Pickering</surname>
          <given-names>Simone</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Data curation</role>
        <role content-type="http://credit.casrai.org/">Software</role>
        <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff001"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9686-035X</contrib-id>
        <name>
          <surname>Wiggins</surname>
          <given-names>Sean M.</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Conceptualization</role>
        <role content-type="http://credit.casrai.org/">Software</role>
        <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff001"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5418-9799</contrib-id>
        <name>
          <surname>Hildebrand</surname>
          <given-names>John A.</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Conceptualization</role>
        <role content-type="http://credit.casrai.org/">Data curation</role>
        <role content-type="http://credit.casrai.org/">Funding acquisition</role>
        <role content-type="http://credit.casrai.org/">Methodology</role>
        <role content-type="http://credit.casrai.org/">Software</role>
        <role content-type="http://credit.casrai.org/">Supervision</role>
        <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff001"/>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <addr-line>Scripps Institution of Oceanography, University of California, San Diego, La Jolla, California, United States of America</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Schneidman-Duhovny</surname>
          <given-names>Dina</given-names>
        </name>
        <role>Editor</role>
        <xref ref-type="aff" rid="edit1"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>Hebrew University of Jerusalem, ISRAEL</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p>The authors have declared that no competing interests exist</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>asolsonaberga@ucsd.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>13</day>
      <month>1</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <month>1</month>
      <year>2020</year>
    </pub-date>
    <volume>16</volume>
    <issue>1</issue>
    <elocation-id>e1007598</elocation-id>
    <history>
      <date date-type="received">
        <day>3</day>
        <month>5</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>13</day>
        <month>12</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2020 Solsona-Berga et al</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Solsona-Berga et al</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pcbi.1007598.pdf"/>
    <abstract>
      <p>Passive acoustic monitoring has become an important data collection method, yielding massive datasets replete with biological, environmental and anthropogenic information. Automated signal detectors and classifiers are needed to identify events within these datasets, such as the presence of species-specific sounds or anthropogenic noise. These automated methods, however, are rarely a complete substitute for expert analyst review. The ability to visualize and annotate acoustic events efficiently can enhance scientific insights from large, previously intractable datasets. A MATLAB-based graphical user interface, called <italic>DetEdit</italic>, was developed to accelerate the editing and annotating of automated detections from extensive acoustic datasets. This tool is highly-configurable and multipurpose, with uses ranging from annotation and classification of individual signals or signal-clusters and evaluation of signal properties, to identification of false detections and false positive rate estimation. <italic>DetEdit</italic> allows users to step through acoustic events, displaying a range of signal features, including time series of received levels, long-term spectral averages, time intervals between detections, and scatter plots of peak frequency, RMS, and peak-to-peak received levels. Additionally, it displays either individual, or averaged sound pressure waveforms, and power spectra within each acoustic event. These views simultaneously provide analysts with signal-level detail and encounter-level context. <italic>DetEdit</italic> creates datasets of signal labels for further analyses, such as training classifiers and quantifying occurrence, abundances, or trends. Although designed for evaluating underwater-recorded odontocete echolocation click detections, <italic>DetEdit</italic> can be adapted to almost any stereotyped impulsive signal. Our software package complements available tools for the bioacoustic community and is provided open source at <ext-link ext-link-type="uri" xlink:href="https://github.com/MarineBioAcousticsRC/DetEdit">https://github.com/MarineBioAcousticsRC/DetEdit</ext-link>.</p>
    </abstract>
    <funding-group>
      <award-group id="award001">
        <funding-source>
          <institution>Southeast Fisheries Science Center - National Oceanic and Atmospheric Administration</institution>
        </funding-source>
        <award-id>NA15OAR4320071</award-id>
        <principal-award-recipient>
          <name>
            <surname>Frasier</surname>
            <given-names>Kaitlin E.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award002">
        <funding-source>
          <institution>Consortium for Ocean Leadership, Inc.</institution>
        </funding-source>
        <award-id>SA 12-10</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5418-9799</contrib-id>
          <name>
            <surname>Hildebrand</surname>
            <given-names>John A.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award003">
        <funding-source>
          <institution>Consortium for Ocean Leadership, Inc.</institution>
        </funding-source>
        <award-id>SA 15-16</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5418-9799</contrib-id>
          <name>
            <surname>Hildebrand</surname>
            <given-names>John A.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award004">
        <funding-source>
          <institution>Consortium for Ocean Leadership, Inc.</institution>
        </funding-source>
        <award-id>SA 18-16</award-id>
        <principal-award-recipient>
          <name>
            <surname>Frasier</surname>
            <given-names>Kaitlin E.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <funding-statement>The work was supported by BP/Gulf of Mexico Research Initiative administered by Consortium for Ocean Leadership, Inc. (SA 12-10, SA 15-16, and SA 18-16, <ext-link ext-link-type="uri" xlink:href="https://oceanleadership.org">https://oceanleadership.org</ext-link>), and the Southeast Fisheries Science Center-National Oceanic and Atmospheric Administration (NA15OAR4320071, <ext-link ext-link-type="uri" xlink:href="https://www.sefsc.noaa.gov">https://www.sefsc.noaa.gov</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="5"/>
      <table-count count="0"/>
      <page-count count="10"/>
    </counts>
    <custom-meta-group>
      <custom-meta>
        <meta-name>PLOS Publication Stage</meta-name>
        <meta-value>vor-update-to-uncorrected-proof</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>Publication Update</meta-name>
        <meta-value>2020-01-24</meta-value>
      </custom-meta>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>All relevant data are within the manuscript and its Supporting Information files.</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>All relevant data are within the manuscript and its Supporting Information files.</p>
  </notes>
</front>
<body>
  <disp-quote>
    <p>This is a <italic>PLOS Computational Biology</italic> Software paper.</p>
  </disp-quote>
  <sec sec-type="intro" id="sec001">
    <title>Introduction</title>
    <p>A variety of animals produce species-specific acoustic signals, including marine mammals, fish, crustaceans, primates, bats, birds, anurans, and insects [<xref rid="pcbi.1007598.ref001" ref-type="bibr">1</xref>–<xref rid="pcbi.1007598.ref009" ref-type="bibr">9</xref>]. Acoustic analysis has become a standard method in studies of animal vocal communication, and manual detection of acoustic cues was initially the common practice. However, advances in recording hardware speeds, battery life and data storage capacity have increased the rate of acoustic data accumulation to a point where reliance on manual analysis has become unmanageable. This is the case for bottom-mounted acoustic recorders that record 2-byte samples at 200,000 samples/sec autonomously for months to over a year, yielding up to 16 terabytes of acoustic data storage from a single deployment. Automated detection and classification algorithms have become necessary for the analysis process. These algorithms provide more consistent and comparable estimates throughout a study period and across studies when processing long-term time series. They are less prone to bias than human analysts, and can be quantified more objectively. However, they cannot be used without supervision, and typically require performance evaluation or correction at some point in the processing pipeline. For instance, a detection algorithm’s performance must be estimated across a representative range of recording conditions. Labels may need to be applied to detections to train automatic classification routines, and misclassifications may need to be quantified or corrected [<xref rid="pcbi.1007598.ref010" ref-type="bibr">10</xref>]. In the absence of tools capable of performing these tasks across large datasets, the tasks are often done manually by an analyst sampling a subset of data which may not be representative of the full dataset. Further, because they are time consuming to estimate, performance metrics are often only measured for a few datasets and then assumed to apply more broadly. Despite the potential of automated classification tools to produce reliable, repeatable decisions and results from large datasets, manual review remains an important part of the process for additional scientific insights, since analysts are best able to judge the context-dependent nature of biological data.</p>
    <p>The aim of our software described herein is to facilitate efficient exploration, characterization, and correction of signal detections and classifications in large, long-term acoustic datasets. A custom graphical user interface (GUI) tool, <italic>DetEdit</italic>, is presented to accelerate and enhance the process of acoustic big data analysis by combining signal-level detail with encounter-level context. This tool will facilitate training of machine learning algorithms for species classification. It can be applied to stereotyped signals that are characterized by spectral shape, such as underwater sounds produced by odontocetes, crustaceans, sonar, and ships, or terrestrial sounds, including calls made by bats and swiftlets. As examples, the use of <italic>DetEdit</italic> is illustrated with two case studies of odontocetes from long-term time series.</p>
  </sec>
  <sec sec-type="materials|methods" id="sec002">
    <title>Design and implementation</title>
    <sec id="sec003">
      <title>Overview</title>
      <p>The <italic>DetEdit</italic> package provides a set of tools designed to parse, manipulate, and visualize acoustic detections in a workflow format using a user-interface developed in MATLAB (Mathworks, Natick, MA). The package, which depends on the core <monospace>detEdit</monospace> function, implements a hierarchical pipeline that incorporates data preprocessing, visualization, and manipulation tools (<xref ref-type="fig" rid="pcbi.1007598.g001">Fig 1</xref>). The main functions and files found in the repository with a brief summary of their usage is described at <ext-link ext-link-type="uri" xlink:href="https://github.com/MarineBioAcousticsRC/DetEdit/wiki/How-It-Works#Table">https://github.com/MarineBioAcousticsRC/DetEdit/wiki/How-It-Works#Table</ext-link>. The processing pipeline begins by using <italic>DetEdit</italic> to navigate through successive encounters, with modifications made to annotation matrices based on user decisions. This process is repeated as needed and also supports manual assessment of false positive rates.</p>
      <fig id="pcbi.1007598.g001" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1007598.g001</object-id>
        <label>Fig 1</label>
        <caption>
          <title>Schematic representation of the <italic>DetEdit</italic> workflow.</title>
          <p>The workflow is a multi-step process where the user: (1) creates a Long Term Spectral Average (LTSA, see Workflow section for details) of the complete long-term acoustic recording, (2) provides detections to create a TPWS (start Time, Peak-to-peak received sound pressure level, Waveform, Spectrum) file or executes a simple energy detector, (3) creates LTSAs per bout, (4) executes the interface <italic>DetEdit</italic> to annotate detections, and (5) identify false positive detections. If desired, histograms and plots are created to summarize the annotated data. The schematic workflow includes functions (white boxes), data files (gray boxes), and optional steps (dashed lines).</p>
        </caption>
        <graphic xlink:href="pcbi.1007598.g001"/>
      </fig>
      <p>A set of pre-processing tools are provided to produce the input required to use <monospace>detEdit</monospace>, the central user-interface tool. A generic impulse detector (<monospace>Edetect</monospace>) is provided to detect signals and produce matrices of detection parameters for input into <italic>DetEdit</italic>. Others (<monospace>mkLtsa</monospace>, <monospace>mkLTSAsessions</monospace>), compute spectral averages. The primary tool, <monospace>detEdit</monospace>, allows users to visualize the data with specific acoustic features, interactively explore and manually annotate data. Other functions, including <monospace>modDet</monospace> and <monospace>summaryParams</monospace> are post-processing tools, manipulating detections based on user annotations and producing plot-based data summaries.</p>
    </sec>
    <sec id="sec004">
      <title>Workflow</title>
      <p>The process of editing detections involves following several steps to create the parameters needed for the interface (<xref ref-type="fig" rid="pcbi.1007598.g001">Fig 1</xref>):</p>
      <p><bold>Step 1: Create LTSA files</bold>. The <italic>DetEdit</italic> package relies on <monospace>mkLtsa</monospace> to read audio files (wav format) and compress these data into long-term spectral averages (LTSA), which are power spectra calculated at regular intervals for the entire audio file [<xref rid="pcbi.1007598.ref011" ref-type="bibr">11</xref>]. LTSA are produced for the audio files by specifying the time average and frequency-bin size. They facilitate exploration and provide an easily visualized overview for long-term acoustic data.</p>
      <p><bold>Step 2: Create TPWS files</bold>. The input to the <italic>DetEdit</italic> GUI is a MATLAB binary file labeled as “TPWS” (Time, Peak-to-peak received levels, Waveform and Spectra parameters) that contains matrices of the acoustic detection parameters. These matrices can be created manually by the user or with <monospace>make_TPWS</monospace> which builds the following four primary variables to visualize detections in the interface:</p>
      <list list-type="bullet">
        <list-item>
          <p><monospace>MTT</monospace>: a vector of detection times.</p>
        </list-item>
        <list-item>
          <p><monospace>MPP</monospace>: a vector of peak-to-peak received levels.</p>
        </list-item>
        <list-item>
          <p><monospace>MSP</monospace>: a matrix of normalized spectra.</p>
        </list-item>
        <list-item>
          <p><monospace>MSN</monospace>: a matrix of normalized waveforms.</p>
        </list-item>
      </list>
      <p>If no detections are given, the package provides <monospace>Edetect</monospace> to assist in detecting acoustic events in audio files. This generic detector applies a configurable band pass filter and returns events that meet or exceed a minimum received level threshold and satisfy other configurable acoustic criteria.</p>
      <p><bold>Step 3: Create LTSA Sessions files</bold>. Detections are grouped in user-defined time bouts, defined as periods of stereotyped signals separated from prior and subsequent detections by a minimum specified time gap. <monospace>mkLTSAsessions</monospace> takes an LTSA and produces the following two variables needed to represent subsets of the LTSA per bout:</p>
      <list list-type="bullet">
        <list-item>
          <p><monospace>pt</monospace>: a vector of power spectral average start times.</p>
        </list-item>
        <list-item>
          <p><monospace>pwr</monospace>: a matrix of power spectral density averages.</p>
        </list-item>
      </list>
      <sec id="sec005">
        <title>Visualization, annotation, and manipulation of data</title>
        <p>After building the parameter files, and specifying the input directories and display parameters in a script (see <monospace>myDataSettings</monospace> as an example<monospace>),</monospace> the user evokes the interface by calling <monospace>detEdit</monospace>. Predefined parameters for eleven species of odontocetes (e.g. beaked whale, dolphin, and sperm whale) are provided in <monospace>initSpParams</monospace>. The data are organized and displayed in bouts of detections allowing users to annotate large batches of detections (see <ext-link ext-link-type="uri" xlink:href="https://github.com/MarineBioAcousticsRC/DetEdit/wiki/Getting-Started">https://github.com/MarineBioAcousticsRC/DetEdit/wiki/Getting-Started</ext-link> for more details). Seven panels are displayed to provide the signal detail and context needed to discriminate between different types of detections (<xref ref-type="fig" rid="pcbi.1007598.g002">Fig 2</xref>). The main interactive panel displays peak-to-peak received levels (RL<sub>pp</sub> dB re 1μPa) of detected signals, with the concurrent LTSA, and time between detections (time difference between one detected signal and the next). The concurrent waveforms, spectra, transformed version of root-mean-square (RL<sub>rms</sub> dB re 1μPa) and peak frequencies are displayed on additional interactive panels. RL<sub>rms</sub> summarizes the distribution of energy within a waveform, and is transformed to facilitate the annotation of signals of consistent shape but with varying amplitude (<xref ref-type="fig" rid="pcbi.1007598.g003">Fig 3A and 3B</xref>). Presuming that signals of a consistent shape will increase linearly in both RL<sub>pp</sub> and RL<sub>rms</sub>, the slope is arranged vertically, where transformed RL<sub>rms</sub> = RMS–correction factor* (RL<sub>pp</sub>−RL<sub>pp</sub> threshold) (<xref ref-type="fig" rid="pcbi.1007598.g003">Fig 3C and 3D</xref>). This transformation emphasizes values that deviate from the basic relationship. Generally for a vertical transformation when the increase in RL<sub>pp</sub> is the same as in RL<sub>rms</sub>, a correction factor equal to one is appropriate. Signal types that do not meet this linear increment required a different correction factor to enable the vertical arrangement.</p>
        <fig id="pcbi.1007598.g002" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1007598.g002</object-id>
          <label>Fig 2</label>
          <caption>
            <title>Visualization of acoustic data in the <italic>DetEdit</italic> interface.</title>
            <p>Seven panels are displayed with three over the event period: (A) RL<sub>pp</sub>, (B) LTSA, (C) time interval between detections; and four other showing various detection metrics and details: (D) normalized spectral density, (E) normalized waveforms, (F) RL<sub>pp</sub> versus transformed RL<sub>rms</sub>, and (G) peak frequencies versus transformed RL<sub>rms</sub> for sperm whale (<italic>Physeter macrocephalus</italic>) signal detections with true detections as blue, manually identified false detections of delphinid signals as red, and one detection using the selection tool displayed as black. All detections from the recording are shown in gray on the background if specified by the user to ease comparison of distributions across bouts. Customized classification thresholds are displayed as thin red lines (F and G).</p>
          </caption>
          <graphic xlink:href="pcbi.1007598.g002"/>
        </fig>
        <fig id="pcbi.1007598.g003" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1007598.g003</object-id>
          <label>Fig 3</label>
          <caption>
            <title>Conceptualization of transformed RL<sub>rms</sub>.</title>
            <p>Example signals of identical RL<sub>pp</sub> with low RMS indicating a signal characterized by few high amplitude cycles (A), and high RMS indicating a sustained signal with many cycles at high amplitude (B). (C) Signal A and B with varying RL<sub>pp</sub> plotted as a function of RL<sub>pp</sub> versus RL<sub>rms</sub> displaying a linear relationship, (D) vertical slope for transformed RL<sub>rms</sub>.</p>
          </caption>
          <graphic xlink:href="pcbi.1007598.g003"/>
        </fig>
        <p>Following a simple list of keyboard shortcut commands and the use of a paintbrush tool (see <ext-link ext-link-type="uri" xlink:href="https://github.com/ScrippsWhaleAcoustics/DetEdit/wiki/How-It-Works#Tools">https://github.com/MarineBioAcousticsRC/DetEdit/wiki/How-It-Works#Tools</ext-link> for more details), the user can parse the data by selecting single or multiple detections to interactively visualize the features or averaged features of the selection, and compare these parameters with the parameters of other detections within the displayed bout. Thresholds can be defined for peak frequency, transformed RL<sub>rms</sub> and RL<sub>pp</sub> to automate the process of annotating data (<xref ref-type="fig" rid="pcbi.1007598.g002">Fig 2F and 2G</xref>). Any detection lower than the selected thresholds will be automatically labeled and displayed as a false positive. All annotations can be reversed and labeled as true positives or specified signal types by using a palette of colors with the paintbrush tool (<xref ref-type="fig" rid="pcbi.1007598.g004">Fig 4</xref>). All changes made through the interactive interface are updated and stored in the corresponding annotation files. False detection files (FD.mat) contain all start times of signals labeled as false detections. Detection type files (ID.mat) contain all start times of signals labeled as a specified type of detection, with the corresponding color and integer value representing the species or category.</p>
        <fig id="pcbi.1007598.g004" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1007598.g004</object-id>
          <label>Fig 4</label>
          <caption>
            <title>Example of labeling different detected signal types.</title>
            <p>Specified signal types of odontocete species represented in different colors, <italic>Stenella</italic> spp. (pink), <italic>Grampus griseus</italic> (purple), and <italic>Mesoplodon europaeus</italic> (green).</p>
          </caption>
          <graphic xlink:href="pcbi.1007598.g004"/>
        </fig>
      </sec>
    </sec>
    <sec id="sec006">
      <title>Post-processing</title>
      <p>After manually annotating all detections, decisions stored in different annotation file types are used to modify the detection parameters stored in TPWS files. <monospace>modDet</monospace> excludes all false detections and ID detections (if specified by the user) from the TPWS files and displays exploratory plots as histograms of RL<sub>pp</sub>, peak frequency and time interval between detections for each file. The process of using <monospace>detEdit</monospace> and <monospace>modDet</monospace> can be repeated iteratively until all detections are labeled, or a sufficiently low percentage of false detections is obtained (<xref ref-type="fig" rid="pcbi.1007598.g001">Fig 1</xref>). For every iteration, new detection and annotation files are generated and stored together in a common directory to keep track of all changes.</p>
      <p>When specified using a simple keyboard shortcut commands, <monospace>detEdit</monospace> allows rapid estimation of a detector’s false positive rate across a systematic random sample of detections (<xref ref-type="fig" rid="pcbi.1007598.g005">Fig 5</xref>). The estimation procedure can be done at the signal-level where selected individual signals are sequentially evaluated as true or false within a bout. Alternatively, the procedure can be conducted at the level of a time-bin. This process consists of deciding if at least one signal within the defined interval is a true detection, whereby the entire bin is considered “true”. Signals or intervals evaluated for false positive rate estimation are selected systematically (every Nth detection) across a file. A matrix with the false positive rate per bout is built and stored in true detection files (TD.mat).</p>
      <fig id="pcbi.1007598.g005" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1007598.g005</object-id>
        <label>Fig 5</label>
        <caption>
          <title>Example of evaluating false positive rates at the signal-level.</title>
          <p>Detected signals being evaluated within the bout are shown in yellow. Evaluation is done in a consecutive manner, with the current signal marked with a yellow circle, and previously evaluated signals displayed as yellow dots.</p>
        </caption>
        <graphic xlink:href="pcbi.1007598.g005"/>
      </fig>
      <p>Finally, a data summary is given with histogram plots of RL<sub>pp</sub>, time interval between detections, and a time series of weekly presence of the true detections at the signal-level and the time-bin level (<xref ref-type="supplementary-material" rid="pcbi.1007598.s001">S1 Fig</xref>).</p>
    </sec>
  </sec>
  <sec sec-type="results" id="sec007">
    <title>Results</title>
    <p><italic>DetEdit</italic> has been used to analyze acoustic recordings of odontocete vocalizations [<xref rid="pcbi.1007598.ref012" ref-type="bibr">12</xref>–<xref rid="pcbi.1007598.ref015" ref-type="bibr">15</xref>]. The utility of <italic>DetEdit</italic> for facilitating analyses of large datasets is demonstrated with two case studies that performed species-level analyses of large acoustic datasets. A variety of reproducible results from these examples are included in the repository and described at <ext-link ext-link-type="uri" xlink:href="https://github.com/ScrippsWhaleAcoustics/DetEdit/wiki/Getting-Started">https://github.com/MarineBioAcousticsRC/DetEdit/wiki/Getting-Started</ext-link></p>
    <sec id="sec008">
      <title>Interactive visualization of acoustic features to discriminate odontocete species</title>
      <p>The acoustic features displayed in the panels of the interface are helpful parameters for distinguishing signals from multiple odontocete species. Odontocetes produce highly-directional echolocation clicks with species-specific characteristic spectral shape and duration. These characteristics can be distinguished from the panels that show averaged normalized spectral density and waveforms of all signals within bouts, and can be compared with individual selected detections.</p>
      <p>Odontocetes often echolocate at a characteristic rate [<xref rid="pcbi.1007598.ref016" ref-type="bibr">16</xref>]. The time interval between detections (<xref ref-type="fig" rid="pcbi.1007598.g002">Fig 2C</xref>), also known as inter-click-interval (ICI), is most variable when multiple animals are recorded simultaneously and the received impulse trains become interleaved. When only a single animal is pointing at the sensor and echolocating, a consistent echolocation click rate is likely to be received. Since echolocation signals are directional, there may be variability in ICI even in the single animal case due to changes in behavior and the detectability associated with animal orientation. The ICI time series, together with RL<sub>pp</sub> of the detections throughout the encounter, and the concurrent spectral characteristics displayed in the LTSA panel allow context-supported interpretation of the data and differentiation of signal types.</p>
      <p>The RL<sub>pp</sub> versus transformed RL<sub>rms</sub> display is a unique feature of <italic>DetEdit</italic> that allows users to distinguish between signals with the similar maximum amplitude, but different pulse characteristics in the time domain. Signals of short duration appear on the right side of the plot and signals of long duration appear on the left side of the plot. The same transformed RL<sub>rms</sub> is shown with respect to peak frequency into another display. These panels help distinguish between odontocete species’ clicks, as well as some human signals (e.g. echosounders, ship noise).</p>
    </sec>
    <sec id="sec009">
      <title>Case Study 1: Sperm whales</title>
      <p>A routine for verifying candidate sperm whale echolocation clicks was developed to distinguish signals with a multi-step approach (<xref ref-type="supplementary-material" rid="pcbi.1007598.s002">S1 Text</xref>). This case study illustrates the versatility of <italic>DetEdit</italic> for different labeling purposes and manual labeling of false detections. A total of 202 TB of data (containing 34 million sperm whale clicks) was verified and corresponding false positive rates were calculated.</p>
      <p>An example of editing acoustic data with sperm whale clicks within a mixed species time period is shown in <xref ref-type="fig" rid="pcbi.1007598.g002">Fig 2</xref>. The window panels of both RL<sub>pp</sub> and peak frequencies with respect to transformed RL<sub>rms</sub> were particularly useful in distinguishing sperm whale detections from other odontocetes. In this case, the removal of dolphin clicks was possible by selecting clicks with lower RMS. To accelerate the removal of low RMS detections, a RL<sub>pp</sub> threshold was implemented to automatically label all detections below the threshold as false positives (<xref ref-type="fig" rid="pcbi.1007598.g002">Fig 2F</xref>). Also, noisy periods of time corresponding to impulsive shipping noise were visually identified and flagged with the brushing tool for exclusion.</p>
    </sec>
    <sec id="sec010">
      <title>Case Study 2: Delphinids</title>
      <p>This example shows how <italic>DetEdit</italic> supports the identification of distinct delphinid click types within large datasets (<ext-link ext-link-type="uri" xlink:href="https://github.com/ScrippsWhaleAcoustics/DetEdit/wiki/Getting-Started">https://github.com/MarineBioAcousticsRC/DetEdit/wiki/Getting-Started</ext-link>). Unsupervised clustering tools were used to automatically classify signals into categories and to assist human analysts with processing multi-species acoustic encounters [<xref rid="pcbi.1007598.ref012" ref-type="bibr">12</xref>]. A total of 171 TB of data (with 115 million dolphin clicks) was verified, and corresponding false positive rates for each species were calculated. Clusters were evaluated using the interface, with different color codes distinguishing the multiple detection types identified (<xref ref-type="fig" rid="pcbi.1007598.g004">Fig 4</xref>). Multiple encounters of overlapping species were visually distinguishable and flagging clicks in different colors made identification of different species possible. The selection tool supported this process by allowing manual selection of individual or multiple detections to compare with the different color-coded detections.</p>
    </sec>
  </sec>
  <sec id="sec011">
    <title>Availability and future directions</title>
    <p><italic>DetEdit</italic> as a MATLAB package with example datasets for different species and detailed instructions are available from GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/ScrippsWhaleAcoustics/DetEdit">https://github.com/MarineBioAcousticsRC/DetEdit</ext-link>). The current code runs under MATLAB R2014b or newer versions. More details on the implementation and the examples are given at <ext-link ext-link-type="uri" xlink:href="https://github.com/ScrippsWhaleAcoustics/DetEdit/wiki">https://github.com/MarineBioAcousticsRC/DetEdit/wiki</ext-link>.</p>
    <p>Future directions include adding additional user-friendly GUI tools, incorporating clustering techniques to pre-label data using machine learning based classifiers, supporting visualization of signal templates for assisted classification, and providing certainty scores to complement classification and false positive labels.</p>
  </sec>
  <sec sec-type="supplementary-material" id="sec012">
    <title>Supporting information</title>
    <supplementary-material content-type="local-data" id="pcbi.1007598.s001">
      <label>S1 Fig</label>
      <caption>
        <title>Example plots of data summary.</title>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pcbi.1007598.s001.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pcbi.1007598.s002">
      <label>S1 Text</label>
      <caption>
        <title>Detection algorithm for sperm whale echolocation clicks.</title>
        <p>(DOCX)</p>
      </caption>
      <media xlink:href="pcbi.1007598.s002.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="pcbi.1007598.ref001">
      <label>1</label>
      <mixed-citation publication-type="book"><name><surname>Anorim</surname><given-names>MCP</given-names></name>. <chapter-title>Diversity of sound production in fish</chapter-title> In: <name><surname>Collin</surname><given-names>S.P.</given-names></name>, <name><surname>Moller</surname><given-names>P.</given-names></name>, <name><surname>Kapoor</surname><given-names>BG</given-names></name>, editor. <chapter-title>Communication in Fishes</chapter-title>
<publisher-loc>Enfield, HH</publisher-loc>: <publisher-name>Science Publishers</publisher-name>; <year>2006</year> pp. <fpage>71</fpage>–<lpage>105</lpage>.</mixed-citation>
    </ref>
    <ref id="pcbi.1007598.ref002">
      <label>2</label>
      <mixed-citation publication-type="journal"><name><surname>Brinkløv</surname><given-names>S</given-names></name>, <name><surname>Fenton</surname><given-names>MB</given-names></name>, <name><surname>Ratcliffe</surname><given-names>JM</given-names></name>. Echolocation in oilbirds and swiftlets. <source>Front Physiol</source>. <year>2013</year>;<volume>4</volume>: <fpage>1</fpage>–<lpage>12</lpage>. <pub-id pub-id-type="doi">10.3389/fphys.2013.00001</pub-id><pub-id pub-id-type="pmid">23372552</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007598.ref003">
      <label>3</label>
      <mixed-citation publication-type="book"><name><surname>Brown</surname><given-names>PE</given-names></name>, <name><surname>Grinnell</surname><given-names>AD</given-names></name>. <chapter-title>Echolocation Ontogeny in Bats</chapter-title> In: <name><surname>Busnel</surname><given-names>R-G</given-names></name>, <name><surname>Fish</surname><given-names>JF</given-names></name>, editors. <source>Animal Sonar Systems</source>. <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Springer US</publisher-name>; <year>1980</year> pp. <fpage>355</fpage>–<lpage>377</lpage>. <pub-id pub-id-type="doi">10.1007/978-1-4684-7254-7</pub-id>_15</mixed-citation>
    </ref>
    <ref id="pcbi.1007598.ref004">
      <label>4</label>
      <mixed-citation publication-type="journal"><name><surname>Gerhardt</surname><given-names>HC</given-names></name>, <name><surname>Huber</surname><given-names>F</given-names></name>, <name><surname>Simmons</surname><given-names>AM</given-names></name>. <article-title><italic>Acoustic Communication in Insects and Anurans</italic>: <italic>Common Problems and Diverse Solutions</italic></article-title>. <source>J Acoust Soc Am</source>. <year>2003</year>; <pub-id pub-id-type="doi">10.1121/1.1591773</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007598.ref005">
      <label>5</label>
      <mixed-citation publication-type="book"><name><surname>Hawkins</surname><given-names>AD</given-names></name>. <chapter-title>Underwater Sound and Fish Behaviour</chapter-title> In: <name><surname>Pitcher</surname><given-names>TJ</given-names></name>, editor. <source>The Behaviour of Teleost Fishes</source>. <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Springer US</publisher-name>; <year>1986</year> pp. <fpage>114</fpage>–<lpage>151</lpage>. <pub-id pub-id-type="doi">10.1007/978-1-4684-8261-4_5</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007598.ref006">
      <label>6</label>
      <mixed-citation publication-type="journal"><name><surname>Kroodsma</surname><given-names>EH</given-names></name>. <article-title>Acoustic Communication in Birds: Production, Perception and Design Features of Sounds. Kroodsma, Edward H. Miller HO, editor</article-title>. <source>Elsevier Science</source>; <year>1982</year>.</mixed-citation>
    </ref>
    <ref id="pcbi.1007598.ref007">
      <label>7</label>
      <mixed-citation publication-type="book"><name><surname>Quam</surname><given-names>RM</given-names></name>, <name><surname>Ramsier</surname><given-names>MA</given-names></name>, <name><surname>Fay</surname><given-names>RR</given-names></name>. <chapter-title>Primate Hearing and Communication</chapter-title> [Internet]. <name><surname>Popper</surname><given-names>AN</given-names></name>, editor. <publisher-loc>Cham</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>; <year>2017</year>
<pub-id pub-id-type="doi">10.1007/978-3-319-59478-1</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007598.ref008">
      <label>8</label>
      <mixed-citation publication-type="journal"><name><surname>Richardson</surname><given-names>WJ</given-names></name>, <name><surname>Greene</surname><given-names>CRJ</given-names></name>, <name><surname>Malme</surname><given-names>CI</given-names></name>, <name><surname>Thomson</surname><given-names>DH</given-names></name>. <source>Marine Mammals and Noise</source>. San Diego: Academic Press; <year>1995</year>.</mixed-citation>
    </ref>
    <ref id="pcbi.1007598.ref009">
      <label>9</label>
      <mixed-citation publication-type="journal"><name><surname>Versluis</surname><given-names>M.</given-names></name><article-title>How Snapping Shrimp Snap: Through Cavitating Bubbles</article-title>. <source>Science</source> (80-). American Association for the Advancement of Science; <year>2000</year>;<volume>289</volume>: <fpage>2114</fpage>–<lpage>2117</lpage>. <pub-id pub-id-type="doi">10.1126/science.289.5487.2114</pub-id>
<?supplied-pmid 11000111?><pub-id pub-id-type="pmid">11000111</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007598.ref010">
      <label>10</label>
      <mixed-citation publication-type="journal"><name><surname>Marques</surname><given-names>TA</given-names></name>, <name><surname>Thomas</surname><given-names>L</given-names></name>, <name><surname>Martin</surname><given-names>SW</given-names></name>, <name><surname>Mellinger</surname><given-names>DK</given-names></name>, <name><surname>Ward</surname><given-names>JA</given-names></name>, <name><surname>Moretti</surname><given-names>DJ</given-names></name>, <etal>et al</etal><article-title>Estimating animal population density using passive acoustics</article-title>. <source>Biol Rev</source>. <year>2013</year>;<volume>88</volume>: <fpage>287</fpage>–<lpage>309</lpage>. <pub-id pub-id-type="doi">10.1111/brv.12001</pub-id><?supplied-pmid 23190144?><pub-id pub-id-type="pmid">23190144</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007598.ref011">
      <label>11</label>
      <mixed-citation publication-type="journal"><name><surname>Wiggins</surname><given-names>SM</given-names></name>, <name><surname>Hildebrand</surname><given-names>JA</given-names></name>. <article-title>High-frequency Acoustic Recording Package (HARP) for broad-band, long-term marine mammal monitoring</article-title>. <source>Int Symp Underw Technol UT 2007—Int Work Sci Use Submar Cables Relat Technol 2007</source>. <year>2007</year>; <fpage>551</fpage>–<lpage>557</lpage>. <pub-id pub-id-type="doi">10.1109/UT.2007.370760</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007598.ref012">
      <label>12</label>
      <mixed-citation publication-type="journal"><name><surname>Frasier</surname><given-names>KE</given-names></name>, <name><surname>Roch</surname><given-names>MA</given-names></name>, <name><surname>Soldevilla</surname><given-names>MS</given-names></name>, <name><surname>Wiggins</surname><given-names>SM</given-names></name>, <name><surname>Garrison</surname><given-names>LP</given-names></name>, <name><surname>Hildebrand</surname><given-names>JA</given-names></name>. <article-title>Automated classification of dolphin echolocation click types from the Gulf of Mexico</article-title>. <source>PLOS Comput Biol</source>. <year>2017</year>;<volume>13</volume>: <fpage>e1005823</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005823</pub-id><?supplied-pmid 29216184?><pub-id pub-id-type="pmid">29216184</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007598.ref013">
      <label>13</label>
      <mixed-citation publication-type="journal"><name><surname>Hildebrand</surname><given-names>JA</given-names></name>, <name><surname>Baumann-Pickering</surname><given-names>S</given-names></name>, <name><surname>Frasier</surname><given-names>KE</given-names></name>, <name><surname>Trickey</surname><given-names>JS</given-names></name>, <name><surname>Merkens</surname><given-names>KP</given-names></name>, <name><surname>Wiggins</surname><given-names>SM</given-names></name>, <etal>et al</etal><article-title>Passive acoustic monitoring of beaked whale densities in the Gulf of Mexico</article-title>. <source>Sci Rep</source>. Nature Publishing Group; <year>2015</year>;<volume>5</volume>: <fpage>16343</fpage><pub-id pub-id-type="doi">10.1038/srep16343</pub-id><?supplied-pmid 26559743?><pub-id pub-id-type="pmid">26559743</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007598.ref014">
      <label>14</label>
      <mixed-citation publication-type="other">DCLDE Committee. 8th DCLDE Workshop. Challenge [Internet]. 2018. Available: <ext-link ext-link-type="uri" xlink:href="http://sabiod.univ-tln.fr/DCLDE/challenge.html">http://sabiod.univ-tln.fr/DCLDE/challenge.html</ext-link></mixed-citation>
    </ref>
    <ref id="pcbi.1007598.ref015">
      <label>15</label>
      <mixed-citation publication-type="book"><name><surname>Solsona Berga</surname><given-names>A.</given-names></name><chapter-title>Advancement of methods for passive acoustic monitoring: a framework for the study of deep-diving cetacean. TDX (Tesis Dr en Xarxa)</chapter-title><publisher-name>Universitat Politècnica de Catalunya</publisher-name>; <year>2019</year>; Available: <ext-link ext-link-type="uri" xlink:href="http://hdl.handle.net/2117/129269">http://hdl.handle.net/2117/129269</ext-link></mixed-citation>
    </ref>
    <ref id="pcbi.1007598.ref016">
      <label>16</label>
      <mixed-citation publication-type="journal"><name><surname>Madsen</surname><given-names>PT</given-names></name>, <name><surname>Johnson</surname><given-names>M</given-names></name>, <name><surname>de Soto</surname><given-names>NA</given-names></name>, <name><surname>Zimmer</surname><given-names>WMX</given-names></name>, <name><surname>Tyack</surname><given-names>P</given-names></name>. <article-title>Biosonar performance of foraging beaked whales (Mesoplodon densirostris)</article-title>. <source>J Exp Biol</source>. <year>2005</year>;<volume>208</volume>: <fpage>181</fpage>–<lpage>94</lpage>. <pub-id pub-id-type="doi">10.1242/jeb.01327</pub-id><?supplied-pmid 15634839?><pub-id pub-id-type="pmid">15634839</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
