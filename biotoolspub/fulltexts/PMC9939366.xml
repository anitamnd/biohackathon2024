<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_APSB1403 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEgr4 jpg ?>
<?FILEgr5 jpg ?>
<?FILEgr6 jpg ?>
<?FILEga1 jpg ?>
<?FILEmmc1 pdf ?>
<?FILEmmc2 xlsx ?>
<?FILEmmc3 xlsx ?>
<?FILEmmc4 xlsx ?>
<?FILEsi1 svg ?>
<?FILEsi2 svg ?>
<?FILEsi3 svg ?>
<?FILEsi4 svg ?>
<?FILEsi5 svg ?>
<?FILEsi6 svg ?>
<?FILEsi7 svg ?>
<?FILEsi8 svg ?>
<?FILEsi9 svg ?>
<?FILEsi10 svg ?>
<?FILEsi11 svg ?>
<?FILEsi12 svg ?>
<?FILEsi13 svg ?>
<?FILEsi14 svg ?>
<?FILEsi15 svg ?>
<?FILEsi16 svg ?>
<?FILEsi17 svg ?>
<?FILEsi18 svg ?>
<?FILEsi19 svg ?>
<?FILEsi20 svg ?>
<?FILEsi21 svg ?>
<?FILEsi22 svg ?>
<?FILEsi23 svg ?>
<?FILEsi24 svg ?>
<?FILEsi25 svg ?>
<?FILEsi26 svg ?>
<?FILEsi27 svg ?>
<?FILEsi28 svg ?>
<?FILEsi29 svg ?>
<?FILEsi30 svg ?>
<?FILEsi31 svg ?>
<?FILEsi32 svg ?>
<?FILEsi33 svg ?>
<?FILEsi34 svg ?>
<?FILEsi35 svg ?>
<?FILEsi36 svg ?>
<?FILEsi37 svg ?>
<?FILEsi38 svg ?>
<?FILEsi39 svg ?>
<?FILEsi40 svg ?>
<?FILEsi41 svg ?>
<?FILEsi42 svg ?>
<?FILEsi43 svg ?>
<?FILEsi44 svg ?>
<?FILEsi45 svg ?>
<?FILEsi46 svg ?>
<?FILEsi47 svg ?>
<?FILEsi48 svg ?>
<?FILEsi49 svg ?>
<?FILEsi50 svg ?>
<?FILEsi51 svg ?>
<?FILEsi52 svg ?>
<?FILEsi53 svg ?>
<?FILEsi54 svg ?>
<?FILEsi55 svg ?>
<?FILEsi56 svg ?>
<?FILEsi57 svg ?>
<?FILEsi58 svg ?>
<?FILEsi59 svg ?>
<?FILEsi60 svg ?>
<?FILEsi61 svg ?>
<?FILEsi62 svg ?>
<?FILEsi63 svg ?>
<?FILEsi64 svg ?>
<?FILEsi65 svg ?>
<?FILEsi66 svg ?>
<?FILEsi67 svg ?>
<?FILEsi68 svg ?>
<?FILEsi69 svg ?>
<?FILEsi70 svg ?>
<?FILEsi71 svg ?>
<?FILEsi72 svg ?>
<?FILEsi73 svg ?>
<?FILEsi74 svg ?>
<?FILEsi75 svg ?>
<?FILEsi76 svg ?>
<?FILEsi80 svg ?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Acta Pharm Sin B</journal-id>
    <journal-id journal-id-type="iso-abbrev">Acta Pharm Sin B</journal-id>
    <journal-title-group>
      <journal-title>Acta Pharmaceutica Sinica. B</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">2211-3835</issn>
    <issn pub-type="epub">2211-3843</issn>
    <publisher>
      <publisher-name>Elsevier</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9939366</article-id>
    <article-id pub-id-type="pii">S2211-3835(22)00211-8</article-id>
    <article-id pub-id-type="doi">10.1016/j.apsb.2022.05.004</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Tools</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Kinome-wide polypharmacology profiling of small molecules by multi-task graph isomorphism network approach</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au1">
        <name>
          <surname>Bao</surname>
          <given-names>Lingjie</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
        <xref rid="fn1" ref-type="fn">†</xref>
      </contrib>
      <contrib contrib-type="author" id="au2">
        <name>
          <surname>Wang</surname>
          <given-names>Zhe</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
        <xref rid="fn1" ref-type="fn">†</xref>
      </contrib>
      <contrib contrib-type="author" id="au3">
        <name>
          <surname>Wu</surname>
          <given-names>Zhenxing</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author" id="au4">
        <name>
          <surname>Luo</surname>
          <given-names>Hao</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author" id="au5">
        <name>
          <surname>Yu</surname>
          <given-names>Jiahui</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author" id="au6">
        <name>
          <surname>Kang</surname>
          <given-names>Yu</given-names>
        </name>
        <email>tingjunhou@zju.edu.cn</email>
        <xref rid="aff1" ref-type="aff">a</xref>
        <xref rid="cor1" ref-type="corresp">∗</xref>
      </contrib>
      <contrib contrib-type="author" id="au7">
        <name>
          <surname>Cao</surname>
          <given-names>Dongsheng</given-names>
        </name>
        <email>oriental-cds@163.com</email>
        <xref rid="aff3" ref-type="aff">c</xref>
        <xref rid="cor1" ref-type="corresp">∗</xref>
      </contrib>
      <contrib contrib-type="author" id="au8">
        <name>
          <surname>Hou</surname>
          <given-names>Tingjun</given-names>
        </name>
        <email>yukang@zju.edu.cn</email>
        <xref rid="aff1" ref-type="aff">a</xref>
        <xref rid="aff2" ref-type="aff">b</xref>
        <xref rid="cor1" ref-type="corresp">∗</xref>
      </contrib>
      <aff id="aff1"><label>a</label>Innovation Institute for Artificial Intelligence in Medicine of Zhejiang University, College of Pharmaceutical Sciences, Zhejiang University, Hangzhou 310058, China</aff>
      <aff id="aff2"><label>b</label>State Key Lab of CAD&amp;CG, Zhejiang University, Hangzhou 310058, China</aff>
      <aff id="aff3"><label>c</label>Xiangya School of Pharmaceutical Sciences, Central South University, Changsha 410013, China</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>∗</label>Corresponding authors. Tel./fax: +86 571 88208412. <email>tingjunhou@zju.edu.cn</email><email>oriental-cds@163.com</email><email>yukang@zju.edu.cn</email></corresp>
      <fn id="fn1">
        <label>†</label>
        <p id="ntpara0015">These authors made equal contributions to this work.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>12</day>
      <month>5</month>
      <year>2022</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="ppub">
      <month>1</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>12</day>
      <month>5</month>
      <year>2022</year>
    </pub-date>
    <volume>13</volume>
    <issue>1</issue>
    <fpage>54</fpage>
    <lpage>67</lpage>
    <history>
      <date date-type="received">
        <day>21</day>
        <month>3</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>15</day>
        <month>4</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>30</day>
        <month>4</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022 Chinese Pharmaceutical Association and Institute of Materia Medica, Chinese Academy of Medical Sciences. Production and hosting by Elsevier B.V.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder>Chinese Pharmaceutical Association and Institute of Materia Medica, Chinese Academy of Medical Sciences</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).</license-p>
      </license>
    </permissions>
    <abstract id="abs0010">
      <p>Prediction of the interactions between small molecules and their targets play important roles in various applications of drug development, such as lead discovery, drug repurposing and elucidation of potential drug side effects. Therefore, a variety of machine learning-based models have been developed to predict these interactions. In this study, a model called auxiliary multi-task graph isomorphism network with uncertainty weighting (AMGU) was developed to predict the inhibitory activities of small molecules against 204 different kinases based on the multi-task Graph Isomorphism Network (MT-GIN) with the auxiliary learning and uncertainty weighting strategy. The calculation results illustrate that the AMGU model outperformed the descriptor-based models and state-of-the-art graph neural networks (GNN) models on the internal test set. Furthermore, it also exhibited much better performance on two external test sets, suggesting that the AMGU model has enhanced generalizability due to its great transfer learning capacity. Then, a naïve model-agnostic interpretable method for GNN called edges masking was devised to explain the underlying predictive mechanisms, and the consistency of the interpretability results for 5 typical epidermal growth factor receptor (EGFR) inhibitors with their structure‒activity relationships could be observed. Finally, a free online web server called KIP was developed to predict the kinome-wide polypharmacology effects of small molecules (<ext-link ext-link-type="uri" xlink:href="http://cadd.zju.edu.cn/kip" id="intref0010">http://cadd.zju.edu.cn/kip</ext-link>).</p>
    </abstract>
    <abstract abstract-type="graphical" id="abs0015">
      <title>Graphical abstract</title>
      <p>A multi-task graph isomorphism network model and related online web server were developed to predict the kinome-wide polypharmacology effects of small molecules with high accuracy.<fig id="undfig1" position="anchor"><alt-text id="alttext0010">Image 1</alt-text><graphic xlink:href="ga1"/></fig></p>
    </abstract>
    <kwd-group id="kwrds0010">
      <title>Key words</title>
      <kwd>Kinome-wide polypharmacology</kwd>
      <kwd>Machine learning</kwd>
      <kwd>Kinases</kwd>
      <kwd>Graph neural networks</kwd>
      <kwd>Artificial intelligence</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <label>1</label>
    <title>Introduction</title>
    <p id="p0010">The human kinome contains around 500 kinases, accounting for roughly 1.7% of the whole human genome<xref rid="bib1" ref-type="bibr"><sup>1</sup></xref>. Protein kinases can catalyze the transfer of the terminal phosphate group of adenosine triphosphate (ATP) to substrate proteins, which plays a pivotal role in signal transductions and regulation of a wide range of cellular processes<xref rid="bib2" ref-type="bibr"><sup>2</sup></xref>. Moreover, aberrant kinase signaling has been linked to a variety of diseases, such as cancer, autoimmune disorders, diabetes, and neurological disorders<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref>. The US Food and Drug Administration (FDA) has approved 71 small molecule kinase inhibitors by May 2021<xref rid="bib4" ref-type="bibr"><sup>4</sup></xref>. Despite the significant progress made in recent years, some challenges still need to be overcome in the field of kinase drug discovery. On the one hand, previous studies were focusing on only a small subset of human kinases, while most others were overlooked<xref rid="bib5" ref-type="bibr"><sup>5</sup></xref><sup>,</sup><xref rid="bib6" ref-type="bibr"><sup>6</sup></xref>. Thus, new techniques to reveal the activities of these understudied kinases and discovery of new small-molecule inhibitors for the treatment of associated complicated indications are urgently required<xref rid="bib6" ref-type="bibr"><sup>6</sup></xref><sup>,</sup><xref rid="bib7" ref-type="bibr"><sup>7</sup></xref>. On the other hand, most kinase inhibitors bind to the highly conserved ATP binding pockets in a competitive manner, which may lead to undesirable off-target effects<xref rid="bib6" ref-type="bibr"><sup>6</sup></xref><sup>,</sup><xref rid="bib8" ref-type="bibr"><sup>8</sup></xref>. Certainly, inhibiting numerous kinases at the same time may improve the efficacy of a kinase inhibitor and its ability to treat several types of cancers and many incurable diseases<xref rid="bib5" ref-type="bibr"><sup>5</sup></xref><sup>,</sup><xref rid="bib6" ref-type="bibr"><sup>6</sup></xref>. Hence, detecting the interactions between small molecules and kinase targets is quite critical to elucidate potential off-target effects, facilitate drug repurposing and discover new kinase inhibitors<xref rid="bib9" ref-type="bibr"><sup>9</sup></xref></p>
    <p id="p0015">With the rapid accumulation of experimental bioactivity data for small molecules against kinases, many machine learning (ML)-based ligand-centric models have been developed to predict the kinome-wide polypharmacology of small molecules<xref rid="bib10" ref-type="bibr">10</xref>, <xref rid="bib11" ref-type="bibr">11</xref>, <xref rid="bib12" ref-type="bibr">12</xref>, <xref rid="bib13" ref-type="bibr">13</xref>, <xref rid="bib14" ref-type="bibr">14</xref>, <xref rid="bib15" ref-type="bibr">15</xref>, <xref rid="bib16" ref-type="bibr">16</xref>. For example, in 2017, Merget et al.<xref rid="bib17" ref-type="bibr"><sup>17</sup></xref> developed a series of single-task ligand-based kinase inhibition classification models based on the connectivity-based and feature-based Morgan fingerprints for over 280 kinases using random forest (RF), naïve Bayes (NB), K-nearest neighbor (KNN), and deep neural network (DNN). The single-task RF models achieved the best performance with an average Area Under the Receiver Operating Characteristic curve (AUROC) of 0.76. In 2018, Sorin et al.<xref rid="bib18" ref-type="bibr"><sup>18</sup></xref> created the single-task RF models for 104 kinases based on the extended connectivity fingerprints (ECFPs) and pharmacophoric fingerprints (PFPs), which were then used to predict the inhibitory activities of small molecules against 104 kinases retrieved from ChEMBL. The models yielded good predictions with median sensitivity and specificity of higher than 0.8 for 90 kinase tasks and a median AUROC higher than 0.9 for 96 kinase tasks. Later, Janssen et al.<xref rid="bib19" ref-type="bibr"><sup>19</sup></xref> proposed the Drug Discovery Maps (DDM) method, which used the t-distributed stochastic neighbor embedding (t-SNE) algorithm to generate a visualization map of chemical similarity based on molecular fingerprints and biological similarity. DDM was also employed to find a novel inhibitor toward FMS-like tyrosine kinase 3 (FLT3), which was confirmed by biochemical assays. Recently, multi-task learning has attracted extensive attention and it is an inductive transfer approach that improves generalization by utilizing domain information contained in the training data of multiple related learning tasks as an inductive bias<xref rid="bib20" ref-type="bibr"><sup>20</sup></xref>. In 2019, Rodríguez-Pérez et al.<xref rid="bib21" ref-type="bibr"><sup>21</sup></xref> developed the single-task support vector machines (SVM), single-task RF, and multi-task DNN (MT-DNN) models based on the ECFPs and Molecular ACCess System (MACCS) fingerprints to predict the inhibitory activities of small molecules against 103 kinases. The MT-DNN models achieved the best prediction performance with a median Balanced Accuracy (BA) exceeding 0.8 and a median Matthews correlation coefficient (MCC) exceeding 0.75. Following that, Li et al.<xref rid="bib22" ref-type="bibr"><sup>22</sup></xref> developed the MT-DNN model based on the ECFPs to predict the inhibitory effects of small molecules against 391 kinases using the large-scale bioactivity data. In the internal test set, the model outperformed the standard single-task RF models with an AUROC of 0.90, especially for these kinases with limited activity data.</p>
    <p id="p0020">Despite the advances made in this field, there are still flaws and issues that need to be addressed. First, almost all models mentioned above were developed based on expert-crafted descriptors as molecular representation, which may not fully exploit the data's characteristics. As a novel form of deep learning (DL) algorithm, GNN can produce task-specific representation for molecules from data in an adaptable manner. In many molecular property prediction tasks, GNN models show better performances than descriptor-based models<xref rid="bib23" ref-type="bibr">23</xref>, <xref rid="bib24" ref-type="bibr">24</xref>, <xref rid="bib25" ref-type="bibr">25</xref>, <xref rid="bib26" ref-type="bibr">26</xref>, <xref rid="bib27" ref-type="bibr">27</xref>. However, the GNN algorithms have never been utilized to predict protein kinase inhibition profiles. Therefore, it is quite valuable to explore the application of these state-of-the-art methods in predicting kinase inhibitory activities. Secondly, while multi-task learning has been effectively implemented in this field, previous researches have failed to account for task conflicts during the training process, which may result in inferior model performance<xref rid="bib28" ref-type="bibr"><sup>28</sup></xref><sup>,</sup><xref rid="bib29" ref-type="bibr"><sup>29</sup></xref>. To address these issues in multi-task learning, dynamic weighting strategies were suggested, but it is unknown whether these dynamic weighting algorithms are useful in multi-task learning for drug discovery. Third, the reported studies have never provided any interpretability to explain the underlying predictive mechanisms for the prediction models.</p>
    <p id="p0025">In this study, we proposed auxiliary multi-task graph isomorphism network with uncertainty weighting (AMGU), a new multi-task GNN model that can predict the inhibition profiles for small molecules against 204 kinases. As a comparison, four different descriptor-based models and five GNN-based models were also built. In both the internal and external testing, the AMGU model beat the other 9 models, highlighting its superiority in the prediction of kinase inhibition profiles. Compared with single-task models, the AMGU model could effectively enhance the performances of the separate tasks from related tasks, and the advantages were more noticeable for the tasks with fewer data. In addition, AMGU has the potential to uncover the relevance between the inhibition data of different kinases, which could aid in the discovery of “group-selective” kinase inhibitors. Moreover, we proposed a naïve model-agnostic explanation method named edges masking to interpret the underlying predictive mechanisms behind the AMGU model. Finally, a web server for the kinome-wide polypharmacology profiling of small molecules was developed and freely accessible at <ext-link ext-link-type="uri" xlink:href="http://cadd.zju.edu.cn/kip" id="intref0015">http://cadd.zju.edu.cn/kip</ext-link>.</p>
  </sec>
  <sec id="sec2">
    <label>2</label>
    <title>Material and methods</title>
    <sec id="sec2.1">
      <label>2.1</label>
      <title>Dataset collection and preparation</title>
      <p id="p0030">The “Human and mouse protein kinases: classification and index” file was downloaded from the UniProt database (<ext-link ext-link-type="uri" xlink:href="https://www.uniprot.org/docs/pkinfam" id="intref0020">https://www.uniprot.org/docs/pkinfam</ext-link>) to extract a list of the specified UniProt identifiers of human kinases (organism: “<italic>Homo sapiens</italic>”)<xref rid="bib30" ref-type="bibr"><sup>30</sup></xref>. The ChEMBL database (Release 27) was searched for the experimental bioactivity data (<italic>i.e</italic>., IC<sub>50</sub>, <italic>K</italic><sub>d</sub> and <italic>K</italic><sub>i</sub>) by querying the UniProt identifiers of human kinases on the ChEMBL website<xref rid="bib15" ref-type="bibr"><sup>15</sup></xref>. Five independent kinase assays were additionally collected, including the Davis dataset<xref rid="bib12" ref-type="bibr"><sup>12</sup></xref>, Anastassiadis dataset<xref rid="bib11" ref-type="bibr"><sup>11</sup></xref>, Metz dataset<xref rid="bib14" ref-type="bibr"><sup>14</sup></xref>, Published Kinase Inhibitor Set (PKIS)<xref rid="bib16" ref-type="bibr"><sup>16</sup></xref>, and Published Kinase Inhibitor Set 2 (PKIS 2)<xref rid="bib13" ref-type="bibr"><sup>13</sup></xref>. The inhibitory activity data points in the Anastassiadis dataset were transformed to IC<sub>50</sub> values <italic>via</italic> the equation previously defined<xref rid="bib31" ref-type="bibr"><sup>31</sup></xref>. The dataset was then processed through the following steps to assure data quality:<list list-type="simple" id="olist0010"><list-item id="o0010"><label>(1)</label><p id="p0035">The organic molecules without biological activity records or clear chemical structures (SMILES string) and the inorganic compounds were removed. For each molecule, additional salts and solvents in the structure were removed using the Python script from Merget et al.<xref rid="bib17" ref-type="bibr"><sup>17</sup></xref></p></list-item><list-item id="o0015"><label>(2)</label><p id="p0040">High-confidence biochemical assays were kept (confidence level ≥8, ensuring that there was a reported direct interaction between the ligand and its protein target), while the assays for mutated kinase targets were removed<xref rid="bib32" ref-type="bibr"><sup>32</sup></xref>.</p></list-item><list-item id="o0020"><label>(3)</label><p id="p0045">For the classification tasks, a reasonable threshold of 1 μmol/L was defined to distinguish active and inactive compounds according to the previous study<xref rid="bib22" ref-type="bibr"><sup>22</sup></xref>. For the PKIS and PKIS 2 datasets, an inhibition rate over 50% at 1 μmol/L was defined as the active threshold<xref rid="bib22" ref-type="bibr"><sup>22</sup></xref>. The Davis dataset, Metz dataset, Anastassiadis dataset, and ChEMBL dataset were integrated as the training set. The compound-kinase pairs with both positive and negative labels in all datasets were eliminated due to data conflict. Moreover, the tasks with less than 40 positive and 40 negative samples were excluded from the training dataset to ensure data quality. The PKIS and PKIS 2 datasets were served as the external test sets for further evaluation. The external test datasets were also stripped of the compound-kinase pairs found in the training set. Furthermore, only the tasks on the external test sets with at least one positive and one negative sample were evaluated.</p></list-item><list-item id="o0025"><label>(4)</label><p id="p0050">For the regression tasks, the bioactivity data points with exact IC<sub>50</sub> values were further extracted from the ChEMBL dataset and Anastassiadis dataset. The geometric mean of all the potency values was determined as the final potency annotation for any inhibitor that has multiple IC<sub>50</sub> values for the same kinase<xref rid="bib33" ref-type="bibr"><sup>33</sup></xref>. At last, the negative logarithm values of IC<sub>50</sub> of the bioactivity data points were recorded in the datasets. These bioactivity data for the regression tasks were also used in the auxiliary learning.</p></list-item></list></p>
      <p id="p0055">A summary for all the datasets can be seen in <xref rid="tbl1" ref-type="table">Table 1</xref>. The detailed information for each task in the dataset can be found in Supporting Information <xref rid="appsec3" ref-type="sec">Table S1</xref>. All datasets can be accessed at <ext-link ext-link-type="uri" xlink:href="http://cadd.zju.edu.cn/kip" id="intref0025">http://cadd.zju.edu.cn/kip</ext-link>.<table-wrap position="float" id="tbl1"><label>Table 1</label><caption><p>Summary of the datasets.</p></caption><alt-text id="alttext0065">Table 1</alt-text><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Dataset</th><th colspan="4">Classification<hr/></th><th colspan="2">Regression<hr/></th></tr><tr><th>Total</th><th>Positive</th><th>Negative</th><th>Task</th><th>Total</th><th>Task</th></tr></thead><tbody><tr><td>Training set</td><td>260,183</td><td>116,515</td><td>143,668</td><td>204</td><td>122,676</td><td>202</td></tr><tr><td>PKIS</td><td>46,179</td><td>2758</td><td>43,421</td><td>131</td><td>/</td><td>/</td></tr><tr><td>PKIS 2</td><td>116,052</td><td>10,509</td><td>105,543</td><td>186</td><td>/</td><td>/</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="sec2.2">
      <label>2.2</label>
      <title>Molecule graph representation</title>
      <p id="p0060">In GNN, a molecule can be seen as a topological molecular graph with hydrogen-depleted nodes and edges, where nodes represent atoms and edges represent bonds. In this study, molecules were converted to molecular graphs and utilized as the inputs for the GNN-based models. As indicated in <xref rid="tbl2" ref-type="table">Table 2</xref>, <xref rid="tbl3" ref-type="table">Table 3</xref>, eight types of atom features and four types of bond features were used as the initial features of atoms and bonds. All atomic information was represented by the one-hot form except that the formal charge was in the integer form. All bond information was encoded in the one-hot form. According to the study reported by Kip et al.<xref rid="bib34" ref-type="bibr"><sup>34</sup></xref>, self-connected undirected edges were added to the atoms for the GNN-based models except for the Directed Message Passing Neural Network (DMPNN) model. The DGL-LifeSci (version 0.2.5)<xref rid="bib35" ref-type="bibr"><sup>35</sup></xref> was used to transform molecules into bi-directed molecular graphs.<table-wrap position="float" id="tbl2"><label>Table 2</label><caption><p>The atom initial features used in GNN.</p></caption><alt-text id="alttext0070">Table 2</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Atom feature</th><th>Size</th><th>Description</th></tr></thead><tbody><tr><td>Atom symbol</td><td>43</td><td>[C, N, O, S, F, Si, P, Cl, Br, Mg, Na, Ca, Fe, As, Al, I, B, V, K, Tl, Yb, Sb, Sn, Ag, Pd, Co, Se, Ti, Zn, H, Li, Ge, Cu, Au, Ni, Cd, In, Mn, Zr, Cr, Pt, Hg, Pb] (one-hot)</td></tr><tr><td>Atom degree</td><td>11</td><td>Number of covalent bonds [0,1,2,3,4,5,6,7,8,9,10] (one-hot)</td></tr><tr><td>Implicit valence</td><td>6</td><td>The implicit valence of an atom [1,2,3,4,5,6] (one-hot)</td></tr><tr><td>Hydrogens</td><td>7</td><td>The number of implicit Hs on the atom [0,1,2,3,4,5,6] (one-hot)</td></tr><tr><td>Atom<break/>Hybridization</td><td>5</td><td>[SP, SP2, SP3, SP3D, SP3D2] (one-hot)</td></tr><tr><td>Aromaticity</td><td>1</td><td>Whether this atom is part of an aromatic system [0/1] (one-hot)</td></tr><tr><td>Formal charge</td><td>1</td><td>‒2‒2 (integer)</td></tr><tr><td>Chirality</td><td>4</td><td>The chirality type of an atom [unspecified, tetrahedral CW, CCW, or other] (one-hot)</td></tr></tbody></table></table-wrap><table-wrap position="float" id="tbl3"><label>Table 3</label><caption><p>The initial edge features used in GNN.</p></caption><alt-text id="alttext0075">Table 3</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Edge feature</th><th>Size</th><th>Description</th></tr></thead><tbody><tr><td>Bond type</td><td>4</td><td>[single, double, triple, aromatic] (one-hot)</td></tr><tr><td>Conjugation</td><td>1</td><td>Whether the bond is conjugated [0/1] (one-hot)</td></tr><tr><td>Ring</td><td>1</td><td>Whether the bond is part of a ring [0/1] (one-hot)</td></tr><tr><td>Stereo</td><td>6</td><td>[none, any, <italic>E</italic>/<italic>Z</italic> or <italic>cis</italic>/<italic>trans</italic>] (one-hot)</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="sec2.3">
      <label>2.3</label>
      <title>AMGU</title>
      <p id="p0065">AMGU is a multi-task GNN with the uncertainty weighting (UN) and the auxiliary learning strategy. The graph neural network layers employed in AMGU are Graph Isomorphism Network (GIN). The extra regression tasks functioned as the auxiliary learning components were incorporated and embedded in the last layer of the model to improve the generalization ability of the classification tasks. Then the uncertainty weighting strategy was utilized to resolve the potential conflicts between tasks by dynamically adjusting the weight for each task. The AMGU model is schematically depicted in <xref rid="fig1" ref-type="fig">Fig. 1</xref>.<fig id="fig1"><label>Figure 1</label><caption><p>(A) The schematic overview of the AMGU model. The molecule is first represented by its initial atom and edge features and then fed into the AMGU model. The atom and edge features are fed into the message-passing layer to transform their features from the previous adjacency layer. The outputs from the final message-passing layer are reduced to vectors by the readout function (summation here), which is then used for predicting the inhibitory activities of molecules towards different kinases <italic>via</italic> the stacked fully connected layers. The extra regression tasks (in purple) are served as the auxiliary learning component to improve the generalization of the classification tasks. The total loss of AMGU is the linear weighted average of the tasks with the task weights updated by the uncertainty weighting strategy. The task-specific parameters from the last layer of the model are retrieved to capture the relevance between tasks. (B) The details of the AMGU model. The GIN module is solely used to aggregate the data from the nearby nodes and perform linear transformations (without nonlinear activation function), while the ReLU module serves as the activation function in the network.</p></caption><alt-text id="alttext0035">Figure 1</alt-text><graphic xlink:href="gr1"/></fig></p>
      <p id="p0070"><italic>Graph Isomorphism Network.</italic> Graph Isomorphism Network (GIN) is a simple graph neural network proposed by Xu et al<xref rid="bib36" ref-type="bibr"><sup>36</sup></xref>. The Xu's study illustrates that its discriminative/representational ability is equal to the power of the Weisfeiler-Lehman test<xref rid="bib36" ref-type="bibr"><sup>36</sup></xref>. As shown in <xref rid="fig1" ref-type="fig">Fig. 1</xref>A, the architecture of GIN can be divided into three different parts: (1) message-passing layer, (2) read-out layer, and (3) fully connected layers<xref rid="bib36" ref-type="bibr"><sup>36</sup></xref><sup>,</sup><xref rid="bib37" ref-type="bibr"><sup>37</sup></xref>.</p>
      <p id="p0075">In the message-passing layer, the GIN model follows a recursive neighborhood aggregation scheme, where each node aggregates the feature vectors of its neighbors to form the new feature vector through nonlinear transformation. Specifically, for every node <inline-formula><mml:math id="M1" altimg="si1.svg"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula>, the node features <inline-formula><mml:math id="M2" altimg="si2.svg"><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>v</mml:mi><mml:mi>l</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> are updated as Eq. <xref rid="fd1" ref-type="disp-formula">(1)</xref>:<disp-formula id="fd1"><label>(1)</label><mml:math id="M3" altimg="si3.svg" alttext="Equation 1."><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo linebreak="badbreak">=</mml:mo><mml:mtext>ReLU</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mi>g</mml:mi><mml:mi>l</mml:mi></mml:msubsup><mml:mo linebreak="badbreak">×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>v</mml:mi><mml:mi>l</mml:mi></mml:msubsup><mml:mo linebreak="badbreak">+</mml:mo><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo linebreak="badbreak">∈</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:msubsup><mml:mi>h</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">+</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mi>g</mml:mi><mml:mi>l</mml:mi></mml:msubsup><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M4" altimg="si4.svg"><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is the node features of node <inline-formula><mml:math id="M5" altimg="si5.svg"><mml:mrow><mml:mi>ν</mml:mi></mml:mrow></mml:math></inline-formula> after <inline-formula><mml:math id="M6" altimg="si6.svg"><mml:mrow><mml:mi>l</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> iterations, <inline-formula><mml:math id="M7" altimg="si7.svg"><mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the set of the neighbors of node <inline-formula><mml:math id="M8" altimg="si5.svg"><mml:mrow><mml:mi>ν</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M9" altimg="si8.svg"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>g</mml:mi><mml:mi>l</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M10" altimg="si9.svg"><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mi>g</mml:mi><mml:mi>l</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> are the weight and bias, respectively, and <inline-formula><mml:math id="M11" altimg="si10.svg"><mml:mrow><mml:mtext>ReLU</mml:mtext></mml:mrow></mml:math></inline-formula> denotes the rectified linear activation function. After <italic>k</italic> iterations, the node features vector <inline-formula><mml:math id="M12" altimg="si11.svg"><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> captures the structural information within the node's <italic>k</italic>-hop network neighborhoods. Followed by the message-passing layer, a permutation invariant function as the readout function is designed to aggregate all features <inline-formula><mml:math id="M13" altimg="si11.svg"><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> in the final iteration into the graph embedding <inline-formula><mml:math id="M14" altimg="si12.svg"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of the entire graph <inline-formula><mml:math id="M15" altimg="si13.svg"><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:math></inline-formula>. Here, a summation function was used to directly aggregate the node features to gain the graph embedding <inline-formula><mml:math id="M16" altimg="si12.svg"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> as Eq. <xref rid="fd2" ref-type="disp-formula">(2)</xref>:<disp-formula id="fd2"><label>(2)</label><mml:math id="M17" altimg="si14.svg" alttext="Equation 2."><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mtext>v</mml:mtext><mml:mo linebreak="badbreak">∈</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:munder><mml:msubsup><mml:mi>h</mml:mi><mml:mi>v</mml:mi><mml:mi>l</mml:mi></mml:msubsup></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M18" altimg="si13.svg"><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:math></inline-formula> denotes the whole molecular graph.</p>
      <p id="p0080">Subsequently, the graph embedding <inline-formula><mml:math id="M19" altimg="si12.svg"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is fed to the fully connected layers and undertakes the nonlinear transformation as follows:<disp-formula id="fd3"><label>(3)</label><mml:math id="M20" altimg="si15.svg" alttext="Equation 3."><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo linebreak="badbreak">=</mml:mo><mml:mtext>ReLU</mml:mtext><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:msup><mml:mi>W</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:mo linebreak="badbreak">+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M21" altimg="si16.svg"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mi>l</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is the weights of <inline-formula><mml:math id="M22" altimg="si17.svg"><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> layer in neural network, <inline-formula><mml:math id="M23" altimg="si18.svg"><mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mi>l</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is the bias of <inline-formula><mml:math id="M24" altimg="si80.svg"><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mrow><mml:mtext>t</mml:mtext><mml:mtext>h</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> layer in neural network, <inline-formula><mml:math id="M25" altimg="si19.svg"><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mi>l</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> denotes the input layers, and <inline-formula><mml:math id="M26" altimg="si20.svg"><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> denotes the output layer. Then the output of this layer is fed to the subsequent unit in the next layer and performs the same operation. The result of the final output layer is used as the solution for the problem<xref rid="bib38" ref-type="bibr"><sup>38</sup></xref>. For the regression task, the output is calculated as the same as Eq. <xref rid="fd3" ref-type="disp-formula">(3)</xref> but without the ReLU activation function. While for the binary classification task, the sigmoid activation function is applied in the last layer to ensure getting the probability output in the range [0,1] for a particular task as Eq. <xref rid="fd4" ref-type="disp-formula">(4)</xref>:<disp-formula id="fd4"><label>(4)</label><mml:math id="M27" altimg="si21.svg" alttext="Equation 4."><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:msup><mml:mi>W</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:mo linebreak="badbreak">+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mi>l</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M28" altimg="si22.svg"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:math></inline-formula> denotes the sigmoid activation function. Finally, a loss function is used to compute the losses between the neural network outputs and true labels, which is used to guide the updating of neural network parameters. The loss function for each task is the weighted binary cross entropy, which imposes a higher penalty for the misclassification of the minority class and aids in the development of a discriminative model to handle the imbalanced distribution of the active and inactive points in our datasets<xref rid="bib39" ref-type="bibr"><sup>39</sup></xref>. The expression is as Eq. <xref rid="fd5" ref-type="disp-formula">(5)</xref>:<disp-formula id="fd5"><label>(5)</label><mml:math id="M29" altimg="si23.svg" alttext="Equation 5."><mml:msup><mml:mi>L</mml:mi><mml:mi>j</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>N</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mo>−</mml:mo><mml:mfrac><mml:msubsup><mml:mi>N</mml:mi><mml:mtext>neg</mml:mtext><mml:mi>j</mml:mi></mml:msubsup><mml:msup><mml:mi>N</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mfrac><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mspace width="0.25em"/><mml:mtext>log</mml:mtext><mml:mfenced><mml:msup><mml:mover><mml:mi>y</mml:mi><mml:mo>ˆ</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mfenced><mml:mo>−</mml:mo><mml:mfrac><mml:msubsup><mml:mi>N</mml:mi><mml:mtext>pos</mml:mtext><mml:mi>j</mml:mi></mml:msubsup><mml:msup><mml:mi>N</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mfrac><mml:mfenced><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mi>log</mml:mi><mml:mfenced><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mover><mml:mi>y</mml:mi><mml:mo>ˆ</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math></disp-formula>where <inline-formula><mml:math id="M30" altimg="si24.svg"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>ˆ</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the output of the model for sample <inline-formula><mml:math id="M31" altimg="si25.svg"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula> in task <inline-formula><mml:math id="M32" altimg="si26.svg"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M33" altimg="si27.svg"><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the ground-truth value for sample <inline-formula><mml:math id="M34" altimg="si25.svg"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula> in task <inline-formula><mml:math id="M35" altimg="si26.svg"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M36" altimg="si28.svg"><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> denotes the number of the samples in task <inline-formula><mml:math id="M37" altimg="si26.svg"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M38" altimg="si29.svg"><mml:mrow><mml:msubsup><mml:mi>N</mml:mi><mml:mtext>neg</mml:mtext><mml:mi>j</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is the number of the negative samples in task <inline-formula><mml:math id="M39" altimg="si26.svg"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M40" altimg="si30.svg"><mml:mrow><mml:msubsup><mml:mi>N</mml:mi><mml:mtext>pos</mml:mtext><mml:mi>j</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is the number of the positive samples in task <inline-formula><mml:math id="M41" altimg="si26.svg"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula>.</p>
      <p id="p0085">For single-task Graph Isomorphism Network (ST-GIN), the weighted binary cross entropy was used as the loss function and ST-GIN only outputted one prediction value. While for multi-task Graph Isomorphism Network (MT-GIN), the loss function and network structure were slightly different from that for ST-GIN. In the MT-GIN, the hard-parameter sharing architecture was used (<xref rid="fig1" ref-type="fig">Fig. 1</xref>A)<xref rid="bib40" ref-type="bibr"><sup>40</sup></xref>. The shared chunk, which was the layer preceding the last layer in the network, shared parameters between tasks. The last layer in MT-GIN, on the other hand, had its own set of task-specific parameters and could predict several targets at once. The loss function of every task was set as mentioned above Eq. <xref rid="fd5" ref-type="disp-formula">(5)</xref> and the final loss of all tasks was commonly set to be an average of the single tasks’ losses <inline-formula><mml:math id="M42" altimg="si31.svg"><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. The loss function of multi-task is as Eq. <xref rid="fd6" ref-type="disp-formula">(6)</xref>:<disp-formula id="fd6"><label>(6)</label><mml:math id="M43" altimg="si32.svg" alttext="Equation 6."><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mtext>all</mml:mtext></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>C</mml:mi></mml:mfrac><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>C</mml:mi></mml:munderover><mml:msup><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msup><mml:mo linebreak="goodbreak">⋅</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M44" altimg="si33.svg"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula> is the number of total tasks, and <inline-formula><mml:math id="M45" altimg="si34.svg"><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is the task-specific weight for task <inline-formula><mml:math id="M46" altimg="si26.svg"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula> which is set to be uniform in common multi-task learning models.</p>
      <p id="p0090">In addition to the previously mentioned primary components in the architecture of neural networks, the batch norm (BN) and dropout sections were added and shown in <xref rid="fig1" ref-type="fig">Fig. 1</xref>B. The batch norm component was used to speed up and improve the stability of neural networks<xref rid="bib41" ref-type="bibr"><sup>41</sup></xref>. The dropout portion was employed to keep the neural networks from overfitting<xref rid="bib42" ref-type="bibr"><sup>42</sup></xref>.</p>
      <p id="p0095"><italic>Uncertainty weighting (UN).</italic> Uncertainty weighting is a dynamic weighting strategy that can adaptively change task-specific weights <inline-formula><mml:math id="M47" altimg="si35.svg"><mml:mrow><mml:msup><mml:mi mathvariant="italic">w</mml:mi><mml:mi mathvariant="italic">j</mml:mi></mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="italic">t</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> during the training process to balance these potential conflicts and it may achieve high performance. As shown in Eq. <xref rid="fd6" ref-type="disp-formula">(6)</xref>, the final loss function is often assumed to be a linear weighted average of the single tasks’ losses <inline-formula><mml:math id="M48" altimg="si36.svg"><mml:mspace width="0.25em"/><mml:msup><mml:mi mathvariant="italic">L</mml:mi><mml:mi mathvariant="italic">j</mml:mi></mml:msup></mml:math></inline-formula> in multi-task learning. When using stochastic gradient descent to minimize this loss function, the network parameters in the shared layers <inline-formula><mml:math id="M49" altimg="si37.svg"><mml:mrow><mml:msub><mml:mi mathvariant="italic">W</mml:mi><mml:mrow><mml:mtext>s</mml:mtext><mml:mtext>h</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are updated as Eq. <xref rid="fd7" ref-type="disp-formula">(7)</xref>:<disp-formula id="fd7"><label>(7)</label><mml:math id="M50" altimg="si38.svg" alttext="Equation 7."><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mtext>sh</mml:mtext></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mtext>sh</mml:mtext></mml:msub><mml:mo linebreak="goodbreak">−</mml:mo><mml:mi>η</mml:mi><mml:mspace width="0.25em"/><mml:mfrac><mml:mn>1</mml:mn><mml:mi>C</mml:mi></mml:mfrac><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>C</mml:mi></mml:munderover><mml:msup><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msup><mml:mo linebreak="goodbreak">⋅</mml:mo><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:msup><mml:mi>L</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mtext>sh</mml:mtext></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M51" altimg="si39.svg"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mtext>sh</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> denotes shared parameters and <inline-formula><mml:math id="M52" altimg="si40.svg"><mml:mrow><mml:mi>η</mml:mi></mml:mrow></mml:math></inline-formula> denotes learning rate in the neural network. According to Eq. <xref rid="fd5" ref-type="disp-formula">(5)</xref>, we can draw the following conclusions that the network parameters updated may be suboptimal when the task gradients conflict, or dominated by one task when its gradient magnitude is much higher than those for the other tasks<xref rid="bib29" ref-type="bibr"><sup>29</sup></xref><sup>,</sup><xref rid="bib43" ref-type="bibr"><sup>43</sup></xref>. To deal with this difficulty, some approaches by setting the task-specific weights <inline-formula><mml:math id="M53" altimg="si34.svg"><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> in the loss<xref rid="bib44" ref-type="bibr"><sup>44</sup></xref>. However, searching for appropriate task-specific weights by hand is a difficult and expensive process. Hence, uncertainty weighting was proposed as a dynamic weighting strategy by Kendall et al.<xref rid="bib44" ref-type="bibr"><sup>44</sup></xref>. During the training process, the homoscedastic uncertainty was exploited to balance the task-specific weights optimally. The relative confidence between tasks can be captured using homoscedastic uncertainty as task-dependent uncertainty. Uncertainty weighting strategy would automatically assign high task-specific weights for the tasks with low homoscedastic uncertainty. In their study, the loss function of multi-task classification tasks can be written in Eq. <xref rid="fd8" ref-type="disp-formula">(8)</xref>:<disp-formula id="fd8"><label>(8)</label><mml:math id="M54" altimg="si41.svg" alttext="Equation 8."><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mtext>all</mml:mtext></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>C</mml:mi></mml:mfrac><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>C</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:msup><mml:mi>L</mml:mi><mml:mi>j</mml:mi></mml:msup><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>log</mml:mtext><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M55" altimg="si42.svg"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is <inline-formula><mml:math id="M56" altimg="si43.svg"><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mtext>th</mml:mtext></mml:msup></mml:mrow></mml:math></inline-formula> task's noise parameters, which is relevant to the task's homoscedastic uncertainty in task <inline-formula><mml:math id="M57" altimg="si26.svg"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula>. Here, <inline-formula><mml:math id="M58" altimg="si42.svg"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, as a learnable parameter for task <inline-formula><mml:math id="M59" altimg="si26.svg"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula>, is updated through standard backpropagation in every batch and can essentially balance the task-specific losses during training process. Large value <inline-formula><mml:math id="M60" altimg="si42.svg"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes high task's homoscedastic uncertainty hence reducing the task weight for task <inline-formula><mml:math id="M61" altimg="si26.svg"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula>. In all tasks, <inline-formula><mml:math id="M62" altimg="si44.svg"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> was initialized initialize to 1, which meant that each task weight was uniform at first and then updated according to the homoscedastic uncertainty of tasks.</p>
      <p id="p0100"><italic>Auxiliary learning with regression tasks. A</italic>uxiliary learning is similar to multi-task learning but aims to improve the performance on some primary tasks<xref rid="bib40" ref-type="bibr"><sup>40</sup></xref>. The auxiliary module is optimized in tandem with the multi-task learning network during training, and it serves as additional regularization by applying an inductive bias to the shared layers. In the testing phase, only the original multi-task learning network was retained<xref rid="bib45" ref-type="bibr"><sup>45</sup></xref>. With the goal of improving the generalization of these classification tasks, the regression tasks were used as the auxiliary tasks, and a new model termed Aux-MT-GIN was constructed. The mean squared error loss (MSE) was used as the loss function and as shown in Eq. <xref rid="fd9" ref-type="disp-formula">(9)</xref>:<disp-formula id="fd9"><label>(9)</label><mml:math id="M63" altimg="si45.svg" alttext="Equation 9."><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mi>r</mml:mi></mml:msup><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>N</mml:mi><mml:mi>r</mml:mi></mml:msup></mml:mfrac><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:munder><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msup><mml:mo linebreak="badbreak">−</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>ˆ</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M64" altimg="si46.svg"><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mi>r</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> denotes the loss of regression task <inline-formula><mml:math id="M65" altimg="si47.svg"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M66" altimg="si48.svg"><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mi>r</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> denotes the number of the samples in regression task <inline-formula><mml:math id="M67" altimg="si47.svg"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M68" altimg="si49.svg"><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the true label and <inline-formula><mml:math id="M69" altimg="si50.svg"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>ˆ</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the predicted value of the model. The total loss of auxiliary learning can be calculated as Eq. <xref rid="fd10" ref-type="disp-formula">(10)</xref>:<disp-formula id="fd10"><label>(10)</label><mml:math id="M70" altimg="si51.svg" alttext="Equation 10."><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mtext>all</mml:mtext></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>C</mml:mi></mml:mfrac><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>C</mml:mi></mml:munderover><mml:msup><mml:mi>L</mml:mi><mml:mi>j</mml:mi></mml:msup><mml:mo linebreak="goodbreak">+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>R</mml:mi></mml:mfrac><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>R</mml:mi></mml:munderover><mml:msup><mml:mi>L</mml:mi><mml:mi>r</mml:mi></mml:msup></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M71" altimg="si52.svg"><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> denotes the number of the regression tasks.</p>
      <p id="p0105">Furthermore, to handle potential conflicts between the classification tasks and regression tasks, the uncertainty weighting strategy was introduced and a new model named AMGU was developed, and the total loss was recast as Eq. <xref rid="fd11" ref-type="disp-formula">(11)</xref>:<disp-formula id="fd11"><label>(11)</label><mml:math id="M72" altimg="si53.svg" alttext="Equation 11."><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mtext>all</mml:mtext></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>C</mml:mi></mml:mfrac><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>C</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:msup><mml:mi>L</mml:mi><mml:mi>j</mml:mi></mml:msup><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>log</mml:mtext><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>R</mml:mi></mml:mfrac><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>R</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:msup><mml:mi>L</mml:mi><mml:mi>r</mml:mi></mml:msup><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>log</mml:mtext><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M73" altimg="si54.svg"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the homoscedastic uncertainty in regression task. All <inline-formula><mml:math id="M74" altimg="si76.svg"><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:math></inline-formula> and <inline-formula><mml:math id="M75" altimg="si76.svg"><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:math></inline-formula> are initialized as 1 at the beginning.</p>
    </sec>
    <sec id="sec2.4">
      <label>2.4</label>
      <title>Model construction and evaluation protocols</title>
      <p id="p0110">For the classification task, the training data were split into the training set, validation set, and internal test set with the ratio of 8:1:1 by using the “random stratified shuffle split” strategy to ensure that the percentage of the samples for each class was approximately preserved in each subset. In each task, the data from the training set, validation set, and internal test set were blended to be used for the multi-task learning. For the regression tasks in the auxiliary learning, the regression labels were provided if the same compound-kinase pair can be found in the classification tasks otherwise removed from the training set, validation set and internal test set. The training set was used to train the model, the validation set was used to search for the best combination of hyperparameters, and the internal test set and two external test sets were further used to evaluate the performance of each model.</p>
      <p id="p0115">The AMGU model was developed by using PyTorch (version 1.5.0)<xref rid="bib46" ref-type="bibr"><sup>46</sup></xref> and Deep Graph Library package (version 0.6.0)<xref rid="bib47" ref-type="bibr"><sup>47</sup></xref>. The total loss of AMGU was the linear weighted average of the weighted cross entropy for every task with task weight updated according to the uncertainty weighting strategy. The Adam algorithm was used to optimize the parameters in the model and the task weights<xref rid="bib48" ref-type="bibr"><sup>48</sup></xref>. To avoid overfitting, an early stop was utilized with the patience of 20 based on the average AUROC of all tasks in the internal validation set. If there is no improvement on the average AUROC in 5 consecutive epochs, the learning rate halves. The maximum number of the training epochs was set to 500. The detailed hyperparameters of different models can refer to Supporting Information. The final results were given as the mean and standard deviation for all models, which were performed with the seeds ranging from 0 to 9. For all models, the open-source library scikit-optimize was used to search for hyperparameters based on the average AUROC for all tasks in the internal validation sets<xref rid="bib49" ref-type="bibr"><sup>49</sup></xref>. To achieve acceptable performance, the model was subjected to 100 trials of hyperparameters search.</p>
      <p id="p0120">Several binary classification evaluation metrics were used to evaluate the performance of the classification models, including accuracy, precision (P), recall (R), F1-measure (F1), Matthews correlation coefficient (MCC), balanced accuracy (BA), the area under the receiver operating characteristic (AUROC) and the area under the precision-recall curve (AUPRC)<xref rid="bib50" ref-type="bibr"><sup>50</sup></xref>. These metrics are defined as Eqs. <xref rid="fd12" ref-type="disp-formula">(12)</xref>, <xref rid="fd13" ref-type="disp-formula">(13)</xref>, <xref rid="fd14" ref-type="disp-formula">(14)</xref>, <xref rid="fd15" ref-type="disp-formula">(15)</xref>, <xref rid="fd16" ref-type="disp-formula">(16)</xref>, <xref rid="fd17" ref-type="disp-formula">(17)</xref>:<disp-formula id="fd12"><label>(12)</label><mml:math id="M76" altimg="si55.svg" alttext="Equation 12."><mml:mrow><mml:mtext>Accuracy</mml:mtext><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>TN</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>TP</mml:mtext></mml:mrow><mml:mrow><mml:mtext>TN</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>TP</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FP</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="fd13"><label>(13)</label><mml:math id="M77" altimg="si56.svg" alttext="Equation 13."><mml:mrow><mml:mtext>Recall</mml:mtext><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mtext>TP</mml:mtext><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="fd14"><label>(14)</label><mml:math id="M78" altimg="si57.svg" alttext="Equation 14."><mml:mrow><mml:mtext>Precision</mml:mtext><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mtext>TP</mml:mtext><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FP</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="fd15"><label>(15)</label><mml:math id="M79" altimg="si58.svg" alttext="Equation 15."><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo linebreak="badbreak">×</mml:mo><mml:mtext>Recall</mml:mtext><mml:mo linebreak="badbreak">×</mml:mo><mml:mtext>Precision</mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mtext>Recall</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>Precision</mml:mtext><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="fd16"><label>(16)</label><mml:math id="M80" altimg="si59.svg" alttext="Equation 16."><mml:mrow><mml:mtext>MCC</mml:mtext><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo linebreak="badbreak">×</mml:mo><mml:mtext>TN</mml:mtext><mml:mo linebreak="badbreak">−</mml:mo><mml:mtext>FP</mml:mtext><mml:mo linebreak="badbreak">×</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow><mml:msqrt><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mtext>TP</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>TN</mml:mtext><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mtext>TP</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FN</mml:mtext><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mtext>TN</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FP</mml:mtext><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mtext>TN</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FN</mml:mtext><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="fd17"><label>(17)</label><mml:math id="M81" altimg="si60.svg" alttext="Equation 17."><mml:mrow><mml:mtext>Balanced accuracy</mml:mtext><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo linebreak="goodbreak">×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mfrac><mml:mtext>TP</mml:mtext><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfrac><mml:mo linebreak="badbreak">+</mml:mo><mml:mfrac><mml:mtext>TN</mml:mtext><mml:mrow><mml:mtext>TN</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FP</mml:mtext></mml:mrow></mml:mfrac><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where TP, TN, FP and FN are the numbers of true positives, true negatives, false positives and false negatives, respectively. The AUROC was usually used to illustrate the model's ability to discriminate between positive samples and negative samples. When the dataset is imbalanced, especially when there are very few positive samples, AUROC may provide an optimistic view towards the model and AUPRC may be a better choice than AUROC.</p>
      <p id="p0125">For the regression tasks, coefficient of determination (<inline-formula><mml:math id="M82" altimg="si61.svg"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>) was used to evaluate the performance of these tasks and defined as follows for every task as Eq. <xref rid="fd18" ref-type="disp-formula">(18)</xref>:<disp-formula id="fd18"><label>(18)</label><mml:math id="M83" altimg="si62.svg" alttext="Equation 18."><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn><mml:mo linebreak="goodbreak">−</mml:mo><mml:mfrac><mml:mrow><mml:mo>∑</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msup><mml:mo linebreak="badbreak">−</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>ˆ</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mo>∑</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msup><mml:mo linebreak="badbreak">−</mml:mo><mml:mover accent="true"><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M84" altimg="si63.svg"><mml:mrow><mml:mover accent="true"><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is the average of all <inline-formula><mml:math id="M85" altimg="si49.svg"><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>.</p>
      <p id="p0130">Pearson correlation coefficient (PCC) was used to evaluate the relevance between the task-specific parameters in the classification models, and it is defined as Eq. <xref rid="fd19" ref-type="disp-formula">(19)</xref>:<disp-formula id="fd19"><label>(19)</label><mml:math id="M86" altimg="si64.svg" alttext="Equation 19."><mml:mrow><mml:msub><mml:mtext>PCC</mml:mtext><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>cov</mml:mtext><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mi>Y</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M87" altimg="si65.svg"><mml:mrow><mml:mtext>cov</mml:mtext><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the covariance between <inline-formula><mml:math id="M88" altimg="si66.svg"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M89" altimg="si67.svg"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M90" altimg="si68.svg"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>X</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the standard deviation of <inline-formula><mml:math id="M91" altimg="si66.svg"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="M92" altimg="si69.svg"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>Y</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the standard deviation of <inline-formula><mml:math id="M93" altimg="si67.svg"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula>.</p>
      <p id="p0135">Other four descriptor-based models and five GNN-based classification models were established and evaluated in the same way. The uncertainty weighting strategy was not applied in these models and the total loss was the average cross entropy for all tasks. The descriptor-based models include single-task random forest (ST-RF)<xref rid="bib51" ref-type="bibr"><sup>51</sup></xref>, single-task extreme gradient boosting (ST-XGBoost)<xref rid="bib52" ref-type="bibr"><sup>52</sup></xref>, single-task Deep Neural Networks (ST-DNN) and multi-task Deep Neural Networks (MT-DNN)<xref rid="bib38" ref-type="bibr"><sup>38</sup></xref>, and the GNN-based models include single-task Graph Isomorphism Network (ST-GIN)<xref rid="bib36" ref-type="bibr"><sup>36</sup></xref>, multi-task Graph Attention network (MT-GAT)<xref rid="bib53" ref-type="bibr"><sup>53</sup></xref>, multi-task Attentive FP (MT-Attentive FP)<xref rid="bib54" ref-type="bibr"><sup>54</sup></xref>, multi-task Directed Message Passing Neural Network (MT-DMPNN)<xref rid="bib55" ref-type="bibr"><sup>55</sup></xref> and multi-task Graph Isomorphism Network (MT-GIN). The detailed information of these ML models can be seen in Supporting Information.</p>
    </sec>
    <sec id="sec2.5">
      <label>2.5</label>
      <title>Interpretation for models</title>
      <p id="p0140">Although DL has posed a considerable impact in chemistry, one important issue that cannot be overlooked is the lack of interpretability in this field. Interpretability is essential because it guarantees our trust and transparency in the decision process of ML models<xref rid="bib24" ref-type="bibr"><sup>24</sup></xref>. Hence, we designed a strategy called edges masking to understand the results of the models in order to ensure that the conclusion derived from the model is rational. The edges masking was designed to offer the interpretability results for the molecular property prediction tasks in the GNN model, and it was inspired by the idea from this article<xref rid="bib56" ref-type="bibr"><sup>56</sup></xref>.</p>
      <p id="p0145">As seen in <xref rid="fig2" ref-type="fig">Fig. 2</xref>, the concept behind this strategy is simple and straightforward. The importance score for each atom in the molecule was calculated as follows. First, an original molecular graph is fed into a GNN model to get a base prediction value. Then, by masking out some chemically significant substructures in the molecule, a modified molecular graph is created. Briefly, the edges around a specified atom are masked out to construct the modified molecule graph, which is roughly equivalent to masking out the related functional groups at the molecular level with some rationality in chemistry. The modified molecule graph is fed into the GNN model and it gets a masking prediction value. The <inline-formula><mml:math id="M94" altimg="si70.svg"><mml:mrow><mml:mtext>Important score</mml:mtext></mml:mrow></mml:math></inline-formula> of this specified atom is the difference between the base prediction value and the masking prediction value as Eq. <xref rid="fd20" ref-type="disp-formula">(20)</xref>:<disp-formula id="fd20"><label>(20)</label><mml:math id="M95" altimg="si71.svg" alttext="Equation 20."><mml:mrow><mml:msub><mml:mtext>Importance score</mml:mtext><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msup><mml:msub><mml:mi>G</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M96" altimg="si13.svg"><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:math></inline-formula> represents the original molecule graph, <inline-formula><mml:math id="M97" altimg="si72.svg"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the <inline-formula><mml:math id="M98" altimg="si73.svg"><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mtext>t</mml:mtext><mml:mtext>h</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> atom in the molecule, <inline-formula><mml:math id="M99" altimg="si74.svg"><mml:mrow><mml:msup><mml:msub><mml:mi>G</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> represents the modified molecule graph by masking out the edges around <inline-formula><mml:math id="M100" altimg="si72.svg"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="M101" altimg="si75.svg"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:math></inline-formula> represents any GNN model. The importance score of each atom in a molecule is calculated. Then, by dividing by the largest absolute importance score value, these importance scores of all atoms in the molecule are normalized. The more important the contribution of the chemical environment around an atom to the model's output, the higher its importance score.<fig id="fig2"><label>Figure 2</label><caption><p>The computation method of the importance score for a specified atom.</p></caption><alt-text id="alttext0040">Figure 2</alt-text><graphic xlink:href="gr2"/></fig></p>
    </sec>
  </sec>
  <sec id="sec3">
    <label>3</label>
    <title>Result and discussion</title>
    <sec id="sec3.1">
      <label>3.1</label>
      <title>The overall performance of AMGU on 204 kinases</title>
      <p id="p0150">The average AUROC and average AUPRC are used to assess the performances of AMGU and the other ML models. The performances of all the tested models are summarized in <xref rid="tbl4" ref-type="table">Table 4</xref>, and the detailed information of each model performance is presented in Supporting Information <xref rid="appsec3" ref-type="sec">Table S2</xref>. We can observe that the multi-task learning models significantly outperform the single-task learning models across all the three sets, with a 6.48%–10.97% absolute gap in the average AUROC and a 9.33%–12.69% absolute gap in the average AUPRC between the multi-task learning models and single-task learning models across the three test sets, demonstrating that the multi-task learning models are more superior for kinase inhibition prediction. In addition, the AMGU model outperforms the other multi-task learning models, with an average AUROC of 0.9425 and an average AUPRC of 0.8907 for the internal test set, an average AUROC of 0.8708 and an average AUPRC of 0.3773 for the PKIS dataset, and an average AUROC of 0.7796 and an average AUPRC of 0.3436 for the PKIS2 dataset. The AMGU model outperforms MT-GIN on both the internal and external test sets, demonstrating that the auxiliary learning and uncertainty weighting strategy have favorable contributions the multi-task learning in kinase inhibition prediction.<table-wrap position="float" id="tbl4"><label>Table 4</label><caption><p>The performances of all the models on the internal test set and external test sets<xref rid="tbl4fna" ref-type="table-fn">a</xref>.</p></caption><alt-text id="alttext0080">Table 4</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Dataset</th><th>Model</th><th>AUROC</th><th>AUPRC<xref rid="tbl4fnb" ref-type="table-fn">b</xref></th></tr></thead><tbody><tr><td rowspan="10">Internal test set</td><td>ST-RF</td><td>0.8777 ± 0.0659</td><td>0.7974 ± 0.1621</td></tr><tr><td>ST-XGB</td><td>0.8666 ± 0.0747</td><td>0.7831 ± 0.1719</td></tr><tr><td>ST-DNN</td><td>0.8677 ± 0.0750</td><td>0.7891 ± 0.1649</td></tr><tr><td>ST-GIN</td><td>0.8643 ± 0.0762</td><td>0.7723 ± 0.1766</td></tr><tr><td>MT-DNN</td><td>0.9403 ± 0.0321</td><td>0.8849 ± 0.1032</td></tr><tr><td>MT-Attentive_FP</td><td>0.9345 ± 0.0345</td><td>0.8778 ± 0.1059</td></tr><tr><td>MT-DMPNN</td><td>0.9380 ± 0.0330</td><td>0.8831 ± 0.1026</td></tr><tr><td>MT-GAT</td><td>0.9347 ± 0.0353</td><td>0.8788 ± 0.1033</td></tr><tr><td>MT-GIN</td><td>0.9415 ± 0.0332</td><td>0.8879 ± 0.1005</td></tr><tr><td>AMGU</td><td><bold>0.9425 ± 0.0321</bold></td><td><bold>0.8907 ± 0.1003</bold></td></tr><tr><td rowspan="10">PKIS</td><td>ST-RF</td><td>0.7699 ± 0.1150</td><td>0.2601 ± 0.2153</td></tr><tr><td>ST-XGB</td><td>0.7611 ± 0.1131</td><td>0.2504 ± 0.1990</td></tr><tr><td>ST-DNN</td><td>0.7842 ± 0.1110</td><td>0.2709 ± 0.1923</td></tr><tr><td>ST-GIN</td><td>0.7820 ± 0.1176</td><td>0.2554 ± 0.1913</td></tr><tr><td>MT-DNN</td><td>0.8323 ± 0.0900</td><td>0.3266 ± 0.1922</td></tr><tr><td>MT-Attentive_FP</td><td>0.8431 ± 0.0833</td><td>0.3136 ± 0.1999</td></tr><tr><td>MT-DMPNN</td><td>0.8526 ± 0.0820</td><td>0.3291 ± 0.2038</td></tr><tr><td>MT-GAT</td><td>0.8387 ± 0.0779</td><td>0.3208 ± 0.1948</td></tr><tr><td>MT-GIN</td><td>0.8455 ± 0.0769</td><td>0.3319 ± 0.2045</td></tr><tr><td>AMGU</td><td><bold>0.8708 ± 0.0773</bold></td><td><bold>0.3773 ± 0.2097</bold></td></tr><tr><td rowspan="10">PKIS 2</td><td>ST-RF</td><td>0.7071 ± 0.0744</td><td>0.2352 ± 0.1290</td></tr><tr><td>ST-XGB</td><td>0.6921 ± 0.0741</td><td>0.2273 ± 0.1240</td></tr><tr><td>ST-DNN</td><td>0.6951 ± 0.0786</td><td>0.2379 ± 0.1300</td></tr><tr><td>ST-GIN</td><td>0.7029 ± 0.0787</td><td>0.2273 ± 0.1270</td></tr><tr><td>MT-DNN</td><td>0.7482 ± 0.0663</td><td>0.3015 ± 0.1463</td></tr><tr><td>MT-Attentive_FP</td><td>0.7607 ± 0.0722</td><td>0.3039 ± 0.1440</td></tr><tr><td>MT-DMPNN</td><td>0.7594 ± 0.0701</td><td>0.3090 ± 0.1503</td></tr><tr><td>MT-GAT</td><td>0.7573 ± 0.0692</td><td>0.3040 ± 0.1442</td></tr><tr><td>MT-GIN</td><td>0.7619 ± 0.0704</td><td>0.3188 ± 0.1539</td></tr><tr><td>AMGU</td><td><bold>0.7796 ± 0.0741</bold></td><td><bold>0.3436 ± 0.1607</bold></td></tr></tbody></table><table-wrap-foot><fn id="tbl4fna"><label>a</label><p id="ntpara0020">The maximum values of AUROC and AUPRC for different datasets are bold.</p></fn></table-wrap-foot><table-wrap-foot><fn id="tbl4fnb"><label>b</label><p id="ntpara0025">The low AUPRC values in two external test sets due to low positive rate in these datasets.</p></fn></table-wrap-foot></table-wrap></p>
      <p id="p0155">Some other important binary classification evaluation metrics were also utilized to evaluate the models, with the ST-GIN model and MT-GIN as the baseline to further verify the advantages of the AMGU model. The performances of AMGU, MT-GIN and ST-GIN on various datasets are listed in <xref rid="tbl5" ref-type="table">Table 5</xref>. When compared with the ST-GIN model, the performance of AMGU is always better than that of ST-GIN on the different datasets under the main classification metrics. When compared with the MT-GIN model, the AMGU model shows close or even slightly higher performance than the MT-GIN model in the internal test set. However, the main advantages of the AMGU model over the MT-GIN model are shown in two external datasets collected from the result of high throughput screening. It is noteworthy that all metrics except Recall of the AGMU model have been boosted. Compared with the MT-GIN model, more stringent criteria are taken in the AMGU model to distinguish positive samples from negative samples, which results in the decline of Recall and the boost of Precision. But taking overall improvement under the main classification metrics into consideration, it is acceptable for the lower value of Recall in the AMGU model than the MT-GIN model (but still higher than the ST-GIN model). Hence, the AMGU model is more suitable than the MT-GIN model for applications in real-world scenarios to some extent. These results further illustrate the advantages of the AMGU model.<table-wrap position="float" id="tbl5"><label>Table 5</label><caption><p>The performances of different methods on the different dataset in terms of main classification metrics<xref rid="tbl5fna" ref-type="table-fn">a</xref>.</p></caption><alt-text id="alttext0085">Table 5</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Dataset</th><th>Metric</th><th>ST-GIN</th><th>MT-GIN</th><th>AMGU</th></tr></thead><tbody><tr><td rowspan="8">Internal test set</td><td>AUROC</td><td>0.8643 ± 0.0762</td><td>0.9415 ± 0.0332</td><td><bold>0.9425 ± 0.0321</bold></td></tr><tr><td>AUPRC</td><td>0.7723 ± 0.1766</td><td>0.8879 ± 0.1005</td><td><bold>0.8907 ± 0.1003</bold></td></tr><tr><td>P</td><td>0.6795 ± 0.1878</td><td><bold>0.7888 ± 0.1348</bold></td><td>0.7849 ± 0.1401</td></tr><tr><td>R</td><td>0.7613 ± 0.1293</td><td>0.8564 ± 0.0712</td><td><bold>0.8571 ± 0.0724</bold></td></tr><tr><td>F1</td><td>0.7077 ± 0.1639</td><td><bold>0.8140 ± 0.1074</bold></td><td><bold>0.8122 ± 0.1109</bold></td></tr><tr><td>Accuracy</td><td>0.8163 ± 0.0650</td><td><bold>0.8854 ± 0.0330</bold></td><td>0.8846 ± 0.0330</td></tr><tr><td>BA</td><td>0.7930 ± 0.0785</td><td>0.8720 ± 0.0427</td><td><bold>0.8716 ± 0.0433</bold></td></tr><tr><td>MCC</td><td>0.5653 ± 0.1653</td><td><bold>0.7222 ± 0.1002</bold></td><td><bold>0.7204 ± 0.1030</bold></td></tr><tr><td rowspan="8">PKIS</td><td>AUROC</td><td>0.7820 ± 0.1176</td><td>0.8455 ± 0.0769</td><td><bold>0.8708 ± 0.0773</bold></td></tr><tr><td>AUPRC</td><td>0.2554 ± 0.1913</td><td>0.3319 ± 0.2045</td><td><bold>0.3773 ± 0.2097</bold></td></tr><tr><td>P</td><td>0.1560 ± 0.1038</td><td>0.1732 ± 0.1137</td><td><bold>0.2020 ± 0.1249</bold></td></tr><tr><td>R</td><td>0.6181 ± 0.2147</td><td><bold>0.7438 ± 0.1700</bold></td><td>0.7390 ± 0.1929</td></tr><tr><td>F1</td><td>0.2303 ± 0.1243</td><td>0.2610 ± 0.1350</td><td><bold>0.2938 ± 0.1427</bold></td></tr><tr><td>Accuracy</td><td>0.7874 ± 0.0942</td><td>0.7853 ± 0.0832</td><td><bold>0.8157 ± 0.0866</bold></td></tr><tr><td>BA</td><td>0.7077 ± 0.1072</td><td>0.7659 ± 0.0832</td><td><bold>0.7797 ± 0.0926</bold></td></tr><tr><td>MCC</td><td>0.2138 ± 0.1141</td><td>0.2673 ± 0.1050</td><td><bold>0.3007 ± 0.1185</bold></td></tr><tr><td rowspan="8">PKIS 2</td><td>AUROC</td><td>0.7029 ± 0.0787</td><td>0.7619 ± 0.0704</td><td><bold>0.7796 ± 0.0741</bold></td></tr><tr><td>AUPRC</td><td>0.2273 ± 0.1270</td><td>0.3188 ± 0.1539</td><td><bold>0.3436 ± 0.1607</bold></td></tr><tr><td>P</td><td>0.1974 ± 0.1125</td><td>0.2340 ± 0.1265</td><td><bold>0.2631 ± 0.1409</bold></td></tr><tr><td>R</td><td>0.4745 ± 0.1555</td><td><bold>0.5822 ± 0.1519</bold></td><td>0.5693 ± 0.1682</td></tr><tr><td>F1</td><td>0.2575 ± 0.1127</td><td>0.3133 ± 0.1278</td><td><bold>0.3346 ± 0.1329</bold></td></tr><tr><td>Accuracy</td><td>0.7744 ± 0.0893</td><td>0.7904 ± 0.0730</td><td><bold>0.8139 ± 0.0757</bold></td></tr><tr><td>BA</td><td>0.6381 ± 0.0658</td><td>0.6954 ± 0.0660</td><td><bold>0.7027 ± 0.0720</bold></td></tr><tr><td>MCC</td><td>0.1859 ± 0.0970</td><td>0.2601 ± 0.1041</td><td><bold>0.2844 ± 0.1145</bold></td></tr></tbody></table><table-wrap-foot><fn id="tbl5fna"><label>a</label><p id="ntpara0030">The maximum values of metrics for different datasets are bold.</p></fn></table-wrap-foot></table-wrap></p>
      <p id="p0160">Furthermore, we compared the performances of our model and the Li's model<xref rid="bib22" ref-type="bibr"><sup>22</sup></xref> for the same tasks on the PKIS and PKIS 2 datasets in terms of AUROC because only the metric of AUROC was reported by Li et al. There are 121 same targets in the PKIS dataset and 170 same targets in the PKIS 2 dataset. The average AUROC values of the same tasks for the AMGU and Li's models were calculated and compared. As shown in <xref rid="tbl6" ref-type="table">Table 6</xref>, on both PKIS and PKIS 2 datasets, our model performs better in those tasks. Furthermore, the AUROC values for the two separate external test sets show substantial difference between our model and Li's model, highlighting the superiority of our model. More details of the comparison between our model and Li's model can be found in Supporting Information <xref rid="appsec3" ref-type="sec">Table S3</xref>.<table-wrap position="float" id="tbl6"><label>Table 6</label><caption><p>The comparison of the performances between our model and the Li's model.</p></caption><alt-text id="alttext0090">Table 6</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Dataset</th><th>AMGU</th><th>Li's Model</th><th><italic>P</italic> value<xref rid="tbl6fna" ref-type="table-fn">a</xref></th></tr></thead><tbody><tr><td>PKIS</td><td>0.8724 ± 0.0777</td><td>0.8203 ± 0.1017</td><td>0.000</td></tr><tr><td>PKIS 2</td><td>0.7740 ± 0.0700</td><td>0.7073 ±0.0756</td><td>0.000</td></tr></tbody></table><table-wrap-foot><fn id="tbl6fna"><label>a</label><p id="ntpara0035">The significance differences for the Mann‒Whitney U test.</p></fn></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="sec3.2">
      <label>3.2</label>
      <title>The contribution of auxiliary learning and uncertainty weighting strategy</title>
      <p id="p0165">For kinase inhibition prediction, we also investigated the role of the auxiliary learning and uncertainty weighting strategies in the multi-task learning. For conducting the ablation experiments, the MT-GIN model with the auxiliary learning (Aux-MT-GIN) and the MT-GIN model with the uncertainty weighting strategy (MT-GIN-UN) were developed. The results of the ablation experiments are shown in <xref rid="tbl7" ref-type="table">Table 7</xref>. The overall performances of the different models on the two external test sets rank as follows: AMGU &gt; MT-GIN-UN &gt; MT-GIN &gt; Aux-MT-GIN. When the auxiliary learning strategy was solely used with the regression tasks in MT-GIN, it (Aux-MT-GIN) degrades the performances of the classification tasks when compared with the original MT-GIN. This might be attributed to the fact that the regression tasks have larger-scale losses than the classification tasks, making the Aux-MT-GIN model pay more attention to regression tasks so that the auxiliary learning part in the model can't really work even downgrade the performance of original tasks. When using only the uncertainty weighting in MT-GIN, it (MT-GIN-UN) can achieve a minor improvement in the two independent external test sets, demonstrating the effectiveness of the uncertainty weighting in this dataset to some extent. When these two techniques were combined in MT-GIN to generate the AMGU model, it yields a minor improvement in all the test sets, with a maximum absolute improvement of 3.6% in AUPRC and 2.16% in AUROC when compared with the original MT-GIN model. Therefore, the improved performance of the AMGU model might be attributed to the combination of these two tactics. The losses and task weights in the classification and regression tasks are taken into account for further analysis, and their outcome is presented in <xref rid="tbl8" ref-type="table">Table 8</xref>. Due to the low homoscedastic uncertainty (training loss) in the classification tasks, the uncertainty weighting strategy might automatically balance any conflicts between the regression and classification tasks and provide larger weights to the classification tasks than the regression tasks. The classification task weights are nearly twice as high as the regression task weights, causing our model to pay more attention to the classification tasks and improving its performance on the classification tasks.<table-wrap position="float" id="tbl7"><label>Table 7</label><caption><p>The performances of the Auxiliary leaning.</p></caption><alt-text id="alttext0095">Table 7</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Dataset</th><th>Model</th><th>AUROC</th><th>AUPRC</th></tr></thead><tbody><tr><td rowspan="4">Internal test set</td><td>MT-GIN</td><td>0.9415 ± 0.0306</td><td>0.8877 ± 0.0967</td></tr><tr><td>MT-GIN-UN</td><td>0.9384 ± 0.0320</td><td>0.8824 ± 0.1007</td></tr><tr><td>Aux-MT-GIN</td><td>0.9379 ± 0.0329</td><td>0.8826 ± 0.1045</td></tr><tr><td>AMGU</td><td>0.9425 ± 0.0321</td><td>0.8907 ± 0.1003</td></tr><tr><td rowspan="4">PKIS</td><td>MT-GIN</td><td>0.8455 ± 0.0769</td><td>0.3319 ± 0.2045</td></tr><tr><td>MT-GIN-UN</td><td>0.8546 ± 0.0756</td><td>0.3436 ± 0.2043</td></tr><tr><td>Aux-MT-GIN</td><td>0.8303 ± 0.0872</td><td>0.3182 ± 0.1966</td></tr><tr><td>AMGU</td><td>0.8708 ± 0.0773</td><td>0.3773 ± 0.2097</td></tr><tr><td rowspan="4">PKIS 2</td><td>MT-GIN</td><td>0.7619 ± 0.0704</td><td>0.3188 ± 0.1539</td></tr><tr><td>MT-GIN-UN</td><td>0.7661 ± 0.0701</td><td>0.3187 ± 0.1550</td></tr><tr><td>Aux-MT-GIN</td><td>0.7522 ± 0.0686</td><td>0.2919 ± 0.1478</td></tr><tr><td>AMGU</td><td>0.7796 ± 0.0741</td><td>0.3436 ± 0.1607</td></tr></tbody></table></table-wrap><table-wrap position="float" id="tbl8"><label>Table 8</label><caption><p>The task weights and training loss of all tasks in AMGU<xref rid="tbl8fna" ref-type="table-fn">a</xref>.</p></caption><alt-text id="alttext0100">Table 8</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Task</th><th>Task weights</th><th>Training loss</th></tr></thead><tbody><tr><td>Regression tasks</td><td>1.1299 ± 0.0266</td><td>0.0956 ± 0.0478</td></tr><tr><td>Classification tasks</td><td>2.3144 ± 0.0209</td><td>0.0616 ± 0.0212</td></tr></tbody></table><table-wrap-foot><fn id="tbl8fna"><label>a</label><p id="ntpara0040">The task weights and training loss of all tasks are presented mean ± standard deviation.</p></fn></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="sec3.3">
      <label>3.3</label>
      <title>Tasks with small data volumes benefit more from AMGU model</title>
      <p id="p0170">As mentioned above, the AMGU model outperforms the ST-GIN model on nearly 90% of the tasks. Our results show that the tasks with small data quantities benefit more than those with large data volumes. <xref rid="fig3" ref-type="fig">Fig. 3</xref> depicts the association between the training data size and the performances of the AMGU and ST-GIN models. Obviously, the AMGU model consistently outperforms the ST-GIN model due to the significant transfer learning effect of multi-task learning<xref rid="bib22" ref-type="bibr"><sup>22</sup></xref><sup>,</sup><xref rid="bib57" ref-type="bibr"><sup>57</sup></xref>, suggesting that related tasks can benefit from the AMGU model. In addition, as demonstrated in <xref rid="fig3" ref-type="fig">Fig. 3</xref>, the tasks with small data volumes benefit more from the AMGU model than those with large data volumes. It is quite challenging to build highly discriminative classifiers using single-task learning methods because some kinase targets have limited activity data. The AMGU model can exploit the domain information from related tasks and boost the performances of these tasks with limited data points, which may hopefully aid the discovery of new inhibitors for these understudied kinases.<fig id="fig3"><label>Figure 3</label><caption><p>The performances of ST-GIN and AMGU on the tasks with different data volumes. A bar indicates the average AUROC or AUPRC of tasks with the number of bioactivity data points within the underlying range. The significance differences are under Mann–Whitney U test and shown in the figure according to the following standards: ∗0.01≤<italic>P</italic>&lt;0.05; ∗∗0.001≤<italic>P</italic>&lt; 0.01; ∗∗∗<italic>P</italic>&lt; 0.001.</p></caption><alt-text id="alttext0045">Figure 3</alt-text><graphic xlink:href="gr3"/></fig></p>
    </sec>
    <sec id="sec3.4">
      <label>3.4</label>
      <title>AMGU can reveal the correlation between different kinases</title>
      <p id="p0175">Another advantage of our multi-task learning model is that it can recognize data characteristics and learn the underlying correlations between different kinases automatically. The parameters of the task-specific parameters of the classification tasks in the final fully-connected layer of the AMGU model (<xref rid="fig1" ref-type="fig">Fig. 1</xref>) were extracted independently to analyze the relevance between these tasks. Every task was represented by a vector with 1001 dimensions. The Pearson correlation coefficient for each vector pair was calculated. As illustrated in <xref rid="fig4" ref-type="fig">Fig. 4</xref>, the inhibitors within the same group tend to be clustered more closely than those outside the group, particularly for the CMGC, AGC, CK1, CAMK, and Tyr kinase groups (in red box). This result is consistent with the fact that there are some kinase inhibitors referred to as “group-selective inhibitors” which are broadly active against a single group of kinases but selective outside that group<xref rid="bib12" ref-type="bibr"><sup>12</sup></xref>. As a result, we can deduce that multi-task learning can conceptually capture the inherent characteristics of data.<fig id="fig4"><label>Figure 4</label><caption><p>The Pearson correlation coefficient between different classification tasks. All tasks are organized by the kinase groups, which are based on the UniProt database's classification. The tasks against the same group begin with the name of the group and end with the name of the other group on the axis.</p></caption><alt-text id="alttext0050">Figure 4</alt-text><graphic xlink:href="gr4"/></fig></p>
    </sec>
    <sec id="sec3.5">
      <label>3.5</label>
      <title>Interpretation for EGRF inhibitors</title>
      <p id="p0180">In addition to the model's accuracy, it is quite important to interpret the underlying predictive mechanisms behind the AMGU model. Here, 4-anilinoquinazolines derivatives, one of the most important classes of EGFR inhibitors, were selected to interpret the prediction results of the EGFR inhibition prediction task for the AMGU model because their quantitative structure–activity relationships (QSAR) have been well-established<xref rid="bib58" ref-type="bibr"><sup>58</sup></xref><sup>,</sup><xref rid="bib59" ref-type="bibr"><sup>59</sup></xref>. The scaffold structure of 4-anilinoquinazolines is shown in <xref rid="fig5" ref-type="fig">Fig. 5</xref>A. As proven previously, the N<sub>1</sub> and N<sub>3</sub> atoms in 4-anilinoquinazolines play the most critical roles in suppressing the activity of EGFR<xref rid="bib59" ref-type="bibr"><sup>59</sup></xref>.<fig id="fig5"><label>Figure 5</label><caption><p>(A) The scaffold structure of 4-anilinoquinazolines derivatives. (B) The interpretability results of representative EGFR inhibitors. The chemical name and the model's anticipated value are annotated below the molecule structure. The first-generation and second-generation EGFR inhibitors are shown in the first row and second row, respectively, and the acrylamide moiety is highlighted with a black dotted circle.</p></caption><alt-text id="alttext0055">Figure 5</alt-text><graphic xlink:href="gr5"/></fig></p>
      <p id="p0185">The edges masking strategy was used to interpret the prediction results of the AMGU model. Two first-generation inhibitors (<italic>i.e</italic>., gefitinib and erlotinib) and three second-generation inhibitors (<italic>i.e</italic>., afatinib, canertinib, and dacomitinib) of EGFR were analyzed as the representative examples. <xref rid="fig5" ref-type="fig">Fig. 5</xref>B depicts the results of the interpretability test. For all these inhibitors, the edges masking interpretability approach highlights the atoms around N<sub>1</sub> and N<sub>3</sub>, indicating the relevance of the chemical environment around these atoms contributing to the model's output. Moreover, the acrylamide moiety is the key component of the second-generation EGFR inhibitors, and it serves as an electrophilic warhead that conducts Michael addition with the conserved C797 residue in the EGFR active region<xref rid="bib60" ref-type="bibr"><sup>60</sup></xref>. The removal of this substructure from the molecule alters this type of inhibitors from covalent to non-covalent, but does not significantly change its inhibitory activity. Our model also identified these small variations in molecules as well, as seen in <xref rid="fig5" ref-type="fig">Fig. 5</xref>B, but did not give this component a high importance score.</p>
      <p id="p0190">Overall, the consistency of the interpretability results with the QSAR of 4-anilinoquinazolines demonstrates that our model has learned several critical molecular structures to some level, which raises our confidence to the model.</p>
    </sec>
    <sec id="sec3.6">
      <label>3.6</label>
      <title>Web server for the identification of kinase inhibitors</title>
      <p id="p0195">To share our models with other chemists and pharmacologists, we developed a web server called Kinase Inhibition Prediction (KIP) (<ext-link ext-link-type="uri" xlink:href="http://cadd.zju.edu.cn/kip" id="intref0030">http://cadd.zju.edu.cn/kip</ext-link>) to profile the kinome-wide polypharmacology effects of small molecules (<xref rid="fig6" ref-type="fig">Fig. 6</xref>). The KIP, which was developed based on the Django framework, is freely available to non-commercial users. The web server is able to predict the biological activities towards kinases for small molecules, and it can also provide an interpretable explanation from the model to the positive outcome. Given the website's user-friendliness, the RDKit (Release 2019.09.1)<xref rid="bib61" ref-type="bibr"><sup>61</sup></xref> and Plotly (<ext-link ext-link-type="uri" xlink:href="https://plotly.com/python/" id="intref0035">https://plotly.com/python/</ext-link>) were employed for the depiction of molecules and the visualization of results, respectively. The datasets utilized in this study and the trained models are also available on this website.<fig id="fig6"><label>Figure 6</label><caption><p>The illustrations of the KIP webservice. (A) Input page of the website; (B) Result page of the website.</p></caption><alt-text id="alttext0060">Figure 6</alt-text><graphic xlink:href="gr6"/></fig></p>
      <p id="p0200">The workflow of KIP is as follows: (i) The client-side (browser) submits a query molecule for bioactivity prediction by sketching its structure from the ChemDoodle panel<xref rid="bib62" ref-type="bibr"><sup>62</sup></xref> or inputting its SMILES. Multiple-molecule Comma-Separated Values (CSV) or Structure Data File (SDF) files are also acceptable for submission. (ii) The kinome-wide inhibitory activities against 204 kinases are predicted at the server-side based on the AMGU model. (iii) The model's outputs are saved in a CSV file that can be downloaded. The visualization of the result and the explanation of the result are also available online for inspection.</p>
    </sec>
  </sec>
  <sec id="sec4">
    <label>4</label>
    <title>Conclusions</title>
    <p id="p0205">In this study, the AMGU model based on the MT-GIN method combined with the auxiliary learning and uncertainty weighting strategy was proposed and used to simultaneously predict the inhibition profiles of small molecules against 204 kinases. The calculation results illustrate that the overall performance of the AMGU model is better than those of the other descriptor-based and GNN-based models, including ST-XGB, ST-RF, ST-DNN, MT-DNN, ST-GIN, MT-Attentive FP, MT-DMPNN, MT-GAT and MT-GIN on the test datasets. The ablation studies were carried out to further verify the effectiveness of the AMGU model. When both the auxiliary learning and uncertainty weighting strategy were integrated with the MT-GIN method, the corresponding models yielded better performance, suggesting that the combination of these two strategies jointly improved the performance of the model. The AMGU model can automatically assign higher task weights for the classification tasks due to low homoscedastic uncertainty in the classification tasks when analyzing the task weights and training losses of the classification and regression tasks. The AMGU mode has two advantages. One is that, due to the strong transfer learning ability, it can improve the generalizability for nearly all tasks, especially those with limited data. Another advantage is that our multi-task learning model can comprehend the data's characteristics and learn the potential correlation between tasks automatically to a certain extent. The model's correlation is based on the kinase group, which corresponds to the real-world situation where some “group-selective inhibitors” exist that are active against a single kinase group but selective outside of that group. Then, a simple model-agnostic interpretable strategy for GNN called edges masking was designed to understand the model's potential decision-making process. The consistency between the interpretability results of five representative EGFR inhibitors and their QSAR showed that our model could learn some key molecular structures. Finally, a freely accessible webserver named KIP was developed for the implementation of the well-trained models. Overall, our multi-task learning model makes large-scale kinome-wide virtual profiling of small molecular simple, which could help explain unwanted side effects, repurpose drugs, and discover new hit compounds.</p>
  </sec>
</body>
<back>
  <ref-list id="cebib0010">
    <title>References</title>
    <ref id="bib1">
      <label>1</label>
      <element-citation publication-type="journal" id="sref1">
        <person-group person-group-type="author">
          <name>
            <surname>Manning</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Whyte</surname>
            <given-names>D.B.</given-names>
          </name>
          <name>
            <surname>Martinez</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Hunter</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Sudarsanam</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>The protein kinase complement of the human genome</article-title>
        <source>Science</source>
        <volume>298</volume>
        <year>2002</year>
        <fpage>1912</fpage>
        <lpage>1934</lpage>
        <pub-id pub-id-type="pmid">12471243</pub-id>
      </element-citation>
    </ref>
    <ref id="bib2">
      <label>2</label>
      <element-citation publication-type="journal" id="sref2">
        <person-group person-group-type="author">
          <name>
            <surname>Cohen</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>The origins of protein phosphorylation. Nat</article-title>
        <source>Cell Biol</source>
        <volume>4</volume>
        <year>2002</year>
        <fpage>E127</fpage>
        <lpage>E130</lpage>
      </element-citation>
    </ref>
    <ref id="bib3">
      <label>3</label>
      <element-citation publication-type="journal" id="sref3">
        <person-group person-group-type="author">
          <name>
            <surname>Eglen</surname>
            <given-names>R.M.</given-names>
          </name>
          <name>
            <surname>Reisine</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>The current status of drug discovery against the human kinome</article-title>
        <source>Assay Drug Dev Technol</source>
        <volume>7</volume>
        <year>2009</year>
        <fpage>22</fpage>
        <lpage>43</lpage>
        <pub-id pub-id-type="pmid">19382888</pub-id>
      </element-citation>
    </ref>
    <ref id="bib4">
      <label>4</label>
      <element-citation publication-type="journal" id="sref4">
        <person-group person-group-type="author">
          <name>
            <surname>Roskoski</surname>
            <given-names>R.</given-names>
            <suffix>Jr.</suffix>
          </name>
        </person-group>
        <article-title>Properties of FDA-approved small molecule protein kinase inhibitors: a 2021 update</article-title>
        <source>Pharmacol Res</source>
        <volume>165</volume>
        <year>2021</year>
        <fpage>105463</fpage>
        <pub-id pub-id-type="pmid">33513356</pub-id>
      </element-citation>
    </ref>
    <ref id="bib5">
      <label>5</label>
      <element-citation publication-type="journal" id="sref5">
        <person-group person-group-type="author">
          <name>
            <surname>Attwood</surname>
            <given-names>M.M.</given-names>
          </name>
          <name>
            <surname>Fabbro</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Sokolov</surname>
            <given-names>A.V.</given-names>
          </name>
          <name>
            <surname>Knapp</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Schioth</surname>
            <given-names>H.B.</given-names>
          </name>
        </person-group>
        <article-title>Trends in kinase drug discovery: targets, indications and inhibitor design</article-title>
        <source>Nat Rev Drug Discov</source>
        <volume>20</volume>
        <year>2021</year>
        <fpage>839</fpage>
        <lpage>861</lpage>
        <pub-id pub-id-type="pmid">34354255</pub-id>
      </element-citation>
    </ref>
    <ref id="bib6">
      <label>6</label>
      <element-citation publication-type="journal" id="sref6">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Nielsen</surname>
            <given-names>T.E.</given-names>
          </name>
          <name>
            <surname>Clausen</surname>
            <given-names>M.H.</given-names>
          </name>
        </person-group>
        <article-title>FDA-approved small-molecule kinase inhibitors</article-title>
        <source>Trends Pharmacol Sci</source>
        <volume>36</volume>
        <year>2015</year>
        <fpage>422</fpage>
        <lpage>439</lpage>
        <pub-id pub-id-type="pmid">25975227</pub-id>
      </element-citation>
    </ref>
    <ref id="bib7">
      <label>7</label>
      <element-citation publication-type="journal" id="sref7">
        <person-group person-group-type="author">
          <name>
            <surname>Essegian</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Khurana</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Stathias</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Schürer</surname>
            <given-names>S.C.</given-names>
          </name>
        </person-group>
        <article-title>The clinical kinase index: a method to prioritize understudied kinases as drug targets for the treatment of cancer</article-title>
        <source>Cell Rep Med</source>
        <volume>1</volume>
        <year>2020</year>
        <fpage>100128</fpage>
        <pub-id pub-id-type="pmid">33205077</pub-id>
      </element-citation>
    </ref>
    <ref id="bib8">
      <label>8</label>
      <element-citation publication-type="journal" id="sref8">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>P.L.</given-names>
          </name>
          <name>
            <surname>Gray</surname>
            <given-names>N.S.</given-names>
          </name>
        </person-group>
        <article-title>Targeting cancer with small molecule kinase inhibitors</article-title>
        <source>Nat Rev Cancer</source>
        <volume>9</volume>
        <year>2009</year>
        <fpage>28</fpage>
        <lpage>39</lpage>
        <pub-id pub-id-type="pmid">19104514</pub-id>
      </element-citation>
    </ref>
    <ref id="bib9">
      <label>9</label>
      <element-citation publication-type="journal" id="sref9">
        <person-group person-group-type="author">
          <name>
            <surname>Tucker</surname>
            <given-names>J.A.</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>M.P.</given-names>
          </name>
        </person-group>
        <article-title>Recent advances in kinase drug discovery part I: the editors' take</article-title>
        <source>Int J Mol Sci</source>
        <volume>22</volume>
        <year>2021</year>
        <fpage>7560</fpage>
        <pub-id pub-id-type="pmid">34299180</pub-id>
      </element-citation>
    </ref>
    <ref id="bib10">
      <label>10</label>
      <element-citation publication-type="journal" id="sref10">
        <person-group person-group-type="author">
          <name>
            <surname>Ferrè</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Palmeri</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Helmer-Citterich</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Computational methods for analysis and inference of kinase/inhibitor relationships</article-title>
        <source>Front Genet</source>
        <volume>5</volume>
        <year>2014</year>
        <fpage>196</fpage>
        <pub-id pub-id-type="pmid">25071826</pub-id>
      </element-citation>
    </ref>
    <ref id="bib11">
      <label>11</label>
      <element-citation publication-type="journal" id="sref11">
        <person-group person-group-type="author">
          <name>
            <surname>Anastassiadis</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Deacon</surname>
            <given-names>S.W.</given-names>
          </name>
          <name>
            <surname>Devarajan</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Peterson</surname>
            <given-names>J.R.</given-names>
          </name>
        </person-group>
        <article-title>Comprehensive assay of kinase catalytic activity reveals features of kinase inhibitor selectivity</article-title>
        <source>Nat Biotechnol</source>
        <volume>29</volume>
        <year>2011</year>
        <fpage>1039</fpage>
        <lpage>1045</lpage>
        <pub-id pub-id-type="pmid">22037377</pub-id>
      </element-citation>
    </ref>
    <ref id="bib12">
      <label>12</label>
      <element-citation publication-type="journal" id="sref12">
        <person-group person-group-type="author">
          <name>
            <surname>Davis</surname>
            <given-names>M.I.</given-names>
          </name>
          <name>
            <surname>Hunt</surname>
            <given-names>J.P.</given-names>
          </name>
          <name>
            <surname>Herrgard</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Ciceri</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Wodicka</surname>
            <given-names>L.M.</given-names>
          </name>
          <name>
            <surname>Pallares</surname>
            <given-names>G.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Comprehensive analysis of kinase inhibitor selectivity</article-title>
        <source>Nat Biotechnol</source>
        <volume>29</volume>
        <year>2011</year>
        <fpage>1046</fpage>
        <lpage>1051</lpage>
        <pub-id pub-id-type="pmid">22037378</pub-id>
      </element-citation>
    </ref>
    <ref id="bib13">
      <label>13</label>
      <element-citation publication-type="journal" id="sref13">
        <person-group person-group-type="author">
          <name>
            <surname>Drewry</surname>
            <given-names>D.H.</given-names>
          </name>
          <name>
            <surname>Wells</surname>
            <given-names>C.I.</given-names>
          </name>
          <name>
            <surname>Andrews</surname>
            <given-names>D.M.</given-names>
          </name>
          <name>
            <surname>Angell</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Al-Ali</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Axtman</surname>
            <given-names>A.D.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Progress towards a public chemogenomic set for protein kinases and a call for contributions</article-title>
        <source>PLoS One</source>
        <volume>12</volume>
        <year>2017</year>
        <object-id pub-id-type="publisher-id">e0181585</object-id>
      </element-citation>
    </ref>
    <ref id="bib14">
      <label>14</label>
      <element-citation publication-type="journal" id="sref14">
        <person-group person-group-type="author">
          <name>
            <surname>Metz</surname>
            <given-names>J.T.</given-names>
          </name>
          <name>
            <surname>Johnson</surname>
            <given-names>E.F.</given-names>
          </name>
          <name>
            <surname>Soni</surname>
            <given-names>N.B.</given-names>
          </name>
          <name>
            <surname>Merta</surname>
            <given-names>P.J.</given-names>
          </name>
          <name>
            <surname>Kifle</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Hajduk</surname>
            <given-names>P.J.</given-names>
          </name>
        </person-group>
        <article-title>Navigating the kinome</article-title>
        <source>Nat Chem Biol</source>
        <volume>7</volume>
        <year>2011</year>
        <fpage>200</fpage>
        <lpage>202</lpage>
        <pub-id pub-id-type="pmid">21336281</pub-id>
      </element-citation>
    </ref>
    <ref id="bib15">
      <label>15</label>
      <element-citation publication-type="journal" id="sref15">
        <person-group person-group-type="author">
          <name>
            <surname>Bento</surname>
            <given-names>A.P.</given-names>
          </name>
          <name>
            <surname>Gaulton</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Hersey</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Bellis</surname>
            <given-names>L.J.</given-names>
          </name>
          <name>
            <surname>Chambers</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Davies</surname>
            <given-names>M.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The ChEMBL bioactivity database: an update</article-title>
        <source>Nucleic Acids Res</source>
        <volume>42</volume>
        <year>2014</year>
        <fpage>D1083</fpage>
        <lpage>D1090</lpage>
        <pub-id pub-id-type="pmid">24214965</pub-id>
      </element-citation>
    </ref>
    <ref id="bib16">
      <label>16</label>
      <element-citation publication-type="journal" id="sref16">
        <person-group person-group-type="author">
          <name>
            <surname>Elkins</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Fedele</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Szklarz</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Azeez</surname>
            <given-names>K.R.A.</given-names>
          </name>
          <name>
            <surname>Salah</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Mikolajczyk</surname>
            <given-names>J.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Comprehensive characterization of the published kinase inhibitor set</article-title>
        <source>Nat Biotechnol</source>
        <volume>34</volume>
        <year>2016</year>
        <fpage>95</fpage>
        <lpage>103</lpage>
        <pub-id pub-id-type="pmid">26501955</pub-id>
      </element-citation>
    </ref>
    <ref id="bib17">
      <label>17</label>
      <element-citation publication-type="journal" id="sref17">
        <person-group person-group-type="author">
          <name>
            <surname>Merget</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Turk</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Eid</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Rippmann</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Fulle</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Profiling prediction of kinase inhibitors: toward the virtual assay</article-title>
        <source>J Med Chem</source>
        <volume>60</volume>
        <year>2017</year>
        <fpage>474</fpage>
        <lpage>485</lpage>
        <pub-id pub-id-type="pmid">27966949</pub-id>
      </element-citation>
    </ref>
    <ref id="bib18">
      <label>18</label>
      <element-citation publication-type="journal" id="sref18">
        <person-group person-group-type="author">
          <name>
            <surname>Avram</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Bora</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Halip</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Curpan</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Modeling kinase inhibition using highly confident data sets</article-title>
        <source>J Chem Inf Model</source>
        <volume>58</volume>
        <year>2018</year>
        <fpage>957</fpage>
        <lpage>967</lpage>
        <pub-id pub-id-type="pmid">29708742</pub-id>
      </element-citation>
    </ref>
    <ref id="bib19">
      <label>19</label>
      <element-citation publication-type="journal" id="sref19">
        <person-group person-group-type="author">
          <name>
            <surname>Janssen</surname>
            <given-names>A.P.A.</given-names>
          </name>
          <name>
            <surname>Grimm</surname>
            <given-names>S.H.</given-names>
          </name>
          <name>
            <surname>Wijdeven</surname>
            <given-names>R.H.M.</given-names>
          </name>
          <name>
            <surname>Lenselink</surname>
            <given-names>E.B.</given-names>
          </name>
          <name>
            <surname>Neefjes</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>van Boeckel</surname>
            <given-names>C.A.A.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Drug discovery maps, a machine learning model that visualizes and predicts kinome-ilnhibitor interaction landscapes</article-title>
        <source>J Chem Inf Model</source>
        <volume>59</volume>
        <year>2019</year>
        <fpage>1221</fpage>
        <lpage>1229</lpage>
        <pub-id pub-id-type="pmid">30372617</pub-id>
      </element-citation>
    </ref>
    <ref id="bib20">
      <label>20</label>
      <element-citation publication-type="journal" id="sref20">
        <person-group person-group-type="author">
          <name>
            <surname>Caruana</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Multitask learning</article-title>
        <source>Mach Learn</source>
        <volume>28</volume>
        <year>1997</year>
        <fpage>41</fpage>
        <lpage>75</lpage>
      </element-citation>
    </ref>
    <ref id="bib21">
      <label>21</label>
      <element-citation publication-type="journal" id="sref21">
        <person-group person-group-type="author">
          <name>
            <surname>Rodriguez-Perez</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Bajorath</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Multitask machine learning for classifying highly and weakly potent kinase inhibitors</article-title>
        <source>ACS Omega</source>
        <volume>4</volume>
        <year>2019</year>
        <fpage>4367</fpage>
        <lpage>4375</lpage>
      </element-citation>
    </ref>
    <ref id="bib22">
      <label>22</label>
      <element-citation publication-type="journal" id="sref22">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Xiong</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Fu</surname>
            <given-names>Z.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep learning enhancing kinome-wide polypharmacology profiling: model construction and experiment validation</article-title>
        <source>J Med Chem</source>
        <volume>63</volume>
        <year>2020</year>
        <fpage>8723</fpage>
        <lpage>8737</lpage>
        <pub-id pub-id-type="pmid">31364850</pub-id>
      </element-citation>
    </ref>
    <ref id="bib23">
      <label>23</label>
      <element-citation publication-type="journal" id="sref23">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Ramsundar</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Feinberg</surname>
            <given-names>E.N.</given-names>
          </name>
          <name>
            <surname>Gomes</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Geniesse</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Pappu</surname>
            <given-names>A.S.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>MoleculeNet: a benchmark for molecular machine learning</article-title>
        <source>Chem Sci</source>
        <volume>9</volume>
        <year>2018</year>
        <fpage>513</fpage>
        <lpage>530</lpage>
        <pub-id pub-id-type="pmid">29629118</pub-id>
      </element-citation>
    </ref>
    <ref id="bib24">
      <label>24</label>
      <element-citation publication-type="journal" id="sref24">
        <person-group person-group-type="author">
          <name>
            <surname>Ying</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Bourgeois</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>You</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zitnik</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Leskovec</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>GNNExplainer: generating explanations for graph neural networks</article-title>
        <source>Adv Neural Inf Process Syst</source>
        <volume>32</volume>
        <year>2019</year>
        <fpage>9240</fpage>
        <lpage>9251</lpage>
        <pub-id pub-id-type="pmid">32265580</pub-id>
      </element-citation>
    </ref>
    <ref id="bib25">
      <label>25</label>
      <element-citation publication-type="journal" id="sref25">
        <person-group person-group-type="author">
          <name>
            <surname>Jiang</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Hsieh</surname>
            <given-names>C.Y.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Liao</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Z.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Could graph neural networks learn better molecular representation for drug discovery? A comparison study of descriptor-based and graph-based models</article-title>
        <source>J Cheminf</source>
        <volume>13</volume>
        <year>2021</year>
        <fpage>12</fpage>
      </element-citation>
    </ref>
    <ref id="bib26">
      <label>26</label>
      <element-citation publication-type="journal" id="sref26">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Hsieh</surname>
            <given-names>C.Y.</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Hou</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Mining toxicity information from large amounts of toxicity data</article-title>
        <source>J Med Chem</source>
        <volume>64</volume>
        <year>2021</year>
        <fpage>6924</fpage>
        <lpage>6936</lpage>
        <pub-id pub-id-type="pmid">33961429</pub-id>
      </element-citation>
    </ref>
    <ref id="bib27">
      <label>27</label>
      <element-citation publication-type="journal" id="sref27">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>X.C.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>C.K.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Z.J.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>Z.X.</given-names>
          </name>
          <name>
            <surname>Yi</surname>
            <given-names>J.C.</given-names>
          </name>
          <name>
            <surname>Hsieh</surname>
            <given-names>C.Y.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>MG-BERT: leveraging unsupervised atomic representation learning for molecular property prediction</article-title>
        <source>Briefings Bioinf</source>
        <volume>22</volume>
        <year>2021</year>
        <fpage>bbab152</fpage>
      </element-citation>
    </ref>
    <ref id="bib28">
      <label>28</label>
      <element-citation publication-type="journal" id="sref28">
        <person-group person-group-type="author">
          <name>
            <surname>Vandenhende</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Georgoulis</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Van Gansbeke</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Proesmans</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Dai</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Van Gool</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Multi-task learning for dense prediction tasks: a survey</article-title>
        <source>IEEE Trans Pattern Anal Mach Intell</source>
        <volume>44</volume>
        <year>2022</year>
        <fpage>3614</fpage>
        <lpage>3633</lpage>
        <pub-id pub-id-type="pmid">33497328</pub-id>
      </element-citation>
    </ref>
    <ref id="bib29">
      <label>29</label>
      <element-citation publication-type="journal" id="sref29">
        <person-group person-group-type="author">
          <name>
            <surname>Sener</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Koltun</surname>
            <given-names>V.</given-names>
          </name>
        </person-group>
        <article-title>Multi-task learning as multi-objective optimization</article-title>
        <source>arXiv</source>
        <year>2019</year>
        <object-id pub-id-type="publisher-id">1810.04650</object-id>
      </element-citation>
    </ref>
    <ref id="bib30">
      <label>30</label>
      <element-citation publication-type="journal" id="sref30">
        <person-group person-group-type="author">
          <name>
            <surname>Bateman</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>M.J.</given-names>
          </name>
          <name>
            <surname>O'Donovan</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Magrane</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Alpi</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Antunes</surname>
            <given-names>R.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>UniProt: the universal protein knowledgebase</article-title>
        <source>Nucleic Acids Res</source>
        <volume>45</volume>
        <year>2017</year>
        <fpage>D158</fpage>
        <lpage>D169</lpage>
        <pub-id pub-id-type="pmid">27899622</pub-id>
      </element-citation>
    </ref>
    <ref id="bib31">
      <label>31</label>
      <element-citation publication-type="journal" id="sref31">
        <person-group person-group-type="author">
          <name>
            <surname>Sutherland</surname>
            <given-names>J.J.</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Cahya</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Vieth</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>What general conclusions can we draw from kinase profiling data sets?</article-title>
        <source>Biochim Biophys Acta</source>
        <volume>1834</volume>
        <year>2013</year>
        <fpage>1425</fpage>
        <lpage>1433</lpage>
        <pub-id pub-id-type="pmid">23333421</pub-id>
      </element-citation>
    </ref>
    <ref id="bib32">
      <label>32</label>
      <element-citation publication-type="journal" id="sref32">
        <person-group person-group-type="author">
          <name>
            <surname>Tang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Szwajda</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Shakyawar</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Hintsanen</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Wennerberg</surname>
            <given-names>K.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Making sense of large-scale kinase inhibitor bioactivity data sets: a comparative and integrative analysis</article-title>
        <source>J Chem Inf Model</source>
        <volume>54</volume>
        <year>2014</year>
        <fpage>735</fpage>
        <lpage>743</lpage>
        <pub-id pub-id-type="pmid">24521231</pub-id>
      </element-citation>
    </ref>
    <ref id="bib33">
      <label>33</label>
      <element-citation publication-type="journal" id="sref33">
        <person-group person-group-type="author">
          <name>
            <surname>Hu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Bajorath</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Exploring the scaffold universe of kinase inhibitors</article-title>
        <source>J Med Chem</source>
        <volume>58</volume>
        <year>2015</year>
        <fpage>315</fpage>
        <lpage>332</lpage>
        <pub-id pub-id-type="pmid">25192260</pub-id>
      </element-citation>
    </ref>
    <ref id="bib34">
      <label>34</label>
      <element-citation publication-type="journal" id="sref34">
        <person-group person-group-type="author">
          <name>
            <surname>Kipf</surname>
            <given-names>T.N.</given-names>
          </name>
          <name>
            <surname>Welling</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Semi-supervised classification with graph convolutional networks</article-title>
        <source>arXiv</source>
        <year>2016</year>
        <object-id pub-id-type="publisher-id">02907</object-id>
        <comment>1609</comment>
      </element-citation>
    </ref>
    <ref id="bib35">
      <label>35</label>
      <element-citation publication-type="journal" id="sref35">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Fan</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Gu</surname>
            <given-names>Y.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Dgl-lifesci: an open-source toolkit for deep learning on graphs in life science</article-title>
        <source>ACS Omega</source>
        <volume>6</volume>
        <year>2021</year>
        <fpage>27233</fpage>
        <lpage>27238</lpage>
        <pub-id pub-id-type="pmid">34693143</pub-id>
      </element-citation>
    </ref>
    <ref id="bib36">
      <label>36</label>
      <element-citation publication-type="journal" id="sref36">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Leskovec</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Jegelka</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>How powerful are graph neural networks?</article-title>
        <source>arXiv</source>
        <year>2018</year>
        <object-id pub-id-type="publisher-id">00826</object-id>
        <comment>1810</comment>
      </element-citation>
    </ref>
    <ref id="bib37">
      <label>37</label>
      <element-citation publication-type="journal" id="sref37">
        <person-group person-group-type="author">
          <name>
            <surname>Gilmer</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Schoenholz</surname>
            <given-names>S.S.</given-names>
          </name>
          <name>
            <surname>Riley</surname>
            <given-names>P.F.</given-names>
          </name>
          <name>
            <surname>Vinyals</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Dahl</surname>
            <given-names>G.E.</given-names>
          </name>
        </person-group>
        <article-title>Neural message passing for quantum chemistry</article-title>
        <source>arXiv</source>
        <year>2017</year>
        <object-id pub-id-type="publisher-id">01212</object-id>
        <comment>1704</comment>
      </element-citation>
    </ref>
    <ref id="bib38">
      <label>38</label>
      <element-citation publication-type="journal" id="sref38">
        <person-group person-group-type="author">
          <name>
            <surname>Shrestha</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Mahmood</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Review of deep learning algorithms and architectures</article-title>
        <source>IEEE Access</source>
        <volume>7</volume>
        <year>2019</year>
        <fpage>53040</fpage>
        <lpage>53065</lpage>
      </element-citation>
    </ref>
    <ref id="bib39">
      <label>39</label>
      <mixed-citation publication-type="other" id="sref39">Brownlee J. Imbalanced classification with python: choose better metrics, balance skewed classes, cost-sensitive learning. Available from: <ext-link ext-link-type="uri" xlink:href="https://download.csdn.net/download/DomicZhong/19844813" id="intref0045">https://download.csdn.net/download/DomicZhong/19844813</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib40">
      <label>40</label>
      <element-citation publication-type="journal" id="sref40">
        <person-group person-group-type="author">
          <name>
            <surname>Ruder</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>An overview of multi-task learning in deep neural networks</article-title>
        <source>arXiv</source>
        <year>2017</year>
        <fpage>1706</fpage>
        <comment>05098</comment>
      </element-citation>
    </ref>
    <ref id="bib41">
      <label>41</label>
      <element-citation publication-type="journal" id="sref41">
        <person-group person-group-type="author">
          <name>
            <surname>Ioffe</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Szegedy</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>Batch normalization: accelerating deep network training by reducing internal covariate shift</article-title>
        <source>arXiv</source>
        <year>2015</year>
        <object-id pub-id-type="publisher-id">1502.03167</object-id>
      </element-citation>
    </ref>
    <ref id="bib42">
      <label>42</label>
      <element-citation publication-type="journal" id="sref42">
        <person-group person-group-type="author">
          <name>
            <surname>Srivastava</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Krizhevsky</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Salakhutdinov</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title>
        <source>J Mach Learn Res</source>
        <volume>15</volume>
        <year>2014</year>
        <fpage>1929</fpage>
        <lpage>1958</lpage>
      </element-citation>
    </ref>
    <ref id="bib43">
      <label>43</label>
      <element-citation publication-type="journal" id="sref43">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Badrinarayanan</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>C.Y.</given-names>
          </name>
          <name>
            <surname>Rabinovich</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Gradient normalization for adaptive loss balancing in deep multitask networks</article-title>
        <source>arXiv</source>
        <year>2018</year>
        <fpage>1711</fpage>
        <comment>02257</comment>
      </element-citation>
    </ref>
    <ref id="bib44">
      <label>44</label>
      <element-citation publication-type="journal" id="sref44">
        <person-group person-group-type="author">
          <name>
            <surname>Kendall</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Gal</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Cipolla</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</article-title>
        <source>arXiv</source>
        <year>2018</year>
        <object-id pub-id-type="publisher-id">07115v2.1705</object-id>
      </element-citation>
    </ref>
    <ref id="bib45">
      <label>45</label>
      <element-citation publication-type="journal" id="sref45">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zhuang</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Yin</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <article-title>Auxiliary learning for deep multi-task learning</article-title>
        <source>arXiv</source>
        <year>2019</year>
        <object-id pub-id-type="publisher-id">02214.1909</object-id>
      </element-citation>
    </ref>
    <ref id="bib46">
      <label>46</label>
      <element-citation publication-type="journal" id="sref46">
        <person-group person-group-type="author">
          <name>
            <surname>Paszke</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Gross</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Massa</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Lerer</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Bradbury</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Chanan</surname>
            <given-names>G.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Pytorch: an imperative style, high-performance deep learning library</article-title>
        <source>arXiv</source>
        <year>2019</year>
        <object-id pub-id-type="publisher-id">01703.1912</object-id>
      </element-citation>
    </ref>
    <ref id="bib47">
      <label>47</label>
      <element-citation publication-type="journal" id="sref47">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Ye</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Gan</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>X.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep graph library: a graph-centric, highly-performant package for graph neural networks</article-title>
        <source>arXiv</source>
        <year>2019</year>
        <object-id pub-id-type="publisher-id">01315.1909</object-id>
      </element-citation>
    </ref>
    <ref id="bib48">
      <label>48</label>
      <element-citation publication-type="journal" id="sref48">
        <person-group person-group-type="author">
          <name>
            <surname>Kingma</surname>
            <given-names>D.P.</given-names>
          </name>
          <name>
            <surname>Ba</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Adam: A method for stochastic optimization</article-title>
        <source>arXiv</source>
        <year>2014</year>
        <object-id pub-id-type="publisher-id">1412.6980</object-id>
      </element-citation>
    </ref>
    <ref id="bib49">
      <label>49</label>
      <mixed-citation publication-type="other" id="sref49">Head T, Kumar M, Nahrstaedt H, Louppe G, Shcherbatyi I. Scikit-optimize (0.9.7). Zenodo. Available from: <pub-id pub-id-type="doi">10.5281/zenodo.6451894</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib50">
      <label>50</label>
      <element-citation publication-type="journal" id="sref50">
        <person-group person-group-type="author">
          <name>
            <surname>Fawcett</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>An introduction to ROC analysis</article-title>
        <source>Pattern Recogn Lett</source>
        <volume>27</volume>
        <year>2006</year>
        <fpage>861</fpage>
        <lpage>874</lpage>
      </element-citation>
    </ref>
    <ref id="bib51">
      <label>51</label>
      <element-citation publication-type="journal" id="sref51">
        <person-group person-group-type="author">
          <name>
            <surname>Breiman</surname>
          </name>
        </person-group>
        <article-title>Random forests</article-title>
        <source>Mach Learn</source>
        <volume>45</volume>
        <year>2001</year>
        <fpage>5</fpage>
        <lpage>32</lpage>
      </element-citation>
    </ref>
    <ref id="bib52">
      <label>52</label>
      <element-citation publication-type="journal" id="sref52">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Guestrin</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>XGBoost: a scalable tree boosting system</article-title>
        <source>arXiv</source>
        <year>2016</year>
        <fpage>1603</fpage>
        <comment>02754v3</comment>
      </element-citation>
    </ref>
    <ref id="bib53">
      <label>53</label>
      <element-citation publication-type="journal" id="sref53">
        <person-group person-group-type="author">
          <name>
            <surname>Velickovic</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Cucurull</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Casanova</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Romero</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Lio</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Graph attention networks</article-title>
        <source>Stat</source>
        <volume>1050</volume>
        <year>2017</year>
        <fpage>20</fpage>
      </element-citation>
    </ref>
    <ref id="bib54">
      <label>54</label>
      <element-citation publication-type="journal" id="sref54">
        <person-group person-group-type="author">
          <name>
            <surname>Xiong</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Zhong</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Wan</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Pushing the boundaries of molecular representation for drug discovery with the graph attention mechanism</article-title>
        <source>J Med Chem</source>
        <volume>63</volume>
        <year>2020</year>
        <fpage>8749</fpage>
        <lpage>8760</lpage>
        <pub-id pub-id-type="pmid">31408336</pub-id>
      </element-citation>
    </ref>
    <ref id="bib55">
      <label>55</label>
      <element-citation publication-type="journal" id="sref55">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Swanson</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Jin</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Coley</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Eiden</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>H.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Analyzing learned molecular representations for property prediction</article-title>
        <source>J Chem Inf Model</source>
        <volume>59</volume>
        <year>2019</year>
        <fpage>3370</fpage>
        <lpage>3388</lpage>
        <pub-id pub-id-type="pmid">31361484</pub-id>
      </element-citation>
    </ref>
    <ref id="bib56">
      <label>56</label>
      <element-citation publication-type="journal" id="sref56">
        <person-group person-group-type="author">
          <name>
            <surname>Riniker</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Landrum</surname>
            <given-names>G.A.</given-names>
          </name>
        </person-group>
        <article-title>Similarity maps—a visualization strategy for molecular fingerprints and machine-learning methods</article-title>
        <source>J Cheminf</source>
        <volume>5</volume>
        <year>2013</year>
        <fpage>43</fpage>
      </element-citation>
    </ref>
    <ref id="bib57">
      <label>57</label>
      <element-citation publication-type="journal" id="sref57">
        <person-group person-group-type="author">
          <name>
            <surname>de la Vega de León</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Gillet</surname>
            <given-names>V.J.</given-names>
          </name>
        </person-group>
        <article-title>Effect of missing data on multitask prediction methods</article-title>
        <source>J Cheminf</source>
        <volume>10</volume>
        <year>2018</year>
        <fpage>1</fpage>
        <lpage>12</lpage>
      </element-citation>
    </ref>
    <ref id="bib58">
      <label>58</label>
      <element-citation publication-type="journal" id="sref58">
        <person-group person-group-type="author">
          <name>
            <surname>Kamath</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Buolamwini</surname>
            <given-names>J.K.</given-names>
          </name>
        </person-group>
        <article-title>Targeting EGFR and HER-2 receptor tyrosine kinases for cancer drug discovery and development</article-title>
        <source>Med Res Rev</source>
        <volume>26</volume>
        <year>2006</year>
        <fpage>569</fpage>
        <lpage>594</lpage>
        <pub-id pub-id-type="pmid">16788977</pub-id>
      </element-citation>
    </ref>
    <ref id="bib59">
      <label>59</label>
      <element-citation publication-type="journal" id="sref59">
        <person-group person-group-type="author">
          <name>
            <surname>Ismail</surname>
            <given-names>R.S.M.</given-names>
          </name>
          <name>
            <surname>Ismail</surname>
            <given-names>N.S.M.</given-names>
          </name>
          <name>
            <surname>Abuserii</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Abou El Ella</surname>
            <given-names>D.A.</given-names>
          </name>
        </person-group>
        <article-title>Recent advances in 4-aminoquinazoline based scaffold derivatives targeting EGFR kinases as anticancer agents</article-title>
        <source>Future J Pharm Sci</source>
        <volume>2</volume>
        <year>2016</year>
        <fpage>9</fpage>
        <lpage>19</lpage>
      </element-citation>
    </ref>
    <ref id="bib60">
      <label>60</label>
      <element-citation publication-type="journal" id="sref60">
        <person-group person-group-type="author">
          <name>
            <surname>Ghosh</surname>
            <given-names>A.K.</given-names>
          </name>
          <name>
            <surname>Samanta</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Mondal</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>W.R.</given-names>
          </name>
        </person-group>
        <article-title>Covalent inhibition in drug discovery</article-title>
        <source>ChemMedChem</source>
        <volume>14</volume>
        <year>2019</year>
        <fpage>889</fpage>
        <lpage>906</lpage>
        <pub-id pub-id-type="pmid">30816012</pub-id>
      </element-citation>
    </ref>
    <ref id="bib61">
      <label>61</label>
      <element-citation publication-type="book" id="sref61">
        <person-group person-group-type="author">
          <name>
            <surname>Landrum</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <part-title>RDKit: open-source cheminformatics from machine learning to chemical registration</part-title>
        <year>1 Mar 2014</year>
        <publisher-name>Release</publisher-name>
        <comment>Available from: </comment>
        <pub-id pub-id-type="doi">10.5281/zenodo.10398</pub-id>
      </element-citation>
    </ref>
    <ref id="bib62">
      <label>62</label>
      <element-citation publication-type="journal" id="sref62">
        <person-group person-group-type="author">
          <name>
            <surname>Burger</surname>
            <given-names>M.C.</given-names>
          </name>
        </person-group>
        <article-title>ChemDoodle Web Components: HTML5 toolkit for chemical graphics, interfaces, and informatics</article-title>
        <source>J Cheminf</source>
        <volume>7</volume>
        <year>2015</year>
        <fpage>1</fpage>
        <lpage>7</lpage>
      </element-citation>
    </ref>
  </ref-list>
  <sec id="appsec1">
    <title>Author contributions</title>
    <p id="p0215">Lingjie Bao and Zhe Wang designed and performed experiments, analyzed data, and wrote the paper. Zhenxing Wu, Hao Luo and Jiahui Yu performed some experiments. Yu Kang, Dongsheng Cao and Tingjun Hou initiated the study, organized, designed, and wrote the paper. All authors read and approved the final manuscript.</p>
  </sec>
  <sec id="appsec2">
    <title>Conflicts of interest</title>
    <p id="p0220">The authors declare no conflicts of interest.</p>
  </sec>
  <sec id="appsec3" sec-type="supplementary-material">
    <label>Appendix A</label>
    <title>Supplementary data</title>
    <p id="p0225">The following are the Supplementary data to this article:<supplementary-material content-type="local-data" id="mmc1"><caption><title>Multimedia component 1</title></caption><media xlink:href="mmc1.pdf"><alt-text>Multimedia component 1</alt-text></media></supplementary-material><supplementary-material content-type="local-data" id="mmc2"><caption><title>Multimedia component 2</title></caption><media xlink:href="mmc2.xlsx"><alt-text>Multimedia component 2</alt-text></media></supplementary-material><supplementary-material content-type="local-data" id="mmc3"><caption><title>Multimedia component 3</title></caption><media xlink:href="mmc3.xlsx"><alt-text>Multimedia component 3</alt-text></media></supplementary-material><supplementary-material content-type="local-data" id="mmc4"><caption><title>Multimedia component 4</title></caption><media xlink:href="mmc4.xlsx"><alt-text>Multimedia component 4</alt-text></media></supplementary-material></p>
  </sec>
  <ack id="ack0010">
    <title>Acknowledgments</title>
    <p id="p0210">This work was financially supported by <funding-source id="gs1"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100012166</institution-id><institution>National Key Research and Development Program of China</institution></institution-wrap></funding-source> (2021YFF1201400), <funding-source id="gs2"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source> (21575128, 81773632, 22173118), and <funding-source id="gs3"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100004731</institution-id><institution>Natural Science Foundation of Zhejiang Province</institution></institution-wrap></funding-source> (LZ19H300001, China), and <funding-source id="gs4">Science and Technology Innovation Program of Hunan Province</funding-source> (2021RC4011, China).</p>
  </ack>
  <fn-group>
    <fn id="d35e1067">
      <p id="ntpara0010">Peer review under responsibility of Chinese Pharmaceutical Association and Institute of Materia Medica, Chinese Academy of Medical Sciences.</p>
    </fn>
    <fn id="appsec4" fn-type="supplementary-material">
      <label>Appendix A</label>
      <p id="p0230">Supporting data to this article can be found online at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.apsb.2022.05.004" id="intref0040">https://doi.org/10.1016/j.apsb.2022.05.004</ext-link>.</p>
    </fn>
  </fn-group>
</back>
