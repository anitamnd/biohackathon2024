<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6323831</article-id>
    <article-id pub-id-type="publisher-id">2584</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-018-2584-5</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>BO-LSTM: classifying relations via long short-term memory networks along biomedical ontologies</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7965-6536</contrib-id>
        <name>
          <surname>Lamurias</surname>
          <given-names>Andre</given-names>
        </name>
        <address>
          <email>alamurias@lasige.di.fc.ul.pt</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Sousa</surname>
          <given-names>Diana</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Clarke</surname>
          <given-names>Luka A.</given-names>
        </name>
        <address>
          <email>laclarke@fc.ul.pt</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Couto</surname>
          <given-names>Francisco M.</given-names>
        </name>
        <address>
          <email>fcouto@di.fc.ul.pt</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2181 4263</institution-id><institution-id institution-id-type="GRID">grid.9983.b</institution-id><institution>LASIGE, Faculdade de Ciências, Universidade de Lisboa, </institution></institution-wrap>Lisboa, 1749 016 Portugal </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2181 4263</institution-id><institution-id institution-id-type="GRID">grid.9983.b</institution-id><institution>University of Lisboa, Faculty of Sciences, BioISI - Biosystems &amp; Integrative Sciences Institute, </institution></institution-wrap>Campo Grande, C8 bdg, Lisboa, 1749 016 Portugal </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>7</day>
      <month>1</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>7</day>
      <month>1</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>20</volume>
    <elocation-id>10</elocation-id>
    <history>
      <date date-type="received">
        <day>25</day>
        <month>7</month>
        <year>2018</year>
      </date>
      <date date-type="accepted">
        <day>12</day>
        <month>12</month>
        <year>2018</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License(<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p>Recent studies have proposed deep learning techniques, namely recurrent neural networks, to improve biomedical text mining tasks. However, these techniques rarely take advantage of existing domain-specific resources, such as ontologies. In Life and Health Sciences there is a vast and valuable set of such resources publicly available, which are continuously being updated. Biomedical ontologies are nowadays a mainstream approach to formalize existing knowledge about entities, such as genes, chemicals, phenotypes, and disorders. These resources contain supplementary information that may not be yet encoded in training data, particularly in domains with limited labeled data.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>We propose a new model to detect and classify relations in text, BO-LSTM, that takes advantage of domain-specific ontologies, by representing each entity as the sequence of its ancestors in the ontology. We implemented BO-LSTM as a recurrent neural network with long short-term memory units and using open biomedical ontologies, specifically Chemical Entities of Biological Interest (ChEBI), Human Phenotype, and Gene Ontology. We assessed the performance of BO-LSTM with drug-drug interactions mentioned in a publicly available corpus from an international challenge, composed of 792 drug descriptions and 233 scientific abstracts. By using the domain-specific ontology in addition to word embeddings and WordNet, BO-LSTM improved the F1-score of both the detection and classification of drug-drug interactions, particularly in a document set with a limited number of annotations. We adapted an existing DDI extraction model with our ontology-based method, obtaining a higher F1 score than the original model. Furthermore, we developed and made available a corpus of 228 abstracts annotated with relations between genes and phenotypes, and demonstrated how BO-LSTM can be applied to other types of relations.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p>Our findings demonstrate that besides the high performance of current deep learning techniques, domain-specific ontologies can still be useful to mitigate the lack of labeled data.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Text mining</kwd>
      <kwd>Drug-drug interactions</kwd>
      <kwd>Deep learning</kwd>
      <kwd>Long short term memory</kwd>
      <kwd>Relation extraction</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>FCT (Portugal)</institution>
        </funding-source>
        <award-id>PD/BD/106083/2015</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>FCT (Portugal)</institution>
        </funding-source>
        <award-id>UID/MULTI/04046/2013</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>FCT (Portugal)</institution>
        </funding-source>
        <award-id>UID/CEC/00408/2013</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001871</institution-id>
            <institution>Fundação para a Ciência e a Tecnologia</institution>
          </institution-wrap>
        </funding-source>
        <award-id>PTDC/CCI-BIO/28685/2017</award-id>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p>Current relation extraction methods employ machine learning algorithms, often using kernel functions in conjunction with Support Vector Machines [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR2">2</xref>] or based on features extracted from the text [<xref ref-type="bibr" rid="CR3">3</xref>]. In recent years, deep learning techniques have obtained promising results in various Natural Language Processing (NLP) tasks [<xref ref-type="bibr" rid="CR4">4</xref>], including relation extraction [<xref ref-type="bibr" rid="CR5">5</xref>]. These techniques have the advantage of being easily adaptable to multiple domains, using models pre-trained on unlabeled documents [<xref ref-type="bibr" rid="CR6">6</xref>]. The success of deep learning for text mining is in part due to the high quantity of raw data available and the development of word vector models such as word2vec [<xref ref-type="bibr" rid="CR7">7</xref>] and GloVe [<xref ref-type="bibr" rid="CR8">8</xref>]. These models can use unlabeled data to predict the most probable word according to the context words (or vice-versa), leading to meaningful vector representations of the words in a corpus, known as word embeddings.</p>
    <p>A high volume of biomedical information relevant to the detection of Adverse Drug Reactions (ADRs), such as Drug-Drug Interactions (DDI), is mainly available in articles and patents [<xref ref-type="bibr" rid="CR9">9</xref>]. A recent review of studies about the causes of hospitalization in adult patients has found that ADRs were the most common cause, accounting for 7% of hospitalizations [<xref ref-type="bibr" rid="CR10">10</xref>]. Another systematic review focused on the European population, identified that 3.5% of hospital admissions were due to ADRs, while 10.1% of the patients experienced ADRs during hospitalization [<xref ref-type="bibr" rid="CR11">11</xref>].</p>
    <p>The knowledge encoded in the ChEBI (Chemical Entities of Biological Interest) ontology is highly valuable for detection and classification of DDIs, since it provides not only the important characteristics of each individual compound but also, more importantly, the underlying semantics of the relations between compounds. For instance, dopamine (CHEBI:18243), a chemical compound with several important roles in the brain and body, can be characterized as being a catecholamine (CHEBI:33567), an aralkylamino compound (CHEBI:64365) and an organic aromatic compound (CHEBI:33659) (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). When predicting if a certain drug interacts with dopamine, its ancestors will provide additional information that is not usually directly expressed in the text. While the reader can consult additional materials to better understand a biomedical document, current relation extraction models are trained solely on features extracted from the training corpus. Thus, ontologies confer an advantage to relation extraction models due to the semantics encoded in them regarding a particular domain. Since ontologies are described in a common machine-readable format, methods based on ontologies can be applied to different domains and incorporated with other sources of knowledge, bridging the semantic gap between relation extraction models, data sources, and results [<xref ref-type="bibr" rid="CR12">12</xref>].
<fig id="Fig1"><label>Fig. 1</label><caption><p>An excerpt of the ChEBI ontology showing the first ancestors of dopamine, using “is-a” relationships</p></caption><graphic xlink:href="12859_2018_2584_Fig1_HTML" id="MO1"/></fig>
</p>
    <sec id="Sec2">
      <title>Deep learning for biomedical NLP</title>
      <p>Current state-of-the-art text mining methods employ deep learning techniques, such as Recurrent Neural Networks (RNN), to train classification models based on word embeddings and other features. These methods use architectures composed of multiple layers, where each layer attempts to learn a different kind of representation of the input data. This way, different types of tasks can be trained using the same input data. Furthermore, there is no need to manually craft features for a specific task.</p>
      <p>Long Short-Term Memory (LSTM) networks have been proposed as an alternative to regular RNN [<xref ref-type="bibr" rid="CR13">13</xref>]. LSTMs are a type of RNN that can handle long dependencies, and thus are suitable for NLP tasks, which involve long sequences of words. When training the weights of an RNN, the contribution of the gradients may vanish while propagating for long sequences of words. LSTM units account for this vanishing gradient problem through a gated architecture, which makes it easier for the model to capture long-term dependencies. Recently, LSTMs have been applied to relation extraction tasks in various domains. Miwa and Bansal [<xref ref-type="bibr" rid="CR14">14</xref>] presented a model that extracted entities and relations based on bidirectional tree-structured and sequential LSTM-RNNs. The authors evaluated this model on three datasets, including the SemEval 2010 Task 8 dataset, which defines 10 general semantic relations types between nominals [<xref ref-type="bibr" rid="CR15">15</xref>].</p>
      <p>Bidirectional LSTMs have been proposed for relation extraction, obtaining better results than one-directional LSTMs on the SemEval 2010 dataset [<xref ref-type="bibr" rid="CR16">16</xref>]. In this case, at each time step, there are two LSTM layers, one that reads the sentence from left to right, and another that reads from right to left. The output of both layers is combined to produce a final score.</p>
      <p>The model proposed by Xu et al. [<xref ref-type="bibr" rid="CR17">17</xref>] combines Shortest Dependency Paths (SDP) between two entities in a sentence with linguistic information. SDPs are informative features for relations extraction since these contain the words of the sentence that refer directly to both entities. This model has a multichannel architecture, where each channel makes use of information from a different source along the SDP. The main channel, which contributes the most to the performance of the model, uses word embeddings trained on the English Wikipedia with word2vec. Additionally, the authors study the effect of adding channels consisting of the part-of-speech tags of each word, the grammatical relations between the words of the SDP, and the WordNet hypernyms of each word. Using all four channels, the F1-score of the SemEval 2010 Task 8 was 0.0135 higher than when using only the word embeddings channel. Although WordNet can be considered an ontology, its semantic properties were not integrated in this work, since only the word class is extracted, and the relations between classes are not considered.</p>
      <p>Deep learning approaches to DDI classification have been proposed in recent years, using the SemEval 2013: Task 9 DDI extraction corpus to train and evaluate their performance. Zhao et al. [<xref ref-type="bibr" rid="CR18">18</xref>] proposed a syntax convolutional neural network for DDI extraction, using word embeddings. Due to its success on other domains, LSTMs have also been used for DDI extraction [<xref ref-type="bibr" rid="CR19">19</xref>–<xref ref-type="bibr" rid="CR22">22</xref>]. Xu et al. [<xref ref-type="bibr" rid="CR21">21</xref>] proposed a method that combines domain-specific biomedical resources to train embedding vectors for biomedical concepts. However, their approach uses only contextual information from patient records and journal abstracts and does not take into account the relations between concepts that an ontology provides. While these works are similar to ours, we present the first model that makes use of a domain-ontology to classify DDIs.</p>
    </sec>
    <sec id="Sec3">
      <title>Ontologies for biomedical text mining</title>
      <p>While machine learning classifiers trained on word embeddings can learn to detect relations between entities, these classifiers may miss the underlying semantics of the entities according to their respective domain. However, the semantics of a given domain are, in some cases, available in the form of an ontology. Ontologies aim at providing a structured representation of the semantics of the concepts in a domain and their relations [<xref ref-type="bibr" rid="CR23">23</xref>]. In this paper, we consider a domain-specific ontology as a directed acyclic graph where each node is a concept (or entity) of the domain and the edges represent known relations between these concepts [<xref ref-type="bibr" rid="CR24">24</xref>]. This is a common representation of existing biomedical ontologies, which are nowadays a mainstream approach to formalize knowledge about entities, such as genes, chemicals, phenotypes, and disorders.</p>
      <p>Biomedical ontologies are usually publicly available and cover a large variety of topics related to Life and Health Sciences. In this paper, we use ChEBI, an ontology for chemical compounds with biological interest, where each node corresponds to a chemical compound [<xref ref-type="bibr" rid="CR25">25</xref>]. The latest release of ChEBI contains nearly 54k compounds and 163k relationships. Note that, the success of exploring a given biomedical ontology for performing a specific task can be easily extended to other topics due to the common structure of biomedical ontologies. For example, the same measures of metadata quality have been successfully applied to resources annotated with different biomedical ontologies [<xref ref-type="bibr" rid="CR26">26</xref>].</p>
      <p>Other authors have previously combined ontological information with neural networks, to improve the learning capabilities of a model. Li et al. [<xref ref-type="bibr" rid="CR27">27</xref>] mapped each word to a WordNet sense disambiguation to account for the different meanings that a word may have and the relations between word senses. Ma et al. [<xref ref-type="bibr" rid="CR28">28</xref>] proposed the LSTM-OLSI model, which indexes documents based on the word-level contextual information from the DBpedia ontology and document-level topic modeling. Some authors have explored graph embedding techniques, converting relations to a low dimensional space which represents the structure and properties of the graph [<xref ref-type="bibr" rid="CR29">29</xref>]. For example, Kong et al. [<xref ref-type="bibr" rid="CR30">30</xref>] combined heterogeneous sources of information, such as ontologies, to perform multi-label classification, while Dasigi et al. [<xref ref-type="bibr" rid="CR31">31</xref>] presented an embedding model based on ontology concepts to represent word tokens.</p>
      <p>However, few authors have explored biomedical ontologies for relation extraction. Textpresso is a project that aims at helping database curation by automatically extracting biomedical relations from research articles [<xref ref-type="bibr" rid="CR32">32</xref>]. Their approach incorporates an internal ontology to identify which terms may participate in relations according to their semantics. Other approaches measure the similarity between the entities and use the value as a feature for a machine learning classifier [<xref ref-type="bibr" rid="CR33">33</xref>]. One of the teams that participated in the BioCreative VI ChemProt task used ChEBI and Protein Ontology to extract additional features for a neural network model that extracted relation between chemicals and proteins [<xref ref-type="bibr" rid="CR34">34</xref>]. To the best of our knowledge, our work is the first attempt at incorporating ancestry information from biomedical ontologies with deep learning to extract relations from text.</p>
      <p>In this manuscript, we propose a new model, BO-LSTM that can explore domain information from ontologies to improve the task of biomedical relation extraction using deep learning techniques. We compare the effect of using ChEBI, a domain-specific ontology, and WordNet, a generic English language ontology, as external sources of information to train a classification model based on LSTM networks. This model was evaluated on a publicly available corpus of 792 drug descriptions and 233 scientific abstracts annotated with DDIs relevant to the study of adverse drug effects. Using the domain-specific ontology in addition to word embeddings and WordNet, BO-LSTM improved the F1-score of the classification of DDIs by 0.0207. Our model was particularly efficient with document types that were less represented in the training data. Moreover, we improved the F1-score of an existing DDI extraction model by 0.022 by adding our proposed ontology information, and demonstrated its applicability to other domains by generating a corpus of gene-phenotype relations and training our model on that corpus. The code and results obtained with the model can be found on our GitHub repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/lasigeBioTM/BOLSTM">https://github.com/lasigeBioTM/BOLSTM</ext-link>), while a Docker image is also available (<ext-link ext-link-type="uri" xlink:href="https://hub.docker.com/r/andrelamurias/bolstm">https://hub.docker.com/r/andrelamurias/bolstm</ext-link>), simplifying the process of training new classifiers and applying them to new data. We also made available the corpus produced for gene-phenotype relations, where each entity is mapped to an ontology concept. These results support our hypothesis that domain-specific information is useful to complement data-intensive approaches such as deep learning.</p>
    </sec>
  </sec>
  <sec id="Sec4">
    <title>Methods</title>
    <p>In this section, we describe the proposed BO-lSTM model in detail, as shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, with a focus on the aspects that refer to the use of biomedical ontologies.
<fig id="Fig2"><label>Fig. 2</label><caption><p>BO-LSTM Model architecture, using a sentence from the Drug-Drug Interactions corpus as an example. Each box represents a layer, with an output dimension, and merging lines represent concatenation. We refer to <bold>a</bold> as the Word embeddings channel, <bold>b</bold> the WordNet channel and <bold>c</bold> the ancestors concatenation channel and <bold>d</bold> the common ancestors channel</p></caption><graphic xlink:href="12859_2018_2584_Fig2_HTML" id="MO2"/></fig>
</p>
    <sec id="Sec5">
      <title>Data preparation</title>
      <p>The objective of our work is to identify and classify relations between biomedical entities found in natural language text. We assume that the relevant entities are already recognized. Therefore, we process the input data in order to generate instances to be classified by the model. Considering the set of entities <italic>E</italic> mentioned in a sentence, we generate <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\binom {E}{2}$\end{document}</tex-math><mml:math id="M2"><mml:mfenced close=")" open="(" separators=""><mml:mfrac linethickness="0.0pt"><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2018_2584_Article_IEq1.gif"/></alternatives></inline-formula> instances of that sentence. We refer to each instance as a candidate pair, identified by the two entities that constitute that pair, regardless of the order. A relation extraction model will assign a class to each candidate pair. In some cases, it is enough to simply classify the candidate pairs as negative or positive, while in other cases different types of positive relations are considered.</p>
      <p>An instance should contain the information necessary to classify a candidate pair. Therefore, after tokenizing each sentence, we obtain the Shortest Dependency Path (SDP) between the entities of the pair. For example, in the sentence “Laboratory Tests Response to Plenaxis <sub><italic>e</italic>1</sub> should be monitored by measuring serum total testosterone <sub><italic>e</italic>1</sub> concentrations just prior to administration on Day 29 and every 8 weeks thereafter”, the shortest path between the entities would be Plenaxis - Response - monitored - by - measuring - concentrations - testosterone. For both tokenization and dependency parsing, we use the spaCy software library (<ext-link ext-link-type="uri" xlink:href="https://spacy.io/">https://spacy.io/</ext-link>). The text of each entity that appears in the SDP, including the candidate entities, is replaced by the generic string to reduce the effect of specific entity names on the model. For each element of the SDP, we obtain the WordNet hypernym class using the tool developed by Ciaramita and Altun [<xref ref-type="bibr" rid="CR35">35</xref>].</p>
      <p>To focus our attention on the effect of the ontology information, we use pre-trained word embedding vectors. Pyysalo et al. [<xref ref-type="bibr" rid="CR36">36</xref>] released a set of vectors trained on PubMed abstracts (nearly 23 million) and PubMed Central full documents (nearly 700k), with the word2vec algorithm [<xref ref-type="bibr" rid="CR7">7</xref>]. Since these vectors were trained on a large biomedical corpus, it is likely that its vocabulary will contain more words relevant to the biomedical domain than the vocabulary of a generic corpus.</p>
      <p>We match each entity to an ontology concept so that we can then obtain its ancestors. Ontology concepts contain an ID, a preferred label, and, in most cases, synonyms. While pre-processing the data, we match each entity to the ontology using fuzzy matching. The adopted implementation uses the Levenshtein distance to assign a score to each match.</p>
      <p>Our pipeline first attempts to match the entity string to a concept label. If the match has a score equal to or higher than 0.7 (determined empirically), we accept that match and assign the concept ID to that entity. Otherwise, we match to a list of synonyms of ontology concepts. If that match has a score higher than the original score, we assign the ID of the matched synonym to the entity, otherwise, we revert to the original match. It is preferable to match to a concept label since these are more specific and should reflect the most common nomenclature of the concepts. This way, every entity was matched to a ChEBI concept, either to its preferred label or to a synonym. Due to the automatic linking method used, we cannot assume that every match is correct, but fuzzy matching has been used for similar purposes [<xref ref-type="bibr" rid="CR37">37</xref>], so we can assume that the best match is chosen. We matched 9020 unique entities to the preferred label and 877 to synonyms, and 1283 unique entities had an exact match to either a preferred label or synonym.</p>
      <p>The DDI corpus used to evaluate our method has a high imbalance of positive and negative relations, which hinders the training of a classification model. Even though only entities mentioned in the same sentence are considered as candidate DDIs, there is still a ratio of 1:5.9 positive to negative instances. Other authors have suggested reducing the number of negative relations through simple rules [<xref ref-type="bibr" rid="CR38">38</xref>, <xref ref-type="bibr" rid="CR39">39</xref>]. We excluded from training and automatically classify as negative the pairs that fit the following rules: 
<list list-type="bullet"><list-item><p>entities have the same text (regardless of case): in nearly every case a drug does not interact with itself;</p></list-item><list-item><p>the only text between the candidate pair is punctuation: consecutive entities, in the form of lists and enumerations, are not interacting, as well as instances where the abbreviation of an entity is introduced;</p></list-item><list-item><p>both entities have anti-positive governors: we follow the methodology proposed by [<xref ref-type="bibr" rid="CR38">38</xref>], where the headwords of entities that do not interact are used to filter less informative instances.</p></list-item></list></p>
      <p>With this filtering strategy, we used only 15,697 of the 27,792 pairs of the training corpus, obtaining a ratio of 1:3.5 positive to negative instances.</p>
      <p>We developed a corpus of 228 abstracts annotated with human phenotype-gene relations, which we refer to as the HP corpus, to demonstrate how our model could be applied to other relation extraction tasks. This corpus was based on an existing corpus that were manually annotated with 2773 concepts of the Human Phenotype Ontology [<xref ref-type="bibr" rid="CR40">40</xref>], corresponding to 2170 unique concepts. The developers of the Human Phenotype Ontology made available a file that links phenotypes and genes that are associated with the same diseases. Each gene of this file was automatically annotated on the HP corpus through exact string matching, resulting in 360 gene entity mentions. Then, we assumed that every gene-phenotype pair that co-occurred in the same sentence was a positive instance if this relation existed in the file. While the phenotype entities were manually mapped to the Human Phenotype Ontology, we had to employ an automatic method to obtain the most representative Gene Ontology [<xref ref-type="bibr" rid="CR41">41</xref>, <xref ref-type="bibr" rid="CR42">42</xref>] concept of each gene, giving preference to concepts inferred from experiments. We applied the same pre-processing steps as for the DDI corpus, except for entity matching and negative instance filtering. This corpus is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/lasigeBioTM/BOLSTM/tree/master/HP%20corpus">https://github.com/lasigeBioTM/BOLSTM/tree/master/HP%20corpus</ext-link>.</p>
    </sec>
    <sec id="Sec6">
      <title>BO-LSTM model</title>
      <p>The main contribution of this work is the integration of ontology information with a neural network classification model. A domain-specific ontology is a formal definition of the concepts related to a specific subject. We can define an ontology as a tuple &lt;<italic>C</italic>,<italic>R</italic>&gt;, where C is the set of concepts and R the set of relations between the concepts, where each relation is a pair of concepts (<italic>c</italic><sub>1</sub>,<italic>c</italic><sub>2</sub>) with <italic>c</italic><sub>1</sub>,<italic>c</italic><sub>2</sub>∈<italic>E</italic>. In our case, we consider only subsumption relations (is-a), which are transitive, i.e. if (<italic>c</italic><sub>1</sub>,<italic>c</italic><sub>2</sub>)∈<italic>R</italic> and (<italic>c</italic><sub>2</sub>,<italic>c</italic><sub>3</sub>)∈<italic>R</italic>, then we can assume that (<italic>c</italic><sub>1</sub>,<italic>c</italic><sub>3</sub>) is a valid relation. Then, the ancestors of concept <italic>c</italic> are given by 
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ Anc(c) = {a : (c, a) \in T}  $$ \end{document}</tex-math><mml:math id="M4"><mml:mtext mathvariant="italic">Anc</mml:mtext><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>:</mml:mo><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo><mml:mo>∈</mml:mo><mml:mi>T</mml:mi></mml:math><graphic xlink:href="12859_2018_2584_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>where <italic>T</italic> is the transitive closure of <italic>R</italic> on the set <italic>E</italic>, i.e., the smallest relation set on <italic>E</italic> that contains <italic>R</italic> and is transitive. Using this definition, we can define the common ancestors of concepts <italic>c</italic><sub>1</sub> and <italic>c</italic><sub>2</sub> as 
<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ CA\left(c_{1}, c_{2}\right) = Anc\left(c_{1}\right) \cap Anc \left(c_{2}\right)  $$ \end{document}</tex-math><mml:math id="M6"><mml:mtext mathvariant="italic">CA</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mtext mathvariant="italic">Anc</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>∩</mml:mo><mml:mtext mathvariant="italic">Anc</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12859_2018_2584_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>and the concatenation of the ancestors of concepts <italic>c</italic><sub>1</sub> and <italic>c</italic><sub>2</sub> as 
<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ Conc\left(c_{1}, c_{2}\right) = Anc \left(c_{1}\right) \oplus Anc\left(c_{2}\right)  $$ \end{document}</tex-math><mml:math id="M8"><mml:mtext mathvariant="italic">Conc</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mtext mathvariant="italic">Anc</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>⊕</mml:mo><mml:mtext mathvariant="italic">Anc</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12859_2018_2584_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>We consider two types of representations of a candidate pair based on the ancestry of its elements: the first consisting of the concatenation of the sequence of ancestors of each entity; and second, consisting of the common ancestors between both entities. Each set of ancestors is sorted by its position in the ontology so that more general concepts are in the first positions and the final position is the concept itself. Common ancestors are also used in some semantic similarity measures [<xref ref-type="bibr" rid="CR43">43</xref>–<xref ref-type="bibr" rid="CR45">45</xref>], since they normally represent the common information between two concepts. Due to the fact that in some cases there can be almost no overlap between the ancestors of two concepts, the concatenation provides an alternative representation.</p>
      <p>We first represent each ontology concept as a one-hot vector <italic>v</italic><sub><italic>c</italic></sub>, a vector of zeros except for the position corresponding to the ID of the concept. The ontology embedding layer transforms these sparse vectors into dense vectors, known as embeddings, through an embedding matrix <inline-formula id="IEq2"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$M \in \mathbb {R}^{D \times C}$\end{document}</tex-math><mml:math id="M10"><mml:mi>M</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>×</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2018_2584_Article_IEq2.gif"/></alternatives></inline-formula>, where <italic>D</italic> is the dimensionality of the embedding layer and <italic>C</italic> is the number of concepts of the ontology. Then, the output of the embedding layer is given by 
<disp-formula id="Equa"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$f(c) = M \cdot v_{c} $$ \end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math><graphic xlink:href="12859_2018_2584_Article_Equa.gif" position="anchor"/></alternatives></disp-formula> In our experiments, we set the dimensionality of the ontology embedding layer as 50, and initialized its values randomly. Then, these values were tuned during training through back-propagation.</p>
      <p>The sequence of vectors representing the ancestors of the terms is then fed into the LSTM layer. Figure <xref rid="Fig3" ref-type="fig">3</xref> exemplifies how we adapted this architecture to our model, using a sequence of ontology concepts as input. After the LSTM layer, we use a max pool layer which is then fed into a dense layer with a sigmoid activation function. We experimented with bypassing this dense layer, obtaining inferior results. Finally, a softmax layer outputs the probability of each class.
<fig id="Fig3"><label>Fig. 3</label><caption><p>BO-LSTM unit, using a sequence of ChEBI ontology concepts as an example. Circle refers to sigmoid function and rectangle to tanh, while “x” and “+” refer to element-wise multiplication and addition. <italic>h</italic>: hidden unit; <inline-formula id="IEq3"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\tilde {m}$\end{document}</tex-math><mml:math id="M14"><mml:mover accent="true"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2018_2584_Article_IEq3.gif"/></alternatives></inline-formula>: candidate memory cell; <italic>m</italic>: memory cell; <italic>i</italic> input gate; <italic>f</italic> forget gate; <italic>o</italic>: output gate</p></caption><graphic xlink:href="12859_2018_2584_Fig3_HTML" id="MO3"/></fig>
</p>
      <p>Each configuration of our model was trained through mini-batch gradient descent with the Adam algorithm [<xref ref-type="bibr" rid="CR46">46</xref>] and with cross-entropy as the loss function, with a learning rate of 0.001 We used the dropout strategy [<xref ref-type="bibr" rid="CR47">47</xref>] to reduce overfitting on the trained embeddings and weights. We used a dropout rate of 0.5 on every layer except the penultimate and output layers. We tuned the hyperparameters common to all configurations using only the word embeddings channel on the validation set. Each model was trained until the validation loss stopped decreasing. The experiments were performed on an Intel Xeon CPU (X3470 @ 2.93 GHz) with 16 GB of RAM and on a GeForce GTX 1080 Ti GPU with 11GB of RAM.</p>
      <p>The ChEBI and WordNet embedding layers were trained along with the other layers of the network. The DDI corpus contains 1757 of the 109k concepts of the ChEBI ontology. Since this is a relatively small vocabulary, we believe that this approach is robust enough to tune the weights. For the size of the WordNet embedding layer, we used 50 as suggested by Xu et al. [<xref ref-type="bibr" rid="CR17">17</xref>], while for the ChEBI embedding layer, we tested 50, 100 and 150, obtaining the best performance with 50.</p>
    </sec>
    <sec id="Sec7">
      <title>Baseline models</title>
      <p>As a baseline, we implemented a model based on the SDP-LSTM model of Xu et al. [<xref ref-type="bibr" rid="CR17">17</xref>]. The SDP-LSTM model makes use of four types of information: word embeddings, part-of-speech tags, grammatical relations and WordNet hypernyms, which we refer to as channels. Each channel uses a specific type of input information to train an LSTM-based RNN layer, which is then connected to a max pooling layer, the output of the channel. The output of each channel is concatenated, and connected to a densely-connected hidden layer, with a sigmoid activation function, while a softmax layer outputs the probabilities of each class.</p>
      <p>Xu et al. show that it is possible to obtain high performance on a relation extraction task using only the word representations channel. For this reason, we use a version of our model with only this channel as the baseline. We employ the previously mentioned pre-trained word embeddings as input to the LSTM layer.</p>
      <p>Additionally, we make use of WordNet as an external source of information. The authors of the SDP-LSTM model showed that WordNet contributed to an improvement of the F1-score on a relation extraction task. We use the tool developed by Ciaramita and Altun [<xref ref-type="bibr" rid="CR35">35</xref>] to obtain the WordNet classes of each word according to 41 semantic categories, such as “noun.group” and “verb.change”. The embeddings of this channel were set to be 50-dimensional and tuned during the training of the model.</p>
      <p>We adopted a second baseline model to make a stronger comparison with other DDI extraction models, based on the model presented by Zhang et al. [<xref ref-type="bibr" rid="CR48">48</xref>]. Their model uses the sentence and SDP of each instance to train a hierarchical LSTM network. This model is constituted by two levels of LSTMs which learn feature representations of the sentence and SDP based on word, part-of-speech and distance to entity. An embedding attention mechanism is used to weight the importance of each word to the two entities that constitute each pair. We kept the architecture and hyperparameters of their model, and added another type of input, based on the common ancestors and concatenation of each entity’s ancestors. We applied the same attention mechanism, so that the most relevant ancestors have a larger weight on the LSTM. We ran the original Zhang et al. model to replicate the results, and then ran again with ontology information.</p>
    </sec>
  </sec>
  <sec id="Sec8" sec-type="results">
    <title>Results</title>
    <p>We evaluated the performance of our BO-LSTM model on the SemEval 2013: Task 9 DDI extraction corpus [<xref ref-type="bibr" rid="CR49">49</xref>]. This gold standard corpus consists of 792 texts from DrugBank [<xref ref-type="bibr" rid="CR50">50</xref>], describing chemical compounds, and 233 abstracts from the Medline database [<xref ref-type="bibr" rid="CR51">51</xref>]. DrugBank is a cheminformatics database containing detailed drug and drug target information, while Medline is a database of bibliographic information of scientific articles in Life and Health Sciences. Each document was annotated with pharmacological substances and sentence-level DDIs. We refer to each combination of entities mentioned in the same sentence as a candidate pair, which could either be positive if the text describes a DDI, or negative otherwise. In other words, a negative candidate is a candidate pair that is not described as interacting in the text. Each positive DDI was assigned one of four possible classes: mechanism, effect, advice, and int, when none of the others were applicable.</p>
    <p>In the context of the competition, the corpus was separated into training and testing sets, containing both DrugBank and Medline documents. We maintained the test set partition and evaluated on it, as it is the standard procedure on this gold standard. After shuffling we used 80% of the training set to train the model and 20% as a validation set. This way, the validation set contained both DrugBank and Medline documents, and overfitting to a specific document type is avoided. It has been shown that the DDIs of the Medline documents are more difficult to detect and classify, with the best systems having almost a 30 point F1-score difference to the DrugBank documents [<xref ref-type="bibr" rid="CR52">52</xref>].</p>
    <p>We implemented the BO-LSTM model in Keras, a Python-based deep learning library, using the TensorFlow backend. The overall architecture of the BO-LSTM model is presented in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. More details about each layer can be found in the “<xref rid="Sec4" ref-type="sec">Methods</xref>” section. We focused on the effect of using different sources of information to train the model. As such, we tuned the hyperparameters to obtain reasonable results, using as reference the values provided by other authors that have applied LSTMs to this gold standard [<xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR19">19</xref>]. We first trained the model using only the word embeddings of the SDP of each candidate pair (Fig. <xref rid="Fig2" ref-type="fig">2</xref>a). Then we tested the effect of adding the WordNet classes as a separate embedding and LSTM layer (Fig. <xref rid="Fig2" ref-type="fig">2</xref>b) Finally, we tested two variations of the ChEBI channel: first using the concatenation of the sequence of ancestors of each entity (Fig. <xref rid="Fig2" ref-type="fig">2</xref>c), and second using the sequence of common ancestors of both entities (Fig. <xref rid="Fig2" ref-type="fig">2</xref>d).</p>
    <p>Table <xref rid="Tab1" ref-type="table">1</xref> shows the DDI detection results obtained with each configuration using the evaluation tool provided by the SemEval 2013: Task 9 organizers on the gold standard, while Table <xref rid="Tab2" ref-type="table">2</xref> shows the DDI classification results, using the same evaluation tool and gold standard. The difference between these two tasks is that while detection ignores the type of interactions, the classification task requires identifying the positive pairs and also their correct interaction type. We compare the performance on the whole gold standard, and on each document type (DrugBank and Medline). The first row of each table shows the results obtained using an LSTM network trained solely on the word embeddings of the SDP of each candidate pair. Then, we studied the impact of adding each information channel on the performance of the model, and the effect of using all information channels, as shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Evaluation scores obtained for the DDI detection task on the DDI corpus and on each type of document, comparing different configurations of the model</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left" colspan="3">DDI test</th><th align="left" colspan="3">DrugBank</th><th align="left" colspan="3">Medline</th></tr><tr><th align="left">Configuration</th><th align="left">P</th><th align="left">R</th><th align="left">F</th><th align="left">P</th><th align="left">R</th><th align="left">F</th><th align="left">P</th><th align="left">R</th><th align="left">F</th></tr></thead><tbody><tr><td align="left">Word embeddings</td><td align="left">0.7551</td><td align="left">0.6865</td><td align="left">0.7192</td><td align="left">0.7620</td><td align="left">0.7158</td><td align="left">0.7382</td><td align="left">0.6389</td><td align="left">0.377</td><td align="left">0.4742</td></tr><tr><td align="left">+ WordNet</td><td align="left">0.716</td><td align="left">0.6936</td><td align="left">0.7046</td><td align="left">0.7267</td><td align="left">0.7143</td><td align="left">0.7204</td><td align="left">0.5800</td><td align="left">0.4754</td><td align="left">0.5225</td></tr><tr><td align="left">+ Common Ancestors</td><td align="left"><bold>0.7661</bold></td><td align="left">0.6738</td><td align="left">0.7170</td><td align="left"><bold>0.7723</bold></td><td align="left">0.7003</td><td align="left">0.7345</td><td align="left"><bold>0.6667</bold></td><td align="left">0.3607</td><td align="left">0.4681</td></tr><tr><td align="left">+ Concat. Ancestors</td><td align="left">0.7078</td><td align="left">0.7489</td><td align="left">0.7278</td><td align="left">0.7166</td><td align="left">0.7578</td><td align="left">0.7366</td><td align="left">0.6032</td><td align="left"><bold>0.623</bold></td><td align="left"><bold>0.6129</bold></td></tr><tr><td align="left">+ WordNet + Ancestors</td><td align="left">0.6572</td><td align="left"><bold>0.8184</bold></td><td align="left"><bold>0.7290</bold></td><td align="left">0.6601</td><td align="left"><bold>0.8385</bold></td><td align="left"><bold>0.7387</bold></td><td align="left">0.5574</td><td align="left">0.5574</td><td align="left">0.5574</td></tr></tbody></table><table-wrap-foot><p>Evaluation metrics used: Precision (P), Recall (R) and F1-score (F). Each row represents the addition of an information source to the initial configuration</p><p>Boldface indicates the configuration with highest score for each measure</p></table-wrap-foot></table-wrap><table-wrap id="Tab2"><label>Table 2</label><caption><p>Evaluation scores obtained for the DDI classification task on the DDI corpus and on each type of document, comparing different configurations of the model</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left" colspan="3">DDI test</th><th align="left" colspan="3">DrugBank</th><th align="left" colspan="3">Medline</th></tr><tr><th align="left">Configuration</th><th align="left">P</th><th align="left">R</th><th align="left">F</th><th align="left">P</th><th align="left">R</th><th align="left">F</th><th align="left">P</th><th align="left">R</th><th align="left">F</th></tr></thead><tbody><tr><td align="left">Word embeddings</td><td align="left">0.5819</td><td align="left">0.5291</td><td align="left">0.5542</td><td align="left">0.5868</td><td align="left">0.5512</td><td align="left">0.5685</td><td align="left">0.5000</td><td align="left">0.2951</td><td align="left">0.3711</td></tr><tr><td align="left">+ WordNet</td><td align="left">0.5754</td><td align="left">0.5574</td><td align="left">0.5663</td><td align="left">0.5845</td><td align="left">0.5745</td><td align="left"><bold>0.5795</bold></td><td align="left">0.4600</td><td align="left">0.3770</td><td align="left">0.4144</td></tr><tr><td align="left">+ Common Anc.</td><td align="left"><bold>0.5968</bold></td><td align="left">0.5248</td><td align="left">0.5585</td><td align="left"><bold>0.6045</bold></td><td align="left">0.5481</td><td align="left">0.5749</td><td align="left"><bold>0.5152</bold></td><td align="left">0.2787</td><td align="left">0.3617</td></tr><tr><td align="left">+ Concat. Anc.</td><td align="left">0.5282</td><td align="left">0.5589</td><td align="left">0.5431</td><td align="left">0.5286</td><td align="left">0.5590</td><td align="left">0.5434</td><td align="left">0.4921</td><td align="left"><bold>0.5082</bold></td><td align="left"><bold>0.5000</bold></td></tr><tr><td align="left">+ WordNet + Anc.</td><td align="left">0.5182</td><td align="left"><bold>0.6454</bold></td><td align="left"><bold>0.5749</bold></td><td align="left">0.5171</td><td align="left"><bold>0.6568</bold></td><td align="left">0.5787</td><td align="left">0.4590</td><td align="left">0.4590</td><td align="left">0.4590</td></tr></tbody></table><table-wrap-foot><p>Evaluation metrics used: Precision (P), Recall (R) and F1-score (F). Each row represents the addition of an information source to the initial configuration</p><p>Boldface indicates the configuration with highest score for each measure</p></table-wrap-foot></table-wrap>
</p>
    <p>For the detection task, using the concatenation of ancestors results in an improvement of the F1-score in the Medline dataset, contributing to an overall improvement of the F1-score in the full test set. The most notable improvement was in the recall of the Medline dataset, where the concatenation of ancestors increased this score by 0.246. The usage of ontology ancestors did not improve the F1-score of detection of DDIs in the DrugBank dataset. In every test set, it is possible to observe that the concatenation of ancestors results in a higher recall while considering only the common ancestors is more beneficial to precision. Combining both approaches with the WordNet channel results in a higher F1-score.</p>
    <p>Regarding the classification task (Table <xref rid="Tab2" ref-type="table">2</xref>), the F1-score was improved on each dataset by the usage of the ontology channel. Considering only the common ancestors led to an improvement of the F1-score in the DrugBank dataset and on the full corpus, while the concatenation improved the Medline F1-score, similarly to the detection results.</p>
    <p>To better understand the contribution of each channel, we studied the relations detected by each configuration by one or more channels, and which of those were also present in the gold standard. Figures <xref rid="Fig4" ref-type="fig">4</xref> and <xref rid="Fig5" ref-type="fig">5</xref> show the intersection of the results of each channel in the full, DrugBank, and Medline test sets. We compare only the results of the detection task, as it is simpler to analyze and show the differences in the results of different configurations. In Fig. <xref rid="Fig4" ref-type="fig">4</xref>, we can visualize false negatives as the number of relations unique to the gold standard and the false positives of each configuration as the number of relations that does not intersect with the gold standard. The difference between the values of this figure and the sum of their respective values in Fig. <xref rid="Fig5" ref-type="fig">5</xref> is due to the system being executed once for each dataset. Overall 369 relations in the full test set were not detected by any configuration of our system, out of a total of 979 relations in the gold standard. We can observe that 60 relations were detected only when adding the ontology channels.
<fig id="Fig4"><label>Fig. 4</label><caption><p>Venn diagram demonstrating the contribution of each configuration of the model to the results of the full test set. The intersection of each channel with the gold standard represents the number of true positives of that channel, while the remaining correspond to false negatives and false positives</p></caption><graphic xlink:href="12859_2018_2584_Fig4_HTML" id="MO4"/></fig><fig id="Fig5"><label>Fig. 5</label><caption><p>Venn diagram demonstrating the contribution of each configuration of the model to the DrugBank (<bold>a</bold>) and Medline (<bold>b</bold>) test set results. The intersection of each channel with the gold standard represents the number of true positives of that channel, while the remaining correspond to false negatives and false positives</p></caption><graphic xlink:href="12859_2018_2584_Fig5_HTML" id="MO5"/></fig>
</p>
    <p>In the Medline test set, the ontology channel identified 7 relations that were not identified by any other configuration (Fig. <xref rid="Fig5" ref-type="fig">5</xref>b). One of these relations was the effect of quinpirole treatment on amphetamine sensitization. Quinpirole has 27 ancestors in the ChEBI ontology, while amphetamine has 17, and they share 10 of these ancestors, with the most informative being “organonitrogen compound”. While this information is not described in the original text, but only encoded in the ontology, it is relevant to understand if the two entities can participate in a relation. However, this comes at the cost of precision, since 10 incorrect DDIs were classified by this configuration.</p>
    <p>To empirically compare our results with the state-of-the-art of the DDI extraction, we compiled the most relevant works on this task in Table <xref rid="Tab3" ref-type="table">3</xref>. The first line refers to the system that obtained the best results on the original SemEval task [<xref ref-type="bibr" rid="CR38">38</xref>, <xref ref-type="bibr" rid="CR53">53</xref>]. Since then, other authors have presented approaches for this task, most recently using deep learning algorithms. In Table <xref rid="Tab3" ref-type="table">3</xref> we compare the machine learning architecture used by each system, and the results reported by the authors. Since some authors focused only on the DDI classification task, we could not obtain the DDI detection results for those systems, hence the missing values. We were only able to replicate the results of Zhang et al. [<xref ref-type="bibr" rid="CR48">48</xref>]. Since this system followed an architecture similar to ours, we adapted the model with our ontology-based channel, as described in the “<xref rid="Sec4" ref-type="sec">Methods</xref>” section. This modification to the model resulted in an improvement of 0.022 to the F1-score. Our version of this model is also available on our page along with the BO-LSTM model.
<table-wrap id="Tab3"><label>Table 3</label><caption><p>Comparison of DDI extraction systems</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">System</th><th align="left">Architecture</th><th align="left">Best F1-score</th></tr></thead><tbody><tr><td align="left">FBK-irst [<xref ref-type="bibr" rid="CR38">38</xref>]</td><td align="left">SVM</td><td align="left">0.651</td></tr><tr><td align="left">SCNN [<xref ref-type="bibr" rid="CR18">18</xref>]</td><td align="left">CNN</td><td align="left">0.686</td></tr><tr><td align="left">Joint AB-LSTM [<xref ref-type="bibr" rid="CR19">19</xref>]</td><td align="left">LSTM</td><td align="left">0.6939</td></tr><tr><td align="left">Att-BLSTM [<xref ref-type="bibr" rid="CR22">22</xref>]</td><td align="left">LSTM</td><td align="left">0.773</td></tr><tr><td align="left">DLSTM [<xref ref-type="bibr" rid="CR20">20</xref>]</td><td align="left">LSTM</td><td align="left">0.6839</td></tr><tr><td align="left">BR-LSTM [<xref ref-type="bibr" rid="CR21">21</xref>]</td><td align="left">LSTM</td><td align="left">0.7115</td></tr><tr><td align="left">Zhang et al. 2018 [<xref ref-type="bibr" rid="CR48">48</xref>]</td><td align="left">LSTM</td><td align="left">0.729</td></tr><tr><td align="left">Zhang et al. 2018 + BO-LSTM</td><td align="left">LSTM</td><td align="left">0.751</td></tr></tbody></table><table-wrap-foot><p>The architectures mentioned are Support Vector Machines (SVM), Convolutional Neural Networks (CNN) and LSTMs</p></table-wrap-foot></table-wrap>
</p>
    <p>We used the HP corpus to demonstrate the generalizability of our method. This case-study served only as a proof-of-concept, it was not our intent to measure the performance of the model, given the limited number of annotations and the dependence on the quality of using exact string matching to identify the genes. For example, we may have missed correct relations in the corpus, because they were not in the reference file or the gene name was not correctly identified.</p>
    <p>Therefore, we used 60% (137 documents) of the corpus to train the model and 40% (91 documents) to manually evaluate the relations predicted with that model. For example, in the following sentence:</p>
    <p><graphic position="anchor" xlink:href="12859_2018_2584_Figa_HTML" id="MO6"/> the model identified the relation between the phenotype “angiofibromas” and the gene “MEN1”. One recurrently identified relation by our model that was not present on the phenotype-gene associations file is between the phenotype ’neurofibromatosis’ and the gene ’NF2’:</p>
    <p>
      <graphic position="anchor" xlink:href="12859_2018_2584_Figb_HTML" id="MO7"/>
    </p>
    <p>Despite this relation not being described in the previous sentence, it is predicted given its presence in the phenotype-gene associations files. With a larger number of annotations in the training corpus, we expect this error to disappear.</p>
  </sec>
  <sec id="Sec9" sec-type="discussion">
    <title>Discussion</title>
    <p>Comparing the results across the two types of documents, we can observe that our model was most beneficial to the Medline test set. This set contains only 1301 sentences from 142 documents for training, while the DrugBank set contains 5675 sentences from 572 documents. Naturally, the patterns of the DrugBank documents will be easier to learn than the ones of the Medline documents because more examples are shown to the model. Furthermore, the Medline set has 0.18 relations per sentence, while the DrugBank set has 0.67 relations per sentence. This means that DDIs are described much more sparsely than in the DrugBank set. This demonstrates that our model is able to obtain useful knowledge that is not described in the text.</p>
    <p>One disadvantage of incorporating domain information in a machine learning approach is that it reduces its applicability to other domains. However, biomedical ontologies have become ubiquitous in biomedical research. One of the most successful cases of a biomedical ontology is the Gene Ontology, maintained by the Gene Ontology Consortium [<xref ref-type="bibr" rid="CR54">54</xref>]. The Gene Ontology defines over 40,000 concepts used to describe the properties of genes. This project is constantly updated, with new concepts and relations being added every day. However, there are ontologies for more specific subjects, such as microRNAs [<xref ref-type="bibr" rid="CR55">55</xref>], radiology terms [<xref ref-type="bibr" rid="CR56">56</xref>] and rare diseases [<xref ref-type="bibr" rid="CR57">57</xref>]. BioPortal is a repository of biomedical ontology, currently hosting 685 ontologies. Furthermore, while manually labeled corpora are created specifically to train and evaluate text mining applications, ontologies have diverse applications, i.e., they are not developed for this specific purpose.</p>
    <p>We evaluate the proposed model on the DDI corpus because it is associated with a SemEval task, and for this reason, it has been the subject of many studies since its release. However, while applying our model to a single domain, we designed its architecture so it can fit any other domain-specific ontology. To demonstrate this, we developed a corpus of gene-phenotype relations annotated with Human Phenotype and Gene ontology concepts, and applied our model to it. Therefore, the methodology proposed can be easily followed to apply to any other biomedical ontology that describes the concepts of a particular domain. For example, the Disease Ontology [<xref ref-type="bibr" rid="CR58">58</xref>], that describes relations between human diseases, could be used with the BO-LSTM model on a disease relation extraction task, as long as there is an annotated training corpus.</p>
    <p>While we studied the potential of domain-specific ontologies based only on the ancestors of each entity, there are other ways to integrate semantic information from ontologies into neural networks. For example, one could consider only the ancestors with the highest information content, since those would be the most helpful to characterize an entity. The information content can be estimated either by the probability of a given term in the ontology or in an external dataset. Alternatively, a semantic similarity measure that accounts for non-transitive relations could be used to find similar concepts to the entities of the relation [<xref ref-type="bibr" rid="CR59">59</xref>], or one that considers only the most relevant ancestors [<xref ref-type="bibr" rid="CR60">60</xref>]. The quality of the ontology embeddings could also be improved by pre-training on a larger dataset, which would include a wider variety of concepts.</p>
  </sec>
  <sec id="Sec10" sec-type="conclusion">
    <title>Conclusions</title>
    <p>This work demonstrates how domain-specific ontologies can improve deep learning models for classification of biomedical relations. We developed a model, BO-LSTM which combines biomedical ontologies with LSTM units to detect and classify relations in text. In this manuscript, we demonstrate that ontologies can improve the performance of deep learning techniques for biomedical relation extraction, in particular for situations with a limited number of annotations available, which was the case of the Medline dataset. Furthermore, we explored how it can be adapted to other relation extraction domains, for example, gene-phenotype relations. Considering that biomedical ontologies are openly available and regularly updated as the knowledge on the domain progresses, they should be considered important information sources for relation extraction.</p>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>ADR</term>
        <def>
          <p>Adverse Drug Reactions</p>
        </def>
      </def-item>
      <def-item>
        <term>ChEBI</term>
        <def>
          <p>Chemical Entities of Biological Interest</p>
        </def>
      </def-item>
      <def-item>
        <term>DDI</term>
        <def>
          <p>Drug-drug interactions</p>
        </def>
      </def-item>
      <def-item>
        <term>LSTM</term>
        <def>
          <p>Long short-term memory</p>
        </def>
      </def-item>
      <def-item>
        <term>NLP</term>
        <def>
          <p>Natural Language Processing</p>
        </def>
      </def-item>
      <def-item>
        <term>RNN</term>
        <def>
          <p>Recurrent Neural Networks</p>
        </def>
      </def-item>
      <def-item>
        <term>SDP</term>
        <def>
          <p>Shortest Dependency Paths</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <ack>
    <title>Acknowledgements</title>
    <p>We acknowledge the help of Nuno Dionisio in setting up the machine to run the experiments.</p>
    <sec id="d29e1838">
      <title>Funding</title>
      <p>This work was supported by FCT through funding of the DeST: Deep Semantic Tagger project, ref. PTDC/CCI-BIO/28685/2017, LaSIGE Research Unit, ref. UID/CEC/00408/2013 and BioISI, ref. ID/MULTI/04046/2013. AL is recipient of a fellowship from BioSys PhD programme (ref PD/BD/106083/2015) from FCT (Portugal).</p>
    </sec>
    <sec id="d29e1843">
      <title>Availability of data and materials</title>
      <p>The data and code used for this study are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/lasigeBioTM/BOLSTM">https://github.com/lasigeBioTM/BOLSTM</ext-link>.</p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="COI-statement">
    <sec>
      <title>Ethics approval and consent to participate</title>
      <p>Not applicable.</p>
    </sec>
    <sec>
      <title>Consent for publication</title>
      <p>Not applicable.</p>
    </sec>
    <sec>
      <title>Competing interests</title>
      <p>The authors declare that they have no competing interests.</p>
    </sec>
    <sec>
      <title>Publisher’s Note</title>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </sec>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zelenko</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Zelenko</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Aone</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Aone</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Richardella</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Richardella</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Kernel Methods for Relation Extraction</article-title>
        <source>J Mach Learn Res</source>
        <year>2003</year>
        <volume>3</volume>
        <fpage>1083</fpage>
        <lpage>106</lpage>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2</label>
      <mixed-citation publication-type="other">Reichartz F, Korte H, Paass G. Semantic relation extraction with kernels over typed dependency trees. Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD ’10; 2010, p. 773. 10.1145/1835804.1835902.</mixed-citation>
    </ref>
    <ref id="CR3">
      <label>3</label>
      <mixed-citation publication-type="other">Kambhatla N. Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations. Proceedings of the ACL 2004 on Interactive poster and demonstration sessions; 2004, p. 22. 10.3115/1219044.1219066.</mixed-citation>
    </ref>
    <ref id="CR4">
      <label>4</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Collobert</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Weston</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Bottou</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Karlen</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kavukcuoglu</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Kuksa</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Natural language processing (almost) from scratch</article-title>
        <source>J Mach Learn Res</source>
        <year>2011</year>
        <volume>12</volume>
        <issue>Aug</issue>
        <fpage>2493</fpage>
        <lpage>537</lpage>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Lamurias</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Couto</surname>
            <given-names>FM</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Ranganathan</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gribskov</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Nakai</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Schönbach</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Text mining for bioinformatics using biomedical literature</article-title>
        <source>Encyclopedia of Bioinformatics and Computational Biology</source>
        <year>2019</year>
        <publisher-loc>Oxford</publisher-loc>
        <publisher-name>Academic Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Erhan</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Courville</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Manzagol</surname>
            <given-names>P-A</given-names>
          </name>
          <name>
            <surname>Vincent</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Why does unsupervised pre-training help deep learning?</article-title>
        <source>J Mach Learn Res</source>
        <year>2010</year>
        <volume>11</volume>
        <fpage>625</fpage>
        <lpage>60</lpage>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Mikolov</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Corrado</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Dean</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Distributed representations of words and phrases and their compositionality</article-title>
        <source>Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2. NIPS’13</source>
        <year>2013</year>
        <publisher-loc>USA</publisher-loc>
        <publisher-name>Curran Associates Inc.</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8</label>
      <mixed-citation publication-type="other">Pennington J, Socher R, Manning CD. Glove: Global vectors for word representation. In: Empirical Methods in Natural Language Processing (EMNLP): 2014. p. 1532–43. <ext-link ext-link-type="uri" xlink:href="http://www.aclweb.org/anthology/D14-1162">http://www.aclweb.org/anthology/D14-1162</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>CC</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Community challenges in biomedical text mining over 10 years: Success, failure and the future</article-title>
        <source>Brief Bioinform</source>
        <year>2016</year>
        <volume>17</volume>
        <issue>1</issue>
        <fpage>132</fpage>
        <lpage>44</lpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbv024</pub-id>
        <pub-id pub-id-type="pmid">25935162</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Al Hamid</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ghaleb</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Aljadhey</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Aslanpour</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>A systematic review of hospitalization resulting from medicine-related problems in adult patients</article-title>
        <source>Br J Clin Pharmacol</source>
        <year>2014</year>
        <volume>78</volume>
        <issue>2</issue>
        <fpage>202</fpage>
        <lpage>17</lpage>
        <pub-id pub-id-type="doi">10.1111/bcp.12293</pub-id>
        <pub-id pub-id-type="pmid">24283967</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bouvy</surname>
            <given-names>JC</given-names>
          </name>
          <name>
            <surname>De Bruin</surname>
            <given-names>ML</given-names>
          </name>
          <name>
            <surname>Koopmanschap</surname>
            <given-names>MA</given-names>
          </name>
        </person-group>
        <article-title>Epidemiology of adverse drug reactions in europe: a review of recent observational studies</article-title>
        <source>Drug Saf</source>
        <year>2015</year>
        <volume>38</volume>
        <issue>5</issue>
        <fpage>437</fpage>
        <lpage>53</lpage>
        <pub-id pub-id-type="doi">10.1007/s40264-015-0281-0</pub-id>
        <pub-id pub-id-type="pmid">25822400</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12</label>
      <mixed-citation publication-type="other">Dou D, Wang H, Liu H. Semantic data mining: A survey of ontology-based approaches. In: Proceedings of the 2015 IEEE 9th International Conference on Semantic Computing (IEEE ICSC 2015): 2015. p. 244–51. 10.1109/ICOSC.2015.7050814.</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hochreiter</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Schmidhuber</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Long short-term memory</article-title>
        <source>Neural Comput</source>
        <year>1997</year>
        <volume>9</volume>
        <issue>8</issue>
        <fpage>1735</fpage>
        <lpage>80</lpage>
        <pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id>
        <pub-id pub-id-type="pmid">9377276</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14</label>
      <mixed-citation publication-type="other">Miwa M, Bansal M. End-to-end Relation Extraction using LSTMs on Sequences and Tree Structures. In: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Vol. 1 (Long Papers): 2016. p. 10. <ext-link ext-link-type="uri" xlink:href="https://doi.org/doi:10.18653/v1/P16-1105.1601.0770">https://doi.org/doi:10.18653/v1/P16-1105.1601.0770</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15</label>
      <mixed-citation publication-type="other">Hendrickx I, Kim SN, Kozareva Z, Nakov P, Ó Séaghdha D, Padó S, Pennacchiotti M, Romano L, Szpakowicz S. Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In: Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions: 2009. p. 94–9. Association for Computational Linguistics.</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16</label>
      <mixed-citation publication-type="other">Zhang S, Zheng D, Hu X, Yang M. Bidirectional long short-term memory networks for relation classification. In: Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation: 2015. p. 73–8.</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17</label>
      <mixed-citation publication-type="other">Xu Y, Mou L, Li G, Chen Y. Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Paths. In: In Proceedings of Conference on Empirical Methods in Natural Language Processing: 2015. p. 1785–94. <ext-link ext-link-type="uri" xlink:href="https://doi.org/doi:10.18653/v1/D15-1206.1508.03720">https://doi.org/doi:10.18653/v1/D15-1206.1508.03720</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Drug drug interaction extraction from biomedical literature using syntax convolutional neural network</article-title>
        <source>Bioinformatics</source>
        <year>2016</year>
        <volume>32</volume>
        <issue>November</issue>
        <fpage>486</fpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btw486</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sahu</surname>
            <given-names>SK</given-names>
          </name>
          <name>
            <surname>Anand</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Drug-Drug Interaction Extraction from Biomedical Text Using Long Short Term Memory Network</article-title>
        <source>CEUR Work Proc</source>
        <year>2017</year>
        <volume>1828</volume>
        <fpage>53</fpage>
        <lpage>9</lpage>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20</label>
      <mixed-citation publication-type="other">Wang W, Yang X, Yang C, Guo X, Zhang X, Wu C. Dependency-based long short term memory network for drug-drug interaction extraction. BMC Bioinforma. 2017; 18(Suppl 16). 10.1186/s12859-017-1962-8.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Leveraging biomedical resources in bi-lstm for drug-drug interaction extraction</article-title>
        <source>IEEE Access</source>
        <year>2018</year>
        <volume>6</volume>
        <fpage>33432</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1109/ACCESS.2018.2845840</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22</label>
      <mixed-citation publication-type="other">Zheng W, Lin H, Luo L, Zhao Z, Li Z, Zhang Y, Yang Z, Wang J. An attention-based effective neural model for drug-drug interactions extraction; 2017, pp. 1–11. 10.1186/s12859-017-1855-x.</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Couto</surname>
            <given-names>FM</given-names>
          </name>
          <name>
            <surname>Lamurias</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Ranganathan</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gribskov</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Nakai</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Schönbach</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Semantic similarity definition</article-title>
        <source>Encyclopedia of Bioinformatics and Computational Biology</source>
        <year>2019</year>
        <publisher-loc>Oxford</publisher-loc>
        <publisher-name>Academic Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Smith</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Ashburner</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Rosse</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Bard</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Bug</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Ceusters</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Goldberg</surname>
            <given-names>LJ</given-names>
          </name>
          <name>
            <surname>Eilbeck</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Ireland</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Mungall</surname>
            <given-names>CJ</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The obo foundry: coordinated evolution of ontologies to support biomedical data integration</article-title>
        <source>Nat Biotechnol</source>
        <year>2007</year>
        <volume>25</volume>
        <issue>11</issue>
        <fpage>1251</fpage>
        <pub-id pub-id-type="doi">10.1038/nbt1346</pub-id>
        <pub-id pub-id-type="pmid">17989687</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hastings</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>De Matos</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Dekker</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ennis</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Harsha</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Kale</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Muthukrishnan</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Owen</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Turner</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Williams</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Steinbeck</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>The ChEBI reference database and ontology for biologically relevant chemistry: Enhancements for 2013</article-title>
        <source>Nucleic Acids Res</source>
        <year>2013</year>
        <volume>41</volume>
        <issue>D1</issue>
        <fpage>456</fpage>
        <lpage>63</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gks1146</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26</label>
      <mixed-citation publication-type="other">Ferreira JD, Inácio B, Salek RM, Couto FM. Assessing public metabolomics metadata, towards improving quality. J Integr Bioinforma. 2017; 14(4).</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Lin</surname>
            <given-names>C-Y</given-names>
          </name>
          <name>
            <surname>Xue</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Learning word sense embeddings from word sense definitions</article-title>
        <source>Natural Language Understanding and Intelligent Applications</source>
        <year>2016</year>
        <publisher-loc>Cham</publisher-loc>
        <publisher-name>Springer</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ma</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>B</surname>
            <given-names>H-tZ</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>An Ontology-Based Latent Semantic Indexing Approach Using Long Short-Term Memory Networks</article-title>
        <source>Web and Big Data</source>
        <year>2017</year>
        <volume>10366</volume>
        <issue>2</issue>
        <fpage>185</fpage>
        <lpage>99</lpage>
        <pub-id pub-id-type="doi">10.1007/978-3-319-63579-8_15</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29</label>
      <mixed-citation publication-type="other">Goyal P, Ferrara E. Graph embedding techniques, applications, and performance: A survey; 2017. arXiv preprint arXiv:1705.02801.</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Kong</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>PS</given-names>
          </name>
        </person-group>
        <article-title>Multi-label classification by mining label and instance correlations from heterogeneous information networks</article-title>
        <source>Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. KDD ’13</source>
        <year>2013</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>ACM</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Dasigi</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Ammar</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Dyer</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Hovy</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <source>Ontology-aware token embeddings for prepositional phrase attachment</source>
        <year>2017</year>
        <publisher-loc>Stroudsburg</publisher-loc>
        <publisher-name>Association for Computational Linguistics</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Müller</surname>
            <given-names>H-MM</given-names>
          </name>
          <name>
            <surname>Kenny</surname>
            <given-names>EE</given-names>
          </name>
          <name>
            <surname>Sternberg</surname>
            <given-names>PW</given-names>
          </name>
        </person-group>
        <article-title>Textpresso: an ontology-based information retrieval and extraction system for biological literature</article-title>
        <source>PLoS Biol</source>
        <year>2004</year>
        <volume>2</volume>
        <issue>11</issue>
        <fpage>309</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pbio.0020309</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lamurias</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ferreira</surname>
            <given-names>JD</given-names>
          </name>
          <name>
            <surname>Couto</surname>
            <given-names>FM</given-names>
          </name>
        </person-group>
        <article-title>Identifying interactions between chemical entities in biomedical text</article-title>
        <source>J Integr Bioinforma</source>
        <year>2014</year>
        <volume>11</volume>
        <issue>3</issue>
        <fpage>1</fpage>
        <lpage>16</lpage>
        <pub-id pub-id-type="doi">10.1515/jib-2014-247</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34</label>
      <mixed-citation publication-type="other">Tripodi I, Boguslav M, Haylu N, Hunter LE. Knowledge-base-enriched relation extraction. In: Proceedings of the Sixth BioCreative Challenge Evaluation Workshop. Bethesda, MD USA, vol. 1: 2017. p. 163–6.</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35</label>
      <mixed-citation publication-type="other">Ciaramita M, Altun Y. Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger. In: Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing: 2006. p. 594–602. Association for Computational Linguistics.</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36</label>
      <mixed-citation publication-type="other">Pyysalo S, Ginter F, Moen H, Salakoski T, Ananiadou S. Distributional Semantics Resources for Biomedical Text Processing; 2013.</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bhasuran</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Murugesan</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Abdulkadhar</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Natarajan</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Stacked ensemble combined with fuzzy matching for biomedical named entity recognition of diseases</article-title>
        <source>J Biomed Inform</source>
        <year>2016</year>
        <volume>64</volume>
        <fpage>1</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2016.09.009</pub-id>
        <pub-id pub-id-type="pmid">27634494</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chowdhury</surname>
            <given-names>MFM</given-names>
          </name>
          <name>
            <surname>Lavelli</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>FBK-irst: A multi-phase kernel based approach for drug-drug interaction detection and classification that exploits linguistic information</article-title>
        <source>Atlanta, Georgia, USA</source>
        <year>2013</year>
        <volume>351</volume>
        <fpage>53</fpage>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Yeganova</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Wilbur</surname>
            <given-names>WJ</given-names>
          </name>
        </person-group>
        <article-title>Extracting drug–drug interactions from literature using a rich feature-based linear kernel approach</article-title>
        <source>J Biomed Inform</source>
        <year>2015</year>
        <volume>55</volume>
        <fpage>23</fpage>
        <lpage>30</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2015.03.002</pub-id>
        <pub-id pub-id-type="pmid">25796456</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Köhler</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Vasilevsky</surname>
            <given-names>NA</given-names>
          </name>
          <name>
            <surname>Engelstad</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Foster</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>McMurry</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Aymé</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Baynam</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Bello</surname>
            <given-names>SM</given-names>
          </name>
          <name>
            <surname>Boerkoel</surname>
            <given-names>CF</given-names>
          </name>
          <name>
            <surname>Boycott</surname>
            <given-names>KM</given-names>
          </name>
          <name>
            <surname>Brudno</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Buske</surname>
            <given-names>OJ</given-names>
          </name>
          <name>
            <surname>Chinnery</surname>
            <given-names>PF</given-names>
          </name>
          <name>
            <surname>Cipriani</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Connell</surname>
            <given-names>LE</given-names>
          </name>
          <name>
            <surname>Dawkins</surname>
            <given-names>HJS</given-names>
          </name>
          <name>
            <surname>DeMare</surname>
            <given-names>LE</given-names>
          </name>
          <name>
            <surname>Devereau</surname>
            <given-names>AD</given-names>
          </name>
          <name>
            <surname>de Vries</surname>
            <given-names>BBA</given-names>
          </name>
          <name>
            <surname>Firth</surname>
            <given-names>HV</given-names>
          </name>
          <name>
            <surname>Freson</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Greene</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Hamosh</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Helbig</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Hum</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Jähn</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>James</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Krause</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>F Laulederkind</surname>
            <given-names>SJ</given-names>
          </name>
          <name>
            <surname>Lochmüller</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Lyon</surname>
            <given-names>GJ</given-names>
          </name>
          <name>
            <surname>Ogishima</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Olry</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ouwehand</surname>
            <given-names>WH</given-names>
          </name>
          <name>
            <surname>Pontikos</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Rath</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Schaefer</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Scott</surname>
            <given-names>RH</given-names>
          </name>
          <name>
            <surname>Segal</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Sergouniotis</surname>
            <given-names>PI</given-names>
          </name>
          <name>
            <surname>Sever</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>CL</given-names>
          </name>
          <name>
            <surname>Straub</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Thompson</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Turner</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Turro</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Veltman</surname>
            <given-names>MWM</given-names>
          </name>
          <name>
            <surname>Vulliamy</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>von Ziegenweidt</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zankl</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Züchner</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Zemojtel</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Jacobsen</surname>
            <given-names>JOB</given-names>
          </name>
          <name>
            <surname>Groza</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Smedley</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Mungall</surname>
            <given-names>CJ</given-names>
          </name>
          <name>
            <surname>Haendel</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Robinson</surname>
            <given-names>PN</given-names>
          </name>
        </person-group>
        <article-title>The human phenotype ontology in 2017</article-title>
        <source>Nucleic Acids Res</source>
        <year>2017</year>
        <volume>45</volume>
        <issue>D1</issue>
        <fpage>865</fpage>
        <lpage>76</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkw1039</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ashburner</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ball</surname>
            <given-names>CA</given-names>
          </name>
          <name>
            <surname>Blake</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Botstein</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Butler</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Cherry</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Davis</surname>
            <given-names>AP</given-names>
          </name>
          <name>
            <surname>Dolinski</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Dwight</surname>
            <given-names>SS</given-names>
          </name>
          <name>
            <surname>Eppig</surname>
            <given-names>JT</given-names>
          </name>
          <name>
            <surname>Harris</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Hill</surname>
            <given-names>DP</given-names>
          </name>
          <name>
            <surname>Issel-Tarver</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Kasarskis</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Lewis</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Matese</surname>
            <given-names>JC</given-names>
          </name>
          <name>
            <surname>Richardson</surname>
            <given-names>JE</given-names>
          </name>
          <name>
            <surname>Ringwald</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Rubin</surname>
            <given-names>GM</given-names>
          </name>
          <name>
            <surname>Sherlock</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Gene ontology: tool for the unification of biology. The Gene Ontology Consortium</article-title>
        <source>Nat Genet</source>
        <year>2000</year>
        <volume>25</volume>
        <issue>1</issue>
        <fpage>25</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1038/75556</pub-id>
        <pub-id pub-id-type="pmid">10802651</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>authors listed</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Expansion of the Gene Ontology knowledgebase and resources</article-title>
        <source>Nucleic Acids Res</source>
        <year>2017</year>
        <volume>45</volume>
        <issue>D1</issue>
        <fpage>331</fpage>
        <lpage>8</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkw1108</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Resnik</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Using information content to evaluate semantic similarity in a taxonomy</article-title>
        <source>International Joint Conference on Artificial Intelligence, vol. 14</source>
        <year>1995</year>
        <publisher-loc>San Francisco</publisher-loc>
        <publisher-name>Citeseer</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44</label>
      <mixed-citation publication-type="other">Jiang JJ, Conrath DW. Semantic similarity based on corpus statistics and lexical taxonomy. CoRR cmp-lg/9709008. 1997;:19–33.</mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45</label>
      <mixed-citation publication-type="other">Lin D. An information-theoretic definition of similarity. In: Proceedings of the Fifteenth International Conference on Machine Learning, ICML ’98. San Francisco: Morgan Kaufmann Publishers Inc.: 1998. p. 296–304. <ext-link ext-link-type="uri" xlink:href="http://dl.acm.org/citation.cfm?id=645527.657297">http://dl.acm.org/citation.cfm?id=645527.657297</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR46">
      <label>46</label>
      <mixed-citation publication-type="other">Kingma DP, Ba J. Adam: A method for stochastic optimization. CoRR. 2014; abs/1412.6980. <ext-link ext-link-type="uri" xlink:href="https://doi.org/1412.6980">https://doi.org/1412.6980</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR47">
      <label>47</label>
      <mixed-citation publication-type="other">Hinton GE, Srivastava N, Krizhevsky A, Sutskever I, Salakhutdinov R. Improving neural networks by preventing co-adaptation of feature detectors. CoRR. 2012; abs/1207.0580. <ext-link ext-link-type="uri" xlink:href="https://doi.org/1207.0580">https://doi.org/1207.0580</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR48">
      <label>48</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Dumontier</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Drug-drug interaction extraction via hierarchical rnns on sequence and shortest dependency paths</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>34</volume>
        <issue>5</issue>
        <fpage>828</fpage>
        <lpage>35</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx659</pub-id>
        <pub-id pub-id-type="pmid">29077847</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Herrero-Zazo</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Segura-Bedmar</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Martínez</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Declerck</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>The DDI corpus: An annotated corpus with pharmacological substances and drug-drug interactions</article-title>
        <source>J Biomed Inform</source>
        <year>2013</year>
        <volume>46</volume>
        <issue>5</issue>
        <fpage>914</fpage>
        <lpage>20</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2013.07.011</pub-id>
        <pub-id pub-id-type="pmid">23906817</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wishart</surname>
            <given-names>DS</given-names>
          </name>
          <name>
            <surname>Feunang</surname>
            <given-names>YD</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>AC</given-names>
          </name>
          <name>
            <surname>Lo</surname>
            <given-names>EJ</given-names>
          </name>
          <name>
            <surname>Marcu</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Grant</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Sajed</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Johnson</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Sayeeda</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Assempour</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Iynkkaran</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Maciejewski</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gale</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Wilson</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Chin</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Cummings</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Le</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Pon</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Knox</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Wilson</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>DrugBank 5.0: a major update to the DrugBank database for 2018</article-title>
        <source>Nucleic Acids Res</source>
        <year>2018</year>
        <volume>46</volume>
        <issue>D1</issue>
        <fpage>1074</fpage>
        <lpage>82</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkx1037</pub-id>
      </element-citation>
    </ref>
    <ref id="CR51">
      <label>51</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wheeler</surname>
            <given-names>DL</given-names>
          </name>
          <name>
            <surname>Barrett</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Benson</surname>
            <given-names>DA</given-names>
          </name>
          <name>
            <surname>Bryant</surname>
            <given-names>SH</given-names>
          </name>
          <name>
            <surname>Canese</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Chetvernin</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Church</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>DiCuccio</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Edgar</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Federhen</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Database resources of the national center for biotechnology information</article-title>
        <source>Nucleic Acids Res</source>
        <year>2006</year>
        <volume>35</volume>
        <issue>suppl_1</issue>
        <fpage>5</fpage>
        <lpage>12</lpage>
      </element-citation>
    </ref>
    <ref id="CR52">
      <label>52</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Segura-Bedmar</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Martínez</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Herrero-Zazo</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Lessons learnt from the DDIExtraction-2013 Shared Task</article-title>
        <source>J Biomed Inform</source>
        <year>2014</year>
        <volume>51</volume>
        <issue>May</issue>
        <fpage>152</fpage>
        <lpage>64</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2014.05.007</pub-id>
        <pub-id pub-id-type="pmid">24858490</pub-id>
      </element-citation>
    </ref>
    <ref id="CR53">
      <label>53</label>
      <mixed-citation publication-type="other">Segura-Bedmar I, Martínez P, Zazo MH. Semeval-2013 task 9: Extraction of drug-drug interactions from biomedical texts (ddiextraction 2013). In: Second Joint Conference on Lexical and Computational Semantics (* SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), vol. 2: 2013. p. 341–50.</mixed-citation>
    </ref>
    <ref id="CR54">
      <label>54</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ashburner</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ball</surname>
            <given-names>CA</given-names>
          </name>
          <name>
            <surname>Blake</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Botstein</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Butler</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Cherry</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Davis</surname>
            <given-names>AP</given-names>
          </name>
          <name>
            <surname>Dolinski</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Dwight</surname>
            <given-names>SS</given-names>
          </name>
          <name>
            <surname>Eppig</surname>
            <given-names>JT</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Gene ontology: tool for the unification of biology</article-title>
        <source>Nat Genet</source>
        <year>2000</year>
        <volume>25</volume>
        <issue>1</issue>
        <fpage>25</fpage>
        <pub-id pub-id-type="doi">10.1038/75556</pub-id>
        <pub-id pub-id-type="pmid">10802651</pub-id>
      </element-citation>
    </ref>
    <ref id="CR55">
      <label>55</label>
      <mixed-citation publication-type="other">Dritsou V, Topalis P, Mitraka E, Dialynas E, Louis C. mirnao: An ontology unfolding the domain of micrornas. In: IWBBIO: 2014. p. 989–1000.</mixed-citation>
    </ref>
    <ref id="CR56">
      <label>56</label>
      <mixed-citation publication-type="other">Langlotz CP. RadLex: a new method for indexing online educational materials. Radiological Society of North America; 2006.</mixed-citation>
    </ref>
    <ref id="CR57">
      <label>57</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rath</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Olry</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Dhombres</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Brandt</surname>
            <given-names>MM</given-names>
          </name>
          <name>
            <surname>Urbero</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Ayme</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Representation of rare diseases in health information systems: the orphanet approach to serve a wide range of end users</article-title>
        <source>Hum Mutat</source>
        <year>2012</year>
        <volume>33</volume>
        <issue>5</issue>
        <fpage>803</fpage>
        <lpage>8</lpage>
        <pub-id pub-id-type="doi">10.1002/humu.22078</pub-id>
        <pub-id pub-id-type="pmid">22422702</pub-id>
      </element-citation>
    </ref>
    <ref id="CR58">
      <label>58</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kibbe</surname>
            <given-names>WA</given-names>
          </name>
          <name>
            <surname>Arze</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Felix</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Mitraka</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Bolton</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Fu</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Mungall</surname>
            <given-names>CJ</given-names>
          </name>
          <name>
            <surname>Binder</surname>
            <given-names>JX</given-names>
          </name>
          <name>
            <surname>Malone</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Vasant</surname>
            <given-names>D</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Disease ontology 2015 update: an expanded and updated database of human diseases for linking biomedical knowledge through disease data</article-title>
        <source>Nucleic Acids Res</source>
        <year>2014</year>
        <volume>43</volume>
        <issue>D1</issue>
        <fpage>1071</fpage>
        <lpage>8</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gku1011</pub-id>
      </element-citation>
    </ref>
    <ref id="CR59">
      <label>59</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Ou</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Cui</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Non-transitive hashing with latent similarity components</article-title>
        <source>Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</source>
        <year>2015</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>ACM</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR60">
      <label>60</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lamurias</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ferreira</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Couto</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Improving chemical entity recognition through h-index based semantic similarity</article-title>
        <source>J Cheminformatics</source>
        <year>2015</year>
        <volume>7</volume>
        <issue>Suppl 1</issue>
        <fpage>13</fpage>
        <lpage>120</lpage>
        <pub-id pub-id-type="doi">10.1186/1758-2946-7-S1-S13</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
