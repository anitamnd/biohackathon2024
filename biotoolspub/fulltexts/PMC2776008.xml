<?DTDIdentifier.IdentifierValue article.dtd?>
<?DTDIdentifier.IdentifierType system?>
<?SourceDTD.DTDName article.dtd?>
<?SourceDTD.Version 1.0?>
<?ConverterInfo.XSLTName bmc2nlm.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Algorithms Mol Biol</journal-id>
    <journal-title>Algorithms for Molecular Biology : AMB</journal-title>
    <issn pub-type="epub">1748-7188</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">2776008</article-id>
    <article-id pub-id-type="publisher-id">1748-7188-4-13</article-id>
    <article-id pub-id-type="pmid">19849839</article-id>
    <article-id pub-id-type="doi">10.1186/1748-7188-4-13</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Grammatical-Restrained Hidden Conditional Random Fields for Bioinformatics applications</article-title>
    </title-group>
    <contrib-group>
      <contrib id="A1" corresp="yes" contrib-type="author">
        <name>
          <surname>Fariselli</surname>
          <given-names>Piero</given-names>
        </name>
        <xref ref-type="aff" rid="I1">1</xref>
        <email>piero.fariselli@unibo.it</email>
      </contrib>
      <contrib id="A2" contrib-type="author">
        <name>
          <surname>Savojardo</surname>
          <given-names>Castrense</given-names>
        </name>
        <xref ref-type="aff" rid="I1">1</xref>
        <email>savojard@biocomp.unibo.it</email>
      </contrib>
      <contrib id="A3" contrib-type="author">
        <name>
          <surname>Martelli</surname>
          <given-names>Pier Luigi</given-names>
        </name>
        <xref ref-type="aff" rid="I1">1</xref>
        <email>gigi@biocomp.unibo.it</email>
      </contrib>
      <contrib id="A4" contrib-type="author">
        <name>
          <surname>Casadio</surname>
          <given-names>Rita</given-names>
        </name>
        <xref ref-type="aff" rid="I1">1</xref>
        <email>casadio@biocomp.unibo.it</email>
      </contrib>
    </contrib-group>
    <aff id="I1"><label>1</label>Biocomputing Group, University of Bologna, via Irnerio 42, 40126 Bologna, Italy</aff>
    <pub-date pub-type="collection">
      <year>2009</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>22</day>
      <month>10</month>
      <year>2009</year>
    </pub-date>
    <volume>4</volume>
    <fpage>13</fpage>
    <lpage>13</lpage>
    <ext-link ext-link-type="uri" xlink:href="http://www.almob.org/content/4/1/13"/>
    <history>
      <date date-type="received">
        <day>12</day>
        <month>6</month>
        <year>2009</year>
      </date>
      <date date-type="accepted">
        <day>22</day>
        <month>10</month>
        <year>2009</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright Â© 2009 Fariselli et al; licensee BioMed Central Ltd.</copyright-statement>
      <copyright-year>2009</copyright-year>
      <copyright-holder>Fariselli et al; licensee BioMed Central Ltd.</copyright-holder>
      <license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0">
        <p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0"/>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</p>
        <!--<rdf xmlns="http://web.resource.org/cc/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:dcterms="http://purl.org/dc/terms"><Work xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:dcterms="http://purl.org/dc/terms/" rdf:about=""><license rdf:resource="http://creativecommons.org/licenses/by/2.0"/><dc:type rdf:resource="http://purl.org/dc/dcmitype/Text"/><dc:author>
               Fariselli
               Piero
               
               piero.fariselli@unibo.it
            </dc:author><dc:title>
            Grammatical-Restrained Hidden Conditional Random Fields for Bioinformatics applications
         </dc:title><dc:date>2009</dc:date><dcterms:bibliographicCitation>Algorithms for Molecular Biology 4(1): 13-. (2009)</dcterms:bibliographicCitation><dc:identifier type="sici">1748-7188(2009)4:1&#x0003c;13&#x0003e;</dc:identifier><dcterms:isPartOf>urn:ISSN:1748-7188</dcterms:isPartOf><License rdf:about="http://creativecommons.org/licenses/by/2.0"><permits rdf:resource="http://web.resource.org/cc/Reproduction" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/Distribution" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Notice" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Attribution" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/DerivativeWorks" xmlns=""/></License></Work></rdf>-->
      </license>
    </permissions>
    <abstract>
      <sec>
        <title>Background</title>
        <p>Discriminative models are designed to naturally address classification tasks. However, some applications require the inclusion of grammar rules, and in these cases generative models, such as Hidden Markov Models (HMMs) and Stochastic Grammars, are routinely applied.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>We introduce Grammatical-Restrained Hidden Conditional Random Fields (GRHCRFs) as an extension of Hidden Conditional Random Fields (HCRFs). GRHCRFs while preserving the discriminative character of HCRFs, can assign labels in agreement with the production rules of a defined grammar. The main GRHCRF novelty is the possibility of including in HCRFs prior knowledge of the problem by means of a defined grammar. Our current implementation allows <italic>regular grammar </italic>rules. We test our GRHCRF on a typical biosequence labeling problem: the prediction of the topology of Prokaryotic outer-membrane proteins.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p>We show that in a typical biosequence labeling problem the GRHCRF performs better than CRF models of the same complexity, indicating that GRHCRFs can be useful tools for biosequence analysis applications.</p>
      </sec>
      <sec>
        <title>Availability</title>
        <p>GRHCRF software is available under GPLv3 licence at the website</p>
        <p>
          <ext-link ext-link-type="uri" xlink:href="http://www.biocomp.unibo.it/~savojard/biocrf-0.9.tar.gz."/>
        </p>
      </sec>
    </abstract>
  </article-meta>
</front>
<body>
  <sec>
    <title>Background</title>
    <p>Sequence labeling is a general task addressed in many different scientific fields, including Bioinformatics and Computational Linguistics [<xref ref-type="bibr" rid="B1">1</xref>-<xref ref-type="bibr" rid="B3">3</xref>]. Recently Conditional Random Fields (CRFs) have been introduced as a new promising framework to solve sequence labeling problems [<xref ref-type="bibr" rid="B4">4</xref>]. CRFs offer several advantages over Hidden Markov Models (HMMs), including the ability of relaxing strong independence assumptions made in HMMs [<xref ref-type="bibr" rid="B4">4</xref>]. CRFs have been successfully applied in biosequence analysis and structural predictions [<xref ref-type="bibr" rid="B5">5</xref>-<xref ref-type="bibr" rid="B11">11</xref>]. However, several problems of sequence analysis can be successfully addressed only by designing a grammar in order to provide meaningful results. For instance in gene prediction tasks exons must be linked in such a way that the donor and acceptor junctions define regions whose length is multiple of three (according to the genetic code), and in protein structure prediction, helical segments shorter than 4 residues should be consider meaningless, being this the shortest allowed length for a protein helical motif [<xref ref-type="bibr" rid="B1">1</xref>,<xref ref-type="bibr" rid="B2">2</xref>]. In this kind of problems, the training sets generally consist of pairs of observed and label sequences and very often the number of the different labels representing the experimental evidence is small compared to the grammar requirements and the length distribution of the segments for the different labels. Then a direct mapping of one-label to one state results in poor predictive performances and HMMs trained for these applications routinely separate labels from state names. The separation of state names and labels allows to model a huge number of concurring paths compatible with the grammar and with the experimental labels without increasing the time and space computational complexity [<xref ref-type="bibr" rid="B1">1</xref>].</p>
    <p>In analogy with the HMM approach, in this paper we develop a discriminative model that incorporates regular-grammar production rules with the aim of integrating the different capabilities of generative and discriminative models. In order to model labels and states disjointly, the regular grammar has to be included in the structure of a Hidden Conditional Random Field (HCRF) [<xref ref-type="bibr" rid="B12">12</xref>-<xref ref-type="bibr" rid="B14">14</xref>]. Previously, McCallum et al. [<xref ref-type="bibr" rid="B13">13</xref>] introduced a special HCRF that exploits a specific automaton to align sequences.</p>
    <p>The model here introduced as Grammatical-Restrained Hidden Conditional Random Field (GRHCRF), separates the states from the labels and restricts the accepted predictions only to those allowed by a predefined grammar. By this, it is possible to cast into the model prior knowledge of the problem at hand, that may not be captured directly from the learning associations and ensures that only meaningful solutions are provided.</p>
    <p>In principle CRFs can directly model the same GRHCRF grammar. However, given the fully-observable nature of the CRFs [<xref ref-type="bibr" rid="B12">12</xref>], the observed sequences must be re-labelled to obtain a bijection between states and labels. This implies that only one specific and <italic>unique </italic>state path for each observed sequence must be selected. On the contrary with GRHCRFs that allow the separation between labels and states, an arbitrary large number of different state paths, corresponding to the same experimentally observed labels, can be counted at the same time. In order to fully exploit this path degeneration in the prediction phase, the decoding algorithm must take into account all possible paths, and the posterior-Viterbi (instead of the Viterbi) should be adopted [<xref ref-type="bibr" rid="B15">15</xref>].</p>
    <p>In this paper we define the model as an extension of a HCRF, we provide the basic inference equations and we introduce a new decoding algorithm for CRF models. We then compare the new GRHCRF with CRFs of the same complexity on a Bioinformatics task whose solution must comply with a given grammar: the prediction of the topological models of Prokaryotic outer membrane proteins. We show that in this task the GRHCRF performance is higher than to those achieved by CRF and HMM models of the same complexity.</p>
  </sec>
  <sec sec-type="methods">
    <title>Methods</title>
    <p>In what follows <bold>x </bold>is the random variable over the data sequences to be labeled, <bold>y </bold>is the random variable over the corresponding label sequences and <bold>s</bold> is the random variable over the hidden states. We use an upper-script index when we deal with multiple sequences. The problem that we want to model is then described by the observed sequences <bold>x</bold><sup>(<italic>i</italic>)</sup>, by the labels <bold>y</bold><sup>(<italic>i</italic>) </sup>and by the underlying grammar <italic>G </italic>that is specified by its production rules with respect to the set of the hidden states. Although it is possible to imagine more complex models, in what follows we restrict each state to have only one possible associated label. Thus we define a function that maps each hidden state to a given label as:</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i1.gif"/>
      </disp-formula>
    </p>
    <p>The difference between the CRF and GRHCRF (or HCRF) models can be seen in Figure <xref ref-type="fig" rid="F1">1</xref>, where their graphical structure is presented. GRHCRF and HCRF are indistinguishable from their graphical structure representation since it depicts only the conditional dependence among the random variables. Since the number of the states |{<bold>s</bold>}| is always greater than the number of possible labels |{<bold>y</bold>}| the GRHCRFs (HCRFs) have more expressive power than the corresponding CRFs.</p>
    <fig position="float" id="F1">
      <label>Figure 1</label>
      <caption>
        <p><bold>Graphical structure of a linear-CRF (left) and a linear GRHCRF/HCRF (right)</bold>.</p>
      </caption>
      <graphic xlink:href="1748-7188-4-13-1"/>
    </fig>
    <p>We further restrict our model to linear HCRF, so that the computational complexity of the inference algorithms remains linear with respect to the sequence length. This choice implies that the embedded grammar will be <italic>regular</italic>. Our implementation and tests are based on first order HCRFs with explicit transition functions (<italic>t</italic><sub><italic>k</italic></sub>(<italic>s</italic><sub><italic>j</italic>-1</sub>, <italic>s</italic><sub><italic>j</italic></sub>, <bold>x</bold>)) and state functions (<italic>g</italic><sub><italic>k</italic></sub>(<italic>s</italic><sub><italic>j</italic></sub>, <bold>x</bold>)) unrolled over each sequence position <italic>j</italic>.</p>
    <p>However, for sake of clarity in the following we use the compact notation:</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i2.gif"/>
      </disp-formula>
    </p>
    <p>where <italic>f</italic><sub><italic>k</italic></sub>(<italic>s</italic><sub><italic>j</italic>-1</sub>, <italic>s</italic><sub><italic>j</italic></sub>, <bold>x</bold>) can be either a transition feature function <italic>t</italic><sub><italic>l</italic></sub>(<italic>s</italic><sub><italic>j</italic>-1</sub>, <italic>s</italic><sub><italic>j</italic></sub>, <bold>x</bold>) or a state feature function <italic>gn</italic>(<italic>s</italic><sub><italic>j</italic></sub>, <bold>x</bold>). Following the usual notation [<xref ref-type="bibr" rid="B16">16</xref>] we extend the local functions to include the hidden states as</p>
    <p>
      <disp-formula id="bmcM1">
        <label>(1)</label>
        <graphic xlink:href="1748-7188-4-13-i3.gif"/>
      </disp-formula>
    </p>
    <p>and we set the two constraints as:</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i4.gif"/>
      </disp-formula>
    </p>
    <p>With this choice, the local function Ï<sub><italic>j</italic></sub>(<bold>s</bold>, <bold>y</bold>, <bold>x</bold>) becomes zero when the labeling (Î©(<italic>s</italic><sub><italic>j</italic></sub>, <italic>y</italic><sub><italic>j</italic></sub>)) or the grammar production rules (Î(<italic>s</italic>, <italic>s</italic>')) are not allowed. In turn this sets to zero the corresponding probabilities. As in the case of the HCRF, for the whole sequence we define Î¨(<bold>s</bold>, <bold>y</bold>, <bold>x</bold>) = Î <sub><italic>j</italic></sub>Ï<sub><italic>j</italic></sub>(<bold>s</bold>, <bold>y</bold>, <bold>x</bold>) and the normalization factors (or partition functions) can be obtained summing over all possible sequences of hidden states (or latent variables):</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i5.gif"/>
      </disp-formula>
    </p>
    <p>or summing over all possible sequences of labels and hidden states:</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i6.gif"/>
      </disp-formula>
    </p>
    <p>Using the normalization factors the joint probability of a label sequence <bold>y </bold>and an hidden state sequence <bold>s </bold>given an observation sequence <bold>x </bold>is:</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i7.gif"/>
      </disp-formula>
    </p>
    <p>The probability of an hidden state sequence given a label sequence and an observation sequence is:</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i8.gif"/>
      </disp-formula>
    </p>
    <p>Finally, the probability of a label sequence given an observation sequence can be computed as follows:</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i9.gif"/>
      </disp-formula>
    </p>
    <sec>
      <title>Parameter estimation</title>
      <p>The model parameters (Î¸) can be obtained by maximizing the log-likelihood of the data:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i10.gif"/>
        </disp-formula>
      </p>
      <p>where the different sequences are supposed to be independent and identically distributed random variables.</p>
      <p>Taking the first derivative with respect to parameter Î»<sub><italic>k </italic></sub>of the objective function we obtain:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i11.gif"/>
        </disp-formula>
      </p>
      <p>where, in analogy with the Boltzmann machines and HMMs for labelled sequences [<xref ref-type="bibr" rid="B17">17</xref>], <inline-formula><inline-graphic xlink:href="1748-7188-4-13-i12.gif"/></inline-formula> and â± an be seen as <italic>clamped </italic>and <italic>free </italic>phases. After simple computations we can rewrite the derivative as:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i13.gif"/>
        </disp-formula>
      </p>
      <p>where the <italic>E</italic><sub><italic>p</italic>(<bold>s</bold>|<bold>y</bold>, <bold>x</bold>) </sub>[<italic>f</italic><sub><italic>k</italic></sub>] and <italic>E</italic><sub><italic>p</italic>(<bold>s</bold>, <bold>y</bold>|<bold>x</bold>) </sub>[<italic>f</italic><sub><italic>k</italic></sub>] are the expected values of the feature function <italic>f</italic><sub><italic>k </italic></sub>computed in the clamped and free phases, respectively. Differently from the standard CRF, both expectations have to be computed using the Forward and Backward algorithms. These algorithms must take into consideration the grammar restraints.</p>
      <p>To avoid overfitting, we regularize the objective function using a Gaussian prior, so that the function to maximize has the form of:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i14.gif"/>
        </disp-formula>
      </p>
      <p>and the corresponding gradient is:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i15.gif"/>
        </disp-formula>
      </p>
      <p>Alternatively, the Expectation Maximization procedure can be adopted [<xref ref-type="bibr" rid="B16">16</xref>].</p>
    </sec>
    <sec>
      <title>Computing the expectations</title>
      <p>The partition functions and the expectations can be computed using the dynamic programming by defining the so called forward and backward algorithms [<xref ref-type="bibr" rid="B1">1</xref>,<xref ref-type="bibr" rid="B2">2</xref>,<xref ref-type="bibr" rid="B4">4</xref>]. For the clamped phase the forward algorithm is:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i16.gif"/>
        </disp-formula>
      </p>
      <p>where the clamped phase matrix <italic>M</italic><sub><italic>C </italic></sub>takes into account both the grammar constraint (Î(<italic>s</italic>', <italic>s</italic>)) and the current given labeling <bold>y </bold><inline-formula><inline-graphic xlink:href="1748-7188-4-13-i17.gif"/></inline-formula>.</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i18.gif"/>
        </disp-formula>
      </p>
      <p>The forward algorithm for the free phase is computed as:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i19.gif"/>
        </disp-formula>
      </p>
      <p>where the free phase matrix <italic>M</italic><sub><italic>F </italic></sub>is defined as:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i20.gif"/>
        </disp-formula>
      </p>
      <p>It should be noted that also in the free phase the algorithm has to take into account the grammar production rules Î(<italic>s</italic>', <italic>s</italic>) and only the paths that are in agreement with the grammar are counted. Analogously, the backward algorithms can be computed for the clamped phase as:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i21.gif"/>
        </disp-formula>
      </p>
      <p>where <italic>L</italic><sup>(<italic>i</italic>) </sup>is the length of the <italic>i</italic><sup><italic>th </italic></sup>protein. For the free phase we have:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i22.gif"/>
        </disp-formula>
      </p>
      <p>The expectations of the feature functions (<italic>E</italic><sub><italic>p</italic>(<bold>s</bold>|<bold>y</bold>, <bold>x</bold>) </sub>[<italic>f</italic><sub><italic>k</italic></sub>], <italic>E</italic><sub><italic>p</italic>(<bold>s</bold>, <bold>y</bold>|<bold>x</bold>) </sub>[<italic>f</italic><sub><italic>k</italic></sub>]) are computed as:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i23.gif"/>
        </disp-formula>
      </p>
      <p>The partition functions can be computed using both forward or backward algorithms as:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i24.gif"/>
        </disp-formula>
      </p>
      <p>where for simplicity we dropped out the sequence upper-script ((<italic>i</italic>)).</p>
    </sec>
    <sec>
      <title>Decoding</title>
      <p>Decoding is the task of assigning labels (<bold>y</bold>) to an unknown observation sequence <bold>x</bold>. Viterbi algorithm is routinely applied as decoding for the CRFs, since it finds the most probable path of an observation sequence given a CRF model [<xref ref-type="bibr" rid="B4">4</xref>]. Viterbi algorithm is particular effective when there is a single strong highly probable path, while when several paths compete (have similar probabilities), <italic>posterior decoding </italic>may perform significantly better. However, the selected state path of the posterior decoding may not be allowed by the grammar. A simple solution of this problem is provided by the posterior-Viterbi decoding, that was previously introduced for HMMs [<xref ref-type="bibr" rid="B15">15</xref>]. Posterior-Viterbi, exploits the posterior probabilities and at the same time preserves the grammatical constraint. This algorithm consists of three steps:</p>
      <p>â¢ for each position <italic>j </italic>and state <italic>s </italic>â <inline-formula><inline-graphic xlink:href="1748-7188-4-13-i25.gif"/></inline-formula>, compute posterior probability <italic>p</italic>(<italic>s</italic><sub><italic>j </italic></sub>= <italic>s</italic>|<bold>x</bold>)</p>
      <p>â¢ find the allowed state path</p>
      <p>S* = argmax<sub><bold>s </bold></sub>Î <sub><italic>j </italic></sub><italic>p</italic>(<italic>s</italic><sub><italic>j </italic></sub>= <bold>s</bold>|<bold>x</bold>)</p>
      <p>â¢ assig to <bold>x </bold>a label sequence <bold>y </bold>so that <italic>y</italic><sub><italic>j </italic></sub>= Î(<italic>s</italic><sub><italic>j</italic></sub>) for each position <italic>j</italic></p>
      <p>The first step can be accomplished using the Forward-Backward algorithm as described for the free phase of parameter estimation. In order to find the best allowed state path, a Viterbi search is performed over posterior probabilities. In what follows <italic>Ï</italic><sub><italic>j</italic></sub>(<italic>s</italic>|<bold>x</bold>) is the most probable allowed path of length <italic>j </italic>ending in state <italic>s </italic>and <italic>Ï</italic><sub><italic>j</italic></sub>(<italic>s</italic>) is a traceback pointer. The algorithm can be described as follows:</p>
      <p>1. Initialization:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i26.gif"/>
        </disp-formula>
      </p>
      <p>2. Recursion</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i27.gif"/>
        </disp-formula>
      </p>
      <p>3. Termination and Traceback</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i28.gif"/>
        </disp-formula>
      </p>
      <p>The labels are assigned to the observed sequence according to the state path <bold>s</bold>*. It is also possible to consider a slightly modified version of the algorithm where, for each position, the posterior probability of the <italic>labels </italic>is considered, and the states with the same label have associated the same posterior probability. The rationale behind this is to consider the aggregate probability of all state paths corresponding to the same sequence of labels to improve the overall per label accuracy. In many applications this variant of the algorithm might perform better.</p>
    </sec>
    <sec>
      <title>Implementation</title>
      <p>We implemented the GRHCRF as linear HCRF in C++ language. Our GRHCRF can deal with sequences of symbols as well as sequence profiles. A <italic>sequence profile </italic>of a protein <italic>p </italic>is a matrix <italic>X </italic>whose rows represent the sequence positions and whose columns are the 20 possible amino acids. Each element <italic>X </italic>[<italic>i</italic>] [<italic>a</italic>] of the sequence profile represents the frequency of the residue type <italic>a </italic>in the aligned position <italic>i</italic>. The profile positions are normalized such as Î£<sub><italic>a</italic></sub><italic>X</italic>[<italic>i</italic>][<italic>a</italic>] = 1 (for each <italic>i</italic>).</p>
      <p>In order to take into account the information of the neighboring residues we define a symmetric sliding window of length <italic>w </italic>centered into the <italic>i</italic>-th residue. With this choice the state feature functions are defined as:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i29.gif"/>
        </disp-formula>
      </p>
      <p>where <italic>s </italic>runs over all possible states, <italic>a </italic>runs over the different observed symbols <italic>A </italic>(in our case the 20 residues) and <italic>k </italic>runs over the neighbor residues (from - <inline-formula><inline-graphic xlink:href="1748-7188-4-13-i30.gif"/></inline-formula> to <inline-formula><inline-graphic xlink:href="1748-7188-4-13-i31.gif"/></inline-formula>). When dealing with single sequences, the state functions are simply products of Kronecker's deltas:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i32.gif"/>
        </disp-formula>
      </p>
      <p>while in the case of sequence profiles, the state features are real-valued and assume the profile scores:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i33.gif"/>
        </disp-formula>
      </p>
    </sec>
  </sec>
  <sec>
    <title>Measures of performance</title>
    <p>To evaluate the accuracy we define the classical label-based indices, such as:</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i34.gif"/>
      </disp-formula>
    </p>
    <p>where <italic>p </italic>and <italic>N </italic>are the total number of correct predictions and total number of examples, respectively. The Matthews correlation coefficient (C) for a given class s is defined as:</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i35.gif"/>
      </disp-formula>
    </p>
    <p><italic>p</italic>(<italic>s</italic>) and n(<italic>s</italic>) are respectively the true positive and true negative predictions for class <italic>s</italic>, while <italic>o</italic>(<italic>s</italic>) and <italic>u</italic>(<italic>s</italic>) are the numbers of false positives and false negatives with respect to that class. The sensitivity (coverage, <italic>Sn</italic>) for each class <italic>s </italic>is defined as</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i36.gif"/>
      </disp-formula>
    </p>
    <p>The specificity (accuracy, <italic>Sp</italic>) is the probability of correct predictions and it is defined as follows:</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i37.gif"/>
      </disp-formula>
    </p>
    <p>However, these measures cannot discriminate between similar and dissimilar segment distributions and do not provide any clues about the number of proteins that are correctly predicted. For this reason we introduce a <italic>protein-based </italic>index, the Protein OVerlap (POV) measure. We consider a protein prediction to be correct only if the number of predicted and observed transmembrane segments (in the structurally resolved proteins, see Outer-membrane protein data set section) is the same and if all corresponding pairs have a minimum segment overlap. POV is a binary measure (0 or 1) and for a given protein sequence <italic>s </italic>is defined as:</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i38.gif"/>
      </disp-formula>
    </p>
    <p>Where <inline-formula><inline-graphic xlink:href="1748-7188-4-13-i39.gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="1748-7188-4-13-i40.gif"/></inline-formula> are the numbers of predicted and observed segments, while <italic>p</italic><sub><italic>i </italic></sub>and <italic>o</italic><sub><italic>i </italic></sub>are the <italic>i</italic><sup><italic>th </italic></sup>predicted and observed segments, respectively. The threshold Î¸ is defined as the mean of the half lengths of the segments:</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i41.gif"/>
      </disp-formula>
    </p>
    <p>where <italic>L</italic><sub><italic>p</italic></sub>(= |<italic>p</italic><sub><italic>i</italic></sub>|) and <italic>L</italic><sub><italic>o</italic></sub>(= |<italic>o</italic><sub><italic>i</italic></sub>|) are the lengths of the predicted and observed segments, respectively. For a set of proteins the average of all POVs over the total number of proteins <italic>N </italic>is:</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i42.gif"/>
      </disp-formula>
    </p>
    <p>To evaluate the average standard deviation of our predictions, we performed a bootstrapping procedure with 100 runs over 60% of the predicted data sets.</p>
  </sec>
  <sec>
    <title>Results and Discussion</title>
    <sec>
      <title>Problem definition</title>
      <p>The prediction of the topology of the outer membrane proteins in Prokaryote organisms is a challenging task that was addressed several times given its biological relevance [<xref ref-type="bibr" rid="B18">18</xref>-<xref ref-type="bibr" rid="B20">20</xref>]. The problem can be defined as: given a protein sequence that is known to be inserted in the outer membrane of a Prokaryotic cell, we want to predict the number and the location with respect to the membrane plane of the membrane-spanning segments. From experimental results, we know that the outer membrane of Prokaryotes imposes some constraints to the topological models such as:</p>
      <p>â¢ both C and N termini of the protein chain lie in the periplasmic side of the cell (inside) and this implies that the number of the spanning segments is even;</p>
      <p>â¢ membrane spanning segments have a minimal segment length (â¥ 3 residues);</p>
      <p>â¢ the transmembrane-segment lengths are distributed accordingly to a probability density distribution that can be experimentally determined and must be taken into account.</p>
      <p>For the reasons listed above the best performing predictors described in literature are based on HMMs and among them the best performing single-method in the task of the topology prediction is HMM-B2TMR [<xref ref-type="bibr" rid="B18">18</xref>] (see Table <xref ref-type="table" rid="T1">1</xref> in [<xref ref-type="bibr" rid="B20">20</xref>]).</p>
      <table-wrap position="float" id="T1">
        <label>Table 1</label>
        <caption>
          <p>Prediction of the topology of the Prokaryotic outer membrane proteins.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <td align="left">
                <bold>Method</bold>
              </td>
              <td align="center">
                <bold>POV</bold>
              </td>
              <td align="center">
                <bold>Q2</bold>
              </td>
              <td align="center">
                <bold>C(t)</bold>
              </td>
              <td align="center">
                <bold>Sn(t)</bold>
              </td>
              <td align="center">
                <bold>Sp(t)</bold>
              </td>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left">CRF-1 (Vit)</td>
              <td align="center">0.26 Â± 0.05</td>
              <td align="center">0.72 Â± 0.01</td>
              <td align="center">0.47 Â± 0.02</td>
              <td align="center">0.59 Â± 0.01</td>
              <td align="center">0.80 Â± 0.01</td>
            </tr>
            <tr>
              <td align="left">CRF-1 (Pvit)</td>
              <td align="center">0.39 Â± 0.05</td>
              <td align="center">0.77 Â± 0.01</td>
              <td align="center">0.54 Â± 0.02</td>
              <td align="center">0.71 Â± 0.01</td>
              <td align="center">0.80 Â± 0.01</td>
            </tr>
            <tr>
              <td/>
              <td/>
              <td/>
              <td/>
              <td/>
              <td/>
            </tr>
            <tr>
              <td align="left">CRF-2 (Vit)</td>
              <td align="center">0.34 Â± 0.05</td>
              <td align="center">0.76 Â± 0.01</td>
              <td align="center">0.52 Â± 0.03</td>
              <td align="center">0.63 Â± 0.02</td>
              <td align="center">0.82 Â± 0.02</td>
            </tr>
            <tr>
              <td align="left">CRF-2 (Pvit)</td>
              <td align="center">0.47 Â± 0.05</td>
              <td align="center">0.80 Â± 0.01</td>
              <td align="center">0.60 Â± 0.03</td>
              <td align="center">0.74 Â± 0.02</td>
              <td align="center">0.82 Â± 0.02</td>
            </tr>
            <tr>
              <td/>
              <td/>
              <td/>
              <td/>
              <td/>
              <td/>
            </tr>
            <tr>
              <td align="left">CRF-3 (Vit)</td>
              <td align="center">0.29 Â± 0.04</td>
              <td align="center">0.72 Â± 0.01</td>
              <td align="center">0.45 Â± 0.02</td>
              <td align="center">0.60 Â± 0.02</td>
              <td align="center">0.79 Â± 0.01</td>
            </tr>
            <tr>
              <td align="left">CRF-3 (Pvit)</td>
              <td align="center">0.45 Â± 0.04</td>
              <td align="center">0.76 Â± 0.01</td>
              <td align="center">0.52 Â± 0.02</td>
              <td align="center">0.70 Â± 0.02</td>
              <td align="center">0.79 Â± 0.01</td>
            </tr>
            <tr>
              <td/>
              <td/>
              <td/>
              <td/>
              <td/>
              <td/>
            </tr>
            <tr>
              <td align="left">GRHCRF</td>
              <td align="center">0.66 Â± 0.04</td>
              <td align="center">0.85 Â± 0.01</td>
              <td align="center">0.70 Â± 0.03</td>
              <td align="center">0.83 Â± 0.01</td>
              <td align="center">0.84 Â± 0.01</td>
            </tr>
            <tr>
              <td/>
              <td/>
              <td/>
              <td/>
              <td/>
              <td/>
            </tr>
            <tr>
              <td align="left">HMM-B2TMR</td>
              <td align="center">0.58 Â± 0.04</td>
              <td align="center">0.80 Â± 0.01</td>
              <td align="center">0.62 Â± 0.02</td>
              <td align="center">0.82 Â± 0.02</td>
              <td align="center">0.83 Â± 0.01</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p>C(t), Sn(t) and Sp(t) are reported for the transmembrane segments (t).</p>
          <p>Vit = Viterbi decoding, Pvit = posterior-Viterbi decoding.</p>
          <p>For GRHCRF and HMM-B2TMR we used the posterior-Viterbi decoding.</p>
          <p>Models are detailed in the text. Scoring indices are described in Measure of Accuracy section.</p>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>Outer-membrane protein data set</title>
      <p>The training set consists of 38 high-resolution experimentally determined outer-membrane proteins of Prokaryotes, whose sequence identity between each pair is less than 40%. We then generated 19 subsets for the cross-validation experiments, such as there is no sequence identity greater than 25% and no functional similarity between two elements belonging to disjoint sets. The annotation consists of three different labelings that correspond to: <italic>inner loop </italic>(i), <italic>outer loop </italic>(o) and <italic>transmembrane </italic>(t). This assignment was obtained using the DSSP program [<xref ref-type="bibr" rid="B21">21</xref>] by selecting the <italic>Î²</italic>-strands that span the outer membrane. The dataset with the annotations and the cross-validation sets are available with the program at <ext-link ext-link-type="uri" xlink:href="http://www.biocomp.unibo.it/~savojard/biocrf-0.9.tar.gz"/>.</p>
      <p>For each protein in the dataset, a profile based on a multiple sequence alignment was created using the PSI-BLAST program on the non-redundant dataset of sequences (uniref90 as described in <ext-link ext-link-type="uri" xlink:href="http://www.uniprot.org/help/uniref"/>). PSI-BLAST runs were performed using a fixed number of cycles set to 3 and an e-value of 0.001.</p>
    </sec>
    <sec>
      <title>Prediction of the topology of Prokaryotic outer membrane proteins</title>
      <p>The topology of outer-membrane proteins in Prokaryotes can be described assigning each residue to one of three types: inner loop (<italic>i</italic>), transmembrane <italic>Î²</italic>-strand (<italic>t</italic>), outer loop (<italic>o</italic>). These three types are defined according to the experimental evidence and are the <italic>terminal </italic>symbols of the grammar. The chemico-physical and geometrical characteristics of the three types of segments as deduced by the available structures in the PDB suggest how to build a grammar (or the corresponding automaton) for the prediction of the topology. We performed our experiments using the automaton depicted in Figure <xref ref-type="fig" rid="F2">2</xref>, which was previously introduced to model our HMM-B2TMR [<xref ref-type="bibr" rid="B18">18</xref>] (this automaton is substantially similar to all other HMMs used for this task [<xref ref-type="bibr" rid="B19">19</xref>,<xref ref-type="bibr" rid="B20">20</xref>]). It is essentially based on three different types of states. The states of the automaton are the <italic>non-terminal </italic>symbols of the regular grammar and the arrows represent the allowed transitions (or production rules). The states represented with squares describe the transmembrane strands while the states shown with circles represent the loops (Figure <xref ref-type="fig" rid="F2">2</xref>). A statistics on the non-redundant database of outer membrane proteins presently available, indicates that the length of the strands of the training set ranges from 3 to 22 residues (with an average length of 12 residues). In Prokaryotic outer membrane proteins the inner loops are generally shorter than outer loops. Furthermore, both the N-terminus and C-terminus of all the proteins lie in the inner side of the membrane [<xref ref-type="bibr" rid="B18">18</xref>]. These constraints are modelled by means of the allowed transitions between the states.</p>
      <fig position="float" id="F2">
        <label>Figure 2</label>
        <caption>
          <p><bold>Automaton structure designed for the prediction of the topology of the outer-membrane proteins in Prokaryotes with GRHCRFs and HMMs</bold>.</p>
        </caption>
        <graphic xlink:href="1748-7188-4-13-2"/>
      </fig>
      <p>The automaton described in Figure <xref ref-type="fig" rid="F2">2</xref> assigns labels to observed sequences that can be obtained using different state paths. This ambiguity leads to an ensemble of paths that must be taken into account during the likelihood maximization by summing up all possible trajectories compliant with the experimentally assigned labels (see Method section).</p>
      <p>However, this ambiguity does not permit the adoption of the automaton of Figure <xref ref-type="fig" rid="F2">2</xref> for CRF learning, since to train CRFs a bijective mapping between states and labels is required. On the contrary, with the automaton of Figure <xref ref-type="fig" rid="F2">2</xref>, several different state paths can be obtained (in theory a factorial number) that are in agreement with the automaton and with the experimental labels.</p>
      <p>For this reason and for sake of comparison, we designed three other automata (Figure <xref ref-type="fig" rid="F3">3a, b</xref> and <xref ref-type="fig" rid="F3">3c</xref>) that have the same number of states but are non-ambiguous in term of state mapping. Then, starting from the experimentally derived labels, three different sets of re-labelled sequences can be derived to train CRFs (here referred as CRF1, CRF2 and CRF3).</p>
      <fig position="float" id="F3">
        <label>Figure 3</label>
        <caption>
          <p><bold>Three different non-ambigous automata derived from the one depicted in Figure 2</bold>. These automata are designed to have a bijective mapping between the states and the labels (after the corresponding re-labeling of the sequences). In the text they are referred as CRF1 (a), CRF2 (b) and CRF3 (c).</p>
        </caption>
        <graphic xlink:href="1748-7188-4-13-3"/>
      </fig>
      <p>All compared methods take as input sequence profile and are bench-marked as shown in Table <xref ref-type="table" rid="T1">1</xref>. In the case of non-ambiguous automata of the CRFs, we tested both the Viterbi and posterior-Viterbi algorithms since given the <italic>Viterbi-like </italic>learning of the CRFs it is not <italic>a priori </italic>predictable which one of the two decodings performs better on this particular task. From Table <xref ref-type="table" rid="T1">1</xref> it is clear that assigning the labels according to the posterior-Viterbi always leads to better performance than with the Viterbi (see CRF in Table <xref ref-type="table" rid="T1">1</xref>). This indicates that also in other tasks where CRFs are applied, the posterior-Viterbi here described can increase the overall decoding accuracy. Furthermore, the fact that both HMM-B2TMR and GRHCRF perform better than the others, implies that in the tasks where the observed labels may hide a more complex structure, as in the case of the prediction of the Prokaryotic outer membrane proteins, it is advantageous exploiting the ambiguity by taking into consideration multiple concurring paths at the same time, both during training and decoding (see Method section). Considering that underlying grammar is the same, the discriminative GRHCRF outperforms the generative model (HMM-B2TMR). This indicates that the GRHCRF can substitute the HMM-based models when the labeling prediction is the major issue. In order to asses the confidence level of our results, we computed pairwise t-tests between the GRHCRF and the other methods. From the t-test results reported in Table <xref ref-type="table" rid="T2">2</xref>, it is evident that the measures of the performace shown in Table <xref ref-type="table" rid="T1">1</xref> can be considered significant with a confidence level greater than 80% (see the most relevant index POV).</p>
      <table-wrap position="float" id="T2">
        <label>Table 2</label>
        <caption>
          <p>Confidence level of the results reported in Table 1.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <td align="left">
                <bold>Methods</bold>
              </td>
              <td align="center">
                <bold>POV</bold>
              </td>
              <td align="center">
                <bold>Q2</bold>
              </td>
              <td align="center">
                <bold>C(t)</bold>
              </td>
              <td align="center">
                <bold>Sn(t)</bold>
              </td>
              <td align="center">
                <bold>Sp(t)</bold>
              </td>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left">GRHCRF vs CRF-1</td>
              <td align="center">98.0%</td>
              <td align="center">99.5%</td>
              <td align="center">99.5%</td>
              <td align="center">99.8%</td>
              <td align="center">99.5%</td>
            </tr>
            <tr>
              <td align="left">GRHCRF vs CRF-2</td>
              <td align="center">96.0%</td>
              <td align="center">99.5%</td>
              <td align="center">99.5%</td>
              <td align="center">99.5%</td>
              <td align="center">99.5%</td>
            </tr>
            <tr>
              <td align="left">GRHCRF vs CRF-3</td>
              <td align="center">96.0%</td>
              <td align="center">99.5%</td>
              <td align="center">99.5%</td>
              <td align="center">99.5%</td>
              <td align="center">99.5%</td>
            </tr>
            <tr>
              <td align="left">GRHCRF vs HMM-B2TMR</td>
              <td align="center">80.0%</td>
              <td align="center">96.0%</td>
              <td align="center">99.0%</td>
              <td align="center">98.0%</td>
              <td align="center">99.5%</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p>The confidence level on the significance of the differences was computed with a t-test.</p>
          <p>For all methods we consider the best results of Table 1.</p>
        </table-wrap-foot>
      </table-wrap>
    </sec>
  </sec>
  <sec>
    <title>Conclusion</title>
    <p>In this paper we presented a new class of conditional random fields that assigns labels in agreement with production rules of a defined regular grammar. The main novelty of GRHCRF is then the introduction of an explicit regular grammar that defines the prior knowledge of the problem at hand, eliminating the need of relabelling the observed sequences. The GRHCRF predictions satisfy the grammar production rules by construction, so that only meaningful solutions are provided. In [<xref ref-type="bibr" rid="B13">13</xref>], an automaton was included to restrain the solution of a HCRFs. However in that case, it was hard-coded in the model in order to train finite-state string edit distance. On the contrary, GRHCRFs are designed to provide solutions in agreement with defined regular grammars that are provided as further input to the model. To the best of our knowledge, this is the first time that this is described. In principle, the grammar may be very complex, however, to maintain the tractability of the inference algorithm, we restrict our implementation to regular grammars. Extensions to context-free grammars can be designed by modifying the inference algorithms at the expense of the computational complexity of the final models. Since the Grammatical-Restrained HCRF can be seen as an extension of linear HCRF [<xref ref-type="bibr" rid="B13">13</xref>,<xref ref-type="bibr" rid="B14">14</xref>], the GRHCRF is also related to the models that deal with latent variables such as Dynamic CRFs [<xref ref-type="bibr" rid="B22">22</xref>].</p>
    <p>In this paper we also test the GRHCRFs on a real biological problem that require grammatical constraints: the prediction of the topology of Prokaryotic outer-membrane proteins. When applied to this biosequence analysis problem we show that GRHCRFs perform similarly or better than the corresponding CRFs and HMMs indicating that GRHCRFs can be profitably applied when a discriminative problem requires grammatical constraints.</p>
    <p>Finally we also present the posterior-Viterbi decoding algorithm for CRFs that was previously designed for HMMs and that can be of general interest and application, since in many cases posterior-Viterbi can perform significantly better than the classical Viterbi algorithm.</p>
  </sec>
  <sec>
    <title>Competing interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </sec>
  <sec>
    <title>Authors' contributions</title>
    <p>PF and CS formalized the GRHCRF model. CS wrote the GRHCRF code. CS and PF performed the experiments. PF, PLM and RC defined the problem and provided the data. CS, PF, PLM and RC authored the manuscript.</p>
  </sec>
</body>
<back>
  <ack>
    <sec>
      <title>Acknowledgements</title>
      <p>We thank MIUR for the PNR 2003 project (FIRB art.8) termed LIBI-Laboratorio Internazionale di BioInformatica delivered to R. Casadio. This work was also supported by the Biosapiens Network of Excellence project (a grant of the European Unions VI Framework Programme).</p>
    </sec>
  </ack>
  <ref-list>
    <ref id="B1">
      <citation citation-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Durbin</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <source>Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids</source>
        <year>1999</year>
        <publisher-name>Cambridge Univ Pr, reprint edition</publisher-name>
      </citation>
    </ref>
    <ref id="B2">
      <citation citation-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Baldi</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Brunak</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <source>Bioinformatics: The Machine Learning Approach</source>
        <year>2001</year>
        <edition>2</edition>
        <publisher-name>MIT Press</publisher-name>
      </citation>
    </ref>
    <ref id="B3">
      <citation citation-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Manning</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>SchÃ¼tze</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <source>Foundations of Statistical Natural Language Processing</source>
        <year>1999</year>
        <publisher-name>MIT Press</publisher-name>
      </citation>
    </ref>
    <ref id="B4">
      <citation citation-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Lafferty</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>McCallum</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Pereira</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data</article-title>
        <source>Proceedings of ICML01</source>
        <year>2001</year>
        <fpage>282</fpage>
        <lpage>289</lpage>
      </citation>
    </ref>
    <ref id="B5">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Carbonell</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Weigele</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Gopalakrishnan</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Protein fold recognition using segmentation conditional random fields (SCRFs)</article-title>
        <source>Journal of Computational Biology</source>
        <year>2006</year>
        <volume>13</volume>
        <fpage>394</fpage>
        <lpage>406</lpage>
        <pub-id pub-id-type="pmid">16597248</pub-id>
        <pub-id pub-id-type="doi">10.1089/cmb.2006.13.394</pub-id>
      </citation>
    </ref>
    <ref id="B6">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sato</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Sakakibara</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>RNA secondary structural alignment with conditional random fields</article-title>
        <source>Bioinformatics</source>
        <year>2005</year>
        <volume>21</volume>
        <fpage>237</fpage>
        <lpage>242</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bti1139</pub-id>
      </citation>
    </ref>
    <ref id="B7">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Sauer</surname>
            <given-names>UH</given-names>
          </name>
        </person-group>
        <article-title>OnD-CRF: predicting order and disorder in proteins conditional random fields</article-title>
        <source>Bioinformatics</source>
        <year>2008</year>
        <volume>24</volume>
        <fpage>1401</fpage>
        <lpage>1402</lpage>
        <pub-id pub-id-type="pmid">18430742</pub-id>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btn132</pub-id>
      </citation>
    </ref>
    <ref id="B8">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>CT</given-names>
          </name>
          <name>
            <surname>Yuan</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wilson</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>An unsupervised conditional random fields approach for clustering gene expression time series</article-title>
        <source>Bioinformatics</source>
        <year>2008</year>
        <volume>24</volume>
        <fpage>2467</fpage>
        <lpage>2473</lpage>
        <pub-id pub-id-type="pmid">18718949</pub-id>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btn375</pub-id>
      </citation>
    </ref>
    <ref id="B9">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>MH</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>XL</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Protein protein interaction site prediction based on conditional random fields</article-title>
        <source>Bioinformatics</source>
        <year>2007</year>
        <volume>23</volume>
        <fpage>597</fpage>
        <lpage>604</lpage>
        <pub-id pub-id-type="pmid">17234636</pub-id>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btl660</pub-id>
      </citation>
    </ref>
    <ref id="B10">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dang</surname>
            <given-names>TH</given-names>
          </name>
          <name>
            <surname>Van Leemput</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Verschoren</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Laukens</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Prediction of kinase-specific phosphorylation sites using conditional random fields</article-title>
        <source>Bioinformatics</source>
        <year>2008</year>
        <volume>24</volume>
        <fpage>2857</fpage>
        <lpage>2864</lpage>
        <pub-id pub-id-type="pmid">18940828</pub-id>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btn546</pub-id>
      </citation>
    </ref>
    <ref id="B11">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xia</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Su</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>MICAlign: a sequence-to-structure alignment tool integrating multiple sources of information in conditional random fields</article-title>
        <source>Bioinformatics</source>
        <year>2009</year>
        <volume>25</volume>
        <fpage>1433</fpage>
        <lpage>1434</lpage>
        <pub-id pub-id-type="pmid">19359356</pub-id>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btp251</pub-id>
      </citation>
    </ref>
    <ref id="B12">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Quattoni</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Morency</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Demirdjian</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Hidden Conditional Random Fields for Gesture Recognition</article-title>
        <source>CVPR</source>
        <year>2006</year>
        <volume>II</volume>
        <fpage>1521</fpage>
        <lpage>1527</lpage>
      </citation>
    </ref>
    <ref id="B13">
      <citation citation-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>McCallum</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bellare</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Pereira</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>A Conditional Random Field for Discriminatively-trained Finite-state String Edit Distance</article-title>
        <source>Proceedings of the 21th Annual Conference on Uncertainty in Artificial Intelligence (UAI-05)</source>
        <year>2005</year>
        <volume>388</volume>
        <publisher-name>Arlington, Virginia: AUAI Press</publisher-name>
      </citation>
    </ref>
    <ref id="B14">
      <citation citation-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Quattoni</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Collins</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Darrell</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Saul LK, Weiss Y, Bottou L</surname>
          </name>
        </person-group>
        <article-title>Conditional Random Fields for Object Recognition</article-title>
        <source>Advances in Neural Information Processing Systems 17</source>
        <year>2005</year>
        <publisher-name>Cambridge, MA: MIT Press</publisher-name>
        <fpage>1097</fpage>
        <lpage>1104</lpage>
      </citation>
    </ref>
    <ref id="B15">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fariselli</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Martelli</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Casadio</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>A new decoding algorithm for hidden Markov models improves the prediction of the topology of all-beta membrane proteins</article-title>
        <source>BMC Bioinformatics</source>
        <year>2005</year>
        <volume>6</volume>
        <fpage>S12</fpage>
        <pub-id pub-id-type="pmid">16351738</pub-id>
        <pub-id pub-id-type="doi">10.1186/1471-2105-6-S4-S12</pub-id>
      </citation>
    </ref>
    <ref id="B16">
      <citation citation-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Sutton</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>McCallum</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <source>An Introduction to Conditional Random Fields for Relational Learning</source>
        <year>2006</year>
        <publisher-name>MIT Press</publisher-name>
      </citation>
    </ref>
    <ref id="B17">
      <citation citation-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Krogh</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Hidden Markov Models for Labeled Sequences</article-title>
        <source>Proceedings of the 12th IAPR ICPR'94</source>
        <year>1994</year>
        <publisher-name>IEEE Computer Society Press</publisher-name>
        <fpage>140</fpage>
        <lpage>144</lpage>
      </citation>
    </ref>
    <ref id="B18">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Martelli</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Fariselli</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Krogh</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Casadio</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>A sequence-profile-based HMM for predicting and discriminating beta barrel membrane proteins</article-title>
        <source>Bioinformatics</source>
        <year>2002</year>
        <volume>18</volume>
        <fpage>46</fpage>
        <lpage>53</lpage>
      </citation>
    </ref>
    <ref id="B19">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bigelow</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Petrey</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Przybylski</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Predicting transmembrane beta-barrels in proteomes</article-title>
        <source>Nucleic Acids Res</source>
        <year>2004</year>
        <volume>2566-2577</volume>
        <fpage>32</fpage>
      </citation>
    </ref>
    <ref id="B20">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bagos</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Liakopoulos</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Hamodrakas</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Evaluation of methods for predicting the topology of beta-barrel outer membrane proteins and a consensus prediction method</article-title>
        <source>BMC Bioinformatics</source>
        <year>2005</year>
        <volume>6</volume>
        <fpage>7</fpage>
        <lpage>20</lpage>
        <pub-id pub-id-type="pmid">15647112</pub-id>
        <pub-id pub-id-type="doi">10.1186/1471-2105-6-7</pub-id>
      </citation>
    </ref>
    <ref id="B21">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kabsch</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Dictionary of protein secondary structure: pattern recognition of hydrogen-bonded and geometrical features</article-title>
        <source>Biopolymers</source>
        <year>1983</year>
        <volume>22</volume>
        <fpage>2577</fpage>
        <lpage>2637</lpage>
        <pub-id pub-id-type="pmid">6667333</pub-id>
        <pub-id pub-id-type="doi">10.1002/bip.360221211</pub-id>
      </citation>
    </ref>
    <ref id="B22">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sutton</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>McCallum</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Rohanimanesh</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data</article-title>
        <source>J Mach Learn Res</source>
        <year>2007</year>
        <volume>8</volume>
        <fpage>693</fpage>
        <lpage>723</lpage>
      </citation>
    </ref>
  </ref-list>
</back>
<?DTDIdentifier.IdentifierValue article.dtd?>
<?DTDIdentifier.IdentifierType system?>
<?SourceDTD.DTDName article.dtd?>
<?SourceDTD.Version 1.0?>
<?ConverterInfo.XSLTName bmc2nlm.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Algorithms Mol Biol</journal-id>
    <journal-title>Algorithms for Molecular Biology : AMB</journal-title>
    <issn pub-type="epub">1748-7188</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">2776008</article-id>
    <article-id pub-id-type="publisher-id">1748-7188-4-13</article-id>
    <article-id pub-id-type="pmid">19849839</article-id>
    <article-id pub-id-type="doi">10.1186/1748-7188-4-13</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Grammatical-Restrained Hidden Conditional Random Fields for Bioinformatics applications</article-title>
    </title-group>
    <contrib-group>
      <contrib id="A1" corresp="yes" contrib-type="author">
        <name>
          <surname>Fariselli</surname>
          <given-names>Piero</given-names>
        </name>
        <xref ref-type="aff" rid="I1">1</xref>
        <email>piero.fariselli@unibo.it</email>
      </contrib>
      <contrib id="A2" contrib-type="author">
        <name>
          <surname>Savojardo</surname>
          <given-names>Castrense</given-names>
        </name>
        <xref ref-type="aff" rid="I1">1</xref>
        <email>savojard@biocomp.unibo.it</email>
      </contrib>
      <contrib id="A3" contrib-type="author">
        <name>
          <surname>Martelli</surname>
          <given-names>Pier Luigi</given-names>
        </name>
        <xref ref-type="aff" rid="I1">1</xref>
        <email>gigi@biocomp.unibo.it</email>
      </contrib>
      <contrib id="A4" contrib-type="author">
        <name>
          <surname>Casadio</surname>
          <given-names>Rita</given-names>
        </name>
        <xref ref-type="aff" rid="I1">1</xref>
        <email>casadio@biocomp.unibo.it</email>
      </contrib>
    </contrib-group>
    <aff id="I1"><label>1</label>Biocomputing Group, University of Bologna, via Irnerio 42, 40126 Bologna, Italy</aff>
    <pub-date pub-type="collection">
      <year>2009</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>22</day>
      <month>10</month>
      <year>2009</year>
    </pub-date>
    <volume>4</volume>
    <fpage>13</fpage>
    <lpage>13</lpage>
    <ext-link ext-link-type="uri" xlink:href="http://www.almob.org/content/4/1/13"/>
    <history>
      <date date-type="received">
        <day>12</day>
        <month>6</month>
        <year>2009</year>
      </date>
      <date date-type="accepted">
        <day>22</day>
        <month>10</month>
        <year>2009</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright Â© 2009 Fariselli et al; licensee BioMed Central Ltd.</copyright-statement>
      <copyright-year>2009</copyright-year>
      <copyright-holder>Fariselli et al; licensee BioMed Central Ltd.</copyright-holder>
      <license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0">
        <p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0"/>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</p>
        <!--<rdf xmlns="http://web.resource.org/cc/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:dcterms="http://purl.org/dc/terms"><Work xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:dcterms="http://purl.org/dc/terms/" rdf:about=""><license rdf:resource="http://creativecommons.org/licenses/by/2.0"/><dc:type rdf:resource="http://purl.org/dc/dcmitype/Text"/><dc:author>
               Fariselli
               Piero
               
               piero.fariselli@unibo.it
            </dc:author><dc:title>
            Grammatical-Restrained Hidden Conditional Random Fields for Bioinformatics applications
         </dc:title><dc:date>2009</dc:date><dcterms:bibliographicCitation>Algorithms for Molecular Biology 4(1): 13-. (2009)</dcterms:bibliographicCitation><dc:identifier type="sici">1748-7188(2009)4:1&#x0003c;13&#x0003e;</dc:identifier><dcterms:isPartOf>urn:ISSN:1748-7188</dcterms:isPartOf><License rdf:about="http://creativecommons.org/licenses/by/2.0"><permits rdf:resource="http://web.resource.org/cc/Reproduction" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/Distribution" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Notice" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Attribution" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/DerivativeWorks" xmlns=""/></License></Work></rdf>-->
      </license>
    </permissions>
    <abstract>
      <sec>
        <title>Background</title>
        <p>Discriminative models are designed to naturally address classification tasks. However, some applications require the inclusion of grammar rules, and in these cases generative models, such as Hidden Markov Models (HMMs) and Stochastic Grammars, are routinely applied.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>We introduce Grammatical-Restrained Hidden Conditional Random Fields (GRHCRFs) as an extension of Hidden Conditional Random Fields (HCRFs). GRHCRFs while preserving the discriminative character of HCRFs, can assign labels in agreement with the production rules of a defined grammar. The main GRHCRF novelty is the possibility of including in HCRFs prior knowledge of the problem by means of a defined grammar. Our current implementation allows <italic>regular grammar </italic>rules. We test our GRHCRF on a typical biosequence labeling problem: the prediction of the topology of Prokaryotic outer-membrane proteins.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p>We show that in a typical biosequence labeling problem the GRHCRF performs better than CRF models of the same complexity, indicating that GRHCRFs can be useful tools for biosequence analysis applications.</p>
      </sec>
      <sec>
        <title>Availability</title>
        <p>GRHCRF software is available under GPLv3 licence at the website</p>
        <p>
          <ext-link ext-link-type="uri" xlink:href="http://www.biocomp.unibo.it/~savojard/biocrf-0.9.tar.gz."/>
        </p>
      </sec>
    </abstract>
  </article-meta>
</front>
<body>
  <sec>
    <title>Background</title>
    <p>Sequence labeling is a general task addressed in many different scientific fields, including Bioinformatics and Computational Linguistics [<xref ref-type="bibr" rid="B1">1</xref>-<xref ref-type="bibr" rid="B3">3</xref>]. Recently Conditional Random Fields (CRFs) have been introduced as a new promising framework to solve sequence labeling problems [<xref ref-type="bibr" rid="B4">4</xref>]. CRFs offer several advantages over Hidden Markov Models (HMMs), including the ability of relaxing strong independence assumptions made in HMMs [<xref ref-type="bibr" rid="B4">4</xref>]. CRFs have been successfully applied in biosequence analysis and structural predictions [<xref ref-type="bibr" rid="B5">5</xref>-<xref ref-type="bibr" rid="B11">11</xref>]. However, several problems of sequence analysis can be successfully addressed only by designing a grammar in order to provide meaningful results. For instance in gene prediction tasks exons must be linked in such a way that the donor and acceptor junctions define regions whose length is multiple of three (according to the genetic code), and in protein structure prediction, helical segments shorter than 4 residues should be consider meaningless, being this the shortest allowed length for a protein helical motif [<xref ref-type="bibr" rid="B1">1</xref>,<xref ref-type="bibr" rid="B2">2</xref>]. In this kind of problems, the training sets generally consist of pairs of observed and label sequences and very often the number of the different labels representing the experimental evidence is small compared to the grammar requirements and the length distribution of the segments for the different labels. Then a direct mapping of one-label to one state results in poor predictive performances and HMMs trained for these applications routinely separate labels from state names. The separation of state names and labels allows to model a huge number of concurring paths compatible with the grammar and with the experimental labels without increasing the time and space computational complexity [<xref ref-type="bibr" rid="B1">1</xref>].</p>
    <p>In analogy with the HMM approach, in this paper we develop a discriminative model that incorporates regular-grammar production rules with the aim of integrating the different capabilities of generative and discriminative models. In order to model labels and states disjointly, the regular grammar has to be included in the structure of a Hidden Conditional Random Field (HCRF) [<xref ref-type="bibr" rid="B12">12</xref>-<xref ref-type="bibr" rid="B14">14</xref>]. Previously, McCallum et al. [<xref ref-type="bibr" rid="B13">13</xref>] introduced a special HCRF that exploits a specific automaton to align sequences.</p>
    <p>The model here introduced as Grammatical-Restrained Hidden Conditional Random Field (GRHCRF), separates the states from the labels and restricts the accepted predictions only to those allowed by a predefined grammar. By this, it is possible to cast into the model prior knowledge of the problem at hand, that may not be captured directly from the learning associations and ensures that only meaningful solutions are provided.</p>
    <p>In principle CRFs can directly model the same GRHCRF grammar. However, given the fully-observable nature of the CRFs [<xref ref-type="bibr" rid="B12">12</xref>], the observed sequences must be re-labelled to obtain a bijection between states and labels. This implies that only one specific and <italic>unique </italic>state path for each observed sequence must be selected. On the contrary with GRHCRFs that allow the separation between labels and states, an arbitrary large number of different state paths, corresponding to the same experimentally observed labels, can be counted at the same time. In order to fully exploit this path degeneration in the prediction phase, the decoding algorithm must take into account all possible paths, and the posterior-Viterbi (instead of the Viterbi) should be adopted [<xref ref-type="bibr" rid="B15">15</xref>].</p>
    <p>In this paper we define the model as an extension of a HCRF, we provide the basic inference equations and we introduce a new decoding algorithm for CRF models. We then compare the new GRHCRF with CRFs of the same complexity on a Bioinformatics task whose solution must comply with a given grammar: the prediction of the topological models of Prokaryotic outer membrane proteins. We show that in this task the GRHCRF performance is higher than to those achieved by CRF and HMM models of the same complexity.</p>
  </sec>
  <sec sec-type="methods">
    <title>Methods</title>
    <p>In what follows <bold>x </bold>is the random variable over the data sequences to be labeled, <bold>y </bold>is the random variable over the corresponding label sequences and <bold>s</bold> is the random variable over the hidden states. We use an upper-script index when we deal with multiple sequences. The problem that we want to model is then described by the observed sequences <bold>x</bold><sup>(<italic>i</italic>)</sup>, by the labels <bold>y</bold><sup>(<italic>i</italic>) </sup>and by the underlying grammar <italic>G </italic>that is specified by its production rules with respect to the set of the hidden states. Although it is possible to imagine more complex models, in what follows we restrict each state to have only one possible associated label. Thus we define a function that maps each hidden state to a given label as:</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i1.gif"/>
      </disp-formula>
    </p>
    <p>The difference between the CRF and GRHCRF (or HCRF) models can be seen in Figure <xref ref-type="fig" rid="F1">1</xref>, where their graphical structure is presented. GRHCRF and HCRF are indistinguishable from their graphical structure representation since it depicts only the conditional dependence among the random variables. Since the number of the states |{<bold>s</bold>}| is always greater than the number of possible labels |{<bold>y</bold>}| the GRHCRFs (HCRFs) have more expressive power than the corresponding CRFs.</p>
    <fig position="float" id="F1">
      <label>Figure 1</label>
      <caption>
        <p><bold>Graphical structure of a linear-CRF (left) and a linear GRHCRF/HCRF (right)</bold>.</p>
      </caption>
      <graphic xlink:href="1748-7188-4-13-1"/>
    </fig>
    <p>We further restrict our model to linear HCRF, so that the computational complexity of the inference algorithms remains linear with respect to the sequence length. This choice implies that the embedded grammar will be <italic>regular</italic>. Our implementation and tests are based on first order HCRFs with explicit transition functions (<italic>t</italic><sub><italic>k</italic></sub>(<italic>s</italic><sub><italic>j</italic>-1</sub>, <italic>s</italic><sub><italic>j</italic></sub>, <bold>x</bold>)) and state functions (<italic>g</italic><sub><italic>k</italic></sub>(<italic>s</italic><sub><italic>j</italic></sub>, <bold>x</bold>)) unrolled over each sequence position <italic>j</italic>.</p>
    <p>However, for sake of clarity in the following we use the compact notation:</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i2.gif"/>
      </disp-formula>
    </p>
    <p>where <italic>f</italic><sub><italic>k</italic></sub>(<italic>s</italic><sub><italic>j</italic>-1</sub>, <italic>s</italic><sub><italic>j</italic></sub>, <bold>x</bold>) can be either a transition feature function <italic>t</italic><sub><italic>l</italic></sub>(<italic>s</italic><sub><italic>j</italic>-1</sub>, <italic>s</italic><sub><italic>j</italic></sub>, <bold>x</bold>) or a state feature function <italic>gn</italic>(<italic>s</italic><sub><italic>j</italic></sub>, <bold>x</bold>). Following the usual notation [<xref ref-type="bibr" rid="B16">16</xref>] we extend the local functions to include the hidden states as</p>
    <p>
      <disp-formula id="bmcM1">
        <label>(1)</label>
        <graphic xlink:href="1748-7188-4-13-i3.gif"/>
      </disp-formula>
    </p>
    <p>and we set the two constraints as:</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i4.gif"/>
      </disp-formula>
    </p>
    <p>With this choice, the local function Ï<sub><italic>j</italic></sub>(<bold>s</bold>, <bold>y</bold>, <bold>x</bold>) becomes zero when the labeling (Î©(<italic>s</italic><sub><italic>j</italic></sub>, <italic>y</italic><sub><italic>j</italic></sub>)) or the grammar production rules (Î(<italic>s</italic>, <italic>s</italic>')) are not allowed. In turn this sets to zero the corresponding probabilities. As in the case of the HCRF, for the whole sequence we define Î¨(<bold>s</bold>, <bold>y</bold>, <bold>x</bold>) = Î <sub><italic>j</italic></sub>Ï<sub><italic>j</italic></sub>(<bold>s</bold>, <bold>y</bold>, <bold>x</bold>) and the normalization factors (or partition functions) can be obtained summing over all possible sequences of hidden states (or latent variables):</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i5.gif"/>
      </disp-formula>
    </p>
    <p>or summing over all possible sequences of labels and hidden states:</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i6.gif"/>
      </disp-formula>
    </p>
    <p>Using the normalization factors the joint probability of a label sequence <bold>y </bold>and an hidden state sequence <bold>s </bold>given an observation sequence <bold>x </bold>is:</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i7.gif"/>
      </disp-formula>
    </p>
    <p>The probability of an hidden state sequence given a label sequence and an observation sequence is:</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i8.gif"/>
      </disp-formula>
    </p>
    <p>Finally, the probability of a label sequence given an observation sequence can be computed as follows:</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i9.gif"/>
      </disp-formula>
    </p>
    <sec>
      <title>Parameter estimation</title>
      <p>The model parameters (Î¸) can be obtained by maximizing the log-likelihood of the data:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i10.gif"/>
        </disp-formula>
      </p>
      <p>where the different sequences are supposed to be independent and identically distributed random variables.</p>
      <p>Taking the first derivative with respect to parameter Î»<sub><italic>k </italic></sub>of the objective function we obtain:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i11.gif"/>
        </disp-formula>
      </p>
      <p>where, in analogy with the Boltzmann machines and HMMs for labelled sequences [<xref ref-type="bibr" rid="B17">17</xref>], <inline-formula><inline-graphic xlink:href="1748-7188-4-13-i12.gif"/></inline-formula> and â± an be seen as <italic>clamped </italic>and <italic>free </italic>phases. After simple computations we can rewrite the derivative as:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i13.gif"/>
        </disp-formula>
      </p>
      <p>where the <italic>E</italic><sub><italic>p</italic>(<bold>s</bold>|<bold>y</bold>, <bold>x</bold>) </sub>[<italic>f</italic><sub><italic>k</italic></sub>] and <italic>E</italic><sub><italic>p</italic>(<bold>s</bold>, <bold>y</bold>|<bold>x</bold>) </sub>[<italic>f</italic><sub><italic>k</italic></sub>] are the expected values of the feature function <italic>f</italic><sub><italic>k </italic></sub>computed in the clamped and free phases, respectively. Differently from the standard CRF, both expectations have to be computed using the Forward and Backward algorithms. These algorithms must take into consideration the grammar restraints.</p>
      <p>To avoid overfitting, we regularize the objective function using a Gaussian prior, so that the function to maximize has the form of:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i14.gif"/>
        </disp-formula>
      </p>
      <p>and the corresponding gradient is:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i15.gif"/>
        </disp-formula>
      </p>
      <p>Alternatively, the Expectation Maximization procedure can be adopted [<xref ref-type="bibr" rid="B16">16</xref>].</p>
    </sec>
    <sec>
      <title>Computing the expectations</title>
      <p>The partition functions and the expectations can be computed using the dynamic programming by defining the so called forward and backward algorithms [<xref ref-type="bibr" rid="B1">1</xref>,<xref ref-type="bibr" rid="B2">2</xref>,<xref ref-type="bibr" rid="B4">4</xref>]. For the clamped phase the forward algorithm is:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i16.gif"/>
        </disp-formula>
      </p>
      <p>where the clamped phase matrix <italic>M</italic><sub><italic>C </italic></sub>takes into account both the grammar constraint (Î(<italic>s</italic>', <italic>s</italic>)) and the current given labeling <bold>y </bold><inline-formula><inline-graphic xlink:href="1748-7188-4-13-i17.gif"/></inline-formula>.</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i18.gif"/>
        </disp-formula>
      </p>
      <p>The forward algorithm for the free phase is computed as:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i19.gif"/>
        </disp-formula>
      </p>
      <p>where the free phase matrix <italic>M</italic><sub><italic>F </italic></sub>is defined as:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i20.gif"/>
        </disp-formula>
      </p>
      <p>It should be noted that also in the free phase the algorithm has to take into account the grammar production rules Î(<italic>s</italic>', <italic>s</italic>) and only the paths that are in agreement with the grammar are counted. Analogously, the backward algorithms can be computed for the clamped phase as:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i21.gif"/>
        </disp-formula>
      </p>
      <p>where <italic>L</italic><sup>(<italic>i</italic>) </sup>is the length of the <italic>i</italic><sup><italic>th </italic></sup>protein. For the free phase we have:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i22.gif"/>
        </disp-formula>
      </p>
      <p>The expectations of the feature functions (<italic>E</italic><sub><italic>p</italic>(<bold>s</bold>|<bold>y</bold>, <bold>x</bold>) </sub>[<italic>f</italic><sub><italic>k</italic></sub>], <italic>E</italic><sub><italic>p</italic>(<bold>s</bold>, <bold>y</bold>|<bold>x</bold>) </sub>[<italic>f</italic><sub><italic>k</italic></sub>]) are computed as:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i23.gif"/>
        </disp-formula>
      </p>
      <p>The partition functions can be computed using both forward or backward algorithms as:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i24.gif"/>
        </disp-formula>
      </p>
      <p>where for simplicity we dropped out the sequence upper-script ((<italic>i</italic>)).</p>
    </sec>
    <sec>
      <title>Decoding</title>
      <p>Decoding is the task of assigning labels (<bold>y</bold>) to an unknown observation sequence <bold>x</bold>. Viterbi algorithm is routinely applied as decoding for the CRFs, since it finds the most probable path of an observation sequence given a CRF model [<xref ref-type="bibr" rid="B4">4</xref>]. Viterbi algorithm is particular effective when there is a single strong highly probable path, while when several paths compete (have similar probabilities), <italic>posterior decoding </italic>may perform significantly better. However, the selected state path of the posterior decoding may not be allowed by the grammar. A simple solution of this problem is provided by the posterior-Viterbi decoding, that was previously introduced for HMMs [<xref ref-type="bibr" rid="B15">15</xref>]. Posterior-Viterbi, exploits the posterior probabilities and at the same time preserves the grammatical constraint. This algorithm consists of three steps:</p>
      <p>â¢ for each position <italic>j </italic>and state <italic>s </italic>â <inline-formula><inline-graphic xlink:href="1748-7188-4-13-i25.gif"/></inline-formula>, compute posterior probability <italic>p</italic>(<italic>s</italic><sub><italic>j </italic></sub>= <italic>s</italic>|<bold>x</bold>)</p>
      <p>â¢ find the allowed state path</p>
      <p>S* = argmax<sub><bold>s </bold></sub>Î <sub><italic>j </italic></sub><italic>p</italic>(<italic>s</italic><sub><italic>j </italic></sub>= <bold>s</bold>|<bold>x</bold>)</p>
      <p>â¢ assig to <bold>x </bold>a label sequence <bold>y </bold>so that <italic>y</italic><sub><italic>j </italic></sub>= Î(<italic>s</italic><sub><italic>j</italic></sub>) for each position <italic>j</italic></p>
      <p>The first step can be accomplished using the Forward-Backward algorithm as described for the free phase of parameter estimation. In order to find the best allowed state path, a Viterbi search is performed over posterior probabilities. In what follows <italic>Ï</italic><sub><italic>j</italic></sub>(<italic>s</italic>|<bold>x</bold>) is the most probable allowed path of length <italic>j </italic>ending in state <italic>s </italic>and <italic>Ï</italic><sub><italic>j</italic></sub>(<italic>s</italic>) is a traceback pointer. The algorithm can be described as follows:</p>
      <p>1. Initialization:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i26.gif"/>
        </disp-formula>
      </p>
      <p>2. Recursion</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i27.gif"/>
        </disp-formula>
      </p>
      <p>3. Termination and Traceback</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i28.gif"/>
        </disp-formula>
      </p>
      <p>The labels are assigned to the observed sequence according to the state path <bold>s</bold>*. It is also possible to consider a slightly modified version of the algorithm where, for each position, the posterior probability of the <italic>labels </italic>is considered, and the states with the same label have associated the same posterior probability. The rationale behind this is to consider the aggregate probability of all state paths corresponding to the same sequence of labels to improve the overall per label accuracy. In many applications this variant of the algorithm might perform better.</p>
    </sec>
    <sec>
      <title>Implementation</title>
      <p>We implemented the GRHCRF as linear HCRF in C++ language. Our GRHCRF can deal with sequences of symbols as well as sequence profiles. A <italic>sequence profile </italic>of a protein <italic>p </italic>is a matrix <italic>X </italic>whose rows represent the sequence positions and whose columns are the 20 possible amino acids. Each element <italic>X </italic>[<italic>i</italic>] [<italic>a</italic>] of the sequence profile represents the frequency of the residue type <italic>a </italic>in the aligned position <italic>i</italic>. The profile positions are normalized such as Î£<sub><italic>a</italic></sub><italic>X</italic>[<italic>i</italic>][<italic>a</italic>] = 1 (for each <italic>i</italic>).</p>
      <p>In order to take into account the information of the neighboring residues we define a symmetric sliding window of length <italic>w </italic>centered into the <italic>i</italic>-th residue. With this choice the state feature functions are defined as:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i29.gif"/>
        </disp-formula>
      </p>
      <p>where <italic>s </italic>runs over all possible states, <italic>a </italic>runs over the different observed symbols <italic>A </italic>(in our case the 20 residues) and <italic>k </italic>runs over the neighbor residues (from - <inline-formula><inline-graphic xlink:href="1748-7188-4-13-i30.gif"/></inline-formula> to <inline-formula><inline-graphic xlink:href="1748-7188-4-13-i31.gif"/></inline-formula>). When dealing with single sequences, the state functions are simply products of Kronecker's deltas:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i32.gif"/>
        </disp-formula>
      </p>
      <p>while in the case of sequence profiles, the state features are real-valued and assume the profile scores:</p>
      <p>
        <disp-formula>
          <graphic xlink:href="1748-7188-4-13-i33.gif"/>
        </disp-formula>
      </p>
    </sec>
  </sec>
  <sec>
    <title>Measures of performance</title>
    <p>To evaluate the accuracy we define the classical label-based indices, such as:</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i34.gif"/>
      </disp-formula>
    </p>
    <p>where <italic>p </italic>and <italic>N </italic>are the total number of correct predictions and total number of examples, respectively. The Matthews correlation coefficient (C) for a given class s is defined as:</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i35.gif"/>
      </disp-formula>
    </p>
    <p><italic>p</italic>(<italic>s</italic>) and n(<italic>s</italic>) are respectively the true positive and true negative predictions for class <italic>s</italic>, while <italic>o</italic>(<italic>s</italic>) and <italic>u</italic>(<italic>s</italic>) are the numbers of false positives and false negatives with respect to that class. The sensitivity (coverage, <italic>Sn</italic>) for each class <italic>s </italic>is defined as</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i36.gif"/>
      </disp-formula>
    </p>
    <p>The specificity (accuracy, <italic>Sp</italic>) is the probability of correct predictions and it is defined as follows:</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i37.gif"/>
      </disp-formula>
    </p>
    <p>However, these measures cannot discriminate between similar and dissimilar segment distributions and do not provide any clues about the number of proteins that are correctly predicted. For this reason we introduce a <italic>protein-based </italic>index, the Protein OVerlap (POV) measure. We consider a protein prediction to be correct only if the number of predicted and observed transmembrane segments (in the structurally resolved proteins, see Outer-membrane protein data set section) is the same and if all corresponding pairs have a minimum segment overlap. POV is a binary measure (0 or 1) and for a given protein sequence <italic>s </italic>is defined as:</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i38.gif"/>
      </disp-formula>
    </p>
    <p>Where <inline-formula><inline-graphic xlink:href="1748-7188-4-13-i39.gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="1748-7188-4-13-i40.gif"/></inline-formula> are the numbers of predicted and observed segments, while <italic>p</italic><sub><italic>i </italic></sub>and <italic>o</italic><sub><italic>i </italic></sub>are the <italic>i</italic><sup><italic>th </italic></sup>predicted and observed segments, respectively. The threshold Î¸ is defined as the mean of the half lengths of the segments:</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i41.gif"/>
      </disp-formula>
    </p>
    <p>where <italic>L</italic><sub><italic>p</italic></sub>(= |<italic>p</italic><sub><italic>i</italic></sub>|) and <italic>L</italic><sub><italic>o</italic></sub>(= |<italic>o</italic><sub><italic>i</italic></sub>|) are the lengths of the predicted and observed segments, respectively. For a set of proteins the average of all POVs over the total number of proteins <italic>N </italic>is:</p>
    <p>
      <disp-formula>
        <graphic xlink:href="1748-7188-4-13-i42.gif"/>
      </disp-formula>
    </p>
    <p>To evaluate the average standard deviation of our predictions, we performed a bootstrapping procedure with 100 runs over 60% of the predicted data sets.</p>
  </sec>
  <sec>
    <title>Results and Discussion</title>
    <sec>
      <title>Problem definition</title>
      <p>The prediction of the topology of the outer membrane proteins in Prokaryote organisms is a challenging task that was addressed several times given its biological relevance [<xref ref-type="bibr" rid="B18">18</xref>-<xref ref-type="bibr" rid="B20">20</xref>]. The problem can be defined as: given a protein sequence that is known to be inserted in the outer membrane of a Prokaryotic cell, we want to predict the number and the location with respect to the membrane plane of the membrane-spanning segments. From experimental results, we know that the outer membrane of Prokaryotes imposes some constraints to the topological models such as:</p>
      <p>â¢ both C and N termini of the protein chain lie in the periplasmic side of the cell (inside) and this implies that the number of the spanning segments is even;</p>
      <p>â¢ membrane spanning segments have a minimal segment length (â¥ 3 residues);</p>
      <p>â¢ the transmembrane-segment lengths are distributed accordingly to a probability density distribution that can be experimentally determined and must be taken into account.</p>
      <p>For the reasons listed above the best performing predictors described in literature are based on HMMs and among them the best performing single-method in the task of the topology prediction is HMM-B2TMR [<xref ref-type="bibr" rid="B18">18</xref>] (see Table <xref ref-type="table" rid="T1">1</xref> in [<xref ref-type="bibr" rid="B20">20</xref>]).</p>
      <table-wrap position="float" id="T1">
        <label>Table 1</label>
        <caption>
          <p>Prediction of the topology of the Prokaryotic outer membrane proteins.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <td align="left">
                <bold>Method</bold>
              </td>
              <td align="center">
                <bold>POV</bold>
              </td>
              <td align="center">
                <bold>Q2</bold>
              </td>
              <td align="center">
                <bold>C(t)</bold>
              </td>
              <td align="center">
                <bold>Sn(t)</bold>
              </td>
              <td align="center">
                <bold>Sp(t)</bold>
              </td>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left">CRF-1 (Vit)</td>
              <td align="center">0.26 Â± 0.05</td>
              <td align="center">0.72 Â± 0.01</td>
              <td align="center">0.47 Â± 0.02</td>
              <td align="center">0.59 Â± 0.01</td>
              <td align="center">0.80 Â± 0.01</td>
            </tr>
            <tr>
              <td align="left">CRF-1 (Pvit)</td>
              <td align="center">0.39 Â± 0.05</td>
              <td align="center">0.77 Â± 0.01</td>
              <td align="center">0.54 Â± 0.02</td>
              <td align="center">0.71 Â± 0.01</td>
              <td align="center">0.80 Â± 0.01</td>
            </tr>
            <tr>
              <td/>
              <td/>
              <td/>
              <td/>
              <td/>
              <td/>
            </tr>
            <tr>
              <td align="left">CRF-2 (Vit)</td>
              <td align="center">0.34 Â± 0.05</td>
              <td align="center">0.76 Â± 0.01</td>
              <td align="center">0.52 Â± 0.03</td>
              <td align="center">0.63 Â± 0.02</td>
              <td align="center">0.82 Â± 0.02</td>
            </tr>
            <tr>
              <td align="left">CRF-2 (Pvit)</td>
              <td align="center">0.47 Â± 0.05</td>
              <td align="center">0.80 Â± 0.01</td>
              <td align="center">0.60 Â± 0.03</td>
              <td align="center">0.74 Â± 0.02</td>
              <td align="center">0.82 Â± 0.02</td>
            </tr>
            <tr>
              <td/>
              <td/>
              <td/>
              <td/>
              <td/>
              <td/>
            </tr>
            <tr>
              <td align="left">CRF-3 (Vit)</td>
              <td align="center">0.29 Â± 0.04</td>
              <td align="center">0.72 Â± 0.01</td>
              <td align="center">0.45 Â± 0.02</td>
              <td align="center">0.60 Â± 0.02</td>
              <td align="center">0.79 Â± 0.01</td>
            </tr>
            <tr>
              <td align="left">CRF-3 (Pvit)</td>
              <td align="center">0.45 Â± 0.04</td>
              <td align="center">0.76 Â± 0.01</td>
              <td align="center">0.52 Â± 0.02</td>
              <td align="center">0.70 Â± 0.02</td>
              <td align="center">0.79 Â± 0.01</td>
            </tr>
            <tr>
              <td/>
              <td/>
              <td/>
              <td/>
              <td/>
              <td/>
            </tr>
            <tr>
              <td align="left">GRHCRF</td>
              <td align="center">0.66 Â± 0.04</td>
              <td align="center">0.85 Â± 0.01</td>
              <td align="center">0.70 Â± 0.03</td>
              <td align="center">0.83 Â± 0.01</td>
              <td align="center">0.84 Â± 0.01</td>
            </tr>
            <tr>
              <td/>
              <td/>
              <td/>
              <td/>
              <td/>
              <td/>
            </tr>
            <tr>
              <td align="left">HMM-B2TMR</td>
              <td align="center">0.58 Â± 0.04</td>
              <td align="center">0.80 Â± 0.01</td>
              <td align="center">0.62 Â± 0.02</td>
              <td align="center">0.82 Â± 0.02</td>
              <td align="center">0.83 Â± 0.01</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p>C(t), Sn(t) and Sp(t) are reported for the transmembrane segments (t).</p>
          <p>Vit = Viterbi decoding, Pvit = posterior-Viterbi decoding.</p>
          <p>For GRHCRF and HMM-B2TMR we used the posterior-Viterbi decoding.</p>
          <p>Models are detailed in the text. Scoring indices are described in Measure of Accuracy section.</p>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>Outer-membrane protein data set</title>
      <p>The training set consists of 38 high-resolution experimentally determined outer-membrane proteins of Prokaryotes, whose sequence identity between each pair is less than 40%. We then generated 19 subsets for the cross-validation experiments, such as there is no sequence identity greater than 25% and no functional similarity between two elements belonging to disjoint sets. The annotation consists of three different labelings that correspond to: <italic>inner loop </italic>(i), <italic>outer loop </italic>(o) and <italic>transmembrane </italic>(t). This assignment was obtained using the DSSP program [<xref ref-type="bibr" rid="B21">21</xref>] by selecting the <italic>Î²</italic>-strands that span the outer membrane. The dataset with the annotations and the cross-validation sets are available with the program at <ext-link ext-link-type="uri" xlink:href="http://www.biocomp.unibo.it/~savojard/biocrf-0.9.tar.gz"/>.</p>
      <p>For each protein in the dataset, a profile based on a multiple sequence alignment was created using the PSI-BLAST program on the non-redundant dataset of sequences (uniref90 as described in <ext-link ext-link-type="uri" xlink:href="http://www.uniprot.org/help/uniref"/>). PSI-BLAST runs were performed using a fixed number of cycles set to 3 and an e-value of 0.001.</p>
    </sec>
    <sec>
      <title>Prediction of the topology of Prokaryotic outer membrane proteins</title>
      <p>The topology of outer-membrane proteins in Prokaryotes can be described assigning each residue to one of three types: inner loop (<italic>i</italic>), transmembrane <italic>Î²</italic>-strand (<italic>t</italic>), outer loop (<italic>o</italic>). These three types are defined according to the experimental evidence and are the <italic>terminal </italic>symbols of the grammar. The chemico-physical and geometrical characteristics of the three types of segments as deduced by the available structures in the PDB suggest how to build a grammar (or the corresponding automaton) for the prediction of the topology. We performed our experiments using the automaton depicted in Figure <xref ref-type="fig" rid="F2">2</xref>, which was previously introduced to model our HMM-B2TMR [<xref ref-type="bibr" rid="B18">18</xref>] (this automaton is substantially similar to all other HMMs used for this task [<xref ref-type="bibr" rid="B19">19</xref>,<xref ref-type="bibr" rid="B20">20</xref>]). It is essentially based on three different types of states. The states of the automaton are the <italic>non-terminal </italic>symbols of the regular grammar and the arrows represent the allowed transitions (or production rules). The states represented with squares describe the transmembrane strands while the states shown with circles represent the loops (Figure <xref ref-type="fig" rid="F2">2</xref>). A statistics on the non-redundant database of outer membrane proteins presently available, indicates that the length of the strands of the training set ranges from 3 to 22 residues (with an average length of 12 residues). In Prokaryotic outer membrane proteins the inner loops are generally shorter than outer loops. Furthermore, both the N-terminus and C-terminus of all the proteins lie in the inner side of the membrane [<xref ref-type="bibr" rid="B18">18</xref>]. These constraints are modelled by means of the allowed transitions between the states.</p>
      <fig position="float" id="F2">
        <label>Figure 2</label>
        <caption>
          <p><bold>Automaton structure designed for the prediction of the topology of the outer-membrane proteins in Prokaryotes with GRHCRFs and HMMs</bold>.</p>
        </caption>
        <graphic xlink:href="1748-7188-4-13-2"/>
      </fig>
      <p>The automaton described in Figure <xref ref-type="fig" rid="F2">2</xref> assigns labels to observed sequences that can be obtained using different state paths. This ambiguity leads to an ensemble of paths that must be taken into account during the likelihood maximization by summing up all possible trajectories compliant with the experimentally assigned labels (see Method section).</p>
      <p>However, this ambiguity does not permit the adoption of the automaton of Figure <xref ref-type="fig" rid="F2">2</xref> for CRF learning, since to train CRFs a bijective mapping between states and labels is required. On the contrary, with the automaton of Figure <xref ref-type="fig" rid="F2">2</xref>, several different state paths can be obtained (in theory a factorial number) that are in agreement with the automaton and with the experimental labels.</p>
      <p>For this reason and for sake of comparison, we designed three other automata (Figure <xref ref-type="fig" rid="F3">3a, b</xref> and <xref ref-type="fig" rid="F3">3c</xref>) that have the same number of states but are non-ambiguous in term of state mapping. Then, starting from the experimentally derived labels, three different sets of re-labelled sequences can be derived to train CRFs (here referred as CRF1, CRF2 and CRF3).</p>
      <fig position="float" id="F3">
        <label>Figure 3</label>
        <caption>
          <p><bold>Three different non-ambigous automata derived from the one depicted in Figure 2</bold>. These automata are designed to have a bijective mapping between the states and the labels (after the corresponding re-labeling of the sequences). In the text they are referred as CRF1 (a), CRF2 (b) and CRF3 (c).</p>
        </caption>
        <graphic xlink:href="1748-7188-4-13-3"/>
      </fig>
      <p>All compared methods take as input sequence profile and are bench-marked as shown in Table <xref ref-type="table" rid="T1">1</xref>. In the case of non-ambiguous automata of the CRFs, we tested both the Viterbi and posterior-Viterbi algorithms since given the <italic>Viterbi-like </italic>learning of the CRFs it is not <italic>a priori </italic>predictable which one of the two decodings performs better on this particular task. From Table <xref ref-type="table" rid="T1">1</xref> it is clear that assigning the labels according to the posterior-Viterbi always leads to better performance than with the Viterbi (see CRF in Table <xref ref-type="table" rid="T1">1</xref>). This indicates that also in other tasks where CRFs are applied, the posterior-Viterbi here described can increase the overall decoding accuracy. Furthermore, the fact that both HMM-B2TMR and GRHCRF perform better than the others, implies that in the tasks where the observed labels may hide a more complex structure, as in the case of the prediction of the Prokaryotic outer membrane proteins, it is advantageous exploiting the ambiguity by taking into consideration multiple concurring paths at the same time, both during training and decoding (see Method section). Considering that underlying grammar is the same, the discriminative GRHCRF outperforms the generative model (HMM-B2TMR). This indicates that the GRHCRF can substitute the HMM-based models when the labeling prediction is the major issue. In order to asses the confidence level of our results, we computed pairwise t-tests between the GRHCRF and the other methods. From the t-test results reported in Table <xref ref-type="table" rid="T2">2</xref>, it is evident that the measures of the performace shown in Table <xref ref-type="table" rid="T1">1</xref> can be considered significant with a confidence level greater than 80% (see the most relevant index POV).</p>
      <table-wrap position="float" id="T2">
        <label>Table 2</label>
        <caption>
          <p>Confidence level of the results reported in Table 1.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <td align="left">
                <bold>Methods</bold>
              </td>
              <td align="center">
                <bold>POV</bold>
              </td>
              <td align="center">
                <bold>Q2</bold>
              </td>
              <td align="center">
                <bold>C(t)</bold>
              </td>
              <td align="center">
                <bold>Sn(t)</bold>
              </td>
              <td align="center">
                <bold>Sp(t)</bold>
              </td>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left">GRHCRF vs CRF-1</td>
              <td align="center">98.0%</td>
              <td align="center">99.5%</td>
              <td align="center">99.5%</td>
              <td align="center">99.8%</td>
              <td align="center">99.5%</td>
            </tr>
            <tr>
              <td align="left">GRHCRF vs CRF-2</td>
              <td align="center">96.0%</td>
              <td align="center">99.5%</td>
              <td align="center">99.5%</td>
              <td align="center">99.5%</td>
              <td align="center">99.5%</td>
            </tr>
            <tr>
              <td align="left">GRHCRF vs CRF-3</td>
              <td align="center">96.0%</td>
              <td align="center">99.5%</td>
              <td align="center">99.5%</td>
              <td align="center">99.5%</td>
              <td align="center">99.5%</td>
            </tr>
            <tr>
              <td align="left">GRHCRF vs HMM-B2TMR</td>
              <td align="center">80.0%</td>
              <td align="center">96.0%</td>
              <td align="center">99.0%</td>
              <td align="center">98.0%</td>
              <td align="center">99.5%</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p>The confidence level on the significance of the differences was computed with a t-test.</p>
          <p>For all methods we consider the best results of Table 1.</p>
        </table-wrap-foot>
      </table-wrap>
    </sec>
  </sec>
  <sec>
    <title>Conclusion</title>
    <p>In this paper we presented a new class of conditional random fields that assigns labels in agreement with production rules of a defined regular grammar. The main novelty of GRHCRF is then the introduction of an explicit regular grammar that defines the prior knowledge of the problem at hand, eliminating the need of relabelling the observed sequences. The GRHCRF predictions satisfy the grammar production rules by construction, so that only meaningful solutions are provided. In [<xref ref-type="bibr" rid="B13">13</xref>], an automaton was included to restrain the solution of a HCRFs. However in that case, it was hard-coded in the model in order to train finite-state string edit distance. On the contrary, GRHCRFs are designed to provide solutions in agreement with defined regular grammars that are provided as further input to the model. To the best of our knowledge, this is the first time that this is described. In principle, the grammar may be very complex, however, to maintain the tractability of the inference algorithm, we restrict our implementation to regular grammars. Extensions to context-free grammars can be designed by modifying the inference algorithms at the expense of the computational complexity of the final models. Since the Grammatical-Restrained HCRF can be seen as an extension of linear HCRF [<xref ref-type="bibr" rid="B13">13</xref>,<xref ref-type="bibr" rid="B14">14</xref>], the GRHCRF is also related to the models that deal with latent variables such as Dynamic CRFs [<xref ref-type="bibr" rid="B22">22</xref>].</p>
    <p>In this paper we also test the GRHCRFs on a real biological problem that require grammatical constraints: the prediction of the topology of Prokaryotic outer-membrane proteins. When applied to this biosequence analysis problem we show that GRHCRFs perform similarly or better than the corresponding CRFs and HMMs indicating that GRHCRFs can be profitably applied when a discriminative problem requires grammatical constraints.</p>
    <p>Finally we also present the posterior-Viterbi decoding algorithm for CRFs that was previously designed for HMMs and that can be of general interest and application, since in many cases posterior-Viterbi can perform significantly better than the classical Viterbi algorithm.</p>
  </sec>
  <sec>
    <title>Competing interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </sec>
  <sec>
    <title>Authors' contributions</title>
    <p>PF and CS formalized the GRHCRF model. CS wrote the GRHCRF code. CS and PF performed the experiments. PF, PLM and RC defined the problem and provided the data. CS, PF, PLM and RC authored the manuscript.</p>
  </sec>
</body>
<back>
  <ack>
    <sec>
      <title>Acknowledgements</title>
      <p>We thank MIUR for the PNR 2003 project (FIRB art.8) termed LIBI-Laboratorio Internazionale di BioInformatica delivered to R. Casadio. This work was also supported by the Biosapiens Network of Excellence project (a grant of the European Unions VI Framework Programme).</p>
    </sec>
  </ack>
  <ref-list>
    <ref id="B1">
      <citation citation-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Durbin</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <source>Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids</source>
        <year>1999</year>
        <publisher-name>Cambridge Univ Pr, reprint edition</publisher-name>
      </citation>
    </ref>
    <ref id="B2">
      <citation citation-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Baldi</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Brunak</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <source>Bioinformatics: The Machine Learning Approach</source>
        <year>2001</year>
        <edition>2</edition>
        <publisher-name>MIT Press</publisher-name>
      </citation>
    </ref>
    <ref id="B3">
      <citation citation-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Manning</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>SchÃ¼tze</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <source>Foundations of Statistical Natural Language Processing</source>
        <year>1999</year>
        <publisher-name>MIT Press</publisher-name>
      </citation>
    </ref>
    <ref id="B4">
      <citation citation-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Lafferty</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>McCallum</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Pereira</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data</article-title>
        <source>Proceedings of ICML01</source>
        <year>2001</year>
        <fpage>282</fpage>
        <lpage>289</lpage>
      </citation>
    </ref>
    <ref id="B5">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Carbonell</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Weigele</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Gopalakrishnan</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Protein fold recognition using segmentation conditional random fields (SCRFs)</article-title>
        <source>Journal of Computational Biology</source>
        <year>2006</year>
        <volume>13</volume>
        <fpage>394</fpage>
        <lpage>406</lpage>
        <pub-id pub-id-type="pmid">16597248</pub-id>
        <pub-id pub-id-type="doi">10.1089/cmb.2006.13.394</pub-id>
      </citation>
    </ref>
    <ref id="B6">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sato</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Sakakibara</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>RNA secondary structural alignment with conditional random fields</article-title>
        <source>Bioinformatics</source>
        <year>2005</year>
        <volume>21</volume>
        <fpage>237</fpage>
        <lpage>242</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bti1139</pub-id>
      </citation>
    </ref>
    <ref id="B7">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Sauer</surname>
            <given-names>UH</given-names>
          </name>
        </person-group>
        <article-title>OnD-CRF: predicting order and disorder in proteins conditional random fields</article-title>
        <source>Bioinformatics</source>
        <year>2008</year>
        <volume>24</volume>
        <fpage>1401</fpage>
        <lpage>1402</lpage>
        <pub-id pub-id-type="pmid">18430742</pub-id>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btn132</pub-id>
      </citation>
    </ref>
    <ref id="B8">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>CT</given-names>
          </name>
          <name>
            <surname>Yuan</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wilson</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>An unsupervised conditional random fields approach for clustering gene expression time series</article-title>
        <source>Bioinformatics</source>
        <year>2008</year>
        <volume>24</volume>
        <fpage>2467</fpage>
        <lpage>2473</lpage>
        <pub-id pub-id-type="pmid">18718949</pub-id>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btn375</pub-id>
      </citation>
    </ref>
    <ref id="B9">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>MH</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>XL</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Protein protein interaction site prediction based on conditional random fields</article-title>
        <source>Bioinformatics</source>
        <year>2007</year>
        <volume>23</volume>
        <fpage>597</fpage>
        <lpage>604</lpage>
        <pub-id pub-id-type="pmid">17234636</pub-id>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btl660</pub-id>
      </citation>
    </ref>
    <ref id="B10">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dang</surname>
            <given-names>TH</given-names>
          </name>
          <name>
            <surname>Van Leemput</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Verschoren</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Laukens</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Prediction of kinase-specific phosphorylation sites using conditional random fields</article-title>
        <source>Bioinformatics</source>
        <year>2008</year>
        <volume>24</volume>
        <fpage>2857</fpage>
        <lpage>2864</lpage>
        <pub-id pub-id-type="pmid">18940828</pub-id>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btn546</pub-id>
      </citation>
    </ref>
    <ref id="B11">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xia</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Su</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>MICAlign: a sequence-to-structure alignment tool integrating multiple sources of information in conditional random fields</article-title>
        <source>Bioinformatics</source>
        <year>2009</year>
        <volume>25</volume>
        <fpage>1433</fpage>
        <lpage>1434</lpage>
        <pub-id pub-id-type="pmid">19359356</pub-id>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btp251</pub-id>
      </citation>
    </ref>
    <ref id="B12">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Quattoni</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Morency</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Demirdjian</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Hidden Conditional Random Fields for Gesture Recognition</article-title>
        <source>CVPR</source>
        <year>2006</year>
        <volume>II</volume>
        <fpage>1521</fpage>
        <lpage>1527</lpage>
      </citation>
    </ref>
    <ref id="B13">
      <citation citation-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>McCallum</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bellare</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Pereira</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>A Conditional Random Field for Discriminatively-trained Finite-state String Edit Distance</article-title>
        <source>Proceedings of the 21th Annual Conference on Uncertainty in Artificial Intelligence (UAI-05)</source>
        <year>2005</year>
        <volume>388</volume>
        <publisher-name>Arlington, Virginia: AUAI Press</publisher-name>
      </citation>
    </ref>
    <ref id="B14">
      <citation citation-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Quattoni</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Collins</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Darrell</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Saul LK, Weiss Y, Bottou L</surname>
          </name>
        </person-group>
        <article-title>Conditional Random Fields for Object Recognition</article-title>
        <source>Advances in Neural Information Processing Systems 17</source>
        <year>2005</year>
        <publisher-name>Cambridge, MA: MIT Press</publisher-name>
        <fpage>1097</fpage>
        <lpage>1104</lpage>
      </citation>
    </ref>
    <ref id="B15">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fariselli</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Martelli</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Casadio</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>A new decoding algorithm for hidden Markov models improves the prediction of the topology of all-beta membrane proteins</article-title>
        <source>BMC Bioinformatics</source>
        <year>2005</year>
        <volume>6</volume>
        <fpage>S12</fpage>
        <pub-id pub-id-type="pmid">16351738</pub-id>
        <pub-id pub-id-type="doi">10.1186/1471-2105-6-S4-S12</pub-id>
      </citation>
    </ref>
    <ref id="B16">
      <citation citation-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Sutton</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>McCallum</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <source>An Introduction to Conditional Random Fields for Relational Learning</source>
        <year>2006</year>
        <publisher-name>MIT Press</publisher-name>
      </citation>
    </ref>
    <ref id="B17">
      <citation citation-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Krogh</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Hidden Markov Models for Labeled Sequences</article-title>
        <source>Proceedings of the 12th IAPR ICPR'94</source>
        <year>1994</year>
        <publisher-name>IEEE Computer Society Press</publisher-name>
        <fpage>140</fpage>
        <lpage>144</lpage>
      </citation>
    </ref>
    <ref id="B18">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Martelli</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Fariselli</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Krogh</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Casadio</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>A sequence-profile-based HMM for predicting and discriminating beta barrel membrane proteins</article-title>
        <source>Bioinformatics</source>
        <year>2002</year>
        <volume>18</volume>
        <fpage>46</fpage>
        <lpage>53</lpage>
      </citation>
    </ref>
    <ref id="B19">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bigelow</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Petrey</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Przybylski</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Predicting transmembrane beta-barrels in proteomes</article-title>
        <source>Nucleic Acids Res</source>
        <year>2004</year>
        <volume>2566-2577</volume>
        <fpage>32</fpage>
      </citation>
    </ref>
    <ref id="B20">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bagos</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Liakopoulos</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Hamodrakas</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Evaluation of methods for predicting the topology of beta-barrel outer membrane proteins and a consensus prediction method</article-title>
        <source>BMC Bioinformatics</source>
        <year>2005</year>
        <volume>6</volume>
        <fpage>7</fpage>
        <lpage>20</lpage>
        <pub-id pub-id-type="pmid">15647112</pub-id>
        <pub-id pub-id-type="doi">10.1186/1471-2105-6-7</pub-id>
      </citation>
    </ref>
    <ref id="B21">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kabsch</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Dictionary of protein secondary structure: pattern recognition of hydrogen-bonded and geometrical features</article-title>
        <source>Biopolymers</source>
        <year>1983</year>
        <volume>22</volume>
        <fpage>2577</fpage>
        <lpage>2637</lpage>
        <pub-id pub-id-type="pmid">6667333</pub-id>
        <pub-id pub-id-type="doi">10.1002/bip.360221211</pub-id>
      </citation>
    </ref>
    <ref id="B22">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sutton</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>McCallum</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Rohanimanesh</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data</article-title>
        <source>J Mach Learn Res</source>
        <year>2007</year>
        <volume>8</volume>
        <fpage>693</fpage>
        <lpage>723</lpage>
      </citation>
    </ref>
  </ref-list>
</back>
