<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8767664</article-id>
    <article-id pub-id-type="publisher-id">4570</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-022-04570-9</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>CellSeg: a robust, pre-trained nucleus segmentation and pixel quantification software for highly multiplexed fluorescence images</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Lee</surname>
          <given-names>Michael Y.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1599-8865</contrib-id>
        <name>
          <surname>Bedia</surname>
          <given-names>Jacob S.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2057-9182</contrib-id>
        <name>
          <surname>Bhate</surname>
          <given-names>Salil S.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9335-5414</contrib-id>
        <name>
          <surname>Barlow</surname>
          <given-names>Graham L.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0804-3342</contrib-id>
        <name>
          <surname>Phillips</surname>
          <given-names>Darci</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff6">6</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3452-7292</contrib-id>
        <name>
          <surname>Fantl</surname>
          <given-names>Wendy J.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
        <xref ref-type="aff" rid="Aff7">7</xref>
        <xref ref-type="aff" rid="Aff8">8</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8862-9043</contrib-id>
        <name>
          <surname>Nolan</surname>
          <given-names>Garry P.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff7">7</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1792-1768</contrib-id>
        <name>
          <surname>Schürch</surname>
          <given-names>Christian M.</given-names>
        </name>
        <address>
          <email>christian.schuerch@med.uni-tuebingen.de</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff9">9</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.168010.e</institution-id><institution-id institution-id-type="ISNI">0000000419368956</institution-id><institution>Department of Microbiology and Immunology, </institution><institution>Stanford University School of Medicine, </institution></institution-wrap>Stanford, CA 94305 USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.168010.e</institution-id><institution-id institution-id-type="ISNI">0000000419368956</institution-id><institution>Department of Pathology, </institution><institution>Stanford University School of Medicine, </institution></institution-wrap>Stanford, CA 94305 USA </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.168010.e</institution-id><institution-id institution-id-type="ISNI">0000000419368956</institution-id><institution>Department of Computer Science, </institution></institution-wrap>Stanford, CA 94305 USA </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.168010.e</institution-id><institution-id institution-id-type="ISNI">0000000419368956</institution-id><institution>Department of Urology, </institution><institution>Stanford University School of Medicine, </institution></institution-wrap>Stanford, CA 94305 USA </aff>
      <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.168010.e</institution-id><institution-id institution-id-type="ISNI">0000000419368956</institution-id><institution>Department of Bioengineering, </institution><institution>Stanford University School of Medicine, </institution></institution-wrap>Stanford, CA 94305 USA </aff>
      <aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="GRID">grid.168010.e</institution-id><institution-id institution-id-type="ISNI">0000000419368956</institution-id><institution>Department of Dermatology, </institution><institution>Stanford University School of Medicine, </institution></institution-wrap>Stanford, CA 94305 USA </aff>
      <aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="GRID">grid.168010.e</institution-id><institution-id institution-id-type="ISNI">0000000419368956</institution-id><institution>Stanford Cancer Institute, </institution><institution>Stanford University School of Medicine, </institution></institution-wrap>Stanford, CA 94305 USA </aff>
      <aff id="Aff8"><label>8</label><institution-wrap><institution-id institution-id-type="GRID">grid.168010.e</institution-id><institution-id institution-id-type="ISNI">0000000419368956</institution-id><institution>Department of Obstetrics and Gynecology, </institution><institution>Stanford University School of Medicine, </institution></institution-wrap>Stanford, CA 94305 USA </aff>
      <aff id="Aff9"><label>9</label><institution-wrap><institution-id institution-id-type="GRID">grid.411544.1</institution-id><institution-id institution-id-type="ISNI">0000 0001 0196 8249</institution-id><institution>Department of Pathology and Neuropathology, </institution><institution>University Hospital and Comprehensive Cancer Center Tübingen, </institution></institution-wrap>Tübingen, Germany </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>18</day>
      <month>1</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>18</day>
      <month>1</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>23</volume>
    <elocation-id>46</elocation-id>
    <history>
      <date date-type="received">
        <day>20</day>
        <month>4</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>10</day>
        <month>1</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Algorithmic cellular segmentation is an essential step for the quantitative analysis of highly multiplexed tissue images. Current segmentation pipelines often require manual dataset annotation and additional training, significant parameter tuning, or a sophisticated understanding of programming to adapt the software to the researcher’s need. Here, we present CellSeg, an open-source, pre-trained nucleus segmentation and signal quantification software based on the Mask region-convolutional neural network (R-CNN) architecture. CellSeg is accessible to users with a wide range of programming skills.
</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">CellSeg performs at the level of top segmentation algorithms in the 2018 Kaggle Data Challenge both qualitatively and quantitatively and generalizes well to a diverse set of multiplexed imaged cancer tissues compared to established state-of-the-art segmentation algorithms. Automated segmentation post-processing steps in the CellSeg pipeline improve the resolution of immune cell populations for downstream single-cell analysis. Finally, an application of CellSeg to a highly multiplexed colorectal cancer dataset acquired on the CO-Detection by indEXing (CODEX) platform demonstrates that CellSeg can be integrated into a multiplexed tissue imaging pipeline and lead to accurate identification of validated cell populations.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p id="Par3">CellSeg is a robust cell segmentation software for analyzing highly multiplexed tissue images, accessible to biology researchers of any programming skill level.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Deep learning</kwd>
      <kwd>Segmentation</kwd>
      <kwd>Image analysis</kwd>
      <kwd>CODEX</kwd>
      <kwd>Mask R-CNN</kwd>
      <kwd>Multiplexed imaging</kwd>
      <kwd>Pre-trained model</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id>
            <institution>National Institutes of Health</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000005</institution-id>
            <institution>U.S. Department of Defense</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000865</institution-id>
            <institution>Bill and Melinda Gates Foundation</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000038</institution-id>
            <institution>U.S. Food and Drug Administration</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000884</institution-id>
            <institution>Cancer Research Institute</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000289</institution-id>
            <institution>Cancer Research UK</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100014547</institution-id>
            <institution>Parker Institute for Cancer Immunotherapy</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100009576</institution-id>
            <institution>Kenneth Rainin Foundation</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001711</institution-id>
            <institution>Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>BRCA Foundation</institution>
        </funding-source>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par21">Tissue imaging and single-cell analysis can reveal previously undetected biological structure and uncover subtle spatial relationships between cells. Recently, the development of antibody-based multiplexed imaging methods has enabled deep single-cell phenotyping of tissue microenvironments [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR9">9</xref>]. This analysis has been especially useful in cancer studies, where these imaging platforms have revealed nuanced tumor architecture and interactions between tumor, immune and stromal cells, and healthy host tissue [<xref ref-type="bibr" rid="CR10">10</xref>–<xref ref-type="bibr" rid="CR16">16</xref>]. In such highly multiplexed tissue imaging studies, the quality and accuracy of downstream analyses depend critically on the precise identification and correct phenotypic assignment of single cells, which requires accurate demarcation of each cell’s boundary and quantification of its marker expression. This is usually accomplished using an automated segmentation and signal quantification algorithm [<xref ref-type="bibr" rid="CR17">17</xref>]. At a minimum, a segmentation algorithm takes an image as input and produces a set of masks denoting the boundary of each identified cell.</p>
    <p id="Par22">Commonly used segmentation algorithms include watershed (WTS) combined with thresholding [<xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR19">19</xref>] and level-set techniques [<xref ref-type="bibr" rid="CR20">20</xref>]. For example, WTS segmentation was recently used to identify cells in highly multiplexed fluorescence microscopy datasets of mouse and human tissues [<xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR21">21</xref>]. However, these methods can be sensitive to noise within the image including blurred cell–cell contact boundaries and imaging artifacts such as antibody aggregates. Additionally, they are often not robust to variations in cell size or morphology and require significant parameter tuning for expected cell size, shape of nucleus, and cell density [<xref ref-type="bibr" rid="CR22">22</xref>]. These limitations make their application to segmenting images of tumor tissue challenging, since cancers consist of a variety of cell shapes, sizes, and densities. Advances in deep learning architectures have transformed cell image analysis [<xref ref-type="bibr" rid="CR23">23</xref>], and these models have been extended to applications in single-cell segmentation [<xref ref-type="bibr" rid="CR24">24</xref>–<xref ref-type="bibr" rid="CR31">31</xref>]. Leading among these algorithms is the Mask R-CNN architecture, which has previously shown positive performance on other segmentation tasks [<xref ref-type="bibr" rid="CR26">26</xref>, <xref ref-type="bibr" rid="CR32">32</xref>]. However, deep learning algorithms usually require pre-labeled training data for the specific segmentation task, leading to substantial and time-consuming human input to obtain a high-quality segmentation.</p>
    <p id="Par23">While pre-trained deep learning architectures exist, such as StarDist [<xref ref-type="bibr" rid="CR33">33</xref>] and Cellpose [<xref ref-type="bibr" rid="CR26">26</xref>], an additional issue is the subsequent handling of the segmentation output to produce single-cell statistics used for downstream analysis, including pixel quantification. This results in either extra coding or exporting of masks to another image processing software where additional commands allow quantification of pixels in the segmented image to produce single-cell statistics. Further processing steps, including expanding mask boundaries and reducing noise in the statistics must be completed separately. Complete segmentation pipelines including CellProfiler [<xref ref-type="bibr" rid="CR34">34</xref>] or ilastik [<xref ref-type="bibr" rid="CR35">35</xref>] address these concerns but still often require hands-on user input, such as manual annotation or processing of segmentation results.</p>
    <p id="Par24">Here, we present CellSeg (<ext-link ext-link-type="uri" xlink:href="https://michaellee1.github.io/CellSegSite/index.html">https://michaellee1.github.io/CellSegSite/index.html</ext-link>), an easy-to-use, pre-trained, Mask R-CNN-based cell segmentation and pixel quantification software. Users supply a set of tissue images to CellSeg, and the software returns the segmented images and a table of single-cell statistics including each cell’s location, nucleus size, and mean pixel values in each imaging channel. CellSeg is open-source and available for Windows, Mac, or Linux. It is capable of segmenting JPG, PNG, and TIFF images of any image size, subject to hardware requirements reported below. CellSeg is accessible to individuals of all programming levels and requires minimal user input. For most uses, the pre-trained CellSeg model requires no additional manual annotation of training data, no additional training, and limited parameter tuning to produce a high-quality segmentation. The software has been designed to work as a library, so more advanced Python users can customize the pipeline to fit their needs. Applications detailed below demonstrate that CellSeg robustly segments highly multiplexed fluorescence images from a variety of healthy and cancerous tissues. CellSeg exceeds an established WTS segmentation pipeline both in terms of user friendliness as well as segmentation accuracy and performs at the level of two state-of-the-art deep learning segmentation algorithms.</p>
  </sec>
  <sec id="Sec2">
    <title>Implementation</title>
    <sec id="Sec3">
      <title>Overview of CellSeg pipeline</title>
      <p id="Par25">The CellSeg software is implemented in Python and run using Jupyter Notebook [<xref ref-type="bibr" rid="CR36">36</xref>]. CellSeg first extracts a user-specified nucleus color channel for segmentation (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, step 1). Through iterative visual inspection, we found increasing the brightness of the nuclear channel can improve segmentation performance, especially in images with weak nuclear stain signal. CellSeg therefore scales each nuclear image’s brightness by a fixed constant computed from a reference image specified by the user. After scaling, CellSeg splits the nuclear stain image into several overlapping cropped images to accelerate segmentation (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, step 1). Each image crop is segmented, and the resulting segmented crops are stitched into the full segmented multi-channel image (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, steps 2–3). To eliminate erroneously segmented imaging artifacts, which often appear as high intensity speckles or clusters, CellSeg removes objects smaller than a user-specified threshold from the set of segmented cells.<fig id="Fig1"><label>Fig. 1</label><caption><p>CellSeg pipeline. Overview of CellSeg software with following steps. (1) Extract nuclear channel and crop images to segment. (2) Segment each image crop with CellSeg. (3) Stitch together segmented crops. (4) Expand boundaries of cells using mask expansion. (5) Perform lateral bleed compensation, then compute and output single-cell statistics for N markers</p></caption><graphic xlink:href="12859_2022_4570_Fig1_HTML" id="MO1"/></fig></p>
      <p id="Par26">After segmentation, two optional post-processing steps follow: mask expansion and lateral bleed compensation (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, steps 4–5). Mask expansion extends the boundary of each segmented nucleus by a user-specified number of pixels to capture cell membrane fluorescent signal (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, step 4). Lateral bleed compensation aims to correct fluorescent signal spillover between adjacent cells, an issue often seen in immunofluorescence imaging of dense tissues. The details of these steps and their performance are discussed below. After post-processing, CellSeg computes the mean pixel value for each marker over the set of pixels contained in each identified cell. In fluorescent images, these values can be seen as a proxy to the expression level of each imaged protein in each cell. CellSeg saves each cell’s (X, Y) coordinate and pixel quantifications to a table in both comma-separated value (CSV) and flow-cytometry standard (FCS) formats (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, step 5). This data output format is recognizable by popular downstream single-cell analysis software, including flow cytometry gating programs such as CellEngine (<ext-link ext-link-type="uri" xlink:href="https://cellengine.com">https://cellengine.com</ext-link>), Cytobank (<ext-link ext-link-type="uri" xlink:href="https://www.cytobank.org">https://www.cytobank.org</ext-link>), or FlowJo (<ext-link ext-link-type="uri" xlink:href="https://www.flowjo.com">https://www.flowjo.com</ext-link>), as well as cell clustering programs like VorteX [<xref ref-type="bibr" rid="CR37">37</xref>]. For visual inspection of segmentation quality, the user can also optionally generate images of the segmented tissue with overlaid masks and a TIFF stack of mask regions of interest (ROIs) which can be viewed in other image analysis programs like Fiji/ImageJ [<xref ref-type="bibr" rid="CR38">38</xref>].</p>
    </sec>
    <sec id="Sec4">
      <title>User accessibility</title>
      <p id="Par27">CellSeg is an open-source segmentation software accessible to individuals with a range of programming skills. On the CellSeg webpage (<ext-link ext-link-type="uri" xlink:href="https://michaellee1.github.io/CellSegSite/index.html">https://michaellee1.github.io/CellSegSite/index.html</ext-link>), we created detailed tutorials for software installation, setup, and segmentation configuration. These tutorials assume no prior programming knowledge, and they walk the user through all phases of the CellSeg pipeline. The user has the option to run CellSeg using either a Jupyter Notebook containing a step-by-step walkthrough of the pipeline with comments or a fully automated script for segmenting several images. For more advanced users, the components of the pipeline can be used as a library. This allows users to incorporate algorithms from CellSeg into their personal segmentation pipelines or use CellSeg algorithms individually during exploratory image analysis. CellSeg can be run in the background on any computer with sufficient storage for the input image files with at least 16 GB RAM. While it does not require a GPU, CellSeg can be accelerated for those with access to hardware using the open-source package tensorflow-gpu [<xref ref-type="bibr" rid="CR39">39</xref>], and instructions for GPU acceleration are provided on the webpage.</p>
    </sec>
    <sec id="Sec5">
      <title>Training</title>
      <p id="Par28">CellSeg was trained on a dataset of fluorescent and brightfield biological microscopy images from the 2018 Kaggle Data Science Bowl containing 29,464 ground truth segmented nuclei (Fig. <xref rid="Fig2" ref-type="fig">2</xref>A) [<xref ref-type="bibr" rid="CR40">40</xref>]. These images were acquired with variations in cell phenotype, size, image zoom, and brightness. Of note, CellSeg was not trained on any highly multiplexed tissue imaging data. Both the CellSeg architecture and training method were optimized for robustness to variations in cell size and morphology. Further details of CellSeg's architecture and training can be found in the “<xref rid="Sec13" ref-type="sec">Methods</xref>” section.<fig id="Fig2"><label>Fig. 2</label><caption><p>Training and Benchmarking CellSeg performance on the 2018 Kaggle data challenge. <bold>A</bold> Information on Kaggle dataset used to develop, train, and test CellSeg. CellSeg final performance was assessed on a test set provided by the Kaggle data challenge using mean average precision (mAP) score. <bold>B</bold> CellSeg segmentation of representative fluorescence image from the Kaggle test set. White arrowheads: cells with blurred nuclear boundaries <bold>C</bold> CellSeg segmentation of representative H&amp;E-stained brightfield image. Red arrows: nuclear debris. <bold>D</bold> CellSeg performance compared to other top performing segmentation algorithms in data science bowl. Columns show mean average precision (mean AP) scores reported on Kaggle DSB2018 stage 2 test set and average F1 scores. For nucleAIzer, reported scores from the original publication [<xref ref-type="bibr" rid="CR29">29</xref>] are displayed. For StarDist, brightfield and fluorescence images were segmented using 2D_versatile_he pre-trained model and 2D_versatile_fluo pre-trained model, respectively. For Cellpose, the pre-trained nuclei segmentation model was used (see “<xref rid="Sec13" ref-type="sec">Methods”</xref> section for testing details)</p></caption><graphic xlink:href="12859_2022_4570_Fig2_HTML" id="MO2"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec6">
    <title>Results and discussion</title>
    <sec id="Sec7">
      <title>CellSeg architecture achieves high performance on Kaggle data challenge test set</title>
      <p id="Par29">After implementing CellSeg, we validated each step of the pipeline: architecture, segmentation, segmentation post-processing, and output. First, we quantitatively evaluated our trained Mask R-CNN segmentation architecture using the 2018 Kaggle Data Science Bowl [<xref ref-type="bibr" rid="CR40">40</xref>]. This challenge dataset enabled assessment of the performance of our trained Mask R-CNN architecture in comparison with top performers in this competition and more recently published neural network-based cell segmentation algorithms such as nucleAIzer [<xref ref-type="bibr" rid="CR29">29</xref>], StarDist [<xref ref-type="bibr" rid="CR33">33</xref>], and Cellpose [<xref ref-type="bibr" rid="CR26">26</xref>].</p>
      <p id="Par30">CellSeg was tested for segmentation quality on a ground truth segmented validation set of 3717 nuclei from Kaggle (Fig. <xref rid="Fig2" ref-type="fig">2</xref>A). We found that CellSeg qualitatively performed well on fluorescence images where it accurately identified individual nuclei even when boundaries between two nuclei were blurred (Fig. <xref rid="Fig2" ref-type="fig">2</xref>B). CellSeg accurately identified many nuclei in the brightfield image stained with hematoxylin &amp; eosin (H&amp;E) (Fig. <xref rid="Fig2" ref-type="fig">2</xref>C), but overall performed better in fluorescent images, due potentially to the higher amounts of nuclear debris and other artifacts in the H&amp;E-stained images. The Kaggle challenge used mean average precision (mAP) as the segmentation quality metric (“<xref rid="Sec13" ref-type="sec">Methods</xref>” section). A higher mAP score corresponds to a more accurate segmentation on the Kaggle competition’s test set. By the mAP metric, CellSeg performed among the top algorithms in the competition and comparably to nucleAIzer, while attaining a higher mAP score than both StarDist and Cellpose (Fig. <xref rid="Fig2" ref-type="fig">2</xref>D).</p>
    </sec>
    <sec id="Sec8">
      <title>CellSeg outperforms an established WTS segmentation algorithm on a multi-tissue microarray</title>
      <p id="Par31">Next, we evaluated CellSeg’s performance on a set of tissues representing a variety of cell and nuclear sizes, densities, and morphologies. Using a tissue microarray (TMA) comprised of human tumor and healthy tissues and imaged with the fluorescent nuclear marker DRAQ5 [<xref ref-type="bibr" rid="CR15">15</xref>], we qualitatively assessed the performance of CellSeg against an established WTS algorithm that was tuned using hyperparameters selected by an expert pathologist for optimal segmentation quality [<xref ref-type="bibr" rid="CR3">3</xref>]. We found that CellSeg was less sensitive to variations in nucleus size and morphology, outperforming the WTS algorithm on most tissues (Fig. <xref rid="Fig3" ref-type="fig">3</xref>). Both CellSeg and WTS correctly segmented immune cells and cells with spindly nuclei (Fig. <xref rid="Fig3" ref-type="fig">3</xref>A, B). While WTS tended to incorrectly segment large nuclei into several smaller masks in glioblastoma multiforme (GBM), hepatocellular carcinoma (HCC), and seminoma tissues, CellSeg correctly identified both large and small nuclei in the same images (Fig. <xref rid="Fig3" ref-type="fig">3</xref>C–E). CellSeg robustly identified nuclei with significant variations in brightness, as observed in the GBM and HCC tissues. CellSeg and WTS with expert annotation did not perform well on an image of T-cell acute lymphoblastic leukemia (T-ALL), a highly dense tumor composed of small lymphocytes with obscured nuclear boundaries (Fig. <xref rid="Fig3" ref-type="fig">3</xref>F). Manually segmenting such tissues is challenging even for expert pathologists. These findings underscore the importance of both tissue quality and clear separation between individual cells as key parameters for optimal segmentation. Overall, CellSeg performed at least as well as the established WTS algorithm on all tissues, while clearly outperforming it on three tissues (GBM, HCC, and seminoma).<fig id="Fig3"><label>Fig. 3</label><caption><p>CellSeg performance on diverse human FFPE tissues. CellSeg performance on representative tissue images from a multi-tumor tissue microarray imaged with CODEX, all stains are DRAQ5 nuclear stain. <bold>A.</bold> Healthy spleen shows small cells. <bold>B.</bold> Dermatofibrosarcoma protuberans (DFSP) shows spindly nuclei. <bold>C.</bold> Glioblastoma multiforme (GBM) shows large, misshapen cells. <bold>D.</bold> Hepatocellular carcinoma (HCC) shows large, round cells. <bold>E.</bold> Seminoma shows a blend of large tumor cell nuclei and small nuclei from tumor-infiltrating lymphocytes. <bold>F.</bold> T-cell acute lymphoblastic leukemia (T-ALL) shows densely packed cells. Scale bar, 20 μm. Fluorescence intensity increased in original images for visualization purposes</p></caption><graphic xlink:href="12859_2022_4570_Fig3_HTML" id="MO3"/></fig></p>
    </sec>
    <sec id="Sec9">
      <title>CellSeg performs at the level of state-of-the-art neural network-based segmentation algorithms</title>
      <p id="Par32">Next, we qualitatively compared CellSeg’s performance to two recently published neural network-based segmentation algorithms, StarDist [<xref ref-type="bibr" rid="CR33">33</xref>] and Cellpose [<xref ref-type="bibr" rid="CR26">26</xref>]. The six tissues from our TMA shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref> were segmented using pre-trained models of StarDist (2D versatile fluo) and Cellpose, and the segmentation masks were compared to masks generated by CellSeg. All three algorithms performed comparably well on spleen and HCC (Fig. <xref rid="Fig4" ref-type="fig">4</xref>A, D). Compared to CellSeg, StarDist segmented more objects with low fluorescence intensity in GBM and seminoma (Fig. <xref rid="Fig4" ref-type="fig">4</xref>C, E). However, StarDist also oversegmented many nuclei in DFSP, while CellSeg identified them accurately, suggesting that CellSeg is more robust to variations in nuclear morphology (Fig. <xref rid="Fig4" ref-type="fig">4</xref>B). Both StarDist and CellSeg identified more nuclei than Cellpose in DFSP, GBM, and seminoma (Fig. <xref rid="Fig4" ref-type="fig">4</xref>B, C, E). All three algorithms performed poorly on T-ALL (Fig. <xref rid="Fig4" ref-type="fig">4</xref>F). In summary, CellSeg performs at the level of state-of-the-art neural network-based segmentation algorithms when applied to a real-world dataset.<fig id="Fig4"><label>Fig. 4</label><caption><p>CellSeg performs comparably to established deep learning-based segmentation algorithms on diverse human FFPE tissues<bold>.</bold> Representative images from tissues described in Fig. <xref rid="Fig3" ref-type="fig">3</xref> are shown. <bold>A.</bold> StarDist, Cellpose, and CellSeg show comparable performance on spleen. <bold>B.</bold> StarDist oversegments several spindly nuclei in DFSP (arrows), while CellSeg and Cellpose segment nuclei accurately. <bold>C.</bold> StarDist and CellSeg segment more low intensity objects in GBM (arrows). <bold>D.</bold> all three algorithms perform similarly well on HCC. <bold>E.</bold> StarDist and CellSeg segment more low intensity objects in seminoma (arrows). <bold>F.</bold> All three algorithms perform relatively poorly on T-ALL. Scale bar, 20 μm</p></caption><graphic xlink:href="12859_2022_4570_Fig4_HTML" id="MO4"/></fig></p>
    </sec>
    <sec id="Sec10">
      <title>CellSeg post-processing steps improve downstream resolution of immune populations</title>
      <p id="Par33">In the development of CellSeg, we addressed two post-segmentation issues. First, because CellSeg is a nucleus segmentation algorithm, the boundary identifying a cell’s nucleus often fails to capture the fluorescent signal of its plasma membrane where many of the protein markers used for cellular identification are located (e.g., CD45 denoting an immune cell, and EpCAM denoting an epithelial cell). Second, fluorescent imaging often results in spatial fluorescent spillover between adjacent cells, creating noise in quantification of protein expression [<xref ref-type="bibr" rid="CR3">3</xref>]. To resolve these issues, two optional steps follow segmentation with CellSeg: (1) mask expansion (Fig. <xref rid="Fig5" ref-type="fig">5</xref>A, step 1) and (2) lateral bleed compensation (Fig. <xref rid="Fig5" ref-type="fig">5</xref>A, steps 2–3). Mask expansion extends the boundary surrounding each segmented nucleus by a user-defined number of pixels. This allows for quantification of the plasma membrane fluorescent signal. Next, lateral bleed compensation corrects for fluorescence spillover between adjacent cells. As described in Goltsev et al. [<xref ref-type="bibr" rid="CR3">3</xref>], this algorithm computes the surface contact ratios between physically adjacent cells and uses this value to simultaneously boost signal from a cell and reduce spatial spillover noise from neighboring cells.<fig id="Fig5"><label>Fig. 5</label><caption><p>Testing lateral bleed compensation on a CODEX dataset of colorectal cancer samples. <bold>A.</bold> Schematic demonstrating post-processing of CellSeg segmentation with following steps. (1) Grow cell boundaries by user defined number of pixels (growth of two pixels shown). (2) Compute inverse adjacency matrix from cell–cell. (3) Multiply inverted adjacency matrix by marker pixel intensity vector to obtain compensated single-cell pixel quantifications table. <bold>B.</bold> Effects of lateral bleed compensation on double-positive cell populations in the CRC dataset for three pairs of mutually exclusive markers (CD8 vs. CD4, Cytokeratin vs. CD45, CD20 vs. CD3). Data shown are from one of the two CRC TMAs (TMA A), with comparable bleed compensation performance for the other TMA (TMA B, data not shown)</p></caption><graphic xlink:href="12859_2022_4570_Fig5_HTML" id="MO5"/></fig></p>
      <p id="Par34">We tested the efficacy of the lateral bleed compensation algorithm with CellSeg on an immunofluorescence dataset imaged on the CO-Detection by indEXing (CODEX) platform. CODEX iteratively visualizes protein-antibody binding events, allowing for the quantification of more than 50 protein targets in formalin-fixed, paraffin-embedded (FFPE) or fresh-frozen tissue sections [<xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR15">15</xref>]. Using CODEX, we recently imaged two TMAs containing 140 samples from 35 patients with colorectal cancer (CRC). In this study, WTS was used to segment the images [<xref ref-type="bibr" rid="CR15">15</xref>]. This particular WTS segmentation used the same bleed compensation algorithm that we implemented for CellSeg. Single-cell marker quantifications from the WTS segmentation were extensively validated in this study, providing us with a baseline for the expected expression profiles of cell types against which we could validate our bleed compensation algorithm.</p>
      <p id="Par35">Using CellSeg, we segmented the 140 CRC images either with or without bleed compensation and gated the segmented data in CellEngine. In this dataset, the expression of certain pairs of imaged protein markers are expected to be mutually exclusive based on their known biology, including CD20/CD3, CD8/CD4, and cytokeratin/CD45. However, the presence of several densely populated immune cell regions, as observed in the CRC dataset, can lead to the erroneous identification of cells that are positive for both markers due to spatial fluorescent spillover. Applying an approach previously used to assess compensation [<xref ref-type="bibr" rid="CR3">3</xref>], we measured the efficacy of bleed compensation by the observed reduction in the frequency of cells double positive for any of these pairs of markers (Fig. <xref rid="Fig5" ref-type="fig">5</xref>B). Lateral bleed compensation reduced the frequency of double positive cells in both the CD8/CD4 and CD20/CD3 pairs, with a lesser reduction of double positive cells in the cytokeratin/CD45 pair. Therefore, the lateral bleed compensation implemented in CellSeg improves the resolution of immune cell expression profiles in a dataset with densely packed immune cell regions.</p>
    </sec>
    <sec id="Sec11">
      <title>Phenotyping using CellSeg output recapitulates validated cell populations in the CRC CODEX Dataset</title>
      <p id="Par36">In previously published work using the CODEX pipeline, WTS segmentation provided single-cell fluorescent intensity statistics that were used to assign cell phenotypes in the CRC dataset [<xref ref-type="bibr" rid="CR15">15</xref>]. To assess whether CellSeg could replace WTS in the CODEX pipeline, we performed a head-to-head comparison by gating segmented cells in all 140 samples with key phenotyping markers of major cell types as validated by an expert pathologist.</p>
      <p id="Par37">To confirm that the gated cell types derived from WTS and CellSeg occupied similar regions within the CRC tumors, we directly visualized these cell types on the CRC images. This analysis showed that CellSeg and WTS generated the same cell phenotypes with comparable spatial organization within the CRC tumors (Fig. <xref rid="Fig6" ref-type="fig">6</xref>A). Further validation was performed by examining the original fluorescent image displaying key phenotyping markers (Fig. <xref rid="Fig6" ref-type="fig">6</xref>B). Both CellSeg and WTS cell types, identified by gating, had the expected cell morphology and fluorescence marker profile when inspecting the corresponding regions of the fluorescent image.<fig id="Fig6"><label>Fig. 6</label><caption><p>Recapitulating previously identified cell populations using CellSeg. <bold>A.</bold> Visualization of identified populations in a representative CRC tissue. Points on scatter plots show positions of cells on the displayed tissue image in Fig. <xref rid="Fig5" ref-type="fig">5</xref>B. Population identity obtained through gating. <bold>B.</bold> Fluorescent image of a representative CRC tissue. Expression of six phenotyping markers used in Fig. <xref rid="Fig5" ref-type="fig">5</xref>A shown. <bold>C.</bold> Population correlation analysis between CellSeg and WTS. Each point corresponds to a TMA spot, where the X value is the gated population count from WTS and the Y value is the count computed from CellSeg. Least-squares regression line displayed along with r<sup>2</sup> value. T cells are defined as CD45<sup>+</sup>CD3<sup>+</sup>CD20<sup>−</sup>Cytokeratin<sup>−</sup>; macrophages as CD45<sup>+</sup>CD20<sup>−</sup>CD3<sup>−</sup> and CD68<sup>+</sup>, CD163<sup>+</sup>, or CD68<sup>+</sup>CD163<sup>+</sup>; B cells as CD45<sup>+</sup>CD20<sup>+</sup>CD3<sup>−</sup>Cytokeratin<sup>−</sup>; and tumor as Cytokeratin<sup>+</sup>CD45<sup>−</sup></p></caption><graphic xlink:href="12859_2022_4570_Fig6_HTML" id="MO6"/></fig></p>
      <p id="Par38">For a more global quality assessment of our analysis, we correlated the cell types quantified by CellSeg with those identified by WTS. For each sample, we correlated the absolute number of cells in each gated population using the outputs of CellSeg and WTS (Fig. <xref rid="Fig6" ref-type="fig">6</xref>C). As examples, we depict gated T cells, macrophages, B cells, and tumor cells, all of which showed very strong positive correlations between the WTS and CellSeg segmented cell counts. The cell types generated from the CellSeg segmentation matched previously validated cell types generated from WTS in the CRC dataset. However, while cell counts from CellSeg and WTS were correlated, WTS segmentation generally resulted in higher numbers of tumor cells (Fig. <xref rid="Fig6" ref-type="fig">6</xref>C). This is likely due to over-segmentation of large tumor cell nuclei, as observed in GBM, HCC, and seminoma (Fig. <xref rid="Fig3" ref-type="fig">3</xref>C–E), suggesting that CellSeg is superior to WTS for tumor cell identification. These findings demonstrate that tissue analysis using CellSeg can recapitulate previously published findings in a multiplexed fluorescence imaging study.</p>
    </sec>
  </sec>
  <sec id="Sec12">
    <title>Conclusions</title>
    <p id="Par39">In summary, we present CellSeg, a robust single-cell segmentation and quantification software for tissue images. Our software has been designed to be accessible to researchers of all programming skill levels. For novice programmers, we have created detailed tutorials on how to implement and use CellSeg (<ext-link ext-link-type="uri" xlink:href="https://michaellee1.github.io/CellSegSite/index.html">https://michaellee1.github.io/CellSegSite/index.html</ext-link>). For more advanced Python users, the components of the CellSeg pipeline function as a library to complete customized image analysis or segmentation tasks. As more sophisticated segmentation algorithms emerge, future researchers can use the CellSeg pipeline and combine it with their algorithm of choice. Importantly, our pre-trained segmentation algorithm works “out-of-the-box” for many single-cell segmentation tasks, without requiring any additional manual training by the user.</p>
    <p id="Par40">We validated each step of the CellSeg pipeline: architecture, segmentation, post-processing, and output. In a post-competition evaluation, the pre-trained CellSeg architecture scored among the top performers from the 2018 Kaggle data challenge. CellSeg also qualitatively outperformed an established segmentation algorithm on a multi-tissue TMA. Both qualitative and quantitative comparisons demonstrated that CellSeg performs comparably to state-of-the-art deep learning-based segmentation algorithms. Finally, using a CODEX CRC dataset, we showed that the post-processing steps in our CellSeg pipeline improved resolution of mutually exclusive cell populations while recapitulating previously published cell populations in the dataset. CellSeg is therefore a powerful tool that has shown robust performance on a wide variety of tissue segmentation tasks and should help researchers with their needs in cell segmentation and marker quantification in biological imaging data.</p>
  </sec>
  <sec id="Sec13">
    <title>Methods</title>
    <sec id="Sec14">
      <title>Architecture</title>
      <p id="Par41">CellSeg is based on the Matterport implementation of Mask R-CNN [<xref ref-type="bibr" rid="CR32">32</xref>, <xref ref-type="bibr" rid="CR41">41</xref>]. CellSeg uses a slightly modified loss function during training. The original Mask R-CNN paper uses L = L<sub>cls</sub> + L<sub>box</sub> + L<sub>mask</sub>, where the loss is the additive sum of class loss, bounding box loss, and mask loss as defined in the paper. A new hyperparameter was added to arrive at L = αL<sub>cls</sub> + L<sub>box</sub> + L<sub>mask</sub>. Through experimental analysis, it was found that reducing the contribution of class loss in our single-class model improved convergence during training. For the results in this paper, α = 0.5.</p>
    </sec>
    <sec id="Sec15">
      <title>Training</title>
      <p id="Par42">To prevent overfitting and to extend the training data, multiple image augmentation techniques were used, which contributed significantly to CellSeg’s quantitative performance. The first was simple random field sampling. At train time, 512 × 512 pixel crops of the image were used, selected randomly from the image. Using the imgaug library version 0.2.9, contrast normalization, brightness, Gaussian blur, zooms scaling the X and Y axes independently, vertical and horizontal flips, and rotations were also modulated throughout training [<xref ref-type="bibr" rid="CR43">43</xref>]. No augmentations were conducted at test time. The model was trained in minibatch sizes of 16 using stochastic gradient descent with momentum. Transfer learning was utilized for the dataset, with weights used from a Mask R-CNN model that trained on COCO, a segmentation challenge with 91 object types and 2.5 million labeled instances [<xref ref-type="bibr" rid="CR44">44</xref>]. Auxiliary functions for training were adapted from the DeepRetina DSB2018 training scripts, although our model did not use their trained weights [<xref ref-type="bibr" rid="CR42">42</xref>]. The network was trained for 150 epochs with decaying learning rate with the base model frozen, to let only the top layers train. Then, the full network was trained for 25 epochs at a very low learning rate. All training was done on an Nvidia GTX 1080 Ti GPU and a Dual Intel® Xeon® Silver 4114 10-core CPU, taking about 41 h to train.</p>
    </sec>
    <sec id="Sec16">
      <title>Mean average precision metric</title>
      <p id="Par43">For each segmented image, mean average precision was computed using intersection over union (IoU) between segmented masks and ground truth masks as follows. IoU compares the overlap between the CellSeg segmentation of a cell and a ground truth manual segmentation. First, for each segmented mask, the pixel IoU, defined as the ratio of the overlap between ground truth mask A and CellSeg mask B to the total area that A and B cover was computed as IoU(A, B) = (A ∩ B)/(A ∪ B). IoU values range from 0 to 1, with 1 denoting a perfectly segmented cell, i.e., the ground truth. The number of cells with IoU values exceeding threshold t were computed, where t ranges from 0.5 to 0.95 in increments of 0.05. At each t, a precision value Q(t) was calculated as: Q(t) = TP(t)/(TP(t) + FP(t) + FN(t)) where TP, FP, and FN were the number of true positives, false positives, and false negatives identified in an image, respectively. A TP is defined as an object with a pixel IoU above the threshold t. The Average Precision (AP) of an image was then computed as the mean over the thresholds: AP = (1/|thresholds|)∑<sub>t</sub>Q(t). Finally, the mean AP (mAP) was computed as the mean over the AP of each image in the test dataset.</p>
    </sec>
    <sec id="Sec17">
      <title>Quantitative segmentation evaluation</title>
      <p id="Par44">Mask predictions were made on Kaggle DSB 2018 stage 2 test set images for CellSeg and uploaded to the DSB 2018 page on the Kaggle website (after the competition closed) to obtain mAP score. For StarDist (version 0.7.1), fluorescence and brightfield images were segmented using 2D_versatile fluo and 2D_versatile_HE pre-trained models, respectively. For Cellpose (version 0.6.5), fluorescence images were segmented using channels = [0,0] with all other parameters set to default. Brightfield images were segmented using parameters channels = [1,0], invert = True, and flow_threshold = 0.8 with all other parameters set to default. Predicted masks for each segmentation algorithm were saved and uploaded to Kaggle to obtain the mAP score.</p>
    </sec>
    <sec id="Sec18">
      <title>TMA qualitative segmentation evaluation</title>
      <p id="Par45">CellSeg and an optimized WTS segmentation algorithm [<xref ref-type="bibr" rid="CR3">3</xref>] were both used to segment six tissue images from a multi-tissue TMA. Representative images from the results of each segmentation were selected for Fig. <xref rid="Fig3" ref-type="fig">3</xref>. We used the best-focus image returned by the 3D WTS algorithm to compare segmentation results. Size parametrization for WTS was hand-verified by a board-certified surgical pathologist (C.M.S.). For qualitative comparison between CellSeg, StarDist, and CellPose in Fig. <xref rid="Fig4" ref-type="fig">4</xref>, 300 × 400 pixel patches were sampled randomly from each tissue image to visualize. StarDist (version 0.7.1) 2D_versatile_fluo pre-trained model was used for segmentation with default parameters. For Cellpose, the pre-trained model (version 0.6.5) with no modifications was used. Segmentation results were visualized with mask ROI overlays in ImageJ.</p>
    </sec>
    <sec id="Sec19">
      <title>Mask expansion algorithms and lateral bleed compensation</title>
      <p id="Par46">We implemented two mask expansion algorithms. The first algorithm expands the boundary of each mask by a user-defined number of pixels. If this expansion leads to two overlapping masks, the algorithm assigns each pixel in the overlapping region to the mask whose center is closest to the pixel. The first mask expansion algorithm is computationally efficient and works well for tissue images with cells of similar size. However, the algorithm biases pixel assignment towards smaller masks, since the center of these masks are generally closer to the overlap region than the centers of larger masks. To correct for this, the second expansion algorithm iterates over the masks, expanding each mask by 1 pixel until it collides with another pre-existing mask boundary, at which point growth in that direction stops. The algorithm proceeds until each mask has been expanded by the user-defined number of pixels. This algorithm mitigates the need for assigning pixels based on distance to cell center at the cost of more computation time. Through iterative visual inspection of masks with and without mask expansion, we found that growth by 1 or 2 pixels is usually sufficient to capture most membrane protein signal. The lateral bleed compensation algorithm implemented in the CellSeg pipeline is the same as in previously published work from our group, readers are directed to the original paper for the details of the algorithm [<xref ref-type="bibr" rid="CR3">3</xref>].</p>
    </sec>
    <sec id="Sec20">
      <title>Benchmarking CellSeg</title>
      <p id="Par47">Segmentation of the CRC dataset and multi-tissue TMA was performed on a Dual Intel® Xeon® Silver 4114 10-core CPU. Resulting segmented data was gated in CellEngine (<ext-link ext-link-type="uri" xlink:href="https://cellengine.com">https://cellengine.com</ext-link>). When evaluating fluorescent bleed compensation on the CRC dataset, samples were aggregated by TMA, resulting in two gates for each cell population, one corresponding to each TMA. When performing population correlation analysis, cell types were gated by sample, resulting in tailored gates for the 140 samples. Gating was performed independently for the WTS dataset and the CellSeg dataset in a blinded fashion (J.S.B.). Resulting identified populations were validated by an expert in flow cytometry (C.M.S.). Population correlation analysis was performed in R Studio. The script is available upon request. Plots in Fig. <xref rid="Fig6" ref-type="fig">6</xref>a and c were generated using the R package ggplot2 [<xref ref-type="bibr" rid="CR45">45</xref>]. The tissue image for Fig. <xref rid="Fig6" ref-type="fig">6</xref>b was obtained in Fiji/ImageJ [<xref ref-type="bibr" rid="CR38">38</xref>].</p>
    </sec>
  </sec>
  <sec id="Sec21">
    <title>Availability and requirements</title>
    <p id="Par48">Project Name: CellSeg. Project home page: <ext-link ext-link-type="uri" xlink:href="https://michaellee1.github.io/CellSegSite/index.html">https://michaellee1.github.io/CellSegSite/index.html</ext-link>. Operating system(s): Windows, MacOS, or Linux. Programming language: Python. Other requirements: Web browser, internet connection, Conda, Jupyter, minimum 16 GB RAM, other Python package dependencies listed on project home page. License: MIT. Restrictions for Non-academics: None.</p>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>CODEX</term>
        <def>
          <p id="Par4">CO-detection by indexing</p>
        </def>
      </def-item>
      <def-item>
        <term>CPU</term>
        <def>
          <p id="Par5">Central processing unit</p>
        </def>
      </def-item>
      <def-item>
        <term>CRC</term>
        <def>
          <p id="Par6">Colorectal cancer</p>
        </def>
      </def-item>
      <def-item>
        <term>CSV</term>
        <def>
          <p id="Par7">Comma-separated values</p>
        </def>
      </def-item>
      <def-item>
        <term>DFSP</term>
        <def>
          <p id="Par8">Dermatofibrosarcoma protuberans</p>
        </def>
      </def-item>
      <def-item>
        <term>FCS</term>
        <def>
          <p id="Par9">Flow cytometry standard</p>
        </def>
      </def-item>
      <def-item>
        <term>FFPE</term>
        <def>
          <p id="Par10">Formalin-fixed paraffin-embedded</p>
        </def>
      </def-item>
      <def-item>
        <term>GBM</term>
        <def>
          <p id="Par11">Glioblastoma multiforme</p>
        </def>
      </def-item>
      <def-item>
        <term>GPU</term>
        <def>
          <p id="Par12">Graphics processing unit</p>
        </def>
      </def-item>
      <def-item>
        <term>HCC</term>
        <def>
          <p id="Par13">Hepatocellular carcinoma</p>
        </def>
      </def-item>
      <def-item>
        <term>IoU</term>
        <def>
          <p id="Par14">Intersection over union</p>
        </def>
      </def-item>
      <def-item>
        <term>mAP</term>
        <def>
          <p id="Par15">Mean average precision</p>
        </def>
      </def-item>
      <def-item>
        <term>R-CNN</term>
        <def>
          <p id="Par16">Region-convolutional neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>ROI</term>
        <def>
          <p id="Par17">Region of interest</p>
        </def>
      </def-item>
      <def-item>
        <term>T-ALL</term>
        <def>
          <p id="Par18">T-cell acute lymphoblastic leukemia</p>
        </def>
      </def-item>
      <def-item>
        <term>TMA</term>
        <def>
          <p id="Par19">Tissue microarray</p>
        </def>
      </def-item>
      <def-item>
        <term>WTS</term>
        <def>
          <p id="Par20">Watershed</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
    <fn>
      <p>Michael Y. Lee and Jacob S. Bedia are Co-first authors.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>We thank Yu Xin Wang and Colin Holbrook (Baxter Laboratory for Stem Cell Biology, Stanford University School of Medicine) for constructive feedback on the software and Andrew Janowczyk (University of Lausanne and Swiss Institute of Bioinformatics, Switzerland, and Case Western Reserve University, Cleveland, OH) for critically reading the manuscript.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors' contributions</title>
    <p>Conceptualization: M.Y.L., S.S.B., G.L.B., D.P. and C.M.S.; Methodology: M.Y.L. and J.S.B.; Software: M.Y.L. and J.S.B.; Validation: M.Y.L., J.S.B., S.S.B., G.L.B. and C.M.S.; Formal Analysis: M.Y.L. and J.S.B.; Investigation: M.Y.L. and J.S.B.; Resources: W.J.F., G.P.N. and C.M.S.; Writing – Original Draft: M.Y.L. and J.S.B.; Writing – Review and Editing: M.Y.L., J.S.B., S.S.B., G.L.B., D.P., W.J.F. and C.M.S.; Supervision: S.S.B., G.L.B. and C.M.S.; Project Administration: J.S.B.; Funding acquisition: S.S.B., D.P., W.J.F., G.P.N. and C.M.S. Reading and approval of the final manuscript: All authors.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work was supported by the U.S. National Institutes of Health (2U19AI057229-16, 5P01HL10879707, 5R01GM10983604, 5R33CA18365403, 5U01AI101984-07, 5UH2AR06767604, 5R01CA19665703, 5U54CA20997103, 5F99CA212231-02, 1F32CA233203-01, 5U01AI140498-02, 1U54HG010426-01, 5U19AI100627-07, 1R01HL120724-01A1, R33CA183692, R01HL128173-04, 5P01AI131374-02, 5UG3DK114937-02, 1U19AI135976-01, IDIQ17X149, 1U2CCA233238-01, 1U2CCA233195-01 to G.P.N.; R21CA231280, 1R01CA234553-01A1, P01HL10879709 to W.J.F.); the U.S. Department of Defense (W81XWH-12–1-0591 to G.P.N.; W81XWH-14–1-0180 to W.J.F. and G.P.N.); the U.S. Food and Drug Administration (HHSF223201610018C, DSTL/AGR/00980/01 to G.P.N.); Cancer Research UK (C27165/A29073 to G.P.N.); the Bill and Melinda Gates Foundation (OPP1113682 to G.P.N.); the Cancer Research Institute; the Parker Institute for Cancer Immunotherapy; the Kenneth Rainin Foundation (2018–575); the Silicon Valley Community Foundation (2017–175329 and 2017–177799-5022); the Beckman Center for Molecular and Genetic Medicine; Juno Therapeutics, Inc. (122401); Pfizer, Inc. (123214); Celgene, Inc. (133826, 134073); Vaxart, Inc. (137364); and the Rachford &amp; Carlotta A. Harris Endowed Chair (G.P.N.). W.J.F. was supported by the 2019 Cancer Innovation Award from the Stanford Cancer Institute, an NCI-designated Comprehensive Cancer Center; the Department of Urology at Stanford University; the BRCA Foundation; the V Foundation for cancer; a gift from the Gray Foundation; and a Parker Institute for Cancer Immunotherapy Bedside to Bench grant. C.M.S. was supported by the Swiss National Science Foundation (P300PB_171189, P400PM_183915). D.P. was supported by an NIH T32 Fellowship (AR007422), an NIH F32 Fellowship (CA233203), a Stanford Dean’s Postdoctoral Fellowship, a Stanford Cancer Institute Fellowship, and Stanford’s Dermatology Department. S.S.B. was supported by a Bio-X Stanford Interdisciplinary Graduate Fellowship and Stanford Bioengineering. G.L.B was supported by NIH 5U01AI101984 as well as an NIH T32 fellowship through the Stanford Molecular and Cellular Immunobiology Program (5T32AI007290-34). The funding bodies played no role in the design of the study or collection, analysis, or interpretation of data, nor in writing the manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>Raw data and materials used in this study are published and available for download [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR40">40</xref>]. The gating data and associated statistics shown in Figs. <xref rid="Fig5" ref-type="fig">5</xref> and <xref rid="Fig6" ref-type="fig">6</xref> are available from the corresponding author on reasonable request.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar1">
      <title>Ethics approval and consent to participate</title>
      <p id="Par49">Not applicable.</p>
    </notes>
    <notes id="FPar2">
      <title>Consent for publication</title>
      <p id="Par50">Not applicable.</p>
    </notes>
    <notes id="FPar4" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par51">M.Y.L. is a co-founder of and has equity in Biodock, Inc. G.P.N. is a co-founder and stockholder of Akoya Biosciences, Inc., and inventor on patent US9909167. C.M.S. is a scientific advisor to, has stock options in, and has received research funding from Enable Medicine, Inc. The other authors declare that no competing financial interests exist.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Agasti</surname>
            <given-names>SS</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Schueder</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Sukumar</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Jungmann</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Yin</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>DNA-barcoded labeling probes for highly multiplexed exchange-PAINT imaging</article-title>
        <source>Chem Sci</source>
        <year>2017</year>
        <volume>8</volume>
        <fpage>3080</fpage>
        <lpage>3091</lpage>
        <pub-id pub-id-type="doi">10.1039/C6SC05420J</pub-id>
        <pub-id pub-id-type="pmid">28451377</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Angelo</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Bendall</surname>
            <given-names>SC</given-names>
          </name>
          <name>
            <surname>Finck</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Hale</surname>
            <given-names>MB</given-names>
          </name>
          <name>
            <surname>Hitzman</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Borowsky</surname>
            <given-names>AD</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Multiplexed ion beam imaging (MIBI) of human breast tumors</article-title>
        <source>Nat Med</source>
        <year>2014</year>
        <volume>20</volume>
        <fpage>436</fpage>
        <lpage>442</lpage>
        <pub-id pub-id-type="doi">10.1038/nm.3488</pub-id>
        <pub-id pub-id-type="pmid">24584119</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Goltsev</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Samusik</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Kennedy-Darling</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Bhate</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Hale</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Vazquez</surname>
            <given-names>G</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep profiling of mouse splenic architecture with CODEX multiplexed imaging</article-title>
        <source>Cell</source>
        <year>2018</year>
        <volume>174</volume>
        <fpage>968</fpage>
        <lpage>981.e15</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2018.07.010</pub-id>
        <pub-id pub-id-type="pmid">30078711</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <mixed-citation publication-type="other">Gut G, Herrmann MD, Pelkmans L. Multiplexed protein maps link subcellular organization to cellular states. Science. 2018;361.</mixed-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Hennrick</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Drew</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>A colorful future of quantitative pathology: validation of Vectra technology using chromogenic multiplexed immunohistochemistry and prostate tissue microarrays</article-title>
        <source>Hum Pathol</source>
        <year>2013</year>
        <volume>44</volume>
        <fpage>29</fpage>
        <lpage>38</lpage>
        <pub-id pub-id-type="doi">10.1016/j.humpath.2012.05.009</pub-id>
        <pub-id pub-id-type="pmid">22944297</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <mixed-citation publication-type="other">Lin J-R, Izar B, Wang S, Yapp C, Mei S, Shah PM, et al. Highly multiplexed immunofluorescence imaging of human tissues and tumors using t-CyCIF and conventional optical microscopes. eLife. 2018;7.</mixed-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Saka</surname>
            <given-names>SK</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Kishi</surname>
            <given-names>JY</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>W</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Immuno-SABER enables highly multiplexed and amplified protein imaging in tissues</article-title>
        <source>Nat Biotechnol</source>
        <year>2019</year>
        <volume>37</volume>
        <fpage>1080</fpage>
        <lpage>1090</lpage>
        <pub-id pub-id-type="doi">10.1038/s41587-019-0207-y</pub-id>
        <pub-id pub-id-type="pmid">31427819</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schubert</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Bonnekoh</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Pommer</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Philipsen</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Böckelmann</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Malykh</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Analyzing proteome topology and function by automated multidimensional fluorescence microscopy</article-title>
        <source>Nat Biotechnol</source>
        <year>2006</year>
        <volume>24</volume>
        <fpage>1270</fpage>
        <lpage>1278</lpage>
        <pub-id pub-id-type="doi">10.1038/nbt1250</pub-id>
        <pub-id pub-id-type="pmid">17013374</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Woehrstein</surname>
            <given-names>JB</given-names>
          </name>
          <name>
            <surname>Donoghue</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Dai</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Avendaño</surname>
            <given-names>MS</given-names>
          </name>
          <name>
            <surname>Schackmann</surname>
            <given-names>RCJ</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Rapid sequential in situ multiplexing with DNA-exchange-imaging in neuronal cells and tissues</article-title>
        <source>Nano Lett</source>
        <year>2017</year>
        <volume>17</volume>
        <fpage>6131</fpage>
        <lpage>6139</lpage>
        <pub-id pub-id-type="doi">10.1021/acs.nanolett.7b02716</pub-id>
        <pub-id pub-id-type="pmid">28933153</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ali</surname>
            <given-names>HR</given-names>
          </name>
          <name>
            <surname>Jackson</surname>
            <given-names>HW</given-names>
          </name>
          <name>
            <surname>Zanotelli</surname>
            <given-names>VRT</given-names>
          </name>
          <name>
            <surname>Danenberg</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Fischer</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Bardwell</surname>
            <given-names>H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Imaging mass cytometry and multiplatform genomics define the phenogenomic landscape of breast cancer</article-title>
        <source>Nat Cancer</source>
        <year>2020</year>
        <volume>1</volume>
        <fpage>163</fpage>
        <lpage>175</lpage>
        <pub-id pub-id-type="doi">10.1038/s43018-020-0026-6</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gerdes</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Sevinsky</surname>
            <given-names>CJ</given-names>
          </name>
          <name>
            <surname>Sood</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Adak</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Bello</surname>
            <given-names>MO</given-names>
          </name>
          <name>
            <surname>Bordwell</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Highly multiplexed single-cell analysis of formalin-fixed, paraffin-embedded cancer tissue</article-title>
        <source>Proc Natl Acad Sci USA</source>
        <year>2013</year>
        <volume>110</volume>
        <fpage>11982</fpage>
        <lpage>11987</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.1300136110</pub-id>
        <pub-id pub-id-type="pmid">23818604</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Giesen</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>HAO</given-names>
          </name>
          <name>
            <surname>Schapiro</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Zivanovic</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Jacobs</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Hattendorf</surname>
            <given-names>B</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Highly multiplexed imaging of tumor tissues with subcellular resolution by mass cytometry</article-title>
        <source>Nat Methods</source>
        <year>2014</year>
        <volume>11</volume>
        <fpage>417</fpage>
        <lpage>422</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.2869</pub-id>
        <pub-id pub-id-type="pmid">24584193</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jackson</surname>
            <given-names>HW</given-names>
          </name>
          <name>
            <surname>Fischer</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Zanotelli</surname>
            <given-names>VRT</given-names>
          </name>
          <name>
            <surname>Ali</surname>
            <given-names>HR</given-names>
          </name>
          <name>
            <surname>Mechera</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Soysal</surname>
            <given-names>SD</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The single-cell pathology landscape of breast cancer</article-title>
        <source>Nature</source>
        <year>2020</year>
        <volume>578</volume>
        <fpage>615</fpage>
        <lpage>620</lpage>
        <pub-id pub-id-type="doi">10.1038/s41586-019-1876-x</pub-id>
        <pub-id pub-id-type="pmid">31959985</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Keren</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Bosse</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Marquez</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Angoshtari</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Jain</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Varma</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A structured tumor-immune microenvironment in triple negative breast cancer revealed by multiplexed ion beam imaging</article-title>
        <source>Cell</source>
        <year>2018</year>
        <volume>174</volume>
        <fpage>1373</fpage>
        <lpage>1387.e19</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2018.08.039</pub-id>
        <pub-id pub-id-type="pmid">30193111</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schürch</surname>
            <given-names>CM</given-names>
          </name>
          <name>
            <surname>Bhate</surname>
            <given-names>SS</given-names>
          </name>
          <name>
            <surname>Barlow</surname>
            <given-names>GL</given-names>
          </name>
          <name>
            <surname>Phillips</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>Noti</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Zlobec</surname>
            <given-names>I</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Coordinated cellular neighborhoods orchestrate antitumoral immunity at the colorectal cancer invasive front</article-title>
        <source>Cell</source>
        <year>2020</year>
        <volume>182</volume>
        <fpage>1341</fpage>
        <lpage>1359.e19</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2020.07.005</pub-id>
        <pub-id pub-id-type="pmid">32763154</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Phillips</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Matusiak</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Gutierrez</surname>
            <given-names>BR</given-names>
          </name>
          <name>
            <surname>Bhate</surname>
            <given-names>SS</given-names>
          </name>
          <name>
            <surname>Barlow</surname>
            <given-names>GL</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Immune cell topography predicts response to PD-1 blockade in cutaneous T cell lymphoma</article-title>
        <source>Nat Commun</source>
        <year>2021</year>
        <volume>12</volume>
        <fpage>6726</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-021-26974-6</pub-id>
        <pub-id pub-id-type="pmid">34795254</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schüffler</surname>
            <given-names>PJ</given-names>
          </name>
          <name>
            <surname>Schapiro</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Giesen</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>HAO</given-names>
          </name>
          <name>
            <surname>Bodenmiller</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Buhmann</surname>
            <given-names>JM</given-names>
          </name>
        </person-group>
        <article-title>Automatic single cell segmentation on highly multiplexed tissue images</article-title>
        <source>Cytometry A</source>
        <year>2015</year>
        <volume>87</volume>
        <fpage>936</fpage>
        <lpage>942</lpage>
        <pub-id pub-id-type="doi">10.1002/cyto.a.22702</pub-id>
        <pub-id pub-id-type="pmid">26147066</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wong</surname>
            <given-names>STC</given-names>
          </name>
        </person-group>
        <article-title>A novel cell segmentation method and cell phase identification using markov model</article-title>
        <source>IEEE Trans Inf Technol Biomed</source>
        <year>2009</year>
        <volume>13</volume>
        <fpage>152</fpage>
        <lpage>157</lpage>
        <pub-id pub-id-type="doi">10.1109/TITB.2008.2007098</pub-id>
        <pub-id pub-id-type="pmid">19272857</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Padfield</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Rittscher</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Roysam</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Coupled minimum-cost flow cell tracking for high-throughput quantitative analysis</article-title>
        <source>Med Image Anal</source>
        <year>2011</year>
        <volume>15</volume>
        <fpage>650</fpage>
        <lpage>668</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2010.07.006</pub-id>
        <pub-id pub-id-type="pmid">20864383</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Maška</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Daněk</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Garasa</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Rouzaut</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Muñoz-Barrutia</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ortiz-de-Solorzano</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Segmentation and shape tracking of whole fluorescent cells based on the Chan-Vese model</article-title>
        <source>IEEE Trans Med Imaging</source>
        <year>2013</year>
        <volume>32</volume>
        <fpage>995</fpage>
        <lpage>1006</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2013.2243463</pub-id>
        <pub-id pub-id-type="pmid">23372077</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Phillips</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Schürch</surname>
            <given-names>CM</given-names>
          </name>
          <name>
            <surname>Khodadoust</surname>
            <given-names>MS</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>YH</given-names>
          </name>
          <name>
            <surname>Nolan</surname>
            <given-names>GP</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Highly multiplexed phenotyping of immunoregulatory proteins in the tumor microenvironment by CODEX tissue imaging</article-title>
        <source>Front Immunol</source>
        <year>2021</year>
        <volume>12</volume>
        <fpage>1763</fpage>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xing</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Robust nucleus/cell detection and segmentation in digital pathology and microscopy images: a comprehensive review</article-title>
        <source>IEEE Rev Biomed Eng</source>
        <year>2016</year>
        <volume>9</volume>
        <fpage>234</fpage>
        <lpage>263</lpage>
        <pub-id pub-id-type="doi">10.1109/RBME.2016.2515127</pub-id>
        <pub-id pub-id-type="pmid">26742143</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Godinez</surname>
            <given-names>WJ</given-names>
          </name>
          <name>
            <surname>Hossain</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Lazic</surname>
            <given-names>SE</given-names>
          </name>
          <name>
            <surname>Davies</surname>
            <given-names>JW</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>A multi-scale convolutional neural network for phenotyping high-content cellular images</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>33</volume>
        <fpage>2010</fpage>
        <lpage>2019</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx069</pub-id>
        <pub-id pub-id-type="pmid">28203779</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Van Valen DA, Kudo T, Lane KM, Macklin DN, Quach NT, DeFelice MM, et al. Deep learning automates the quantitative analysis of individual cells in live-cell imaging experiments. PLOS Comput Biol. 2016;12:e1005177.</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Al-Kofahi</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zaltsman</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Graves</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Marshall</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Rusu</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>A deep learning-based algorithm for 2-D cell segmentation in microscopy images</article-title>
        <source>BMC Bioinform</source>
        <year>2018</year>
        <volume>19</volume>
        <fpage>365</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-018-2375-z</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stringer</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Michaelos</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pachitariu</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Cellpose: a generalist algorithm for cellular segmentation</article-title>
        <source>Nat Methods</source>
        <year>2021</year>
        <volume>18</volume>
        <fpage>100</fpage>
        <lpage>106</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-020-01018-x</pub-id>
        <pub-id pub-id-type="pmid">33318659</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Greenwald NF, Miller G, Moen E, Kong A, Kagel A, Dougherty T, et al. Whole-cell segmentation of tissue images with human-level performance using large-scale data annotation and deep learning. Nat Biotechnol. 2021;1–11.</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bannon</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Moen</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Schwartz</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Borba</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Kudo</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Greenwald</surname>
            <given-names>N</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>DeepCell Kiosk: scaling deep learning–enabled cellular image analysis with Kubernetes</article-title>
        <source>Nat Methods</source>
        <year>2021</year>
        <volume>18</volume>
        <fpage>43</fpage>
        <lpage>45</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-020-01023-0</pub-id>
        <pub-id pub-id-type="pmid">33398191</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hollandi</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Szkalisity</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Toth</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tasnadi</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Molnar</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Mathe</surname>
            <given-names>B</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>nucleAIzer: a parameter-free deep learning framework for nucleus segmentation using image style transfer</article-title>
        <source>Cell Syst</source>
        <year>2020</year>
        <volume>10</volume>
        <fpage>453</fpage>
        <lpage>458.e6</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cels.2020.04.003</pub-id>
        <pub-id pub-id-type="pmid">34222682</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Ronneberger</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Fischer</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Brox</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Navab</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Hornegger</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wells</surname>
            <given-names>WM</given-names>
          </name>
          <name>
            <surname>Frangi</surname>
            <given-names>AF</given-names>
          </name>
        </person-group>
        <article-title>U-Net: convolutional networks for biomedical image segmentation</article-title>
        <source>Medical image computing and computer-assisted intervention—MICCAI 2015</source>
        <year>2015</year>
        <publisher-loc>Cham</publisher-loc>
        <publisher-name>Springer</publisher-name>
        <fpage>234</fpage>
        <lpage>241</lpage>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Isensee</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Jaeger</surname>
            <given-names>PF</given-names>
          </name>
          <name>
            <surname>Kohl</surname>
            <given-names>SAA</given-names>
          </name>
          <name>
            <surname>Petersen</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Maier-Hein</surname>
            <given-names>KH</given-names>
          </name>
        </person-group>
        <article-title>nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</article-title>
        <source>Nat Methods</source>
        <year>2021</year>
        <volume>18</volume>
        <fpage>203</fpage>
        <lpage>211</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-020-01008-z</pub-id>
        <pub-id pub-id-type="pmid">33288961</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">He K, Gkioxari G, Dollár P, Girshick R. Mask R-CNN. In: 2017 IEEE international conference on computer vision (ICCV). 2017. p. 2980–8.</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">Schmidt U, Weigert M, Broaddus C, Myers G. Cell Detection with Star-convex Polygons. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/180603535">arXiv:180603535</ext-link> [cs]. 2018;11071:265–73.</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">McQuin C, Goodman A, Chernyshev V, Kamentsky L, Cimini BA, Karhohs KW, et al. CellProfiler 3.0: next-generation image processing for biology. PLOS Biol. 2018;16:e2005970.</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Berg</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kutra</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kroeger</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Straehle</surname>
            <given-names>CN</given-names>
          </name>
          <name>
            <surname>Kausler</surname>
            <given-names>BX</given-names>
          </name>
          <name>
            <surname>Haubold</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>ilastik: interactive machine learning for (bio)image analysis</article-title>
        <source>Nat Methods</source>
        <year>2019</year>
        <volume>16</volume>
        <fpage>1226</fpage>
        <lpage>1232</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-019-0582-9</pub-id>
        <pub-id pub-id-type="pmid">31570887</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Kluyver T, Ragan-Kelley B, Perez F, Granger B, Bussonier M, Frederic J, et al. Jupyter Notebooks—a publishing format for reproducible computational workflows. In: Positioning and Power in Academic Publishing: Players, Agents and Agendas. IOS Press; 2016. p. 87–90.</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Samusik</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Good</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Spitzer</surname>
            <given-names>MH</given-names>
          </name>
          <name>
            <surname>Davis</surname>
            <given-names>KL</given-names>
          </name>
          <name>
            <surname>Nolan</surname>
            <given-names>GP</given-names>
          </name>
        </person-group>
        <article-title>Automated mapping of phenotype space with single-cell data</article-title>
        <source>Nat Methods</source>
        <year>2016</year>
        <volume>13</volume>
        <fpage>493</fpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.3863</pub-id>
        <pub-id pub-id-type="pmid">27183440</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schindelin</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Arganda-Carreras</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Frise</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Kaynig</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Longair</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pietzsch</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Fiji: an open-source platform for biological-image analysis</article-title>
        <source>Nat Methods</source>
        <year>2012</year>
        <volume>9</volume>
        <fpage>676</fpage>
        <lpage>682</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.2019</pub-id>
        <pub-id pub-id-type="pmid">22743772</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, et al. TensorFlow: large-scale machine learning on heterogeneous distributed systems. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/160304467">arXiv:160304467</ext-link> [cs]. 2016.</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Caicedo</surname>
            <given-names>JC</given-names>
          </name>
          <name>
            <surname>Goodman</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Karhohs</surname>
            <given-names>KW</given-names>
          </name>
          <name>
            <surname>Cimini</surname>
            <given-names>BA</given-names>
          </name>
          <name>
            <surname>Ackerman</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Haghighi</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Nucleus segmentation across imaging experiments: the 2018 Data Science Bowl</article-title>
        <source>Nat Methods</source>
        <year>2019</year>
        <volume>16</volume>
        <fpage>1247</fpage>
        <lpage>1253</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-019-0612-7</pub-id>
        <pub-id pub-id-type="pmid">31636459</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Abdulla W. Mask R-CNN for object detection and instance segmentation on Keras and TensorFlow. 2017.</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <mixed-citation publication-type="other">Lopez-Urrutia A. Deep Retina 3th place solution to Kaggle’s 2018 Data Science Bowl. GitHub.</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Jung AB. <ext-link ext-link-type="uri" xlink:href="https://imgaug.readthedocs.io/en/latest/">https://imgaug.readthedocs.io/en/latest/</ext-link>. Imgaug. 2019. <ext-link ext-link-type="uri" xlink:href="https://imgaug.readthedocs.io/en/latest/">https://imgaug.readthedocs.io/en/latest/</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Lin</surname>
            <given-names>T-Y</given-names>
          </name>
          <name>
            <surname>Maire</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Belongie</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Hays</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Perona</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Ramanan</surname>
            <given-names>D</given-names>
          </name>
          <etal/>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Fleet</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Pajdla</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Schiele</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Tuytelaars</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Microsoft COCO: common objects in context</article-title>
        <source>Computer vision—ECCV 2014</source>
        <year>2014</year>
        <publisher-loc>Cham</publisher-loc>
        <publisher-name>Springer</publisher-name>
        <fpage>740</fpage>
        <lpage>755</lpage>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <mixed-citation publication-type="other">Wickham H. ggplot2: Elegant graphics for data analysis. 2nd edition. Springer; 2016.</mixed-citation>
    </ref>
  </ref-list>
</back>
