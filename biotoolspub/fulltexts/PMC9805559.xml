<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9805559</article-id>
    <article-id pub-id-type="pmid">36342203</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btac720</article-id>
    <article-id pub-id-type="publisher-id">btac720</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Paper</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Gene Expression</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Vaeda computationally annotates doublets in single-cell RNA sequencing data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Schriever</surname>
          <given-names>Hannah</given-names>
        </name>
        <aff><institution>Department of Developmental Biology, University of Pittsburgh</institution>, Pittsburgh, PA 15201, <country country="US">USA</country></aff>
        <aff><institution>Canegie Mellon—University of Pittsburgh Joint PhD Program, University of Pittsburgh</institution>, Pittsburgh, PA 15201, <country country="US">USA</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-1460-5487</contrib-id>
        <name>
          <surname>Kostka</surname>
          <given-names>Dennis</given-names>
        </name>
        <aff><institution>Department of Developmental Biology, University of Pittsburgh</institution>, Pittsburgh, PA 15201, <country country="US">USA</country></aff>
        <aff><institution>Department of Computational &amp; Systems Biology and Center for Evolutionary Biology and Medicine, University of Pittsburgh</institution>, Pittsburgh, PA 15201, <country country="US">USA</country></aff>
        <xref rid="btac720-cor1" ref-type="corresp"/>
        <!--kostka@pitt.edu-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Mathelier</surname>
          <given-names>Anthony</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btac720-cor1">To whom correspondence should be addressed. Email: <email>kostka@pitt.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>1</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2022-11-07">
      <day>07</day>
      <month>11</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>07</day>
      <month>11</month>
      <year>2022</year>
    </pub-date>
    <volume>39</volume>
    <issue>1</issue>
    <elocation-id>btac720</elocation-id>
    <history>
      <date date-type="received">
        <day>15</day>
        <month>6</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>23</day>
        <month>10</month>
        <year>2022</year>
      </date>
      <date date-type="editorial-decision">
        <day>26</day>
        <month>10</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>05</day>
        <month>11</month>
        <year>2022</year>
      </date>
      <date date-type="corrected-typeset">
        <day>17</day>
        <month>11</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btac720.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Single-cell RNA sequencing (scRNA-seq) continues to expand our knowledge by facilitating the study of transcriptional heterogeneity at the level of single cells. Despite this technology’s utility and success in biomedical research, technical artifacts are present in scRNA-seq data. Doublets/multiplets are a type of artifact that occurs when two or more cells are tagged by the same barcode, and therefore they appear as a single cell. Because this introduces non-existent transcriptional profiles, doublets can bias and mislead downstream analysis. To address this limitation, computational methods to annotate and remove doublets form scRNA-seq datasets are needed.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We introduce vaeda (Variational Auto-Encoder for Doublet Annotation), a new approach for computational annotation of doublets in scRNA-seq data. Vaeda integrates a variational auto-encoder and Positive-Unlabeled learning to produce doublet scores and binary doublet calls. We apply vaeda, along with seven existing doublet annotation methods, to 16 benchmark datasets and find that vaeda performs competitively in terms of doublet scores and doublet calls. Notably, vaeda outperforms other python-based methods for doublet annotation. Altogether, vaeda is a robust and competitive method for scRNA-seq doublet annotation and may be of particular interest in the context of python-based workflows.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>Vaeda is available at <ext-link xlink:href="https://github.com/kostkalab/vaeda" ext-link-type="uri">https://github.com/kostkalab/vaeda</ext-link>, and the version used for the results we present here is archived at zenodo (<ext-link xlink:href="https://doi.org/10.5281/zenodo.7199783" ext-link-type="uri">https://doi.org/10.5281/zenodo.7199783</ext-link>).</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>University of Pittsburgh School of Medicine</institution>
          </institution-wrap>
        </funding-source>
        <award-id>T32 5T32EB009403-13</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Institute of Heath</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>NIH</institution>
            <institution-id institution-id-type="DOI">10.13039/100000002</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Institute of Biomedical Imaging and Bioengineering</institution>
            <institution-id institution-id-type="DOI">10.13039/100000070</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>NIBIB</institution>
            <institution-id institution-id-type="DOI">10.13039/100000070</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="8"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Single-cell RNA sequencing (scRNA-seq) continues to impact our understanding of diverse biomedical domains by providing high-resolution gene expression measurements at scale. Resulting datasets often comprise many thousands of cells or more; while they provide valuable insights, they are also limited by technical artifacts, like doublets/multiplets. Doublets or multiplets occur when two or more cells receive the same identifier during library construction and thus appear as one single cell. As such, doublets introduce nonexistent expression profiles, which can lead to incorrect interpretation of data in downstream analysis. While there are experimental techniques (<xref rid="btac720-B9" ref-type="bibr">McGinnis <italic toggle="yes">et al.</italic>, 2019b</xref>; <xref rid="btac720-B13" ref-type="bibr">Stoeckius <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btac720-B20" ref-type="bibr">Zheng <italic toggle="yes">et al.</italic>, 2017</xref>) that identify and annotate doublets, these methods are currently not typically employed (reasons include an increased experimental burden), and they are not available for pre-existing datasets.</p>
    <p>Therefore, computational methods to identify doublets are needed. Current methods that address this challenge include the R packages scDblFinder (<xref rid="btac720-B3" ref-type="bibr">Germain <italic toggle="yes">et al.</italic>, 2021</xref>), doubletFinder (<xref rid="btac720-B8" ref-type="bibr">McGinnis <italic toggle="yes">et al.</italic>, 2019a</xref>), the cxds, bcds, and hybrid methods of the scds approach (<xref rid="btac720-B1" ref-type="bibr">Bais and Kostka, 2020</xref>), the python package scrublet (<xref rid="btac720-B16" ref-type="bibr">Wolock <italic toggle="yes">et al.</italic>, 2019</xref>) and solo (<xref rid="btac720-B2" ref-type="bibr">Bernstein <italic toggle="yes">et al.</italic>, 2020</xref>), which is used from the command line. Many of these methods share core concepts in their approach, for example generating artificial doublets from observed data, which then form the basis of deriving doublet scores for computational doublet detection. Here, we propose vaeda (Variational Auto-Encoder for Doublet Annotation), a new python-based computational approach with a similar paradigm to existing methods; vaeda combines variational autoencoders [VAEs] and Positive-Unlabeled Learning [PU-Learning, <xref rid="btac720-B7" ref-type="bibr">Liu <italic toggle="yes">et al.</italic> (2003)</xref> and <xref rid="btac720-B11" ref-type="bibr">Mordelet and Vert (2014)</xref>] to annotate doublets in scRNA-seq data. Similar to solo, vaeda uses a VAE to derive a low-dimensional representation of the input data. PU-Learning, on the other hand, is a learning framework designed for instances where there is a set of positively labeled examples and a set of unlabeled examples. In the context of doublet detection, the unlabeled examples are input data, while the examples with labels are artificially generated doublets. Therefore, PU-Learning appears well-suited for doublet detection; however, to our knowledge, while PU-Learning has been used to identify cell-free droplets in scRNA-seq data by <xref rid="btac720-B19" ref-type="bibr">Yan <italic toggle="yes">et al.</italic> (2021)</xref>, it has not been used to identify doublets.</p>
    <p>In the following, we present the vaeda method in detail; we also apply it to 16 benchmark datasets with experimental doublet annotation (<xref rid="btac720-B17" ref-type="bibr">Xi and Li, 2021a</xref>) and show that vaeda can accurately annotate doublets in scRNA-seq data, performing well when compared to seven existing methods. Particularly, vaeda integrates seamlessly into the scanpy (<xref rid="btac720-B15" ref-type="bibr">Wolf <italic toggle="yes">et al.</italic>, 2018</xref>) workflow and drastically outperforms scrublet, the only other python-based doublet detection method. Furthermore, we assess robustness of performance for all methods we study, and overall, we observe meaningful differences in how well the methods’ doublet scores correlate with experimental doublet annotations. Nevertheless, we also find that performance differences of binary doublet calls are less pronounced. This is of particular interest, because in practice, binary doublet calls are what enable researchers to remove artifacts and improve their data quality.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Vaeda method for doublet annotation</title>
      <p>The vaeda method for doublet annotation consists of several steps summarized in <xref rid="btac720-F1" ref-type="fig">Figure 1</xref>. In the following, we describe each step in more detail.</p>
      <fig position="float" id="btac720-F1">
        <label>Fig. 1.</label>
        <caption>
          <p>Summary of the vaeda method. Input cells <bold><italic toggle="yes">X</italic></bold> are subjected to data augmentation, where artificial doublets are simulated, a preliminary doublet score <bold>s</bold> is derived, and an augmented dataset <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is created. Next, a low-dimensional representation <bold><italic toggle="yes">Z</italic></bold> of <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is derived, using a cluster-aware variational autoencoder. Finally, positive unlabeled learning is used to derive final doublet scores <italic toggle="yes">ξ</italic> for each input cell/barcode</p>
        </caption>
        <graphic xlink:href="btac720f1" position="float"/>
      </fig>
      <sec>
        <title>2.1.1 Input data, doublet simulation and gene selection</title>
        <p>The input data for vaeda are raw, un-normalized count matrix <italic toggle="yes">X</italic> with <italic toggle="yes">n</italic> rows (one per cell barcode, encompassing singlets and doublets/multiplets) and <italic toggle="yes">p</italic> columns (one per gene). These data are then used to simulate artificial doublets as follows: An index pair (<italic toggle="yes">i</italic>, <italic toggle="yes">j</italic>) is sampled randomly and a doublet precursor <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is created by adding the corresponding rows of <italic toggle="yes">X</italic>: <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Index pairs are retained for ‘homotypic’ simulated doublet exclusion after clustering. Next, to generate an artificial doublet <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, its library size <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mi>ℓ</mml:mi></mml:math></inline-formula> (i.e. the number of counts) is determined as a random sample of all library sizes in <italic toggle="yes">X</italic> (i.e. the row sums of <italic toggle="yes">X</italic>) that are at least as large as the larger library size of <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Then, <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is determined as <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:msub><mml:msub><mml:mrow/><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. With this approach, we create a count matrix <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula> of <italic toggle="yes">n</italic> simulated doublets, and vaeda proceeds with the augmented matrix <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> obtained by stacking <italic toggle="yes">X</italic> and <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula> (and scaling and centering columns), and associated labels <bold>y</bold> indicating simulated doublets:
<disp-formula id="E1"><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>X</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>X</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:munder><mml:munder><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo stretchy="false">︸</mml:mo></mml:munder><mml:mi>n</mml:mi></mml:munder><mml:mo>,</mml:mo><mml:munder><mml:munder><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">︸</mml:mo></mml:munder><mml:mi>n</mml:mi></mml:munder><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>Finally, genes (i.e. columns) of <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> are selected by: (a) removing columns (i.e. genes) that had zero values for more than 99% of rows (i.e. cells) and then (b) focusing on the <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> most variable columns (we choose <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mn>2000</mml:mn></mml:mrow></mml:math></inline-formula>), where variability is defined by variation. Overall this procedure yields an initial augmented expression matrix <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>.</p>
      </sec>
      <sec>
        <title>2.1.2 Adjusting the number of simulated doublets</title>
        <p>Next, to adjust the number of simulated doublets in the augmented count matrix, vaeda uses the following approach. First, an initial estimate for the number of doublets present in the input data is derived. To that end, the augmented data <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is projected on its first 30 principal components. Next, a <italic toggle="yes">k</italic> nearest neighbor (knn) graph is constructed, where we set <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula>. Then, for each cell the fraction of its nearest neighbors that are simulated doublets is calculated as a (preliminary) doublet score <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>. A score cutoff <italic toggle="yes">c</italic> is determined as the 25% quantile of simulated doublets’ scores. Finally, the number of doublets <italic toggle="yes">n<sub>d</sub></italic> in the original data is estimated as <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mi>d</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mn>1</mml:mn><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>≥</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, the number of input cells with scores equal or larger than the cutoff.</p>
        <p>Second, <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mi>d</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula> cells are randomly selected from simulated doublets by sampling without replacement, where the sampling probability for doublet <italic toggle="yes">k</italic> is given by <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. This results in <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <bold>y</bold> consisting of <italic toggle="yes">n</italic> zeros and <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> ones.</p>
      </sec>
      <sec>
        <title>2.1.3 Low-dimensional representation by cluster-aware variational auto-encoding</title>
        <p>We derive a low-dimensional representation of the augmented data using a cluster-aware autoencoder; this representation will then in turn form the basis of doublet annotation.</p>
        <p><italic toggle="yes">Clustering:</italic> First, we group cells in the augmented dataset <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> into clusters, using the Leiden algorithm (<xref rid="btac720-B14" ref-type="bibr">Traag <italic toggle="yes">et al.</italic>, 2019</xref>). Specifically, the rows of <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> are centered and scaled to unit variance. After scaling, <inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is then projected onto its first 30 principal components. Next, for small datasets (1000 cells or less), a neighborhood graph (<xref rid="btac720-B10" ref-type="bibr">McInnes <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btac720-B15" ref-type="bibr">Wolf <italic toggle="yes">et al.</italic>, 2018</xref>) is computed followed by Leiden clustering. For larger datasets, we pre-cluster the projected data using mini-batch k-means [similar to <xref rid="btac720-B4" ref-type="bibr">Hicks <italic toggle="yes">et al.</italic> (2021)</xref>, with <italic toggle="yes">k</italic> set to 10% of the number of cells] and generate meta-cells (i.e. cluster centers). Again, the first 30 principal components of meta-cells are used to compute a neighborhood graph, followed by Leiden clustering. This step creates cell annotations <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> with <italic toggle="yes">c<sub>i</sub></italic> = <italic toggle="yes">k</italic> if cell <italic toggle="yes">i</italic> is assigned to cluster <italic toggle="yes">k</italic>.</p>
        <p><italic toggle="yes">Homotypic doublet exclusion:</italic> Next, we remove ‘homotypic’ simulated doublets, which are defined as simulated doublets that are comprised of cells from the same cluster. We use index pairs from the doublet simulation step to determine which simulated doublets to exclude. Specifically, doublet <italic toggle="yes">x<sub>d</sub></italic> is considered a ‘homotypic’ simulated doublet if for the index pair (<italic toggle="yes">i</italic>, <italic toggle="yes">j</italic>) comprising <italic toggle="yes">x<sub>d</sub></italic>, we have <italic toggle="yes">c<sub>i</sub></italic> = <italic toggle="yes">c<sub>j</sub></italic>.</p>
        <p><italic toggle="yes">Cluster-aware autoencoder:</italic> The cluster-aware autoencoder then takes <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> (log-transformed and scaled as described previously) and <bold>c</bold> as input and consists of an encoder network, a decoder network, and a cluster classifier. Let an input instance (i.e. cell and label) be denoted by <inline-formula id="IE31"><mml:math id="IM31" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p>
        <p>The encoder network consists of an input layer (<inline-formula id="IE32"><mml:math id="IM32" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> neurons), followed by a dense layer with 256 neurons, batch normalization and dropout (rate = 0.3) layers, and a tensor flow probability layer parameterizing a <italic toggle="yes">d</italic>-dimensional Normal distribution with diagonal covariance matrix (we use <italic toggle="yes">d </italic>=<italic toggle="yes"> </italic>5); <italic toggle="yes">d</italic> is the dimension of the latent space representation of the input data.</p>
        <p>The decoder network consists of an input layer (<italic toggle="yes">d</italic> neurons), a dense layer of 256 neurons, followed batch normalization, dropout (rate = 0.3) layer, and a tensor flow probability layer parameterizing a <italic toggle="yes">p</italic>-dimensional Normal distribution with diagonal covariance matrix, modeling the input data, <inline-formula id="IE33"><mml:math id="IM33" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>.</p>
        <p>The cluster classifier consists of an input layer (<italic toggle="yes">d</italic> neurons), batch normalization and a fully connected layer with the number neurons equal to the number of clusters present and with <monospace>softmax</monospace> activation, modeling each cell’s cluster assignment. Let the cluster classifier’s class assignments be denoted by <inline-formula id="IE34"><mml:math id="IM34" display="inline" overflow="scroll"><mml:mrow><mml:mo>ζ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, and let ϑ denote the model’s trainable parameters.</p>
        <p>The loss function of vaeda is defined as:
<disp-formula id="E2"><mml:math id="M2" display="block" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>ϑ</mml:mo><mml:mo>|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mo>μ</mml:mo></mml:mrow><mml:mi>d</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mo>μ</mml:mo></mml:mrow><mml:mi>e</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi>e</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∥</mml:mo><mml:mi mathvariant="normal">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mo>+</mml:mo><mml:mo> </mml:mo><mml:mo>β</mml:mo><mml:mo>·</mml:mo><mml:mtext>CCE</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mo>ζ</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where the first two terms represent the loss function of the auto-encoder, and the third term is the classification loss of the cluster classifier (categorical cross entropy). Specifically, <inline-formula id="IE35"><mml:math id="IM35" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>μ</mml:mo></mml:mrow><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:mo>ϑ</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE36"><mml:math id="IM36" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:mo>ϑ</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> represent output of the decoder’s final layer and are parameters of a Normal distribution modeling the input data, while <inline-formula id="IE37"><mml:math id="IM37" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>μ</mml:mo></mml:mrow><mml:mi>e</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:mo>ϑ</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE38"><mml:math id="IM38" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi>e</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:mo>ϑ</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> represent output of the encoder network’s final layer. Therefore, the first term denotes the negative log likelihood of the input data (=reconstruction error), while the second term is the Kullback–Leibler divergence between the (probabilistic) low-dimensional representation of the input and a standard Normal distribution of appropriate dimensions (=regularization). CCE is the categorical cross entropy between the input’s cluster label and the cluster classifier’s output <inline-formula id="IE39"><mml:math id="IM39" display="inline" overflow="scroll"><mml:mrow><mml:mo>ζ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:mo>ϑ</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, and <italic toggle="yes">β</italic> is a parameter adjusting the scale between autoencoder loss and classifier loss (we set <inline-formula id="IE40"><mml:math id="IM40" display="inline" overflow="scroll"><mml:mrow><mml:mo>β</mml:mo><mml:mo>=</mml:mo><mml:mn>20</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:math></inline-formula>). Vaeda is trained using the Adamax optimizer with default options. After the third epoch, the learning rate (0.001) decays at a rate of 0.75. Ten percent of input data is used as a validation set and training stops if validation loss has not improved for 20 epochs. Moving forward, we use the auto-encoder’s low-dimensional representation of the input data <inline-formula id="IE41"><mml:math id="IM41" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> which we denote <inline-formula id="IE42"><mml:math id="IM42" display="inline" overflow="scroll"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>.</p>
      </sec>
      <sec>
        <title>2.1.4 Doublet scoring by PU learning</title>
        <p>To score cells as potential doublets we use a positive unlabeled learning approach, with the rationale that doublet labels on the input data are not observed (unlabeled), whereas we have positive labels on the simulated doublets in the augmented, reduced-dimensional dataset <inline-formula id="IE43"><mml:math id="IM43" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Before we apply bagging PU learning, we augment <inline-formula id="IE44"><mml:math id="IM44" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> by appending preliminary doublet scores <bold>s</bold> we used for the initial estimate of the number of doublets (<inline-formula id="IE45"><mml:math id="IM45" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>), so that we finally use
<disp-formula id="E3"><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>Z</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>Z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula>for doublet annotation. The PU bagging approach we use is summarized in Algorithm 1, which is adapted from the original publication (<xref rid="btac720-B11" ref-type="bibr">Mordelet and Vert, 2014</xref>). Unlabeled examples <inline-formula id="IE46"><mml:math id="IM46" display="inline" overflow="scroll"><mml:mi mathvariant="script">U</mml:mi></mml:math></inline-formula> are the first <italic toggle="yes">n</italic> rows of <inline-formula id="IE47"><mml:math id="IM47" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>Z</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, whereas the positive examples <inline-formula id="IE48"><mml:math id="IM48" display="inline" overflow="scroll"><mml:mi mathvariant="script">P</mml:mi></mml:math></inline-formula> are the last <inline-formula id="IE49"><mml:math id="IM49" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> rows of <inline-formula id="IE50"><mml:math id="IM50" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>Z</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>. As classifier <italic toggle="yes">f</italic>(<italic toggle="yes">x</italic>), we use logistic regression, implemented as a neural network with an input layer (<italic toggle="yes">d </italic>+<italic toggle="yes"> </italic>1 neurons), a batch normalization layer, and a dense output layer with sigmoid activation. We determine the number of epochs for training as follows: The network is trained on the first fold for 250 epochs; then the <monospace>KneeLocator</monospace> function of the <monospace>kneed</monospace> python module (<xref rid="btac720-B12" ref-type="bibr">Satopaa <italic toggle="yes">et al.</italic>, 2011</xref>) is used to find an inflection point in the loss curve, which is then used to determine the number of training epochs for all folds. Scores <inline-formula id="IE51"><mml:math id="IM51" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mo>ξ</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> returned by PU bagging for the rows of <inline-formula id="IE52"><mml:math id="IM52" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>Z</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> that represent input cells are the doublet scores reported by the vaeda method.</p>
        <p>
          <boxed-text id="btac720-BOX1" position="float">
            <label>Algorithm 1:</label>
            <caption>
              <p>PU learning, adapted from Mordelet and Vert (2014)</p>
            </caption>
            <p><bold>INPUT:</bold> <inline-formula id="IE53"><mml:math id="IM53" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">P</mml:mi><mml:mo>=</mml:mo><mml:mtext>positive</mml:mtext><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mtext>examples</mml:mtext><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mi mathvariant="script">U</mml:mi><mml:mo>=</mml:mo><mml:mtext>unlabeled</mml:mtext><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mtext>examples</mml:mtext><mml:mo>,</mml:mo></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mtext>number</mml:mtext><mml:mo> </mml:mo><mml:mtext>of</mml:mtext><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mtext>repetitions</mml:mtext><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mtext>number of folds</mml:mtext></mml:mrow></mml:math></inline-formula></p>
            <p>
              <bold>OUPUT:</bold>
              <inline-formula id="IE54">
                <mml:math id="IM54" display="inline" overflow="scroll">
                  <mml:mrow>
                    <mml:mo> </mml:mo>
                    <mml:mi mathvariant="normal">A</mml:mi>
                    <mml:mo> </mml:mo>
                    <mml:mtext>score</mml:mtext>
                    <mml:mo> </mml:mo>
                    <mml:mi mathvariant="script">U</mml:mi>
                    <mml:mo>→</mml:mo>
                    <mml:mi mathvariant="double-struck">R</mml:mi>
                    <mml:mo>∋</mml:mo>
                    <mml:mo>ξ</mml:mo>
                  </mml:mrow>
                </mml:math>
              </inline-formula>
            </p>
            <p>
              <bold>Initialize: for</bold>
              <inline-formula id="IE55">
                <mml:math id="IM55" display="inline" overflow="scroll">
                  <mml:mrow>
                    <mml:mi>x</mml:mi>
                    <mml:mo>∈</mml:mo>
                    <mml:mi mathvariant="script">U</mml:mi>
                  </mml:mrow>
                </mml:math>
              </inline-formula>
              <bold>do</bold>
            </p>
            <p>
              <inline-formula id="E4">
                <mml:math id="M4" display="inline" overflow="scroll">
                  <mml:mrow>
                    <mml:mi> n</mml:mi>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mi>x</mml:mi>
                    <mml:mo stretchy="false">)</mml:mo>
                    <mml:mo>←</mml:mo>
                    <mml:mn>0</mml:mn>
                  </mml:mrow>
                </mml:math>
              </inline-formula>
            </p>
            <p>
              <inline-formula id="E5">
                <mml:math id="M5" display="inline" overflow="scroll">
                  <mml:mrow>
                    <mml:mi> f</mml:mi>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mi>x</mml:mi>
                    <mml:mo stretchy="false">)</mml:mo>
                    <mml:mo>←</mml:mo>
                    <mml:mn>0</mml:mn>
                  </mml:mrow>
                </mml:math>
              </inline-formula>
            </p>
            <p>
              <bold>end</bold>
            </p>
            <p>
              <bold>for</bold>
              <inline-formula id="IE56">
                <mml:math id="IM56" display="inline" overflow="scroll">
                  <mml:mrow>
                    <mml:mi>n</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mn>1</mml:mn>
                    <mml:mo>…</mml:mo>
                    <mml:mi>N</mml:mi>
                  </mml:mrow>
                </mml:math>
              </inline-formula>
              <bold>do</bold>
            </p>
            <p> Randomly split <inline-formula id="IE57"><mml:math id="IM57" display="inline" overflow="scroll"><mml:mi mathvariant="script">U</mml:mi></mml:math></inline-formula> into <italic toggle="yes">K</italic> folds <inline-formula id="IE58"><mml:math id="IM58" display="inline" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula></p>
            <p> <bold>for</bold><inline-formula id="IE59"><mml:math id="IM59" display="inline" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>…</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula><bold>do</bold></p>
            <p>  Train classifier <italic toggle="yes">f<sub>k</sub></italic> to discriminate <inline-formula id="IE60"><mml:math id="IM60" display="inline" overflow="scroll"><mml:mi mathvariant="script">P</mml:mi></mml:math></inline-formula> against <inline-formula id="IE61"><mml:math id="IM61" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></p>
            <p>  <bold>Update: for</bold><inline-formula id="IE62"><mml:math id="IM62" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">U</mml:mi><mml:mo>∖</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula><bold>do</bold></p>
            <p>
              <inline-formula id="E6">
                <mml:math id="M6" display="inline" overflow="scroll">
                  <mml:mrow>
                    <mml:mi>   f</mml:mi>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mi>x</mml:mi>
                    <mml:mo stretchy="false">)</mml:mo>
                    <mml:mo>←</mml:mo>
                    <mml:mi>f</mml:mi>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mi>x</mml:mi>
                    <mml:mo stretchy="false">)</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>f</mml:mi>
                      </mml:mrow>
                      <mml:mi>k</mml:mi>
                    </mml:msub>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mi>x</mml:mi>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                </mml:math>
              </inline-formula>
            </p>
            <p>
              <inline-formula id="E7">
                <mml:math id="M7" display="inline" overflow="scroll">
                  <mml:mi>   n</mml:mi>
                  <mml:mo>(</mml:mo>
                  <mml:mi>x</mml:mi>
                  <mml:mo>)</mml:mo>
                  <mml:mo>←</mml:mo>
                  <mml:mi>n</mml:mi>
                  <mml:mo>(</mml:mo>
                  <mml:mi>x</mml:mi>
                  <mml:mo>)</mml:mo>
                  <mml:mo>+</mml:mo>
                  <mml:mn>1</mml:mn>
                </mml:math>
              </inline-formula>
            </p>
            <p>  <bold>end</bold></p>
            <p> <bold>end</bold></p>
            <p>
              <bold>end</bold>
            </p>
            <p><bold>Return:</bold> score <inline-formula id="IE63"><mml:math id="IM63" display="inline" overflow="scroll"><mml:mrow><mml:mo>ξ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>←</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo> </mml:mo><mml:mtext>for</mml:mtext><mml:mo> </mml:mo><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">U</mml:mi></mml:mrow></mml:math></inline-formula></p>
          </boxed-text>
        </p>
      </sec>
      <sec>
        <title>2.1.5 Doublet calling</title>
        <p>In addition to doublet scores, vaeda also provides doublet calls as a binary prediction for whether a cell is a singlet or a doublet. Vaeda generates these calls by selecting a threshold <inline-formula id="IE64"><mml:math id="IM64" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>⋆</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> where all cells scoring above this threshold are called as doublets and all cells scoring below this threshold are called as singlets (e.g. <xref rid="btac720-F2" ref-type="fig">Fig. 2B</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S3</xref>). The threshold is selected by the following minimization problem
<disp-formula id="E8"><mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>⋆</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>argmin</mml:mtext></mml:mrow><mml:mi>t</mml:mi></mml:munder><mml:mtext>FNR</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mtext>FPR</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo>α</mml:mo><mml:mo>·</mml:mo><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mo>μ</mml:mo><mml:mo>,</mml:mo><mml:mo>σ</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE65"><mml:math id="IM65" display="inline" overflow="scroll"><mml:mrow><mml:mtext>FNR</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the fraction of simulated doublets called singlets at threshold <italic toggle="yes">t</italic> (<inline-formula id="IE66"><mml:math id="IM66" display="inline" overflow="scroll"><mml:mo>≈</mml:mo></mml:math></inline-formula> false negative rate), <inline-formula id="IE67"><mml:math id="IM67" display="inline" overflow="scroll"><mml:mrow><mml:mtext>FPR</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the fraction of input cells called doublets at threshold <italic toggle="yes">t</italic> (<inline-formula id="IE68"><mml:math id="IM68" display="inline" overflow="scroll"><mml:mo>≈</mml:mo></mml:math></inline-formula> false positive rate), and <inline-formula id="IE69"><mml:math id="IM69" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the number of input cells called doublets at threshold <italic toggle="yes">t</italic>. <inline-formula id="IE70"><mml:math id="IM70" display="inline" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mo>μ</mml:mo><mml:mo>,</mml:mo><mml:mo>σ</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the (Gaussian) log-likelihood of observing <inline-formula id="IE71"><mml:math id="IM71" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> doublets given an expected number of doublets <italic toggle="yes">μ</italic> with standard deviation <italic toggle="yes">σ</italic>. If <italic toggle="yes">μ</italic> is not provided by the user, we use <inline-formula id="IE72"><mml:math id="IM72" display="inline" overflow="scroll"><mml:mrow><mml:mo>μ</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>·</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, motivated by the heuristic that <inline-formula id="IE73"><mml:math id="IM73" display="inline" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi><mml:mo>·</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is a good approximation for the fraction of doublets in a dataset (e.g. see <xref rid="btac720-B3" ref-type="bibr">Germain <italic toggle="yes">et al.</italic>, 2021</xref>). Via a binomial model, we arrive at a variance of <inline-formula id="IE74"><mml:math id="IM74" display="inline" overflow="scroll"><mml:mrow><mml:mo>μ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mo>μ</mml:mo><mml:mo>/</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> for the number of doublets, which determines the parameters of the log-likelihood term. We square the log likelihood to flatten its minimum, and the parameter <italic toggle="yes">α</italic> controls a trade-off between misclassification of simulated doublets and the number of expected doublets (we set <inline-formula id="IE75"><mml:math id="IM75" display="inline" overflow="scroll"><mml:mrow><mml:mo>α</mml:mo><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">max</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mo>μ</mml:mo><mml:mo>,</mml:mo><mml:mo>σ</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">t</italic><sub>max</sub> is the largest possible threshold on the input data). We note that this approach is similar to that of <xref rid="btac720-B3" ref-type="bibr">Germain <italic toggle="yes">et al.</italic> (2021)</xref>; however, the expected number of doublets is incorporated differently in our approach.</p>
        <fig position="float" id="btac720-F2">
          <label>Fig. 2.</label>
          <caption>
            <p>Vaeda’s latent representation preserves doublet annotation information and vaeda’s caller correlates with actual doublet fractions. (<bold>A</bold>) The performance of a knn classifier predicting annotated doublets on the input data (left) and on vaeda’s latent representation (right) is shown. (<bold>B</bold>) The cost function vaeda minimizes for doublet calling for the HMEC-rep-MULTI dataset is shown. (<bold>C</bold>) Vaeda-estimated doublets on the x-axis and experimentally annotated doublets on the y-axis for 16 benchmark datasets is shown. The line <italic toggle="yes">y </italic>=<italic toggle="yes"> x</italic> is shown in black. (<bold>D</bold>) UMAP density plots of four datasets (rows) with all cells, experimentally annotated singlets and doublets (columns two and three), as well as vaeda-simulated doublets (column four) are shown. Darker areas denote regions of higher cell density</p>
          </caption>
          <graphic xlink:href="btac720f2" position="float"/>
        </fig>
      </sec>
    </sec>
    <sec>
      <title>2.2 Benchmark data and application of existing methods</title>
      <p>Scrublet (<xref rid="btac720-B16" ref-type="bibr">Wolock <italic toggle="yes">et al.</italic>, 2019</xref>), scDblFinder (<xref rid="btac720-B3" ref-type="bibr">Germain <italic toggle="yes">et al.</italic>, 2021</xref>), doubletFinder (<xref rid="btac720-B8" ref-type="bibr">McGinnis <italic toggle="yes">et al.</italic>, 2019a</xref>), hybrid, bcds, and cxds (<xref rid="btac720-B1" ref-type="bibr">Bais and Kostka, 2020</xref>) were run with default parameters using code from the R package DoubletCollection (<xref rid="btac720-B18" ref-type="bibr">Xi and Li, 2021b</xref>). These methods also provide binary doublet calls in addition to continuous doublet scores, so we modified the package to access these calls for our doublet call analysis. Solo was run via the command line using parameters provided in the file <monospace>model_json</monospace> on solo’s github page (<ext-link xlink:href="https://github.com/calico/solo" ext-link-type="uri">https://github.com/calico/solo</ext-link>). We use 16 datasets from the benchmark study of <xref rid="btac720-B17" ref-type="bibr">Xi and Li (2021a</xref>), downloaded from Zenodo (<ext-link xlink:href="https://zenodo.org/record/4062232#.X3YR9Hn0kuU" ext-link-type="uri">https://zenodo.org/record/4062232#.X3YR9Hn0kuU</ext-link>).</p>
      <p>The benchmarking datasets pbmc-ch, cline-ch (<xref rid="btac720-B13" ref-type="bibr">Stoeckius <italic toggle="yes">et al.</italic>, 2018</xref>), mkidney-ch (<xref rid="btac720-B2" ref-type="bibr">Bernstein <italic toggle="yes">et al.</italic>, 2020</xref>), hm-12k, hm-6k (<xref rid="btac720-B20" ref-type="bibr">Zheng <italic toggle="yes">et al.</italic>, 2017</xref>), pbmc-1A-dm, pbmc-1B-dm, pbmc-1C-dm, pbmc-2ctrl-dm, pbmc-2stim-dm, J293t-dm (<xref rid="btac720-B5" ref-type="bibr">Kang <italic toggle="yes">et al.</italic>, 2018</xref>), pdx-MULTI, HMEC-orig-MULTI, HMEC-rep-MULTI, HEK-HMEC-MULTI, nuc-MULTI (<xref rid="btac720-B9" ref-type="bibr">McGinnis <italic toggle="yes">et al.</italic>, 2019b</xref>) are experimentally annotated using oligo tag or genetic methods and are further summarized in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref>.</p>
    </sec>
    <sec>
      <title>2.3 Analyses with down-sampled datasets</title>
      <p>For several analyses, we randomly down-sampled datasets 10 times to contain a fraction of cells (we used 95%), while preserving the original singlet to doublet ratio. Each doublet annotation method was then run on each down-sampled dataset resulting in 10 annotation scores for each cell per method per dataset. Because of its longer running time solo was only run on five sub-samples instead of 10 (for run times, see <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S5</xref>). To test for a difference in performance between two methods on a specific dataset we then used a Wilcoxon rank-sum test. To test for performance difference across all datasets, we used a paired Wilcoxon rank-sum test is used, where the pairing takes into account systematic differences between datasets. In cases of comparing solo to other methods, we only use the five repetitions where solo was applied.</p>
      <p>To obtain a standard deviation of a performance measure averaged across datasets (e.g. <xref rid="btac720-F3" ref-type="fig">Fig. 3</xref>), we estimated standard deviations for each dataset and then used standard error propagation.</p>
      <fig position="float" id="btac720-F3">
        <label>Fig. 3.</label>
        <caption>
          <p>Vaeda performs well compared with other doublet annotations methods. (<bold>A</bold>) Average precision (AUPRC, in percent) for doublet annotation methods (columns) across datasets (rows) is shown. The row labeled ’mean’ denotes the average across datasets. A ranking of method performance is shown each dataset; lighter entries indicate worse performance, while darker entries indicate better performance in each row. (<bold>B</bold>) Average precision for repeatedly down-sampling 95% of cells across datasets for each method is shown; error bars indicate three standard deviations</p>
        </caption>
        <graphic xlink:href="btac720f3" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>2.4 Missed versus captured doublets</title>
      <p>We characterize and contrast experimentally annotated doublets we call ‘captured’ and ‘missed’ (<xref rid="btac720-F4" ref-type="fig">Fig. 4</xref>). We define a captured doublet as a doublet that is classified as a doublet by at least four (out of eight) methods and a missed doublet as a doublet that is misclassified as a singlet by all of the methods. In this analysis, we use for each method the <italic toggle="yes">m</italic> top-scoring doublets and as computationally annotated doublets, where <italic toggle="yes">m</italic> is the number of experimentally annotated doublets (i.e. we do not use the methods’ doublet callers). For each doublet, we then obtain the fraction of singlets amongst its k-nearest neighbors (<italic toggle="yes">k</italic> = 5). <xref rid="btac720-F4" ref-type="fig">Figure 4</xref> shows violin plots of these singlet fractions, stratified by missed versus captured doubles. Circles represent averages for each dataset.</p>
      <fig position="float" id="btac720-F4">
        <label>Fig. 4.</label>
        <caption>
          <p>Missed and captured doublets. (<bold>A</bold>) The number of experimentally annotated doublets, stratified by the number of methods that annotated them is shown. (<bold>B</bold>) Density plots of the fraction of singlets in a doublet’s neighborhood, stratified by whether the doubled is consistently missed, or captured is shown</p>
        </caption>
        <graphic xlink:href="btac720f4" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 Doublet detection with the vaeda method</title>
      <p>Here, we present the vaeda method for computational annotation of doublets in single-cell RNA sequencing data. Vaeda reflects other doublet annotation methods (<xref rid="btac720-B1" ref-type="bibr">Bais and Kostka, 2020</xref>; <xref rid="btac720-B2" ref-type="bibr">Bernstein <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac720-B3" ref-type="bibr">Germain <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac720-B8" ref-type="bibr">McGinnis <italic toggle="yes">et al.</italic>, 2019a</xref>; <xref rid="btac720-B16" ref-type="bibr">Wolock <italic toggle="yes">et al.</italic>, 2019</xref>), in that artificially generated doublets (we also call them ‘simulated’ doublets) are used as a means to infer actual (or ‘real’) doublets in a dataset. Conditional on simulated doublets, this approach naturally leads to a statistical learning setup discriminating artificial doublets from input data, and classification scores are then used for doublet annotation. However, real doublets are present in the input data as well. With vaeda, we explicitly account for this by viewing the learning task as a Positive-Unlabeled (PU) learning problem, where simulated data is viewed as a positive set (<italic toggle="yes">P</italic>), while input data is assumed unlabeled. This approach differs from standard classification, which implicitly assumes that no doublets are present in the input.</p>
      <p>Briefly, the vaeda method works as follows. After variable gene selection and generation of artificial doublets, clustering is performed and a cluster-aware VAE is used to learn a latent representation of input data and artificial doublets both. Learned latent projections, together with a knn-feature encapsulating the fraction of simulated doublets in each cell’s neighborhood, are then used as input for PU bagging (<xref rid="btac720-B11" ref-type="bibr">Mordelet and Vert, 2014</xref>) to derive doublet scores. Further on, vaeda can perform doublet calling. Similar to <xref rid="btac720-B3" ref-type="bibr">Germain <italic toggle="yes">et al.</italic> (2021)</xref>, vaeda balances a heuristically expected number of doublets with false positive and false negative doublet calls, as quantified by the number of misclassified simulated doublets and input cells, respectively. See Section 2 for details and <xref rid="btac720-F1" ref-type="fig">Figure 1</xref> for an overview of vaeda. In the following, we assess vaeda on 16 benchmark datasets (<xref rid="btac720-B17" ref-type="bibr">Xi and Li, 2021a</xref>) where doublets have been experimentally annotated.</p>
      <sec>
        <title>3.1.1 Ablation analysis of the vaeda method</title>
        <p>In order to evaluate the relative importance of vaeda’s different components we performed ablation analyses. Specifically, we assessed the following components of our method: (i) inclusion of the fraction of (simulated) doublets in a cell’s neighborhood into the learning problem, versus not including them; (ii) classification algorithm: knn classifier versus logistic-regression classifier; (iii) excluding simulated doublets that might be homotypic, i.e. doublets simulated by combining two cells from the same cluster; (iv) type of low-dimensional representation: pca versus variational auto-encoder versus cluster-aware variational auto-encoder; (v) PU learning versus regular classification. We measured performance for every combination of these components, and results are summarized in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S1</xref>.</p>
        <p>We find that a combination of a cluster-aware autoencoder, homotypic doublet exclusion, PU learning with a logistic regression type classifier, and including the neighborhood doublet fraction as a feature yielded the best results (average area under the precision recall curve: 55.8%, see <xref rid="btac720-F3" ref-type="fig">Fig. 3A</xref>). We also find that combinations including the neighborhood fraction of doublets and a logistic regression type classifier perform best, while methods without this feature using a logistic regression classifier perform worst. Combinations with a knn-type classifier perform in-between. Focusing on twelve high-performing combinations (i.e. with neighborhood doublet fraction and logistic regression classification), we find that combinations including PU learning perform better than those using regular classification (four of the top-six and two of the top-three combinations use PU learning). Results with regards to the low-dimensional embedding are a bit less clear; however, two of the top-three combinations use a cluster-aware vae (one uses PCA), while only one of the worst-performing combinations uses this approach for dimension reduction (the other two methods used are PCA and a regular auto-encoder). Overall this analysis motivates our design of the vaeda method. While the biggest effects came from the neighborhood doublet fraction and classifier type, we note that PU learning and the cluster-aware autoencoder increased average performance from 54.5% to 55.8%, a mild but noticeable improvement.</p>
      </sec>
      <sec>
        <title>3.1.2 Vaeda’s latent representation preserves doublet annotation</title>
        <p>Vaeda learns a latent space representation of the data by training a cluster-aware VAE. We quantitatively and qualitatively assessed how well the latent representation preserves experimental doublet annotation. First, to quantitatively assess whether this embedding preserves local label (=doublet) information, we followed the approach of <xref rid="btac720-B21" ref-type="bibr">Zhou and Troyanskaya (2021)</xref> and use a knn (<italic toggle="yes">k</italic> = 5) classifier trained using experimental doublet annotations to predict cell labels in both the input space and in the latent space. We find that label accuracy in vaeda’s latent space is 91.598% (averaged over datasets), approximately the same as in the input space (91.586%) (see <xref rid="btac720-F2" ref-type="fig">Fig. 2A</xref>). This indicates that vaeda’s latent representation accurately reflects local cell label information.</p>
        <p>Next, to qualitatively assess vaeda’s simulation of artificial doublets, we visualized simulated and experimentally annotated doublets using vaeda’s latent representation. Plots for all benchmark datasets are shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S2</xref> (examples shown in <xref rid="btac720-F2" ref-type="fig">Fig. 2D</xref>). Overall, we observe good agreement between experimentally annotated and simulated doublets. However, for some datasets (HMEC-rep-MULTI, cline-ch, mkidney-ch, pbmc-1B, and pbmc-1C), there exist doublet populations that are annotated but not covered well by simulated doublets. We also note that regions with high densities of singlets are typically distinct from high-density simulated doublet regions. Two exceptions are hm-6k and hm-12k, which both have small groups of simulated doublets overlapping annotated singlets. We note that experimental doublet annotations for these data do not consider homotypic doublets (i.e. doublets that occur when two cells of the same cell type combine). <xref rid="btac720-F2" ref-type="fig">Figure 2D</xref> shows two examples of good real-simulated doublet overlap on the left, and HMEC-rep-MULTI and cline-ch on the right.</p>
      </sec>
      <sec>
        <title>3.1.3 Vaeda’s doublet scores and doublet calling reflect experimental annotations</title>
        <p>Doublet scores produced by the vaeda method correlate well with experimental doublet annotation. Briefly, across 16 benchmark datasets, vaeda achieves an average area under the precision recall curve (AUPRC) of 55.8%, with AUPRCs for individual datasets reaching 97% and 95% (<xref rid="btac720-F3" ref-type="fig">Fig. 3A</xref>). More details about vaeda’s doublet score performance and a comparison with other methods are presented in the next section.</p>
        <p>In addition to calculating doublet scores for each cell, vaeda can also classify cells as either a doublet or a singlet as described in Section 2.1.5. Briefly, vaeda’s doublet caller simultaneously minimizes false positive rate, false negative rate, and the squared log likelihood of the predicted number of doublets given an expected number of doublets (<xref rid="btac720-F2" ref-type="fig">Fig. 2B</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S3</xref>). The expected number of doublets is derived from the observation that, across experiments, the probability of a cell being a doublet scales linearly with the total number of cells, also see Section 2.1.5. <xref rid="btac720-F2" ref-type="fig">Figure 2C</xref> shows vaeda’s doublet calling results, compared to the number of experimentally annotated doublets. The correlation between annotated and estimated doublet rates (<inline-formula id="IE76"><mml:math id="IM76" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.59</mml:mn></mml:mrow></mml:math></inline-formula>) is a noticeable improvement over using purely the expected number discussed above (<inline-formula id="IE77"><mml:math id="IM77" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.52</mml:mn></mml:mrow></mml:math></inline-formula>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S4</xref>). Further on, performance on datasets with reasonable doublet rates (less than 20%) appears more accurate, while the doublet fraction in two datasets with high doublet density (37.3% and 31.0%) is underestimated.</p>
        <p>In summary, these results demonstrate that the vaeda method is a reasonably designed method that generates accurate doublet scores for scRNA-seq data; it is able to perform meaningful classification into singlet versus doublet cells, outperforming a reasonable baseline based on cell-numbers alone. Next, we compared vaeda with other computational methods for doublet annotation.</p>
      </sec>
    </sec>
    <sec>
      <title>3.2 Comparison with other doublet annotation methods</title>
      <p>To compare vaeda to other methods, we used 16 benchmarking datasets that have previously been used to assess doublet annotation performance by <xref rid="btac720-B17" ref-type="bibr">Xi and Li (2021a</xref>). We proceed by comparing vaeda with scDblFinder, doubletFinder, hybrid, bcds, cxds, solo, and scrublet, while library size was included as a baseline. We used average precision (AUPRC) as the main performance metric, because the datasets are imbalanced (typically the fraction of doublets is low, see <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref>). Results are summarized in <xref rid="btac720-F3" ref-type="fig">Figure 3A</xref>.</p>
      <p>We find that vaeda outperforms all competing methods on four datasets; this is slightly worse than scDblFinder (five), but better than doubletFinder (two), Solo (three), cxds (three), and scrublet (one). Hybrid and bcds did not outperform all other methods on any dataset. In terms of average performance, vaeda’s AUPRC (averaged across datasets) is 55.8%, second compared with 56.4% for scDblFinder (the best method, on average) and 53% for doubletFinder (the number three method), see <xref rid="btac720-F3" ref-type="fig">Figure 3A</xref>.</p>
      <sec>
        <title>3.2.1 Vaeda performs competitively with existing methods</title>
        <p>Performance results presented in the previous section were computed using all cells in each benchmark dataset. To study if the performance differences we observed were robust and meaningful, we sub-sampled cells in each benchmark dataset repeatedly and observed the resulting empirical distribution of performance metrics. <xref rid="btac720-F3" ref-type="fig">Figure 3B</xref> shows results using repeated (ten times) 95% down-sampling in terms of average AUPRC. Examining average performance, we find that vaeda outperforms all other methods, except scDblFinder and doubletFinder. We observe a noticeable decrease in average performance between the top three methods (scDblFinder, vaeda, and doubletFinder) and other methods. Similarly, bcds, scrublet, and cxds on average perform worse than the rest. We also quantitatively assessed performance differences using Wilcoxon rank-sum tests for pairs of methods on the 95% down-sampled datasets, aggregating across all 16 datasets (see Sections 2 and 2.3); we find that (using continuous doublet scores and AUPRC) scDblFinder outperforms vaeda and vaeda outperforms doubletFinder (<xref rid="btac720-T1" ref-type="table">Table 1</xref>). However, we note that even though scDblFinder outperforms vaeda on average, vaeda achieves significantly better performance on the HEK-HMEC-MULTI, HEK-orig-MULTI, and HMEC-rep-MULTI datasets, and there are no significant differences between veada and scDblFinder on the J293t-dm, hm-6k, and hm-12k datasets (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref>).</p>
        <table-wrap position="float" id="btac720-T1">
          <label>Table 1.</label>
          <caption>
            <p>Comparison of methods aggregated across benchmark datasets</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="center" span="1"/>
              <col valign="top" align="center" span="1"/>
              <col valign="top" align="center" span="1"/>
              <col valign="top" align="center" span="1"/>
              <col valign="top" align="center" span="1"/>
              <col valign="top" align="center" span="1"/>
              <col valign="top" align="center" span="1"/>
              <col valign="top" align="center" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1"/>
                <th align="left" rowspan="1" colspan="1">scDF</th>
                <th align="left" rowspan="1" colspan="1">va</th>
                <th align="left" rowspan="1" colspan="1">DF</th>
                <th align="left" rowspan="1" colspan="1">so</th>
                <th align="left" rowspan="1" colspan="1">hb</th>
                <th align="left" rowspan="1" colspan="1">bc</th>
                <th align="left" rowspan="1" colspan="1">sc</th>
                <th align="left" rowspan="1" colspan="1">cx</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">scDblFinder (scDF)</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">scDF</td>
                <td rowspan="1" colspan="1">scDF</td>
                <td rowspan="1" colspan="1">scDF</td>
                <td rowspan="1" colspan="1">scDF</td>
                <td rowspan="1" colspan="1">scDF</td>
                <td rowspan="1" colspan="1">scDF</td>
                <td rowspan="1" colspan="1">scDF</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">vaeda (va)</td>
                <td rowspan="1" colspan="1">scDF</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">va</td>
                <td rowspan="1" colspan="1">va</td>
                <td rowspan="1" colspan="1">va</td>
                <td rowspan="1" colspan="1">va</td>
                <td rowspan="1" colspan="1">va</td>
                <td rowspan="1" colspan="1">va</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">doubletFinder (DF)</td>
                <td rowspan="1" colspan="1">scDF</td>
                <td rowspan="1" colspan="1">va</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">DF</td>
                <td rowspan="1" colspan="1">DF</td>
                <td rowspan="1" colspan="1">DF</td>
                <td rowspan="1" colspan="1">DF</td>
                <td rowspan="1" colspan="1">DF</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">solo (so)</td>
                <td rowspan="1" colspan="1">scDF</td>
                <td rowspan="1" colspan="1">va</td>
                <td rowspan="1" colspan="1">DF</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">
                  <inline-formula id="IE78">
                    <mml:math id="IM78" display="inline" overflow="scroll">
                      <mml:mo>≈</mml:mo>
                    </mml:math>
                  </inline-formula>
                </td>
                <td rowspan="1" colspan="1">so</td>
                <td rowspan="1" colspan="1">so</td>
                <td rowspan="1" colspan="1">so</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">hybrid (hb)</td>
                <td rowspan="1" colspan="1">scDF</td>
                <td rowspan="1" colspan="1">va</td>
                <td rowspan="1" colspan="1">DF</td>
                <td rowspan="1" colspan="1">
                  <inline-formula id="IE79">
                    <mml:math id="IM79" display="inline" overflow="scroll">
                      <mml:mo>≈</mml:mo>
                    </mml:math>
                  </inline-formula>
                </td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">hb</td>
                <td rowspan="1" colspan="1">hb</td>
                <td rowspan="1" colspan="1">hb</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">bcds (bc)</td>
                <td rowspan="1" colspan="1">scDF</td>
                <td rowspan="1" colspan="1">va</td>
                <td rowspan="1" colspan="1">DF</td>
                <td rowspan="1" colspan="1">so</td>
                <td rowspan="1" colspan="1">hb</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">
                  <inline-formula id="IE80">
                    <mml:math id="IM80" display="inline" overflow="scroll">
                      <mml:mo>≈</mml:mo>
                    </mml:math>
                  </inline-formula>
                </td>
                <td rowspan="1" colspan="1">bc</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">scrublet (sc)</td>
                <td rowspan="1" colspan="1">scDF</td>
                <td rowspan="1" colspan="1">va</td>
                <td rowspan="1" colspan="1">DF</td>
                <td rowspan="1" colspan="1">so</td>
                <td rowspan="1" colspan="1">hb</td>
                <td rowspan="1" colspan="1">
                  <inline-formula id="IE81">
                    <mml:math id="IM81" display="inline" overflow="scroll">
                      <mml:mo>≈</mml:mo>
                    </mml:math>
                  </inline-formula>
                </td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">
                  <inline-formula id="IE82">
                    <mml:math id="IM82" display="inline" overflow="scroll">
                      <mml:mo>≈</mml:mo>
                    </mml:math>
                  </inline-formula>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">cxds (cx)</td>
                <td rowspan="1" colspan="1">scDF</td>
                <td rowspan="1" colspan="1">va</td>
                <td rowspan="1" colspan="1">DF</td>
                <td rowspan="1" colspan="1">so</td>
                <td rowspan="1" colspan="1">hb</td>
                <td rowspan="1" colspan="1">bc</td>
                <td rowspan="1" colspan="1">
                  <inline-formula id="IE83">
                    <mml:math id="IM83" display="inline" overflow="scroll">
                      <mml:mo>≈</mml:mo>
                    </mml:math>
                  </inline-formula>
                </td>
                <td rowspan="1" colspan="1">–</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn1">
              <p><italic toggle="yes">Note</italic>: Paired Wilcoxon rank-sum tests were used identify significant (<italic toggle="yes">P</italic> ≤ 0.05) performance differences between pairs of methods using 10 95% down-samplings of each dataset. The method with the better performance is indicated. <inline-formula id="IE84"><mml:math id="IM84" display="inline" overflow="scroll"><mml:mo>≈</mml:mo></mml:math></inline-formula> implies <italic toggle="yes">P </italic>&gt;<italic toggle="yes"> </italic>0.05.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <p>We also compared all pairs of doublet detection methods stratified by dataset, using paired Wilcoxon rank-sum tests to decide ‘wins’ (<italic toggle="yes">P</italic> ≤ 0.05, higher performance), ‘ties’ (<italic toggle="yes">P </italic>&gt;<italic toggle="yes"> </italic>0.05), and ‘losses’ (<italic toggle="yes">P</italic> ≤ 0.05, lower performance). <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S8C</xref> shows the results. For each method, there are seven competitor methods and 16 datasets yielding 7 × 16 = 112 comparisons. Only scDblFinder, vaeda and doubletFinder are able to win more than half of their comparisons (83, 67, and 64, respectively). For the top-three methods (scDblFinder, vaeda, and doubletFinder), we also provide pairwise comparisons, stratified by dataset, in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref>.</p>
      </sec>
      <sec>
        <title>3.2.2 Vaeda performs competitively at doublet calling</title>
        <p>In practice, doublet annotation methods are used to filter putative/annotated doublets from scRNA-seq datasets. In addition to doublet scores, vaeda is equipped with a doublet caller that provides a binary label for this purpose. Here, we compare vaeda’s doublet calling with other methods and find that it performs competitively. Specifically, we applied vaeda along with other methods to the 16 benchmark datasets and recorded doublet calls. We then calculated the f1 score, mcc, precision, recall, and accuracy for the calls and averaged across datasets (<xref rid="btac720-T2" ref-type="table">Table 2</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S9</xref>). We see that vaeda performs well overall, with the second highest f1-score; compared to scDblFinder, vaeda appears to tradeoff recall (52.5% versus 56.9%) to gain precision (59% versus 53.7%), but coming out a little behind overall. Interestingly doubletFinder does not perform well, calling too few doublets overall (see Section 4). We also observe that there is noticeable spread in performance between different datasets (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S9</xref>).</p>
        <table-wrap position="float" id="btac720-T2">
          <label>Table 2.</label>
          <caption>
            <p>Doublet calling across 16 benchmark datasets (averaged performance)</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1"/>
                <th align="left" rowspan="1" colspan="1">f1</th>
                <th align="left" rowspan="1" colspan="1">mcc</th>
                <th align="left" rowspan="1" colspan="1">Precision</th>
                <th align="left" rowspan="1" colspan="1">Recall</th>
                <th align="left" rowspan="1" colspan="1">Accuracy</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">scDblFinder</td>
                <td rowspan="1" colspan="1">51.4</td>
                <td rowspan="1" colspan="1">47.7</td>
                <td rowspan="1" colspan="1">53.7</td>
                <td rowspan="1" colspan="1">56.8</td>
                <td rowspan="1" colspan="1">89.3</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">vaeda</td>
                <td rowspan="1" colspan="1">49.6</td>
                <td rowspan="1" colspan="1">46.8</td>
                <td rowspan="1" colspan="1">59.0</td>
                <td rowspan="1" colspan="1">52.5</td>
                <td rowspan="1" colspan="1">89.2</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">hybrid</td>
                <td rowspan="1" colspan="1">47.9</td>
                <td rowspan="1" colspan="1">43.0</td>
                <td rowspan="1" colspan="1">48.7</td>
                <td rowspan="1" colspan="1">53.3</td>
                <td rowspan="1" colspan="1">87.8</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">solo</td>
                <td rowspan="1" colspan="1">47.6</td>
                <td rowspan="1" colspan="1">43.3</td>
                <td rowspan="1" colspan="1">48.8</td>
                <td rowspan="1" colspan="1">54.8</td>
                <td rowspan="1" colspan="1">88.0</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">bcds</td>
                <td rowspan="1" colspan="1">45.3</td>
                <td rowspan="1" colspan="1">40.9</td>
                <td rowspan="1" colspan="1">49.1</td>
                <td rowspan="1" colspan="1">48.6</td>
                <td rowspan="1" colspan="1">88.0</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">cxds</td>
                <td rowspan="1" colspan="1">39.6</td>
                <td rowspan="1" colspan="1">33.7</td>
                <td rowspan="1" colspan="1">39.3</td>
                <td rowspan="1" colspan="1">48.3</td>
                <td rowspan="1" colspan="1">85.1</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">scrublet</td>
                <td rowspan="1" colspan="1">35.2</td>
                <td rowspan="1" colspan="1">37.5</td>
                <td rowspan="1" colspan="1">80.4</td>
                <td rowspan="1" colspan="1">27.8</td>
                <td rowspan="1" colspan="1">89.3</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">lib-sze</td>
                <td rowspan="1" colspan="1">27.5</td>
                <td rowspan="1" colspan="1">20.6</td>
                <td rowspan="1" colspan="1">29.5</td>
                <td rowspan="1" colspan="1">30.7</td>
                <td rowspan="1" colspan="1">84.5</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">doubletFinder</td>
                <td rowspan="1" colspan="1">0.7</td>
                <td rowspan="1" colspan="1">4.0</td>
                <td rowspan="1" colspan="1">87.5</td>
                <td rowspan="1" colspan="1">0.3</td>
                <td rowspan="1" colspan="1">87.8</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <p>To investigate whether calling doublets is better than using the expected number of doublets, based on the heuristic <inline-formula id="IE85"><mml:math id="IM85" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>·</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> (see Sections 2 and 2.1.5) to select a cutoff, we recalculated performance metrics for all methods with this approach. <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S3</xref> shows the results, and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S4</xref> shows the difference in performance. For most methods, performance differences between this approach and method-specific determination of the number of doublets are small; exceptions are scrublet and doubletFinder, with both of them losing precision and gaining recall as more doublets are called.</p>
        <p>To test for significant differences in performance in doublet calling, we used Wilcoxon rank-sum tests, like before. <xref rid="btac720-T3" ref-type="table">Table 3</xref> shows results comparing scDblFinder, vaeda, and doubletFinder for f1 score, mcc, precision, recall, and accuracy. We find that vaeda performs comparable to scDblFinder in terms of mcc, precision, and accuracy, and slightly worse in terms of f1 score and recall.</p>
        <table-wrap position="float" id="btac720-T3">
          <label>Table 3.</label>
          <caption>
            <p>Comparison of doublet calls averaged across benchmark datasets</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="left" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1"/>
                <th align="left" rowspan="1" colspan="1">f1</th>
                <th align="left" rowspan="1" colspan="1">mcc</th>
                <th align="left" rowspan="1" colspan="1">Precision</th>
                <th align="left" rowspan="1" colspan="1">Recall</th>
                <th align="left" rowspan="1" colspan="1">Accuracy</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">vaeda-scDblFinder</td>
                <td rowspan="1" colspan="1">scDF</td>
                <td rowspan="1" colspan="1">
                  <inline-formula id="IE86">
                    <mml:math id="IM86" display="inline" overflow="scroll">
                      <mml:mo>≈</mml:mo>
                    </mml:math>
                  </inline-formula>
                </td>
                <td rowspan="1" colspan="1">
                  <inline-formula id="IE87">
                    <mml:math id="IM87" display="inline" overflow="scroll">
                      <mml:mo>≈</mml:mo>
                    </mml:math>
                  </inline-formula>
                </td>
                <td rowspan="1" colspan="1">scDF</td>
                <td rowspan="1" colspan="1">
                  <inline-formula id="IE88">
                    <mml:math id="IM88" display="inline" overflow="scroll">
                      <mml:mo>≈</mml:mo>
                    </mml:math>
                  </inline-formula>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">vaeda-doubletFinder</td>
                <td rowspan="1" colspan="1">vaeda</td>
                <td rowspan="1" colspan="1">vaeda</td>
                <td rowspan="1" colspan="1">
                  <inline-formula id="IE89">
                    <mml:math id="IM89" display="inline" overflow="scroll">
                      <mml:mo>≈</mml:mo>
                    </mml:math>
                  </inline-formula>
                </td>
                <td rowspan="1" colspan="1">vaeda</td>
                <td rowspan="1" colspan="1">
                  <inline-formula id="IE90">
                    <mml:math id="IM90" display="inline" overflow="scroll">
                      <mml:mo>≈</mml:mo>
                    </mml:math>
                  </inline-formula>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">scDblFinder-doubletFinder</td>
                <td rowspan="1" colspan="1">scDF</td>
                <td rowspan="1" colspan="1">scDF</td>
                <td rowspan="1" colspan="1">DF</td>
                <td rowspan="1" colspan="1">scDF</td>
                <td rowspan="1" colspan="1">
                  <inline-formula id="IE91">
                    <mml:math id="IM91" display="inline" overflow="scroll">
                      <mml:mo>≈</mml:mo>
                    </mml:math>
                  </inline-formula>
                </td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn2">
              <p><italic toggle="yes">Note</italic>: Paired Wilcoxon rank-sum tests were used identify significant (<italic toggle="yes">P</italic> ≤ 0.05) performance differences between methods’ doublet calls. <inline-formula id="IE92"><mml:math id="IM92" display="inline" overflow="scroll"><mml:mo>≈</mml:mo></mml:math></inline-formula> means <italic toggle="yes">P </italic>&gt;<italic toggle="yes"> </italic>0.05.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
      </sec>
      <sec>
        <title>3.2.3 Consistently misclassified doublets may be homotypic</title>
        <p>In a majority of benchmarking datasets, there is a large subset of annotated doublets that are misclassified by every method (<xref rid="btac720-F4" ref-type="fig">Fig. 4A</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Figs. S10 and S11</xref>). We suspect that these consistently misclassified doublets, or ‘missed’ doublets, are composed primarily of homotypic doublets (i.e. doublets that are composed of two cells of the same type). Support for this hypothesis comes from the fact that the only two datasets that do not have a substantial amount of (consistently) missed doublets are the hm-6k and hm-12k datasets, where homotypic doublets are not annotated. In order to further test this idea, we measured the mixing between singlets, captured doublets, and missed doublets; in this analysis captured doublets are annotated by at least four methods, and missed doublets were not annotated by any of the eight methods we applied. Mixing was measured as the average fraction of singlets in the neighborhoods of the missed/captured doublets as described in Section 2.4. We found that missed doublets have higher mixing with singlets than captured doublets, providing support to the idea that the missed doublets are homotypic (i.e. near singlets presumably of the same type), see <xref rid="btac720-F4" ref-type="fig">Figure 4B</xref>.</p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>4 Discussion</title>
    <p>Here, we present vaeda, a new tool for computational doublet detection. The vaeda method uses a similar paradigm to other approaches, where doublets scores are produced using artificially generated doublets [e.g. <xref rid="btac720-B1" ref-type="bibr">Bais and Kostka, 2020</xref>; <xref rid="btac720-B3" ref-type="bibr">Germain <italic toggle="yes">et al.</italic>, 2021</xref>, and others]. While auto-encoders have been used in the context of doublet detection before (<xref rid="btac720-B2" ref-type="bibr">Bernstein <italic toggle="yes">et al.</italic>, 2020</xref>), vaeda is unique in that it combines a cluster-aware variational auto-encoder with a PU-Learning approach. We carefully studied the effect of both of these concepts and showed that, even though other design choices have greater impact, they do enable a noticeable increase in performance.</p>
    <p>We assessed vaeda on 16 benchmark datasets, and find that, overall, vaeda produces accurate doublet scores and is able to derive binary doublet predictions that reflect experimental annotations well. Further on, we find that vaeda’s latent representation is helpful in determining datasets where simulated doublets agree well with experimental annotations, and cases where simulated and experimentally annotated doublets show more pronounced differences. We illustrate this with four examples in <xref rid="btac720-F2" ref-type="fig">Figure 2D</xref>, with HMEC-rep-MULIT and cline-ch as examples where the distributions simulated and annotated doublets show differences. We note that vaeda, as well as other methods (see <xref rid="btac720-F3" ref-type="fig">Fig. 3</xref>), do not perform particularly well on these data, indicating that improving our ability to simulate doublets might be a promising approach to improve method performance.</p>
    <p>In terms of assessing method performance, we note that ultimately the metrics most relevant in practice are those for binary predictions, rather than rank-based metrics like area under the ROC curve or average precision (i.e. area under the precision recall curve). Such metrics are reported in <xref rid="btac720-T2" ref-type="table">Table 2</xref>, and we see that vaeda outperforms doubletFinder and performs comparable so scDblFinder in terms of mcc, precision, and accuracy; in terms of f1 score and recall scDblFinder performs a bit better than vaeda. Therefore, we conclude that vaeda performs comparable to other state-of-the-art methods (scDblFinder and doubletFinder), but note that scDblFinder has slightly better average performance in terms of f1 score and recall. We highlight, however, that the only other native python method is scrublet, which performs significantly worse than vaeda.</p>
    <p>Interestingly, DoubletFinder’s doublet calling heuristic with default parameters does not perform well in <xref rid="btac720-T2" ref-type="table">Table 2</xref>. DoubletFinder’s doublet scores perform well overall, however. We show this in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S5</xref>, where we use doublet scores to call the annotated number of doublets across benchmark datasets. This analysis places DoubletFinder into the top three methods in terms of f1 measure, for example.</p>
    <p>We do note that the benchmarking datasets used in this work, and other doublet annotation manuscripts, are not entirely representative as they do not exhaustively cover technical and biological diversity. To add flexibility to our doublet calling approach, the parameter <italic toggle="yes">μ</italic> in our thresholding function (Section 2.1.5) adjustable by the user to take specific attributes of their data into account.</p>
    <p>In summary, we have shown that vaeda is a state-of-the-art method for computationally annotating doublets in scRNA-seq data, and that it is the top choice for python workflows. It also provides a low-dimensional data representation that can complement other approaches and be useful for data analysis and visualization. It therefore is a useful tool for single-cell RNA sequencing data analysis.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btac720_Supplementary_Data</label>
      <media xlink:href="btac720_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgement</title>
    <p>H.S. and D.K. would like to thank the Kostka and Chikina labs for feedback and discussion.</p>
    <sec>
      <title>Funding</title>
      <p>This work was supported by the University of Pittsburgh School of Medicine [T32 5T32EB009403-13] from National Institute of Heath (NIH) National Institute of Biomedical Imaging and Bioengineering (NIBIB).</p>
      <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
    </sec>
  </ack>
  <sec sec-type="data-availability">
    <title>Data availability</title>
    <p>The benchmarking scRNA-seq datasets are available for download on zenodo from the original authors (<ext-link xlink:href="https://doi.org/10.5281/zenodo.4062232" ext-link-type="uri">https://doi.org/10.5281/zenodo.4062232</ext-link>). Code to reproduce results reported in this manuscript in addition to doublet scores and doublet calls produced by each method are available on zenodo (<ext-link xlink:href="https://doi.org/10.5281/zenodo.7199783" ext-link-type="uri">https://doi.org/10.5281/zenodo.7199783</ext-link>).</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btac720-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bais</surname><given-names>A.S.</given-names></string-name>, <string-name><surname>Kostka</surname><given-names>D.</given-names></string-name></person-group> (<year>2020</year>) <article-title>scds: computational annotation of doublets in single-cell RNA sequencing data</article-title>. <source>Bioinformatics</source>, <volume>36</volume>, <fpage>1150</fpage>–<lpage>1158</lpage>.<pub-id pub-id-type="pmid">31501871</pub-id></mixed-citation>
    </ref>
    <ref id="btac720-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bernstein</surname><given-names>N.J.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>Solo: doublet identification in single-cell RNA-seq via semi-supervised deep learning</article-title>. <source>Cell Syst</source>., <volume>11</volume>, <fpage>95</fpage>–<lpage>101.e5</lpage>.<pub-id pub-id-type="pmid">32592658</pub-id></mixed-citation>
    </ref>
    <ref id="btac720-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Germain</surname><given-names>P.-L.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Doublet identification in single-cell sequencing data using scDblFinder</article-title>. <source>F1000Research</source>, <volume>10</volume>, <fpage>979</fpage>.<pub-id pub-id-type="pmid">35814628</pub-id></mixed-citation>
    </ref>
    <ref id="btac720-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hicks</surname><given-names>S.C.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>mbkmeans: fast clustering for single cell data using mini-batch k-means</article-title>. <source>PLoS Comput. Biol</source>., <volume>17</volume>, <fpage>e1008625</fpage>.<pub-id pub-id-type="pmid">33497379</pub-id></mixed-citation>
    </ref>
    <ref id="btac720-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kang</surname><given-names>H.M.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>Multiplexed droplet single-cell RNA-sequencing using natural genetic variation</article-title>. <source>Nat. Biotechnol</source>., <volume>36</volume>, <fpage>89</fpage>–<lpage>94</lpage>.<pub-id pub-id-type="pmid">29227470</pub-id></mixed-citation>
    </ref>
    <ref id="btac720-B7">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>B.</given-names></string-name></person-group><etal>et al</etal> (<year>2003</year>) Building text classifiers using positive and unlabeled examples. In: <italic toggle="yes">Third IEEE International Conference on Data Mining, Melbourne, FL, USA</italic>. pp. <fpage>179</fpage>–<lpage>186</lpage>.</mixed-citation>
    </ref>
    <ref id="btac720-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McGinnis</surname><given-names>C.S.</given-names></string-name></person-group><etal>et al</etal> (<year>2019a</year>) <article-title>Doubletfinder: doublet detection in single-cell RNA sequencing data using artificial nearest neighbors</article-title>. <source>Cell Syst</source>., <volume>8</volume>, <fpage>329</fpage>–<lpage>337.e4</lpage>.<pub-id pub-id-type="pmid">30954475</pub-id></mixed-citation>
    </ref>
    <ref id="btac720-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McGinnis</surname><given-names>C.S.</given-names></string-name></person-group><etal>et al</etal> (<year>2019b</year>) <article-title>Multi-seq: sample multiplexing for single-cell RNA sequencing using lipid-tagged indices</article-title>. <source>Nat. Methods</source>, <volume>16</volume>, <fpage>619</fpage>–<lpage>626</lpage>.<pub-id pub-id-type="pmid">31209384</pub-id></mixed-citation>
    </ref>
    <ref id="btac720-B10">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>McInnes</surname><given-names>L.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) UMAP: uniform manifold approximation and projection for dimension reduction. <italic toggle="yes">ArXiv e-prints.</italic></mixed-citation>
    </ref>
    <ref id="btac720-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mordelet</surname><given-names>F.</given-names></string-name>, <string-name><surname>Vert</surname><given-names>J.-P.</given-names></string-name></person-group> (<year>2014</year>) <article-title>A bagging SVM to learn from positive and unlabeled examples</article-title>. <source>Patt. Recogn. Lett</source>., <volume>37</volume>, <fpage>201</fpage>–<lpage>209</lpage>.</mixed-citation>
    </ref>
    <ref id="btac720-B12">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Satopaa</surname><given-names>V.</given-names></string-name></person-group><etal>et al</etal> (<year>2011</year>) Finding a “kneedle” in a haystack: detecting knee points in system behavior. In: <italic toggle="yes">2011 31st International Conference on Distributed Computing Systems Workshops, Minneapolis, Minnesota, USA</italic>, pp. <fpage>166</fpage>–<lpage>171</lpage>.</mixed-citation>
    </ref>
    <ref id="btac720-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stoeckius</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>Cell hashing with barcoded antibodies enables multiplexing and doublet detection for single cell genomics</article-title>. <source>Genome Biol</source>., <volume>19</volume>, <fpage>1</fpage>–<lpage>12</lpage>.<pub-id pub-id-type="pmid">29301551</pub-id></mixed-citation>
    </ref>
    <ref id="btac720-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Traag</surname><given-names>V.A.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>From Louvain to Leiden: guaranteeing well-connected communities</article-title>. <source>Sci. Rep</source>., <volume>9</volume>, <fpage>5233</fpage>.<pub-id pub-id-type="pmid">30914743</pub-id></mixed-citation>
    </ref>
    <ref id="btac720-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolf</surname><given-names>F.A.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>SCANPY: large-scale single-cell gene expression data analysis</article-title>. <source>Genome Biol</source>., <volume>19</volume>, <fpage>15</fpage>.<pub-id pub-id-type="pmid">29409532</pub-id></mixed-citation>
    </ref>
    <ref id="btac720-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolock</surname><given-names>S.L.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Scrublet: computational identification of cell doublets in single-cell transcriptomic data</article-title>. <source>Cell Syst</source>., <volume>8</volume>, <fpage>281</fpage>–<lpage>291.e9</lpage>.<pub-id pub-id-type="pmid">30954476</pub-id></mixed-citation>
    </ref>
    <ref id="btac720-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xi</surname><given-names>N.M.</given-names></string-name>, <string-name><surname>Li</surname><given-names>J.J.</given-names></string-name></person-group> (<year>2021a</year>) <article-title>Benchmarking computational doublet-detection methods for single-cell RNA sequencing data</article-title>. <source>Cell Syst</source>., <volume>12</volume>, <fpage>176</fpage>–<lpage>194.e6</lpage>.<pub-id pub-id-type="pmid">33338399</pub-id></mixed-citation>
    </ref>
    <ref id="btac720-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xi</surname><given-names>N.M.</given-names></string-name>, <string-name><surname>Li</surname><given-names>J.J.</given-names></string-name></person-group> (<year>2021b</year>) <article-title>Protocol for executing and benchmarking eight computational doublet-detection methods in single-cell RNA sequencing data analysis</article-title>. <source>STAR Protoc</source>., <volume>2</volume>, <fpage>100699</fpage>.<pub-id pub-id-type="pmid">34382023</pub-id></mixed-citation>
    </ref>
    <ref id="btac720-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yan</surname><given-names>F.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Emptynn: a neural network based on positive and unlabeled learning to remove cell-free droplets and recover lost cells in scRNA-seq data</article-title>. <source>Patterns</source>, <volume>2</volume>, <fpage>100311</fpage>.<pub-id pub-id-type="pmid">34430929</pub-id></mixed-citation>
    </ref>
    <ref id="btac720-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zheng</surname><given-names>G.X.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) <article-title>Massively parallel digital transcriptional profiling of single cells</article-title>. <source>Nat. Commun</source>., <volume>8</volume>, <fpage>14049</fpage>.<pub-id pub-id-type="pmid">28091601</pub-id></mixed-citation>
    </ref>
    <ref id="btac720-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhou</surname><given-names>J.</given-names></string-name>, <string-name><surname>Troyanskaya</surname><given-names>O.G.</given-names></string-name></person-group> (<year>2021</year>) <article-title>An analytical framework for interpretable and generalizable single-cell data analysis</article-title>. <source>Nat. Methods</source>, <volume>18</volume>, <fpage>1317</fpage>–<lpage>1321</lpage>.<pub-id pub-id-type="pmid">34725480</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
