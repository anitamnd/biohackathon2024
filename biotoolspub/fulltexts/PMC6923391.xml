<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sci Rep</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id>
    <journal-title-group>
      <journal-title>Scientific Reports</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2045-2322</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6923391</article-id>
    <article-id pub-id-type="publisher-id">55431</article-id>
    <article-id pub-id-type="doi">10.1038/s41598-019-55431-0</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>UNI-EM: An Environment for Deep Neural Network-Based Automated Segmentation of Neuronal Electron Microscopic Images</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Urakubo</surname>
          <given-names>Hidetoshi</given-names>
        </name>
        <address>
          <email>hurakubo@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6637-0221</contrib-id>
        <name>
          <surname>Bullmann</surname>
          <given-names>Torsten</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0950-7460</contrib-id>
        <name>
          <surname>Kubota</surname>
          <given-names>Yoshiyuki</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Oba</surname>
          <given-names>Shigeyuki</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ishii</surname>
          <given-names>Shin</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 0372 2033</institution-id><institution-id institution-id-type="GRID">grid.258799.8</institution-id><institution>Integrated Systems Biology Laboratory, Department of Systems Science, Graduate School of Informatics, </institution><institution>Kyoto University, </institution></institution-wrap>Yoshida-Honmachi 36-1, Sakyo-ku, Kyoto 606-8501 Japan </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2230 9752</institution-id><institution-id institution-id-type="GRID">grid.9647.c</institution-id><institution>Carl-Ludwig-Institute for Physiology, </institution><institution>University Leipzig, </institution></institution-wrap>Liebigstr. 27, 04103 Leipzig, Germany </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2272 1771</institution-id><institution-id institution-id-type="GRID">grid.467811.d</institution-id><institution>Division of Cerebral Circuitry, </institution><institution>National Institute for Physiological Sciences, </institution></institution-wrap>5-1 Myodaiji-Higashiyama, Okazaki, Aichi 444-8787 Japan </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1763 208X</institution-id><institution-id institution-id-type="GRID">grid.275033.0</institution-id><institution>Department of Physiological Sciences, </institution><institution>The Graduate University for Advanced Studies (SOKENDAI), </institution></institution-wrap>5-1 Myodaiji-Higashiyama, Okazaki, Aichi 444-8787 Japan </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>19</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>19</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>9</volume>
    <elocation-id>19413</elocation-id>
    <history>
      <date date-type="received">
        <day>16</day>
        <month>4</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>28</day>
        <month>11</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Recently, there has been rapid expansion in the field of micro-connectomics, which targets the three-dimensional (3D) reconstruction of neuronal networks from stacks of two-dimensional (2D) electron microscopy (EM) images. The spatial scale of the 3D reconstruction increases rapidly owing to deep convolutional neural networks (CNNs) that enable automated image segmentation. Several research teams have developed their own software pipelines for CNN-based segmentation. However, the complexity of such pipelines makes their use difficult even for computer experts and impossible for non-experts. In this study, we developed a new software program, called UNI-EM, for 2D and 3D CNN-based segmentation. UNI-EM is a software collection for CNN-based EM image segmentation, including ground truth generation, training, inference, postprocessing, proofreading, and visualization. UNI-EM incorporates a set of 2D CNNs, i.e., U-Net, ResNet, HighwayNet, and DenseNet. We further wrapped flood-filling networks (FFNs) as a representative 3D CNN-based neuron segmentation algorithm. The 2D- and 3D-CNNs are known to demonstrate state-of-the-art level segmentation performance. We then provided two example workflows: mitochondria segmentation using a 2D CNN and neuron segmentation using FFNs. By following these example workflows, users can benefit from CNN-based segmentation without possessing knowledge of Python programming or CNN frameworks.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Software</kwd>
      <kwd>Neural circuits</kwd>
      <kwd>Software</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100003382</institution-id>
            <institution>MEXT | JST | Core Research for Evolutional Science and Technology (CREST)</institution>
          </institution-wrap>
        </funding-source>
        <award-id>JPMJCR1652</award-id>
        <award-id>JPMJCR1652</award-id>
        <principal-award-recipient>
          <name>
            <surname>Urakubo</surname>
            <given-names>Hidetoshi</given-names>
          </name>
          <name>
            <surname>Ishii</surname>
            <given-names>Shin</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100001691</institution-id>
            <institution>MEXT | Japan Society for the Promotion of Science (JSPS)</institution>
          </institution-wrap>
        </funding-source>
        <award-id>17K00404</award-id>
        <award-id>17H06310</award-id>
        <principal-award-recipient>
          <name>
            <surname>Urakubo</surname>
            <given-names>Hidetoshi</given-names>
          </name>
          <name>
            <surname>Ishii</surname>
            <given-names>Shin</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1" sec-type="introduction">
    <title>Introduction</title>
    <p id="Par2">In recent years, there has been a rapid expansion in the field of micro-connectomics, which targets the three-dimensional (3D) reconstruction of neuronal networks from stacks of two-dimensional (2D) electron microscopy (EM) images<sup><xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR3">3</xref></sup>. Neuroscientists have successfully reconstructed large-scale neural circuits from species, such as mice<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, fruit flies<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>, and zebrafish<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. Such large-scale reconstructions require neuronal boundary detection (or neuron segmentation) of large numbers of EM images, and automation is critical even for smaller-scale segmentation.</p>
    <p id="Par3">For automated neuron segmentation, studies have validated the effectiveness of deep convolutional neural networks (CNNs)<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. In particular, U-Net, which is a type of CNN, showed the highest accuracy in a neuron segmentation contest<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>, and similar CNNs also proved effective<sup><xref ref-type="bibr" rid="CR9">9</xref>–<xref ref-type="bibr" rid="CR11">11</xref></sup>. Three-dimensional CNNs have also been developed for higher segmentation accuracy. Januszewski <italic>et al</italic>. developed a type of recursive 3D CNN called flood filling networks (FFNs)<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>, which showed the highest segmentation accuracy in a public 3D EM dataset (FIB-25)<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> and the second highest in another public 3D EM dataset (3D segmentation of neurites in EM images, SNEMI3D)<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. Therefore, the use of such CNNs has become critical for accurate neuron segmentation.</p>
    <p id="Par4">Most CNN source codes are publicly available; however, it is not easy to perform segmentation even with these source codes. Users are required to prepare ground truth segmentation for their own EM images first and then to conduct preprocessing tasks, such as data conversion. The preprocessing and use of CNNs often require users to learn the underlying programming language, which is generally Python. After performing CNN-based segmentation, users need to conduct postprocessing, including proofreading, annotation, and visualization. In short, CNN-based neuron segmentation, although an important task, constitutes only a portion of the entire segmentation procedure.</p>
    <p id="Par5">Advanced connectomics laboratories have developed their own software pipelines to employ CNN-based segmentation, including Rhoana<sup><xref ref-type="bibr" rid="CR15">15</xref>,<xref ref-type="bibr" rid="CR16">16</xref></sup>, Eyewire<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>, and the FFN segmentation pipeline<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. The main objective of these pipelines is large-scale 3D reconstructions that are conducted by large teams including computer experts for setup and maintenance. They are too complicated to be used by smaller teams. EM segmentation is also handled by sophisticated standalone software packages, such as Reconstruct<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, Ilastik<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>, Knossos<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>, Microscopy Image Browser<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>, and VAST lite<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>. However, most software packages only target manual segmentation<sup><xref ref-type="bibr" rid="CR18">18</xref>,<xref ref-type="bibr" rid="CR20">20</xref>,<xref ref-type="bibr" rid="CR22">22</xref></sup>, and others currently do not support CNN-based segmentation<sup><xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR21">21</xref></sup>. Recently, a plug-in for the widely used ImageJ software was developed to handle CNN-based segmentation<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>. The use of this plug-in is advantageous; however, it currently provides only four types of U-Net models, and users need to launch a server on a Linux computer to train the U-Nets.</p>
    <p id="Par6">We therefore developed a unified environment for CNN-based automated segmentation of EM images (UNI-EM) for researchers with limited programming skills. UNI-EM implements several 2D CNNs<sup><xref ref-type="bibr" rid="CR8">8</xref>–<xref ref-type="bibr" rid="CR11">11</xref></sup> and 3D FFNs<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> on the widely used Tensorflow framework/Python<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. It also includes the proofreading software Dojo<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> as well as a series of 2D/3D filters for classic image processing. Those features enable users to follow the procedure of CNN-based segmentation, i.e., ground truth generation, training, inference, postprocessing, proofreading, and visualization. UNI-EM currently supports two major operating systems (OSs): Microsoft Windows 10 (64 bit) and Linux. We also provide Python installation-free versions of UNI-EM (Pyinstaller version). Thus, users do not need to install Python or any modules for CNN-based segmentation.</p>
  </sec>
  <sec id="Sec2" sec-type="results">
    <title>Results</title>
    <sec id="Sec3">
      <title>Outline of software</title>
      <p id="Par7">UNI-EM is a software collection for CNN-based EM image segmentation that includes ground truth generation, training, inference, postprocessing, proofreading, and visualization (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). UNI-EM is written in Python 3.6 and runs on Microsoft Windows 10 (64 bit) and Linux. We also built UNI-EM on the Python application bundler called Pyinstaller on Windows 10; thus, users can employ UNI-EM without installing the Python programming environment. CPU and GPU versions are available, and users can maximize the performance using the GPU version if the computer is equipped with an NVIDIA GPU card that has a NVIDIA compute capability over 3.5. The developed Python source code with an online manual is available at the public repository GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/urakubo/UNI-EM">https://github.com/urakubo/UNI-EM</ext-link>).<fig id="Fig1"><label>Figure 1</label><caption><p>GUIs of UNI-EM. (<bold>A</bold>) Proofreader Dojo with extension. The GUI of Dojo was reorganized. Users can rectify mis-segmentation as well as build the ground truth using paint functions. The reorganized Dojo supports the import/export functions of EM/segmentation image stack files. (<bold>B</bold>) 3D annotator. A 3D viewer (left) is associated with the object tables (right) that display segmented object and marker points. Visualization results and tables are exportable as png and csv files, respectively. The GUIs in (<bold>A</bold>,<bold>B</bold>) are provided as web applications. Multiple users can access these GUIs through equipped or external web browsers.</p></caption><graphic xlink:href="41598_2019_55431_Fig1_HTML" id="d29e467"/></fig></p>
      <p id="Par8">The main component of UNI-EM is a web-based proofreading software, Dojo (Fig. <xref rid="Fig1" ref-type="fig">1A</xref>)<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. Dojo provides a graphical user interface (GUI) for users to correct mis-segmentation arising from automated EM segmentation. We extended Dojo to have file import/export functions (png/tiff files), a more sophisticated GUI, and multiscale paint functions. With these extensions, users can employ Dojo not only for proofreading, but also for ground truth generation, both of which are important manual operation procedures for CNN-based segmentation. Dojo consists of a Python-based web/database server and an HTML5/JavaScript-based client interface. The server–client system allows multiple users to access it simultaneously through web browsers in an OS-independent manner. UNI-EM equips its own web browser called Chromium for the standalone use of Dojo with either a mouse or a stylus.</p>
      <p id="Par9">We also developed a new 3D annotator to visualize the proofread objects in a 3D space as well as to annotate these segmented objects (Fig. <xref rid="Fig1" ref-type="fig">1B</xref>). This annotator is a surface mesh-based 3D viewer with a table that shows segmented objects. Users can change the color and brightness of target objects and export the visualization results as png image files, as well as assign a name to each object and put marker points on the object surface. The results of these annotations can be exported as csv files for further analyses.</p>
      <p id="Par10">We then implemented a U-Net equipped with a GUI as a representative 2D CNN for EM-image segmentation<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>. U-Net has characteristic contracting and expansive convolution layers with skip connections, which showed the highest segmentation accuracy in the EM Segmentation Challenge in the International Symposium on Biomedical Imaging 2012 Conference (ISBI 2012) at the time of publication<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>. We similarly implemented ResNet<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>, Highway-Net<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, and Dense-Net<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. All of the CNNs accept single-channel (gray-scale) or three-channel (RGB) images. Users can choose any combination of these CNNs, loss functions, training times, and data augmentation methods, through a command panel.</p>
      <p id="Par11">We further wrapped FFNs as a representative algorithm of 3D CNN-based neuron segmentation<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. FFNs are a recurrent CNN that infers a volume mask indicating whether target voxels belong to the centered object, and the inference program obtains an overall volume mask for each object using a flood filling algorithm. FFNs have outperformed many other algorithms in the segmentation accuracies of FIB-25<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> and SNEMI3D<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. Users can conduct a series of FFN processes, i.e., preprocessing, training, inference, and postprocessing, through a command panel.</p>
      <p id="Par12">The 2D CNNs and 3D FFNs were implemented on the Tensorflow framework<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. Its resource monitor Tensorboard can be conveniently accessed from UNI-EM, so users can easily check the status of a target CNN, such as the network topology and loss function. UNI-EM also has a GUI for 2D/3D classic image filters. Users can apply multiple image filters simultaneously to a stack of 2D images in a single execution. The target images of the CNNs and classic filters are opened/closed through a folder manager. Further, users can implement new CNN models through the “Plugin” dropdown menu. Details on how to implement a new CNN are outlined in the online manual (see Data availability).</p>
    </sec>
    <sec id="Sec4">
      <title>Example workflows</title>
      <p id="Par13">In this section, we demonstrate how users can benefit from UNI-EM by introducing two example workflows. The first one is mitochondria segmentation using 2D CNNs, and the second one is neuron segmentation using 3D FFNs. In both cases, we targeted an EM image stack that was prepared for SNEMI3D<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. The target brain region is the mouse somatosensory cortex, and the EM images were obtained using scanning electron microscopy (SEM) in combination with an automatic tape-collecting ultra-microtome system (ATUM/SEM)<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. The spatial resolution of the EM images was 6 nm per pixel (xy-plane) and 30 nm per Z slice, and the overall image volume was 6.1 × 6.1 × 3 μm. The images were passed through a contrast-limited adaptive histogram equalization filter (CLAHE; block size 127, histogram bins 256, max slope 1.50) before segmentation.</p>
      <sec id="Sec5">
        <title>Case 1: Mitochondria segmentation using 2D CNN</title>
        <p id="Par14">Mitochondria are abundant where the metabolic demand is high, such as in synapses and active axons<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR28">28</xref></sup>, and their detection and quantification are important for treating neuronal diseases<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. Because mitochondria possess characteristic oval shapes<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, their segmentation is a good target for 2D CNN-based segmentation<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>. However, it is not accessible to inexperienced users (Fig. <xref rid="Fig2" ref-type="fig">2A</xref>). Firstly, inexperienced users need to learn how to use Python, install a CNN framework, and download an implementation of the target CNN from a public repository. The other software packages need to be installed for ground truth generation, post-processing, and proofreading (Fig. <xref rid="Fig2" ref-type="fig">2A</xref>). These steps can be learned, but a major hurdle is the transfer of data, especially to a CNN, when the users must convert EM/segmentation images into HDF5 or npz format files. To confirm that UNI-EM decreases the arduousness of these tasks (Fig. <xref rid="Fig2" ref-type="fig">2B</xref>), two test users (H.K. and Y.F.) who were not skilled in Python programming were requested to perform the following procedure (Fig. <xref rid="Fig2" ref-type="fig">2C</xref>):<list list-type="order"><list-item><p id="Par15">Ground truth generation. The test users painted the mitochondrial regions of a single EM image using UNI-EM (Dojo). The generated ground truth was exported as an 8-bit grayscale PNG file (~20 min).</p></list-item><list-item><p id="Par16">Training. A 16-layer ResNet with a least-square loss function was trained using the ground truth (~10 min computation time).</p></list-item><list-item><p id="Par17">Inference. The trained ResNet was applied to test the EM images to obtain inferred 2D segmentation (~1 min).</p></list-item><list-item><p id="Par18">Postprocessing. The inferred 2D segmentation images were binarized, and then each isolated region in 3D space was labeled with a specific ID number (~10 min).</p></list-item><list-item><p id="Par19">Proofreading, annotation, and visualization. The test users proofread it with Dojo and visualized it with the 3D annotator (~30 min).</p></list-item></list><fig id="Fig2"><label>Figure 2</label><caption><p>Example workflow 1: Mitochondria segmentation using 2D CNN. (<bold>A</bold>) Conventional workflow. Users first paint the regions of mitochondria of a target EM image using painting software, e.g., VAST lite (1, top)<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>. This mitochondrial segmentation image (ground truth) and the EM image are transferred to Tensorflow/Python for CNN training and inference (2,3; right). Inferred segmentation is then postprocessed (4, left), e.g., using imageJ, proofread and visualized by VAST lite (5, top). Such relays between software packages are necessary. (<bold>B</bold>) UNI-EM dropdown menu. A series of software (a-d) is located for the CNN-based segmentation (1–5). Standard png/tiff file format is used to connect these software packages. (<bold>C</bold>) Workflow in UNI-EM. Extended Dojo supports paint functions (1; top, left) to draw mitochondrial segmentation (top, right). Users can conduct CNN training (2) and inference (3) through a control panel. A labeling function is also implemented for postprocessing (4, each label is denoted by color). These segmented images are proofread by Dojo (5, left), and visualized by the 3D annotator (5, right).</p></caption><graphic xlink:href="41598_2019_55431_Fig2_HTML" id="d29e626"/></fig></p>
        <p id="Par20">The test users successfully conducted the above procedure within the time indicated in parentheses and obtained the instance segmentation of mitochondria. The segmentation accuracy was sufficiently high without any proofreading (Fig. <xref rid="Fig2" ref-type="fig">2C</xref>, bottom and right panel; RAND score: 0.85; see Methods), as expected from published results on 2D CNN-based segmentation<sup><xref ref-type="bibr" rid="CR31">31</xref>,<xref ref-type="bibr" rid="CR32">32</xref></sup>. The detailed instructions for the mitochondria segmentation task can be found at the public repository GitHub (see Data availability).<fig id="Fig3"><label>Figure 3</label><caption><p>Performance survey in 2D CNN-based segmentation of neurons, synapses, and mitochondria. (<bold>A</bold>) One of target EM images (left, SNEMI3D) and ground truth segmentation (right). Each image panel has 1024 × 1024 voxels (3 nm/voxel in x-y plane), and 100 z-slices (3 nm/voxel in z slice). In the right panel, blue and red lines indicate cellular membranes and synapses, respectively, and green areas indicate mitochondria. (<bold>B</bold>) Training image number dependence of segmentation accuracy (n = 15, mean ± SD; RAND score, see Methods). The RAND score approaches 1 if the inferred segmentation is similar to the ground truth. (<bold>C</bold>) Loss function dependence of segmentation accuracy (n = 60, mean ± SD). Here, “Square” denotes least square, “Softmax” denotes SoftMax cross-entropy, and “Entropy” denotes multi-class and multi-label cross-entropy. (<bold>D</bold>) Network topology dependence of segmentation accuracy (n = 15, mean ± SD). In B-D, all of the parameters except the target parameters were set as follows: the number of training images: 1; loss function: least square; network topology: ResNet; number of layers: 9; number of training epochs: 2000; number of training images: 5 (standard CNN). The 2000 training epochs gave steady states of their losses.</p></caption><graphic xlink:href="41598_2019_55431_Fig3_HTML" id="d29e659"/></fig></p>
        <p id="Par21">In the above process, we requested the test users to use a 16-layer ResNet with a least-square loss function for mitochondrial segmentation. This request was determined based on the following quantitative survey on the segmentation of mitochondria, synapses, and neurons (Fig. <xref rid="Fig3" ref-type="fig">3A</xref>). Here we utilized the RAND score as a measure of segmentation accuracy (see Methods). The larger RAND score denotes higher accuracy. We first confirmed that only one ground truth image was sufficient for the segmentation of mitochondria (Fig. <xref rid="Fig3" ref-type="fig">3B</xref>), and 10 ground truth images were sufficient for neurons and synaptic segmentations. We then confirmed that the square, dice, and logistic loss functions were appropriate for segmentation (Fig. <xref rid="Fig3" ref-type="fig">3C</xref>). All of the 2D CNN types showed high accuracy in mitochondria segmentation (Fig. <xref rid="Fig3" ref-type="fig">3D</xref>, green lines; &gt;0.9 RAND score). In addition, U-Net was not appropriate for membrane segmentation (Fig. <xref rid="Fig3" ref-type="fig">3D</xref>, red line; ~0.3 RAND score), and the segmentation accuracies in synapses are not high regardless of the type of CNN (Fig. <xref rid="Fig3" ref-type="fig">3D</xref>; ~0.3 RAND score). The accuracy of mitochondria segmentation in a standard CNN (network topology: ResNet; loss function: least square; number of layers: 9; training epochs: 2000; number of training images: 5) was indeed comparable with the accuracy in a recent 3D CNN-based, state-of-the-art algorithm<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>. The segmentation accuracy of the 3D CNN was quantified as Jaccard 0.92, Dice 0.96, and conformity 0.91 (semantic segmentation; ATUM/SEM data), whereas that of our standard 2D CNN was quantified as Jaccard 0.91, Dice 0.95, conformity 0.90 (semantic segmentation). Here, the larger scores of Jaccard, Dice, and conformity indicate higher accuracy<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>. Their 3D CNN requires 77 h of training time on a NVIDIA K40 GPU, whereas our standard CNN required only 5 min on a NIVDIA GTX1070 GPU. In addition, the 3D CNN was trained using the 3D ground truth, which requires excessive and tedious manual labeling. Overall, the implemented 2D CNN-based segmentations showed a sufficiently high and competitive accuracy compared to the current state-of-the-art mitochondrial segmentation algorithm<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>.<fig id="Fig4"><label>Figure 4</label><caption><p>Example workflow 2: Neuron segmentation using 3D FFNs. (<bold>A</bold>) Control panel of 3D FFNs. Each tab (1–4) has one execute button for each FFN process. (<bold>B</bold>) Workflow. Computation times are indicated in parentheses. (1) Preprocessing. Ground truth segmentation and EM images are converted to intermediate files. (2) Training. FFNs are trained with the intermediate files. Users can monitor the progress of training using Tensorboard. (3) Inference. (4) Postprocessing. The program can also generate colored inferred segmentation for rough visual inspection. If the segmentation quality is insufficient, users can continue the training process. (5.1) Proofreading using Dojo. (5.2) Visualization by the 3D annotator.</p></caption><graphic xlink:href="41598_2019_55431_Fig4_HTML" id="d29e707"/></fig></p>
      </sec>
      <sec id="Sec6">
        <title>Case 2: Neuron segmentation using 3D FFNs</title>
        <p id="Par22">We next asked a test user (N.Y.) to conduct neuron segmentation using 3D FFNs<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>, which is a primary topic in micro-connectomics. Various 2D and 3D CNNs have been proposed for accurate neuron segmentation<sup><xref ref-type="bibr" rid="CR33">33</xref>,<xref ref-type="bibr" rid="CR34">34</xref></sup>. FFNs currently show some of the highest segmentation accuracies in neuron segmentation<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>, although they require laborious work to generate the 3D ground truth. Users can generate the 3D ground truth using Dojo, but we recommend VAST lite for this purpose<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>. In the present case, we used the ground truth included in the SNEMI3D dataset. The test user successfully conducted the following procedure through the command panel (Fig. <xref rid="Fig4" ref-type="fig">4A</xref>):<list list-type="order"><list-item><p id="Par23">Preprocessing. Stacks of target EM images and ground truth images were converted into FFN-specialized style files (~1 h computation time; Fig. <xref rid="Fig4" ref-type="fig">4B</xref>).</p></list-item><list-item><p id="Par24">Training. FFNs were trained with the preprocessed EM-image/segmentation files (~2 weeks computation time on a NIVDIA GTX1080Ti GPU; Fig. <xref rid="Fig4" ref-type="fig">4B</xref>).</p></list-item><list-item><p id="Par25">Inference. The trained FFNs were applied to a stack of test EM images for the inference of 3D segmentation (~1 h computation time on a NIVDIA GTX1080Ti GPU; Fig. <xref rid="Fig4" ref-type="fig">4B</xref>).</p></list-item><list-item><p id="Par26">Postprocessing. The output segmentation files were converted into a PNG file stack (~10 min computation time; Fig. <xref rid="Fig4" ref-type="fig">4B</xref>).</p></list-item><list-item><p id="Par27">Proofreading and visualization. The converted PNG files and EM images were imported into Dojo for proofreading as well as the 3D annotator for visualization (Fig. <xref rid="Fig4" ref-type="fig">4B</xref>).</p></list-item></list><fig id="Fig5"><label>Figure 5</label><caption><p>Underlying architecture of UNI-EM. UNI-EM has a heterogenous system. Present desktop computers have two types of computational resources: CPU and GPU (top). A GPU is used by Tensorflow for CNN computing (middle), which is not appropriate for shared use. Only the resource monitor Tensorboard can be used by remote users (bottom). Similarly, remote users can use proofreader Dojo and 3D annotator. Only a desktop user (silhouette person) can control all of the UNI-EM functions, including job submission for CNN computing such as training and inference.</p></caption><graphic xlink:href="41598_2019_55431_Fig5_HTML" id="d29e790"/></fig></p>
        <p id="Par28">Note that the trained FFNs directly inferred a 3D instance segmentation from a stack of 2D EM images. The FFNs gave a reasonably accurate neuron segmentation (Fig. <xref rid="Fig4" ref-type="fig">4B</xref>, right), whose RAND score was 0.84 (after 7 million training epochs; see Methods)<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. This score was obtained without any postprocessing and specific parameter turning for the SNEMI3D dataset, and the topological structure of the neurites was well preserved in the segmentation results. Januszewski <italic>et al</italic>. reported a RAND score of 0.975 in the case of the SNEMI3D dataset<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. This score was obtained with two additional processes: automated agglomeration of oversegmentation and a 2D watershed<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. Thus, there is room for further improvement. Although FFNs require long training time (~2 weeks), users can benefit from their precise inference, which drastically decreases the subsequent proofreading work.</p>
      </sec>
    </sec>
    <sec id="Sec7">
      <title>System design</title>
      <p id="Par29">UNI-EM was developed under the Python development environment and Python bindings for v5 of the Qt application framework for GUI (PyQt5). The combination of Python and PyQt5 is typical for Python GUI desktop applications (e.g., Sommer <italic>et al</italic>.<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>), and UNI-EM utilizes this combination for GUI-equipped 2D CNNs and 3D FFNs (Fig. <xref rid="Fig5" ref-type="fig">5</xref>). The desktop application style is appropriate for CNN computing because CNN training/inference often occupies all of the GPU resources of a desktop computer, and the shared usage of a single GPU is ineffective. On the other hand, Dojo, the 3D annotator, and Tensorboard are web applications. The web application style provides remote accessibility to these applications; hence, multiple users can simultaneously use them (remote users in Fig. <xref rid="Fig5" ref-type="fig">5</xref>). Tensorboard enables the remote inspection of CNN training, Dojo enables multiple users to correct mis-segmentation simultaneously, and the 3D annotator enables multiuser annotation. Together, UNI-EM is comprised of desktop and web application systems, and this heterogeneity enables a wide range of applications from individual to shared use.</p>
    </sec>
  </sec>
  <sec id="Sec8" sec-type="discussion">
    <title>Discussion</title>
    <p id="Par30">We presented a software package called UNI-EM for CNN-based automated EM segmentation. UNI-EM unifies pieces of software for CNN-based segmentation. We validated its effectiveness using two example workflows: mitochondria segmentation using a 2D CNN and neuron segmentation using 3D FFNs. Test users who did not possess Python programming skills were able to perform the overall procedure successfully, and the resulting segmentation accuracies were comparable to those of state-of-the-art methods. Therefore, UNI-EM is a beneficial tool for researchers with limited programming skills.</p>
    <p id="Par31">In recent years, the popularity of CNNs in generic image segmentation as well as EM image segmentation has greatly increased<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. Numerous CNN-based segmentation algorithms have been proposed, and their source codes are often released along with journal publication. However, it is difficult to use such CNN source code as doing so often requires knowledge of Python and a CNN framework. In such situations, UNI-EM provides an opportunity for researchers to examine the effectiveness of multiple CNNs based on their own EM images, without knowledge of Python. Based on the results, they can decide if they want to use these CNNs professionally for large-scale segmentation. UNI-EM therefore functions as a testing platform.</p>
    <p id="Par32">Two-dimensional CNN-based segmentation combined with subsequent Z-slice connection into 3D objects is effective if the target objects have simple shapes like that of mitochondria. In the example workflow, the test users successfully extracted the oval-shaped mitochondria within 2 h, and the segmentation accuracy was higher than those of conventional machine learning methods such as AdaBoost<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>. The proposed approach is also effective for neuron segmentation if the users can utilize high-performance Z slice connectors, such as rule-based connectors<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>, multicut algorithms<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>, and the graph-based active learning of agglomeration<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. Incorporation of these connectors into UNI-EM is an important future direction because the current UNI-EM only provides 3D labeling and 3D watersheds to connect the 2D segments.</p>
    <p id="Par33">Many 3D CNNs have been proposed for highly accurate neuron segmentation<sup><xref ref-type="bibr" rid="CR12">12</xref>,<xref ref-type="bibr" rid="CR34">34</xref>,<xref ref-type="bibr" rid="CR37">37</xref>,<xref ref-type="bibr" rid="CR38">38</xref></sup>. FFNs are one such 3D CNNs<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>, but we have to acknowledge two remaining barriers from its common use. First, FFNs require a long training period over one week. Second, they require a certain amount of 3D ground truth segmentation. In our experience, two-week labor was required to manually draw 3D ground truth using a sophisticated paint tool<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>. FFNs are of course still an excellent selection if we consider the time for manual correction of mis-segmentation arising from other segmentation methods.</p>
    <p id="Par34">The proofreading software Dojo with extensions is one of the main components of UNI-EM<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. Similar to Dojo, numerous excellent proofreading and manual segmentation tools are available, e.g., Reconstruct<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, Ilastik<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>, TrakEM2<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>, VAST lite<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>, Knossos<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>, webKnossos<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>, Microscopy Image Browser<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>, CATMAID<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>, NeuTu<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>, and Neuroglancer<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>. The primary advantage of Dojo is its web application architecture. A web application has numerous advantages; there is no need for the end users to install any software except for the web browser, OS independency, and cloud resource accessibility, and multiuser access is typically included. However, a distinct web/database server needs to be launched. To avoid this task, UNI-EM itself contains the backend web/database server of Dojo. Users can employ UNI-EM as both single-user and collaborative applications, without launching any distinct servers.</p>
    <p id="Par35">Almost all of UNI-EM programs are written in high-level interpreter languages, i.e., Python, JavaScript, HTML, and CSS, and only the matching cube mesh generator is currently written in a C++ compiler language. The interpreter languages generally have lesser abilities to manage CPU and memory resources and show reduced performance. On the other hand, CNN frameworks such as TensorFlow and PyTorch provide application programming interfaces on high-level languages, such as Python. Thus, users can easily incorporate new CNN models into UNI-EM. The instructions for extending UNI-EM are provided in an online manual (see Data availability).</p>
  </sec>
  <sec id="Sec9">
    <title>Methods</title>
    <sec id="Sec10">
      <title>RAND score</title>
      <p id="Par36">We utilized the foreground-restricted RAND score as a metric of segmentation performance<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. The RAND score is defined as follows. Suppose <italic>p</italic><sub><italic>ij</italic></sub> is the joint probability that a target pixel belongs to object <italic>i</italic> of inferred segmentation and object <italic>j</italic> of ground truth segmentation (Σ<sub><italic>ij</italic></sub>
<italic>p</italic><sub><italic>ij</italic></sub> = 1). Subsequently, <italic>s</italic><sub><italic>i</italic></sub> = Σ<sub><italic>j</italic></sub>
<italic>p</italic><sub><italic>ij</italic></sub> is the marginal probability for the inferred segmentation, and <italic>t</italic><sub><italic>j</italic></sub> = Σ<sub><italic>i</italic></sub>
<italic>p</italic><sub><italic>ij</italic></sub> is the marginal probability for the ground truth segmentation. Subsequently, the RAND score, <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${V}_{\alpha }^{{\rm{Rand}}}$$\end{document}</tex-math><mml:math id="M2"><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Rand</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2019_55431_Article_IEq1.gif"/></alternatives></inline-formula>, can be defined as follows:<disp-formula id="Equa"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${V}_{\alpha }^{{\rm{Rand}}}=\frac{{\sum }_{ij}{p}_{ij}^{2}}{\alpha {\sum }_{k}{s}_{k}^{2}+(1-\alpha ){\sum }_{k}{t}_{k}^{2}},$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Rand</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:msub><mml:mo>∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:msubsup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo>∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:msubsup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41598_2019_55431_Article_Equa.gif" position="anchor"/></alternatives></disp-formula>where the RAND F-score <italic>α</italic> is set to be 0.5. The split score (<italic>α</italic> → 0) can be interpreted as the precision in the classification of pixel pairs as belonging to the same (positive class) or different objects (negative class). The merge score (<italic>α</italic> → 1) can be interpreted as recall. Generally, <inline-formula id="IEq2"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${V}_{\alpha }^{{\rm{Rand}}}$$\end{document}</tex-math><mml:math id="M6"><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Rand</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2019_55431_Article_IEq2.gif"/></alternatives></inline-formula> becomes equal to 1 if the segmentation is accurate. Note that, as utilized in a neuron segmentation contest<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, the RAND scores of instance segmentation were obtained in the case of neuron segmentation in the 2D CNNs and FFNs (Figs. <xref rid="Fig2" ref-type="fig">2</xref> and <xref rid="Fig4" ref-type="fig">4</xref>), i.e., isolated neurons were counted as independent objects. On the other hand, the RAND scores of semantic segmentation were obtained in the cases of synapses and mitochondria in the 2D CNNs (Fig. <xref rid="Fig2" ref-type="fig">2</xref>) to compare the scores with those in a 3D CNN<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>.</p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>We would like to thank Ryoji Miyamoto, Noboru Yamaguchi, Hiroko Kita, and Yoshihisa Fujita for their technical assistance. This work was partly supported by AMED Brain/MINDS (JP19dm0207001 to SI and JP19dm0207084 to YK), JST CREST (JPMJCR1652), MEXT KAKENHI on Innovative Areas “Brain information dynamics underlying multi-area interconnectivity and parallel processing” (17H06310 to SI and 17H06311 to YK), RIKEN CBS on the Collaboration Research for Development of Techniques in Brain Science Database Field, and JSPS KAKENHI (17K00404 to HU and 19H03336 to YK).</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>H.U. and S.I. designed the study. H.U. designed the overall software architecture. T.B. wrote the 2D CNN-based segmentation programs. Y.K. tested the software package and provided an example workflow. H.U., T.B., Y.K., S.O. and S.I. wrote the manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>The datasets generated and/or analyzed during the current study are available from the corresponding author upon reasonable request. UNI-EM is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/urakubo/UNI-EM">https://github.com/urakubo/UNI-EM</ext-link>.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par37">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Briggman</surname>
            <given-names>KL</given-names>
          </name>
          <name>
            <surname>Bock</surname>
            <given-names>DD</given-names>
          </name>
        </person-group>
        <article-title>Volume electron microscopy for neuronal circuit reconstruction</article-title>
        <source>Curr. Opin. Neurobiol.</source>
        <year>2012</year>
        <volume>22</volume>
        <fpage>154</fpage>
        <lpage>161</lpage>
        <pub-id pub-id-type="doi">10.1016/j.conb.2011.10.022</pub-id>
        <pub-id pub-id-type="pmid">22119321</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Helmstaedter</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Cellular-resolution connectomics: challenges of dense neural circuit reconstruction</article-title>
        <source>Nat. Methods</source>
        <year>2013</year>
        <volume>10</volume>
        <fpage>501</fpage>
        <lpage>507</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.2476</pub-id>
        <pub-id pub-id-type="pmid">23722209</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Morgan</surname>
            <given-names>JL</given-names>
          </name>
          <name>
            <surname>Lichtman</surname>
            <given-names>JW</given-names>
          </name>
        </person-group>
        <article-title>Why not connectomics?</article-title>
        <source>Nat. Methods</source>
        <year>2013</year>
        <volume>10</volume>
        <fpage>494</fpage>
        <lpage>500</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.2480</pub-id>
        <pub-id pub-id-type="pmid">23722208</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>WC</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Anatomy and function of an excitatory network in the visual cortex</article-title>
        <source>Nature</source>
        <year>2016</year>
        <volume>532</volume>
        <fpage>370</fpage>
        <lpage>374</lpage>
        <pub-id pub-id-type="doi">10.1038/nature17192</pub-id>
        <pub-id pub-id-type="pmid">27018655</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <mixed-citation publication-type="other">Li, P. H. <italic>et al</italic>. Automated reconstruction of a serial-section EM <italic>Drosophila</italic> brain with flood-filling networks and local realignment. <italic>bioRxiv</italic>, 605634 (2019).</mixed-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hildebrand</surname>
            <given-names>DGC</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Whole-brain serial-section electron microscopy in larval zebrafish</article-title>
        <source>Nature</source>
        <year>2017</year>
        <volume>545</volume>
        <fpage>345</fpage>
        <lpage>349</lpage>
        <pub-id pub-id-type="doi">10.1038/nature22356</pub-id>
        <pub-id pub-id-type="pmid">28489821</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Arganda-Carreras</surname>
            <given-names>I</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Crowdsourcing the creation of image segmentation algorithms for connectomics</article-title>
        <source>Front. Neuroanat.</source>
        <year>2015</year>
        <volume>9</volume>
        <fpage>142</fpage>
        <pub-id pub-id-type="doi">10.3389/fnana.2015.00142</pub-id>
        <pub-id pub-id-type="pmid">26594156</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ronneberger</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Fischer</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Brox</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>U-Net: Convolutional networks for biomedical image segmentation</article-title>
        <source>In Medical Image Computing and Computer-Assisted Intervention—MICCAI 2015</source>
        <year>2015</year>
        <volume>9351</volume>
        <fpage>234</fpage>
        <lpage>241</lpage>
        <pub-id pub-id-type="doi">10.1007/978-3-319-24574-4_28</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <mixed-citation publication-type="other">He, K., Zhang, X., Ren, S. &amp; Sun, J. Identity mappings in deep residual networks. <italic>European Conf</italic>. <italic>Comput</italic>. <italic>Vision</italic> 630–645 (2016).</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Srivastava</surname>
            <given-names>RK</given-names>
          </name>
          <name>
            <surname>Greff</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Schmidhuber</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Highway networks</article-title>
        <source>arXiv preprint, arXiv</source>
        <year>2015</year>
        <volume>1505</volume>
        <fpage>00387</fpage>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Huang, G., Liu, Z. &amp; Weinberger, K. Q. Densely connected convolutional networks. <italic>arXiv preprint</italic>, arXiv:1608.06993 (2016).</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Januszewski</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>High-precision automated reconstruction of neurons with flood-filling networks</article-title>
        <source>Nat. Methods</source>
        <year>2018</year>
        <volume>15</volume>
        <fpage>605</fpage>
        <lpage>610</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-018-0049-4</pub-id>
        <pub-id pub-id-type="pmid">30013046</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Takemura</surname>
            <given-names>SY</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Synaptic circuits and their variations within different columns in the visual system of Drosophila</article-title>
        <source>Proc. Natl. Acad. Sci. USA</source>
        <year>2015</year>
        <volume>112</volume>
        <fpage>13711</fpage>
        <lpage>13716</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.1509820112</pub-id>
        <pub-id pub-id-type="pmid">26483464</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kasthuri</surname>
            <given-names>N</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Saturated reconstruction of a volume of neocortex</article-title>
        <source>Cell</source>
        <year>2015</year>
        <volume>162</volume>
        <fpage>648</fpage>
        <lpage>661</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2015.06.054</pub-id>
        <pub-id pub-id-type="pmid">26232230</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kaynig</surname>
            <given-names>V</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Large-scale automatic reconstruction of neuronal processes from electron microscopy images</article-title>
        <source>Med. Image Anal.</source>
        <year>2015</year>
        <volume>22</volume>
        <fpage>77</fpage>
        <lpage>88</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2015.02.001</pub-id>
        <pub-id pub-id-type="pmid">25791436</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Haehn, D. <italic>et al</italic>. Scalable interactive visualization for connectomics. <italic>Informatics</italic><bold>4</bold> (2017).</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bae</surname>
            <given-names>JA</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Digital museum of retinal ganglion cells with dense anatomy and physiology</article-title>
        <source>Cell</source>
        <year>2018</year>
        <volume>173</volume>
        <fpage>1293</fpage>
        <lpage>1306 e1219</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2018.04.040</pub-id>
        <pub-id pub-id-type="pmid">29775596</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fiala</surname>
            <given-names>JC</given-names>
          </name>
        </person-group>
        <article-title>Reconstruct: A free editor for serial section microscopy</article-title>
        <source>J. Microsc.</source>
        <year>2005</year>
        <volume>218</volume>
        <fpage>52</fpage>
        <lpage>61</lpage>
        <pub-id pub-id-type="doi">10.1111/j.1365-2818.2005.01466.x</pub-id>
        <pub-id pub-id-type="pmid">15817063</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Sommer, C., Straehle, C., Kothe, U. &amp; Hamprecht, F. A. Ilastik: Interactive learning and segmentation toolkit. <italic>2011 8th IEEE International Symposium on Biomedical Imaging: From Nano to Macro</italic>, 230–233 (2011).</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Helmstaedter</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Briggman</surname>
            <given-names>KL</given-names>
          </name>
          <name>
            <surname>Denk</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>High-accuracy neurite reconstruction for high-throughput neuroanatomy</article-title>
        <source>Nat. Neurosci.</source>
        <year>2011</year>
        <volume>14</volume>
        <fpage>1081</fpage>
        <lpage>1088</lpage>
        <pub-id pub-id-type="doi">10.1038/nn.2868</pub-id>
        <pub-id pub-id-type="pmid">21743472</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Belevich</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Joensuu</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kumar</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Vihinen</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Jokitalo</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Microscopy image browser: A platform for segmentation and analysis of multidimensional datasets</article-title>
        <source>PLoS Biol.</source>
        <year>2016</year>
        <volume>14</volume>
        <fpage>e1002340</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pbio.1002340</pub-id>
        <pub-id pub-id-type="pmid">26727152</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Berger</surname>
            <given-names>DR</given-names>
          </name>
          <name>
            <surname>Seung</surname>
            <given-names>HS</given-names>
          </name>
          <name>
            <surname>Lichtman</surname>
            <given-names>JW</given-names>
          </name>
        </person-group>
        <article-title>VAST (Volume Annotation and Segmentation Tool): Efficient manual and semi-automatic labeling of large 3D image stacks</article-title>
        <source>Front. Neural Circuits</source>
        <year>2018</year>
        <volume>12</volume>
        <fpage>88</fpage>
        <pub-id pub-id-type="doi">10.3389/fncir.2018.00088</pub-id>
        <pub-id pub-id-type="pmid">30386216</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Falk</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>U-Net: Deep learning for cell counting, detection, and morphometry</article-title>
        <source>Nat. Methods</source>
        <year>2019</year>
        <volume>16</volume>
        <fpage>67</fpage>
        <lpage>70</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-018-0261-2</pub-id>
        <pub-id pub-id-type="pmid">30559429</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Abadi, M. <italic>et al</italic>. In <italic>Proceedings of the 12th USENIX conference on Operating Systems Design and Implementation</italic> 265–283 (USENIX Association, Savannah, GA, USA, 2016).</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Haehn</surname>
            <given-names>D</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Design and evaluation of interactive proofreading tools for connectomics</article-title>
        <source>Proceedings IEEE SciVis</source>
        <year>2014</year>
        <volume>20</volume>
        <fpage>2466</fpage>
        <lpage>2475</lpage>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Arganda-Carreras, I., Seung, H. S., Vishwanathan, A. &amp; Berger, D. R. SNEMI3D: 3D Segmentation of neurites in EM images, <ext-link ext-link-type="uri" xlink:href="http://brainiac2.mit.edu/SNEMI3D/">http://brainiac2.mit.edu/SNEMI3D/</ext-link> (2013).</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Saxton</surname>
            <given-names>WM</given-names>
          </name>
          <name>
            <surname>Hollenbeck</surname>
            <given-names>PJ</given-names>
          </name>
        </person-group>
        <article-title>The axonal transport of mitochondria</article-title>
        <source>J. Cell Sci.</source>
        <year>2012</year>
        <volume>125</volume>
        <fpage>2095</fpage>
        <lpage>2104</lpage>
        <pub-id pub-id-type="doi">10.1242/jcs.053850</pub-id>
        <pub-id pub-id-type="pmid">22619228</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ohno</surname>
            <given-names>N</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Mitochondrial immobilization mediated by syntaphilin facilitates survival of demyelinated axons</article-title>
        <source>Proc. Natl. Acad. Sci. USA</source>
        <year>2014</year>
        <volume>111</volume>
        <fpage>9953</fpage>
        <lpage>9958</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.1401155111</pub-id>
        <pub-id pub-id-type="pmid">24958879</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nunnari</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Suomalainen</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Mitochondria: in sickness and in health</article-title>
        <source>Cell</source>
        <year>2012</year>
        <volume>148</volume>
        <fpage>1145</fpage>
        <lpage>1159</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2012.02.035</pub-id>
        <pub-id pub-id-type="pmid">22424226</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Frey</surname>
            <given-names>TG</given-names>
          </name>
          <name>
            <surname>Mannella</surname>
            <given-names>CA</given-names>
          </name>
        </person-group>
        <article-title>The internal structure of mitochondria</article-title>
        <source>Trends Biochem. Sci.</source>
        <year>2000</year>
        <volume>25</volume>
        <fpage>319</fpage>
        <lpage>324</lpage>
        <pub-id pub-id-type="doi">10.1016/S0968-0004(00)01609-1</pub-id>
        <pub-id pub-id-type="pmid">10871882</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Oztel, I., Yolcu, G., Ersoy, I., White, T. &amp; Bunyak, F. Mitochondria segmentation in electron microscopy volumes using deep convolutional neural network. <italic>IEEE Int</italic>. <italic>C</italic>. <italic>Bioinform</italic>. 1195–1200 (2017).</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xiao</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Automatic mitochondria segmentation for EM data using a 3D supervised convolutional network</article-title>
        <source>Front. Neuroanat.</source>
        <year>2018</year>
        <volume>12</volume>
        <fpage>92</fpage>
        <pub-id pub-id-type="doi">10.3389/fnana.2018.00092</pub-id>
        <pub-id pub-id-type="pmid">30450040</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ciresan</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Giusti</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gambardella</surname>
            <given-names>LM</given-names>
          </name>
          <name>
            <surname>Jurgen</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Deep neural networks segment neuronal membranes in electron microscopy images</article-title>
        <source>Adv. Neural Inf. Process. Syst.</source>
        <year>2012</year>
        <volume>25</volume>
        <fpage>2843</fpage>
        <lpage>2851</lpage>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Lee, K., Zung, J., Li, P., Jain, V. &amp; Seung, H. S. Superhuman accuracy on the SNEMI3D connectomics challenge. <italic>arXiv preprint</italic>, arXiv:1706.00120 (2017).</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Beier</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Multicut brings automated neurite segmentation closer to human performance</article-title>
        <source>Nat. Methods</source>
        <year>2017</year>
        <volume>14</volume>
        <fpage>101</fpage>
        <lpage>102</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.4151</pub-id>
        <pub-id pub-id-type="pmid">28139671</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nunez-Iglesias</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kennedy</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Plaza</surname>
            <given-names>SM</given-names>
          </name>
          <name>
            <surname>Chakraborty</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Katz</surname>
            <given-names>WT</given-names>
          </name>
        </person-group>
        <article-title>Graph-based active learning of agglomeration (GALA): A Python library to segment 2D and 3D neuroimages</article-title>
        <source>Front. Neuroinform.</source>
        <year>2014</year>
        <volume>8</volume>
        <fpage>34</fpage>
        <pub-id pub-id-type="doi">10.3389/fninf.2014.00034</pub-id>
        <pub-id pub-id-type="pmid">24772079</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Lee, K., Zlateski, A., Vishwanathan, A. &amp; Seung, H. S. In <italic>Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2</italic> 3573-3581 (MIT Press, Montreal, Canada, 2015).</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zeng</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>DeepEM3D: Approaching human-level performance on 3D anisotropic EM image segmentation</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>33</volume>
        <fpage>2555</fpage>
        <lpage>2562</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx188</pub-id>
        <pub-id pub-id-type="pmid">28379412</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cardona</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>TrakEM2 software for neural circuit reconstruction</article-title>
        <source>PLoS One</source>
        <year>2012</year>
        <volume>7</volume>
        <fpage>e38011</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0038011</pub-id>
        <pub-id pub-id-type="pmid">22723842</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Boergens</surname>
            <given-names>KM</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>WebKnossos: Efficient online 3D data annotation for connectomics</article-title>
        <source>Nat. Methods</source>
        <year>2017</year>
        <volume>14</volume>
        <fpage>691</fpage>
        <lpage>694</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.4331</pub-id>
        <pub-id pub-id-type="pmid">28604722</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Saalfeld</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Cardona</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Hartenstein</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Tomancak</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>CATMAID: Collaborative annotation toolkit for massive amounts of image data</article-title>
        <source>Bioinformatics</source>
        <year>2009</year>
        <volume>25</volume>
        <fpage>1984</fpage>
        <lpage>1986</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btp266</pub-id>
        <pub-id pub-id-type="pmid">19376822</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Olbris</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Plaza</surname>
            <given-names>SM</given-names>
          </name>
        </person-group>
        <article-title>NeuTu: Software for collaborative, large-scale, segmentation-based connectome reconstruction</article-title>
        <source>Front. Neural Circuits</source>
        <year>2018</year>
        <volume>12</volume>
        <fpage>101</fpage>
        <pub-id pub-id-type="doi">10.3389/fncir.2018.00101</pub-id>
        <pub-id pub-id-type="pmid">30483068</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Neuroglancer: webGL-based viewer for volumetric data, <ext-link ext-link-type="uri" xlink:href="https://github.com/google/neuroglancer">https://github.com/google/neuroglancer</ext-link> (2016).</mixed-citation>
    </ref>
  </ref-list>
</back>
