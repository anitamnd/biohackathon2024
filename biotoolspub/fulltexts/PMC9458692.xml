<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Distrib Parallel Databases</journal-id>
    <journal-id journal-id-type="iso-abbrev">Distrib Parallel Databases</journal-id>
    <journal-title-group>
      <journal-title>Distributed and Parallel Databases</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">0926-8782</issn>
    <issn pub-type="epub">1573-7578</issn>
    <publisher>
      <publisher-name>Springer US</publisher-name>
      <publisher-loc>New York</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9458692</article-id>
    <article-id pub-id-type="publisher-id">7414</article-id>
    <article-id pub-id-type="doi">10.1007/s10619-022-07414-w</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Bio-SODA UX: enabling natural language question answering over knowledge graphs with user disambiguation</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Sima</surname>
          <given-names>Ana Claudia</given-names>
        </name>
        <address>
          <email>ana-claudia.sima@sib.swiss</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Mendes de Farias</surname>
          <given-names>Tarcisio</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Anisimova</surname>
          <given-names>Maria</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Dessimoz</surname>
          <given-names>Christophe</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
        <xref ref-type="aff" rid="Aff6">6</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Robinson-Rechavi</surname>
          <given-names>Marc</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zbinden</surname>
          <given-names>Erich</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Stockinger</surname>
          <given-names>Kurt</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.419765.8</institution-id><institution-id institution-id-type="ISNI">0000 0001 2223 3006</institution-id><institution>SIB Swiss Institute of Bioinformatics, </institution></institution-wrap>Lausanne, Switzerland </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.9851.5</institution-id><institution-id institution-id-type="ISNI">0000 0001 2165 4204</institution-id><institution>Department of Computational Biology, </institution><institution>University of Lausanne, </institution></institution-wrap>Lausanne, Switzerland </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.9851.5</institution-id><institution-id institution-id-type="ISNI">0000 0001 2165 4204</institution-id><institution>Department of Ecology and Evolution, </institution><institution>University of Lausanne, </institution></institution-wrap>Lausanne, Switzerland </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.19739.35</institution-id><institution-id institution-id-type="ISNI">0000000122291644</institution-id><institution>ZHAW Zurich University of Applied Sciences, </institution></institution-wrap>Zurich, Switzerland </aff>
      <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.83440.3b</institution-id><institution-id institution-id-type="ISNI">0000000121901201</institution-id><institution>Department of Genetics, Evolution, and Environment, </institution><institution>University College London, </institution></institution-wrap>London, UK </aff>
      <aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="GRID">grid.83440.3b</institution-id><institution-id institution-id-type="ISNI">0000000121901201</institution-id><institution>Department of Computer Science, </institution><institution>University College London, </institution></institution-wrap>London, UK </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>16</day>
      <month>7</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>16</day>
      <month>7</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2022</year>
    </pub-date>
    <volume>40</volume>
    <issue>2-3</issue>
    <fpage>409</fpage>
    <lpage>440</lpage>
    <history>
      <date date-type="accepted">
        <day>21</day>
        <month>6</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Â© The Author(s) 2022</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">The problem of natural language processing over structured data has become a growing research field, both within the relational database and the Semantic Web community, with significant efforts involved in question answering over knowledge graphs (KGQA). However, many of these approaches are either specifically targeted at <italic>open-domain</italic> question answering using DBpedia, or require <italic>large training datasets</italic> to translate a natural language question to SPARQL in order to query the knowledge graph. Hence, these approaches often cannot be applied directly to complex <italic>scientific datasets</italic> where no prior training data is available. In this paper, we focus on the challenges of natural language processing over knowledge graphs of scientific datasets. In particular, we introduce Bio-SODA, a natural language processing engine that does not require training data in the form of question-answer pairs for generating SPARQL queries. Bio-SODA uses a generic graph-based approach for translating user questions to a ranked list of SPARQL candidate queries. Furthermore, Bio-SODA uses a novel ranking algorithm that includes node centrality as a measure of relevance for selecting the best SPARQL candidate query. Our experiments with real-world datasets across several scientific domains, including the official <italic>bioinformatics</italic> Question Answering over Linked Data (QALD) challenge, as well as the CORDIS dataset of European projects, show that Bio-SODA outperforms publicly available KGQA systems by an F1-score of least 20% and by an even higher factor on more complex bioinformatics datasets. Finally, we introduce Bio-SODA UX, a graphical user interface designed to assist users in the exploration of large knowledge graphs and in dynamically disambiguating natural language questions that target the data available in these graphs.</p>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Question answering</kwd>
      <kwd>Knowledge graphs</kwd>
      <kwd>Ranking</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001711</institution-id>
            <institution>Schweizerischer Nationalfonds zur FÃ¶rderung der Wissenschaftlichen Forschung</institution>
          </institution-wrap>
        </funding-source>
        <award-id>407540 167149</award-id>
        <principal-award-recipient>
          <name>
            <surname>Stockinger</surname>
            <given-names>Kurt</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>University of Lausanne</institution>
        </funding-source>
      </award-group>
      <open-access>
        <p>Open access funding provided by University of Lausanne</p>
      </open-access>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>Â© Springer Science+Business Media, LLC, part of Springer Nature 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">The problem of natural language processing over structured data has gained significant traction, both in the Semantic Web communityâwith a focus on answering natural language questions over RDF graph databases [<xref ref-type="bibr" rid="CR1">1</xref>â<xref ref-type="bibr" rid="CR3">3</xref>]âand in the relational database community, where the goal is to answer questions by finding their semantically equivalent translations to SQL [<xref ref-type="bibr" rid="CR4">4</xref>â<xref ref-type="bibr" rid="CR7">7</xref>]. Significant research efforts have been invested in particular in <italic>open-domain</italic> question answering over knowledge graphs. These efforts often use the DBpedia and/or Wikidata knowledge bases, that are composed of structured content from various Wikimedia projects such as Wikipedia. A growing ecosystem of tools is therefore becoming available for solving subtasks of the KGQA problem, such as entity linking [<xref ref-type="bibr" rid="CR8">8</xref>â<xref ref-type="bibr" rid="CR11">11</xref>] or query generation [<xref ref-type="bibr" rid="CR12">12</xref>]. However, most of these tools are specifically targeted at question answering over DBpedia [<xref ref-type="bibr" rid="CR13">13</xref>], not having been applied to other contexts, such as for scientific datasets.</p>
    <p id="Par3">On the one hand, encouraged by the recent success of machine learning methods, several new benchmarks for training and evaluating KGQA systems have been published [<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR15">15</xref>]. On the other hand, most of the existing datasets are synthetic (i.e., not based on real query logs) and generally limited to DBpedia or Wikidata, which may not be representative of knowledge graphs for scientific datasets.</p>
    <p id="Par4">For example, one of the major question answering datasets over DBpedia, LC-Quad [<xref ref-type="bibr" rid="CR14">14</xref>], as well as its updated version, LC-Quad 2.0 [<xref ref-type="bibr" rid="CR15">15</xref>], include only <italic>simple multi-fact questions</italic> that connect at most two facts. In other words, these queries cover at most two or three triple patterns, with a query graph spanning a maximum of two hops, whereas real-world questions tend to be much more complex. In particular, a study of SPARQL query logs [<xref ref-type="bibr" rid="CR16">16</xref>] across multiple knowledge graphs, including DBpedia, has shown that a significant fraction of real-world queries have 10 triple patterns or more. It therefore remains unclear whether existing training sets can serve as representative for real-world natural language processing engines over knowledge graphs in general. All in all, data access and retrieval remain challenging for domain experts who are not familiar with structured query languages, nor with the data models of each scientific dataset that they use.</p>
    <p id="Par5">To illustrate the general problem of natural language processing over knowledge graphs, consider the simple data model in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. Here we see that a drug could be a <italic>possible disease target</italic> for asthma (left branch), as well as potentially having <italic>side effects</italic> such as triggering asthma symptoms (right branch). Now consider the following natural language question: <italic>âWhich drugs are used for asthma?â</italic>. Note that our knowledge graph has no concept or property called <italic>used for</italic>. Hence, this question cannot be easily translated without relying on external knowledge (<italic>e.g.</italic> training data), given that <italic>used for</italic> cannot be directly mapped to either of the two properties (<italic>possibleDiseaseTarget</italic> or <italic>sideEffect</italic>) shown in the figure. However, node centrality metrics, such as the PageRank score of nodes in the knowledge graph, can help capture âcommon senseâ knowledge, <italic>e.g.</italic>, that <italic>asthma</italic> is more commonly a <italic>Disease</italic>, rather than a <italic>Side Effect</italic>.<fig id="Fig1"><label>Fig. 1</label><caption><p>Illustrative data model, simplified from the QALD4 benchmark datasets [<xref ref-type="bibr" rid="CR17">17</xref>]. Consider the following question: <italic>âWhich drugs are used for asthma?â</italic>. In the QALD4 dataset, âasthmaâ appears as both a disease instance (shown in green), as well as a side effect (shown in red). The second interpretation describes drugs that can <italic>trigger</italic> asthma symptoms. Therefore, it is the opposite of the userâs intended question. However, the predicate <italic>used for</italic> in the question cannot be easily linked to either of the properties indicated through arrows in the image. Due to ambiguity, the question is difficult to translate correctly in the absence of external knowledge, without relying on training data (inferring that <italic>used for</italic> implies drug targeting disease) (Color figure online)</p></caption><graphic xlink:href="10619_2022_7414_Fig1_HTML" id="MO1"/></fig></p>
    <p id="Par6">As a step towards bridging the current gap in natural language processing for knowledge graphs of scientific datasets, we introduce Bio-SODA, a system designed to answer <italic>natural language questions</italic> across knowledge graphs where <italic>no prior training data</italic> is available. Bio-SODA relies on a generic graph-based approach in order to translate natural language questions into SPARQL queries. Furthermore, Bio-SODA is designed to <italic>compensate for incompleteness in the data</italic>âeither due to missing schema information or, to some extent, due to missing labels. Although these situations should not occur when following ontology engineering best practices for representing data in RDF, our experience in working with real-world datasets shows that these problems are frequent in practice.</p>
    <p id="Par7">We make our prototype implementation available open-source<xref ref-type="fn" rid="Fn1">1</xref>. We also make available a live demo of Bio-SODA online<xref ref-type="fn" rid="Fn2">2</xref>, where each of the datasets considered in this paper can be queried. The prototype enables both keyword search, as well as full question answering in English. We chose bioinformatics as our <italic>primary</italic> target domain, motivated by the rapid growth of publicly available RDF data in this scientific domain. Specifically, around 8% of the Linked Open Data Cloud originates from the Life Sciences [<xref ref-type="bibr" rid="CR18">18</xref>]. For the purpose of evaluating our system, we use several real-world datasets stemming from different domains. For example, we use the last bioinformatics question answering challenge released as part of the official Question Answering on Large Databases (QALD) series, namely the QALD4 biomedical task [<xref ref-type="bibr" rid="CR17">17</xref>]. Importantly, to-date there is no sufficiently large training dataset of questions and corresponding SPARQL queries to enable the use of machine learning approaches for end-to-end Question Answering in the biomedical field. Finally, to demonstrate the generalizability of Bio-SODA to other domains, we also apply our system to an entirely different context, outside bioinformatics, namely on the CORDIS dataset describing European Union (EU) funded research projects<xref ref-type="fn" rid="Fn3">3</xref>. This dataset is also used in the EU-project INODE (Intelligent Open Data Exploration) [<xref ref-type="bibr" rid="CR19">19</xref>].</p>
    <p id="Par11">This paper, which is an extended version of [<xref ref-type="bibr" rid="CR20">20</xref>], makes the following <bold>contributions</bold>:<list list-type="bullet"><list-item><p id="Par12">We introduce Bio-SODAâa novel natural language processing engine over knowledge graphs that <italic>does not require prior training data</italic> (question-answer pairs) for translating natural language questions into SPARQL.</p></list-item><list-item><p id="Par13">We define a novel ranking algorithm for selecting the best automatically generated SPARQL statements in response to a given natural language question. The ranking algorithm combines <italic>syntactic and semantic similarity</italic>, as well as <italic>node centrality</italic> in the knowledge graph. Many existing question answering systems either rely on simple metrics for ranking, such as the length of the answer query graph [<xref ref-type="bibr" rid="CR6">6</xref>], or require extensive training data in order to learn a ranking function [<xref ref-type="bibr" rid="CR21">21</xref>]. To the best of our knowledge, our approach is the first to take into account all three factors (syntactic and semantic similarity, as well as node centrality) for ranking queries.</p></list-item><list-item><p id="Par14">Our experiments on various real-world datasets show that Bio-SODA outperforms state-of-the-art KGQA systems by 20% on the F1-score using the official QALD4 biomedical benchmark and by an even higher factor on the more complex bioinformatics dataset.</p></list-item><list-item><p id="Par15">Finally, in addition to the work presented in the conference version of this paper (see [<xref ref-type="bibr" rid="CR20">20</xref>]), here we introduce Bio-SODA UX, a prototype graphical user interface enabling users to interact with knowledge graph data and disambiguate natural language questions over the data dynamically; we demonstrate through selected use cases how the interface can assist users in exploring the available data and in finding the information of interest from the underlying knowledge graph.</p></list-item></list>The paper is structured as follows: Sect. <xref rid="Sec2" ref-type="sec">2</xref> places our contribution in the context of the related work. In Sect. <xref rid="Sec3" ref-type="sec">3</xref> we introduce some of the challenges of natural language processing over RDF-based knowledge graphs. In Sect. <xref rid="Sec4" ref-type="sec">4</xref> we explain the high level architecture of Bio-SODA through a concrete example from the biomedical domain. In Sect. <xref rid="Sec5" ref-type="sec">5</xref> we present the detailed system architecture of Bio-SODA followed by a description of the Bio-SODA UX user interface in Sect. <xref rid="Sec8" ref-type="sec">6</xref>. Next, we describe the datasets used for evaluation, their specific challenges and the results obtained, in Sect. <xref rid="Sec14" ref-type="sec">7</xref>. In Sect. <xref rid="Sec20" ref-type="sec">8</xref> we discuss lessons learned from building a natural language processing system for real-world domain datasets. We outline directions for future work in Sect. <xref rid="Sec21" ref-type="sec">9</xref>.</p>
  </sec>
  <sec id="Sec2">
    <title>Related work</title>
    <p id="Par16">The problem of natural language processing and question answering over structured data has been well-studied in recent years, with a growing number of published systems, particularly in open-domain question answering. Recent surveys on natural language interfaces to databases include [<xref ref-type="bibr" rid="CR22">22</xref>, <xref ref-type="bibr" rid="CR23">23</xref>]. However, in this paper we focus on natural language interfaces to <italic>RDF graph databases or RDF-based knowledge graphs</italic>. Natural language interfaces to relational databases are outside the scope of this paper.</p>
    <p id="Par17">In parallel, the biomedical field has seen a growth of dedicated systems for question answering. Examples include GFMed [<xref ref-type="bibr" rid="CR24">24</xref>] and Pomelo [<xref ref-type="bibr" rid="CR25">25</xref>] â the two highest ranked systems in the QALD4 biomedical challenge â as well as more recent systems [<xref ref-type="bibr" rid="CR26">26</xref>]. However, these are generally considered expert systems, with lower generalizability to other domains, given that they extensively rely on manually handcrafted rules and domain expertise.</p>
    <p id="Par18">Our work aims to bridge the gap between the two parallel efforts by solving the <italic>common case</italic> in a domain-independent manner. For this, Bio-SODA relies on a generic graph-based approach in order to generate a ranked list of candidate SPARQL queries from a given question. We enable the addition of custom rules only for <italic>special cases</italic> when needed.</p>
    <p id="Par19">Many recent KGQA systems [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR3">3</xref>] have been evaluated using the LC-Quad benchmark of 5000 questions over DBpedia [<xref ref-type="bibr" rid="CR14">14</xref>]. Although this benchmark is an important step forward, particularly for enabling machine learning approaches, it does not include complex multi-hop questions, which makes it unclear how the results would generalize to this case. For example, the current publicly available implementation of the SPARQL query generation system SQG [<xref ref-type="bibr" rid="CR12">12</xref>], would not work for complex question answering on a new knowledge graph without significant changes to the code base, as it targets question answering over DBpedia and more specifically in the format required by the LC-Quad benchmark.</p>
    <p id="Par20">More recent KGQA systems, such as [<xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR27">27</xref>], support multiple knowledge graphs, but are limited to queries with a complexity of at most three triple patterns. Similarly, existing end-to-end QA systems, based on machine learning approaches, such as [<xref ref-type="bibr" rid="CR28">28</xref>], can only handle simple questions. These approaches have the added drawback that they only generate a single answer, as opposed to multiple candidates. Furthermore, end-to-end approaches suffer from the lack of explainability, which makes it challenging for users to validate the correctness of the result. Explainability in this context has therefore become an active area of research, with solutions proposed including translating back structured queries into natural language sentences [<xref ref-type="bibr" rid="CR29">29</xref>â<xref ref-type="bibr" rid="CR31">31</xref>] or summarizing the entities in the results [<xref ref-type="bibr" rid="CR32">32</xref>].</p>
    <p id="Par21">Disambiguation is one of the major tasks of question answering systems. One possible solution for this is to limit the interface to a controlled natural language and involve the user in constructing questions from the available building blocks. Sparklis [<xref ref-type="bibr" rid="CR33">33</xref>] is a query building system that enables answering controlled natural language questions over knowledge graphs out-of-the-box. However, this process is manual and therefore time-consuming, which makes it less convenient than a true natural language interface.</p>
    <p id="Par22">One of the systems closest to ours is the KG-agnostic WDAqua-core1 [<xref ref-type="bibr" rid="CR1">1</xref>]. The system supports multiple knowledge bases in several languages. However, the system is only available as a demo. Although the authors mention that node relevance can in principle be taken into account for ranking, it is not clear whether the approach was used in the evaluation or whether the ranking function was learned based on training data. An updated version of this question answering system, QAnswer, is presented in [<xref ref-type="bibr" rid="CR34">34</xref>], however this system is also limited to support at most 3 triple-pattern queries.</p>
  </sec>
  <sec id="Sec3">
    <title>Challenges of natural language processing over knowledge graphs</title>
    <p id="Par23">In this section we summarise some of the challenges of natural language processing over knowledge graphs, focusing on scientific knowledge graphs, which shape the architecture of the Bio-SODA system (described in Sects. <xref rid="Sec4" ref-type="sec">4</xref> and <xref rid="Sec5" ref-type="sec">5</xref>).<list list-type="bullet"><list-item><p id="Par24"><bold>Lack of training data.</bold></p><p id="Par25">For many scientific knowledge graphs there is no sufficiently long and diverse log of questions and their corresponding queries in order to derive a representative training set for a machine learning-based solution. So far, existing training corpora have proven costly to construct [<xref ref-type="bibr" rid="CR14">14</xref>], with the added drawback that any semi-automatically generated dataset risks compiling a set of question-answer pairs that are non-representative for the information needs of real users of the KGQA system, <italic>e.g.</italic> domain experts.</p></list-item><list-item><p id="Par26"><bold>Rule-based approaches perform well, but are costly to build and maintain.</bold></p><p id="Par27">So far, state-of-the-art solutions for question answering over generic RDF-based knowledge graphs have been mostly rule-based systems, relying on manually handcrafted rules. For example, GFMed [<xref ref-type="bibr" rid="CR24">24</xref>] and Pomelo [<xref ref-type="bibr" rid="CR25">25</xref>], the top 2 ranked systems in the QALD4 biomedical challenge, have achieved very good results in the challenge, but at the cost of very little generality. In essence, these systems suffer from significant overfitting: to be applicable to a new domain, their rule sets would need extensive or even complete rewriting. Moreover, even for a new dataset within the same domain, for which the schema differs, new rules need to be added in order to accommodate the differences.</p><p id="Par28">In some cases it is beneficial to incorporate a minimal set of rules in KGQA systems, particularly for deriving complex concepts. However, this should be a last resort and not the main translation mechanism, given that a large rule set is hard to maintain and scale.</p></list-item><list-item><p id="Par29"><bold>Schema-less, incomplete data.</bold></p><p id="Par30">One of the strengths of relational databases is to have a database schema which enables strict data modelling and guarantees certain data integrity and data quality aspects. However, since RDF does not strictly enforce a (database) schema, real-world datasets using RDF knowledge graphs often exhibit poor structure [<xref ref-type="bibr" rid="CR35">35</xref>, <xref ref-type="bibr" rid="CR36">36</xref>]. Typical examples are properties with missing or generic domains and ranges. In other words, a question answering system over RDF knowledge graphs typically does not have complete schema information. Hence, an important step when working with such incomplete knowledge graphs is to enrich the existing (incomplete) schema, for example, by inferring property ranges and domains based on instance-level data.</p></list-item><list-item><p id="Par31"><bold>Disambiguation.</bold></p><p id="Par32">In many cases, different users have different expectations (query intents) when asking <italic>the same</italic> question. An example would be the question <italic>What are all the Big Data projects?</italic>, asked over the European Projects dataset. Possible interpretations of this request are either to retrieve all projects in a Big Data call, or all projects by institutions that have the term <italic>Big Data</italic> in their name or all projects whose title or abstract include the terms <italic>Big Data</italic> etc. The system should ultimately let users decide which interpretation was intended when asking the question, also informing them of the range of possible options, according to the available underlying data.</p></list-item></list></p>
  </sec>
  <sec id="Sec4">
    <title>Bio-SODA: a high-level perspective</title>
    <p id="Par33">In this section we use a motivating example to illustrate the natural language processing pipeline of Bio-SODA.</p>
    <p id="Par34">Consider the data model illustrated in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, which combines four different scientific databases. The database <italic>Bgee</italic> on the left contains information about genes and in which parts of the body (anatomical entity) a gene is expressed or absent. The database <italic>Diseasome</italic> in the middle contains information about diseases, as well as drugs targeting each disease. In addition, the drugs are part of the pharmaceutical database <italic>DrugBank</italic> (not explicitly shown in the figure). Finally, the database <italic>Sider</italic> contains information about drugs and their side effects. Correspondences between equivalent drugs in Sider and DrugBank are made through the <italic>sameAs</italic> property.<fig id="Fig2"><label>Fig. 2</label><caption><p>Simplified data model based on the Bgee database and QALD4 [<xref ref-type="bibr" rid="CR17">17</xref>] datasets. The data model is a multigraph, including disjoint properties â such as <italic>isAbsentIn</italic> and <italic>isExpressedIn</italic>, as well as inverse properties, such as <italic>possibleDiseaseTarget</italic> and <italic>possibleDrug</italic>. To make matters more complicated, a <italic>Side Effect</italic> and a <italic>Disease</italic> can be described by the same terms, with instances of the two classes being related via the <italic>sameAs</italic> property. As a result, even simple questions such as <italic>âwhich drugs might lead to strokes?â</italic> are hard to automatically translate correctly in the absence of external knowledge (<italic>i.e.</italic> âlead toâ = âside effectâ)</p></caption><graphic xlink:href="10619_2022_7414_Fig2_HTML" id="MO2"/></fig></p>
    <p id="Par35">Further assume that a domain expert is interested in answering the question: â<italic>What are the drugs for diseases associated with the BRCA</italic><xref ref-type="fn" rid="Fn4">4</xref> genes?â.<fig id="Fig3"><label>Fig. 3</label><caption><p>Simplified answer pipeline for the query â<italic>What are the drugs for diseases associated with the BRCA genes?</italic>â. For the sake of simplicity, PageRank scores are solely displayed when more than one match is found</p></caption><graphic xlink:href="10619_2022_7414_Fig3_HTML" id="MO3"/></fig></p>
    <p id="Par37">The natural language processing pipeline of Bio-SODA for answering this question is illustrated in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. In particular, the main steps involved in translating the natural language question to SPARQL are as follows: first, Bio-SODA matches question tokens, such as âdrugsâ and âdiseasesâ, against the data stored in the database, using an inverted index. This step is called <italic>Lookup Candidate Match</italic>. In this example, all tokens are of length one, <italic>i.e.</italic> composed of a single word. The inverted index enables retrieving not only the URI of each matching candidate, but also its <italic>PageRank</italic> score. An example is shown in parentheses for the first two tokens in the Figure. In addition, the inverted index retrieves the <italic>class and property names</italic> of the match (omitted in the figure for simplicity). For example, the lookup for â<italic>BRCA</italic>â retrieves instances of the class <italic>Diseasome:Genes</italic>, where the <italic>rdfs:label</italic> property matches the user token (â<italic>BRCA1</italic>â, â<italic>BRCA2</italic>â). A few simplified Inverted Index entries are provided in Table <xref rid="Tab1" ref-type="table">1</xref>.</p>
    <p id="Par38">In the <italic>Ranking</italic> step, candidates are grouped together according to class/property<xref ref-type="fn" rid="Fn5">5</xref> and ranked according to string similarity and PageRank score.</p>
    <p id="Par40">In the <italic>Query Graph Construction</italic> step, all the ranked candidates are used to construct a query graph which represents one possible answer or interpretation of the natural language question. For simplicity, Fig. <xref rid="Fig3" ref-type="fig">3</xref> only shows the query graph obtained for the top ranked candidate matches. However, Bio-SODA generates multiple alternative interpretations, for example, also including the interpretation considering <italic>Sider:Drugs</italic> instead of the <italic>DrugBank:Drugs</italic>. This can be tested in the demo page of Bio-SODA for QALD4.</p>
    <p id="Par41">Next, Bio-SODA generates the corresponding SPARQL query for each query graph. Finally, the results are returned by executing the query on the target knowledge graph (see bottom of Fig. <xref rid="Fig3" ref-type="fig">3</xref>).</p>
  </sec>
  <sec id="Sec5">
    <title>Bio-SODA: system architecture</title>
    <p id="Par42">The main building blocks of the Bio-SODA system architecture, shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>, are the following:<list list-type="bullet"><list-item><p id="Par43"><italic>Preprocessing Phase</italic>: This phase includes building indexes for efficient lookup as well as automatically generating a schema graph, which will serve as the basis for constructing candidate SPARQL queries in response to user questions. This phase is only executed once, when initialising the system.</p></list-item><list-item><p id="Par44"><italic>SPARQL Query Generation Phase</italic>: This phase represents the natural language query translation process and includes (1) looking up query tokens in the database, (2) ranking the candidate tokens, (3) constructing the candidate query graphs, (4) ranking the query graphs in order of relevance to the user question; and finally (5) constructing a valid SPARQL query and presenting the results.</p></list-item></list>Additionally, the Bio-SODA UX interface<xref ref-type="fn" rid="Fn6">6</xref>, discussed in Sect. <xref rid="Sec8" ref-type="sec">6</xref>, introduces a further, iterative phase:<list list-type="bullet"><list-item><p id="Par46"><italic>User Dialog and Disambiguation</italic>:</p><p id="Par47">More and more RDF datasets are available in diverse scientific fields, yet for practitioners, they are often difficult to explore. Indeed, the increasing size and complexity of the data necessitate not only faster indexes but also smarter user interfaces providing dynamic querying and filtering possibilities. Our experience in prototyping Bio-SODA showed that, in order to enable data exploration, the system must also guide the user in the process of exploring the data models, assist in disambiguating questions, and finally dynamically choose the most relevant answers for specific use cases. To this purpose we designed the Bio-SODA UX interface, which can be operated online<xref ref-type="fn" rid="Fn7">7</xref> to explore knowledge graphs and assist users without technical knowledge of the underlying data models or query languages in disambiguating questions targeting the data stored in the knowledge graphs.</p></list-item></list><fig id="Fig4"><label>Fig. 4</label><caption><p>Bio-SODA System Architecture</p></caption><graphic xlink:href="10619_2022_7414_Fig4_HTML" id="MO4"/></fig></p>
    <p id="Par49">We will now discuss these phases in more detail.</p>
    <sec id="Sec6">
      <title>Preprocessing phase</title>
      <p id="Par50">The core component of this phase is the <italic>Indexing Module</italic>, which extracts the <italic>Inverted Index</italic> as well as the <italic>Schema Graph</italic> of the RDF data sources:<list list-type="bullet"><list-item><p id="Par51"><italic>Inverted Index</italic>: This index stores the vocabulary of the system. More precisely, all the properties that should be searchable from the RDF data store are indexed, according to a configuration file that specifies the list of properties of interest (by default, all string literals will be indexed). A further configuration option is whether URI fragments should also be parsed and indexed. In this case, these fragments are split by a predefined punctuation list, and through a camel case regex (<italic>e.g.</italic>, âpossibleDiseaseTargetâ will be indexed as the corresponding keywords âpossible disease targetâ).</p><p id="Par52">The inverted index is stored in a relational database for fast searches and it is used to match tokens (sequences of keywords in a user query) against the RDF data. More precisely, the index stores: keywords (N-grams of literals indexed), the indexed instance URI, the class of this instance, the property from which the keywords were indexed (<italic>e.g.</italic> label), as well as the PageRank score of the instance (see Table <xref rid="Tab1" ref-type="table">1</xref>). PageRank scores are computed using the approach presented in [<xref ref-type="bibr" rid="CR32">32</xref>].</p><p id="Par53">We note that the size of the inverted index depends on a few characteristics of the knowledge graph, including the verbosity of literals (<italic>i.e.</italic>, attributes that are strings), as well as the total number of attributes that should be indexed by Bio-SODA (an explicit list of these attributes can be provided in the system). For example, very verbose fields should not be indexed in their entirety, but perhaps in a summary form. This variability reflects also in the size of the inverted index compared to the size of the original dataset. Table <xref rid="Tab2" ref-type="table">2</xref> provides an overview across the 3 datasets considered in this study. For example, the QALD4 and Bioinformatics datasets also contain numerical data, which is not indexed, leading to a smaller index size than in the case of CORDIS. Generally, in terms of time, building the inverted index can take a few hours for large datasets, but this highly depends on the performance and availability of the SPARQL endpoint through which the dataset is accessible, given that the inverted index is built by querying this endpoint. We note that we have not focused on optimizing the inverted index construction and instead leave this as future work.</p></list-item><list-item><p id="Par54"><italic>Schema Graph Extractor</italic>: This module is used in order to enrich the (incomplete) schema of the knowledge graph(s) using instance-level data from the RDF store. The Schema Graph is essentially the <italic>accurate schema of the integrated RDF data</italic> which Bio-SODA automatically extracts from data instances<xref ref-type="fn" rid="Fn8">8</xref>. Moreover, the Schema Graph serves as the basis for constructing candidate query graphs from selected entry points (<italic>i.e.</italic>, matches for tokens in a user question).</p><p id="Par56">Computing a Schema Graph allows the system to compensate for incomplete schema information, for example, in cases where domains and ranges for properties are either missing or ill-defined. A second benefit of the Schema Graph is that it enables integrating multiple data models from different knowledge graphs. More precisely, since the search algorithm works at the level of the Schema Graph, it is agnostic to the actual physical representation of the data, meaning it can be easily extensible to support the case of multiple, complementary, knowledge graphs in the future. The minimal requirement for achieving this is that these KGs overlap, <italic>i.e.</italic>, they have classes in common, such that they can be joined in an integrated Schema Graph.</p><p id="Par57">Extracting the schema graph is achieved via SPARQL queries that compute, for example, domains and ranges of all properties, based on the classes of the instances which they connect. As a simplified example, a triple asserting âMigraine <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M2"><mml:mo stretchy="false">â</mml:mo></mml:math><inline-graphic xlink:href="10619_2022_7414_Article_IEq1.gif"/></alternatives></inline-formula> possibleDrug <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M4"><mml:mo stretchy="false">â</mml:mo></mml:math><inline-graphic xlink:href="10619_2022_7414_Article_IEq2.gif"/></alternatives></inline-formula> Ibuprofenâ will result in <italic>Disease</italic>
<inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M6"><mml:mo stretchy="false">â</mml:mo></mml:math><inline-graphic xlink:href="10619_2022_7414_Article_IEq3.gif"/></alternatives></inline-formula>
<italic>possibleDrug</italic>
<inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M8"><mml:mo stretchy="false">â</mml:mo></mml:math><inline-graphic xlink:href="10619_2022_7414_Article_IEq4.gif"/></alternatives></inline-formula>
<italic>Drug</italic> being added to the Schema Graph.</p><p id="Par58">Currently, as a minimum requirement we assume that each instance in the RDF data has a well-defined class, <italic>i.e.</italic> an explicit <italic>rdf:type</italic>. If this is not the case, additional preprocessing with external tools (for example, using RDF schema discovery techniques [<xref ref-type="bibr" rid="CR36">36</xref>]), would be required in order to properly define types for all RDF instances.</p></list-item></list><table-wrap id="Tab1"><label>Table 1</label><caption><p>Inverted Index Sample</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Lookup Key</th><th align="left">URI</th><th align="left">Class</th><th align="left">Property</th><th align="left">PageRank</th></tr></thead><tbody><tr><td align="left">Stroke</td><td align="left">side_effects:C0038454</td><td align="left">sider:side_effects</td><td align="left">sider:side-EffectName</td><td align="left">0.34</td></tr><tr><td align="left">Drug</td><td align="left">drugbank:drugs</td><td align="left">owl:Class</td><td align="left">rdfs:label</td><td align="left">91</td></tr><tr><td align="left">Drug</td><td align="left">sider:drugs</td><td align="left">owl:Class</td><td align="left">rdfs:label</td><td align="left">2.3</td></tr><tr><td align="left">Possible disease target</td><td align="left">diseasome:possible-DiseaseTarget</td><td align="left">rdf:Property</td><td align="left">uri_match</td><td align="left">80</td></tr></tbody></table><table-wrap-foot><p>The lookup key is used for fast searches based on keywords from a user question. The remaining information is used in attaching candidate matches to the Schema Graph (see description in Sect. <xref rid="Sec5" ref-type="sec">5</xref>) in order to construct the corresponding query graphs. A lookup key can consist of multiple keywords. The same lookup key can appear multiple times</p></table-wrap-foot></table-wrap></p>
      <p id="Par59">
        <table-wrap id="Tab2">
          <label>Table 2</label>
          <caption>
            <p>Descriptions of the size of the 3 public datasets used in our evaluation and their corresponding inverted index</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th align="left">Dataset</th>
                <th align="left">Sources</th>
                <th align="left">Dataset Size</th>
                <th align="left">Index Size</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left">QALD4-biomedical</td>
                <td align="left">Drugbank, Diseasome, Sider</td>
                <td align="left">200 MB</td>
                <td align="left">150 MB</td>
              </tr>
              <tr>
                <td align="left">Bioinformatics</td>
                <td align="left">Bgee, OMA</td>
                <td align="left">30 GB</td>
                <td align="left">8.5 GB</td>
              </tr>
              <tr>
                <td align="left">CORDIS</td>
                <td align="left">EU projects</td>
                <td align="left">1 GB</td>
                <td align="left">1 GB</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </p>
      <p id="Par60">We note here that indexing is a preprocessing step that is only required once, when the system is initialized. Afterwards, updates to the RDF store can be incorporated periodically through incremental updates (appends) to the inverted index, while the Schema Graph only needs to be recomputed in case of schema changes.</p>
    </sec>
    <sec id="Sec7">
      <title>SPARQL query generation phase</title>
      <p id="Par61">Given a natural language question, the goal of the Bio-SODA system is to translate it into a set of ranked candidate SPARQL queries, such that the top ranked query is the closest to the userâs query intent. In the following, we detail the role of each component involved in this translation process, namely the <italic>Lookup Module</italic>, the <italic>Candidate Ranking Module</italic>, the <italic>Query Graph Construction Module</italic>, the <italic>Query Graph Ranking Module</italic> and the <italic>Query Executor Module</italic>.<list list-type="bullet"><list-item><p id="Par62"><italic>Lookup Module</italic>:</p><p id="Par63">The lookup module has the role of retrieving the best candidate matches for tokens identified in a user query. A token is defined by the longest sequence of keywords that matches an entry in the Inverted Index (implemented in a relational database for fast searches). For example, in the question â<italic>What are the possible disease targets of Ibuprofen?</italic>â the two tokens extracted will be â<italic>possible disease target</italic>â (corresponding to an RDF property name) and â<italic>Ibuprofen</italic>â (corresponding to one or more Drug instances).</p></list-item><list-item><p id="Par64"><italic>Candidate Ranking Module</italic>:</p><p id="Par65">The lookup module can return a large number of candidate matches per token. In order to find best candidate matches, the ranking module <italic>groups together equivalent matches and ranks them</italic> in order of relevance to the initial query. For example, instances of the class <italic>Drug</italic> with matching <italic>rdfs:label</italic> are grouped together. In our running example illustrated in Fig. <xref rid="Fig3" ref-type="fig">3</xref>, the genes <italic>BRCA1</italic> and <italic>BRCA2</italic> are a match for the keyword <italic>BRCA</italic>.</p><p id="Par66">Furthermore, both <italic>string similarity</italic> and <italic>node importance</italic> are taken into account when ranking. Including the PageRank score as a measure of importance in the knowledge graph reduces the influence of the quality of labels assigned (labels which can be imprecise, see discussion in Sect. <xref rid="Sec3" ref-type="sec">3</xref>).</p><p id="Par67">The intuition behind this is that domain knowledge graphs usually cluster around a few important concepts, which will be reflected in the PageRank scores of the corresponding nodes. For example, UniProt<xref ref-type="fn" rid="Fn9">9</xref> [<xref ref-type="bibr" rid="CR37">37</xref>], a protein knowledge base containing more than 60 billion triples, currently includes only 177 classes. Out of these, only few classes, such as <italic>Protein</italic> and <italic>Annotation</italic>, have a central role, and will usually be the target of domain expert questions.</p><p id="Par69">Likewise, in the case of the CORDIS EU projects dataset (see Sect. <xref rid="Sec14" ref-type="sec">7</xref> for details), two different classes of Projects are available, <italic>EC-Project</italic> and <italic>ERC-Project</italic>. However, there is significantly more information in the dataset for the first class. In the lack of query logs or handcrafted rules for mapping query tokens to the correct candidates, the PageRank score can serve as a good proxy for ranking candidates according to node centrality, similarly to the initial approach used by web search engines [<xref ref-type="bibr" rid="CR38">38</xref>].</p><p id="Par70">As an added benefit, scoring with PageRank also ensures that metadata matches are prioritized. For example, <italic>Drug</italic> as a class name will rank higher than an instance match.</p><p id="Par71">Finally, to ensure that candidate matches not only have good string similarity, but are also <italic>semantically similar</italic>, word embeddings are also used in the candidate ranking. The similarity comparison ensures that spurious matches, such as <italic>gene</italic> compared to <italic>oogenesis</italic>, are discarded based on a pre-defined similarity threshold in the system configuration.</p><p id="Par72">Any word embeddings can in principle be used with Bio-SODA. For the two main bioinformatics use cases considered in this paper, we use Word Vectors extracted from PubMed, as described in [<xref ref-type="bibr" rid="CR39">39</xref>]. The candidate ranking module presents to the user top N matches per query token, where N is configurable in the system. We note that it is important to limit the number of matches per token for performance reasons. This is because the total number of candidate queries generated for a question with T query tokens (<italic>i.e.</italic> concepts searched by the user) will be in the worst case <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N^{T}$$\end{document}</tex-math><mml:math id="M10"><mml:msup><mml:mi>N</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="10619_2022_7414_Article_IEq5.gif"/></alternatives></inline-formula> (there are up to N matches for each of the T tokens).</p></list-item><list-item><p id="Par73"><italic>Query Graph Construction Module</italic>:</p><p id="Par74">The goal of this module is to use the matches from the previous step to <italic>generate a list of candidate query graphs</italic>. We extend the approach presented in [<xref ref-type="bibr" rid="CR40">40</xref>] to translate matches to query graph patterns. More precisely, we apply the iterative algorithm shown in Algorithm 1: for each set of candidate matches (one match per query token), we augment the Schema Graph by attaching the candidate matches to their corresponding class. Next, we find the minimal subgraph that covers all matches. For this purpose, we solve the approximate Steiner tree problem by computing the minimal spanning tree that covers one match per token.</p><p id="Par75">Note that there might be multiple such subgraphs, given that two classes can be connected via multiple properties. However, unless the user can be involved in disambiguating, it is important to generate all the variants, given that two equal-length subgraphs might actually have opposite semantics. Recall the example shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, where the properties <italic>e.g</italic>, <italic>isAbsentIn</italic>
<italic>versus</italic>
<italic>isExpressedIn</italic> both connect the same two classes, but represent disjoint result sets.</p><p id="Par76">Finally, in some cases handcrafted rules for inferring new concepts or relationships are required, due to the complexity of the corresponding query graphs. In such cases translating user questions into SPARQL cannot be done via simple entity linking methods. Therefore, if needed, our approach also supports adding rules to derive implicit information from the original knowledge graph as part of the question answering pipeline. These rules are implemented as sub-queries similar to the SELECT SPARQL query form. In this case, the rule head is the SPARQL query projection, and the rule body is the WHERE clause content.</p></list-item><list-item><p id="Par77"><italic>Query Graph Ranking Module</italic>:</p><p id="Par78">The query graph ranking module plays an important role in presenting the user with a meaningful, ordered list of results. In contrast to existing work, we do not return the <italic>overall minimal subgraph</italic> as the top result, but rather the <italic>graph that maximizes the sum of the match scores</italic> of the candidates covered. To understand why this is the case, consider the following question: <italic>âWhat are the drugs for asthma?â</italic>. This question translates to a 2-hop query graph, joining <italic>Drug</italic> and <italic>Disease</italic> via the <italic>possibleDiseaseTarget</italic> path (see Fig. <xref rid="Fig2" ref-type="fig">2</xref>). However, one likely scenario is that the description of a <italic>Drug</italic> instance includes the keyword <italic>asthma</italic>. In this case, the minimal query graph would be 1-hop only, retrieving only <italic>Drug</italic> instances that explicitly contain the keyword in the description, probably a small subset of all instances which have the corresponding <italic>Disease</italic> as a possible target. In this case, the minimal result would have good precision, but very low recall.</p></list-item><list-item><p id="Par79"><italic>Query Executor Module</italic>:</p><p id="Par80">Finally, the query executor translates the ranked query graphs into SPARQL queries, assigning meaningful variable names, also adding human-readable fields to the result set wherever possible. Importantly, we do not only return the best result, but rather a ranked list of possible interpretations (top N, where N is configurable in the system). This gives the user the opportunity to inspect the results in order to choose only the interpretation (<italic>i.e.</italic> SPARQL query) that matches the question intent.</p></list-item></list></p>
      <graphic position="anchor" xlink:href="10619_2022_7414_Figa_HTML" id="MO5"/>
    </sec>
  </sec>
  <sec id="Sec8">
    <title>Bio-SODA UX: user dialog interface</title>
    <sec id="Sec9">
      <title>Design</title>
      <p id="Par81">Since natural language questions can be very ambiguous and two users might mean different things when asking the same question, it is important that the system helps the user in exploring the data and in finding the correct answer to a question. Hence, we have built Bio-SODA UX, an interface that guides the user in the data discovery process by disambiguating natural language expressions that have multiple meanings.</p>
      <p id="Par82">To help the user understand the meaning of different results, the concepts are visually represented in a graph of the data model, supported by a data-driven color scheme and additional detailed information.</p>
      <p id="Par83">Bio-SODA UX provides more information about the underlying data and the meaning of the results on hover/click. Once the user received the first answer, a data table is populated with a result set. The immediate feedback of the system helps the user to identify potential refinements of the question or opportunities for disambiguation (i.e. choosing the right concepts). After each interaction with the user, the data table is reloaded to match the current state. While the default view shows only a few key features of the resulting data (e.g. Genes), the system allows for adding other potentially relevant information as âalternative columnsâ (e.g. gene descriptions), which the user can individually keep or discard. After the user has successfully disambiguated, the full set of results can be downloaded as a comma-separated values (CSV) file for further downstream processing.<fig id="Fig5"><label>Fig. 5</label><caption><p>Bio-SODA UX interface for knowledge graph exploration and query disambiguation. The three main components of the interface are: 1) an input field which also provides drop-downs with example candidate matches for each searched concept; 2) the fraction of the data model relevant to the question, shown in graph form; clicking on any node will display additional information in the âDetailsâ box on the right; 3) the results table with options to extend with more attributes related to the concepts in the question</p></caption><graphic xlink:href="10619_2022_7414_Fig5_HTML" id="MO6"/></fig><fig id="Fig6"><label>Fig. 6</label><caption><p>Bio-SODA UX example use case for the question âdrosophila anatomic entities at the embryo developmental stageâ</p></caption><graphic xlink:href="10619_2022_7414_Fig6_HTML" id="MO7"/></fig></p>
    </sec>
    <sec id="Sec10">
      <title>Implementation</title>
      <p id="Par84">The implementation of Bio-SODA UX consists of two main parts - the user web interface and the API. The web interface acts as a mediator between user and API. The API connects to the underlying Bio-SODA question answering system, which enables the natural language translation to SPARQL and query execution. The code for both of these parts is publicly available.</p>
      <p id="Par85">The user interface was developed using the jQuery library<xref ref-type="fn" rid="Fn10">10</xref>, the bootstrap framework<xref ref-type="fn" rid="Fn11">11</xref> for graphical representations, and D3.js<xref ref-type="fn" rid="Fn12">12</xref> for graph and data table visualization. Bio-SODA UX consists of three main parts, shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>: (1) the query input field for entering the usersâ natural language question; this part is also used to present the intermediate results to the user and to allow the user to disambiguate detected concepts via drop-down lists; (2) the node graph which displays the data model to the user, including where the candidate matches attach to the schema graph, providing some intuition of the data; (3) the data table on the bottom of the page, displaying the results corresponding to the current user selection.</p>
      <p id="Par89">The communication between the user interface and the Bio-SODA engine in the back-end is implemented via a series of API calls. We summarise here the main steps involved: <list list-type="order"><list-item><p id="Par90">The front-end invokes the Bio-SODA algorithm when the user first inputs a natural language question. The back-end algorithm will respond to this initial call with the following information:</p></list-item></list><list list-type="bullet"><list-item><p id="Par91">ranked candidate matches for each concept in the original question. For each candidate match, the back-end will also indicate what is the class of the match (e.g. a <italic>Gene</italic>, <italic>Protein</italic> or <italic>Anatomical Entity</italic>) - which helps attach the candidate match to the graph displaying all candidates. The user will then have the option to select the best candidate match from the corresponding drop-down list, while also inspecting the data model graph and seeing how each candidate connects. Furthermore, the response from the back-end includes additional information for each candidate, such as a label or a description of the RDF entry it matched. This information can be seen by clicking on the corresponding node displayed in the graph. Since for any given term, there can be many possible entries, in order to group results together, only one example candidate is provided for each class.</p></list-item><list-item><p id="Par92">a SPARQL query is returned and executed, corresponding to the best answer according Bio-SODA ranking algorithm. The results are displayed in the table on the bottom of the page and the SPARQL query itself can be inspected by clicking âShow SPARQL queryâ.</p></list-item></list><list list-type="simple"><list-item><label>2.</label><p id="Par93">When disambiguating, the user can select a different entry from the drop-down list of a given concept. This will trigger a new API call to the back-end, including the original question and all the current selections for each matched concept, such that the lookup and candidate ranking phases (see Sect. <xref rid="Sec5" ref-type="sec">5</xref>) in the Question Answering Pipeline can this time be skipped. The back-end will return a new SPARQL query and the corresponding results, according to the updated candidates selected by the user.</p></list-item></list></p>
    </sec>
    <sec id="Sec11">
      <title>Use case</title>
      <p id="Par94">Here, we will focus on selected example use cases considering Bio-SODA UX applied over the bioinformatics database of gene expression Bgee.</p>
      <sec id="Sec12">
        <title>Example 1</title>
        <p id="Par95">The user can first provide a natural language question, such as <italic>âWhat are the drosophila anatomic entities at the embryo developmental stage?â</italic>. In response, Bio-SODA UX returns the best-ranked answer in tabular form (see Fig. <xref rid="Fig6" ref-type="fig">6</xref>), while showing examples of candidate matches for each concept in graph form. The graph shows the connections between the chosen concepts and thus supports the user in understanding the underlying data model. For example, it can show how âanatomical entitiesâ and âdevelopmental stagesâ are linked through âgene expression conditionsâ. The data table presents results matching the selected concepts and filters.</p>
        <p id="Par96">The user can analyze every matched concept, such as âdrosophilaâ or âembryoâ, via the displayed graph. Furthermore, additional information, such as the URI of the matched node, its class, the property that matched and the Page Rank score of the node, can be displayed in the âDetailsâ panel on the right side of the page, by clicking on the matched node in the graph. By default, the system chooses the best candidate match for every concept according to the Bio-SODA ranking algorithm (see Sect. <xref rid="Sec5" ref-type="sec">5</xref>). The chosen candidates are also emphasized in the graph in the form of larger nodes. However, alternatives are presented for every concept in the form of drop-down menus. For example, an <italic>embryo</italic> can refer to either an organ (anatomical entity) or a developmental stage. The user can disambiguate the intended meaning by inspecting the available options and choosing the correct candidate match from the drop-down menu. The Bio-SODA UX supports the user by also assigning every concept a distinct color, which is then used throughout the disambiguation process.</p>
        <p id="Par97">In our example, the term âdrosophilaâ can be matched, among other meanings, to an anatomical entity (i.e. an organ). This is because anatomical entities are sometimes explicitly annotated with the species name, e.g. âcrop (Drosophila)â - an equivalent of the mammal stomach in the Drosophila (fly). However, the intended meaning of the concept in the original question is the name of a species (i.e., a <italic>Taxon</italic>), for example âDrosophila yakubaâ or âDrosophila melanogasterâ (the fruit fly).</p>
        <p id="Par98">By default, Bio-SODA ranks the crop higher, however, the ranking is not accurate since we want to receive a list of <italic>all</italic> anatomical entities from Drosophila flies, not only those explicitly <italic>annotated</italic> with the term âDrosophilaâ. The users can in this case clarify their intended meaning by choosing the <italic>Taxon</italic> entry corresponding to the âDrosophilaâ concept, from the drop-down list that is highlighted in purple in the query in Fig. <xref rid="Fig6" ref-type="fig">6</xref>. A further disambiguation required in this case is to then select the âembryoâ candidate match that corresponds to a <italic>developmental stage</italic> (i.e., âembryo stageâ, the second match in the drop-down shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>), as opposed to an anatomical entity (the top ranked candidate, chosen by default for this term). This can be selected from the drop-down menu of the term highlighted in red in the figure. The different candidates are also illustrated in the graph below (nodes colored in red), in order to facilitate understanding which match corresponds to the original intent of the query.<fig id="Fig7"><label>Fig. 7</label><caption><p>Bio-SODA UX example use case for the question âgenes with lung in the descriptionâ</p></caption><graphic xlink:href="10619_2022_7414_Fig7_HTML" id="MO8"/></fig></p>
      </sec>
      <sec id="Sec13">
        <title>Example 2</title>
        <p id="Par99">A second example, shown in Fig. <xref rid="Fig7" ref-type="fig">7</xref>, illustrates the response to the question <italic>âWhat are the genes with lung in the description?â</italic> This question reflects a user with more knowledge of the data model, since it requests information related to a property of Gene instances (i.e., those instances that explicitly contain the term <italic>lung</italic> in the gene description). By default, Bio-SODA ranks the candidate match <italic>lung</italic> corresponding to an anatomical entity (an organ) higher. This usage is much more common, which will reflect in the Page Rank score of the anatomical entity match <italic>Lung</italic>, therefore making it the top scored candidate for this term. However, this can lead to an incorrect or incomplete answer, as it would correspond to genes <italic>expressed</italic> in the lung organ, not those <italic>annotated</italic> with the term. Nevertheless, by inspecting the graph the user can easily clarify the intended meaning, by selecting the entry corresponding to an example of a gene description. This is the node attached to the <italic>Gene</italic> class, shown in the centre of Fig. <xref rid="Fig7" ref-type="fig">7</xref>.</p>
        <p id="Par100">In the drop-down list for the term <italic>lung</italic>, highlighted in purple in the original question, this is the third ranked candidate match. Hovering over the candidate matches will highlight the corresponding nodes in the graph with a red border, making it easier to understand where they attach in the original data model. Moreover, clicking on any of the nodes (e.g. the purple node attached to the Gene class, shown with a red border in Fig. <xref rid="Fig7" ref-type="fig">7</xref>) will also display more information about the candidate, including which class and property matched (i.e. the description, <ext-link ext-link-type="uri" xlink:href="http://purl.org/dc/terms/description">http://purl.org/dc/terms/description</ext-link>, of a Gene, <ext-link ext-link-type="uri" xlink:href="http://purl.org/net/orth#Gene">http://purl.org/net/orth#Gene</ext-link>).</p>
        <p id="Par101">The user can optionally inspect the generated SPARQL statement by clicking on <italic>Show SPARQL query</italic> on the corresponding button in the page, which will also confirm the correct answer for a more advanced user, while helping a novice user better understand how to obtain information for this question via SPARQL queries. Finally, the user can further find out more details about the retrieved genes by following the clickable links provided in the results table.</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec14">
    <title>Experiments</title>
    <p id="Par102">In this section we evaluate the F1-score performance of Bio-SODA for translating natural language questions to SPARQL and compare it against state-of-the-art systems for querying RDF-based knowledge graphs. Note that we focus on top-performing open-source systems that are publicly available for testing and do not require training data [<xref ref-type="bibr" rid="CR22">22</xref>].</p>
    <p id="Par103">In particular, we tested Sparklis [<xref ref-type="bibr" rid="CR33">33</xref>], a generic query builder system for knowledge graphs<xref ref-type="fn" rid="Fn13">13</xref>. Furthermore, we compared against GFMed [<xref ref-type="bibr" rid="CR24">24</xref>] which was top ranked in the QALD4 biomedical challenge and specifically designed for this dataset. Apart from this, we use GFMedâs publicly available grammar<xref ref-type="fn" rid="Fn14">14</xref> to evaluate how the system performs outside of the official QALD4 biomedical dataset. In addition, we compared our approach against SQG [<xref ref-type="bibr" rid="CR12">12</xref>], a system for query generation over knowledge graphs<xref ref-type="fn" rid="Fn15">15</xref>.</p>
    <sec id="Sec15">
      <title>Datasets</title>
      <p id="Par107">
        <table-wrap id="Tab3">
          <label>Table 3</label>
          <caption>
            <p>Descriptions of the 3 public datasets used in our evaluation</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th align="left">Dataset</th>
                <th align="left">Sources</th>
                <th align="left">#Classes</th>
                <th align="left">#Triples</th>
                <th align="left">Size on Disk</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left">QALD4-biomedical</td>
                <td align="left">Drugbank, Diseasome, Sider</td>
                <td align="left">12</td>
                <td align="left">0.69 M</td>
                <td align="left">200 MB</td>
              </tr>
              <tr>
                <td align="left">Bioinformatics</td>
                <td align="left">Bgee, OMA</td>
                <td align="left">37</td>
                <td align="left">430 M</td>
                <td align="left">30 GB</td>
              </tr>
              <tr>
                <td align="left">CORDIS</td>
                <td align="left">EU projects dataset</td>
                <td align="left">26</td>
                <td align="left">6.5 M</td>
                <td align="left">1 GB</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </p>
      <p id="Par108">Three datasets were considered for evaluating Bio-SODA, see Table <xref rid="Tab3" ref-type="table">3</xref>. Importantly, all three are real-world, in-use datasets. For each dataset, we briefly highlight the specific challenges that need to be tackled in the context of designing a generic question answering system: <list list-type="order"><list-item><p id="Par109">The <italic>QALD4 biomedical</italic> dataset is composed of Sider, DrugBank and Diseasome. This dataset includes several challenges such as multiple <italic>Drug</italic> classes and identical terms describing both <italic>Disease</italic> and <italic>Side Effects</italic> instances, which are connected via <italic>owl:sameAs</italic> properties.</p></list-item><list-item><p id="Par110">The <italic>bioinformatics</italic> dataset is composed of the Bgee (gene expression) [<xref ref-type="bibr" rid="CR41">41</xref>] and OMA (orthology) [<xref ref-type="bibr" rid="CR42">42</xref>] RDF stores. Given the highly specialized domain information contained in these sources, a particularity of this dataset is that questions can include complex concepts which translate to long SPARQL query graphs. An added challenge deriving from this is that the same concepts can be connected through multiple equal-length paths with semantically different or even opposite meanings.</p></list-item><list-item><p id="Par111">The <italic>CORDIS</italic> dataset of EU-funded projects. Although this dataset has a simpler schema, the challenge here is that questions can have a higher degree of ambiguity. In some cases, multiple interpretations are valid â for example, many terms are reused often and in a variety of contexts, such as â<italic>Big Data</italic>â. This can be either part of a project title, a topic or even an organization name. Therefore, identifying the query intent in some cases (<italic>e.g.</italic>
<italic>Show Big Data projects</italic>) cannot be done without user disambiguation.</p></list-item></list></p>
    </sec>
    <sec id="Sec16">
      <title>Queries</title>
      <p id="Par112">We have reused the official 50 queries of the QALD4 biomedical challenge<xref ref-type="fn" rid="Fn16">16</xref>. We do not distinguish between training and test queries. Indeed, we report performance metrics for all systems we tested across the entire set of 50 queries. Given that the test set was also available to participants in the official challenge, we believe this to be a fair evaluation. We do not change the questions in the official challenge, not even in cases where we could identify mistakes in the question. Furthermore, as opposed to previous work using this benchmark [<xref ref-type="bibr" rid="CR43">43</xref>], we do not materialize triples based on <italic>owl:sameAs</italic> statements and only use the exact dataset, as provided in the official benchmark.</p>
      <p id="Par114">For the bioinformatics dataset, in collaboration with domain experts, we created a benchmark of 30 queries, in increasing order of complexity, across two datasets, namely Bgee and OMA. The queries represent real information needs of domains experts within the field of gene expression and orthology, using the publicly available RDF data of Bgee<xref ref-type="fn" rid="Fn17">17</xref> and OMA<xref ref-type="fn" rid="Fn18">18</xref>. The average number of triple patterns per query here is 7 (not taking into account joint queries between the two sources, which have even higher complexity), with some questions jointly targeting 4 entities or more (<italic>Gene</italic>, <italic>Species</italic>, <italic>Anatomical Entity</italic>, <italic>Developmental Stage</italic>). In contrast, in existing benchmarks, such as LC-Quad [<xref ref-type="bibr" rid="CR14">14</xref>], queries with only 2 entities are already considered complex.</p>
      <p id="Par117">In order to test Bio-SODA using an entirely different domain, using the CORDIS dataset of EU funded projects, we created a test set of 30 queries in increasing order of complexity. Given the relatively simple structure of this data model, the average number of triple patterns per query is close to that of existing KGQA benchmarks [<xref ref-type="bibr" rid="CR14">14</xref>], with an average 2.3 triple patterns per query. However, the complexity stems from the usage of filters, literals in the query, as well as the higher degree of ambiguity.</p>
      <p id="Par118">Queries across the three datasets include aggregations, negations, and make extensive use of filters.</p>
      <p id="Par119">All questions, as well corresponding SPARQL queries, are available in the Evaluation folder of our GitHub repository<xref ref-type="fn" rid="Fn19">19</xref>.</p>
    </sec>
    <sec id="Sec17">
      <title>Results</title>
      <p id="Par121">We use the standard evaluation metrics of precision (P), recall (R) and F1-score, macro-averaged over all questions in the dataset. For Bio-SODA in particular, although the system generates a ranked list of possible interpretations, we report numbers based on the top answer only (Precision@1). More precisely, these results are evaluated based on the top answer provided by the default ranking of the Bio-SODA system, without any user disambiguation involved. In the Bio-SODA UX interface, this corresponds to the answer (and corresponding SPARQL query) shown to the user by default in response to a given question, without any manual intervention from the part of the user. The results are presented in Table <xref rid="Tab4" ref-type="table">4</xref> and discussed in the following section. For easy accessibility to the Bio-SODA system, as well as reproducibility of the results, we also provide a demo page for each of the three datasets, available online (see Sect. <xref rid="Sec1" ref-type="sec">1</xref>).<table-wrap id="Tab4"><label>Table 4</label><caption><p>Performance of translating natural language questions to SPARQL. By considering a perfect user of the Sparklis tool, the minimum number of manual steps for composing a query (averaged over all queries) is shown between parentheses</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Datasets and Systems</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">F1</th></tr></thead><tbody><tr><td align="left"><bold>Dataset 1: QALD4</bold></td><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">GFMed</td><td align="left">1</td><td align="left">0.99</td><td align="left">0.99</td></tr><tr><td align="left">SQG</td><td align="left">0.42</td><td align="left">0.42</td><td align="left">0.42</td></tr><tr><td align="left">Sparklis (5.5 steps/query)</td><td align="left">0.88</td><td align="left">0.88</td><td align="left">0.88</td></tr><tr><td align="left">Bio-SODA</td><td align="left">0.61</td><td align="left">0.60</td><td align="left">0.60</td></tr><tr><td align="left"><bold>Dataset 2: Bioinformatics</bold></td><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">GFMed</td><td align="left">0</td><td align="left">0</td><td align="left">0</td></tr><tr><td align="left">SQG</td><td align="left">0.16</td><td align="left">0.16</td><td align="left">0.16</td></tr><tr><td align="left">Sparklis</td><td align="left">-</td><td align="left">-</td><td align="left">-</td></tr><tr><td align="left">Bio-SODA</td><td align="left">0.6</td><td align="left">0.6</td><td align="left">0.6</td></tr><tr><td align="left"><bold>Dataset 3: CORDIS</bold></td><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">GFMed</td><td align="left">0</td><td align="left">0</td><td align="left">0</td></tr><tr><td align="left">SQG</td><td align="left">0.33</td><td align="left">0.33</td><td align="left">0.33</td></tr><tr><td align="left">Sparklis (6.2 steps/query)</td><td align="left">1</td><td align="left">1</td><td align="left">1</td></tr><tr><td align="left">Bio-SODA</td><td align="left">0.66</td><td align="left">0.66</td><td align="left">0.66</td></tr></tbody></table></table-wrap></p>
      <p id="Par122">We will now discuss the performance of each system in more detail.</p>
      <p id="Par123"><italic>GFMed</italic> shows the highest F1-score for the QALD4 dataset. However, it cannot (nor was it intended to) be used outside this dataset without rewriting the set of grammar rules that are strictly designed for question answering over specific releases of <italic>Diseasome</italic>, <italic>Drugbank</italic> and <italic>Sider</italic>. Hence, the F1-score for the bioinformatics dataset and the CORDIS datasets is 0.</p>
      <p id="Par124"><italic>SQG</italic> on the other hand, originally evaluated on the LC-Quad [<xref ref-type="bibr" rid="CR14">14</xref>] benchmark, does not support complex multi-hop questions, nor filters or queries involving literals. â<italic>Show me projects which started in</italic>
<bold>2020</bold>?â is an example of such a query, where <italic>2020</italic> is a numerical literal, as opposed to a linkable entity. While in the case of LC-Quad these limitations do not impact performance, all three datasets considered in our evaluation include such features, which explains the poorer performance of SQG: an F1-score of 0.42 in the case of QALD4, only 0.33 in the CORDIS dataset, and finally 0.16 in the case of the bioinformatics dataset. We note that these results are a theoretical best, since for SQG we assume perfect entity and property linking, leading to the highest performance it can achieve.</p>
      <p id="Par125">Finally, <italic>Sparklis</italic> is not a question answering system per-se, but rather a query builder, which helps users form the correct question by composing building blocks starting from examples of class names, properties, values etc. Therefore, in order to answer questions, we needed to rephrase them from the available building blocks <italic>manually</italic>. On the positive side, we found Sparklis to be a powerful system, because it enables building a rich variety of query types out-of-the-box. To achieve this, only the SPARQL endpoint URL of the target RDF data store is required.</p>
      <p id="Par126">Using the query building methodology of Sparklis, 44 out of 50 questions in the QALD4 biomedical benchmark can be answered. Furthermore, all questions in the CORDIS dataset can also be answered. Although this result might seem surprising, recall that the major challenge of this dataset is disambiguation. The manual query building process in Sparklis addresses exactly this problem, provided that the user knows very well how the data are structured and semantically represented. Therefore, on the negative side, we found that the query building methodology requires precise understanding of the data model, especially if multiple classes have the same label, as is the case in QALD4.</p>
      <p id="Par127">For example, answering the question <italic>âWhich drugs might lead to strokes?â</italic> requires knowing that the <italic>Drugs</italic> class to be used is the one in <italic>Sider</italic>, as opposed to the one in <italic>Diseasome</italic>. Furthermore, formulating questions in Sparklis is a manual and therefore time-consuming process. Even when making the strong assumption that the user has perfect knowledge of the data model, as well as of the features of Sparklis (for example, how to correctly formulate aggregations, which can be challenging), the minimal number of manual steps required to formulate questions is on average 5.5 interactions per question for QALD4 and 6.2 for CORDIS, with a maximum of 10 for the more complex questions. In most cases, the question resulting from composing the building blocks will be significantly different from a true natural language question.</p>
      <p id="Par128">We did not pursue this approach on the bioinformatics dataset, because complex concepts in this dataset (ortholog, paralog) cannot be expressed through the query building mechanism. More precisely, Sparklis does not support complex property paths.</p>
      <p id="Par129"><italic>Bio-SODA</italic> is a middle-ground between the generic, but manual approach of Sparklis, and the grammar-based approach of GFMed, which is not easily transferable to a new domain. More precisely, Bio-SODA achieves relatively good performance (around 0.6 F1-score) across the three datasets without requiring manual intervention. The only exception are two custom rules for the bioinformatics dataset, which help answer 4 out of 30 queries.</p>
      <p id="Par130">Although GFMed has the best results for QALD4, it cannot be used outside this dataset without a complete rewriting of the grammar rules. Sparklis also achieves better results on the two datasets tested, but with the big disadvantage that it is a manual approach, where the user must understand the data model in order to compose questions correctly. Our findings are further detailed in the Evaluation folder in our GitHub repository.</p>
    </sec>
    <sec id="Sec18">
      <title>Impact of ranking algorithm</title>
      <p id="Par131">In this section we study the impact for our ranking algorithm on the performance of Bio-SODA. In particular, we conducted an ablation study to quantify the importance of ranking by PageRank score of candidate matches. For this purpose, we disable our ranking algorithm and instead use a simple string similarity-based ranking algorithm for candidate matches, returning the <italic>overall</italic> minimal subgraph as the top answer.</p>
      <p id="Par132">The results, displayed in Table <xref rid="Tab5" ref-type="table">5</xref>, show that ranking makes a crucial difference, in particular for the CORDIS dataset. The reason for this is that for most of the keywords that describe metadata (such as class names, like <italic>Project Topic</italic> or <italic>Subject Area</italic>), there exists in the dataset a project whose acronym matches exactly. For example, there exist projects with acronyms such as <italic>Topic</italic>, <italic>Area</italic>, <italic>Host</italic>, <italic>Code</italic>, which are (according to string similarity only) classified as best matches for tokens in the original question. Constructing the <italic>overall</italic> minimal subgraph leads to wrong results in almost all cases, except for only 3 out of 30 questions, where there is no ambiguity. Note that adding <italic>no other change</italic> aside from considering PageRank scores in ranking enables answering 17 more queries out of 30 for this dataset.<table-wrap id="Tab5"><label>Table 5</label><caption><p>Ablation study on the Bio-SODA performance of translating natural language questions to SPARQL: (a) SPARQL candidate query ranking with node centrality measure versus (b) traditional ranking approach with string similarity and overall minimal subgraph as top result</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">(a) Correct with Bio-SODA Ranking</th><th align="left">(b) Correct with String Similarity Ranking</th></tr></thead><tbody><tr><td align="left">QALD4</td><td align="left">30/50</td><td align="left">23/50</td></tr><tr><td align="left">Bioinformatics</td><td align="left">18/30</td><td align="left">12/30</td></tr><tr><td align="left">CORDIS</td><td align="left">20/30</td><td align="left">3/30</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec19">
      <title>Error analysis and remaining problems</title>
      <p id="Par133">In the QALD4 biomedical benchmark, Bio-SODA correctly answered 30 out of 50 questions with an additional 2 partially correct. We note that 1 question in QALD4 cannot be answered by Sparklis nor Bio-SODA due to missing label information. More precisely, the instance <italic>http://www4.wiwiss.fu-berlin.de/diseasome/resource/genes/EDNRB</italic> is the target of the question âWhich genes are associated with Endothelin receptor type B?â. However, the label <italic>Endothelin receptor type B</italic> is not assigned in the official dataset of the benchmark, nor can it be derived from the URI fragment, for example. Upon closer inspection, it becomes clear that the question is ill-formulated. Since <italic>EDNRB</italic> itself is a gene, the correct question should be âWhich <italic>diseases</italic> are associated with EDNRB?â. In total, we have found at least 4 out of 50 entries in the dataset to contain errors, either in the question formulation, or in the ground truth answer. These have already been discussed in previous studies [<xref ref-type="bibr" rid="CR43">43</xref>].<fig id="Fig8"><label>Fig. 8</label><caption><p>Bio-SODA failure analysis. Out of the total 50 questions in the QALD4 biomedical benchmark, Bio-SODA cannot correctly answer 20. A further 12 out of 30 cannot be answered in the bioinformatics dataset, mainly due to query complexity (some queries having more than 10 triple patterns). Finally, on the CORDIS dataset 10 out of 30 queries cannot be answered, a large fraction of which include features currently unsupported in Bio-SODA: aggregations, comparatives, conjunctions etc</p></caption><graphic xlink:href="10619_2022_7414_Fig8_HTML" id="MO9"/></fig></p>
      <p id="Par134">An additional number of questions cannot be answered by Bio-SODA across the three datasets due to other reasons. We summarise them in Fig. <xref rid="Fig8" ref-type="fig">8</xref>, explained in the following:<list list-type="bullet"><list-item><p id="Par135"><italic>Aggregations</italic>. Our system currently does not support questions that require aggregations, such as <italic>Count</italic>, <italic>Sum</italic> etc. An example of such a question would be <italic>Count the projects in the life sciences domain</italic>. A possible solution to this would be to include pre-defined patterns or training a question classifier for this purpose.</p></list-item><list-item><p id="Par136"><italic>Superlatives/Comparatives</italic>. Another unsupported feature in the current prototype is the use of quantifiers (superlatives or comparatives). An example would be <italic>Which drug has the highest number of side-effects?</italic></p></list-item><list-item><p id="Par137"><italic>Conjunctions</italic>. Conjunctive questions which involve multiple instances of the same class are not supported in the current prototype. An example of such a case is <italic>List drugs that lead to strokes and arthrosis</italic>. This limitation derives from our methodology in computing the minimal subgraph covering candidate matches, which would require special handling for cases when multiple candidates of the same class are present in a question.</p></list-item><list-item><p id="Par138"><italic>Properties with same domain and range</italic>. Stemming from the same limitation mentioned above, these properties are currently not supported. In QALD4, the only instance of this is the <italic>diseaseSubtypeOf</italic> property, which has the <italic>Disease</italic> class as both domain and range. In the bioinformatics dataset we handle symmetric properties describing <italic>ortholog</italic> and <italic>paralog</italic> genes through custom rewrite rules.</p></list-item><list-item><p id="Par139"><italic>Ranking</italic>. One of the major sources of failure in our prototype remains ranking. In the QALD4 dataset, ranking problems affect 4 out of 50 queries. An example is: <italic>What are the diseases caused by Valdecoxib?</italic>. Here, the system cannot correctly choose <italic>Drug - sideEffect - Side_Effect</italic> over the alternative <italic>Disease - possibleDrug - Drug</italic>. The reason for this is that the <italic>Disease</italic> class matches exactly the term in the question, while the <italic>Drug</italic> class in <italic>Diseasome</italic> has a higher PageRank score than the one in <italic>Sider</italic>.</p><p id="Par140">A more complex corner-case is part of the bioinformatics dataset, <italic>What are the genes with lung in the description?</italic> The term <italic>lung</italic> is commonly used to refer to an <italic>Anatomical Entity</italic>. This is also reflected in the node importance of this match in the dataset. Therefore, the system cannot correctly determine that, in the context of this question, it should instead be considered part of the <italic>description</italic> property of a <italic>Gene</italic>. The correct candidate match scores very low, resulting in the correct answer also being ranked too low. However, through user disambiguation in the Bio-SODA UX, this question can also be correctly answered. The process is shown in Fig. <xref rid="Fig7" ref-type="fig">7</xref> and a discussion is included in Sect. <xref rid="Sec11" ref-type="sec">6.3</xref>. A similar example from QALD4 is <italic>Which drugs have bipolar disorder as indication?</italic>, where <italic>bipolar disorder</italic> is matched against a <italic>Disease</italic> instead of a drug indication. In these cases user disambiguation, at the level of candidate matches, is an important component in solving the problem.</p></list-item><list-item><p id="Par141"><italic>Incomplete information</italic>. This problem affects mainly the results in the QALD4 dataset, more precisely 4 out of 50 queries. We have already covered the example of the question targeting the <italic>EDNRB</italic> gene, which lacks the correct label in the official dataset. We currently do not enrich the inverted index with synonyms or external information, which means questions must be formulated in terms of the available vocabulary of the dataset. However, this limitation could be addressed by indexing synonyms from external data sources. Additional three questions cannot be answered because they refer to URIs that do not have any class defined in the data, therefore the system cannot attach the candidate matches anywhere in the Schema Graph.</p><p id="Par142">An example is the <italic>drugType</italic> property, which can take two values, either <ext-link ext-link-type="uri" xlink:href="http://www4.wiwiss.fu-berlin.de/drugbank/resource/drugtype/experimental">http://www4.wiwiss.fu-berlin.de/drugbank/resource/drugtype/experimental</ext-link> or <ext-link ext-link-type="uri" xlink:href="http://www4.wiwiss.fu-berlin.de/drugbank/resource/drugtype/approved">http://www4.wiwiss.fu-berlin.de/drugbank/resource/drugtype/approved</ext-link>. We believe a better modelling of the data should have provided, for example, either these as a <italic>xsd:anyURI</italic> datatype, given they are not used for any other purposes, or defined some class for both.</p></list-item><list-item><p id="Par143"><italic>Query complexity (difficult queries)</italic>. The bioinformatics dataset covers queries with high complexity, which are difficult to solve especially since they include symmetric properties, with multiple instances of the same class, each filtered according to different conditions.</p><p id="Par144">An example of such a question is: <italic>Retrieve Oryctolagus cuniculusâ proteins encoded by genes that are orthologous to Mus musculusâ HBB-Y gene</italic>. Here, the task is to retrieve <italic>Gene</italic> instances in a particular <italic>Taxon</italic> (species), namely the rabbit (<italic>Oryctolagus cuniculus</italic>), which are <italic>orthologs</italic> (symmetric property) of a second instance of <italic>Gene</italic>, labeled <italic>HBB-Y</italic>, in a different species, namely the mouse (<italic>Mus musculus</italic>). The resulting query has over 15 triple patterns, with 3 filters (the 2 species names plus the gene name).</p></list-item><list-item><p id="Par145"><italic>Others</italic>. Two questions in the QALD4 dataset have particular challenges, the first being a stemming error. In the question <italic>Give me drugs in the gaseous state</italic>, the term <italic>gaseous</italic> cannot be correctly stemmed to <italic>gas</italic>. The second type of error is due to unsupported <italic>ASK</italic> queries, <italic>e.g.</italic>
<italic>Are there drugs that target the Protein kinase C beta type?</italic>. Here, Bio-SODA retrieves examples of such drugs, instead of the boolean <italic>True</italic>. However, we do not consider this a fundamental limitation and a question type classifier could be added in future work.</p></list-item></list>We report a more detailed analysis of all systems considered in this paper in the <ext-link ext-link-type="uri" xlink:href="https://github.com/anazhaw/Bio-SODA/tree/master/Evaluation">https://github.com/anazhaw/Bio-SODA/tree/master/Evaluation</ext-link> in our GitHub repository.</p>
    </sec>
  </sec>
  <sec id="Sec20">
    <title>Lessons learned</title>
    <p id="Par146">Considering the challenges of question answering over knowledge graphs introduced in Sect. <xref rid="Sec3" ref-type="sec">3</xref>, we highlight the following design goals for natural language processing engines:<list list-type="bullet"><list-item><p id="Par147"><italic>Generality</italic>: The system should be easily adaptable to new datasets. In particular, the system should be able to answer questions in a new domain with minimal manual intervention and without relying on extensive training data, which is hard to obtain in many domains. Along this line, a desirable property is also the ability to cope with âreal-worldâ datasets, dealing with incompleteness in the data, for example in the form of:<list list-type="bullet"><list-item><p id="Par148">missing schema information (should be inferred from instance-level data);</p></list-item><list-item><p id="Par149">missing labels (should be incorporated from URIs whenever meaningful);</p></list-item></list></p></list-item><list-item><p id="Par150"><italic>Extensibility</italic>: The system should easily work with multiple datasets (provided they are already semantically alignedâ<italic>i.e.</italic>, data integration is a prior requirement). Many studies introduce possible approaches for data integration, including a recent approach for ontology-based data integration, covering one of the bioinformatics use cases presented in this paper [<xref ref-type="bibr" rid="CR44">44</xref>].</p></list-item><list-item><p id="Par151"><italic>Configurability</italic>: The database owner must be able to specify which properties (<italic>e.g.</italic> labels, descriptions) should be searchable using the system. Our experience with real-world datasets showed that in general it is not desirable for <italic>all</italic> properties to be indexed and thus be searchable. As an example, in many cases, fields in the queried data sources can be either redundant or too verbose. In bioinformatics, these are abstracts of papers that are assigned as values to an RDF property, whose length can therefore be up to 300 words. Similarly, in the CORDIS dataset, these are the abstracts of the EU projects. These cases should be handled through a dedicated approach, for example, based on classical information retrieval methods as discussed in [<xref ref-type="bibr" rid="CR45">45</xref>].</p></list-item><list-item><p id="Par152"><italic>Explainability</italic>: The system should clearly guide the user through how a question was processed and interpreted. This starts from explaining which concepts were matched in relation to the original question, continuing with how these candidate matches are composed together in a query graph in order to provide the final SPARQL query. Finally, the query results should be understandable as well. Therefore, the projected variable names should also be meaningful.</p></list-item></list></p>
  </sec>
  <sec id="Sec21">
    <title>Conclusions and outlook</title>
    <p id="Par153">In this paper we have introduced Bio-SODA, a question answering system for domain knowledge graphs, which we evaluated across three real-world datasets pertaining to different domains: biomedical, gene orthology and gene expression, and finally EU-funded projects. Our results have shown that Bio-SODA outperforms state-of-the-art systems that are publicly available for testing by a 20% F1-score improvement and more. The main advantage of Bio-SODA over existing open-source systems is that it can handle <italic>complex, multi-triple pattern queries</italic> without requiring user guidance and training data. Bio-SODA uses a novel ranking approach that takes into account both string and semantic similarity, as well as node centrality of candidate matches. Our experiments demonstrate that our ranking approach improves the quality of results, particularly in the context of datasets which can suffer from redundancy and imprecise labels.</p>
    <p id="Par154">We have also introduced Bio-SODA UX, a graphical interface allowing users to explore the underlying data models and disambiguate their questions dynamically. As future work, we plan to consider the usersâ feedback for learning to rank the best answer among resulting candidate queries. We also plan to evaluate the average number of disambiguation steps required for clarifying the semantics of user questions. As a long term direction for future research, we envision compiling a benchmark of cross-domain question-answer pairs, similarly to the Spider benchmark in the relational database world [<xref ref-type="bibr" rid="CR46">46</xref>], which would enable research into refining pre-trained KGQA models for new domains.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn id="Fn1">
      <label>1</label>
      <p id="Par8">Code at <ext-link ext-link-type="uri" xlink:href="https://github.com/anazhaw/Bio-SODA">https://github.com/anazhaw/Bio-SODA</ext-link></p>
    </fn>
    <fn id="Fn2">
      <label>2</label>
      <p id="Par9">See demo at <ext-link ext-link-type="uri" xlink:href="http://biosoda.expasy.org/">http://biosoda.expasy.org/</ext-link></p>
    </fn>
    <fn id="Fn3">
      <label>3</label>
      <p id="Par10">
        <ext-link ext-link-type="uri" xlink:href="https://cordis.europa.eu/projects">https://cordis.europa.eu/projects</ext-link>
      </p>
    </fn>
    <fn id="Fn4">
      <label>4</label>
      <p id="Par36">Note that, based on the biomedical literature, mutations in the two <italic>BRCA</italic> genes, BRCA1 and BRCA2 (stemming from <italic>BReast CAncer</italic>) are known to be associated with multiple types of cancer.</p>
    </fn>
    <fn id="Fn5">
      <label>5</label>
      <p id="Par39">a FILTER for the token <italic>BRCA</italic> is created on the <italic>Diseasome:Genes</italic> class</p>
    </fn>
    <fn id="Fn6">
      <label>6</label>
      <p id="Par45">Code also at <ext-link ext-link-type="uri" xlink:href="https://github.com/anazhaw/Bio-SODA">https://github.com/anazhaw/Bio-SODA</ext-link>, see <italic>biosodaUX</italic> folder</p>
    </fn>
    <fn id="Fn7">
      <label>7</label>
      <p id="Par48">Demo available at <ext-link ext-link-type="uri" xlink:href="https://biosoda.expasy.org/biosodaUX/">https://biosoda.expasy.org/biosodaUX/</ext-link></p>
    </fn>
    <fn id="Fn8">
      <label>8</label>
      <p id="Par55">Note that multiple RDF sources can be combined, as long as they are semantically aligned - <italic>i.e.</italic> they have at least one common concept, such as <italic>Gene</italic>.</p>
    </fn>
    <fn id="Fn9">
      <label>9</label>
      <p id="Par68">
        <ext-link ext-link-type="uri" xlink:href="https://sparql.uniprot.org/">https://sparql.uniprot.org/</ext-link>
      </p>
    </fn>
    <fn id="Fn10">
      <label>10</label>
      <p id="Par86">
        <ext-link ext-link-type="uri" xlink:href="https://jquery.com">https://jquery.com</ext-link>
      </p>
    </fn>
    <fn id="Fn11">
      <label>11</label>
      <p id="Par87">
        <ext-link ext-link-type="uri" xlink:href="https://getbootstrap.com">https://getbootstrap.com</ext-link>
      </p>
    </fn>
    <fn id="Fn12">
      <label>12</label>
      <p id="Par88">
        <ext-link ext-link-type="uri" xlink:href="https://d3js.org">https://d3js.org</ext-link>
      </p>
    </fn>
    <fn id="Fn13">
      <label>13</label>
      <p id="Par104">A live demo can be tested with any SPARQL endpoint at <ext-link ext-link-type="uri" xlink:href="http://www.irisa.fr/LIS/ferre/sparklis/">http://www.irisa.fr/LIS/ferre/sparklis/</ext-link></p>
    </fn>
    <fn id="Fn14">
      <label>14</label>
      <p id="Par105">See <ext-link ext-link-type="uri" xlink:href="http://cs-gw.utcluj.ro/%7eanca/GFMed/index.html">http://cs-gw.utcluj.ro/~anca/GFMed/index.html</ext-link></p>
    </fn>
    <fn id="Fn15">
      <label>15</label>
      <p id="Par106">Available at <ext-link ext-link-type="uri" xlink:href="https://github.com/AskNowQA/SQG/">https://github.com/AskNowQA/SQG/</ext-link></p>
    </fn>
    <fn id="Fn16">
      <label>16</label>
      <p id="Par113">
        <ext-link ext-link-type="uri" xlink:href="https://github.com/ag-sc/QALD/blob/master/4/data">https://github.com/ag-sc/QALD/blob/master/4/data</ext-link>
      </p>
    </fn>
    <fn id="Fn17">
      <label>17</label>
      <p id="Par115">
        <ext-link ext-link-type="uri" xlink:href="https://bgee.org/sparql">https://bgee.org/sparql</ext-link>
      </p>
    </fn>
    <fn id="Fn18">
      <label>18</label>
      <p id="Par116">
        <ext-link ext-link-type="uri" xlink:href="https://sparql.omabrowser.org/sparql">https://sparql.omabrowser.org/sparql</ext-link>
      </p>
    </fn>
    <fn id="Fn19">
      <label>19</label>
      <p id="Par120">Evaluation in <ext-link ext-link-type="uri" xlink:href="https://github.com/anazhaw/Bio-SODA/">https://github.com/anazhaw/Bio-SODA/</ext-link></p>
    </fn>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>We thank the Swiss National Science Foundation for funding (NRP 75, grant 407540_167149), Lukas Blunschi for the implementation of the SODA system for keyword search system over relational databases [<xref ref-type="bibr" rid="CR47">47</xref>], on which our prototype is based, and Katrin Affolter for important contributions to the natural language processing pipeline in Bio-SODA. We thank Chiara Gabella and SÃ©verine Duvaud from the SIB User Experience Team for their advice on the design of Bio-SODA UX.</p>
  </ack>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>Open access funding provided by University of Lausanne.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Diefenbach</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Both</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Singh</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Maret</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Towards a question answering system over the semantic web</article-title>
        <source>Semantic Web Preprint</source>
        <year>2018</year>
        <volume>2018</volume>
        <fpage>1</fpage>
        <lpage>19</lpage>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <mixed-citation publication-type="other">Zheng, W., Yu, J.X., Zou, L., Cheng, H.: Question answering over knowledge graphs: question understanding via template decomposition. In: Proceedings of the VLDB Endowment 11, pp. 1373â1386 (2018)</mixed-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <mixed-citation publication-type="other">Vakulenko, S., Garcia, J.D.F., Polleres, A., de Rijke, M., Cochez, M.: Message Passing for Complex Question Answering over Knowledge Graphs. In: Proceedings of the 28th ACM International Conference on Information and Knowledge Management, pp. 1431â1440 (2019)</mixed-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Jagadish</surname>
            <given-names>HV</given-names>
          </name>
        </person-group>
        <article-title>Constructing an interactive natural language interface for relational databases</article-title>
        <source>Proc. VLDB Endowm.</source>
        <year>2014</year>
        <volume>8</volume>
        <fpage>73</fpage>
        <lpage>84</lpage>
        <pub-id pub-id-type="doi">10.14778/2735461.2735468</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Jagadish</surname>
            <given-names>HV</given-names>
          </name>
        </person-group>
        <article-title>Understanding natural language queries over relational databases</article-title>
        <source>ACM SIGMOD Rec.</source>
        <year>2016</year>
        <volume>45</volume>
        <fpage>6</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="doi">10.1145/2949741.2949744</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Saha</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Floratou</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sankaranarayanan</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Minhas</surname>
            <given-names>UF</given-names>
          </name>
          <name>
            <surname>Mittal</surname>
            <given-names>AR</given-names>
          </name>
          <name>
            <surname>Ãzcan</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>ATHENA: an ontology-driven system for natural language querying over relational data stores</article-title>
        <source>Proc. VLDB Endowm.</source>
        <year>2016</year>
        <volume>9</volume>
        <fpage>1209</fpage>
        <lpage>1220</lpage>
        <pub-id pub-id-type="doi">10.14778/2994509.2994536</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <mixed-citation publication-type="other">Brunner, U., Stockinger, K.: ValueNet: a natural language-to-SQL system that Learns from Database Information. International Conference on Data Engineering (ICDE) (2021)</mixed-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">Sakor, A., Singh, K., Vidal, M.-E.: An Entity and Relation Linking Framework over DBpedia, FALCON (2019)</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <mixed-citation publication-type="other">Ferragina, P., Scaiella, U.: Tagme: on-the-fly annotation of short text fragments (by wikipedia entities). In: Proceedings of the 19th ACM international conference on Information and knowledge management, pp. 1625â1628 (2010)</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">Mendes, P.N., Jakob, M., GarcÃ­a-Silva, A., Bizer, C.: DBpedia spotlight: shedding light on the web of documents. In: Proceedings of the 7th International Conference on Semantic Systems, pp. 1â8 (2011)</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Olieman, A., Azarbonyad, H., Dehghani, M., Kamps, J., Marx, M.: Entity linking by focusing DBpedia candidate entities. In: Proceedings of the First International Workshop on Entity Recognition &amp; Disambiguation, pp. 13â24 (2014)</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Zafar, H., Napolitano, G., Lehmann, J.: Formal query generation for question answering over knowledge bases. In: European Semantic Web Conference. Springer, Berlin, pp. 714â728 (2018)</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Singh, K., Lytra, I., Radhakrishna, A.S., Shekarpour, S., Vidal, M.-E., Lehmann, J.: No one is perfect: analysing the performance of question answering components over the dbpedia knowledge graph. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1809.10044">arXiv:1809.10044</ext-link> (2018)</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Trivedi, P., Maheshwari, G., Dubey, M., Lehmann, J.: Lc-quad: a corpus for complex question answering over knowledge graphs. In: International Semantic Web Conference. Springer, Berlin, pp. 210â218 (2017)</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Dubey, M., Banerjee, D., Abdelkawi, A., Lehmann, J.: Lc-quad 2.0: a large dataset for complex question answering over wikidata and dbpedia. In: International Semantic Web Conference. Springer, Berlin. pp. 69â78 (2019)</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bonifati</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Martens</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Timm</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>An analytical study of large SPARQL query logs</article-title>
        <source>VLDB J.</source>
        <year>2019</year>
        <volume>2019</volume>
        <fpage>1</fpage>
        <lpage>25</lpage>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <mixed-citation publication-type="other">Unger, C., Forascu, C., Lopez, V., Ngomo, A.-C.N., Cabrio, E., Cimiano, P., Walter, S.: Question answering over linked data (QALD-4) (2014)</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hasnain</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Mehmood</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Zainab</surname>
            <given-names>SS</given-names>
          </name>
          <name>
            <surname>Saleem</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Warren</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Zehra</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Decker</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Rebholz-Schuhmann</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Biofed: federated query processing over life sciences linked open data</article-title>
        <source>J. Biomed. Semant.</source>
        <year>2017</year>
        <volume>8</volume>
        <fpage>13</fpage>
        <pub-id pub-id-type="doi">10.1186/s13326-017-0118-0</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Amer-Yahia</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Koutrika</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Braschler</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Calvanese</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Lanti</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>LÃ¼cke-Tieke</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Mosca</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>de Farias</surname>
            <given-names>TM</given-names>
          </name>
          <name>
            <surname>Papadopoulos</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Patil</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>INODE: building an end-to-end data exploration system in practice</article-title>
        <source>ACM SIGMOD Rec.</source>
        <year>2021</year>
        <volume>50</volume>
        <fpage>23</fpage>
        <lpage>29</lpage>
        <pub-id pub-id-type="doi">10.1145/3516431.3516436</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Sima, A.C., de Farias, T.M., Anisimova, M., Dessimoz, C., Robinson-Rechavi, M., Zbinden, E., Stockinger, K.: Bio-SODA: enabling natural language question answering over knowledge graphs without training data. In: 33rd International Conference on Scientific and Statistical Database Management, pp. 61â72 (2021)</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Maheshwari, G., Trivedi, P., Lukovnikov, D., Chakraborty, N., Fischer, A., Lehmann, J.: Learning to rank query graphs for complex question answering over knowledge graphs. In: International Semantic Web Conference. Springer, Berlin, pp. 487â504 (2019)</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Affolter</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Stockinger</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Bernstein</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>A comparative survey of recent natural language interfaces for databases</article-title>
        <source>VLDB J.</source>
        <year>2019</year>
        <volume>28</volume>
        <fpage>793</fpage>
        <lpage>819</lpage>
        <pub-id pub-id-type="doi">10.1007/s00778-019-00567-8</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Chakraborty, N., Lukovnikov, D., Maheshwari, G., Trivedi, P., Lehmann, J., Fischer, A.: Introduction to Neural Network based Approaches for Question Answering over Knowledge Graphs. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1907.09361">arXiv:1907.09361</ext-link> (2019)</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Marginean</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Question answering over biomedical linked data with grammatical framework</article-title>
        <source>Semantic Web</source>
        <year>2017</year>
        <volume>8</volume>
        <fpage>565</fpage>
        <lpage>580</lpage>
        <pub-id pub-id-type="doi">10.3233/SW-160223</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Hamon, T., Grabar, N., Mougin, F., Thiessard, F.: Description of the POMELO System for the Task 2 of QALD-2014. CLEF (Working Notes) <bold>1212</bold>, 28 (2014)</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hamon</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Grabar</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Mougin</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Querying biomedical linked data with natural language questions</article-title>
        <source>Semantic Web</source>
        <year>2017</year>
        <volume>8</volume>
        <fpage>581</fpage>
        <lpage>599</lpage>
        <pub-id pub-id-type="doi">10.3233/SW-160244</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Diefenbach, D., GimÃ©nez-GarcÄ±a, J., Both, A., Singh, K., Maret, P.: Designing a portable Question Answering System over RDF data, QAnswer KG (2020)</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Lukovnikov, D., Fischer, A., Lehmann, J., Auer, S.: Neural network-based question answering over knowledge graphs on word and character level. In: Proceedings of the 26th international conference on World Wide Web, pp. 1211â1220 (2017)</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Deutch</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Frost</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Gilad</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Explaining Natural Language query results</article-title>
        <source>VLDB J.</source>
        <year>2020</year>
        <volume>29</volume>
        <fpage>485</fpage>
        <lpage>508</lpage>
        <pub-id pub-id-type="doi">10.1007/s00778-019-00584-7</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">Ngomo, A.-C.N., BÃ¼hmann, L., Unger, C., Lehmann, J., Gerber, D.: Sorry, I donât speak SPARQL: translating SPARQL queries into natural language. In: Proceedings of the 22nd International Conference on World Wide Web, pp. 977â988 (2013)</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Kokkalis, A., Vagenas, P., Zervakis, A., Simitsis, A., Koutrika, G., Ioannidis, Y.: Logos: a system for translating queries into narratives. In: Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data, pp. 673â676 (2012)</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Diefenbach, D., Thalhammer, A.: Pagerank and generic entity summarization for rdf knowledge bases. In: European Semantic Web Conference. Springer, Berlin. pp. 145â160 (2018)</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>FerrÃ©</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Sparklis: an expressive query builder for SPARQL endpoints with guidance in natural language</article-title>
        <source>Semantic Web</source>
        <year>2017</year>
        <volume>8</volume>
        <issue>3</issue>
        <fpage>405</fpage>
        <lpage>418</lpage>
        <pub-id pub-id-type="doi">10.3233/SW-150208</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Diefenbach, D., Migliatti, P.H., Qawasmeh, O., Lully, V., Singh, K., Maret, P.: QAnswer: a Question Answering prototype bridging the gap between a considerable part of the LOD cloud and end-users. In: The World Wide Web Conference, pp. 3507â3510 (2019)</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Paulheim</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Bizer</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <source>Type Inference on Noisy rdf Data</source>
        <year>2013</year>
        <publisher-loc>Berlin</publisher-loc>
        <publisher-name>Springer</publisher-name>
        <fpage>510</fpage>
        <lpage>525</lpage>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Kellou-Menouer, K., Kedad, Z.: Schema discovery in RDF data sources. In: International Conference on Conceptual Modeling. Springer, Berlin. pp. 481â495 (2015)</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Redaschi, N., Consortium, U., etÂ al.: Uniprot in RDF: Tackling data integration and distributed annotation with the semantic web. Nat. Preced., pp. 1â1 (2009)</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Page, L., Brin, S., Motwani, R., Winograd, T.: The pagerank citation ranking: Bringing order to the web. Technical Report, Stanford InfoLab (1999)</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <collab>Distributional semantics resources for biomedical text processing</collab>
        </person-group>
        <article-title>Moen, S.P.F.G.H., Ananiadou, T.S.S</article-title>
        <source>Proc. LBM</source>
        <year>2013</year>
        <volume>2013</volume>
        <fpage>39</fpage>
        <lpage>44</lpage>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">Gkirtzou, K., Karozos, K., Vassalos, V., Dalamagas, T.: Keywords-to-sparql translation for rdf data search and exploration. In: International Conference on Theory and Practice of Digital Libraries. Springer, Berlin, pp. 111â123 (2015)</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bastian</surname>
            <given-names>FB</given-names>
          </name>
          <name>
            <surname>Roux</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Niknejad</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Comte</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Costa</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Fonseca</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>De Farias</surname>
            <given-names>TM</given-names>
          </name>
          <name>
            <surname>Moretti</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Parmentier</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>De Laval</surname>
            <given-names>VR</given-names>
          </name>
          <name>
            <surname>Rosikiewicz</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The Bgee suite: integrated curated expression atlas and comparative transcriptomics in animals</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2021</year>
        <volume>49</volume>
        <fpage>D831</fpage>
        <lpage>D847</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkaa793</pub-id>
        <pub-id pub-id-type="pmid">33037820</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Altenhoff</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Train</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Gilbert</surname>
            <given-names>KJ</given-names>
          </name>
          <name>
            <surname>Mediratta</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>de Farias</surname>
            <given-names>TM</given-names>
          </name>
          <name>
            <surname>Moi</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Nevers</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Radoykova</surname>
            <given-names>H-S</given-names>
          </name>
          <name>
            <surname>Rossier</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Vesztrocy</surname>
            <given-names>AW</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>OMA orthology in 2021: website overhaul, conserved isoforms, ancestral gene order and more</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2021</year>
        <volume>49</volume>
        <fpage>D373</fpage>
        <lpage>D379</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkaa1007</pub-id>
        <pub-id pub-id-type="pmid">33174605</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Song, D, Schilder, F., Smiley, C., Brew, C., Zielund, T., Bretz, H., Martin, R., Dale, C., Duprey, J., Miller, T., etÂ al.: TR discover: a natural language interface for querying and analyzing interlinked datasets. In: International Semantic Web Conference. Springer, Berlin, pp. 21â37 (2015)</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <mixed-citation publication-type="other">Sima, A.C., de Farias, T.M., Zbinden, E., Anisimova, M., Gil, M., Stockinger, H., Stockinger, K., Robinson-Rechavi, M., Dessimoz, C.: Enabling semantic queries across federated bioinformatics databases. Database 2019 (2019)</mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <mixed-citation publication-type="other">Nadig, S., Braschler, M., Stockinger, K.: Database Search vs. Information Retrieval: A Novel Method for Studying Natural Language Querying of Semi-Structured Data. In: International Conference on Language Resources and Evaluation (LREC) (2020)</mixed-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <mixed-citation publication-type="other">Yu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z., Ma, J., Li, I., Yao, Q., Roman, S., etÂ al.: Spider: a large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1809.08887">arXiv:1809.08887</ext-link> (2018)</mixed-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Blunschi</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Jossen</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Kossmann</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Mori</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Stockinger</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Soda: generating sql for business users</article-title>
        <source>Proc. VLDB Endowm.</source>
        <year>2012</year>
        <volume>5</volume>
        <fpage>932</fpage>
        <lpage>943</lpage>
        <pub-id pub-id-type="doi">10.14778/2336664.2336667</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
