<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-id journal-id-type="pmc">plosone</journal-id>
    <journal-title-group>
      <journal-title>PLoS ONE</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1932-6203</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6695225</article-id>
    <article-id pub-id-type="publisher-id">PONE-D-19-01588</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pone.0220182</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Computational Techniques</subject>
          <subj-group>
            <subject>Split-Decomposition Method</subject>
            <subj-group>
              <subject>Multiple Alignment Calculation</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Database and Informatics Methods</subject>
          <subj-group>
            <subject>Bioinformatics</subject>
            <subj-group>
              <subject>Sequence Analysis</subject>
              <subj-group>
                <subject>Sequence Alignment</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Database and Informatics Methods</subject>
          <subj-group>
            <subject>Biological Databases</subject>
            <subj-group>
              <subject>Sequence Databases</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Database and Informatics Methods</subject>
          <subj-group>
            <subject>Bioinformatics</subject>
            <subj-group>
              <subject>Sequence Analysis</subject>
              <subj-group>
                <subject>Sequence Databases</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Molecular Biology</subject>
          <subj-group>
            <subject>Macromolecular Structure Analysis</subject>
            <subj-group>
              <subject>Protein Structure</subject>
              <subj-group>
                <subject>Protein Structure Prediction</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Biochemistry</subject>
          <subj-group>
            <subject>Proteins</subject>
            <subj-group>
              <subject>Protein Structure</subject>
              <subj-group>
                <subject>Protein Structure Prediction</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Neural Networks</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Neural Networks</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Mathematical and Statistical Techniques</subject>
          <subj-group>
            <subject>Statistical Methods</subject>
            <subj-group>
              <subject>Forecasting</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Statistics</subject>
            <subj-group>
              <subject>Statistical Methods</subject>
              <subj-group>
                <subject>Forecasting</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Molecular Biology</subject>
          <subj-group>
            <subject>Macromolecular Structure Analysis</subject>
            <subj-group>
              <subject>Protein Structure</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Biochemistry</subject>
          <subj-group>
            <subject>Proteins</subject>
            <subj-group>
              <subject>Protein Structure</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>rawMSA: End-to-end Deep Learning using raw Multiple Sequence Alignments</article-title>
      <alt-title alt-title-type="running-head">rawMSA: End-to-end Deep Learning</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Mirabello</surname>
          <given-names>Claudio</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Conceptualization</role>
        <role content-type="http://credit.casrai.org/">Funding acquisition</role>
        <role content-type="http://credit.casrai.org/">Investigation</role>
        <role content-type="http://credit.casrai.org/">Methodology</role>
        <role content-type="http://credit.casrai.org/">Software</role>
        <role content-type="http://credit.casrai.org/">Visualization</role>
        <role content-type="http://credit.casrai.org/">Writing – original draft</role>
        <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff001"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-3772-8279</contrib-id>
        <name>
          <surname>Wallner</surname>
          <given-names>Björn</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Funding acquisition</role>
        <role content-type="http://credit.casrai.org/">Resources</role>
        <role content-type="http://credit.casrai.org/">Supervision</role>
        <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff001"/>
        <xref ref-type="corresp" rid="cor001">*</xref>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <addr-line>IFM Bioinformatics, Linköping University, Linköping, Sweden</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Zhang</surname>
          <given-names>Yang</given-names>
        </name>
        <role>Editor</role>
        <xref ref-type="aff" rid="edit1"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>University of Michigan, UNITED STATES</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p><bold>Competing Interests: </bold>The authors declare the following competing interest: The fact that we have received support from Nvidia Corporation does not alter our adherence to PLOS ONE policies on sharing data and materials.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>bjorn.wallner@liu.se</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>15</day>
      <month>8</month>
      <year>2019</year>
    </pub-date>
    <volume>14</volume>
    <issue>8</issue>
    <elocation-id>e0220182</elocation-id>
    <history>
      <date date-type="received">
        <day>17</day>
        <month>1</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>10</day>
        <month>7</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2019 Mirabello, Wallner</copyright-statement>
      <copyright-year>2019</copyright-year>
      <copyright-holder>Mirabello, Wallner</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pone.0220182.pdf"/>
    <abstract>
      <p>In the last decades, huge efforts have been made in the bioinformatics community to develop machine learning-based methods for the prediction of structural features of proteins in the hope of answering fundamental questions about the way proteins function and their involvement in several illnesses. The recent advent of Deep Learning has renewed the interest in neural networks, with dozens of methods being developed taking advantage of these new architectures. However, most methods are still heavily based pre-processing of the input data, as well as extraction and integration of multiple hand-picked, and manually designed features. Multiple Sequence Alignments (MSA) are the most common source of information in <italic>de novo</italic> prediction methods. Deep Networks that automatically refine the MSA and extract useful features from it would be immensely powerful. In this work, we propose a new paradigm for the prediction of protein structural features called rawMSA. The core idea behind rawMSA is borrowed from the field of natural language processing to map amino acid sequences into an adaptively learned continuous space. This allows the whole MSA to be input into a Deep Network, thus rendering pre-calculated features such as sequence profiles and other features calculated from MSA obsolete. We showcased the rawMSA methodology on three different prediction problems: secondary structure, relative solvent accessibility and inter-residue contact maps. We have rigorously trained and benchmarked rawMSA on a large set of proteins and have determined that it outperforms classical methods based on position-specific scoring matrices (PSSM) when predicting secondary structure and solvent accessibility, while performing on par with methods using more pre-calculated features in the inter-residue contact map prediction category in CASP12 and CASP13. Clearly demonstrating that rawMSA represents a promising development that can pave the way for improved methods using rawMSA instead of sequence profiles to represent evolutionary information in the coming years. <bold>Availability</bold>: datasets, dataset generation code, evaluation code and models are available at: <ext-link ext-link-type="uri" xlink:href="https://bitbucket.org/clami66/rawmsa">https://bitbucket.org/clami66/rawmsa</ext-link>.</p>
    </abstract>
    <funding-group>
      <award-group id="award001">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100004359</institution-id>
            <institution>Vetenskapsrådet</institution>
          </institution-wrap>
        </funding-source>
        <award-id>2016-05369</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-3772-8279</contrib-id>
          <name>
            <surname>Wallner</surname>
            <given-names>Björn</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award002">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100006358</institution-id>
            <institution>Stiftelsen Blanceflor Boncompagni Ludovisi, född Bildt</institution>
          </institution-wrap>
        </funding-source>
        <principal-award-recipient>
          <name>
            <surname>Mirabello</surname>
            <given-names>Claudio</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award003">
        <funding-source>
          <institution>Nvidia Corporation</institution>
        </funding-source>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-3772-8279</contrib-id>
          <name>
            <surname>Wallner</surname>
            <given-names>Björn</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award004">
        <funding-source>
          <institution>Nvidia Corporation</institution>
        </funding-source>
        <principal-award-recipient>
          <name>
            <surname>Mirabello</surname>
            <given-names>Claudio</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <funding-statement>BW was supported by Swedish Research Council grant, 2016-05369. CM was supported by The Foundation Blanceflor Boncompagni Ludovisi, née Bildt (no grant number). CM and BW were supported by Nvidia Corporation. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="4"/>
      <table-count count="2"/>
      <page-count count="15"/>
    </counts>
    <custom-meta-group>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>The data underlying the results presented in the study are available from <ext-link ext-link-type="uri" xlink:href="https://bitbucket.org/clami66/rawmsa">https://bitbucket.org/clami66/rawmsa</ext-link>.</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>The data underlying the results presented in the study are available from <ext-link ext-link-type="uri" xlink:href="https://bitbucket.org/clami66/rawmsa">https://bitbucket.org/clami66/rawmsa</ext-link>.</p>
  </notes>
</front>
<body>
  <sec sec-type="intro" id="sec001">
    <title>1 Introduction</title>
    <p>Predicting the 3D-structure of a protein from its amino acid sequence has been one of the main objectives of structural bioinformatics for decades now [<xref rid="pone.0220182.ref001" ref-type="bibr">1</xref>], yet a definite solution has not been found yet. The most reliable approaches currently involve <italic>homology modeling</italic>, which allows a known protein structure to be assigned an unknown protein provided that there is a detectable sequence similarity between the two. When homology modeling is not viable, <italic>de novo</italic> techniques are needed, either based on physical-based potentials [<xref rid="pone.0220182.ref002" ref-type="bibr">2</xref>] or knowledge-based potentials [<xref rid="pone.0220182.ref003" ref-type="bibr">3</xref>–<xref rid="pone.0220182.ref006" ref-type="bibr">6</xref>]. In the first case, an energy function is used to estimate the free energy of a given protein conformation along with a search function that tries different conformations in order to minimize the energy function [<xref rid="pone.0220182.ref007" ref-type="bibr">7</xref>]. Unfortunately, even small relatively small proteins have many degrees of freedom making it prohibitively expensive to fold them even on customized computer hardware [<xref rid="pone.0220182.ref008" ref-type="bibr">8</xref>]. Knowledge-based potentials, on the other hand, can be learned using statistics [<xref rid="pone.0220182.ref003" ref-type="bibr">3</xref>] or machine learning [<xref rid="pone.0220182.ref009" ref-type="bibr">9</xref>] to infer useful information from known examples of protein structures. This information can be used to constrain the problem, thus greatly reducing the conformational search space and enable prediction of larger proteins and complexes.</p>
    <p>In the last couple of decades, a variety of machine learning methods have been developed to predict a number of structural properties of proteins: secondary structure (SS) [<xref rid="pone.0220182.ref010" ref-type="bibr">10</xref>–<xref rid="pone.0220182.ref015" ref-type="bibr">15</xref>], relative solvent accessibility (RSA) [<xref rid="pone.0220182.ref015" ref-type="bibr">15</xref>–<xref rid="pone.0220182.ref018" ref-type="bibr">18</xref>], backbone dihedrals [<xref rid="pone.0220182.ref019" ref-type="bibr">19</xref>], disorder [<xref rid="pone.0220182.ref015" ref-type="bibr">15</xref>, <xref rid="pone.0220182.ref020" ref-type="bibr">20</xref>, <xref rid="pone.0220182.ref021" ref-type="bibr">21</xref>], disorder-to-order transition [<xref rid="pone.0220182.ref022" ref-type="bibr">22</xref>, <xref rid="pone.0220182.ref023" ref-type="bibr">23</xref>], contact maps [<xref rid="pone.0220182.ref024" ref-type="bibr">24</xref>–<xref rid="pone.0220182.ref027" ref-type="bibr">27</xref>], and model quality [<xref rid="pone.0220182.ref009" ref-type="bibr">9</xref>, <xref rid="pone.0220182.ref028" ref-type="bibr">28</xref>–<xref rid="pone.0220182.ref030" ref-type="bibr">30</xref>].</p>
    <p>The most important information used by most (if not all) methods above is a multiple sequence alignment (MSA) of sequences homologous to the target protein. The MSA consists of aligned sequences and to allow for comparisons and analysis of MSAs, they are often compressed into position-specific scoring matrices (PSSM), also called sequence profiles, using the fraction of occurrences of different amino acids in the alignment for each position in the sequence. The sequence profile describes the available evolutionary information of the target protein and is better than a single sequence representation, often providing a significant increase in prediction accuracy [<xref rid="pone.0220182.ref031" ref-type="bibr">31</xref>, <xref rid="pone.0220182.ref032" ref-type="bibr">32</xref>]. An obvious limitation of compressing an MSA into a PSSM is the loss of information that could be useful to obtain better predictions. Another potential issue is that whenever the MSA contains few sequences, the statistics encoded in the PSSM will not be as reliable and the prediction system may not be able to distinguish between a reliable and an unreliable PSSM.</p>
    <p>SS, RSA and similar structural properties are sometimes used as intermediate features to constrain and guide the prediction of more complex properties in a number of methods [<xref rid="pone.0220182.ref033" ref-type="bibr">33</xref>–<xref rid="pone.0220182.ref035" ref-type="bibr">35</xref>]. An example of this comes from the methods used for the prediction of inter-residue contact maps, where evolutionary profiles are integrated with predicted SS and RSA to improve performance [<xref rid="pone.0220182.ref036" ref-type="bibr">36</xref>–<xref rid="pone.0220182.ref038" ref-type="bibr">38</xref>].</p>
    <p>More recently, contact map prediction methods have been at the center of renewed interest after the development of a number of techniques to analyze MSAs in search of direct evolutionary couplings. These methods have led to a big leap in the state of the art [<xref rid="pone.0220182.ref039" ref-type="bibr">39</xref>–<xref rid="pone.0220182.ref041" ref-type="bibr">41</xref>]. However, their impressive performance is correlated with the number of sequences in the MSA, and is not as reliable when few sequences are related to the target. This means that evolutionary coupling methods have not completely replaced older machine learning-based systems, but have been integrated, usually in the form of extra inputs, along with the previously mentioned sequence profiles, SS and RSA, into even more complex machine learning systems. At the same time, the Deep Learning has proved to be a useful tool for better integrating the growing number and complexity of input features [<xref rid="pone.0220182.ref042" ref-type="bibr">42</xref>–<xref rid="pone.0220182.ref045" ref-type="bibr">45</xref>].</p>
    <p>However, one might argue that this kind of integrative approach, combining individually derived features, ignores a key aspect of deep learning, i.e. that features should be automatically extracted by the network rather than being provided to the network as inputs [<xref rid="pone.0220182.ref046" ref-type="bibr">46</xref>]. If we wanted to take full advantage of deep learning by using it in the same way it is employed for tasks such as image classification, one idea could be to provide a raw MSA input. Since the MSA is the most basic, lowest level input that methods use, it would make sense not to compress it into profiles, but instead let the deep network extract features as part of the training. However, a MSA is not an image or an audio track, and there is no native way of feeding such a large block of strings as input to a deep network.</p>
    <p>In this work we try to overcome this hurdle and introduce a new system for the <italic>de novo</italic> prediction of structural properties of proteins called rawMSA. The core idea behind rawMSA borrowed from the field of natural language processing a technique called <italic>embedding</italic> [<xref rid="pone.0220182.ref047" ref-type="bibr">47</xref>], which we use to convert each residue character in the MSA into a floating-point vector of variable size. This way of representing residues is adaptively learned by the network based on <italic>context</italic>, i.e. the structural property that we are trying to predict. To showcase the idea, we designed and tested several deep neural networks based on this concept to predict SS, RSA, and Residue-Residue Contact Maps (CMAP).</p>
  </sec>
  <sec sec-type="materials|methods" id="sec002">
    <title>2 Methods</title>
    <sec id="sec003">
      <title>2.1 Inputs</title>
      <p>Unlike the classical machine learning methods for the prediction of protein features, rawMSA does not compress the Multiple Sequence Alignment into a profile but, rather, uses the raw aligned sequences as input and devolves the task of extracting useful features to the deep network. The input to the deep network is a flat FASTA alignment. Before it is passed to the input layer of the neural network, each letter in the input is mapped to an integer ranging from 1 to 25 (20 standard residues plus the non-standard residues <italic>B</italic>, <italic>U</italic>, <italic>Z</italic>, <italic>X</italic> and − for gaps). If the alignment of a protein of length <italic>L</italic> contains <italic>N</italic> sequences, including the target, or “master” sequence, it is translates to an array of <italic>L</italic> × <italic>N</italic> integers. The master sequence occupies the first row of the array, while the following rows contain all the aligned sequences, in the order of output determined by the alignment software. Since MSAs for large protein families can contain up to tens of thousands of sequences, a threshold is set so that no more than <italic>Y</italic> sequences are used. For details on the alignment depth threshold, see the “Architecture” section.</p>
      <p>When training on or predicting SS or RSA, a sliding window of width 31 is applied to the MSA so that <italic>L</italic> separate windows of size 31 × <italic>Y</italic>, one for each residue in the master sequence, are passed to the network. The central column in the window is occupied by the residue in the master sequence for which a prediction is being made and the corresponding aligned residues from the other sequences. Zero-padding is applied at the N- and C- terminals of the master sequence or if the master sequence is shorter than the window size, and at the bottom if the number of aligned sequences is smaller than the maximum alignment depth <italic>Y</italic>. Note that residues are mapped to integers larger than zero and do not interfere with zero-padding.</p>
    </sec>
    <sec id="sec004">
      <title>2.2 Architecture</title>
      <p>We developed two different architectures for three different applications <italic>SS-RSA</italic> for the SS and RSA prediction and <italic>CMAP</italic> for the contact map prediction. In <xref ref-type="fig" rid="pone.0220182.g001">Fig 1</xref> we show an example of the network architecture. The networks trained in this work might use different numbers of convolutional, fully connected or BRNN layers, as well as slightly different parameters, but they all share this same basic overall structure.</p>
      <fig id="pone.0220182.g001" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0220182.g001</object-id>
        <label>Fig 1</label>
        <caption>
          <title>Network architecture for two rawMSA networks.</title>
          <p>On the left, the <italic>SS-RSA</italic> network predicts the secondary structure and relative solvent accessibility of each amino acid; on the right the <italic>CMAP</italic> network predicts the full contact map of the protein. The first layers are in common between the SS-RSA and CMAP architectures, although with slightly different settings, and provide the basis for the rawMSA approach.</p>
        </caption>
        <graphic xlink:href="pone.0220182.g001"/>
      </fig>
      <p>Since with rawMSA we abandoned the use of sequence profiles, which are also useful to represent the amino acid information in a computer-friendly format, i.e. a matrix of floating points, we needed to come up with a way of representing the input. In this case, where the inputs can be very large (up to hundreds of thousands of amino acids), categorical data cannot be translated with sparse, memory inefficient techniques such as <italic>one-hot encoding</italic>.</p>
      <p>To resolve this issue, the first layer of rawMSA is a trained, shallow two-layer neural network called an <italic>embedding layer</italic>. Embeddings are a compact way of representing a discrete set of inputs into a continuous space [<xref rid="pone.0220182.ref047" ref-type="bibr">47</xref>]. This technique is widely used in natural language processing where the inputs are made of a sequence of words taken from a dictionary and mapped to an n-dimensional space in a vector of floats of size <italic>n</italic>. When dealing with word embeddings in natural language, words that represent similar concepts, at least in a certain context, will be in close proximity in the output space. A similar idea might also be very useful when dealing with the discrete set of amino acids [<xref rid="pone.0220182.ref048" ref-type="bibr">48</xref>], since they also share context-dependent similarities. For example, when it comes to the context of secondary structure, if we look at the Chou-Fasman amino acid propensities table [<xref rid="pone.0220182.ref049" ref-type="bibr">49</xref>], glutamic acid and methionine are both strongly associated with alpha-helices, so it might be useful that such amino acids are represented by similar vectors when predicting secondary structure.</p>
      <p>The embedding layers in rawMSA output a vector of size <italic>E</italic> ranging from from 10 to 30, depending on the model, for each input residue in the alignment. In general, we have found that larger embedding vectors tend to give better results. The embedding layer is used both in the SS-RSA and the CMAP networks.</p>
      <sec id="sec005">
        <title>2.2.1 SS-RSA</title>
        <p>In the case of SS-RSA, a 2D convolutional layer is stacked on top of the embedding layer, followed by a max pooling layer. The convolutional layer has a number of filters equal to the dimensionality of the embedding space. The convolution filters have the shape of column vectors, rather than square matrices as is usually the case, thus the size of the convolution windows varies between 1 × 10 to 1 × 30 depending on the model. This means that convolution is performed along each column in the MSA and the information does not spread across columns (i.e. across adjacent residues in the input sequence). Pooling is performed selecting the maximum value in a window of the same size. In this way, if the dimension of the input is 31 residues by 500 alignments before embedding and 31 × 500 × 10 after embedding, this is reduced to a vector of size 31 × 50 × 10 following the convolution and pooling layers, if the convolution and pooling windows are of size 1 × 10. The convolutional and pooling layers are followed by a stack of two Long Short-Term Memory (LSTM) bidirectional recurrent layers, where each LSTM module contains 350 hidden units. The final three layers are fully connected, with softmax layers to output the classification prediction for SS (3 classes: Helix, Strand, Coil) or RSA (4 or 2 classes), depending on the model. Dropout is applied after each recurrent or dense layer to avoid overfitting, with variable fractions of neurons being dropped depending on the model (0.4 to 0.5). All the convolutional layers have ReLU activations and the outputs are zero-padded to match the two first dimensions of the inputs.</p>
      </sec>
      <sec id="sec006">
        <title>2.2.2 CMAP</title>
        <p>For CMAP, the network predicts a whole contact map of size <italic>L</italic> × <italic>L</italic> for a protein of length <italic>L</italic>. The input, in this case, is not split in windows, but we use the whole width MSA at once, while the depth is cut at the <italic>Y</italic> top alignments. The network for the first part of CMAP is similar to SS-RSA, with an embedding layer followed by up to six (rather than only one) 2D convolution/max pooling layers along each MSA column. In this case though, the output of the network is a contact map with shape <italic>L</italic> × <italic>L</italic>, so the preceding layers should represent the interaction between pairs of residues. This change of dimensionality is performed with a custom layer that performs the outer product from the output of the first stack of convolutional layers (<italic>H</italic>):
<disp-formula id="pone.0220182.e001"><alternatives><graphic xlink:href="pone.0220182.e001.jpg" id="pone.0220182.e001g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M1"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>O</mml:mi><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(1)</label></disp-formula>
Where <italic>H</italic> has dimensionality (<italic>L</italic> × <italic>F</italic> × <italic>S</italic>), <inline-formula id="pone.0220182.e002"><alternatives><graphic xlink:href="pone.0220182.e002.jpg" id="pone.0220182.e002g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M2"><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0220182.e003"><alternatives><graphic xlink:href="pone.0220182.e003.jpg" id="pone.0220182.e003g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M3"><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></alternatives></inline-formula> are obtained by adding singleton dimensions to <italic>H</italic> ((<italic>L</italic> × 1 × <italic>F</italic> × <italic>S</italic>) and (1 × <italic>L</italic> × <italic>F</italic> × <italic>S</italic>) respectively).</p>
        <p>This operation generates a four-dimensional hidden tensor <italic>OP</italic> of shape <italic>L</italic> × <italic>L</italic> × <italic>F</italic> × <italic>S</italic>, where <italic>F</italic> and <italic>S</italic> are the last two dimensions of the hidden tensor before the outer product. This output is then reshaped to a three-dimensional tensor of shape <italic>L</italic> × <italic>L</italic> × (<italic>F</italic> * <italic>S</italic>) and passed to a new stack of six to 20 (depending on the model) 2D convolutional layers with squared convolutions of varied size (3x3, 5x5, 10x10) and number of filters (10 to 50). The last convolutional layer has shape <italic>L</italic> × <italic>L</italic> × 2 and is followed by a softmax activation layer that output the contact prediction with a probability from 0 to 1. All the convolutional layers have ReLU activations and the outputs are zero-padded to match the two first dimensions of the inputs. Batch normalization is performed in the outputs of the convolutional layers in the CMAP network.</p>
      </sec>
    </sec>
    <sec id="sec007">
      <title>2.3 Training</title>
      <p>rawMSA was implemented in Python using the Keras library [<xref rid="pone.0220182.ref050" ref-type="bibr">50</xref>] with TensorFlow backend [<xref rid="pone.0220182.ref051" ref-type="bibr">51</xref>]. Training and testing were performed on computers equipped with NVIDIA GeForce 1080Ti, Tesla K80, and Quadro P6000 GPUs.</p>
      <p>The training procedure was run including one protein in each batch block, regardless of the size, and using an RMSprop optimizer with sparse categorical cross-entropy as loss function. The SS-RSA network was trained for five epochs, while the CMAP network was trained for up to 200 epochs. During training, a random 10% of the training samples were reserved for validation, while the rest were used for training. After training, the model with the highest accuracy on the validation data was used for testing.</p>
    </sec>
    <sec id="sec008">
      <title>2.4 Data sets</title>
      <p>The data set is composed of protein chains extracted from a 70% redundancy-reduced version of PDB compiled by PISCES [<xref rid="pone.0220182.ref052" ref-type="bibr">52</xref>] in April 2017 with minimum resolution of 3.0 Å and R-factor 1.0. This set contains 29,653 protein chains.</p>
      <sec id="sec009">
        <title>2.4.1 Avoiding homolog contamination</title>
        <p>When training our networks, we want to make sure that the testing and training sets are rigorously separated so that no protein in the test set is too similar to any protein that the network has already “seen” during the training phase.</p>
        <p>While most secondary structure and solvent accessibility prediction methods have been using 25-30% sequence identity as the threshold to separate testing and training sets [<xref rid="pone.0220182.ref053" ref-type="bibr">53</xref>–<xref rid="pone.0220182.ref056" ref-type="bibr">56</xref>], this practice has been discouraged as it has been shown that it is not sufficient to avoid information leakage [<xref rid="pone.0220182.ref057" ref-type="bibr">57</xref>]. This is apparently also valid for raw MSA inputs and ur tests separating sets at 25% sequence identity yields higher accuracies compared to our final results (data not shown).</p>
        <p>To correctly split training and testing sets, we used two databases based on a structural classification of the proteins: ECOD [<xref rid="pone.0220182.ref058" ref-type="bibr">58</xref>] and SCOPe [<xref rid="pone.0220182.ref059" ref-type="bibr">59</xref>] databases to assign one or more superfamilies to each of the protein chains in the initial set. Then, we removed any chains that were related to more than one superfamily. The set generated from ECOD contains 16,675 proteins (ECOD set), while the one generated from SCOPe contains 9885 proteins (SCOPe set). The SCOPe set contains fewer proteins than the ECOD set since SCOPe has a lower coverage of the PDB. We split each set into five subsets by making sure that no two proteins from the same superfamily were placed in two separate subsets. This ensured that the respective MSA inputs would not be too similar to each other and is the recommended practice when training neural networks using sequence profiles, which are extracted from MSAs [<xref rid="pone.0220182.ref057" ref-type="bibr">57</xref>].</p>
        <p>We used the SCOPe subsets to perform five-fold cross-validation on SS-RSA. We also used one of the ECOD subsets to train and validate CMAP, where the validation set was used to determine when to stop training to avoid overfitting, and to select the models that would be ensembled and tested, i.e. the models with the lowest validation error.</p>
      </sec>
      <sec id="sec010">
        <title>2.4.2 Multiple Sequence Alignments</title>
        <p>The MSAs for both SS-RSA and CMAP were obtained with HHblits [<xref rid="pone.0220182.ref060" ref-type="bibr">60</xref>] by searching with the master sequence against the HMM database clustered at 20% sequence identity from February, 2016 for three iterations, with 50% minimum coverage, 99% sequence similarity threshold, and 0.001 maximum E-value. We also obtained a second set of MSAs by running JackHMMER [<xref rid="pone.0220182.ref061" ref-type="bibr">61</xref>], for three iterations and 1<italic>e</italic> − 3 maximum E-value on the UniRef100 database from February 2016. The HHblits alignments were used to train and test the SS-RSA networks. The HHblits alignments were also used to train the CMAP network, while both the HHblits and the JackHMMER alignments were used as inputs when ensembling CMAP networks (see Ensembling section), as it improved the prediction accuracy. This approach was also tried for the SS-RSA network, but no improvement was observed.</p>
      </sec>
      <sec id="sec011">
        <title>2.4.3 Test sets</title>
        <p>We labeled the CMAP data sets by assigning a native contact map to each protein. A contact was assigned to a pair of residues in a protein if the Euclidean distance between their <italic>Cβ</italic> atoms (<italic>Cα</italic> for Gly) in the crystal structure from the PDB was lower than 8 Å. Otherwise, the two residues were assigned the non-contact label.</p>
        <p>We tested CMAP on the CASP12 RR (Residue-Residue) benchmark [<xref rid="pone.0220182.ref062" ref-type="bibr">62</xref>], which is composed of 37 protein chains/domains of the Free Modeling class (FM), i.e. protein targets for which no obvious protein homologs could be found at the time of the experiment (May-August 2016). To ensure a fair comparison with the predictors which participated in CASP12, we performed the benchmark in the same conditions to which all the other predictors where subjected at the time of the CASP experiment. We made sure that all protein structures (from Apr, 2016) in the training set (Apr, 2016) and sequences (from Feb, 2016) in the HHblist HMM and UniRef100 databases were released before CASP12 started.</p>
        <p>To test SS-RSA, we calculated the secondary structure (SS) and the Relative accessible Surface Area (RSA) with DSSP 2.0.4 [<xref rid="pone.0220182.ref063" ref-type="bibr">63</xref>]. We reduced the eight SS classes (G, H, I, E, B, S, T, C) to the more common three classes: Coil, Helix, Extended (C, H, E). We used the theoretical Maximum Accessibility Surface Area (Max ASA) defined in [<xref rid="pone.0220182.ref064" ref-type="bibr">64</xref>] to calculate the RSA from the absolute surface areas (ASA) in the DSSP output and we used [0, 0.04], (0.04, 0.25], (0.25, 0.5], (0.5, 1] as thresholds for the four-class RSA predictions (Buried, Partially Buried, Partially Accessible, Accessible), and [0, 0.25], (0.25, 1] as thresholds for the two-class RSA predictions (Buried, Accessible). We discarded the proteins for which DSSP could not produce an output, as well as those that had irregularities in their PDB formats. The final set contained 9,680 protein chains.</p>
      </sec>
      <sec id="sec012">
        <title>2.4.4 Quality measures</title>
        <p>The measure of the performance of the trained ensemble of SS-RSA networks is the three-class accuracy (Q3) for SS and the four-class and two-class accuracy for RSA, which are calculated by dividing the number of correctly classified residues by the total number of residues in the dataset.</p>
        <p>CMAP predictions for the CASP12 RR benchmark set were evaluated in accordance with the CASP criteria by calculating the accuracy of the top <italic>L</italic>/5 predicted long-range contacts, where <italic>L</italic> is the length of the protein, and the long-range contacts are contacts between residues with sequence separation distance over 23.</p>
      </sec>
      <sec id="sec013">
        <title>2.4.5 Ensembling</title>
        <p>Ensembling models usually yield a consensus model that performs better than any of the networks included in the ensemble [<xref rid="pone.0220182.ref065" ref-type="bibr">65</xref>]. Several networks both for CMAP and SS-RSA have been trained with different parameters (see “<xref ref-type="sec" rid="sec014">Results</xref>” section). Even though some models have worse performances on average, they are still saved. All the saved models that have been trained on the same set are used at testing time. The outputs from each model are ensembled to determine the final output. This is done by averaging all outputs from the softmax units and selecting the final class by picking the class with the highest average probability.</p>
        <p>In the CMAP case, each model in the ensemble is used to make two predictions for each target using either HHblits or JackHMMER alignment. Although the CMAP network is trained only on HHblits alignments, using the JackHMMER alignments in the ensemble improved the overall accuracy of the predictor.</p>
      </sec>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec014">
    <title>3 Results and discussion</title>
    <sec id="sec015">
      <title>3.1 Embeddings</title>
      <p>Tensorboard in Tensorflow can be used to visualize the output of the embeddings layer to see how each residue type is mapped on the output space. In <xref ref-type="fig" rid="pone.0220182.g002">Fig 2</xref> we show the embeddings outputs for an early version of the SS-RSA network where each amino acid type is mapped on a 4D space. The 4D vectors are projected onto a 2D space by principal component analysis on Tensorboard. In the figure, the amino acids that are the closest (lowest cosine between the 4D vectors) to lysine (<xref ref-type="fig" rid="pone.0220182.g002">Fig 2a</xref>) and tryptophan (<xref ref-type="fig" rid="pone.0220182.g002">Fig 2b</xref>) are highlighted. The amino acids closest to lysine (K) are histidine (H) and arginine (R), which makes sense, since they can all be positively charged. Similarly, the residues closest to the hydrophobic tryptophan (W) are also hydrophobic, indicating that the embeddings can discriminate between different kinds of amino acids and map them onto a space that makes sense from a chemical point of view.</p>
      <fig id="pone.0220182.g002" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0220182.g002</object-id>
        <label>Fig 2</label>
        <caption>
          <title>2D PCA of the space of the embedded vectors representing the single residues.</title>
          <p>In this example, we show the embedding outputs of a simpler network where the original space has a dimensionality of four. The residues that are closest (lower cosine between the 4D vectors) to (a) lysine and (b) tryptophan are colored (the closer the residue, the darker the hue).</p>
        </caption>
        <graphic xlink:href="pone.0220182.g002"/>
      </fig>
    </sec>
    <sec id="sec016">
      <title>3.2 SS and RSA predictions</title>
      <p>We have used the five-fold cross-validation results to determine the testing accuracy for SS-RSA. To compare the rawMSA approach against a classic profile-based method, we trained a separate network by removing the bottom layers from the SS-RSA network (embedding and first 2D convolution/Pooling layer) and we trained it by using the PSSMs calculated from the HHblits alignments as inputs (PSSM network). In order to assess the usefulness of amino acid embeddings, we also train a rawMSA neural network without embedding layer, where the input alignments are transformed using a simple one-hot encoding of each amino acid (One-hot100 network). In this case, because of limits imposed by the amount of memory available on our testing machine, we had to limit the one-hot network to 100 alignments only. We tested and trained the PSSM and one-hot network in the same way we trained the other networks, both for SS and RSA. In <xref ref-type="fig" rid="pone.0220182.g003">Fig 3</xref> we compare the performance of the PSSM and one-hot network against several rawMSA networks trained on different numbers of input sequences (100 to 1000 MSA sequences). The boxplot shows how the rawMSA networks with more input sequences perform generally better, with the rawMSA500 and rawMSA1000 networks performing slightly better than the classic PSSM network in predicting secondary structure, and all the rawMSA networks outperforming the PSSM network in predicting solvent accessibility. We also show that the rawMSA100 network outperforms the One-hot100 network both in the SS and RSA experiments.</p>
      <fig id="pone.0220182.g003" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0220182.g003</object-id>
        <label>Fig 3</label>
        <caption>
          <title>Per target secondary structure (a) and four-class solvent accessibility (b) accuracy for predictions using one hot encoding, a number of rawMSA networks, and a classical PSSM network trained and tested on the same dataset.</title>
          <p>One hot100 skips the rawMSA embedding step and encodes the alignments using one hot encoding, limited to the top100 alignments for memory reasons. Four different rawMSA networks are tested at variable MSA depths, using top 100, 200, 500 or 1000 alignments from the MSAs as input to the SS-RSA network. The average accuracies are shown as red squares.</p>
        </caption>
        <graphic xlink:href="pone.0220182.g003"/>
      </fig>
      <p>The final SS-RSA network is an ensemble of six networks trained in five-fold cross-validation on 100 to 3000 input MSA sequences per target.</p>
      <p>The results are shown in <xref rid="pone.0220182.t001" ref-type="table">Table 1</xref>.</p>
      <table-wrap id="pone.0220182.t001" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0220182.t001</object-id>
        <label>Table 1</label>
        <caption>
          <title>Results for the SS-RSA networks trained to predict secondary structure and solvent accessibility.</title>
        </caption>
        <alternatives>
          <graphic id="pone.0220182.t001g" xlink:href="pone.0220182.t001"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="left" style="border-bottom:thick" rowspan="1" colspan="1">Predictor</th>
                <th align="center" style="border-bottom:thick" rowspan="1" colspan="1"/>
                <th align="center" style="border-bottom:thick" rowspan="1" colspan="1">Accuracy</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">rawMSA SS ensemble</td>
                <td align="center" rowspan="1" colspan="1"/>
                <td align="char" char="." rowspan="1" colspan="1">83.4</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">One-hot100 SS</td>
                <td align="center" rowspan="1" colspan="1"/>
                <td align="char" char="." rowspan="1" colspan="1">80.5</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">rawMSA100 SS</td>
                <td align="center" rowspan="1" colspan="1"/>
                <td align="char" char="." rowspan="1" colspan="1">80.7</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">rawMSA1000 SS</td>
                <td align="center" rowspan="1" colspan="1"/>
                <td align="char" char="." rowspan="1" colspan="1">81.8</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">rawMSA1000 SS BLOSUM62 sort</td>
                <td align="center" rowspan="1" colspan="1"/>
                <td align="char" char="." rowspan="1" colspan="1">80.3</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">rawMSA1000 SS shuffle</td>
                <td align="center" rowspan="1" colspan="1"/>
                <td align="char" char="." rowspan="1" colspan="1">79.8</td>
              </tr>
              <tr>
                <td align="left" style="border-bottom:thick" rowspan="1" colspan="1">PSSM SS</td>
                <td align="center" style="border-bottom:thick" rowspan="1" colspan="1"/>
                <td align="char" char="." style="border-bottom:thick" rowspan="1" colspan="1">81.7</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">rawMSA RSA ensemble</td>
                <td align="center" rowspan="1" colspan="1">(4-class)</td>
                <td align="char" char="." rowspan="1" colspan="1">57.7</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">One-hot100 RSA</td>
                <td align="center" rowspan="1" colspan="1">(4-class)</td>
                <td align="char" char="." rowspan="1" colspan="1">53.7</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">rawMSA100 RSA</td>
                <td align="center" rowspan="1" colspan="1">(4-class)</td>
                <td align="char" char="." rowspan="1" colspan="1">55.0</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">rawMSA1000 RSA</td>
                <td align="center" rowspan="1" colspan="1">(4-class)</td>
                <td align="char" char="." rowspan="1" colspan="1">56.1</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">PSSM RSA</td>
                <td align="center" rowspan="1" colspan="1">(4-class)</td>
                <td align="char" char="." rowspan="1" colspan="1">54.1</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">rawMSA RSA ensemble</td>
                <td align="center" rowspan="1" colspan="1">(2-class)</td>
                <td align="char" char="." rowspan="1" colspan="1">81.2</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
        <table-wrap-foot>
          <fn id="t001fn001">
            <p>The number in the predictor name refers to the number of sequences from the MSA that were used; <italic>BLOSUM62 sort</italic> and <italic>shuffle</italic> refers to different sorting of the sequences in the MSA (see text for details).</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>It is difficult to make a direct comparison of rawMSA against other predictors in literature because of inevitable differences in the datasets. One example of this comes from secondary structure prediction systems, which have recently been reported to predict at accuracies (Q3) of up to 84% [<xref rid="pone.0220182.ref066" ref-type="bibr">66</xref>], yet we have not been able to find a recent study where the reported accuracy is supported by a proper splitting of the training and testing sets (see “Avoiding homolog contamination” paragraph). Running local versions of existing software does not solve that problem since it is not clear exactly which sequences were used for training. Also, in many cases a final network is trained using all available sequence, which means that any test is bound to be contaminated by homologous information. However, given the very large size of our test set, the rigorousness of our experimental setup, and the fact that rawMSA outperforms our own PSSM-based method, we believe that rawMSA compares favorably against the state of the art.</p>
      <p>The convolutional layers of rawMSA depends on the order of the input sequences, since that will change the block of aligned positions from which features are extracted. To estimate the degree if this dependence we trained rawMSA with sequences sorted by sequence similarity using the BLOSUM62 similarity matrix (rawMSA1000 SS BLOSUM62 sort) and with randomly shuffled sequences (rawMSA1000 SS shuffle), see <xref rid="pone.0220182.t001" ref-type="table">Table 1</xref>). Even though the performance decrease is small, neither of these two approaches worked as well as using the MSA as outputted by HHblits, most likely because it is easier to learn from blocks of similar positions with smoother mutational transitions.</p>
    </sec>
    <sec id="sec017">
      <title>3.3 CMAP predictions</title>
      <p>The final CMAP network is an ensemble of 10 networks trained on 10 to 1000 input sequences and varying numbers of layers (10 to 24 convolutional layers). The CMAP predictions for each target have been sorted by the contact probability measure output by the ensemble, then the top <italic>L</italic>/5 long-range contacts have been evaluated against the native contact map. The final accuracy has been calculated as the average of the accuracies for all targets. In order to make a fair comparison against the other predictors, we have downloaded all of the predictions made in CASP12 and evaluated them with the same system. In <xref rid="pone.0220182.t002" ref-type="table">Table 2</xref> we compare the top <italic>L</italic>/5 long-range accuracy of rawMSA CMAP against the top 5 CASP12 predictors.</p>
      <table-wrap id="pone.0220182.t002" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0220182.t002</object-id>
        <label>Table 2</label>
        <caption>
          <title>Comparison of rawMSA against the top 5 contact prediction methods in CASP12.</title>
        </caption>
        <alternatives>
          <graphic id="pone.0220182.t002g" xlink:href="pone.0220182.t002"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="left" style="border-bottom:thick" rowspan="1" colspan="1">Predictor</th>
                <th align="center" style="border-bottom:thick" rowspan="1" colspan="1">Domain Count</th>
                <th align="center" style="border-bottom:thick" rowspan="1" colspan="1">L/5 LR Accuracy</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">rawMSA CMAP</td>
                <td align="center" rowspan="1" colspan="1">37</td>
                <td align="char" char="." rowspan="1" colspan="1">43.8</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">RaptorX-Contact</td>
                <td align="center" rowspan="1" colspan="1">37</td>
                <td align="char" char="." rowspan="1" colspan="1">43.0</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">iFold_1</td>
                <td align="center" rowspan="1" colspan="1">36</td>
                <td align="char" char="." rowspan="1" colspan="1">42.3</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Deepfold-Contact</td>
                <td align="center" rowspan="1" colspan="1">37</td>
                <td align="char" char="." rowspan="1" colspan="1">38.6</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">MetaPSICOV</td>
                <td align="center" rowspan="1" colspan="1">37</td>
                <td align="char" char="." rowspan="1" colspan="1">38.4</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">MULTICOM-CLUSTER</td>
                <td align="center" rowspan="1" colspan="1">37</td>
                <td align="char" char="." rowspan="1" colspan="1">37.9</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
        <table-wrap-foot>
          <fn id="t002fn001">
            <p>All predictions from CASP12 have been re-evaluated to ensure a fair comparison. The accuracy is calculated on the top L/5 long-range contacts.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>rawMSA outperforms the top predictors in CASP12 under the same testing conditions. This is unexpected, since it is the only top predictor not to use any kind of explicit coevolution-based features, or any other inputs than the MSA. On the other hand, CASP12 was held in 2016 and the field has made rapid progress since then. For example, the group behind RaptorX-Contact has reported an improvement of roughly 12 percentage points on this same CASP12 test set only months after the experiment was closed with a new deeper version of their neural network [<xref rid="pone.0220182.ref044" ref-type="bibr">44</xref>].</p>
      <p>Moreover, we expect rawMSA to be better than coevolution-based methods only whenever a relatively small number of sequences can be found in an MSA for a given target sequence, since only up to 1000 input MSA sequences could be used in training and testing because of limits in the amount of GPU RAM available (&lt;25GB). Coevolution-based methods are more accurate as the number of sequences in the MSA increases and better metagenomic datasets [<xref rid="pone.0220182.ref067" ref-type="bibr">67</xref>] will produce larger MSAs for more target sequences. In the latest CASP13 experiment [<xref rid="pone.0220182.ref068" ref-type="bibr">68</xref>], where we participated with a prototype of rawMSA trained on a smaller ensemble of simpler models (up to 400 sequences per MSA) using a smaller sequence databases (Uniclust30), rawMSA was not among the best predictors in the contact prediction category. Nevertheless, the rawMSA prototype still outperformed a number of other coevolution-based methods, in particular the METAPSICOV_baseline method for MSAs with &lt; 400 sequences, see <xref ref-type="fig" rid="pone.0220182.g004">Fig 4</xref>. For sequences with &gt; 400 sequences in the MSA METAPSICOV_baseline method is still better. However, since we observe a clear correlation in testing accuracy and the number of sequences used as input, it is reasonable to expect that rawMSA will benefit from training on GPUs with larger memory allowing more sequences and deeper architectures to be used. In addition, rawMSA CMAP could also be improved by predicting distances, which seem to be direction in which the field is heading.</p>
      <fig id="pone.0220182.g004" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0220182.g004</object-id>
        <label>Fig 4</label>
        <caption>
          <title>Impact of the number of sequences in the MSA on performance for rawMSA and METAPSICOV_baseline in the CASP13 FM targets.</title>
        </caption>
        <graphic xlink:href="pone.0220182.g004"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec018">
    <title>4 Conclusion</title>
    <p>We have presented a new paradigm for the prediction of structural features of proteins called rawMSA, which involves using raw multiple sequence alignments (MSA) of proteins as input instead of compressing them into protein sequence profiles, as is common practice today. Furthermore, rawMSA does not need any other manually designed or otherwise hand-picked extra feature as input, but instead exploits the capability that deep networks have of automatically extracting any relevant feature from the raw data.</p>
    <p>To convert MSAs, which could be described as categorical data, to a more machine-friendly format, rawMSA adopts embeddings, a technique from the field of Natural Language Processing to adaptively map discrete inputs from a dictionary of symbols into vectors in a continuous space.</p>
    <p>To showcase our novel representation of the MSA, we developed a few different flavors of rawMSA to predict secondary structure, relative solvent accessibility and inter-residue contact maps. All these networks use the same and only kind of input, i.e. the MSA. After rigorous testing, we show how rawMSA SS-RSA sets a new state of the art for these kinds of predictions, and rawMSA CMAP performs on par with methods using more pre-calculated features in the inter-residue contact map prediction category in CASP12 and CASP13. Clearly demonstrating that rawMSA represents a promising development that can pave the way for improved methods using rawMSA instead of sequence profiles to represent evolutionary information in the coming years.</p>
  </sec>
</body>
<back>
  <ack>
    <p>The Swedish e-Science Research Center. Computations were performed on resources provided by the Swedish National Infrastructure for Computing (SNIC) at the National Supercomputer Centre (NSC) in Linköping and at the High Performance Computing Center North (HPC2N) in Umeå and at Hops (<ext-link ext-link-type="uri" xlink:href="http://www.hops.io">www.hops.io</ext-link>). We also thank Isak Johansson-Åkhe for helpful discussions.</p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="pone.0220182.ref001">
      <label>1</label>
      <mixed-citation publication-type="journal"><name><surname>Dill</surname><given-names>KA</given-names></name>, <name><surname>Ozkan</surname><given-names>SB</given-names></name>, <name><surname>Weikl</surname><given-names>TR</given-names></name>, <name><surname>Chodera</surname><given-names>JD</given-names></name>, <name><surname>Voelz</surname><given-names>VA</given-names></name>. <article-title>The protein folding problem: when will it be solved?</article-title><source>Current Opinion in Structural Biology</source>. <year>2007</year>;<volume>17</volume>(<issue>3</issue>):<fpage>342</fpage>–<lpage>346</lpage>. <pub-id pub-id-type="doi">10.1016/j.sbi.2007.06.001</pub-id><?supplied-pmid 17572080?><pub-id pub-id-type="pmid">17572080</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref002">
      <label>2</label>
      <mixed-citation publication-type="journal"><name><surname>Shell</surname><given-names>MS</given-names></name>, <name><surname>Ozkan</surname><given-names>SB</given-names></name>, <name><surname>Voelz</surname><given-names>V</given-names></name>, <name><surname>Wu</surname><given-names>GA</given-names></name>, <name><surname>Dill</surname><given-names>KA</given-names></name>. <article-title>Blind test of physics-based prediction of protein structures</article-title>. <source>Biophysical journal</source>. <year>2009</year>;<volume>96</volume>(<issue>3</issue>):<fpage>917</fpage>–<lpage>924</lpage>. <pub-id pub-id-type="doi">10.1016/j.bpj.2008.11.009</pub-id><?supplied-pmid 19186130?><pub-id pub-id-type="pmid">19186130</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref003">
      <label>3</label>
      <mixed-citation publication-type="journal"><name><surname>Sippl</surname><given-names>MJ</given-names></name>. <article-title>Calculation of conformational ensembles from potentials of mean force. An approach to the knowledge-based prediction of local structures in globular proteins</article-title>. <source>Journal of Molecular Biology</source>. <year>1990</year>;<volume>213</volume>(<issue>4</issue>):<fpage>859</fpage>–<lpage>883</lpage>. <pub-id pub-id-type="doi">10.1016/s0022-2836(05)80269-4</pub-id><?supplied-pmid 2359125?><pub-id pub-id-type="pmid">2359125</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref004">
      <label>4</label>
      <mixed-citation publication-type="journal"><name><surname>Jones</surname><given-names>DT</given-names></name>, <name><surname>Taylor</surname><given-names>WR</given-names></name>, <name><surname>Thornton</surname><given-names>JM</given-names></name>. <article-title>A new approach to protein fold recognition</article-title>. <source>Nature</source>. <year>1992</year>;<volume>358</volume>(<issue>6381</issue>):<fpage>86</fpage>–<lpage>89</lpage>. <pub-id pub-id-type="doi">10.1038/358086a0</pub-id><?supplied-pmid 1614539?><pub-id pub-id-type="pmid">1614539</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref005">
      <label>5</label>
      <mixed-citation publication-type="journal"><name><surname>Sippl</surname><given-names>MJ</given-names></name>. <article-title>Knowledge-based potentials for proteins</article-title>. <source>Current Opinion in Structural Biology</source>. <year>1995</year>;<volume>5</volume>(<issue>2</issue>):<fpage>229</fpage>–<lpage>235</lpage>. <pub-id pub-id-type="doi">10.1016/0959-440X(95)80081-6</pub-id><?supplied-pmid 7648326?><pub-id pub-id-type="pmid">7648326</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref006">
      <label>6</label>
      <mixed-citation publication-type="journal"><name><surname>Lazaridis</surname><given-names>T</given-names></name>, <name><surname>Karplus</surname><given-names>M</given-names></name>. <article-title>Effective energy functions for protein structure prediction</article-title>. <source>Current Opinion in Structural Biology</source>. <year>2000</year>;<volume>10</volume>(<issue>2</issue>):<fpage>139</fpage>–<lpage>145</lpage>. <pub-id pub-id-type="doi">10.1016/S0959-440X(00)00063-4</pub-id><?supplied-pmid 10753811?><pub-id pub-id-type="pmid">10753811</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref007">
      <label>7</label>
      <mixed-citation publication-type="journal"><name><surname>Simons</surname><given-names>KT</given-names></name>, <name><surname>Kooperberg</surname><given-names>C</given-names></name>, <name><surname>Huang</surname><given-names>E</given-names></name>, <name><surname>Baker</surname><given-names>D</given-names></name>. <article-title>Assembly of protein tertiary structures from fragments with similar local sequences using simulated annealing and Bayesian scoring functions</article-title>. <source>Journal of Molecular Biology</source>. <year>1997</year>;<volume>268</volume>(<issue>1</issue>):<fpage>209</fpage>–<lpage>225</lpage>. <pub-id pub-id-type="doi">10.1006/jmbi.1997.0959</pub-id><?supplied-pmid 9149153?><pub-id pub-id-type="pmid">9149153</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref008">
      <label>8</label>
      <mixed-citation publication-type="journal"><name><surname>Shaw</surname><given-names>DE</given-names></name>, <name><surname>Maragakis</surname><given-names>P</given-names></name>, <name><surname>Lindorff-Larsen</surname><given-names>K</given-names></name>, <name><surname>Piana</surname><given-names>S</given-names></name>, <name><surname>Dror</surname><given-names>RO</given-names></name>, <name><surname>Eastwood</surname><given-names>MP</given-names></name>, <etal>et al</etal><article-title>Atomic-level characterization of the structural dynamics of proteins</article-title>. <source>Science (New York, NY)</source>. <year>2010</year>;<volume>330</volume>(<issue>6002</issue>):<fpage>341</fpage>–<lpage>346</lpage>. <pub-id pub-id-type="doi">10.1126/science.1187409</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref009">
      <label>9</label>
      <mixed-citation publication-type="journal"><name><surname>Wallner</surname><given-names>B</given-names></name>, <name><surname>Elofsson</surname><given-names>A</given-names></name>. <article-title>Can correct protein models be identified?</article-title><source>Protein Science</source>. <year>2003</year>;<volume>12</volume>(<issue>5</issue>):<fpage>1073</fpage>–<lpage>1086</lpage>. <pub-id pub-id-type="doi">10.1110/ps.0236803</pub-id><?supplied-pmid 12717029?><pub-id pub-id-type="pmid">12717029</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref010">
      <label>10</label>
      <mixed-citation publication-type="journal"><name><surname>Jones</surname><given-names>DT</given-names></name>. <article-title>Protein secondary structure prediction based on position-specific scoring matrices1</article-title>. <source>Journal of Molecular Biology</source>. <year>1999</year>;<volume>292</volume>(<issue>2</issue>):<fpage>195</fpage>–<lpage>202</lpage>. <pub-id pub-id-type="doi">10.1006/jmbi.1999.3091</pub-id><?supplied-pmid 10493868?><pub-id pub-id-type="pmid">10493868</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref011">
      <label>11</label>
      <mixed-citation publication-type="journal"><name><surname>Cuff</surname><given-names>JA</given-names></name>, <name><surname>Clamp</surname><given-names>ME</given-names></name>, <name><surname>Siddiqui</surname><given-names>AS</given-names></name>, <name><surname>Finlay</surname><given-names>M</given-names></name>, <name><surname>Barton</surname><given-names>GJ</given-names></name>. <article-title>JPred: a consensus secondary structure prediction server</article-title>. <source>Bioinformatics</source>. <year>1998</year>;<volume>14</volume>(<issue>10</issue>):<fpage>892</fpage>–<lpage>893</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/14.10.892</pub-id><?supplied-pmid 9927721?><pub-id pub-id-type="pmid">9927721</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref012">
      <label>12</label>
      <mixed-citation publication-type="journal"><name><surname>Pollastri</surname><given-names>G</given-names></name>, <name><surname>Przybylski</surname><given-names>D</given-names></name>, <name><surname>Rost</surname><given-names>B</given-names></name>, <name><surname>Baldi</surname><given-names>P</given-names></name>. <article-title>Improving the prediction of protein secondary structure in three and eight classes using recurrent neural networks and profiles</article-title>. <source>Proteins: Structure, Function, and Bioinformatics</source>. <year>2002</year>;<volume>47</volume>(<issue>2</issue>):<fpage>228</fpage>–<lpage>235</lpage>. <pub-id pub-id-type="doi">10.1002/prot.10082</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref013">
      <label>13</label>
      <mixed-citation publication-type="journal"><name><surname>Pollastri</surname><given-names>G</given-names></name>, <name><surname>Mclysaght</surname><given-names>A</given-names></name>. <article-title>Porter: a new, accurate server for protein secondary structure prediction</article-title>. <source>Bioinformatics</source>. <year>2004</year>;<volume>21</volume>(<issue>8</issue>):<fpage>1719</fpage>–<lpage>1720</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/bti203</pub-id><?supplied-pmid 15585524?><pub-id pub-id-type="pmid">15585524</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref014">
      <label>14</label>
      <mixed-citation publication-type="journal"><name><surname>Drozdetskiy</surname><given-names>A</given-names></name>, <name><surname>Cole</surname><given-names>C</given-names></name>, <name><surname>Procter</surname><given-names>J</given-names></name>, <name><surname>Barton</surname><given-names>GJ</given-names></name>. <article-title>JPred4: a protein secondary structure prediction server</article-title>. <source>Nucleic acids research</source>. <year>2015</year>;<volume>43</volume>(<issue>W1</issue>):<fpage>W389</fpage>–<lpage>94</lpage>. <pub-id pub-id-type="doi">10.1093/nar/gkv332</pub-id><?supplied-pmid 25883141?><pub-id pub-id-type="pmid">25883141</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref015">
      <label>15</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>S</given-names></name>, <name><surname>Li</surname><given-names>W</given-names></name>, <name><surname>Liu</surname><given-names>S</given-names></name>, <name><surname>Xu</surname><given-names>J</given-names></name>. <article-title>RaptorX-Property: a web server for protein structure property prediction</article-title>. <source>Nucleic acids research</source>. <year>2016</year>;<volume>44</volume>(<issue>W1</issue>):<fpage>W430</fpage>–<lpage>5</lpage>. <pub-id pub-id-type="doi">10.1093/nar/gkw306</pub-id><?supplied-pmid 27112573?><pub-id pub-id-type="pmid">27112573</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref016">
      <label>16</label>
      <mixed-citation publication-type="journal"><name><surname>Rost</surname><given-names>B</given-names></name>, <name><surname>Sander</surname><given-names>C</given-names></name>. <article-title>Conservation and prediction of solvent accessibility in protein families</article-title>. <source>Proteins: Structure, Function, and Bioinformatics</source>. <year>1994</year>;<volume>20</volume>(<issue>3</issue>):<fpage>216</fpage>–<lpage>226</lpage>. <pub-id pub-id-type="doi">10.1002/prot.340200303</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref017">
      <label>17</label>
      <mixed-citation publication-type="journal"><name><surname>Pollastri</surname><given-names>G</given-names></name>, <name><surname>Baldi</surname><given-names>P</given-names></name>, <name><surname>Fariselli</surname><given-names>P</given-names></name>, <name><surname>Casadio</surname><given-names>R</given-names></name>. <article-title>Prediction of coordination number and relative solvent accessibility in proteins</article-title>. <source>Proteins: Structure, Function, and Bioinformatics</source>. <year>2002</year>;<volume>47</volume>(<issue>2</issue>):<fpage>142</fpage>–<lpage>153</lpage>. <pub-id pub-id-type="doi">10.1002/prot.10069</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref018">
      <label>18</label>
      <mixed-citation publication-type="journal"><name><surname>Adamczak</surname><given-names>R</given-names></name>, <name><surname>Porollo</surname><given-names>A</given-names></name>, <name><surname>Meller</surname><given-names>J</given-names></name>. <article-title>Accurate prediction of solvent accessibility using neural networks–based regression</article-title>. <source>Proteins: Structure, Function, and Bioinformatics</source>. <year>2004</year>;<volume>56</volume>(<issue>4</issue>):<fpage>753</fpage>–<lpage>767</lpage>. <pub-id pub-id-type="doi">10.1002/prot.20176</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref019">
      <label>19</label>
      <mixed-citation publication-type="journal"><name><surname>Gao</surname><given-names>Y</given-names></name>, <name><surname>Wang</surname><given-names>S</given-names></name>, <name><surname>Deng</surname><given-names>M</given-names></name>, <name><surname>Xu</surname><given-names>J</given-names></name>. <article-title>RaptorX-Angle: real-value prediction of protein backbone dihedral angles through a hybrid method of clustering and deep learning</article-title>. <source>BMC Bioinformatics</source>. <year>2018</year>;<volume>19</volume>(<issue>Suppl 4</issue>):<fpage>100</fpage><pub-id pub-id-type="doi">10.1186/s12859-018-2065-x</pub-id><?supplied-pmid 29745828?><pub-id pub-id-type="pmid">29745828</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref020">
      <label>20</label>
      <mixed-citation publication-type="journal"><name><surname>Linding</surname><given-names>R</given-names></name>, <name><surname>Jensen</surname><given-names>LJ</given-names></name>, <name><surname>Diella</surname><given-names>F</given-names></name>, <name><surname>Bork</surname><given-names>P</given-names></name>, <name><surname>Gibson</surname><given-names>TJ</given-names></name>, <name><surname>Russell</surname><given-names>RB</given-names></name>. <article-title>Protein disorder prediction: implications for structural proteomics</article-title>. <source>Structure</source>. <year>2003</year>;<volume>11</volume>(<issue>11</issue>):<fpage>1453</fpage>–<lpage>1459</lpage>. <pub-id pub-id-type="doi">10.1016/j.str.2003.10.002</pub-id><?supplied-pmid 14604535?><pub-id pub-id-type="pmid">14604535</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref021">
      <label>21</label>
      <mixed-citation publication-type="journal"><name><surname>Ward</surname><given-names>JJ</given-names></name>, <name><surname>McGuffin</surname><given-names>LJ</given-names></name>, <name><surname>Bryson</surname><given-names>K</given-names></name>, <name><surname>Buxton</surname><given-names>BF</given-names></name>, <name><surname>Jones</surname><given-names>DT</given-names></name>. <article-title>The DISOPRED server for the prediction of protein disorder</article-title>. <source>Bioinformatics</source>. <year>2004</year>;<volume>20</volume>(<issue>13</issue>):<fpage>2138</fpage>–<lpage>2139</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/bth195</pub-id><?supplied-pmid 15044227?><pub-id pub-id-type="pmid">15044227</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref022">
      <label>22</label>
      <mixed-citation publication-type="journal"><name><surname>Jones</surname><given-names>DT</given-names></name>, <name><surname>Cozzetto</surname><given-names>D</given-names></name>. <article-title>DISOPRED3: precise disordered region predictions with annotated protein-binding activity</article-title>. <source>Bioinformatics</source>. <year>2015</year>;<volume>31</volume>(<issue>6</issue>):<fpage>857</fpage>–<lpage>863</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btu744</pub-id><?supplied-pmid 25391399?><pub-id pub-id-type="pmid">25391399</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref023">
      <label>23</label>
      <mixed-citation publication-type="journal"><name><surname>Basu</surname><given-names>Sankar</given-names></name>, <name><surname>Söderquist</surname><given-names>Fredrik</given-names></name>, <name><surname>Wallner</surname><given-names>Björn</given-names></name>. <article-title>Proteus: a random forest classifier to predict disorder-to-order transitioning binding regions in intrinsically disordered proteins</article-title>. <source>Journal of computer-aided molecular design</source>. <year>2017</year>;<volume>31</volume>(<issue>5</issue>):<fpage>453</fpage>–<lpage>466</lpage>. <pub-id pub-id-type="doi">10.1007/s10822-017-0020-y</pub-id><?supplied-pmid 28365882?><pub-id pub-id-type="pmid">28365882</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref024">
      <label>24</label>
      <mixed-citation publication-type="journal"><name><surname>Fariselli</surname><given-names>P</given-names></name>, <name><surname>Casadio</surname><given-names>R</given-names></name>. <article-title>A neural network based predictor of residue contacts in proteins</article-title>. <source>Protein engineering</source>. <year>1999</year>;<volume>12</volume>(<issue>1</issue>):<fpage>15</fpage>–<lpage>21</lpage>. <pub-id pub-id-type="doi">10.1093/protein/12.1.15</pub-id><?supplied-pmid 10065706?><pub-id pub-id-type="pmid">10065706</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref025">
      <label>25</label>
      <mixed-citation publication-type="journal"><name><surname>Punta</surname><given-names>M</given-names></name>, <name><surname>Rost</surname><given-names>B</given-names></name>. <article-title>PROFcon: novel prediction of long-range contacts</article-title>. <source>Bioinformatics</source>. <year>2005</year>;<volume>21</volume>(<issue>13</issue>):<fpage>2960</fpage>–<lpage>2968</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/bti454</pub-id><?supplied-pmid 15890748?><pub-id pub-id-type="pmid">15890748</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref026">
      <label>26</label>
      <mixed-citation publication-type="journal"><name><surname>Kukic</surname><given-names>P</given-names></name>, <name><surname>Mirabello</surname><given-names>C</given-names></name>, <name><surname>Tradigo</surname><given-names>G</given-names></name>, <name><surname>Walsh</surname><given-names>I</given-names></name>, <name><surname>Veltri</surname><given-names>P</given-names></name>, <name><surname>Pollastri</surname><given-names>G</given-names></name>. <article-title>Toward an accurate prediction of inter-residue distances in proteins using 2D recursive neural networks</article-title>. <source>BMC Bioinformatics</source>. <year>2014</year>;<volume>15</volume>:<fpage>6</fpage><pub-id pub-id-type="doi">10.1186/1471-2105-15-6</pub-id><?supplied-pmid 24410833?><pub-id pub-id-type="pmid">24410833</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref027">
      <label>27</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>S</given-names></name>, <name><surname>Sun</surname><given-names>S</given-names></name>, <name><surname>Li</surname><given-names>Z</given-names></name>, <name><surname>Zhang</surname><given-names>R</given-names></name>, <name><surname>Xu</surname><given-names>J</given-names></name>. <article-title>Accurate De Novo Prediction of Protein Contact Map by Ultra-Deep Learning Model</article-title>. <source>PLoS Computational Biology</source>. <year>2017</year>;<volume>13</volume>(<issue>1</issue>):<fpage>e1005324</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005324</pub-id><?supplied-pmid 28056090?><pub-id pub-id-type="pmid">28056090</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref028">
      <label>28</label>
      <mixed-citation publication-type="journal"><name><surname>Ray</surname><given-names>A</given-names></name>, <name><surname>Lindahl</surname><given-names>E</given-names></name>, <name><surname>Wallner</surname><given-names>B</given-names></name>. <article-title>Improved model quality assessment using ProQ2</article-title>. <source>BMC Bioinformatics</source>. <year>2012</year>;<volume>13</volume>(<issue>1</issue>):<fpage>224</fpage><pub-id pub-id-type="doi">10.1186/1471-2105-13-224</pub-id><?supplied-pmid 22963006?><pub-id pub-id-type="pmid">22963006</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref029">
      <label>29</label>
      <mixed-citation publication-type="journal"><name><surname>Uziela</surname><given-names>Karolis</given-names></name>, <name><surname>Menendez Hurtado</surname><given-names>David</given-names></name>, <name><surname>Shu</surname><given-names>Nanjiang</given-names></name>, <name><surname>Wallner</surname><given-names>Björn</given-names></name>, <name><surname>Elofsson</surname><given-names>Arne</given-names></name>. <article-title>ProQ3D: improved model quality assessments using deep learning</article-title>. <source>Bioinformatics</source>. <year>2017</year>;<volume>33</volume>(<issue>10</issue>):<fpage>1578</fpage>–<lpage>1580</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btw819</pub-id><?supplied-pmid 28052925?><pub-id pub-id-type="pmid">28052925</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref030">
      <label>30</label>
      <mixed-citation publication-type="journal"><name><surname>Cao</surname><given-names>R</given-names></name>, <name><surname>Adhikari</surname><given-names>B</given-names></name>, <name><surname>Bhattacharya</surname><given-names>D</given-names></name>, <name><surname>Sun</surname><given-names>M</given-names></name>, <name><surname>Hou</surname><given-names>J</given-names></name>, <name><surname>Cheng</surname><given-names>J</given-names></name>. <article-title>QAcon: single model quality assessment using protein structural and contact information with machine learning techniques</article-title>. <source>Bioinformatics</source>. <year>2017</year>;<volume>33</volume>(<issue>4</issue>):<fpage>586</fpage>–<lpage>588</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btw694</pub-id><?supplied-pmid 28035027?><pub-id pub-id-type="pmid">28035027</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref031">
      <label>31</label>
      <mixed-citation publication-type="journal"><name><surname>Rost</surname><given-names>B</given-names></name>, <name><surname>Sander</surname><given-names>C</given-names></name>. <article-title>Prediction of protein secondary structure at better than 70% accuracy</article-title>. <source>Journal of Molecular Biology</source>. <year>1993</year>;<volume>232</volume>(<issue>2</issue>):<fpage>584</fpage>–<lpage>599</lpage>. <pub-id pub-id-type="doi">10.1006/jmbi.1993.1413</pub-id><?supplied-pmid 8345525?><pub-id pub-id-type="pmid">8345525</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref032">
      <label>32</label>
      <mixed-citation publication-type="journal"><name><surname>Cuff</surname><given-names>JA</given-names></name>, <name><surname>Barton</surname><given-names>GJ</given-names></name>. <article-title>Application of multiple sequence alignment profiles to improve protein secondary structure prediction</article-title>. <source>Proteins: Structure, Function, and Bioinformatics</source>. <year>2000</year>;<volume>40</volume>(<issue>3</issue>):<fpage>502</fpage>–<lpage>511</lpage>. <pub-id pub-id-type="doi">10.1002/1097-0134(20000815)40:3&lt;502::AID-PROT170&gt;3.0.CO;2-Q</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref033">
      <label>33</label>
      <mixed-citation publication-type="book"><name><surname>Rohl</surname><given-names>CA</given-names></name>, <name><surname>Strauss</surname><given-names>CE</given-names></name>, <name><surname>Misura</surname><given-names>KM</given-names></name>, <name><surname>Baker</surname><given-names>D</given-names></name>. <chapter-title>Protein structure prediction using Rosetta</chapter-title> In: <source>Methods in Enzymology</source>. <volume>vol. 383</volume>
<publisher-name>Elsevier</publisher-name>; <year>2004</year> p. <fpage>66</fpage>–<lpage>93</lpage>.<pub-id pub-id-type="pmid">15063647</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref034">
      <label>34</label>
      <mixed-citation publication-type="journal"><name><surname>McGuffin</surname><given-names>LJ</given-names></name>, <name><surname>Bryson</surname><given-names>K</given-names></name>, <name><surname>Jones</surname><given-names>DT</given-names></name>. <article-title>The PSIPRED protein structure prediction server</article-title>. <source>Bioinformatics</source>. <year>2000</year>;<volume>16</volume>(<issue>4</issue>):<fpage>404</fpage>–<lpage>405</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/16.4.404</pub-id><?supplied-pmid 10869041?><pub-id pub-id-type="pmid">10869041</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref035">
      <label>35</label>
      <mixed-citation publication-type="journal"><name><surname>Roy</surname><given-names>A</given-names></name>, <name><surname>Kucukural</surname><given-names>A</given-names></name>, <name><surname>Zhang</surname><given-names>Y</given-names></name>. <article-title>I-TASSER: a unified platform for automated protein structure and function prediction</article-title>. <source>Nature Protocols</source>. <year>2010</year>;<volume>5</volume>(<issue>4</issue>):<fpage>725</fpage><pub-id pub-id-type="doi">10.1038/nprot.2010.5</pub-id><?supplied-pmid 20360767?><pub-id pub-id-type="pmid">20360767</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref036">
      <label>36</label>
      <mixed-citation publication-type="journal"><name><surname>Baú</surname><given-names>D</given-names></name>, <name><surname>Martin</surname><given-names>AJ</given-names></name>, <name><surname>Mooney</surname><given-names>C</given-names></name>, <name><surname>Vullo</surname><given-names>A</given-names></name>, <name><surname>Walsh</surname><given-names>I</given-names></name>, <name><surname>Pollastri</surname><given-names>G</given-names></name>. <article-title>Distill: a suite of web servers for the prediction of one-, two-and three-dimensional structural features of proteins</article-title>. <source>BMC Bioinformatics</source>. <year>2006</year>;<volume>7</volume>(<issue>1</issue>):<fpage>402</fpage><pub-id pub-id-type="doi">10.1186/1471-2105-7-402</pub-id><?supplied-pmid 16953874?><pub-id pub-id-type="pmid">16953874</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref037">
      <label>37</label>
      <mixed-citation publication-type="journal"><name><surname>Tegge</surname><given-names>AN</given-names></name>, <name><surname>Wang</surname><given-names>Z</given-names></name>, <name><surname>Eickholt</surname><given-names>J</given-names></name>, <name><surname>Cheng</surname><given-names>J</given-names></name>. <article-title>NNcon: improved protein contact map prediction using 2D-recursive neural networks</article-title>. <source>Nucleic acids research</source>. <year>2009</year>;<volume>37</volume>(<issue>suppl_2</issue>):<fpage>W515</fpage>–<lpage>W518</lpage>. <pub-id pub-id-type="doi">10.1093/nar/gkp305</pub-id><?supplied-pmid 19420062?><pub-id pub-id-type="pmid">19420062</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref038">
      <label>38</label>
      <mixed-citation publication-type="journal"><name><surname>Pollastri</surname><given-names>G</given-names></name>, <name><surname>Baldi</surname><given-names>P</given-names></name>. <article-title>Prediction of contact maps by GIOHMMs and recurrent neural networks using lateral propagation from all four cardinal corners</article-title>. <source>Bioinformatics</source>. <year>2002</year>;<volume>18</volume>(<issue>suppl_1</issue>):<fpage>S62</fpage>–<lpage>S70</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/18.suppl_1.s62</pub-id><?supplied-pmid 12169532?><pub-id pub-id-type="pmid">12169532</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref039">
      <label>39</label>
      <mixed-citation publication-type="journal"><name><surname>Morcos</surname><given-names>Faruck</given-names></name>, <name><surname>Pagnani</surname><given-names>Andrea</given-names></name>, <name><surname>Lunt</surname><given-names>Bryan</given-names></name>, <name><surname>Bertolino</surname><given-names>Arianna</given-names></name>, <name><surname>Marks Debora</surname><given-names>S</given-names></name>, <name><surname>Sander</surname><given-names>Chris</given-names></name>, <etal>et al</etal><article-title>Direct-coupling analysis of residue coevolution captures native contacts across many protein families</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2011</year>;<volume>108</volume>(<issue>49</issue>):<fpage>E1293</fpage>–<lpage>301</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1111471108</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref040">
      <label>40</label>
      <mixed-citation publication-type="journal"><name><surname>Jones</surname><given-names>DT</given-names></name>, <name><surname>Buchan</surname><given-names>DW</given-names></name>, <name><surname>Cozzetto</surname><given-names>D</given-names></name>, <name><surname>Pontil</surname><given-names>M</given-names></name>. <article-title>PSICOV: precise structural contact prediction using sparse inverse covariance estimation on large multiple sequence alignments</article-title>. <source>Bioinformatics</source>. <year>2011</year>;<volume>28</volume>(<issue>2</issue>):<fpage>184</fpage>–<lpage>190</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btr638</pub-id><?supplied-pmid 22101153?><pub-id pub-id-type="pmid">22101153</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref041">
      <label>41</label>
      <mixed-citation publication-type="journal"><name><surname>Ekeberg</surname><given-names>M</given-names></name>, <name><surname>Hartonen</surname><given-names>T</given-names></name>, <name><surname>Aurell</surname><given-names>E</given-names></name>. <article-title>Fast pseudolikelihood maximization for direct-coupling analysis of protein structure from many homologous amino-acid sequences</article-title>. <source>Journal of Computational Physics</source>. <year>2014</year>;<volume>276</volume>:<fpage>341</fpage>–<lpage>356</lpage>. <pub-id pub-id-type="doi">10.1016/j.jcp.2014.07.024</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref042">
      <label>42</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>S</given-names></name>, <name><surname>Sun</surname><given-names>S</given-names></name>, <name><surname>Li</surname><given-names>Z</given-names></name>, <name><surname>Zhang</surname><given-names>R</given-names></name>, <name><surname>Xu</surname><given-names>J</given-names></name>. <article-title>Accurate de novo prediction of protein contact map by ultra-deep learning model</article-title>. <source>PLoS Computational Biology</source>. <year>2017</year>;<volume>13</volume>(<issue>1</issue>):<fpage>e1005324</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005324</pub-id><?supplied-pmid 28056090?><pub-id pub-id-type="pmid">28056090</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref043">
      <label>43</label>
      <mixed-citation publication-type="journal"><name><surname>Adhikari</surname><given-names>B</given-names></name>, <name><surname>Hou</surname><given-names>J</given-names></name>, <name><surname>Cheng</surname><given-names>J</given-names></name>. <article-title>DNCON2: Improved protein contact prediction using two-level deep convolutional neural networks</article-title>. <source>Bioinformatics</source>. <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="pone.0220182.ref044">
      <label>44</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>S</given-names></name>, <name><surname>Sun</surname><given-names>S</given-names></name>, <name><surname>Xu</surname><given-names>J</given-names></name>. <article-title>Analysis of deep learning methods for blind protein contact prediction in CASP12</article-title>. <source>Proteins: Structure, Function, and Bioinformatics</source>. <year>2018</year>;<volume>86</volume>:<fpage>67</fpage>–<lpage>77</lpage>. <pub-id pub-id-type="doi">10.1002/prot.25377</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref045">
      <label>45</label>
      <mixed-citation publication-type="journal"><name><surname>Buchan</surname><given-names>DWA</given-names></name>, <name><surname>Jones</surname><given-names>DT</given-names></name>. <article-title>Improved protein contact predictions with the MetaPSICOV2 server in CASP12</article-title>. <source>Proteins</source>. <year>2018</year>;<volume>86</volume><issue>Suppl 1</issue>:<fpage>78</fpage>–<lpage>83</lpage>. <pub-id pub-id-type="doi">10.1002/prot.25379</pub-id><?supplied-pmid 28901583?><pub-id pub-id-type="pmid">28901583</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref046">
      <label>46</label>
      <mixed-citation publication-type="journal"><name><surname>LeCun</surname><given-names>Y</given-names></name>, <name><surname>Bengio</surname><given-names>Y</given-names></name>, <name><surname>Hinton</surname><given-names>G</given-names></name>. <article-title>Deep learning</article-title>. <source>Nature</source>. <year>2015</year>;<volume>521</volume>(<issue>7553</issue>):<fpage>436</fpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><?supplied-pmid 26017442?><pub-id pub-id-type="pmid">26017442</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref047">
      <label>47</label>
      <mixed-citation publication-type="other">Mikolov T, Chen K, Corrado G, Dean J. Efficient estimation of word representations in vector space. arXiv preprint arXiv:13013781. 2013.</mixed-citation>
    </ref>
    <ref id="pone.0220182.ref048">
      <label>48</label>
      <mixed-citation publication-type="journal"><name><surname>Asgari</surname><given-names>E</given-names></name>, <name><surname>Mofrad</surname><given-names>MR</given-names></name>. <article-title>Continuous distributed representation of biological sequences for deep proteomics and genomics</article-title>. <source>PloS One</source>. <year>2015</year>;<volume>10</volume>(<issue>11</issue>):<fpage>e0141287</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0141287</pub-id><?supplied-pmid 26555596?><pub-id pub-id-type="pmid">26555596</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref049">
      <label>49</label>
      <mixed-citation publication-type="journal"><name><surname>Chou</surname><given-names>PY</given-names></name>, <name><surname>Fasman</surname><given-names>GD</given-names></name>. <article-title>Conformational parameters for amino acids in helical, <italic>β</italic>-sheet, and random coil regions calculated from proteins</article-title>. <source>Biochemistry</source>. <year>1974</year>;<volume>13</volume>(<issue>2</issue>):<fpage>211</fpage>–<lpage>222</lpage>. <pub-id pub-id-type="doi">10.1021/bi00699a001</pub-id><?supplied-pmid 4358939?><pub-id pub-id-type="pmid">4358939</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref050">
      <label>50</label>
      <mixed-citation publication-type="other">Chollet F, et al. Keras; 2015. <ext-link ext-link-type="uri" xlink:href="https://github.com/fchollet/keras">https://github.com/fchollet/keras</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0220182.ref051">
      <label>51</label>
      <mixed-citation publication-type="other">Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, et al. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems; 2015. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org/">https://www.tensorflow.org/</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0220182.ref052">
      <label>52</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>G</given-names></name>, <name><surname>Dunbrack</surname><given-names>RL</given-names><suffix>Jr</suffix></name>. <article-title>PISCES: a protein sequence culling server</article-title>. <source>Bioinformatics</source>. <year>2003</year>;<volume>19</volume>(<issue>12</issue>):<fpage>1589</fpage>–<lpage>1591</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btg224</pub-id><?supplied-pmid 12912846?><pub-id pub-id-type="pmid">12912846</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref053">
      <label>53</label>
      <mixed-citation publication-type="journal"><name><surname>Fang</surname><given-names>C</given-names></name>, <name><surname>Shang</surname><given-names>Y</given-names></name>, <name><surname>Xu</surname><given-names>D</given-names></name>. <article-title>MUFOLD-SS: New deep inception-inside-inception networks for protein secondary structure prediction</article-title>. <source>Proteins: Structure, Function, and Bioinformatics</source>. <year>2018</year>;<volume>86</volume>(<issue>5</issue>):<fpage>592</fpage>–<lpage>598</lpage>. <pub-id pub-id-type="doi">10.1002/prot.25487</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref054">
      <label>54</label>
      <mixed-citation publication-type="other">Torrisi M, Kaleel M, Pollastri G. Porter 5: fast, state-of-the-art ab initio prediction of protein secondary structure in 3 and 8 classes. bioRxiv. 2018; p. 289033.</mixed-citation>
    </ref>
    <ref id="pone.0220182.ref055">
      <label>55</label>
      <mixed-citation publication-type="book"><name><surname>Yang</surname><given-names>Y</given-names></name>, <name><surname>Heffernan</surname><given-names>R</given-names></name>, <name><surname>Paliwal</surname><given-names>K</given-names></name>, <name><surname>Lyons</surname><given-names>J</given-names></name>, <name><surname>Dehzangi</surname><given-names>A</given-names></name>, <name><surname>Sharma</surname><given-names>A</given-names></name>, <etal>et al</etal><chapter-title>Spider2: A package to predict secondary structure, accessible surface area, and main-chain torsional angles by deep neural networks</chapter-title> In: <source>Prediction of Protein Secondary Structure</source>. <publisher-name>Springer</publisher-name>; <year>2017</year> p. <fpage>55</fpage>–<lpage>63</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0220182.ref056">
      <label>56</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Y</given-names></name>, <name><surname>Mao</surname><given-names>H</given-names></name>, <name><surname>Yi</surname><given-names>Z</given-names></name>. <article-title>Protein secondary structure prediction by using deep learning method</article-title>. <source>Knowledge-Based Systems</source>. <year>2017</year>;<volume>118</volume>:<fpage>115</fpage>–<lpage>123</lpage>. <pub-id pub-id-type="doi">10.1016/j.knosys.2016.11.015</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref057">
      <label>57</label>
      <mixed-citation publication-type="journal"><name><surname>Söding</surname><given-names>J</given-names></name>, <name><surname>Remmert</surname><given-names>M</given-names></name>. <article-title>Protein sequence comparison and fold recognition: progress and good-practice benchmarking</article-title>. <source>Current Opinion in Structural Biology</source>. <year>2011</year>;<volume>21</volume>(<issue>3</issue>):<fpage>404</fpage>–<lpage>411</lpage>. <pub-id pub-id-type="doi">10.1016/j.sbi.2011.03.005</pub-id><?supplied-pmid 21458982?><pub-id pub-id-type="pmid">21458982</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref058">
      <label>58</label>
      <mixed-citation publication-type="journal"><name><surname>Cheng</surname><given-names>H</given-names></name>, <name><surname>Schaeffer</surname><given-names>RD</given-names></name>, <name><surname>Liao</surname><given-names>Y</given-names></name>, <name><surname>Kinch</surname><given-names>LN</given-names></name>, <name><surname>Pei</surname><given-names>J</given-names></name>, <name><surname>Shi</surname><given-names>S</given-names></name>, <etal>et al</etal><article-title>ECOD: an evolutionary classification of protein domains</article-title>. <source>PLoS Computational Biology</source>. <year>2014</year>;<volume>10</volume>(<issue>12</issue>):<fpage>e1003926</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003926</pub-id><?supplied-pmid 25474468?><pub-id pub-id-type="pmid">25474468</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref059">
      <label>59</label>
      <mixed-citation publication-type="journal"><name><surname>Fox</surname><given-names>NK</given-names></name>, <name><surname>Brenner</surname><given-names>SE</given-names></name>, <name><surname>Chandonia</surname><given-names>JM</given-names></name>. <article-title>SCOPe: Structural Classification of Proteins—extended, integrating SCOP and ASTRAL data and classification of new structures</article-title>. <source>Nucleic acids research</source>. <year>2013</year>;<volume>42</volume>(<issue>D1</issue>):<fpage>D304</fpage>–<lpage>D309</lpage>. <pub-id pub-id-type="doi">10.1093/nar/gkt1240</pub-id><?supplied-pmid 24304899?><pub-id pub-id-type="pmid">24304899</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref060">
      <label>60</label>
      <mixed-citation publication-type="journal"><name><surname>Remmert</surname><given-names>M</given-names></name>, <name><surname>Biegert</surname><given-names>A</given-names></name>, <name><surname>Hauser</surname><given-names>A</given-names></name>, <name><surname>Söding</surname><given-names>J</given-names></name>. <article-title>HHblits: lightning-fast iterative protein sequence searching by HMM-HMM alignment</article-title>. <source>Nature Methods</source>. <year>2012</year>;<volume>9</volume>(<issue>2</issue>):<fpage>173</fpage>–<lpage>175</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.1818</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref061">
      <label>61</label>
      <mixed-citation publication-type="journal"><name><surname>Johnson</surname><given-names>L Steven</given-names></name>, <name><surname>Eddy</surname><given-names>Sean R</given-names></name>, <name><surname>Portugaly</surname><given-names>Elon</given-names></name>. <article-title>Hidden Markov model speed heuristic and iterative HMM search procedure</article-title>. <source>BMC Bioinformatics</source>. <year>2010</year>;<volume>11</volume>(<issue>1</issue>):<fpage>431</fpage><pub-id pub-id-type="doi">10.1186/1471-2105-11-431</pub-id><?supplied-pmid 20718988?><pub-id pub-id-type="pmid">20718988</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref062">
      <label>62</label>
      <mixed-citation publication-type="journal"><name><surname>Schaarschmidt</surname><given-names>J</given-names></name>, <name><surname>Monastyrskyy</surname><given-names>B</given-names></name>, <name><surname>Kryshtafovych</surname><given-names>A</given-names></name>, <name><surname>Bonvin</surname><given-names>AM</given-names></name>. <article-title>Assessment of contact predictions in CASP12: Co-evolution and deep learning coming of age</article-title>. <source>Proteins: Structure, Function, and Bioinformatics</source>. <year>2018</year>;<volume>86</volume>:<fpage>51</fpage>–<lpage>66</lpage>. <pub-id pub-id-type="doi">10.1002/prot.25407</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref063">
      <label>63</label>
      <mixed-citation publication-type="journal"><name><surname>Joosten</surname><given-names>RP</given-names></name>, <name><surname>Te Beek</surname><given-names>TA</given-names></name>, <name><surname>Krieger</surname><given-names>E</given-names></name>, <name><surname>Hekkelman</surname><given-names>ML</given-names></name>, <name><surname>Hooft</surname><given-names>RW</given-names></name>, <name><surname>Schneider</surname><given-names>R</given-names></name>, <etal>et al</etal><article-title>A series of PDB related databases for everyday needs</article-title>. <source>Nucleic acids research</source>. <year>2010</year>;<volume>39</volume>(<issue>suppl_1</issue>):<fpage>D411</fpage>–<lpage>D419</lpage>. <pub-id pub-id-type="doi">10.1093/nar/gkq1105</pub-id><?supplied-pmid 21071423?><pub-id pub-id-type="pmid">21071423</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref064">
      <label>64</label>
      <mixed-citation publication-type="journal"><name><surname>Tien</surname><given-names>MZ</given-names></name>, <name><surname>Meyer</surname><given-names>AG</given-names></name>, <name><surname>Sydykova</surname><given-names>DK</given-names></name>, <name><surname>Spielman</surname><given-names>SJ</given-names></name>, <name><surname>Wilke</surname><given-names>CO</given-names></name>. <article-title>Maximum allowed solvent accessibilites of residues in proteins</article-title>. <source>PloS one</source>. <year>2013</year>;<volume>8</volume>(<issue>11</issue>):<fpage>e80635</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0080635</pub-id><?supplied-pmid 24278298?><pub-id pub-id-type="pmid">24278298</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref065">
      <label>65</label>
      <mixed-citation publication-type="journal"><name><surname>Naftaly</surname><given-names>U</given-names></name>, <name><surname>Intrator</surname><given-names>N</given-names></name>, <name><surname>Horn</surname><given-names>D</given-names></name>. <article-title>Optimal ensemble averaging of neural networks</article-title>. <source>Network: Computation in Neural Systems</source>. <year>1997</year>;<volume>8</volume>(<issue>3</issue>):<fpage>283</fpage>–<lpage>296</lpage>. <pub-id pub-id-type="doi">10.1088/0954-898X_8_3_004</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref066">
      <label>66</label>
      <mixed-citation publication-type="journal"><name><surname>Yang</surname><given-names>Y</given-names></name>, <name><surname>Gao</surname><given-names>J</given-names></name>, <name><surname>Wang</surname><given-names>J</given-names></name>, <name><surname>Heffernan</surname><given-names>R</given-names></name>, <name><surname>Hanson</surname><given-names>J</given-names></name>, <name><surname>Paliwal</surname><given-names>K</given-names></name>, <etal>et al</etal><article-title>Sixty-five years of the long march in protein secondary structure prediction: the final stretch?</article-title><source>Briefings in bioinformatics</source>. <year>2016</year>;<volume>19</volume>(<issue>3</issue>):<fpage>482</fpage>–<lpage>494</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0220182.ref067">
      <label>67</label>
      <mixed-citation publication-type="journal"><name><surname>Steinegger</surname><given-names>M</given-names></name>, <name><surname>Söding</surname><given-names>J</given-names></name>. <article-title>Clustering huge protein sequence sets in linear time</article-title>. <source>Nature communications</source>. <year>2018</year>;<volume>9</volume>(<issue>1</issue>):<fpage>2542</fpage><pub-id pub-id-type="doi">10.1038/s41467-018-04964-5</pub-id><?supplied-pmid 29959318?><pub-id pub-id-type="pmid">29959318</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0220182.ref068">
      <label>68</label>
      <mixed-citation publication-type="other">CASP. CASP13 Webpage; 2018. <ext-link ext-link-type="uri" xlink:href="http://predictioncenter.org/casp13">http://predictioncenter.org/casp13</ext-link>.</mixed-citation>
    </ref>
  </ref-list>
</back>
