<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Genet</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Genet</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Genet.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Genetics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1664-8021</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9745317</article-id>
    <article-id pub-id-type="publisher-id">1067562</article-id>
    <article-id pub-id-type="doi">10.3389/fgene.2022.1067562</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Genetics</subject>
        <subj-group>
          <subject>Original Research</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>TSSNote-CyaPromBERT: Development of an integrated platform for highly accurate promoter prediction and visualization of <italic>Synechococcus</italic> sp. and <italic>Synechocystis</italic> sp. through a state-of-the-art natural language processing model BERT</article-title>
      <alt-title alt-title-type="left-running-head">Mai et al.</alt-title>
      <alt-title alt-title-type="right-running-head">
        <ext-link xlink:href="https://doi.org/10.3389/fgene.2022.1067562" ext-link-type="uri">10.3389/fgene.2022.1067562</ext-link>
      </alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Mai</surname>
          <given-names>Dung Hoang Anh</given-names>
        </name>
        <uri xlink:href="https://loop.frontiersin.org/people/2050059/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Nguyen</surname>
          <given-names>Linh Thanh</given-names>
        </name>
        <uri xlink:href="https://loop.frontiersin.org/people/2049559/overview"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Lee</surname>
          <given-names>Eun Yeol</given-names>
        </name>
        <xref rid="c001" ref-type="corresp">*</xref>
        <uri xlink:href="https://loop.frontiersin.org/people/925889/overview"/>
      </contrib>
    </contrib-group>
    <aff><institution>Department of Chemical Engineering (BK21 FOUR Integrated Engineering Program)</institution>, <institution>Kyung Hee University</institution>, <addr-line>Yongin-si</addr-line>, <country>South Korea</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p><bold>Edited by:</bold><ext-link xlink:href="https://loop.frontiersin.org/people/531759/overview" ext-link-type="uri">Quan Zou</ext-link>, University of Electronic Science and Technology of China, China</p>
      </fn>
      <fn fn-type="edited-by">
        <p><bold>Reviewed by:</bold><ext-link xlink:href="https://loop.frontiersin.org/people/2011119/overview" ext-link-type="uri">Wang Xi</ext-link>, Harvard University, United States</p>
        <p><ext-link xlink:href="https://loop.frontiersin.org/people/546637/overview" ext-link-type="uri">Qiong Zhang</ext-link>, Affiliated Hospital of Nantong University, China</p>
      </fn>
      <corresp id="c001">*Correspondence: Eun Yeol Lee, <email>eunylee@khu.ac.kr</email>
</corresp>
      <fn fn-type="other">
        <p>This article was submitted to Computational Genomics, a section of the journal Frontiers in Genetics</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>29</day>
      <month>11</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>13</volume>
    <elocation-id>1067562</elocation-id>
    <history>
      <date date-type="received">
        <day>12</day>
        <month>10</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>17</day>
        <month>11</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2022 Mai, Nguyen and Lee.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Mai, Nguyen and Lee</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Since the introduction of the first transformer model with a unique self-attention mechanism, natural language processing (NLP) models have attained state-of-the-art (SOTA) performance on various tasks. As DNA is the blueprint of life, it can be viewed as an unusual language, with its characteristic lexicon and grammar. Therefore, NLP models may provide insights into the meaning of the sequential structure of DNA. In the current study, we employed and compared the performance of popular SOTA NLP models (i.e., XLNET, BERT, and a variant DNABERT trained on the human genome) to predict and analyze the promoters in freshwater cyanobacterium <italic>Synechocystis</italic> sp. PCC 6803 and the fastest growing cyanobacterium <italic>Synechococcus elongatus</italic> sp. UTEX 2973. These freshwater cyanobacteria are promising hosts for phototrophically producing value-added compounds from CO<sub>2</sub>. Through a custom pipeline, promoters and non-promoters from <italic>Synechococcus elongatus</italic> sp. UTEX 2973 were used to train the model. The trained model achieved an AUROC score of 0.97 and F1 score of 0.92. During cross-validation with promoters from <italic>Synechocystis</italic> sp. PCC 6803, the model achieved an AUROC score of 0.96 and F1 score of 0.91. To increase accessibility, we developed an integrated platform (TSSNote-CyaPromBERT) to facilitate large dataset extraction, model training, and promoter prediction from public dRNA-seq datasets. Furthermore, various visualization tools have been incorporated to address the “black box” issue of deep learning and feature analysis. The learning transfer ability of large language models may help identify and analyze promoter regions for newly isolated strains with similar lineages.</p>
    </abstract>
    <kwd-group>
      <kwd>deep learning</kwd>
      <kwd>natural language processing</kwd>
      <kwd>transformer</kwd>
      <kwd>promoter prediction</kwd>
      <kwd>dRNA-Seq</kwd>
      <kwd>differential RNA sequencing</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn001">
          <institution-wrap>
            <institution>National Research Foundation of Korea
</institution>
            <institution-id institution-id-type="doi">10.13039/501100003725</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id award-type="contract" rid="cn001">2015M3D3A1A01064882</award-id>
      </award-group>
    </funding-group>
  </article-meta>
</front>
<body>
  <sec id="s1">
    <title>Introduction</title>
    <p>A classic problem in bioinformatics is the challenge of predicting promoters (<xref rid="B59" ref-type="bibr">Zhang et al., 2022</xref>). Promoter regions are DNA regions where RNA polymerase binds to initiate the transcription process, the first step in the central dogma of molecular biology (<xref rid="B6" ref-type="bibr">Butler and Kadonaga, 2002</xref>). Owing to their essential role in regulating and determining the timing and expression levels of genes needed for vital functions, the prediction and in-depth functional analysis of promoters have been of interest to biologists. Previously, owing to the complexity of cis-regulation networks and lack of data, attempts at developing promoter prediction tools were inadequate (<xref rid="B3" ref-type="bibr">Bhandari et al., 2021</xref>). However, recent advancements in machine learning and deep learning have successfully leveraged genomic data. To date, many groups have successfully constructed promoter prediction tools using traditional machine learning methods, knowledge-based position matrix weight (<xref rid="B19" ref-type="bibr">Huerta and Collado-Vides, 2003</xref>; <xref rid="B5" ref-type="bibr">Burden et al., 2005</xref>; <xref rid="B41" ref-type="bibr">Rangannan and Bansal, 2010</xref>; <xref rid="B11" ref-type="bibr">Di Salvo et al., 2018</xref>) through support vector machines, and artificial neural networks for this logistic regression task (<xref rid="B17" ref-type="bibr">Gordon et al., 2003</xref>; <xref rid="B8" ref-type="bibr">da Silva et al., 2006</xref>; <xref rid="B32" ref-type="bibr">Mann et al., 2007</xref>; <xref rid="B50" ref-type="bibr">Towsey et al., 2008</xref>; <xref rid="B18" ref-type="bibr">He et al., 2018</xref>; <xref rid="B30" ref-type="bibr">Liu et al., 2018</xref>; <xref rid="B40" ref-type="bibr">Rahman et al., 2019</xref>; <xref rid="B56" ref-type="bibr">Xiao et al., 2019</xref>; <xref rid="B60" ref-type="bibr">Zhang et al., 2019</xref>; <xref rid="B28" ref-type="bibr">Li et al., 2021</xref>). Convolutional neural networks (CNN) and recurrent neural network (RNN)-based architectures (long short-term memory, gated recurrent units) have recently become the most popular choices for promoter classification (<xref rid="B34" ref-type="bibr">Nguyen et al., 2016</xref>; <xref rid="B27" ref-type="bibr">Le et al., 2019</xref>; <xref rid="B35" ref-type="bibr">Oubounyt et al., 2019</xref>; <xref rid="B1" ref-type="bibr">Amin et al., 2020</xref>; <xref rid="B62" ref-type="bibr">Zhu et al., 2021</xref>). CNN-based models depend on predetermined kernel size designs to extract and generalize local features; therefore, they might fail to capture long-range contexts. To overcome this limitation, some research groups have integrated RNN-based models to retrieve long-term dependencies. By design, LTSM computations from RNNs are processed sequentially and depend on the outputs of the previous hidden states for the next state to maintain the sentence structure and context; however, this, in turn, leads to the vanishing gradient problem. These limitations pose difficulties and may restrict the scalability and flexibility of constructed models when applied to other species.</p>
    <p>Since its first appearance in 2017, the transformer architecture, with its unique self-attention mechanism, has revolutionized the natural language processing (NLP) field and achieved SOTA performance in various machine learning tasks (<xref rid="B52" ref-type="bibr">Vaswani et al., 2017</xref>). As these transformers perform well, they have made their way to other branches (e.g., computer vision) (<xref rid="B55" ref-type="bibr">Wu et al., 2020</xref>; <xref rid="B2" ref-type="bibr">Arnab et al., 2021</xref>; <xref rid="B61" ref-type="bibr">Zhou et al., 2021</xref>) that were previously dominated by CNNs, and they are now also used in multimodal learning for content generation (<xref rid="B51" ref-type="bibr">Tsai et al., 2019</xref>; <xref rid="B58" ref-type="bibr">Yu et al., 2019</xref>; <xref rid="B14" ref-type="bibr">Dzabraev et al., 2021</xref>). Transformer-based models are versatile and can be incorporated into different architectures owing to their robustness and flexibility through their learning-transfer capability. Considering the sequential nature of DNA, which can be regarded as a natural language with unique grammar and lexicon, transformer-based models are particularly well suited for supervised classification tasks.</p>
    <p>Therefore, adopting a different approach in the current study, we employed and compared transformer-based models for the promoter prediction problem. To date, most of the currently constructed models have been designed for popular species with curated regulatory databases such as humans, fruit flies, mice, <italic>Escherichia coli</italic>, and yeasts (<xref rid="B35" ref-type="bibr">Oubounyt et al., 2019</xref>; <xref rid="B39" ref-type="bibr">Rahman M et al., 2019</xref>; <xref rid="B28" ref-type="bibr">Li et al., 2021</xref>). However, there is still considerable interest in integrating deep-learning techniques for promoter analysis in other (less popular) species. For example, cyanobacteria are an ancient and diverse group of photo-oxygenic prokaryotes with ample potential for the photosynthetic production of value-added chemical compounds from the greenhouse gas CO<sub>2</sub>. Many cyanobacterial species with a high potential for valorizing CO<sub>2</sub> are still being isolated and characterized every year. Some of the most notable genera were <italic>Synechocystis</italic> and <italic>Synechococcus</italic>. These model organisms can convert CO<sub>2</sub> into various useful products (<xref rid="B31" ref-type="bibr">Luan et al., 2019</xref>; <xref rid="B44" ref-type="bibr">Sarnaik et al., 2019</xref>; <xref rid="B29" ref-type="bibr">Lin et al., 2020</xref>; <xref rid="B37" ref-type="bibr">Pattharaprachayakul et al., 2020</xref>; <xref rid="B38" ref-type="bibr">Qiao et al., 2020</xref>; <xref rid="B49" ref-type="bibr">Taylor and Heap, 2020</xref>; <xref rid="B24" ref-type="bibr">Kato and Hasunuma, 2021</xref>; <xref rid="B42" ref-type="bibr">Roh et al., 2021</xref>; <xref rid="B43" ref-type="bibr">Santos-Merino et al., 2021</xref>). Although they have been characterized and researched for a few decades, the application of deep learning for promoter prediction specifically in cyanobacteria is still lacking. Therefore, in this study, we used the promoters of <italic>Synechococcus elongatus</italic> sp. UTEX 2973, the fastest growing cyanobacterium for model training and testing (<xref rid="B45" ref-type="bibr">Song et al., 2016</xref>; <xref rid="B33" ref-type="bibr">Mueller et al., 2017</xref>). We further conducted cross-validation of the promoters of the model organism <italic>Synechocystis</italic> sp. PCC 6803 to test whether the models also work on related species (<xref rid="B20" ref-type="bibr">Ikeuchi and Tabata 2001</xref>). Combined with knowledge-based analysis, in-depth model characterization may help tackle the “black box” problem of deep-learning models.</p>
    <p>To facilitate the development and incorporation of SOTA transformer-based promoter prediction tools, we reconstructed a pipeline (using TSSNote and PromBERT Google Colab notebooks) to compute and extract the promoters of public differential RNA-seq (dRNA-seq) datasets from the National Center for Biotechnology Information Sequence Read Archive (NCBI SRA) database and used them for model training. dRNA-seq is an RNA sequencing technique that allows the determination of TSS at 1 bp resolution by enriching primary transcripts (<xref rid="B4" ref-type="bibr">Bischler et al., 2015</xref>). In contrast to conventional differential expression RNA-seq (RNA-seq), dRNA-seq requires additional treatments and more expensive and complex procedures, making these datasets rather limited. Transfer learning is a core advantage of large-parameter language models. We expect that, with fine-tuning, transformer-based promoter models can be good approximators for other related species. To improve the accessibility to researchers with and without expertise in machine learning, separate modules of the pipeline for promoter extraction, model training, promoter prediction, and visualization were ported into the cloud-based platform Google Colab. We demonstrated that, even without the advantage of the pre-training phase, transformer-based models, such as bidirectional encoder representations from transformers (BERT) and XLNET, are capable of highly accurate promoter prediction for <italic>Synechocystis</italic> and <italic>Synechococcus</italic> species solely through a context-wise self-attention mechanism (<xref rid="B10" ref-type="bibr">Devlin et al., 2018</xref>; <xref rid="B57" ref-type="bibr">Yang et al., 2019</xref>).</p>
  </sec>
  <sec sec-type="materials|methods" id="s2">
    <title>Materials and methods</title>
    <sec id="s2-1">
      <title>Datasets</title>
      <p>Raw dRNA-seq datasets for <italic>Synechocystis</italic> sp. PCC 6803 and <italic>Synechococcus elongatus</italic> sp. UTEX 2973 and for <italic>Synechocystis</italic> sp. PCC 6714 were downloaded from the NCBI SRA database, and genomic DNA sequence assemblies were downloaded from the NCBI RefSeq database (<xref rid="T1" ref-type="table">Table 1</xref>).</p>
      <table-wrap position="float" id="T1">
        <label>TABLE 1</label>
        <caption>
          <p>Datasets employed in this study.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead valign="top">
            <tr>
              <th align="left" rowspan="1" colspan="1">Species</th>
              <th align="left" rowspan="1" colspan="1">SRA accession number</th>
              <th align="left" rowspan="1" colspan="1">Condition</th>
              <th align="left" rowspan="1" colspan="1">TEX treatment</th>
            </tr>
          </thead>
          <tbody valign="top">
            <tr>
              <td rowspan="2" align="left" colspan="1"><italic>Synechococcus elongatus</italic> sp. UTEX2973</td>
              <td align="left" rowspan="1" colspan="1">SRR6334749, SRR6334750</td>
              <td align="left" rowspan="1" colspan="1">Primary transcripts under normal condition</td>
              <td align="left" rowspan="1" colspan="1">TEX (+)</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">SRR6334747, SRR6334748</td>
              <td align="left" rowspan="1" colspan="1">Control under normal condition</td>
              <td align="left" rowspan="1" colspan="1">TEX (-)</td>
            </tr>
            <tr>
              <td rowspan="2" align="left" colspan="1"><italic>Synechocystis</italic> sp. PCC 6803</td>
              <td align="left" rowspan="1" colspan="1">SRR1019366, SRR1019365</td>
              <td align="left" rowspan="1" colspan="1">Primary transcripts under exponential and stationary phase</td>
              <td align="left" rowspan="1" colspan="1">TEX (+)</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">SRR1019368, SRR1019367</td>
              <td align="left" rowspan="1" colspan="1">Secondary reads from 10 different conditions</td>
              <td align="left" rowspan="1" colspan="1">TEX (-)</td>
            </tr>
            <tr>
              <td rowspan="2" align="left" colspan="1"><italic>Synechocystis</italic> sp. PCC 6714</td>
              <td align="left" rowspan="1" colspan="1">SRR1019241</td>
              <td align="left" rowspan="1" colspan="1">Primary reads from stationary phase</td>
              <td align="left" rowspan="1" colspan="1">TEX (+)</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">SRR1019242</td>
              <td align="left" rowspan="1" colspan="1">Secondary reads from 10 different conditions</td>
              <td align="left" rowspan="1" colspan="1">TEX (-)</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>Independent <italic>E. coli</italic> promoter datasets for benchmarking were obtained from <ext-link xlink:href="https://github.com/chenli-bioinfo/promoter" ext-link-type="uri">https://github.com/chenli-bioinfo/promoter</ext-link>.</p>
      <p>Available data and local and Google Colab versions of TSSNote-CyaPromBERT are available at <ext-link xlink:href="https://github.com/hanepira/TSSnote-CyaPromBert" ext-link-type="uri">https://github.com/hanepira/TSSnote-CyaPromBert</ext-link>.</p>
    </sec>
    <sec id="s2-2">
      <title>Constructing promoter extracting module from dRNA-seq datasets</title>
      <p>Because one of the objectives of the current work is to create a cloud-computing-based pipeline that can be applied without strong hardware requirements, we implemented algorithms in a Colab notebook for TSS prediction based on changes in read coverage, in a similar manner to TSSpredator (<xref rid="B13" ref-type="bibr">Dugar et al., 2013</xref>) but with more flexibility for customizations. This promoter extracting module (TSSNote) takes SRA ids for TEX (+) and TEX (-) treatments and fasta from NCBI as inputs and conducts alignment by HISAT2 and read coverage extraction through SAMTools. HISAT2 enables soft-clipping alignment, through which adapters do not interfere with the read alignment. SAMTools are then used to extract read coverage from the plus and minus strands for later computations. The read coverage files from both TEX (+) enrichment and TEX (-) were used to locate and compute the potential TSSs enriched by TEX treatment. Because the quality of dRNAseq datasets is dependent on experimental procedures, after calculating potential TSSs, users can filter TSSs based on the read coverage cut-off or coverage change cut-off. BAM files can be downloaded into local drives for manual observation and curation using NGS genome browsers. The overall design is illustrated in <xref rid="F1" ref-type="fig">Figure 1</xref>, and the detailed workflow of the TSSNote is shown in <xref rid="F2" ref-type="fig">Figure 2</xref>.</p>
      <fig position="float" id="F1">
        <label>FIGURE 1</label>
        <caption>
          <p>Overall scheme for constructing and developing TSSNote and CyaPromBERT. TSSNote facilitates downloading raw dRNA-seq datasets from NCBI SRA database and conducts alignment, sorting, and filtering for extracting promoters and non-promoters. These sequences are later used to train a BERT model for the task of promoter prediction. Randomly generated DNA sequences with similar size to promoter length are added to reduce biases, and overfitting is used to improve the model’s robustness. The trained model is capable of promoter prediction, regional scanning, and visualization at base-pair level.</p>
        </caption>
        <graphic xlink:href="fgene-13-1067562-g001" position="float"/>
      </fig>
      <fig position="float" id="F2">
        <label>FIGURE 2</label>
        <caption>
          <p>Detailed flowchart of TSSNote operation to extract TSSs, promoters, and non-promoter sequences.</p>
        </caption>
        <graphic xlink:href="fgene-13-1067562-g002" position="float"/>
      </fig>
      <p>Read coverage change at a specific location is calculated by the following function:<disp-formula id="equ1"><mml:math id="m1" overflow="scroll"><mml:mrow><mml:mo>△</mml:mo><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>Where: <inline-formula id="inf1"><mml:math id="m2" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> = coverage depth at position i <inline-formula id="inf2"><mml:math id="m3" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> = coverage depth at position <inline-formula id="inf3"><mml:math id="m4" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>
<inline-formula id="inf4"><mml:math id="m5" overflow="scroll"><mml:mrow><mml:mo>∆</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> = change factor from xi to <inline-formula id="inf5"><mml:math id="m6" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>
<inline-formula id="inf6"><mml:math id="m7" overflow="scroll"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:math></inline-formula> = calibration constant to prevent division zero (0.01).</p>
    </sec>
    <sec id="s2-3">
      <title>Promoter and non-promoter sequences extraction</title>
      <p>Promoters were extracted directly upstream from the predicted TSSs. For promoter sequences, ribosomal RNA depletion in dRNAseq experiments may not be 100%; therefore, further trimming methods were implemented. We tested the TSSs identified by TSSNote based on the wildtype dataset with the TSSs proposed in the original publication (<xref rid="B48" ref-type="bibr">Tan et al., 2018</xref>). Even though the implementation method was different, many of the predicted TSSs were consistent. By setting constraints more stringent, through expression strength and degree of changes, more than 90% of the TSSs identified in the wildtype dataset were also found in the original proposed TSSs concatenated from multiple conditions. Therefore, filtered promoter datasets extracted from strongly expressed and enriched TSSs should be sufficiently reliable. As deep-learning models require a large amount of data for accurate generalization, we believe that the flexibility offered by TSSNote can be crucial. Furthermore, read counts and fold-changes in read coverage can provide more information to group and filter promoters based on promoter strength. It can be used independently or together with existing tools for better analysis. In the current work we lowered the constraints to take into account the potential spurious transcriptional events and weak promoters of other sigma factor groups which would be filtered by the method used in the original publication. The good performance on cross validation and clear pattern enrichment indicate that the model has successfully learned key features from the extracted promoters for promoter recognition task.</p>
      <p>The non-promoter sequences were extracted from the “non-promoter” regions. Specifically, Non-promoter sequences were sampled from the downstream of TSSs. If the distance between two neighboring TSSs is larger than 2 times the sequence length, that interval region is marked and used for sampling non promoter sequences. We further added 10% randomly generated sequences to increase noise and reduce overfitting. The non-promoter sequences then are shuffled, and a portion of the non-promoter sequences was used at the ratio 1:1 promoter–non-promoter for model training.</p>
    </sec>
    <sec id="s2-4">
      <title>Model training</title>
      <p>The TSSs of each species from different datasets was extracted and concatenated for model construction using Python wrapper TSSNote, which was written in Python 3.9 as a user-friendly pipeline to conduct raw data gathering using SRA toolkits 3.0 (<ext-link xlink:href="https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=software" ext-link-type="uri">https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=software</ext-link>) and Entrez-direct (<xref rid="B23" ref-type="bibr">Kans 2022</xref>), sequence indexing, read alignment by HISAT2 (<xref rid="B25" ref-type="bibr">Kim et al., 2019</xref>), strand sorting, and read coverage calculation by SAMtools (<xref rid="B9" ref-type="bibr">Danecek et al., 2021</xref>). Promoter sequences were extracted from the calculated TSSs using the Biopython package (<xref rid="B7" ref-type="bibr">Cock et al., 2009</xref>).</p>
      <p>To construct CyaPromBert and evaluate the performance of different transformer-based models, Pytorch 1.11.0 and Pytorch-lightning 1.6.4 (<xref rid="B36" ref-type="bibr">Paszke et al., 2019</xref>). Transformer-based models were constructed using base models from huggingface’s transformer library 4.18.0 (<xref rid="B54" ref-type="bibr">Wolf et al., 2020</xref>).</p>
      <p>The probability was calculated by the sigmoid function:<disp-formula id="equ2"><mml:math id="m8" overflow="scroll"><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>e</mml:mi><mml:mo>−</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
</p>
      <p>The performance of the models was evaluated by precision, recall, F-1, and AUPRC, AUROC scores.<disp-formula id="equ3"><mml:math id="m9" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
<disp-formula id="equ4"><mml:math id="m10" overflow="scroll"><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
<disp-formula id="equ5"><mml:math id="m11" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>t</mml:mi><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>Where: <inline-formula id="inf7"><mml:math id="m12" overflow="scroll"><mml:mrow><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula> = true positive <inline-formula id="inf8"><mml:math id="m13" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula> = false positive <inline-formula id="inf9"><mml:math id="m14" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula> = false negative</p>
      <p>The area under the precision-recall curve (AUPRC) is calculated from the average precision score and AUROC is the area under the receiver operating characteristics.</p>
      <p>Binary cross entropy was used as the loss function.<disp-formula id="equ6"><mml:math id="m15" overflow="scroll"><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mo>*</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>*</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
</p>
      <p>Attention weight visualization libraries, BERTviz 1.4.0, and Captum 0.5.0, were implemented to improve visualization and interpretability (<xref rid="B53" ref-type="bibr">Vig 2019</xref>; <xref rid="B26" ref-type="bibr">Kokhlikyan et al., 2020</xref>). Both TSSNote and the models were first developed and trained on a local workstation equipped with an NVIDIA RTX 3070 before porting and testing on the Google Colab cloud computing service.</p>
    </sec>
  </sec>
  <sec sec-type="results|discussion" id="s3">
    <title>Results and discussion</title>
    <sec id="s3-1">
      <title>Selecting the best performing SOTA transformer-based model for promoter prediction</title>
      <p>The transformer-based architecture has demonstrated that, with sufficient data, matrix multiplications, linear layers, and layer normalization, the deep-learning model can achieve SOTA machine translation tasks without relying on CNN and RNN (<xref rid="B52" ref-type="bibr">Vaswani et al., 2017</xref>). BERT and XLNET are two of the most popular transformer-based language models (<xref rid="B10" ref-type="bibr">Devlin et al., 2018</xref>; <xref rid="B57" ref-type="bibr">Yang et al., 2019</xref>). Fundamentally, these large-language models are stacks of encoding modules from the original transformer model. However, they are pre-trained differently and use different tokenizers. BERT is an autoencoding-based model, whereas XLNet employs an autoregressive method similar to the famous GPT models from OpenAI (<xref rid="B16" ref-type="bibr">Floridi and Chiriatti 2020</xref>). These differences reflect the capability to capture the semantic context for prediction in masked language prediction pretraining, and thus they can affect the performance of the model. However, the corpora, on which both BERT and XLNet were trained, are far different from the genomic DNA sequences; therefore, they might not have pretraining advantages. Thus, we also compared a different variant of BERT (DNABERT) pretrained on human genomic DNA at different kmer lengths (from three to five nucleotides) (<xref rid="B22" ref-type="bibr">Ji et al., 2021</xref>). The DNABERT models outperformed previous CNN-based models for TATA and non-TATA promoter prediction tasks in eukaryotes. To improve the resolution, we trained a byte-level byte-pair-encoding (BPE) tokenizer at a length of one nucleotide (or kmer 1). The operating mechanism is illustrated in <xref rid="F3" ref-type="fig">Figure 3</xref> and the performance results are listed in <xref rid="T2" ref-type="table">Table 2</xref> and <xref rid="F4" ref-type="fig">Figure 4</xref>.</p>
      <fig position="float" id="F3">
        <label>FIGURE 3</label>
        <caption>
          <p>The detailed model architecture of PromBERT for promoter prediction. DNA sequences of fixed length are tokenized using a custom 1bp tokenizer and fed into 144 attention modules. Based on the final tensors in the pooling layer, the classifier calculates the probabilities of promoter and non-promoter using the sigmoid function. Backpropagation was conducted using binary cross entropy loss.</p>
        </caption>
        <graphic xlink:href="fgene-13-1067562-g003" position="float"/>
      </fig>
      <table-wrap position="float" id="T2">
        <label>TABLE 2</label>
        <caption>
          <p>Performance of popular transformer-based NLP models for promoter prediction.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead valign="top">
            <tr>
              <th rowspan="2" align="left" colspan="1">Model and tokenizer</th>
              <th colspan="2" align="left" rowspan="1">AUROC</th>
              <th colspan="2" align="left" rowspan="1">Precision</th>
              <th colspan="2" align="left" rowspan="1">F1 score</th>
              <th colspan="2" align="left" rowspan="1">Support</th>
            </tr>
            <tr>
              <th align="left" rowspan="1" colspan="1">Promoter</th>
              <th align="left" rowspan="1" colspan="1">Non-promoter</th>
              <th align="left" rowspan="1" colspan="1">Promoter</th>
              <th align="left" rowspan="1" colspan="1">Non-promoter</th>
              <th align="left" rowspan="1" colspan="1">Promoter</th>
              <th align="left" rowspan="1" colspan="1">Non-promoter</th>
              <th align="left" rowspan="1" colspan="1">Promoter</th>
              <th align="left" rowspan="1" colspan="1">Non-promoter</th>
            </tr>
          </thead>
          <tbody valign="top">
            <tr>
              <td align="left" rowspan="1" colspan="1">XLNET</td>
              <td align="left" rowspan="1" colspan="1">0.926</td>
              <td align="left" rowspan="1" colspan="1">0.925</td>
              <td align="left" rowspan="1" colspan="1">0.85</td>
              <td align="left" rowspan="1" colspan="1">0.84</td>
              <td align="left" rowspan="1" colspan="1">0.85</td>
              <td align="left" rowspan="1" colspan="1">0.85</td>
              <td align="left" rowspan="1" colspan="1">1018</td>
              <td align="left" rowspan="1" colspan="1">1019</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">XLNET + 1bp tokenizer</td>
              <td align="left" rowspan="1" colspan="1">0.97</td>
              <td align="left" rowspan="1" colspan="1">0.97</td>
              <td align="left" rowspan="1" colspan="1">0.92</td>
              <td align="left" rowspan="1" colspan="1">0.92</td>
              <td align="left" rowspan="1" colspan="1">0.92</td>
              <td align="left" rowspan="1" colspan="1">0.92</td>
              <td align="left" rowspan="1" colspan="1">1018</td>
              <td align="left" rowspan="1" colspan="1">1019</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">BERT-base</td>
              <td align="left" rowspan="1" colspan="1">0.941</td>
              <td align="left" rowspan="1" colspan="1">0.942</td>
              <td align="left" rowspan="1" colspan="1">0.84</td>
              <td align="left" rowspan="1" colspan="1">0.89</td>
              <td align="left" rowspan="1" colspan="1">0.87</td>
              <td align="left" rowspan="1" colspan="1">0.87</td>
              <td align="left" rowspan="1" colspan="1">1001</td>
              <td align="left" rowspan="1" colspan="1">1036</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">BERT-base + 1bp tokenizer</td>
              <td align="left" rowspan="1" colspan="1">0.977</td>
              <td align="left" rowspan="1" colspan="1">0.977</td>
              <td align="left" rowspan="1" colspan="1">0.92</td>
              <td align="left" rowspan="1" colspan="1">0.95</td>
              <td align="left" rowspan="1" colspan="1">0.93</td>
              <td align="left" rowspan="1" colspan="1">0.93</td>
              <td align="left" rowspan="1" colspan="1">1001</td>
              <td align="left" rowspan="1" colspan="1">1036</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">DNABERT3 + kmer 3</td>
              <td align="left" rowspan="1" colspan="1">0.944</td>
              <td align="left" rowspan="1" colspan="1">0.944</td>
              <td align="left" rowspan="1" colspan="1">0.9</td>
              <td align="left" rowspan="1" colspan="1">0.84</td>
              <td align="left" rowspan="1" colspan="1">0.86</td>
              <td align="left" rowspan="1" colspan="1">0.88</td>
              <td align="left" rowspan="1" colspan="1">1008</td>
              <td align="left" rowspan="1" colspan="1">1029</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">DNABERT4 + kmer 4</td>
              <td align="left" rowspan="1" colspan="1">0.944</td>
              <td align="left" rowspan="1" colspan="1">0.944</td>
              <td align="left" rowspan="1" colspan="1">0.88</td>
              <td align="left" rowspan="1" colspan="1">0.86</td>
              <td align="left" rowspan="1" colspan="1">0.87</td>
              <td align="left" rowspan="1" colspan="1">0.87</td>
              <td align="left" rowspan="1" colspan="1">1028</td>
              <td align="left" rowspan="1" colspan="1">1009</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">DNABERT5+ kmer 5</td>
              <td align="left" rowspan="1" colspan="1">0.956</td>
              <td align="left" rowspan="1" colspan="1">0.956</td>
              <td align="left" rowspan="1" colspan="1">0.9</td>
              <td align="left" rowspan="1" colspan="1">0.89</td>
              <td align="left" rowspan="1" colspan="1">0.89</td>
              <td align="left" rowspan="1" colspan="1">0.89</td>
              <td align="left" rowspan="1" colspan="1">1031</td>
              <td align="left" rowspan="1" colspan="1">1006</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <fig position="float" id="F4">
        <label>FIGURE 4</label>
        <caption>
          <p>Average precision scores of the tested transformer-based models.</p>
        </caption>
        <graphic xlink:href="fgene-13-1067562-g004" position="float"/>
      </fig>
      <p>For this particular promoter prediction task (using binary cross entropy as the loss function and F1 score as the key determinants to evaluate model performance), both XLNet-base and BERT-base using a one kmer length byte-level BPE tokenizer had the best performance compared to the default tokenizers or tokenizer at different lengths. Both XLNet+1bp tokenizer and BERT+1bp tokenizer achieved AUROC scores of 0.97 and 0.977, and F1 scores of 0.92 and 0.93 respectively. These two models exhibited comparable performance. However, during training and testing, XLNet used more computing resources than BERT; therefore, we selected the BERT-base + 1bp tokenizer for further investigation. The corpora in which these two base models were pretrained did not contain genomic databases. They should not benefit from the pre-training process for the promoter prediction task. The high performance can be attributed to context awareness (context-based embedding) of the position and composition of the tokens (nucleotides) through the self-attention mechanism. We further tested the performance of the BERT-base + 1bp tokenizer and DNABERT5 + 1bp tokenizer. The results further show that there are no differences in performance. These findings also confirmed that, during training for promoter prediction tasks using BERT, the choice of tokenizer influenced the performance.</p>
      <p>Surprisingly, the DNABERT variants trained in the genomic context performed worse than the BERT-base + 1bp tokenizer. Longer kmer lengths might provide a better context and have more meaningful biological values for interpretation (<xref rid="B22" ref-type="bibr">Ji et al., 2021</xref>); however, the F1 scores of the pretrained DNABERT 3, 4, and five were lower than those of BERT-base and XLNet with the 1bp tokenizer. One possible explanation for this finding is that the 1bp tokenizer better captured nuances at the single-nucleotide level interactions in the training dataset. As the promoter datasets in the current study were extracted solely from TSSs and were not grouped in transcriptional factor classes, less information is required to make decisions. This model may significantly favor specific nucleotides at certain fixed positions. Using tokenizers with longer kmer lengths (for the case of DNABERT) might be better for other genomic applications or designs that require larger curated datasets with expected long-range interactions within those genomic sequences. This is particularly relevant if the model is pre-trained or fine-tuned by permutation and masked language modeling first on the genomic data of the target species. We further tested the influence of promoter length on model performance; however, increasing the promoter length to 200bp did not change the performance of any of the tested models (data not shown).</p>
    </sec>
    <sec id="s3-2">
      <title>Evaluating model performance compared to existing promoter prediction models using independent datasets from <italic>E. coli</italic>
</title>
      <p>To evaluate the robustness of the proposed BERT-base +1bp tokenizer for promoter prediction task, we conducted model training using an independent dataset for σ70 promoters for model benchmarking from a previous study (<xref rid="B59" ref-type="bibr">Zhang et al., 2022</xref>).</p>
      <p>We compared the performance of our model with two promoter prediction webservers iPro70-FMWin (<xref rid="B40" ref-type="bibr">Rahman et al., 2019</xref>) and iPromoter-2L2.0 (<xref rid="B30" ref-type="bibr">Liu et al., 2018</xref>) which were reported to have very high accuracy for σ70 promoters. The results showed that those three models performed equally well on the benchmarking dataset with F1 scores around 91%. Our model performed slightly better across promoter and non-promoter tag (<xref rid="T3" ref-type="table">Table 3</xref>). Since iPro70-FMWin also provides probability scores, we compared the AUPRC scores of this model with our Eco70PromBERT-1bp (<xref rid="F5" ref-type="fig">Figure 5</xref>). Our model had a better AUPRC score of 0.967 compared to 0.953 from iPro70-FMWin.</p>
      <table-wrap position="float" id="T3">
        <label>TABLE 3</label>
        <caption>
          <p>Performance of Eco70PromBERT and popular promoter prediction models for <italic>E.coli</italic> using an independent dataset (σ70 promoters and non-promoters).</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead valign="top">
            <tr>
              <th rowspan="2" align="left" colspan="1">Model and tokenizer</th>
              <th colspan="2" align="left" rowspan="1">AUROC</th>
              <th colspan="2" align="left" rowspan="1">Precision</th>
              <th colspan="2" align="left" rowspan="1">F1 score</th>
              <th colspan="2" align="left" rowspan="1">Support</th>
            </tr>
            <tr>
              <th align="left" rowspan="1" colspan="1">Promoter</th>
              <th align="left" rowspan="1" colspan="1">Non-promoter</th>
              <th align="left" rowspan="1" colspan="1">Promoter</th>
              <th align="left" rowspan="1" colspan="1">Non-promoter</th>
              <th align="left" rowspan="1" colspan="1">Promoter</th>
              <th align="left" rowspan="1" colspan="1">Non-promoter</th>
              <th align="left" rowspan="1" colspan="1">Promoter</th>
              <th align="left" rowspan="1" colspan="1">Non-promoter</th>
            </tr>
          </thead>
          <tbody valign="top">
            <tr>
              <td align="left" rowspan="1" colspan="1">Eco70PromBERT (BERT-base + 1bp tokenizer)</td>
              <td align="left" rowspan="1" colspan="1">0.92</td>
              <td align="left" rowspan="1" colspan="1">0.90</td>
              <td align="left" rowspan="1" colspan="1">0.91</td>
              <td align="left" rowspan="1" colspan="1">0.91</td>
              <td align="left" rowspan="1" colspan="1">0.91</td>
              <td align="left" rowspan="1" colspan="1">0.91</td>
              <td align="left" rowspan="1" colspan="1">110</td>
              <td align="left" rowspan="1" colspan="1">108</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">iPro70-FMWin</td>
              <td align="left" rowspan="1" colspan="1">0.90</td>
              <td align="left" rowspan="1" colspan="1">0.90</td>
              <td align="left" rowspan="1" colspan="1">0.93</td>
              <td align="left" rowspan="1" colspan="1">0.88</td>
              <td align="left" rowspan="1" colspan="1">0.90</td>
              <td align="left" rowspan="1" colspan="1">0.91</td>
              <td align="left" rowspan="1" colspan="1">110</td>
              <td align="left" rowspan="1" colspan="1">108</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">iPromoter-2L2.0</td>
              <td align="left" rowspan="1" colspan="1">0.91</td>
              <td align="left" rowspan="1" colspan="1">0.91</td>
              <td align="left" rowspan="1" colspan="1">0.90</td>
              <td align="left" rowspan="1" colspan="1">0.92</td>
              <td align="left" rowspan="1" colspan="1">0.91</td>
              <td align="left" rowspan="1" colspan="1">0.91</td>
              <td align="left" rowspan="1" colspan="1">110</td>
              <td align="left" rowspan="1" colspan="1">108</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <fig position="float" id="F5">
        <label>FIGURE 5</label>
        <caption>
          <p>Model performance based on receiver operating characteristic curves tested on an independent promoter datasets from <italic>E. coli</italic>. <bold>(A)</bold> iPro70-FMWin <bold>(B)</bold> Eco70PromBERT- (BERT-base + 1bp tokenizer trained on promoter datasets from <italic>E. coli</italic>).</p>
        </caption>
        <graphic xlink:href="fgene-13-1067562-g005" position="float"/>
      </fig>
      <p>The results illustrated the robustness of BERT-base + 1bp tokenizer for promoter prediction task in general. Considering that both iPro70-FMWin and iPromoter-2L2.0 were designed specifically to extract sequence features with various customizations for promoter classification to achieve SOTA performance. The plug-and-play characteristic of large language models like BERT would be better for scalability and broader applications.</p>
    </sec>
    <sec id="s3-3">
      <title>Interpreting the model’s behavior through Monte Carlo sampling and attention score visualization</title>
      <p>Interpreting deep-learning (DL) models is another important aspect of model validation. One of the main issues concerning deep-learning models is the “black box” problem, where users might not know how DL models process and compute the outputs for reverse engineering and understanding. This problem is particularly difficult for large parameter models such as NLP models (e.g., BERT). Specifically, the BERT-base model used in this study consists of 86.8 million trainable parameters from 144 attention modules (12 layers × 12 heads). The use of attention scores to visualize token weights is a commonly used method for improving model understanding. We employed integrated libraries for interpretability, namely BERTviz and Captum, to gain more insight into CyaPromBERT behavior and key features determining true promoters or non-promoters.</p>
      <p>From the BERTviz model view and Captum, it appeared that a large number of self-attention modules focused on -10 element and occasionally on -35 element for sequences classified as promoters (<xref rid="F6" ref-type="fig">Figure 6</xref> and <xref rid="F7" ref-type="fig">Figure 7</xref>). This is understandable, as the training dataset consists of all promoters from different sigma factor groups. In prokaryotes, the promoter regions are AT-rich and depend on the differences between their local structural properties and flanking sequences. The AT-rich -10 element plays a conserved role in DNA unwinding and facilitates transcription. Therefore, the constructed model could capture this local interaction context for promoter classification. Not all attention modules were utilized in the trained model; non-operational modes were observed in several layers and attention heads (cross-attention pointing to &lt;s&gt; and &lt;/s &gt; tokens).</p>
      <fig position="float" id="F6">
        <label>FIGURE 6</label>
        <caption>
          <p>Visualization tools for model interpretability. <bold>(A)</bold> Heatmap based on attention scores of nucleotides (tokens) across 12 layers. <bold>(B)</bold> Heatmap illustrating cross-attention scores of nucleotides (tokens) in the last three layers. In the example heatmap, the self-attention modules focused on -10 element and some positions in the -35 element.</p>
        </caption>
        <graphic xlink:href="fgene-13-1067562-g006" position="float"/>
      </fig>
      <fig position="float" id="F7">
        <label>FIGURE 7</label>
        <caption>
          <p>Motif analysis using attribution weights and reverse enrichment through Monte Carlo sampling. <bold>(A)</bold> Class attributions visualization of a few strong promoters in <italic>Synechococcus elongatus</italic> sp. UTEX 2973 and a non-promoter sequence. <bold>(B)</bold> Transcription factor groups in <italic>Synechocystis sp.</italic> PCC 6803. The relatively conserved region two in group 1 and group 2 retains a motif similar to the consensus -10 element TATAAT. <bold>(C)</bold> The motif learned by the trained model discovered by Monte Carlo sampling.</p>
        </caption>
        <graphic xlink:href="fgene-13-1067562-g007" position="float"/>
      </fig>
      <p>To estimate the closeness of the classifier to the real consensus of the -10 element, we defined a simple Monte Carlo generator using the constructed CyaPromBERT model as the discriminator. The pseudo-random generator generated fixed-length DNA sequences (50 nucleotides) until an expected number of sequences (500 sequences) passed the discriminator (cutoff value ≥0.99). Using this enrichment method, a recognition motif of the GnTAAAATT region was identified with a strong emphasis on thymine at the -11 and adenine at the -10 and -9 positions followed by two thymine bases at -6 and -5 (<xref rid="F7" ref-type="fig">Figure 7C</xref>), which is similar to the consensus motif of the extended -10 element GnTATAAT of the extended -10 element previously reported in <italic>E. coli</italic> (<xref rid="B15" ref-type="bibr">Feklistov and Darst 2011</xref>). Further stretching of GGG was similar to that of the discriminator element in <italic>E. coli</italic>. Reversed enrichment using Monte Carlo sampling did not yield any motifs for non-promoter sequences. Promoters recognized by sigma factor groups have preferred motifs; however, crosstalk between groups does occur due to similarity of the transcriptional factors (<xref rid="F7" ref-type="fig">Figure 7B</xref>). Group 1 (SigA), from the model cyanobacterium <italic>Synechocystis</italic> sp. PCC 6803 has consensus motifs similar to RpoD from <italic>E. coli</italic> (-35 element TTCACA and -10 element TATAAT), whereas the promoters recognized by sigma factor group 2 (SigB,C,D,E,F) have only a consensus motif of TATAAT for the -10 element. Group 3 (sigF,G,H,I) has dissimilar motifs of the -32 element TAGGC and -12 element GGTAA (<xref rid="B21" ref-type="bibr">Imamura and Asayama 2009</xref>). Therefore, the trained model CyaPromBERT potentially learned and gave better attention scores to nucleotide matching the enriched motif to distinguish promoter-like and non-promoter sequences.</p>
    </sec>
    <sec id="s3-4">
      <title>Cross-species validation through <italic>Synechocystis</italic> sp. PCC 6803 and <italic>Synechocystis</italic> sp. PCC 6714 datasets</title>
      <p>As stated above, one of the main objectives of the current work was to use the limited dRNA-seq datasets of some model organisms that are closely related to the organisms of interest to construct curated models capable of high-performance inferencing for species with similar lineages by taking advantage of the learning transferability of deep-learning models. Therefore, we further validated the trained model using promoter and non-promoter datasets prepared from <italic>Synechocystis</italic> sp. PCC 6803 using TSSNote. They were from a different genus than <italic>Synechococcus elongatus</italic> sp. UTEX 2973. The trained model performed well on promoter prediction tasks using datasets consisting of 2840 sequences from <italic>Synechocystis</italic> sp. PCC 6803, with an AUROC score of 0.961 and F1 score of 0.91 (<xref rid="T4" ref-type="table">Table 4</xref>). A slight reduction in performance compared with that of <italic>Synechococcus elongatus</italic> sp. UTEX 2973 may be due to overfitting or differences in genomic preferences between the two species. Additionally, we trained similarly a promoter prediction model from <italic>Synechocystis</italic> sp. PCC 6803 and cross validated it with a closely related species <italic>Synechocystis</italic> sp. PCC 6714. The performance was similar but F1 scores of 0.89 were lower than those from <italic>Synechocystis</italic> sp. PCC 6803 (<xref rid="T5" ref-type="table">Table 5</xref>). However, it should be noted that the quality of datasets for <italic>Synechocystis</italic> sp. PCC 6714 was not high, leading to more noisy data. Regardless, the results still demonstrated the capability of maintaining good performance in cross-species promoter prediction from similar lineages.</p>
      <table-wrap position="float" id="T4">
        <label>TABLE 4</label>
        <caption>
          <p>Cross validation the performance of CyaPromBERT trained on <italic>Synechococcus elongatus</italic> sp. UTEX 2973 for a distantly related species Synechocystis sp. PCC 6803.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead valign="top">
            <tr>
              <th rowspan="2" align="left" colspan="1">Species</th>
              <th colspan="2" align="left" rowspan="1">AUROC</th>
              <th colspan="2" align="left" rowspan="1">Precision</th>
              <th colspan="2" align="left" rowspan="1">F1 score</th>
              <th colspan="2" align="left" rowspan="1">Support</th>
            </tr>
            <tr>
              <th align="left" rowspan="1" colspan="1">Promoter</th>
              <th align="left" rowspan="1" colspan="1">Non-promoter</th>
              <th align="left" rowspan="1" colspan="1">Promoter</th>
              <th align="left" rowspan="1" colspan="1">Non-promoter</th>
              <th align="left" rowspan="1" colspan="1">Promoter</th>
              <th align="left" rowspan="1" colspan="1">Non-promoter</th>
              <th align="left" rowspan="1" colspan="1">Promoter</th>
              <th align="left" rowspan="1" colspan="1">Non-promoter</th>
            </tr>
          </thead>
          <tbody valign="top">
            <tr>
              <td align="left" rowspan="1" colspan="1"><italic>Synechococcus</italic> sp. UTEX 2973</td>
              <td align="left" rowspan="1" colspan="1">0.98</td>
              <td align="left" rowspan="1" colspan="1">0.98</td>
              <td align="left" rowspan="1" colspan="1">0.92</td>
              <td align="left" rowspan="1" colspan="1">0.95</td>
              <td align="left" rowspan="1" colspan="1">0.93</td>
              <td align="left" rowspan="1" colspan="1">0.93</td>
              <td align="left" rowspan="1" colspan="1">1001</td>
              <td align="left" rowspan="1" colspan="1">1036</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1"><italic>Synechococcus</italic> sp. PCC 6803</td>
              <td align="left" rowspan="1" colspan="1">0.96</td>
              <td align="left" rowspan="1" colspan="1">0.96</td>
              <td align="left" rowspan="1" colspan="1">0.88</td>
              <td align="left" rowspan="1" colspan="1">0.94</td>
              <td align="left" rowspan="1" colspan="1">0.91</td>
              <td align="left" rowspan="1" colspan="1">0.91</td>
              <td align="left" rowspan="1" colspan="1">1407</td>
              <td align="left" rowspan="1" colspan="1">1433</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <table-wrap position="float" id="T5">
        <label>TABLE 5</label>
        <caption>
          <p>Cross validation the performance of CyaPromBERT trained on <italic>Synechocystis</italic> sp. PCC 6803 for a closely related species <italic>Synechocystis</italic> sp. PCC 6714.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead valign="top">
            <tr>
              <th rowspan="2" align="left" colspan="1">Species</th>
              <th colspan="2" align="left" rowspan="1">AUROC</th>
              <th colspan="2" align="left" rowspan="1">Precision</th>
              <th colspan="2" align="left" rowspan="1">F1 score</th>
              <th colspan="2" align="left" rowspan="1">Support</th>
            </tr>
            <tr>
              <th align="left" rowspan="1" colspan="1">Promoter</th>
              <th align="left" rowspan="1" colspan="1">Non-promoter</th>
              <th align="left" rowspan="1" colspan="1">Promoter</th>
              <th align="left" rowspan="1" colspan="1">Non-promoter</th>
              <th align="left" rowspan="1" colspan="1">Promoter</th>
              <th align="left" rowspan="1" colspan="1">Non-promoter</th>
              <th align="left" rowspan="1" colspan="1">Promoter</th>
              <th align="left" rowspan="1" colspan="1">Non-promoter</th>
            </tr>
          </thead>
          <tbody valign="top">
            <tr>
              <td align="left" rowspan="1" colspan="1"><italic>Synechococcus</italic> sp. PCC 6803</td>
              <td align="left" rowspan="1" colspan="1">0.97</td>
              <td align="left" rowspan="1" colspan="1">0.97</td>
              <td align="left" rowspan="1" colspan="1">0.91</td>
              <td align="left" rowspan="1" colspan="1">0.92</td>
              <td align="left" rowspan="1" colspan="1">0.91</td>
              <td align="left" rowspan="1" colspan="1">0.92</td>
              <td align="left" rowspan="1" colspan="1">364</td>
              <td align="left" rowspan="1" colspan="1">378</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1"><italic>Synechococcus</italic> sp. PCC 6714</td>
              <td align="left" rowspan="1" colspan="1">0.96</td>
              <td align="left" rowspan="1" colspan="1">0.96</td>
              <td align="left" rowspan="1" colspan="1">0.91</td>
              <td align="left" rowspan="1" colspan="1">0.88</td>
              <td align="left" rowspan="1" colspan="1">0.89</td>
              <td align="left" rowspan="1" colspan="1">0.89</td>
              <td align="left" rowspan="1" colspan="1">330</td>
              <td align="left" rowspan="1" colspan="1">330</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="s3-5">
      <title>The limitations of the pipeline and the trained model</title>
      <p>Despite the fast construction and relatively high performance, a few limitations were present in the current work. First, for TSSNote, the quality and accuracy of promoter extraction depend on the quality of raw dRNAseq datasets and their experimental designs. The quality and performance of the trained model also depend on the quality of the inputs; therefore, selecting suitable parameters and preparing good datasets are the most important part of this pipeline. We tested the pipeline on datasets of the model acetogen <italic>Eubacterium limosum</italic> (<xref rid="B46" ref-type="bibr">Song et al., 2018</xref>). The pipeline produced a model with F1 scores of 0.88 and AUROC scores of 0.89. However, when we tested the pipeline on more dated datasets of other species, the trained models did not perform well. Second, despite the high performance of the test datasets and cross-validation, the trained model still suffers from false positives in the regional scanning mode. Thus, the results should be interpreted as the most potential locations, and further analyses for decision-making should be conducted. There are several possible explanations for this finding. To capture most promoters of the genera <italic>Synechocystis</italic> and <italic>Synechococcus</italic> through the learned pattern, the model focused solely on the interrelationship and composition of nucleotides in the -10 element. Therefore, the model may be confused with AT-rich promoter-like sequences. Another explanation is that transcription is a complex biological process, which is influenced by multiple factors, such as protein–DNA interactions and protein–protein interactions (DNA-binding proteins, transcription factors, enhancers, competition of sigma factors for the holoenzyme RNA polymerase), and the topographical state of the genome (chromosome folding states). The tertiary structures of chromosomes can greatly influence functional DNA-related processes, such as transcription and DNA replication (<xref rid="B12" ref-type="bibr">Dorman, 2019</xref>; <xref rid="B47" ref-type="bibr">Szabo et al., 2019</xref>). Such interactions cannot be fully captured with sequential information, which is another limitation of the current work. Regardless, the transformer architecture is a powerful building block for the construction of multimodal models; therefore, future incorporation of additional data reflecting cis/trans interactions and/or other neural networks may improve the accuracy and reduce false positives to make the model more deterministic. The pipeline and model in the current work may be used for constructing a fast and accessible promoter prediction and screening tool using a deep-learning approach, which can help reduce the time needed for downstream analyses.</p>
    </sec>
  </sec>
  <sec sec-type="conclusion" id="s4">
    <title>Conclusion</title>
    <p>With the rapid evolution and continuous development of next-generation sequencing techniques, an unprecedented vast amount of high-quality biological data has become increasingly accessible to researchers. This ever-expanding source of genomic data is a valuable, yet underexplored, reservoir of knowledge that can provide valuable insights into the mystery of life. Recently, methodological and computational advancements have enabled systematic and high-throughput approaches to elucidate the biological meanings of DNA sequences, in addition to traditional knowledge-based analysis. The traditional method for promoter identification involves dRNA-seq or 5′-CAGE experiments. However, despite the growing number of high-quality RNA-seq datasets, dRNA-seq experiments are still limited and expensive. In the current study, we applied and compared the performance of various SOTA transformer-based models for promoter prediction of <italic>Synechococcus elongatus</italic> sp. UTEX 2973 <italic>and Synechocystis</italic> sp. PCC 6803. The model achieved an AUROC score of 97% and an F1 score of 92% in the validation dataset of the promoters extracted from <italic>Synechococcus elongatus</italic> sp. UTEX 2973 and had an AUROC score of 96% and F1 score of 91% when cross-validated using 7000 promoters from <italic>Synechocystis</italic> sp. PCC 6803. This finding illustrated that core promoter features are conserved in related species, and the dRNA-seq dataset of one model organism is sufficient to construct a curated promoter prediction model.</p>
    <p>Precise promoter prediction is essential to understand the regulatory mechanisms of genes and operons. A key advantage of this study is that it can rapidly identify potential promoter sequences and regions from genomic data with high precision. The model is integrated with the visualization libraries BERTviz and Captum to visualize cross-attention weights, allowing closer observation of base-pair interactions. To increase accessibility to other researchers, both the models and pipeline were ported to the cloud-computing service Google Colab. The pipeline developed (TSSNote and PromBERT) in this study can be applied to other species and lineages to develop fast promoter prediction tools. As transformer architecture has become increasingly popular for multimodal learning, the implementation and analysis of BERT behavior in the context of genomics is another case study for developing more robust implementations of transformers for biological application.</p>
  </sec>
</body>
<back>
  <sec sec-type="data-availability" id="s5">
    <title>Data availability statement</title>
    <p>The datasets presented in this study can be found in online repositories. The names of the repository/repositories and accession number(s) can be found in the article/supplementary material.</p>
  </sec>
  <sec id="s6">
    <title>Author contributions</title>
    <p>DM, Conceptualization, Methodology, Investigation, Writing—review and editing. LN, Review and editing. EL, Funding acquisition, Project administration, Supervision, Writing—review and editing.</p>
  </sec>
  <sec id="s7">
    <title>Funding</title>
    <p>This research was supported by the C1 Gas Refinery Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science and ICT (2015M3D3A1A01064882).</p>
  </sec>
  <sec sec-type="COI-statement" id="s8">
    <title>Conflict of interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec sec-type="disclaimer" id="s9">
    <title>Publisher’s note</title>
    <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amin</surname><given-names>R.</given-names></name><name><surname>Rahman</surname><given-names>C. R.</given-names></name><name><surname>Ahmed</surname><given-names>S.</given-names></name><name><surname>Sifat</surname><given-names>M. H. R.</given-names></name><name><surname>Liton</surname><given-names>M. N. K.</given-names></name><name><surname>Rahman</surname><given-names>M. M.</given-names></name><etal/></person-group> (<year>2020</year>). <article-title>iPromoter-BnCNN: a novel branched CNN-based predictor for identifying and classifying sigma promoters</article-title>. <source>Bioinformatics</source>
<volume>36</volume> (<issue>19</issue>), <fpage>4869</fpage>–<lpage>4875</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btaa609</pub-id>
<pub-id pub-id-type="pmid">32614400</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnab</surname><given-names>A.</given-names></name><name><surname>Dehghani</surname><given-names>M.</given-names></name><name><surname>Heigold</surname><given-names>G.</given-names></name><name><surname>Sun</surname><given-names>C.</given-names></name><name><surname>Lučić</surname><given-names>M.</given-names></name><name><surname>Schmid</surname><given-names>C.</given-names></name></person-group> (<year>2021</year>). <article-title>Vivit: A video vision transformer</article-title>. <source>Proc. IEEE Int. Conf. Comput. Vis.</source>
<volume>30</volume>, <fpage>1811</fpage>–<lpage>1820</lpage>. <pub-id pub-id-type="doi">10.48550/arXiv.2103.15691</pub-id>
</mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bhandari</surname><given-names>N.</given-names></name><name><surname>Khare</surname><given-names>S.</given-names></name><name><surname>Walambe</surname><given-names>R.</given-names></name><name><surname>Kotecha</surname><given-names>K.</given-names></name></person-group> (<year>2021</year>). <article-title>Comparison of machine learning and deep learning techniques in promoter prediction across diverse species</article-title>. <source>PeerJ. Comput. Sci.</source>
<volume>7</volume>, <fpage>e365</fpage>. <pub-id pub-id-type="doi">10.7717/peerj-cs.365</pub-id>
<pub-id pub-id-type="pmid">33817015</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bischler</surname><given-names>T.</given-names></name><name><surname>Tan</surname><given-names>H. S.</given-names></name><name><surname>Nieselt</surname><given-names>K.</given-names></name><name><surname>Sharma</surname><given-names>C. M.</given-names></name></person-group> (<year>2015</year>). <article-title>Differential RNA-seq (dRNA-seq) for annotation of transcriptional start sites and small RNAs in <italic>Helicobacter pylori</italic>
</article-title>. <source>Methods</source>
<volume>86</volume>, <fpage>89</fpage>–<lpage>101</lpage>. <pub-id pub-id-type="doi">10.1016/j.ymeth.2015.06.012</pub-id>
<pub-id pub-id-type="pmid">26091613</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burden</surname><given-names>S.</given-names></name><name><surname>Lin</surname><given-names>Y.-X.</given-names></name><name><surname>Zhang</surname><given-names>R.</given-names></name></person-group> (<year>2005</year>). <article-title>Improving promoter prediction for the NNPP2.2 algorithm: A case study using <italic>Escherichia coli</italic> DNA sequences</article-title>. <source>Bioinformatics</source>
<volume>21</volume> (<issue>5</issue>), <fpage>601</fpage>–<lpage>607</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/bti047</pub-id>
<pub-id pub-id-type="pmid">15454410</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Butler</surname><given-names>J. E.</given-names></name><name><surname>Kadonaga</surname><given-names>J. T.</given-names></name></person-group> (<year>2002</year>). <article-title>The RNA polymerase II core promoter: A key component in the regulation of gene expression</article-title>. <source>Genes Dev.</source>
<volume>16</volume> (<issue>20</issue>), <fpage>2583</fpage>–<lpage>2592</lpage>. <pub-id pub-id-type="doi">10.1101/gad.1026202</pub-id>
<pub-id pub-id-type="pmid">12381658</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cock</surname><given-names>P. J.</given-names></name><name><surname>Antao</surname><given-names>T.</given-names></name><name><surname>Chang</surname><given-names>J. T.</given-names></name><name><surname>Chapman</surname><given-names>B. A.</given-names></name><name><surname>Cox</surname><given-names>C. J.</given-names></name><name><surname>Dalke</surname><given-names>A.</given-names></name><etal/></person-group> (<year>2009</year>). <article-title>Biopython: Freely available Python tools for computational molecular biology and bioinformatics</article-title>. <source>Bioinformatics</source>
<volume>25</volume> (<issue>11</issue>), <fpage>1422</fpage>–<lpage>1423</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btp163</pub-id>
<pub-id pub-id-type="pmid">19304878</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>da Silva</surname><given-names>K. P.</given-names></name><name><surname>Monteiro</surname><given-names>M. I.</given-names></name><name><surname>de Souto</surname><given-names>M. C. P.</given-names></name></person-group> (<year>2006</year>). “<article-title><italic>In silico</italic> prediction of promoter sequences of Bacillus species</article-title>,” in <conf-name>The 2006 IEEE Proc. Int. Jt. Conf. Neural Netw.</conf-name>, <conf-loc>Vancouver, BC, Canada</conf-loc>, <conf-date>16-21 July 2006</conf-date> (<publisher-name>IEEE</publisher-name>), <fpage>1</fpage>–<lpage>5</lpage>.</mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Danecek</surname><given-names>P.</given-names></name><name><surname>Bonfield</surname><given-names>J. K.</given-names></name><name><surname>Liddle</surname><given-names>J.</given-names></name><name><surname>Marshall</surname><given-names>J.</given-names></name><name><surname>Ohan</surname><given-names>V.</given-names></name><name><surname>Pollard</surname><given-names>M. O.</given-names></name><etal/></person-group> (<year>2021</year>). <article-title>Twelve years of SAMtools and BCFtools</article-title>. <source>Gigascience</source>
<volume>10</volume> (<issue>2</issue>), <fpage>giab008</fpage>. <pub-id pub-id-type="doi">10.1093/gigascience/giab008</pub-id>
<pub-id pub-id-type="pmid">33590861</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devlin</surname><given-names>J.</given-names></name><name><surname>Chang</surname><given-names>M.-W.</given-names></name><name><surname>Lee</surname><given-names>K.</given-names></name><name><surname>Toutanova</surname><given-names>K.</given-names></name></person-group> (<year>2018</year>). "<article-title>Bert: Pre-training of deep bidirectional transformers for language understanding</article-title>." <comment><italic>arXiv preprint arXiv:1810.04805</italic></comment>.</mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Salvo</surname><given-names>M.</given-names></name><name><surname>Pinatel</surname><given-names>E.</given-names></name><name><surname>Talà</surname><given-names>A.</given-names></name><name><surname>Fondi</surname><given-names>M.</given-names></name><name><surname>Peano</surname><given-names>C.</given-names></name><name><surname>Alifano</surname><given-names>P.</given-names></name></person-group> (<year>2018</year>). <article-title>G4PromFinder: An algorithm for predicting transcription promoters in GC-rich bacterial genomes based on AT-rich elements and G-quadruplex motifs</article-title>. <source>BMC Bioinforma.</source>
<volume>19</volume> (<issue>1</issue>), <fpage>36</fpage>–<lpage>11</lpage>. <pub-id pub-id-type="doi">10.1186/s12859-018-2049-x</pub-id>
</mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dorman</surname><given-names>C. J.</given-names></name></person-group> (<year>2019</year>). <article-title>DNA supercoiling and transcription in bacteria: A two-way street</article-title>. <source>BMC Mol. Cell Biol.</source>
<volume>20</volume> (<issue>1</issue>), <fpage>26</fpage>–<lpage>29</lpage>. <pub-id pub-id-type="doi">10.1186/s12860-019-0211-6</pub-id>
<pub-id pub-id-type="pmid">31319794</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dugar</surname><given-names>G.</given-names></name><name><surname>Herbig</surname><given-names>A.</given-names></name><name><surname>Förstner</surname><given-names>K. U.</given-names></name><name><surname>Heidrich</surname><given-names>N.</given-names></name><name><surname>Reinhardt</surname><given-names>R.</given-names></name><name><surname>Nieselt</surname><given-names>K.</given-names></name><etal/></person-group> (<year>2013</year>). <article-title>High-resolution transcriptome maps reveal strain-specific regulatory features of multiple Campylobacter jejuni isolates</article-title>. <source>PLoS Genet.</source>
<volume>9</volume> (<issue>5</issue>), <fpage>e1003495</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pgen.1003495</pub-id>
<pub-id pub-id-type="pmid">23696746</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Dzabraev</surname><given-names>M.</given-names></name><name><surname>Kalashnikov</surname><given-names>M.</given-names></name><name><surname>Komkov</surname><given-names>S.</given-names></name><name><surname>Petiushko</surname><given-names>A.</given-names></name></person-group> (<year>2021</year>). “<article-title>Mdmmt: Multidomain multimodal transformer for video retrieval</article-title>,” in <conf-name>Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.</conf-name>, <conf-loc>Nashville, TN, USA</conf-loc>, <conf-date>19-25 June 2021</conf-date> (<publisher-name>IEEE</publisher-name>), <fpage>1</fpage>–<lpage>5</lpage>.</mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feklistov</surname><given-names>A.</given-names></name><name><surname>Darst</surname><given-names>S. A.</given-names></name></person-group> (<year>2011</year>). <article-title>Structural basis for promoter− 10 element recognition by the bacterial RNA polymerase σ subunit</article-title>. <source>Cell</source>
<volume>147</volume> (<issue>6</issue>), <fpage>1257</fpage>–<lpage>1269</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2011.10.041</pub-id>
<pub-id pub-id-type="pmid">22136875</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Floridi</surname><given-names>L.</given-names></name><name><surname>Chiriatti</surname><given-names>M.</given-names></name></person-group> (<year>2020</year>). <article-title>GPT-3: Its nature, scope, limits, and consequences</article-title>. <source>Minds Mach. (Dordr).</source>
<volume>30</volume> (<issue>4</issue>), <fpage>681</fpage>–<lpage>694</lpage>. <pub-id pub-id-type="doi">10.1007/s11023-020-09548-1</pub-id>
</mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gordon</surname><given-names>L.</given-names></name><name><surname>Chervonenkis</surname><given-names>A. Y.</given-names></name><name><surname>Gammerman</surname><given-names>A. J.</given-names></name><name><surname>Shahmuradov</surname><given-names>I. A.</given-names></name><name><surname>Solovyev</surname><given-names>V. V.</given-names></name></person-group> (<year>2003</year>). <article-title>Sequence alignment kernel for recognition of promoter regions</article-title>. <source>Bioinformatics</source>
<volume>19</volume> (<issue>15</issue>), <fpage>1964</fpage>–<lpage>1971</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btg265</pub-id>
<pub-id pub-id-type="pmid">14555630</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>W.</given-names></name><name><surname>Jia</surname><given-names>C.</given-names></name><name><surname>Duan</surname><given-names>Y.</given-names></name><name><surname>Zou</surname><given-names>Q.</given-names></name></person-group> (<year>2018</year>). <article-title>70ProPred: A predictor for discovering sigma70 promoters based on combining multiple features</article-title>. <source>BMC Syst. Biol.</source>
<volume>12</volume> (<issue>4</issue>), <fpage>44</fpage>–<lpage>107</lpage>. <pub-id pub-id-type="doi">10.1186/s12918-018-0570-1</pub-id>
<pub-id pub-id-type="pmid">29745856</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huerta</surname><given-names>A. M.</given-names></name><name><surname>Collado-Vides</surname><given-names>J.</given-names></name></person-group> (<year>2003</year>). <article-title>Sigma70 promoters in <italic>Escherichia coli</italic>: Specific transcription in dense regions of overlapping promoter-like signals</article-title>. <source>J. Mol. Biol.</source>
<volume>333</volume> (<issue>2</issue>), <fpage>261</fpage>–<lpage>278</lpage>. <pub-id pub-id-type="doi">10.1016/j.jmb.2003.07.017</pub-id>
<pub-id pub-id-type="pmid">14529615</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ikeuchi</surname><given-names>M.</given-names></name><name><surname>Tabata</surname><given-names>S.</given-names></name></person-group> (<year>2001</year>). <article-title>Synechocystis sp. PCC 6803—A useful tool in the study of the genetics of cyanobacteria</article-title>. <source>Photosynth. Res.</source>
<volume>70</volume> (<issue>1</issue>), <fpage>73</fpage>–<lpage>83</lpage>. <pub-id pub-id-type="doi">10.1023/A:1013887908680</pub-id>
<pub-id pub-id-type="pmid">16228363</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Imamura</surname><given-names>S.</given-names></name><name><surname>Asayama</surname><given-names>M.</given-names></name></person-group> (<year>2009</year>). <article-title>Sigma factors for cyanobacterial transcription</article-title>. <source>Gene Regul. Syst. Bio.</source>
<volume>3</volume>, <fpage>65</fpage>–<lpage>87</lpage>. <pub-id pub-id-type="doi">10.4137/grsb.s2090</pub-id>
</mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ji</surname><given-names>Y.</given-names></name><name><surname>Zhou</surname><given-names>Z.</given-names></name><name><surname>Liu</surname><given-names>H.</given-names></name><name><surname>Davuluri</surname><given-names>R. V.</given-names></name></person-group> (<year>2021</year>). <article-title>Dnabert: Pre-trained bidirectional encoder representations from transformers model for DNA-language in genome</article-title>. <source>Bioinformatics</source>
<volume>37</volume> (<issue>15</issue>), <fpage>2112</fpage>–<lpage>2120</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btab083</pub-id>
</mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kans</surname><given-names>J.</given-names></name></person-group> (<year>2022</year>). <source>Entrez direct: E-Utilities on the UNIX command line. Entrez <italic>programming</italic> utilities help [internet]</source>. <publisher-loc>Bethesda, Maryland, United States</publisher-loc>: <publisher-name>National Center for Biotechnology Information</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kato</surname><given-names>Y.</given-names></name><name><surname>Hasunuma</surname><given-names>T.</given-names></name></person-group> (<year>2021</year>). <source>Metabolic engineering for carotenoid production using eukaryotic microalgae and prokaryotic cyanobacteria. Carotenoids: Biosynthetic and Biofunctional Approaches</source>. <publisher-loc>Berlin, Germany</publisher-loc>: <publisher-name>Springer</publisher-name>, <fpage>121</fpage>–<lpage>135</lpage>.</mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>D.</given-names></name><name><surname>Paggi</surname><given-names>J. M.</given-names></name><name><surname>Park</surname><given-names>C.</given-names></name><name><surname>Bennett</surname><given-names>C.</given-names></name><name><surname>Salzberg</surname><given-names>S. L.</given-names></name></person-group> (<year>2019</year>). <article-title>Graph-based genome alignment and genotyping with HISAT2 and HISAT-genotype</article-title>. <source>Nat. Biotechnol.</source>
<volume>37</volume> (<issue>8</issue>), <fpage>907</fpage>–<lpage>915</lpage>. <pub-id pub-id-type="doi">10.1038/s41587-019-0201-4</pub-id>
<pub-id pub-id-type="pmid">31375807</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kokhlikyan</surname><given-names>N.</given-names></name><name><surname>Miglani</surname><given-names>V.</given-names></name><name><surname>Martin</surname><given-names>M.</given-names></name><name><surname>Wang</surname><given-names>E.</given-names></name><name><surname>Alsallakh</surname><given-names>B.</given-names></name><name><surname>Reynolds</surname><given-names>J.</given-names></name><etal/></person-group> (<year>2020</year>). "<article-title>Captum: A unified and generic model interpretability library for pytorch</article-title>." <comment><italic>arXiv preprint arXiv:2009.07896</italic></comment>.</mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Le</surname><given-names>N. Q. K.</given-names></name><name><surname>Yapp</surname><given-names>E. K. Y.</given-names></name><name><surname>Nagasundaram</surname><given-names>N.</given-names></name><name><surname>Yeh</surname><given-names>H.-Y.</given-names></name></person-group> (<year>2019</year>). <article-title>Classifying promoters by interpreting the hidden information of DNA sequences via deep learning and combination of continuous fasttext N-grams</article-title>. <source>Front. Bioeng. Biotechnol.</source>
<volume>305</volume>, <fpage>305</fpage>. <pub-id pub-id-type="doi">10.3389/fbioe.2019.00305</pub-id>
</mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>F.</given-names></name><name><surname>Chen</surname><given-names>J.</given-names></name><name><surname>Ge</surname><given-names>Z.</given-names></name><name><surname>Wen</surname><given-names>Y.</given-names></name><name><surname>Yue</surname><given-names>Y.</given-names></name><name><surname>Hayashida</surname><given-names>M.</given-names></name><etal/></person-group> (<year>2021</year>). <article-title>Computational prediction and interpretation of both general and specific types of promoters in <italic>Escherichia coli</italic> by exploiting a stacked ensemble-learning framework</article-title>. <source>Brief. Bioinform.</source>
<volume>22</volume> (<issue>2</issue>), <fpage>2126</fpage>–<lpage>2140</lpage>. <pub-id pub-id-type="doi">10.1093/bib/bbaa049</pub-id>
<pub-id pub-id-type="pmid">32363397</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>P.-C.</given-names></name><name><surname>Zhang</surname><given-names>F.</given-names></name><name><surname>Pakrasi</surname><given-names>H. B.</given-names></name></person-group> (<year>2020</year>). <article-title>Enhanced production of sucrose in the fast-growing cyanobacterium Synechococcus elongatus UTEX 2973</article-title>. <source>Sci. Rep.</source>
<volume>10</volume> (<issue>1</issue>), <fpage>390</fpage>–<lpage>398</lpage>. <pub-id pub-id-type="doi">10.1038/s41598-019-57319-5</pub-id>
<pub-id pub-id-type="pmid">31942010</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>B.</given-names></name><name><surname>Yang</surname><given-names>F.</given-names></name><name><surname>Huang</surname><given-names>D.-S.</given-names></name><name><surname>Chou</surname><given-names>K.-C.</given-names></name></person-group> (<year>2018</year>). <article-title>iPromoter-2L: a two-layer predictor for identifying promoters and their types by multi-window-based PseKNC</article-title>. <source>Bioinformatics</source>
<volume>34</volume> (<issue>1</issue>), <fpage>33</fpage>–<lpage>40</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btx579</pub-id>
<pub-id pub-id-type="pmid">28968797</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luan</surname><given-names>G.</given-names></name><name><surname>Zhang</surname><given-names>S.</given-names></name><name><surname>Wang</surname><given-names>M.</given-names></name><name><surname>Lu</surname><given-names>X.</given-names></name></person-group> (<year>2019</year>). <article-title>Progress and perspective on cyanobacterial glycogen metabolism engineering</article-title>. <source>Biotechnol. Adv.</source>
<volume>37</volume> (<issue>5</issue>), <fpage>771</fpage>–<lpage>786</lpage>. <pub-id pub-id-type="doi">10.1016/j.biotechadv.2019.04.005</pub-id>
<pub-id pub-id-type="pmid">30978387</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mann</surname><given-names>S.</given-names></name><name><surname>Li</surname><given-names>J.</given-names></name><name><surname>Chen</surname><given-names>Y.-P. P.</given-names></name></person-group> (<year>2007</year>). <article-title>A pHMM-ANN based discriminative approach to promoter identification in prokaryote genomic contexts</article-title>. <source>Nucleic Acids Res.</source>
<volume>35</volume> (<issue>2</issue>), <fpage>e12</fpage>. <pub-id pub-id-type="doi">10.1093/nar/gkl1024</pub-id>
<pub-id pub-id-type="pmid">17170007</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mueller</surname><given-names>T. J.</given-names></name><name><surname>Ungerer</surname><given-names>J. L.</given-names></name><name><surname>Pakrasi</surname><given-names>H. B.</given-names></name><name><surname>Maranas</surname><given-names>C. D.</given-names></name></person-group> (<year>2017</year>). <article-title>Identifying the metabolic differences of a fast-growth phenotype in Synechococcus UTEX 2973</article-title>. <source>Sci. Rep.</source>
<volume>7</volume> (<issue>1</issue>), <fpage>41569</fpage>–<lpage>41578</lpage>. <pub-id pub-id-type="doi">10.1038/srep41569</pub-id>
<pub-id pub-id-type="pmid">28139686</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>N. G.</given-names></name><name><surname>Tran</surname><given-names>V. A.</given-names></name><name><surname>Phan</surname><given-names>D.</given-names></name><name><surname>Lumbanraja</surname><given-names>F. R.</given-names></name><name><surname>Faisal</surname><given-names>M. R.</given-names></name><name><surname>Abapihi</surname><given-names>B.</given-names></name><etal/></person-group> (<year>2016</year>). <article-title>DNA sequence classification by convolutional neural network</article-title>. <source>J. Biomed. Sci. Eng.</source>
<volume>9</volume> (<issue>5</issue>), <fpage>280</fpage>–<lpage>286</lpage>. <pub-id pub-id-type="doi">10.4236/jbise.2016.95021</pub-id>
</mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oubounyt</surname><given-names>M.</given-names></name><name><surname>Louadi</surname><given-names>Z.</given-names></name><name><surname>Tayara</surname><given-names>H.</given-names></name><name><surname>Chong</surname><given-names>K. T.</given-names></name></person-group> (<year>2019</year>). <article-title>DeePromoter: Robust promoter predictor using deep learning</article-title>. <source>Front. Genet.</source>
<volume>10</volume>, <fpage>286</fpage>. <pub-id pub-id-type="doi">10.3389/fgene.2019.00286</pub-id>
<pub-id pub-id-type="pmid">31024615</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A.</given-names></name><name><surname>Gross</surname><given-names>S.</given-names></name><name><surname>Massa</surname><given-names>F.</given-names></name><name><surname>Lerer</surname><given-names>A.</given-names></name><name><surname>Bradbury</surname><given-names>J.</given-names></name><name><surname>Chanan</surname><given-names>G.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>Pytorch: An imperative style, high-performance deep learning library</article-title>. <source>Adv. Neural Inf. Process Syst.</source>
<volume>32</volume>, <fpage>8026</fpage>–<lpage>8037</lpage>.</mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pattharaprachayakul</surname><given-names>N.</given-names></name><name><surname>Choi</surname><given-names>J.-i.</given-names></name><name><surname>Incharoensakdi</surname><given-names>A.</given-names></name><name><surname>Woo</surname><given-names>H. M.</given-names></name></person-group> (<year>2020</year>). <article-title>Metabolic engineering and synthetic biology of cyanobacteria for carbon capture and utilization</article-title>. <source>Biotechnol. Bioprocess Eng.</source>
<volume>25</volume> (<issue>6</issue>), <fpage>829</fpage>–<lpage>847</lpage>. <pub-id pub-id-type="doi">10.1007/s12257-019-0447-1</pub-id>
</mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qiao</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>W.</given-names></name><name><surname>Lu</surname><given-names>X.</given-names></name></person-group> (<year>2020</year>). <article-title>Engineering cyanobacteria as cell factories for direct trehalose production from CO2</article-title>. <source>Metab. Eng.</source>
<volume>62</volume>, <fpage>161</fpage>–<lpage>171</lpage>. <pub-id pub-id-type="doi">10.1016/j.ymben.2020.08.014</pub-id>
<pub-id pub-id-type="pmid">32898716</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rahman</surname><given-names>M.</given-names></name><name><surname>Aktar</surname><given-names>U.</given-names></name><name><surname>Jani</surname><given-names>M. R.</given-names></name><name><surname>Shatabda</surname><given-names>S.</given-names></name></person-group> (<year>2019</year>). <article-title>iPro70-FMWin: identifying Sigma70 promoters using multiple windowing and minimal features</article-title>. <source>Mol. Genet. Genomics</source>
<volume>294</volume> (<issue>1</issue>), <fpage>69</fpage>–<lpage>84</lpage>. <pub-id pub-id-type="doi">10.1007/s00438-018-1487-5</pub-id>
<pub-id pub-id-type="pmid">30187132</pub-id></mixed-citation>
    </ref>
    <ref id="B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rahman</surname><given-names>M. S.</given-names></name><name><surname>Aktar</surname><given-names>U.</given-names></name><name><surname>Jani</surname><given-names>M. R.</given-names></name><name><surname>Shatabda</surname><given-names>S.</given-names></name></person-group> (<year>2019</year>). <article-title>iPromoter-FSEn: Identification of bacterial σ70 promoter sequences using feature subspace based ensemble classifier</article-title>. <source>Genomics</source>
<volume>111</volume> (<issue>5</issue>), <fpage>1160</fpage>–<lpage>1166</lpage>. <pub-id pub-id-type="doi">10.1016/j.ygeno.2018.07.011</pub-id>
<pub-id pub-id-type="pmid">30059731</pub-id></mixed-citation>
    </ref>
    <ref id="B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rangannan</surname><given-names>V.</given-names></name><name><surname>Bansal</surname><given-names>M.</given-names></name></person-group> (<year>2010</year>). <article-title>High-quality annotation of promoter regions for 913 bacterial genomes</article-title>. <source>Bioinformatics</source>
<volume>26</volume> (<issue>24</issue>), <fpage>3043</fpage>–<lpage>3050</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btq577</pub-id>
<pub-id pub-id-type="pmid">20956245</pub-id></mixed-citation>
    </ref>
    <ref id="B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roh</surname><given-names>H.</given-names></name><name><surname>Lee</surname><given-names>J. S.</given-names></name><name><surname>Choi</surname><given-names>H. I.</given-names></name><name><surname>Sung</surname><given-names>Y. J.</given-names></name><name><surname>Choi</surname><given-names>S. Y.</given-names></name><name><surname>Woo</surname><given-names>H. M.</given-names></name><etal/></person-group> (<year>2021</year>). <article-title>Improved CO2-derived polyhydroxybutyrate (PHB) production by engineering fast-growing cyanobacterium Synechococcus elongatus UTEX 2973 for potential utilization of flue gas</article-title>. <source>Bioresour. Technol.</source>
<volume>327</volume>, <fpage>124789</fpage>. <pub-id pub-id-type="doi">10.1016/j.biortech.2021.124789</pub-id>
<pub-id pub-id-type="pmid">33556769</pub-id></mixed-citation>
    </ref>
    <ref id="B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Santos-Merino</surname><given-names>M.</given-names></name><name><surname>Torrado</surname><given-names>A.</given-names></name><name><surname>Davis</surname><given-names>G. A.</given-names></name><name><surname>Röttig</surname><given-names>A.</given-names></name><name><surname>Bibby</surname><given-names>T. S.</given-names></name><name><surname>Kramer</surname><given-names>D. M.</given-names></name><etal/></person-group> (<year>2021</year>). <article-title>Improved photosynthetic capacity and photosystem I oxidation via heterologous metabolism engineering in cyanobacteria</article-title>. <source>Proc. Natl. Acad. Sci. U. S. A.</source>
<volume>118</volume> (<issue>11</issue>), <fpage>e2021523118</fpage>. <pub-id pub-id-type="doi">10.1073/pnas.2021523118</pub-id>
<pub-id pub-id-type="pmid">33836593</pub-id></mixed-citation>
    </ref>
    <ref id="B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sarnaik</surname><given-names>A.</given-names></name><name><surname>Abernathy</surname><given-names>M. H.</given-names></name><name><surname>Han</surname><given-names>X.</given-names></name><name><surname>Ouyang</surname><given-names>Y.</given-names></name><name><surname>Xia</surname><given-names>K.</given-names></name><name><surname>Chen</surname><given-names>Y.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>Metabolic engineering of cyanobacteria for photoautotrophic production of heparosan, a pharmaceutical precursor of heparin</article-title>. <source>Algal Res.</source>
<volume>37</volume>, <fpage>57</fpage>–<lpage>63</lpage>. <pub-id pub-id-type="doi">10.1016/j.algal.2018.11.010</pub-id>
</mixed-citation>
    </ref>
    <ref id="B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>K.</given-names></name><name><surname>Tan</surname><given-names>X.</given-names></name><name><surname>Liang</surname><given-names>Y.</given-names></name><name><surname>Lu</surname><given-names>X.</given-names></name></person-group> (<year>2016</year>). <article-title>The potential of Synechococcus elongatus UTEX 2973 for sugar feedstock production</article-title>. <source>Appl. Microbiol. Biotechnol.</source>
<volume>100</volume> (<issue>18</issue>), <fpage>7865</fpage>–<lpage>7875</lpage>. <pub-id pub-id-type="doi">10.1007/s00253-016-7510-z</pub-id>
<pub-id pub-id-type="pmid">27079574</pub-id></mixed-citation>
    </ref>
    <ref id="B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>Y.</given-names></name><name><surname>Shin</surname><given-names>J.</given-names></name><name><surname>Jin</surname><given-names>S.</given-names></name><name><surname>Lee</surname><given-names>J.-K.</given-names></name><name><surname>Kim</surname><given-names>D. R.</given-names></name><name><surname>Kim</surname><given-names>S. C.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>Genome-scale analysis of syngas fermenting acetogenic bacteria reveals the translational regulation for its autotrophic growth</article-title>. <source>BMC Genomics</source>
<volume>19</volume> (<issue>1</issue>), <fpage>837</fpage>–<lpage>915</lpage>. <pub-id pub-id-type="doi">10.1186/s12864-018-5238-0</pub-id>
<pub-id pub-id-type="pmid">30470174</pub-id></mixed-citation>
    </ref>
    <ref id="B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szabo</surname><given-names>Q.</given-names></name><name><surname>Bantignies</surname><given-names>F.</given-names></name><name><surname>Cavalli</surname><given-names>G.</given-names></name></person-group> (<year>2019</year>). <article-title>Principles of genome folding into topologically associating domains</article-title>. <source>Sci. Adv.</source>
<volume>5</volume> (<issue>4</issue>), <fpage>eaaw1668</fpage>. <pub-id pub-id-type="doi">10.1126/sciadv.aaw1668</pub-id>
<pub-id pub-id-type="pmid">30989119</pub-id></mixed-citation>
    </ref>
    <ref id="B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>X.</given-names></name><name><surname>Hou</surname><given-names>S.</given-names></name><name><surname>Song</surname><given-names>K.</given-names></name><name><surname>Georg</surname><given-names>J.</given-names></name><name><surname>Klähn</surname><given-names>S.</given-names></name><name><surname>Lu</surname><given-names>X.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>The primary transcriptome of the fast-growing cyanobacterium Synechococcus elongatus UTEX 2973</article-title>. <source>Biotechnol. Biofuels</source>
<volume>11</volume> (<issue>1</issue>), <fpage>218</fpage>–<lpage>317</lpage>. <pub-id pub-id-type="doi">10.1186/s13068-018-1215-8</pub-id>
<pub-id pub-id-type="pmid">30127850</pub-id></mixed-citation>
    </ref>
    <ref id="B49">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taylor</surname><given-names>G. M.</given-names></name><name><surname>Heap</surname><given-names>J. T.</given-names></name></person-group> (<year>2020</year>). "<article-title>Combinatorial metabolic engineering platform enabling stable overproduction of lycopene from carbon dioxide by cyanobacteria</article-title>." <comment>BioRxiv</comment>.</mixed-citation>
    </ref>
    <ref id="B50">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Towsey</surname><given-names>M.</given-names></name><name><surname>Timms</surname><given-names>P.</given-names></name><name><surname>Hogan</surname><given-names>J.</given-names></name><name><surname>Mathews</surname><given-names>S. A.</given-names></name></person-group> (<year>2008</year>). <article-title>The cross-species prediction of bacterial promoters using a support vector machine</article-title>. <source>Comput. Biol. Chem.</source>
<volume>32</volume> (<issue>5</issue>), <fpage>359</fpage>–<lpage>366</lpage>. <pub-id pub-id-type="doi">10.1016/j.compbiolchem.2008.07.009</pub-id>
<pub-id pub-id-type="pmid">18703385</pub-id></mixed-citation>
    </ref>
    <ref id="B51">
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tsai</surname><given-names>Y.-H. H.</given-names></name><name><surname>Bai</surname><given-names>S.</given-names></name><name><surname>Liang</surname><given-names>P. P.</given-names></name><name><surname>Kolter</surname><given-names>J. Z.</given-names></name><name><surname>Morency</surname><given-names>L.-P.</given-names></name><name><surname>Salakhutdinov</surname><given-names>R.</given-names></name></person-group> (<year>2019</year>). “<article-title>Multimodal transformer for unaligned multimodal language sequences</article-title>,” in <conf-name>Proceedings of The Conference Association for Computational Linguistics Meeting</conf-name>, <conf-date>17 November 2019</conf-date> (<publisher-loc>Italy</publisher-loc>: <publisher-name>NIH Public Access</publisher-name>), <fpage>6558</fpage>–<lpage>6569</lpage>.</mixed-citation>
    </ref>
    <ref id="B52">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaswani</surname><given-names>A.</given-names></name><name><surname>Shazeer</surname><given-names>N.</given-names></name><name><surname>Parmar</surname><given-names>N.</given-names></name><name><surname>Uszkoreit</surname><given-names>J.</given-names></name><name><surname>Jones</surname><given-names>L.</given-names></name><name><surname>Gomez</surname><given-names>A. N.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>Attention is all you need</article-title>. <source>Adv. Neural Inf. Process Syst.</source>
<volume>30</volume>, <fpage>15</fpage>.</mixed-citation>
    </ref>
    <ref id="B53">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vig</surname><given-names>J.</given-names></name></person-group> (<year>2019</year>). <article-title>BertViz: A tool for visualizing multihead self-attention in the BERT model</article-title>. <comment>arXiv</comment>.</mixed-citation>
    </ref>
    <ref id="B54">
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wolf</surname><given-names>T.</given-names></name><name><surname>Debut</surname><given-names>L.</given-names></name><name><surname>Sanh</surname><given-names>V.</given-names></name><name><surname>Chaumond</surname><given-names>J.</given-names></name><name><surname>Delangue</surname><given-names>C.</given-names></name><name><surname>Moi</surname><given-names>A.</given-names></name><etal/></person-group> (<year>2020</year>). “<article-title>Transformers: State-of-the-art natural language processing</article-title>,” in <conf-name>Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations</conf-name>, <conf-date>16 October 2020</conf-date> (<publisher-loc>Italy</publisher-loc>: <publisher-name>Association for Computational Linguistics</publisher-name>), <fpage>38</fpage>–<lpage>45</lpage>.</mixed-citation>
    </ref>
    <ref id="B55">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>B.</given-names></name><name><surname>Xu</surname><given-names>C.</given-names></name><name><surname>Dai</surname><given-names>X.</given-names></name><name><surname>Wan</surname><given-names>A.</given-names></name><name><surname>Zhang</surname><given-names>P.</given-names></name><name><surname>Yan</surname><given-names>Z.</given-names></name><etal/></person-group> (<year>2020</year>). "<article-title>Visual transformers: Token-based image representation and processing for computer vision</article-title>." <comment><italic>arXiv preprint arXiv:2006.03677</italic></comment>.</mixed-citation>
    </ref>
    <ref id="B56">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>X.</given-names></name><name><surname>Xu</surname><given-names>Z.-C.</given-names></name><name><surname>Qiu</surname><given-names>W.-R.</given-names></name><name><surname>Wang</surname><given-names>P.</given-names></name><name><surname>Ge</surname><given-names>H.-T.</given-names></name><name><surname>Chou</surname><given-names>K.-C.</given-names></name></person-group> (<year>2019</year>). <article-title>iPSW (2L)-PseKNC: a two-layer predictor for identifying promoters and their strength by hybrid features via pseudo K-tuple nucleotide composition</article-title>. <source>Genomics</source>
<volume>111</volume> (<issue>6</issue>), <fpage>1785</fpage>–<lpage>1793</lpage>. <pub-id pub-id-type="doi">10.1016/j.ygeno.2018.12.001</pub-id>
<pub-id pub-id-type="pmid">30529532</pub-id></mixed-citation>
    </ref>
    <ref id="B57">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Z.</given-names></name><name><surname>Dai</surname><given-names>Z.</given-names></name><name><surname>Yang</surname><given-names>Y.</given-names></name><name><surname>Carbonell</surname><given-names>J.</given-names></name><name><surname>Salakhutdinov</surname><given-names>R. R.</given-names></name><name><surname>Le</surname><given-names>Q. V.</given-names></name></person-group> (<year>2019</year>). <article-title>Xlnet: Generalized autoregressive pretraining for language understanding</article-title>. <source>Adv. Neural Inf. Process Syst.</source>
<volume>32</volume>, <fpage>1</fpage>–<lpage>10</lpage>.</mixed-citation>
    </ref>
    <ref id="B58">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>J.</given-names></name><name><surname>Li</surname><given-names>J.</given-names></name><name><surname>Yu</surname><given-names>Z.</given-names></name><name><surname>Huang</surname><given-names>Q.</given-names></name></person-group> (<year>2019</year>). <article-title>Multimodal transformer with multi-view visual representation for image captioning</article-title>. <source>IEEE Trans. Circuits Syst. Video Technol.</source>
<volume>30</volume> (<issue>12</issue>), <fpage>4467</fpage>–<lpage>4480</lpage>. <pub-id pub-id-type="doi">10.1109/tcsvt.2019.2947482</pub-id>
</mixed-citation>
    </ref>
    <ref id="B59">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>M.</given-names></name><name><surname>Jia</surname><given-names>C.</given-names></name><name><surname>Li</surname><given-names>F.</given-names></name><name><surname>Li</surname><given-names>C.</given-names></name><name><surname>Zhu</surname><given-names>Y.</given-names></name><name><surname>Akutsu</surname><given-names>T.</given-names></name><etal/></person-group> (<year>2022</year>). <article-title>Critical assessment of computational tools for prokaryotic and eukaryotic promoter prediction</article-title>. <source>Brief. Bioinform.</source>
<volume>23</volume> (<issue>2</issue>), <fpage>bbab551</fpage>. <pub-id pub-id-type="doi">10.1093/bib/bbab551</pub-id>
<pub-id pub-id-type="pmid">35021193</pub-id></mixed-citation>
    </ref>
    <ref id="B60">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>M.</given-names></name><name><surname>Li</surname><given-names>F.</given-names></name><name><surname>Marquez-Lago</surname><given-names>T. T.</given-names></name><name><surname>Leier</surname><given-names>A.</given-names></name><name><surname>Fan</surname><given-names>C.</given-names></name><name><surname>Kwoh</surname><given-names>C. K.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>MULTiPly: A novel multi-layer predictor for discovering general and specific types of promoters</article-title>. <source>Bioinformatics</source>
<volume>35</volume> (<issue>17</issue>), <fpage>2957</fpage>–<lpage>2965</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btz016</pub-id>
<pub-id pub-id-type="pmid">30649179</pub-id></mixed-citation>
    </ref>
    <ref id="B61">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>D.</given-names></name><name><surname>Kang</surname><given-names>B.</given-names></name><name><surname>Jin</surname><given-names>X.</given-names></name><name><surname>Yang</surname><given-names>L.</given-names></name><name><surname>Lian</surname><given-names>X.</given-names></name><name><surname>Jiang</surname><given-names>Z.</given-names></name><etal/></person-group> (<year>2021</year>). "<article-title>Deepvit: Towards deeper vision transformer</article-title>." <comment><italic>arXiv preprint arXiv:2103.11886</italic></comment>.</mixed-citation>
    </ref>
    <ref id="B62">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>Y.</given-names></name><name><surname>Li</surname><given-names>F.</given-names></name><name><surname>Xiang</surname><given-names>D.</given-names></name><name><surname>Akutsu</surname><given-names>T.</given-names></name><name><surname>Song</surname><given-names>J.</given-names></name><name><surname>Jia</surname><given-names>C.</given-names></name></person-group> (<year>2021</year>). <article-title>Computational identification of eukaryotic promoters based on cascaded deep capsule neural networks</article-title>. <source>Brief. Bioinform.</source>
<volume>22</volume> (<issue>4</issue>), <fpage>bbaa299</fpage>. <pub-id pub-id-type="doi">10.1093/bib/bbaa299</pub-id>
<pub-id pub-id-type="pmid">33227813</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
