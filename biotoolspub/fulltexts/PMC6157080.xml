<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6157080</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/bty307</article-id>
    <article-id pub-id-type="publisher-id">bty307</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Papers</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Sequence Analysis</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title><italic>In silico</italic> read normalization using set multi-cover optimization</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Durai</surname>
          <given-names>Dilip A</given-names>
        </name>
        <xref ref-type="aff" rid="bty307-aff1">1</xref>
        <xref ref-type="aff" rid="bty307-aff2">2</xref>
        <xref ref-type="aff" rid="bty307-aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Schulz</surname>
          <given-names>Marcel H</given-names>
        </name>
        <xref ref-type="aff" rid="bty307-aff1">1</xref>
        <xref ref-type="aff" rid="bty307-aff2">2</xref>
        <xref ref-type="corresp" rid="bty307-cor1"/>
        <!--<email>mschulz@mmci.uni-saarland.de</email>-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Berger</surname>
          <given-names>Bonnie</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <aff id="bty307-aff1"><label>1</label>Cluster of Excellence on Multimodal Computing and Interaction, Saarland University, Saarbrücken, Germany</aff>
    <aff id="bty307-aff2"><label>2</label>Department of Computational Biology and Applied Algorithmics, Max Planck Institute for Informatics, Saarbrücken, Germany</aff>
    <aff id="bty307-aff3"><label>3</label>Saarbrücken Graduate School of Computer Science, Saarland University, Saarbrücken, Germany</aff>
    <author-notes>
      <corresp id="bty307-cor1">To whom correspondence should be addressed. <email>mschulz@mmci.uni-saarland.de</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <day>01</day>
      <month>10</month>
      <year>2018</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2018-04-18">
      <day>18</day>
      <month>4</month>
      <year>2018</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>18</day>
      <month>4</month>
      <year>2018</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>34</volume>
    <issue>19</issue>
    <fpage>3273</fpage>
    <lpage>3280</lpage>
    <history>
      <date date-type="received">
        <day>19</day>
        <month>6</month>
        <year>2017</year>
      </date>
      <date date-type="rev-recd">
        <day>16</day>
        <month>3</month>
        <year>2018</year>
      </date>
      <date date-type="accepted">
        <day>18</day>
        <month>4</month>
        <year>2018</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2018. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2018</copyright-year>
      <license license-type="cc-by-nc" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="bty307.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>De Bruijn graphs are a common assembly data structure for sequencing datasets. But with the advances in sequencing technologies, assembling high coverage datasets has become a computational challenge. Read normalization, which removes redundancy in datasets, is widely applied to reduce resource requirements. Current normalization algorithms, though efficient, provide no guarantee to preserve important <italic>k</italic>-mers that form connections between regions in the graph.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>Here, normalization is phrased as a <italic>set multi-cover problem</italic> on reads and a heuristic algorithm, Optimized Read Normalization Algorithm (ORNA), is proposed. ORNA normalizes to the minimum number of reads required to retain all <italic>k</italic>-mers and their relative <italic>k</italic>-mer abundances from the original dataset. Hence, all connections from the original graph are preserved. ORNA was tested on various RNA-seq datasets with different coverage values. It was compared to the current normalization algorithms and was found to be performing better. Normalizing error corrected data allows for more accurate assemblies compared to the normalized uncorrected dataset. Further, an application is proposed in which multiple datasets are combined and normalized to predict novel transcripts that would have been missed otherwise. Finally, ORNA is a general purpose normalization algorithm that is fast and significantly reduces datasets with loss of assembly quality in between [1, 30]% depending on reduction stringency.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>ORNA is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/SchulzLab/ORNA">https://github.com/SchulzLab/ORNA</ext-link>.</p>
      </sec>
      <sec id="s4">
        <title>Supplementary information</title>
        <p><xref ref-type="supplementary-material" rid="sup1">Supplementary data</xref> are available at <italic>Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">Cluster of Excellence on Multi-modal Computing and Interaction</named-content>
        </funding-source>
        <award-id>EXC284</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">German National Science Foundation</named-content>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">International Max Planck Research School for Computer Science, Saarbrücken</named-content>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="8"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>With increasing throughput and decreasing prices of modern sequencers, the generation of high coverage sequencing datasets has become routine. This has spurred the development of a number of different approaches for the <italic>de novo</italic> assembly of genomes and transcriptomes (<xref rid="bty307-B23" ref-type="bibr">Miller <italic>et al.</italic>, 2010</xref>; <xref rid="bty307-B24" ref-type="bibr">Moreton <italic>et al.</italic>, 2016</xref>). However, assembling a large genome or a transcriptome is a resource-intensive task.</p>
    <p>Due to the large size of the datasets one particular line of research has focused on making data structures for <italic>de novo</italic> assembly more space-efficient for one or several datasets as applied to genome and metagenome sequencing (<xref rid="bty307-B6" ref-type="bibr">Chikhi <italic>et al.</italic>, 2016</xref>; <xref rid="bty307-B26" ref-type="bibr">Pell <italic>et al.</italic>, 2012</xref>). Another approach called <italic>compressed genomics</italic> deals with finding a compressed representation of the dataset to speedup computations, which was successfully applied to read alignment and SNP calling (<xref rid="bty307-B3" ref-type="bibr">Berger <italic>et al.</italic>, 2013</xref>; <xref rid="bty307-B19" ref-type="bibr">Loh <italic>et al.</italic>, 2012</xref>).</p>
    <p>Here, we investigate how data reduction approaches affect the performance of non-uniform RNA-seq datasets for the task of <italic>de novo</italic> transcriptome assembly. This is an important problem as current assembly methods, which rely on the de Bruijn graph (DBG), consume a lot of main memory (<xref rid="bty307-B12" ref-type="bibr">Grabherr <italic>et al.</italic>, 2011</xref>; <xref rid="bty307-B29" ref-type="bibr">Robertson <italic>et al.</italic>, 2010</xref>; <xref rid="bty307-B31" ref-type="bibr">Schulz <italic>et al.</italic>, 2012</xref>). However, it is also an interesting question from the perspective of information theory. Which parts of the data are actually being used by the assembler?</p>
    <p>A simple approach to remove sequencing errors and reduce data size is to trim read suffixes and prefixes that are of low quality. This generally leads to decreased assembly performance (<xref rid="bty307-B20" ref-type="bibr">MacManes, 2014</xref>; <xref rid="bty307-B21" ref-type="bibr">Mbandi <italic>et al.</italic>, 2014</xref>) and does not address the high redundancy of current read datasets. Other approaches allow the efficient correction of sequencing errors in RNA-seq datasets, which generally leads to an improved assembly performance (<xref rid="bty307-B16" ref-type="bibr">Le <italic>et al.</italic>, 2013</xref>; <xref rid="bty307-B32" ref-type="bibr">Song and Florea, 2015</xref>), albeit at increased runtime because the data has to be error corrected (EC) first.</p>
    <p>A direct approach to remove redundancy is to cluster reads according to sequence similarity using algorithms like CD-HIT (<xref rid="bty307-B11" ref-type="bibr">Fu <italic>et al.</italic>, 2012</xref>) and remove highly similar reads in the clusters. Though recent improvements have been made for clustering assembled transcripts (<xref rid="bty307-B33" ref-type="bibr">Srivastava <italic>et al.</italic>, 2016</xref>), clustering hundred millions of reads before assembly is still challenging.</p>
    <p>Another widely used approach, in particular in combination with assembly, has been ‘digital normalization’ (Diginorm; <xref rid="bty307-B4" ref-type="bibr">Brown <italic>et al.</italic>, 2012</xref>), implemented in the khmer package (<xref rid="bty307-B7" ref-type="bibr">Crusoe <italic>et al.</italic>, 2015</xref>). Diginorm uses a min-count-sketch data structure to estimate <italic>k</italic>-mer abundance while streaming through the read dataset. Using a user-selected abundance threshold <italic>t</italic>, reads are removed once their median <italic>k</italic>-mer coverage goes beyond <italic>t</italic>. An idea similar to this is Trinity’s <italic>in silico</italic> normalization (TIS) which is a part of the Trinity assembler package (<xref rid="bty307-B13" ref-type="bibr">Haas <italic>et al.</italic>, 2013</xref>). For each read in the dataset, TIS computes the median coverage of the <italic>k</italic>-mers in the read. If the median coverage is less than the desired coverage, the read is always kept. Otherwise, it is kept with a probability which is equal to the ratio of the desired coverage and the median coverage. Additionally, a read is removed if the ratio of SD of <italic>k</italic>-mer coverage to the average <italic>k</italic>-mer coverage of the read is higher than a cutoff. The recently developed NeatFreq algorithm (<xref rid="bty307-B22" ref-type="bibr">McCorrison <italic>et al.</italic>, 2014</xref>) clusters the read into bins based on median <italic>k</italic>-mer frequency.</p>
    <p>The advantage of <italic>k</italic>-mer coverage based normalization is 3-fold: (i) reads with high redundancy are removed leading to reduced memory and runtime requirements for the assembly, (ii) erroneous reads may be removed as part of the process and (iii) normalization is fast and consumes only a fraction of the memory an assembler would take. This essentially lowers the computational complexity of the assembly problem as it was shown that often a large part of the data can be removed, without significantly affecting assembly performance (<xref rid="bty307-B4" ref-type="bibr">Brown <italic>et al.</italic>, 2012</xref>; <xref rid="bty307-B13" ref-type="bibr">Haas <italic>et al.</italic>, 2013</xref>). However, previous algorithms do not give any certainty on preserving important parts of the data containing useful <italic>k</italic>-mers. Reads that contain low-abundant but important <italic>k</italic>-mers may be removed. This might result in losing connections in the DBG and hence a fragmented assembly might be generated. This is especially problematic for sequencing datasets with non-uniform coverage, like RNA-seq and metagenomics.</p>
    <p>Here, the Optimized Read Normalization Algorithm (ORNA) is suggested based on the idea that reads are reduced without losing the DBG backbone (unweighted nodes and edges) and relative node abundances are preserved in the reduced dataset as compared to the original DBG. Given a set of <italic>n</italic> reads, where each read consists of <italic>m k</italic>-mers, read normalization is phrased as a set multi-cover (SMC) optimization problem on reads. In this work, a <inline-formula id="IE1"><mml:math id="IM1"><mml:mrow><mml:mi mathvariant="script">O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mi>m</mml:mi><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> time heuristic algorithm is suggested that is shown to work well in practice. Analysis of normalized and EC data reveal that better assemblies can be produced with significant savings in runtime and memory consumption. The software is freely available at <ext-link ext-link-type="uri" xlink:href="https://github.com/SchulzLab/ORNA">https://github.com/SchulzLab/ORNA</ext-link> under an MIT license.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Problem formulation</title>
      <p>A dataset <inline-formula id="IE2"><mml:math id="IM2"><mml:mrow><mml:mi mathvariant="script">R</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> is a set of <italic>n</italic> reads where each read is a sequence of DNA bases of fixed length <italic>s</italic>. Each read consists of a set of short words (<italic>k</italic>-mers) of length <italic>k</italic>. Most of the <italic>de novo</italic> assemblers start by constructing a DBG. <italic>k</italic>-mers obtained from all the reads in <inline-formula id="IE3"><mml:math id="IM3"><mml:mi mathvariant="script">R</mml:mi></mml:math></inline-formula> are considered as vertices. Two vertices are connected by an edge if they overlap by <italic>k</italic> – 1 bases. Each edge is identified by a unique label <italic>l</italic> of length <italic>k</italic> + 1, such that the source vertex is a prefix of <italic>l</italic> and the destination vertex is a suffix of <italic>l</italic>. Since the labels are also generated from the reads in the dataset, each read <inline-formula id="IE4"><mml:math id="IM4"><mml:mrow><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">R</mml:mi></mml:mrow></mml:math></inline-formula> can be considered as a set of <inline-formula id="IE5"><mml:math id="IM5"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> labels, i.e. <inline-formula id="IE6"><mml:math id="IM6"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where <italic>l<sub>i</sub></italic> is a <italic>k</italic> + 1-mer obtained from the <italic>i</italic>th position in read <italic>r</italic>.</p>
      <p>The objective of a normalization algorithm is to reduce the data as much as possible without having significant impact on the quality of the assembly produced. Since an assembly is produced by traversing paths in the DBG, it may be worthwhile that a new DBG build from the normalized dataset preserves all the (unweighted) nodes and edges of the original graph. Further, normalization should maintain the relative difference of abundance between <italic>k</italic>-mers to resolve complex graph structures.</p>
      <p>Here, we suggest to phrase read normalization as a SMC problem, defined as follows:</p>
      <p>
        <disp-quote content-type="extract">
          <p><bold>Instance:</bold> A dataset <inline-formula id="IE7"><mml:math id="IM7"><mml:mi mathvariant="script">R</mml:mi></mml:math></inline-formula> of <italic>n</italic> reads, a set of <italic>k</italic> + 1-mers (defined as labels) <italic>L</italic> obtained from <inline-formula id="IE8"><mml:math id="IM8"><mml:mi mathvariant="script">R</mml:mi></mml:math></inline-formula> such that <inline-formula id="IE9"><mml:math id="IM9"><mml:mrow><mml:munder><mml:mo>∪</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">R</mml:mi></mml:mrow></mml:munder><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> and a weight <inline-formula id="IE10"><mml:math id="IM10"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>≥</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for every <inline-formula id="IE11"><mml:math id="IM11"><mml:mrow><mml:mi>l</mml:mi><mml:mo>∈</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula>. Note that, each read <inline-formula id="IE12"><mml:math id="IM12"><mml:mrow><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">R</mml:mi></mml:mrow></mml:math></inline-formula> is considered here as a set of labels.</p>
          <p><bold>Valid solutions:</bold><inline-formula id="IE13"><mml:math id="IM13"><mml:mrow><mml:mi mathvariant="script">R</mml:mi><mml:mo>′</mml:mo><mml:mo>⊆</mml:mo><mml:mi mathvariant="script">R</mml:mi></mml:mrow></mml:math></inline-formula> such that <inline-formula id="IE14"><mml:math id="IM14"><mml:mrow><mml:munder><mml:mo>∪</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:mi>R</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:munder><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE15"><mml:math id="IM15"><mml:mrow><mml:mo>∀</mml:mo><mml:mi>l</mml:mi><mml:mo>∈</mml:mo><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mtext>abund</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">R</mml:mi><mml:mo>′</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>≥</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> where <inline-formula id="IE16"><mml:math id="IM16"><mml:mrow><mml:mtext>abund</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">R</mml:mi><mml:mo>′</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> denotes the number of occurrences of <italic>l</italic> in <inline-formula id="IE17"><mml:math id="IM17"><mml:mrow><mml:mi mathvariant="script">R</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula>.</p>
          <p><bold>Objective:</bold><inline-formula id="IE18"><mml:math id="IM18"><mml:mrow><mml:mtext>Minimize</mml:mtext><mml:mo> </mml:mo><mml:mo>|</mml:mo><mml:mi mathvariant="script">R</mml:mi><mml:mo>′</mml:mo><mml:mo>|</mml:mo></mml:mrow></mml:math></inline-formula>.</p>
        </disp-quote>
      </p>
      <p>This SMC formulation seeks to find the smallest set of reads, that covers all labels and satisfies all label weights <italic>w<sub>l</sub></italic>. The SMC problem was shown to be a NP-hard problem (<xref rid="bty307-B5" ref-type="bibr">Chekuri <italic>et al.</italic>, 2012</xref>). A common approximation approach for the SMC problem, is the following greedy approach: an element in the universe is termed as <italic>active</italic> if it has not yet been covered by any of the selected sets. Cost-effectiveness of a set, to be considered for selection, is measured in terms of the number of <italic>active</italic> elements present in the set. The algorithm would iterate over sets and select the one which is the most cost-effective. In the scope of this work, each read represents a set of labels. Thus, a data structure has to be maintained that holds the reads in an order starting from the one which has the largest number of <italic>active</italic> labels. This order has to be updated after every iteration. Thus, given a dataset containing <italic>n</italic> reads, the greedy approach would take <inline-formula id="IE19"><mml:math id="IM19"><mml:mrow><mml:mi mathvariant="script">O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>m</mml:mi><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> time, where <italic>m</italic> is the number of labels in a read. The estimate is under the assumption that the reads are sorted by cost-effectiveness using a binary heap or a similar data structure, which is not efficient enough for the large datasets considered here.</p>
    </sec>
    <sec>
      <title>2.2 ORNA</title>
      <p>In this work, a different heuristic algorithm based on a greedy read selection strategy is used, in which the ordering of reads based on cost-effectiveness is ignored to save runtime. The approach is summarized in <xref ref-type="boxed-text" rid="bty307-BOX1">algorithm 1</xref>. A set <inline-formula id="IE20"><mml:math id="IM20"><mml:mi mathvariant="script">R</mml:mi></mml:math></inline-formula> of <italic>n</italic> reads and a <italic>k</italic>-mer size <italic>k</italic> is given as input. Each read in <inline-formula id="IE21"><mml:math id="IM21"><mml:mi mathvariant="script">R</mml:mi></mml:math></inline-formula> consists of <italic>m</italic> labels. Since each edge label is a <italic>k</italic> + 1-mer, all the possible <inline-formula id="IE22"><mml:math id="IM22"><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>-mers are obtained from the reads and are stored in a bloom filter using the <italic>BuildBloom</italic> (<inline-formula id="IE23"><mml:math id="IM23"><mml:mrow><mml:mi mathvariant="script">R</mml:mi><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula>) function (line 3). This functionality is implemented using the GATB library (<xref rid="bty307-B9" ref-type="bibr">Drezen <italic>et al.</italic>, 2014</xref>), which uses the <italic>BBHash</italic> algorithm for building a <italic>minimal perfect hash function</italic> (<xref rid="bty307-B18" ref-type="bibr">Limasset <italic>et al.</italic>, 2017</xref>) after counting the <italic>k</italic>-mers with the DSK algorithm (<xref rid="bty307-B28" ref-type="bibr">Rizk <italic>et al.</italic>, 2013</xref>). <italic>k</italic>-mer counting and storing the information requires <inline-formula id="IE24"><mml:math id="IM24"><mml:mrow><mml:mi mathvariant="script">O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mi>m</mml:mi><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> time. A counter array <italic>NodeCounter</italic> is maintained for each entry in the bloom filter and is initialized to zero (line 5). This operation requires <inline-formula id="IE25"><mml:math id="IM25"><mml:mrow><mml:mi mathvariant="script">O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> time.</p>
      <p>The dataset is then iterated and each read in the dataset is checked whether it contains a <inline-formula id="IE26"><mml:math id="IM26"><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula>-mer that needs to be covered. This is done by first collecting a set <inline-formula id="IE27"><mml:math id="IM27"><mml:mrow><mml:mi>V</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula> of all the <inline-formula id="IE28"><mml:math id="IM28"><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula>-mers in the read using the <inline-formula id="IE29"><mml:math id="IM29"><mml:mrow><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>K</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>′</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> function (line 8). For each <inline-formula id="IE30"><mml:math id="IM30"><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula>-mer in <inline-formula id="IE31"><mml:math id="IM31"><mml:mrow><mml:mi>V</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula>, the corresponding weight is calculated using the <italic>ObtainWeight</italic> function (line 11). The node counter for the <inline-formula id="IE32"><mml:math id="IM32"><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula>-mer is then checked and incremented if its current value is less than the given weight (line 12–15). A read is accepted if it contains at-least one <inline-formula id="IE33"><mml:math id="IM33"><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula>-mer for which the corresponding counter is incremented otherwise it is rejected (line 17–19). Steps 7–20 require <inline-formula id="IE34"><mml:math id="IM34"><mml:mrow><mml:mi mathvariant="script">O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> time. All the accepted reads then denote the normalized dataset <inline-formula id="IE35"><mml:math id="IM35"><mml:mrow><mml:msub><mml:mi mathvariant="script">R</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> obtained from <inline-formula id="IE36"><mml:math id="IM36"><mml:mi mathvariant="script">R</mml:mi></mml:math></inline-formula>. The overall time complexity of the algorithm is <inline-formula id="IE37"><mml:math id="IM37"><mml:mrow><mml:mi mathvariant="script">O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mi>m</mml:mi><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.
<boxed-text id="bty307-BOX1" position="float" orientation="portrait"><label>Algorithm 1 </label><caption><p>SMC based approach for read normalization</p></caption><p>1: <bold>Input:</bold> Reads <inline-formula id="IE38"><mml:math id="IM38"><mml:mi mathvariant="script">R</mml:mi></mml:math></inline-formula>, <italic>k</italic>mer size <italic>k</italic></p><p>2: <inline-formula id="IE39"><mml:math id="IM39"><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula></p><p>3: <italic>V </italic>=<italic> </italic>BuildBloom(<inline-formula id="IE40"><mml:math id="IM40"><mml:mrow><mml:mi mathvariant="script">R</mml:mi><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula>)</p><p>4: <italic>t </italic>=<italic> </italic>NumberOf<italic>k</italic>mers(<italic>V</italic>)</p><p>5: NodeCounter[1…<italic>t</italic>]<inline-formula id="IE41"><mml:math id="IM41"><mml:mo>←</mml:mo></mml:math></inline-formula>0</p><p>6: <inline-formula id="IE42"><mml:math id="IM42"><mml:mrow><mml:msub><mml:mi mathvariant="script">R</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>ϕ</mml:mo></mml:mrow></mml:math></inline-formula></p><p>7: <bold>for all</bold><inline-formula id="IE43"><mml:math id="IM43"><mml:mrow><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula><bold>do</bold></p><p>8:   <inline-formula id="IE44"><mml:math id="IM44"><mml:mrow><mml:mi>V</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula> = ObtainKmers(<italic>r</italic>,<inline-formula id="IE45"><mml:math id="IM45"><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula>)          ▹ <inline-formula id="IE46"><mml:math id="IM46"><mml:mrow><mml:mi>V</mml:mi><mml:mo>′</mml:mo><mml:mo>⊆</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:math></inline-formula></p><p>9:   flag = 0</p><p>10:    <bold>for all</bold><inline-formula id="IE47"><mml:math id="IM47"><mml:mrow><mml:mi>v</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula><bold>do</bold></p><p>11:     <italic>w </italic>=<italic> </italic>ObtainWeight(<italic>v</italic>, <inline-formula id="IE48"><mml:math id="IM48"><mml:mi mathvariant="script">R</mml:mi></mml:math></inline-formula>)</p><p>12:     <bold>if</bold><italic>NodeCounter</italic>(<italic>v</italic>) &lt; <italic>w</italic><bold>then</bold></p><p>13:      NodeCounter(<italic>v</italic>) = NodeCounter(<italic>v</italic>) + 1</p><p>14:      flag = 1</p><p>15:     <bold>end if</bold></p><p>16:    <bold>end for</bold></p><p>17:    <bold>if</bold><inline-formula id="IE49"><mml:math id="IM49"><mml:mrow><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula><bold>then</bold></p><p>18:     <inline-formula id="IE50"><mml:math id="IM50"><mml:mrow><mml:msub><mml:mi mathvariant="script">R</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">R</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub><mml:mo>∪</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula></p><p>19:    <bold>end if</bold></p><p>20: <bold>end for</bold></p><p>21: <bold>Output:</bold> Reads <inline-formula id="IE51"><mml:math id="IM51"><mml:mrow><mml:msub><mml:mi mathvariant="script">R</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></p></boxed-text></p>
      <p>An important parameter for the algorithm is the weight for each <inline-formula id="IE52"><mml:math id="IM52"><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula>-mer. A naive way to decide the weight for each <inline-formula id="IE53"><mml:math id="IM53"><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula>-mer is to set the same value for each. A DBG based assembler uses <italic>k</italic>-mer abundance information to resolve irregularities in the graph such as bubbles and tips. This information is also used for RNA-seq data to decide which transcripts should be reported. Hence, it is important to retain the relative difference in abundance between <italic>k</italic>-mers, which cannot be achieved with a fixed weight for all <inline-formula id="IE54"><mml:math id="IM54"><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula>-mers. Therefore, each <inline-formula id="IE55"><mml:math id="IM55"><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula>-mer has its own weight <inline-formula id="IE56"><mml:math id="IM56"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, which is defined as:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mo>∀</mml:mo><mml:mi>k</mml:mi><mml:mo>′</mml:mo><mml:mo>∈</mml:mo><mml:mi>V</mml:mi><mml:mo>′</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">⌈</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mtext>log</mml:mtext></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mtext>abund</mml:mtext><mml:mo> </mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>′</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="script">R</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">⌉</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE57"><mml:math id="IM57"><mml:mrow><mml:mtext>abund</mml:mtext><mml:mo> </mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>′</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="script">R</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the abundance of <inline-formula id="IE58"><mml:math id="IM58"><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula> in <inline-formula id="IE59"><mml:math id="IM59"><mml:mi mathvariant="script">R</mml:mi></mml:math></inline-formula>. <italic>b</italic> is the base of the logarithm function and is given by the user.</p>
    </sec>
    <sec>
      <title>2.3 Normalization for paired-end data</title>
      <p>For paired-end (PE) datasets ORNA is run in two passes. As in the single end mode, the dataset is iterated sequentially and only one pair is evaluated at a time. In the first pass, reads of a pair are checked for their acceptance. The pair is accepted only if both the reads of the pair satisfy the acceptance condition, i.e. there is at least one <inline-formula id="IE60"><mml:math id="IM60"><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula>-mer in both the reads which needs to be covered in the normalized dataset. If only one read of the pair is satisfying the condition then the pair is <italic>marked</italic>, otherwise the pair is rejected. The counter for the <inline-formula id="IE61"><mml:math id="IM61"><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula>-mers of <italic>marked</italic> pairs are not incremented at this point. Thus, the first pass might leave some <inline-formula id="IE62"><mml:math id="IM62"><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula>-mers not covered in the normalized dataset. To cover such <inline-formula id="IE63"><mml:math id="IM63"><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula>-mers, in the second pass, the <italic>marked</italic> pairs are again checked for acceptance and a pair is accepted if one of the reads satisfies the acceptance condition.</p>
    </sec>
    <sec>
      <title>2.4 Data retrieval and normalization</title>
      <p>Seven different datasets were used for the analyses shown in this work. Two datasets were downloaded from the SRA database- Brain dataset (<xref rid="bty307-B2" ref-type="bibr">Barbosa-Morais <italic>et al.</italic>, 2012</xref>, SRR332171) which consists of 147 M PE reads with read length 50bps and hESC dataset (<xref rid="bty307-B1" ref-type="bibr">Au <italic>et al.</italic>, 2013</xref>, SRR1020625) which has 142 M PE reads with length 50 bps. The datasets were EC, unless otherwise stated, using SEECER version 0.2 (<xref rid="bty307-B16" ref-type="bibr">Le <italic>et al.</italic>, 2013</xref>) with default parameters. Further, a <italic>combined dataset</italic> of 883 M reads of length 76 bps was obtained by concatenating five ENCODE datasets: 101 M PE reads from hESC (GSM758573), 192 M PE reads from AG04450 (GSM765396), 207 M PE reads from GM12878 (GSM758572), 165 M PE reads from A549 (GSM767854) and 216 M PE reads from HeLa (GSM767847). The datasets were then normalized using ORNA (v0.2), Diginorm (v2.0) and TIS (v2.4.0) with parameter settings (see <xref ref-type="supplementary-material" rid="sup1">Supplementary Tables S1– S3</xref> for parameter settings).</p>
    </sec>
    <sec>
      <title>2.5 Transcriptome assembly and evaluation</title>
      <p>To analyse the quality of the assembly produced from the reduced datasets, three DBG based <italic>de novo</italic> assemblers–Oases (<xref rid="bty307-B31" ref-type="bibr">Schulz <italic>et al.</italic>, 2012</xref>, version 0.2.08), TransABySS (<xref rid="bty307-B29" ref-type="bibr">Robertson <italic>et al.</italic>, 2010</xref>, version 1.5.3) and Trinity (<xref rid="bty307-B12" ref-type="bibr">Grabherr <italic>et al.</italic>, 2011</xref>, version 2.4.0) were used. The assemblers were run with default parameters except the <italic>k</italic>-mer parameter of TransABySS and Oases. TransABySS was run using a single <italic>k</italic>-mer size (<italic>k </italic>=<italic> </italic>21). Oases was run using multiple <italic>k</italic>-mer sizes (<italic>k </italic>=<italic> </italic>21 to <italic>k </italic>=<italic> </italic>49 with an increment of 2) and the assemblies obtained from all <italic>k</italic>-mer sizes were merged using the Oases merge-script.</p>
      <p>The assembled transcripts were evaluated using the REF-EVAL program, which is part of the Detonate (<xref rid="bty307-B17" ref-type="bibr">Li <italic>et al.</italic>, 2014</xref>, version 1.11) package, which is explained briefly. First, a <italic>true assembly</italic> for each dataset (Brain and hESC) was estimated, which estimated all regions from the reference transcript sequences (Ensembl) overlapped by reads. Second, the <italic>true assembly</italic> was bidirectionally aligned against the generated assemblies using Blat (<xref rid="bty307-B15" ref-type="bibr">Kent, 2002</xref>, version 36) and nucleotide precision and recall was obtained. REF-EVAL then reports the F1 score, which is the harmonic mean of the nucleotide precision and recall, as a measure of assembly accuracy. To measure the assembly contiguity, the assembled transcripts were aligned against a reference genome using Blat and the overlap was matched against annotated Ensembl transcripts downloaded from the Ensembl database (<xref rid="bty307-B8" ref-type="bibr">Cunningham <italic>et al.</italic>, 2015</xref>, version 65). The number of Ensembl transcripts that were fully assembled by at least one distinct assembled transcript was obtained and termed as <italic>full-length transcripts</italic>.</p>
      <p>For the analysis of the <italic>combined</italic> dataset, ORNA was used for normalization. The goal of the analysis was to determine how many transcripts are missed by assembling individual datasets, but are assembled using the <italic>combined</italic> datasets. All assemblies for this experiment were run with TransABySS (<italic>k </italic>=<italic> </italic>21). The assembly generated by using all datasets was termed <italic>combined assembly</italic>. Individual dataset assemblies were clustered with the <italic>combined assembly</italic> using CD-HIT-EST (<xref rid="bty307-B11" ref-type="bibr">Fu <italic>et al.</italic>, 2012</xref>, v4.6.4-2015-0603). Similar sequences were clustered together (sequence identity = 99%). Hence, if a transcript in the <italic>combined assembly</italic> is also assembled by any of the individual datasets, then it would be clustered with the sequences assembled from that dataset. All clusters which contained only the sequences from the <italic>combined assembly</italic> were termed as <italic>missed clusters</italic> and the longest sequence of the cluster was considered a <italic>missed transcript</italic>. Aligned missed transcripts were compared to annotations from Ensembl (<xref rid="bty307-B8" ref-type="bibr">Cunningham <italic>et al.</italic>, 2015</xref>, version 65) and GENCODE (<xref rid="bty307-B14" ref-type="bibr">Harrow <italic>et al.</italic>, 2012</xref>, version 17).</p>
    </sec>
    <sec>
      <title>2.6 Correlation analysis</title>
      <p>Further, gene expression values of all RNA-seq datasets for Ensembl transcripts were obtained using Salmon (<xref rid="bty307-B25" ref-type="bibr">Patro <italic>et al.</italic>, 2017</xref>, v0.8.2). The <italic>k</italic>-mer parameter was set to 21 for quantification. Salmon provides quantification results at transcript level. To obtain the quantification at gene level, Transcripts Per Million (TPM) values of all the transcripts for a gene were summed. Spearman’s rank correlation values between original and reduced datasets were computed using the statistical computing language <italic>R</italic> (<xref rid="bty307-B27" ref-type="bibr">R Development Core Team, 2008</xref>, version 3.1.1).</p>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 Comparison to established normalization algorithms</title>
      <p>The task of a read normalization algorithm is to remove the maximum number of reads without compromising the quality of the assembly produced. Most of the <italic>de novo</italic> assemblers start by building a DBG using a fixed/variable <italic>k</italic>-mer size and generate assemblies by traversing paths in the graph. In order to maintain assembly quality, ORNA was designed to: (i) retain all nodes and their connections in the DBG and (ii) retain the relative abundance difference between <italic>k</italic>-mers in the reduced dataset.</p>
      <p>Diginorm and TIS normalization base their decision on the <italic>k</italic>-mer abundance distribution within individual reads. A read might get removed if the median abundance of <italic>k</italic>-mers present in the read exceeds a certain threshold. Hence a <italic>k</italic>-mer having low abundance, if present among high abundance <italic>k</italic>-mers in the read, is also removed. For instance, <xref ref-type="fig" rid="bty307-F1">Figure 1</xref> shows a toy DBG. Nodes in the graph represent <italic>k</italic>-mers and the abundance of each node is represented as a number inside the node. If the desired median abundance is 10 then read <italic>r</italic><sub>1</sub> covering node A, C, E and G would be removed since the median abundance of <italic>k</italic>-mers in <italic>r</italic><sub>1</sub> is 20. Although, this strategy may potentially remove erroneous <italic>k</italic>-mers, it might also result in the loss of true <italic>k</italic>-mers which form connections between nodes in the graph. For instance, removal of read <italic>r</italic><sub>1</sub> in the above example would remove the <italic>k</italic>-mer corresponding to node E (dashed node). Hence, the connection between node C and node G is lost, which possibly results in a fragmented assembly.
</p>
      <fig id="bty307-F1" orientation="portrait" position="float">
        <label>Fig. 1.</label>
        <caption>
          <p>A toy DBG with one traversing read shown denoted <italic>r</italic><sub>1</sub>. Nodes in the DBG represent <italic>k</italic>-mers, numbers inside nodes represent the abundance of the corresponding <italic>k</italic>-mer in the data. Dashed node represents the <italic>k</italic>-mer, which will be lost if <italic>r</italic><sub>1</sub> is removed</p>
        </caption>
        <graphic xlink:href="bty307f1"/>
      </fig>
      <p><bold>ORNA retains all the kmers from the original dataset</bold>. In this work, all the assemblies used for evaluation are generated by constructing a DBG, which uses a <italic>k</italic>-mer size of 21 (except for Trinity which always uses <italic>k</italic>-mer size 25). Hence the size of the edge label would be <italic>k</italic> + 1. <xref ref-type="fig" rid="bty307-F2">Figure 2a</xref> shows the retention of 22-mers in reduced versions of the brain dataset by ORNA, Diginorm and TIS. Diginorm and TIS loose 2–10% of the 22-mers. ORNA considers the normalization problem as a SMC problem. The set of all possible edge labels (of length <italic>k</italic> + 1) serves as the universe. A single read is considered as a set of <italic>k</italic> + 1-mers and a dataset is considered as a collection of such sets. ORNA selects the minimum number of reads, which is required to cover all the elements of the universe a certain number of times. Hence, all edge labels from the original dataset are retained.
</p>
      <fig id="bty307-F2" orientation="portrait" position="float">
        <label>Fig. 2.</label>
        <caption>
          <p>Comparison of <italic>k</italic>-mer information retained by ORNA, Diginorm and TIS. (<bold>a</bold>) Represents percentage of unique <italic>k</italic>-mers retained compared to the original brain dataset (<italic>x</italic>-axis) for various levels of reduction (<italic>y</italic>-axis) by the three algorithms. (<bold>b</bold>) and (<bold>c</bold>) Represent Spearman’s rank correlation values for TPM values obtained by quantifying expression of Ensembl genes using the unreduced and reduced brain dataset and hESC dataset, respectively</p>
        </caption>
        <graphic xlink:href="bty307f2"/>
      </fig>
      <p><bold>ORNA maintains the relative difference of abundance between <italic>k</italic>-mers</bold>. A <italic>de novo</italic> assembler generally uses the <italic>k</italic>-mer abundance information to resolve erroneous graph structures like bubbles and tips, among other things. For instance, in <xref ref-type="fig" rid="bty307-F1">Figure 1</xref>, assume that a bubble is formed by nodes C, D, E and F. An assembler would remove this by converting the less abundant path B-D-F-G to the higher abundant path B-C-E-G. Hence, it is important to maintain the relative abundance difference between the <italic>k</italic>-mers. ORNA uses the log<sub><italic>b</italic></sub> of the abundance of the connection (<italic>k</italic> + 1-mer) in the original dataset. This results in large reduction of highly abundant <italic>k</italic>-mers and little to no reduction of lowly abundant <italic>k</italic>-mers, maintaining relative abundance differences. <xref ref-type="fig" rid="bty307-F2">Figure 2b and c</xref> show the comparison of Spearman’s rank correlation values obtained between TPM values of the reduced and unreduced brain and hESC dataset, respectively. It can be seen that in all cases, the correlation is either similar or higher for ORNA reduced datasets as compared to using Diginorm and TIS for reduction. This indicates that for any % of reduction ORNA is able to better maintain the relative abundance of <italic>k</italic>-mers in genes compared to Diginorm and TIS.</p>
      <p><bold>Comparison of assembly performance</bold>. As mentioned in the above section a read normalization algorithm should not compromise on the quality of the assembly produced. But which quality measure should be used to evaluate the assemblies produced from the normalized datasets? REF-EVAL is a widely used program to evaluate transcriptome assemblies (<xref rid="bty307-B17" ref-type="bibr">Li <italic>et al.</italic>, 2014</xref>). For a given read set it estimates the accuracy of assembly using nucleotide-level F1 scores. The nucleotide F1 score judges an assembly by comparing its coverage of nucleotides with the reference, but it does not measure assembly contiguity (see Materials and methods Section). To achieve this, the number of reconstructed <italic>full-length</italic> transcripts, as determined by aligning the assembled transcripts to a reference sequence and comparing it with the existing gene annotation is used in this work, see Materials and methods Section. The total number of <italic>full-length</italic> transcripts obtained by running the assembler on the original unreduced dataset is considered as <italic>complete</italic>. The performance of a normalization algorithm is measured in terms of <italic>% of complete</italic>. For example, if assembling an unreduced dataset produces 2000 <italic>full-length</italic> transcripts and assembling a normalized dataset produces 1000 <italic>full-length</italic> transcripts, then it is considered that normalized data achieved <italic>50% of complete</italic>. A normalization algorithm <italic>A</italic> is better than an alternative algorithm <italic>B</italic> if <italic>A</italic> achieves a higher <italic>% of complete</italic> with a similar or higher percentage of reads reduced compared to <italic>B</italic>.</p>
      <p>Performance of ORNA was compared against TIS and Diginorm, with effective <italic>k</italic>-mer value 22 on two different datasets, except for assemblies with Trinity were the effective <italic>k</italic>-mer value was 26. Notably, the different parameters of the three algorithms behave quite different with respect to the number of reads reduced in a data-dependent manner. Therefore, the parameters of all algorithms were varied to obtain various normalized datasets. These normalized datasets were then assembled using TransABySS and Trinity. A more challenging test is to use a multi-<italic>k</italic>mer based assembly strategy, where several DBGs are built for different <italic>k</italic>-mer sizes. Here, Oases was used to produce merged assemblies with DBG’s built with <italic>k</italic>-mers 21–49.</p>
      <p>First, the overall assembly quality in terms of nucleotide F1 scores was measured for each assembly using REF-EVAL. It was observed that with higher read reduction values, the nucleotide recall obtained from the corresponding assemblies reduced, but was balanced out by an increase in nucleotide precision (<xref ref-type="supplementary-material" rid="sup1">Supplementary Figs S1 and S2</xref>). Hence, for different normalization parameters, the nucleotide F1 scores were found to be stable and similar to each other although the amount of data reduction varied substantially. <xref rid="bty307-T1" ref-type="table">Table 1</xref> compares the average F1 scores obtained from assemblies generated for various normalization parameters. In most of the cases, ORNA has slightly better F1 scores as compared to other normalization algorithms, except for Oases assemblies of the brain dataset, where TIS performed best. For a similar percentage of reduction, assemblies generated from ORNA reduced datasets were found to have a better F1 score than Diginorm and TIS reduced datasets in most setups (<xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S3</xref>). Notably, ORNA average F1 scores were better (in three cases) or showed a reduction of less than 0.05 compared to the F1 score obtained with the unreduced dataset. This might be due to the fact that ORNA retains all <italic>k</italic>-mers from the original dataset and thus shows little to no loss for the F1 scores calculated on nucleotide level.
<table-wrap id="bty307-T1" orientation="portrait" position="float"><label>Table 1.</label><caption><p>Comparison of average F1 scores using REF-EVAL </p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col valign="top" align="left" span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/></colgroup><thead><tr><th rowspan="1" colspan="1">Method</th><th colspan="3" rowspan="1">Brain<hr/></th><th colspan="3" rowspan="1">hESC<hr/></th></tr><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1">Oases</th><th rowspan="1" colspan="1">TransABySS</th><th rowspan="1" colspan="1">Trinity</th><th rowspan="1" colspan="1">Oases</th><th rowspan="1" colspan="1">TransABySS</th><th rowspan="1" colspan="1">Trinity</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">unreduced</td><td rowspan="1" colspan="1">0.402</td><td rowspan="1" colspan="1">0.441</td><td rowspan="1" colspan="1">0.414</td><td rowspan="1" colspan="1">0.304</td><td rowspan="1" colspan="1">0.577</td><td rowspan="1" colspan="1">0.621</td></tr><tr><td rowspan="1" colspan="1">ORNA</td><td rowspan="1" colspan="1">0.404</td><td rowspan="1" colspan="1"><bold>0.440</bold></td><td rowspan="1" colspan="1"><bold>0.421</bold></td><td rowspan="1" colspan="1"><bold>0.302</bold></td><td rowspan="1" colspan="1"><bold>0.582</bold></td><td rowspan="1" colspan="1"><bold>0.616</bold></td></tr><tr><td rowspan="1" colspan="1">Diginorm</td><td rowspan="1" colspan="1">0.411</td><td rowspan="1" colspan="1">0.418</td><td rowspan="1" colspan="1">0.419</td><td rowspan="1" colspan="1">0.280</td><td rowspan="1" colspan="1">0.578</td><td rowspan="1" colspan="1">0.601</td></tr><tr><td rowspan="1" colspan="1">TIS</td><td rowspan="1" colspan="1"><bold>0.419</bold></td><td rowspan="1" colspan="1">0.437</td><td rowspan="1" colspan="1">0.413</td><td rowspan="1" colspan="1">0.283</td><td rowspan="1" colspan="1">0.579</td><td rowspan="1" colspan="1">0.599</td></tr></tbody></table><table-wrap-foot><fn id="tblfn1"><p><italic>Notes</italic>: Each entry in a cell denotes the average of F1 scores obtained by assembling brain and hESC datasets normalized by the three algorithms (ORNA, Diginorm and TIS) in the rows. Averages are taken over results obtained with several parameters for each algorithm. The F1 score obtained for the original (unreduced) dataset is shown in the first row. Columns denote the assembler used. The highest F1 score obtained comparing all reduction methods is highlighted per assembler.</p></fn></table-wrap-foot></table-wrap></p>
      <p>However, as mentioned above, the F1 score does not capture assembly contiguity and hence the amount of assembled known full-length transcripts was investigated. <xref ref-type="fig" rid="bty307-F3">Figure 3</xref> compares the amount of read reduction (<italic>x</italic>-axis) against the assembly performance as <italic>% of complete</italic> (<italic>y</italic>-axis). For all the cases, it is observed that the quality of the assembly degrades as more reads are being removed from the dataset with the exception of hESC data assembled with Trinity. For Brain (<xref ref-type="fig" rid="bty307-F3">Fig. 3a–c</xref>), all normalization algorithms perform similar at a lower percentage of reduction (60–80%). But at a higher percentage of reduction (80–90% for brain), the assembly performance for Diginorm and TIS reduced datasets degrades much faster than the assembly of ORNA normalized datasets. In other words, assemblies produced by ORNA reduced datasets retain equally many or more <italic>full-length</italic> hits for all assemblers tested. For the hESC dataset, the results were assembler-specific. A similar performance as before was observed for assemblies of hESC using TransABySS (<xref ref-type="fig" rid="bty307-F3">Fig. 3d</xref>). But for Trinity assemblies, some of the reduced datasets gave rise to more <italic>full-length</italic> assemblies compared to the original dataset, with a slight advantage of TIS and Diginorm compared to ORNA (<xref ref-type="fig" rid="bty307-F3">Fig. 3e</xref>). For Oases hESC assemblies, TIS was performing better than the other two approaches. This behavior was in contrast to the observations made from the F1 score analysis, where ORNA reduced datasets was always performing better. It was found that the nucleotide precision of ORNA reduced hESC datasets were better than Diginorm and TIS (<xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S2</xref>) for Oases and Trinity assemblers. But the nucleotide recall, a measure similar to the number of <italic>full-length hits</italic> was either similar or worse for ORNA reduced datasets. Since, the F1 score is the harmonic mean of precision and recall, the smaller values of recall for ORNA got mitigated by the larger improvements in precision. The difference in performance between Oases, Trinity and TransABySS assemblies from the hESC dataset underlines that data reduction shows varying effects on assembly contiguity depending on the assembler used. Although one should note that the differences are within a few percent according to the <inline-formula id="IE64"><mml:math id="IM64"><mml:mrow><mml:mo>%</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mo> </mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math></inline-formula> metric used here and the overall trend behaves similar for all reduction approaches on hESC data.
</p>
      <fig id="bty307-F3" orientation="portrait" position="float">
        <label>Fig. 3.</label>
        <caption>
          <p>Comparison of assemblies generated from ORNA, Diginorm and TIS reduced datasets. Each point on a line corresponds to a different parametrization of the algorithms. The amount of data reduction (<italic>x</italic>-axis) is compared against the assembly performance measured as <italic>% of complete</italic> (<italic>y</italic>-axis, see text). (<bold>a</bold>) and (<bold>d</bold>) Represent TransABySS assemblies (<italic>k </italic>=<italic> </italic>21) applied on normalized brain and hESC data, respectively. (<bold>b</bold>) and (<bold>e</bold>) Represent Trinity assemblies (<italic>k </italic>=<italic> </italic>25) and (<bold>c</bold>) and (<bold>f</bold>) represent Oases multi-<italic>k</italic>mer assemblies applied on normalized brain and hESC data, respectively</p>
        </caption>
        <graphic xlink:href="bty307f3"/>
      </fig>
      <p>For evaluating the PE mode, ORNA was run with <italic>k</italic>-mer value of 22 on the PE brain dataset and was compared against the PE mode of TIS and Diginorm (<xref ref-type="supplementary-material" rid="sup1">Supplementary Table S2</xref>). Normalized datasets were then assembled using TransABySS with <italic>k</italic>-mer size 21. A similar trend to the single-end mode was observed (<xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S4</xref>). At a higher percentage of reduction, the assembly performance of TIS and Diginorm normalized data degraded faster than the performance of ORNA normalized data.</p>
      <p><bold>Comparison of resource requirements</bold>. ORNA stores <italic>k</italic>-mers in bloom filters (<xref rid="bty307-B30" ref-type="bibr">Salikhov <italic>et al.</italic>, 2014</xref>; implemented in the GATB library) making it runtime and memory efficient. <xref rid="bty307-T2" ref-type="table">Table 2</xref> shows the comparison of memory and runtime required by ORNA against those required by Diginorm and TIS for brain, hESC and the combined dataset (see Methods) with <italic>k</italic>-mer value 22. For the calculations, normalized datasets were chosen to have similar read counts. ORNA and TIS were also run using one and 10 threads each. Diginorm is not parallelized. All the algorithms were run on a machine with 16 GB register memory (RDIMM) and 1.5 TB RAM. It can be observed that ORNA generally consumes less than half the memory and runtime required by TIS for a similar percentage of reduction. This is true for both the single and multi-threaded versions. A nice property of Diginorm, is that the user can set the memory used by tuning the number of hashes and the size of each hash. A low number of hashes would reduce the runtime of Diginorm but increases the probability of false positive <italic>k</italic>-mers. A higher number of hashes reduce false positives but Diginorm takes longer. In this work, two different parametrizations, denoted as Diginorm<sup><italic>a</italic></sup> and <sup><italic>b</italic></sup>, were considered. The first uses less and the second a similar amount of memory compared to ORNA. In both cases, ORNA has an advantage of runtime over Diginorm. Similar results were also observed for PE mode (<xref ref-type="supplementary-material" rid="sup1">Supplementary Table S4</xref>). While Diginorm’s memory can be flexibly set, we note that ORNA uses much less space than the assembly of the reduced dataset itself, therefore not restraining the workflow.
<table-wrap id="bty307-T2" orientation="portrait" position="float"><label>Table 2.</label><caption><p>Runtime (in minutes) and memory (in GB) required by different normalization algorithms</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col valign="top" align="left" span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="[" span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="(" span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="(" span="1"/><col valign="top" align="char" char="." span="1"/></colgroup><thead><tr><th rowspan="2" colspan="1">Method</th><th colspan="3" rowspan="1">Brain (147 M–20.1 GB)<hr/></th><th colspan="3" rowspan="1">hESC (142 M–13.2 GB)<hr/></th><th colspan="3" rowspan="1">Combined (883 M–98.1 GB)<hr/></th></tr><tr><th rowspan="1" colspan="1">%Reduced</th><th rowspan="1" colspan="1">Time [min]</th><th rowspan="1" colspan="1">Mem [GB]</th><th rowspan="1" colspan="1">%Reduced</th><th rowspan="1" colspan="1">Time[min]</th><th rowspan="1" colspan="1">Mem [GB]</th><th rowspan="1" colspan="1">%Reduced</th><th rowspan="1" colspan="1">Time[min]</th><th rowspan="1" colspan="1">Mem [GB]</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">ORNA</td><td rowspan="1" colspan="1">83.3</td><td rowspan="1" colspan="1">104 (41)</td><td rowspan="1" colspan="1">6.62 (5.57)</td><td rowspan="1" colspan="1">63.90</td><td rowspan="1" colspan="1">58 (21)</td><td rowspan="1" colspan="1">6.16 (6.11)</td><td rowspan="1" colspan="1">81.4</td><td rowspan="1" colspan="1">740 (314)</td><td rowspan="1" colspan="1">32.9 (33.01)</td></tr><tr><td rowspan="1" colspan="1">Diginorm<sup>a</sup></td><td rowspan="1" colspan="1">81.86</td><td rowspan="1" colspan="1">110</td><td rowspan="1" colspan="1">3.13</td><td rowspan="1" colspan="1">61.63</td><td rowspan="1" colspan="1">115</td><td rowspan="1" colspan="1">3.13</td><td rowspan="1" colspan="1">80.37</td><td rowspan="1" colspan="1">760</td><td rowspan="1" colspan="1">12.51</td></tr><tr><td rowspan="1" colspan="1">Diginorm<sup>b</sup></td><td rowspan="1" colspan="1">81.51</td><td rowspan="1" colspan="1">135</td><td rowspan="1" colspan="1">6.26</td><td rowspan="1" colspan="1">62.59</td><td rowspan="1" colspan="1">126</td><td rowspan="1" colspan="1">6.26</td><td rowspan="1" colspan="1">79.31</td><td rowspan="1" colspan="1">2158</td><td rowspan="1" colspan="1">34.3</td></tr><tr><td rowspan="1" colspan="1">TIS</td><td rowspan="1" colspan="1">83.92</td><td rowspan="1" colspan="1">160 (95)</td><td rowspan="1" colspan="1">20.39 (20.19)</td><td rowspan="1" colspan="1">62.16</td><td rowspan="1" colspan="1">145 (127)</td><td rowspan="1" colspan="1">13.54 (13.53)</td><td rowspan="1" colspan="1">82.04</td><td rowspan="1" colspan="1">1859 (783)</td><td rowspan="1" colspan="1">95.19 (96.09)</td></tr></tbody></table><table-wrap-foot><fn id="tblfn2"><p><italic>Notes</italic>: Time and memory as obtained by running with 10 threads are shown in brackets (if possible). Note that the memory of Diginorm can be set by the user. For comparison it is set such that it uses less or similar memory than ORNA denoted as Diginorm <sup>a</sup> and <sup>b</sup>, respectively. The percent of reads reduced by each method (% reduced) is shown in the first column for each dataset. The total number of reads (in millions) and the file size (in GB) of the original dataset is shown in brackets next to the dataset.</p></fn></table-wrap-foot></table-wrap></p>
    </sec>
    <sec>
      <title>3.2 Combined error correction and normalization generates fast and accurate <italic>de novo</italic> assemblies</title>
      <p>As ORNA retains all the <italic>k</italic>-mers from the original dataset, useless or erroneous <italic>k</italic>-mers are also retained. As <xref ref-type="fig" rid="bty307-F4">Figure 4a</xref> shows, running ORNA on the corrected brain data reduces more reads compared to the uncorrected data and leads to an improvement in assembly performance, tested with Oases multi-<italic>k</italic> assemblies. For a similar % of reduction, the EC data leads to more full length assemblies. This suggests that it is important to error correct the data before normalization with ORNA.
</p>
      <fig id="bty307-F4" orientation="portrait" position="float">
        <label>Fig. 4.</label>
        <caption>
          <p>Oases assembly performance using ORNA on the brain dataset. (<bold>a</bold>) ORNA normalization on uncorrected (dashed line) and error corrected (EC, solid line) data. The points on the curve represent how many full length transcripts (<italic>y</italic>-axis) have been assembled at a particular percent of reduction (<italic>x</italic>-axis). (<bold>b</bold>) Analysis of memory required (<italic>x</italic>-axis) by different assembly strategies namely–assembling uncorrected data (cross), assembling EC data (circle) and assembling EC and normalized data (rectangle). (<bold>c</bold>) Analysis of runtime (<italic>x</italic>-axis) required by the three strategies </p>
        </caption>
        <graphic xlink:href="bty307f4"/>
      </fig>
      <p>For a given dataset, assembly can be performed on–(i) the original dataset, (ii) EC dataset, (iii) EC and normalized dataset. <xref ref-type="fig" rid="bty307-F4">Figure 4b and c</xref> show the maximum memory and runtimes required by applying these different strategies to the brain dataset. As seen from <xref ref-type="fig" rid="bty307-F4">Figure 4b</xref>, memory required to assemble the uncorrected brain dataset is quite high. This is because of erroneous <italic>k</italic>-mers that complicate the graph as well as the high redundancy in the dataset. Memory is reduced by nearly 20% after error correction. It is further reduced by nearly 80% when the EC data is normalized using ORNA and then assembled.</p>
      <p>Runtime required to assemble a high coverage dataset is generally high (<xref ref-type="fig" rid="bty307-F4">Fig. 4c</xref>). Error correcting the data improves the assembly but increases the runtime of the entire process. Normalizing EC data and then assembling it reduces the runtime by nearly 40% but still generates a high number of full length transcripts. Interestingly, for three ORNA parameters (<italic>b </italic>=<italic> </italic>1.3, 1.5, 1.7) the assembly quality (in terms of number of full length transcripts) is better than assembly of the uncorrected data with much lower time and memory consumption. For more stringent parameters (<inline-formula id="IE65"><mml:math id="IM65"><mml:mrow><mml:mi>b</mml:mi><mml:mo>&gt;</mml:mo></mml:mrow></mml:math></inline-formula>1.7) the quality degrades due to a high % of reduction. Note that it is likely that similar conclusions could be made using Diginorm or TIS as normalization method. Thus in combination, error correction and normalization of RNA-seq data can be considered as potent pre-processing steps to an assembly procedure.</p>
    </sec>
    <sec>
      <title>3.3 Finding novel transcripts by joint normalization of large datasets</title>
      <p>Another important application of RNA-seq assembly methods is to find novel, unknown transcripts in well-annotated genomes by analyzing dozens of datasets of specialized tissues, e.g. in cancer (<xref rid="bty307-B34" ref-type="bibr">White <italic>et al.</italic>, 2014</xref>). The common routine in these works is to assemble each RNA-seq dataset individually. It can be argued, that instead of running the assemblies on the individual datasets, a combined dataset may allow to assemble transcripts that are not well covered in an individual dataset. But combining datasets increases the size of the dataset and thereby increases the resource requirements. Hence, the dataset has to be normalized first to remove redundancy. In this work, five diverse RNA-seq datasets were concatenated to form a combined dataset of 883 M reads. The combined dataset was assembled using TransABySS and a total of 5201 full length hits were obtained. Assembling the combined dataset required 357 GB of RAM and took 73 h to produce the final assembly. ORNA reduced the combined dataset to 163 M reads. Assembling the reduced dataset using TransABySS resulted in 4821 full length hits. A memory of 28 GB and a runtime of 25 h were required to assemble the reduced dataset.</p>
      <p>The assembly generated from the reduced combined dataset was then used to obtain missed transcripts. Note that missed transcripts refer to the transcripts, which were only assembled from the combined reduced dataset and not the individual datasets, see Materials and methods Section. The <italic>missed transcripts</italic> were aligned against the genome and compared against Ensembl annotations to estimate the number of <italic>full-length</italic> transcripts that have been missed by the individual assemblies. The biotypes of the <italic>full-length</italic> transcripts were obtained from Ensembl and GENCODE. Overall 381 missing protein coding transcripts were obtained by assembling the <italic>combined</italic> datasets. Along with these, 22 long non-coding RNAs, 49 non-coding RNAs and 15 pseudogenes are also part of the <italic>missed</italic> transcripts. Similar results may be obtained with Diginorm or TIS, albeit only ORNA guarantees that no <italic>k</italic>-mer information is lost.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Discussion and conclusion</title>
    <p>This work presents ORNA, a SMC optimization-based algorithm to reduce the redundancy in NGS data without losing any <italic>k</italic>-mer information important for DBGs. This is done by approximating the minimal number of reads required to retain all <italic>k</italic>-mers from the original dataset. By generating <italic>k</italic>-mer specific normalization weights, ORNA is able to retain the relative abundance difference between <italic>k</italic>-mers. This is important as various assemblers for non-uniform sequencing datasets use the abundance information to resolve complex graph sub-structures. ORNA, when tested on multiple datasets, is able to reduce up to 85% of the data and still able to maintain nearly 80% of useful transcripts.</p>
    <p>The performance of ORNA was compared against two well established normalization algorithms–Diginorm and TIS normalization. Brain and hESC datasets reduced by these three algorithms were assembled using Oases, Trinity and TransABySS. The assembled transcripts were then evaluated using the REF-EVAL component of DETONATE. In general, assemblies generated from ORNA reduced datasets were having a similar or higher F1 score as compared to Diginorm and TIS reduced datasets. Concerning the amount of full-length assemblies, ORNA reduced datasets performed better than Diginorm and TIS reduced datasets on the brain data. For hESC data, the results were heterogeneous and depended on the assembler used. This behaviour was corroborated by the F1-score results for hESC data, where the nucleotide recall of Oases assemblies from ORNA reduced datasets were smaller compared to TIS and Diginorm. There might be several factors such as the distribution of <italic>k</italic>mers in the original dataset and the heuristic approaches used by the assemblers, which might explain these variations.</p>
    <p>As throwing away data may show unexpected results, the same parameter of a normalization algorithm may lead to variable reduction values for different datasets. Currently, there is no clear way how to set these parameters given the data at hand. A recent work has shown how to optimize <italic>k</italic>-mer values for multi-<italic>k</italic>mer assembly (<xref rid="bty307-B10" ref-type="bibr">Durai and Schulz, 2016</xref>). Such an automatic selection procedure for the <italic>k</italic>-mer value used for normalization would be worthwhile as well.</p>
    <p>Despite the previously shown advantage of assembly runtime and memory after read normalization, it was investigated how combining and normalizing many datasets can generate novel transcripts. Intuitively, low-coverage transcripts will never be assembled or even detected in a single sequencing run. However, once many of these experiments are combined the coverage may be sufficient for assembly. It was illustrated that combining five diverse datasets led to the full-length assembly of more than 400 transcripts that would have been missed otherwise. This suggests that novelty detection approaches, e.g. in cancer samples could use that strategy to combine and normalize all datasets.</p>
    <p>Variation in the performance of ORNA, when different measures are used, underlines the fact that ORNA is a heuristic algorithm that can be tuned further for better results. At the moment, ORNA works best when the input data is EC. As it focuses on retaining all the <italic>k</italic>-mers from the original dataset, any erroneous <italic>k</italic>-mer which escapes the filter of error correction is retained. <xref rid="bty307-B35" ref-type="bibr">Zhang <italic>et al.</italic>, 2015</xref> observed that error correction before applying Diginorm to a dataset improved assembly performance. An extra filtering criteria like the quality score can also be incorporated to remove erroneous <italic>k</italic>-mers or to improve the ordering of the reads.</p>
    <p>As a conclusion, this work suggests a SMC based approach for removing read redundancy in large datasets and shows an improvement over the current normalization methods, in particular for single <italic>k</italic>-mer transcriptome assemblies. Although ORNA was only applied on RNA-seq data, the algorithm can also be applied for other non-uniform datasets from metagenomics or single-cell sequencing. The software is freely available at <ext-link ext-link-type="uri" xlink:href="https://github.com/SchulzLab/ORNA">https://github.com/SchulzLab/ORNA</ext-link>.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was supported by the Cluster of Excellence on Multi-modal Computing and Interaction (EXC284) of the German National Science Foundation (DFG); and International Max Planck Research School for Computer Science, Saarbrücken.</p>
    <p><italic>Conflict of Interest</italic>: none declared.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>Supplementary Data</label>
      <media xlink:href="bty307_supplementary.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="bty307-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Au</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Characterization of the human ESC transcriptome by hybrid sequencing</article-title>. <source>Proc. Natl. Acad. Sci. USA</source>, <volume>110</volume>, <fpage>E4821</fpage>–<lpage>E4830</lpage>.<pub-id pub-id-type="pmid">24282307</pub-id></mixed-citation>
    </ref>
    <ref id="bty307-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Barbosa-Morais</surname><given-names>N.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>The evolutionary landscape of alternative splicing in vertebrate species</article-title>. <source>Science</source>, <volume>338</volume>, <fpage>1587</fpage>–<lpage>1593</lpage>.<pub-id pub-id-type="pmid">23258890</pub-id></mixed-citation>
    </ref>
    <ref id="bty307-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Berger</surname><given-names>B.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Computational solutions for omics data</article-title>. <source>Nat. Rev. Genet</source>., <volume>14</volume>, <fpage>333</fpage>–<lpage>346</lpage>.<pub-id pub-id-type="pmid">23594911</pub-id></mixed-citation>
    </ref>
    <ref id="bty307-B4">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Brown</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) A reference-free algorithm for computational normalization of shotgun sequencing data. <italic>ArXiv e-prints.</italic></mixed-citation>
    </ref>
    <ref id="bty307-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chekuri</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>On the set multicover problem in geometric settings</article-title>. <source>ACM Trans. Algorithms</source>, <volume>9</volume>, <fpage>1</fpage>–<lpage>17</lpage>, 1–9.</mixed-citation>
    </ref>
    <ref id="bty307-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chikhi</surname><given-names>R.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Compacting de bruijn graphs from sequencing data quickly and in low memory</article-title>. <source>Bioinformatics</source>, <volume>32</volume>, <fpage>i201</fpage>–<lpage>i208</lpage>.<pub-id pub-id-type="pmid">27307618</pub-id></mixed-citation>
    </ref>
    <ref id="bty307-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Crusoe</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>The khmer software package: enabling efficient nucleotide sequence analysis</article-title>. F1000 Res., <volume>4</volume>:<fpage>900</fpage>.</mixed-citation>
    </ref>
    <ref id="bty307-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cunningham</surname><given-names>F.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Ensembl 2015</article-title>. <source>Nucleic Acids Res</source>., <volume>43</volume>, <fpage>D662</fpage>–<lpage>D669</lpage>.<pub-id pub-id-type="pmid">25352552</pub-id></mixed-citation>
    </ref>
    <ref id="bty307-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Drezen</surname><given-names>E.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>GATB: genome assembly &amp; analysis tool box</article-title>. <source>Bioinformatics</source>, <volume>30</volume>, <fpage>2959.</fpage><pub-id pub-id-type="pmid">24990603</pub-id></mixed-citation>
    </ref>
    <ref id="bty307-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Durai</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Schulz</surname><given-names>M.</given-names></name></person-group> (<year>2016</year>) 
<article-title>Informed k mer selection for de novo transcriptome assembly</article-title>. <source>Bioinformatics</source>, <volume>32</volume>, <fpage>1670.</fpage><pub-id pub-id-type="pmid">27153653</pub-id></mixed-citation>
    </ref>
    <ref id="bty307-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fu</surname><given-names>L.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>CD-HIT: accelerated for clustering the next-generation sequencing data</article-title>. <source>Bioinformatics (Oxford, England)</source>, <volume>28</volume>, <fpage>3150</fpage>–<lpage>3152</lpage>.</mixed-citation>
    </ref>
    <ref id="bty307-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Grabherr</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) 
<article-title>Full-length transcriptome assembly from RNA-Seq data without a reference genome</article-title>. <source>Nat. Biotechnol</source>., <volume>29</volume>, <fpage>644</fpage>–<lpage>652</lpage>.<pub-id pub-id-type="pmid">21572440</pub-id></mixed-citation>
    </ref>
    <ref id="bty307-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Haas</surname><given-names>B.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>De novo transcript sequence reconstruction from RNA-seq using the Trinity platform for reference generation and analysis</article-title>. <source>Nat. Protocols</source>, <volume>8</volume>, <fpage>1494</fpage>–<lpage>1512</lpage>.<pub-id pub-id-type="pmid">23845962</pub-id></mixed-citation>
    </ref>
    <ref id="bty307-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Harrow</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>GENCODE: the reference human genome annotation for The ENCODE Project</article-title>. <source>Genome Res</source>., <volume>22</volume>, <fpage>1760</fpage>–<lpage>1774</lpage>.<pub-id pub-id-type="pmid">22955987</pub-id></mixed-citation>
    </ref>
    <ref id="bty307-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kent</surname><given-names>W.</given-names></name></person-group> (<year>2002</year>) 
<article-title>BLAT—the BLAST-like alignment tool</article-title>. <source>Genome Res</source>., <volume>12</volume>, <fpage>656</fpage>–<lpage>664</lpage>.<pub-id pub-id-type="pmid">11932250</pub-id></mixed-citation>
    </ref>
    <ref id="bty307-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Le</surname><given-names>H.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Probabilistic error correction for RNA sequencing</article-title>. <source>Nucleic Acids Res</source>., <volume>41</volume>, <fpage>e109.</fpage><pub-id pub-id-type="pmid">23558750</pub-id></mixed-citation>
    </ref>
    <ref id="bty307-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Evaluation of de novo transcriptome assemblies from RNA-Seq data</article-title>. <source>Genome Biol</source>., <volume>15</volume>, <fpage>553.</fpage><pub-id pub-id-type="pmid">25608678</pub-id></mixed-citation>
    </ref>
    <ref id="bty307-B18">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Limasset</surname><given-names>R.G.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) Fast and scalable minimal perfect hashing for massive key sets. <italic>ArXiv e-prints.</italic></mixed-citation>
    </ref>
    <ref id="bty307-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Loh</surname><given-names>P.R.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>Compressive genomics</article-title>. <source>Nat. Biotechnol</source>., <volume>30</volume>, <fpage>627</fpage>–<lpage>630</lpage>.<pub-id pub-id-type="pmid">22781691</pub-id></mixed-citation>
    </ref>
    <ref id="bty307-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>MacManes</surname><given-names>M.</given-names></name></person-group> (<year>2014</year>) 
<article-title>On the optimal trimming of high-throughput mRNA sequence data</article-title>. <source>Front. Genet</source>., <volume>5</volume>, <fpage>13.</fpage><pub-id pub-id-type="pmid">24567737</pub-id></mixed-citation>
    </ref>
    <ref id="bty307-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mbandi</surname><given-names>S.K.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>A glance at quality score: implication for de novo transcriptome reconstruction of Illumina reads</article-title>. <source>Front. Genet</source>., <volume>5</volume>, <fpage>17.</fpage><pub-id pub-id-type="pmid">24575122</pub-id></mixed-citation>
    </ref>
    <ref id="bty307-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>McCorrison</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>NeatFreq: reference-free data reduction and coverage normalization for De Novo sequence assembly</article-title>. <source>BMC Bioinformatics</source>, <volume>15</volume>, <fpage>357.</fpage><pub-id pub-id-type="pmid">25407910</pub-id></mixed-citation>
    </ref>
    <ref id="bty307-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Miller</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2010</year>) 
<article-title>Assembly algorithms for next-generation sequencing data</article-title>. <source>Genomics</source>, <volume>95</volume>, <fpage>315</fpage>–<lpage>327</lpage>.<pub-id pub-id-type="pmid">20211242</pub-id></mixed-citation>
    </ref>
    <ref id="bty307-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Moreton</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Assembly, assessment, and availability of de novo generated eukaryotic transcriptomes</article-title>. <source>Front. Genet</source>., <volume>6</volume>, <fpage>361.</fpage><pub-id pub-id-type="pmid">26793234</pub-id></mixed-citation>
    </ref>
    <ref id="bty307-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Patro</surname><given-names>R.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Salmon provides fast and bias-aware quantification of transcript expression</article-title>. <source>Nat. Methods</source>, <volume>14</volume>, <fpage>417</fpage>–<lpage>419</lpage>.<pub-id pub-id-type="pmid">28263959</pub-id></mixed-citation>
    </ref>
    <ref id="bty307-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pell</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>Scaling metagenome sequence assembly with probabilistic de Bruijn graphs</article-title>. <source>PNAS</source>, <volume>109</volume>, <fpage>13272</fpage>–<lpage>13277</lpage>.<pub-id pub-id-type="pmid">22847406</pub-id></mixed-citation>
    </ref>
    <ref id="bty307-B27">
      <mixed-citation publication-type="other"><collab>R Development Core Team</collab>. (<year>2008</year>) <italic>R: A Language and Environment for Statistical Computing.</italic> R Foundation for Statistical Computing, Vienna, Austria, ISBN 3-900051-07-0.</mixed-citation>
    </ref>
    <ref id="bty307-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rizk</surname><given-names>G.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>DSK: k-mer counting with very low memory usage</article-title>. <source>Bioinformatics</source>, <volume>29</volume>, <fpage>652</fpage>–<lpage>653</lpage>.<pub-id pub-id-type="pmid">23325618</pub-id></mixed-citation>
    </ref>
    <ref id="bty307-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Robertson</surname><given-names>G.</given-names></name></person-group><etal>et al</etal> (<year>2010</year>) 
<article-title>De novo assembly and analysis of RNA-seq data</article-title>. <source>Nat. Methods</source>, <volume>7</volume>, <fpage>909</fpage>–<lpage>912</lpage>.<pub-id pub-id-type="pmid">20935650</pub-id></mixed-citation>
    </ref>
    <ref id="bty307-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Salikhov</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Using cascading Bloom filters to improve the memory usage for de Brujin graphs</article-title>. <source>Algorithms Mol. Biol</source>., <volume>9</volume>, <fpage>2.</fpage><pub-id pub-id-type="pmid">24565280</pub-id></mixed-citation>
    </ref>
    <ref id="bty307-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Schulz</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>Oases: robust de novo RNA-seq assembly across the dynamic range of expression levels</article-title>. <source>Bioinformatics (Oxford, England)</source>, <volume>28</volume>, <fpage>1086</fpage>–<lpage>1092</lpage>.</mixed-citation>
    </ref>
    <ref id="bty307-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Song</surname><given-names>L.</given-names></name>, <name name-style="western"><surname>Florea</surname><given-names>L.</given-names></name></person-group> (<year>2015</year>) 
<article-title>Rcorrector: efficient and accurate error correction for Illumina RNA-seq reads</article-title>. <source>GigaScience</source>, <volume>4</volume>, <fpage>48.</fpage><pub-id pub-id-type="pmid">26500767</pub-id></mixed-citation>
    </ref>
    <ref id="bty307-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Srivastava</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>RapMap: a rapid, sensitive and accurate tool for mapping RNA-seq reads to transcriptomes</article-title>. <source>Bioinformatics</source>, <volume>32</volume>, <fpage>i192.</fpage><pub-id pub-id-type="pmid">27307617</pub-id></mixed-citation>
    </ref>
    <ref id="bty307-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>White</surname><given-names>N.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Transcriptome sequencing reveals altered long intergenic non-coding RNAs in lung cancer</article-title>. <source>Genome Biol</source>., <volume>15</volume>, <fpage>429.</fpage><pub-id pub-id-type="pmid">25116943</pub-id></mixed-citation>
    </ref>
    <ref id="bty307-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Q.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Crossing the streams: a framework for streaming analysis of short DNA sequencing reads</article-title>. <source>Peer J. PrePrints</source>, <volume>3</volume>, <fpage>e890v1</fpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
