<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Atypon//DTD Atypon Systems Archival NLM DTD Suite v2.2.0 20090301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName atypon-archivearticle.dtd?>
<?SourceDTD.Version 2.2?>
<?ConverterInfo.XSLTName atyponoasis-sgmlns2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Mol Biol Cell</journal-id>
    <journal-id journal-id-type="iso-abbrev">Mol Biol Cell</journal-id>
    <journal-id journal-id-type="hwp">molbiolcell</journal-id>
    <journal-id journal-id-type="pmc">mbc</journal-id>
    <journal-id journal-id-type="publisher-id">mboc</journal-id>
    <journal-title-group>
      <journal-title>Molecular Biology of the Cell</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1059-1524</issn>
    <issn pub-type="epub">1939-4586</issn>
    <publisher>
      <publisher-name>The American Society for Cell Biology</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7550707</article-id>
    <article-id pub-id-type="pmid">32697683</article-id>
    <article-id pub-id-type="publisher-id">E20-02-0156</article-id>
    <article-id pub-id-type="doi">10.1091/mbc.E20-02-0156</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Brief Reports</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>AnnotatorJ: an ImageJ plugin to ease hand annotation of cellular compartments</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Hollandi</surname>
          <given-names>Réka</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>a</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Diósdi</surname>
          <given-names>Ákos</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>a</sup>
        </xref>
        <xref ref-type="aff" rid="aff2">
          <sup>b</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hollandi</surname>
          <given-names>Gábor</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>a</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Moshkov</surname>
          <given-names>Nikita</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>a</sup>
        </xref>
        <xref ref-type="aff" rid="aff3">
          <sup>c</sup>
        </xref>
        <xref ref-type="aff" rid="aff4">
          <sup>d</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Horváth</surname>
          <given-names>Péter</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>a</sup>
        </xref>
        <xref ref-type="aff" rid="aff5">
          <sup>e</sup>
        </xref>
        <xref ref-type="corresp" rid="cor1">*</xref>
      </contrib>
      <aff id="aff1"><label>a</label>Synthetic and Systems Biology Unit, Biological Research Center, 6726 Szeged, Hungary</aff>
      <aff id="aff2"><label>b</label>Doctoral School of Biology, University of Szeged, 6726 Szeged, Hungary</aff>
      <aff id="aff3"><label>c</label>Doctoral School of Interdisciplinary Medicine, University of Szeged, Koranyi fasor 10, 6720 Szeged, Hungary</aff>
      <aff id="aff4"><label>d</label>National Research University Higher School of Economics, Faculty of Computer Science, 101000 Moscow, Russia</aff>
      <aff id="aff5"><label>e</label>Institute for Molecular Medicine Finland, University of Helsinki, 00014 Helsinki, Finland</aff>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Lippincott-Schwartz</surname>
          <given-names>Jennifer</given-names>
        </name>
        <role>Monitoring Editor</role>
      </contrib>
      <aff>Howard Hughes Medical Institute</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1">*Address correspondence to: Péter Horváth (<email xlink:href="mailto:horvath.peter@brc.hu">horvath.peter@brc.hu</email>).</corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <day>15</day>
      <month>9</month>
      <year>2020</year>
    </pub-date>
    <volume>31</volume>
    <issue>20</issue>
    <fpage>2179</fpage>
    <lpage>2186</lpage>
    <history>
      <date date-type="received">
        <day>27</day>
        <month>2</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>24</day>
        <month>6</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>17</day>
        <month>7</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2020 Hollandi <italic>et al.</italic> “ASCB®,” “The American Society for Cell Biology®,” and “Molecular Biology of the Cell®” are registered trademarks of The American Society for Cell Biology.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by-nc-sa/3.0">
        <license-p>This article is distributed by The American Society for Cell Biology under license from the author(s). Two months after publication it is available to the public under an Attribution–Noncommercial–Share Alike 3.0 Unported Creative Commons License.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:type="simple" xlink:href="mbc-31-2179.pdf"/>
    <abstract>
      <p>AnnotatorJ combines single-cell identification with deep learning (DL) and manual annotation. Cellular analysis quality depends on accurate and reliable detection and segmentation of cells so that the subsequent steps of analyses, for example, expression measurements, may be carried out precisely and without bias. DL has recently become a popular way of segmenting cells, performing unimaginably better than conventional methods. However, such DL applications may be trained on a large amount of annotated data to be able to match the highest expectations. High-quality annotations are unfortunately expensive as they require field experts to create them, and often cannot be shared outside the lab due to medical regulations. We propose AnnotatorJ, an ImageJ plugin for the semiautomatic annotation of cells (or generally, objects of interest) on (not only) microscopy images in 2D that helps find the true contour of individual objects by applying U-Net–based presegmentation. The manual labor of hand annotating cells can be significantly accelerated by using our tool. Thus, it enables users to create such datasets that could potentially increase the accuracy of state-of-the-art solutions, DL or otherwise, when used as training data.</p>
    </abstract>
  </article-meta>
</front>
<body>
  <sec sec-type="intro">
    <title>INTRODUCTION</title>
    <p>Single-cell analysis pipelines begin with an accurate detection of the cells. Even though microscopy analysis software tools aim to become more and more robust to various experimental setups and imaging conditions, most lack efficiency in complex scenarios such as label-free samples or unforeseen imaging conditions (e.g., higher signal-to-noise ratio, novel microscopy, or staining techniques), which opens up a new expectation of such software tools: adaptation ability (<xref rid="B23" ref-type="bibr">Hollandi <italic>et al.</italic>, 2020</xref>). Another crucial requirement is to maintain ease of usage and limit the number of parameters the users need to fine-tune to match their exact data domain.</p>
    <p>Recently, deep learning (DL) methods have proven themselves worthy of consideration in microscopy image analysis tools as they have also been successfully applied in a wider range of applications including but not limited to face detection (<xref rid="B46" ref-type="bibr">Sun <italic>et al.</italic>, 2014</xref>; <xref rid="B47" ref-type="bibr">Taigman <italic>et al.</italic>, 2014</xref>; <xref rid="B43" ref-type="bibr">Schroff <italic>et al.</italic>, 2015</xref>), self-driving cars (<xref rid="B35" ref-type="bibr">Redmon <italic>et al.</italic>, 2016</xref>; <xref rid="B8" ref-type="bibr">Badrinarayanan <italic>et al.</italic>, 2017</xref>, <xref rid="B19" ref-type="bibr">Grigorescu <italic>et al.</italic>, 2019</xref>), and speech recognition (<xref rid="B22" ref-type="bibr">Hinton <italic>et al.</italic>, 2012</xref>). Caicedo <italic>et al.</italic> (<xref rid="B10" ref-type="bibr">Caicedo <italic>et al.</italic>, 2019</xref>) and others (<xref rid="B23" ref-type="bibr">Hollandi <italic>et al.</italic>, 2020</xref>; <xref rid="B30" ref-type="bibr">Moshkov <italic>et al.</italic>, 2020</xref>) proved that single-cell detection and segmentation accuracy can be significantly improved utilizing DL networks. The most popular and widely used deep convolutional neural networks (DCNNs) include Mask R-CNN (<xref rid="B21" ref-type="bibr">He <italic>et al.</italic>, 2017</xref>): an object detection and instance segmentation network; YOLO (<xref rid="B35" ref-type="bibr">Redmon <italic>et al.</italic>, 2016</xref>; <xref rid="B36" ref-type="bibr">Redmon and Farhadi, 2018</xref>): a fast object detector; and U-Net (<xref rid="B37" ref-type="bibr">Ronneberger <italic>et al.</italic>, 2015</xref>): a fully convolutional network specifically intended for bioimage analysis purposes and mostly used for pixel classification. StarDist (<xref rid="B41" ref-type="bibr">Schmidt <italic>et al.</italic>, 2018</xref>) is an instance segmentation DCNN optimal for convex or elliptical shapes (such as nuclei).</p>
    <p>As robustly and accurately as they may perform, these networks rely on sufficient data, both in amount and quality, which tends to be the bottleneck of their applicability in certain cases such as single-cell detection. While in more industrial applications (see <xref rid="B19" ref-type="bibr">Grigorescu <italic>et al.</italic>, 2019</xref> for an overview of autonomous driving) a large amount of training data can be collected relatively easily (see the cityscapes dataset [ <xref rid="B13" ref-type="bibr">Cordts <italic>et al.</italic>, 2016</xref>; available at <ext-link ext-link-type="uri" xlink:href="https://www.cityscapes-dataset.com/">https://www.cityscapes-dataset.com/</ext-link>] of traffic video frames using a car and camera to record and potentially nonexpert individuals to label the objects), clinical data is considerably more difficult, due to ethical constraints, and expensive to gather as expert annotation is required. Datasets available in the public domain such as BBBC (<xref rid="B27" ref-type="bibr">Ljosa <italic>et al.</italic>, 2012</xref>) at <ext-link ext-link-type="uri" xlink:href="https://data.broadinstitute.org/bbbc/">https://data.broadinstitute.org/bbbc/</ext-link>, TNBC (<xref rid="B31" ref-type="bibr">Naylor <italic>et al.</italic>, 2017</xref>, <xref rid="B32" ref-type="bibr">2019</xref>) or TCGA (<xref rid="B11" ref-type="bibr">Cancer Genome Atlas Research Network, 2008</xref>; <xref rid="B25" ref-type="bibr">Kumar <italic>et al.</italic>, 2017</xref>), and detection challenges including ISBI (<xref rid="B12" ref-type="bibr">Coelho <italic>et al.</italic>, 2009</xref>), Kaggle (<ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/">https://www.kaggle.com/</ext-link>, e.g., Data Science Bowl 2018; see at <ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/c/data-science-bowl-2018">https://www.kaggle.com/c/data-science-bowl-2018</ext-link>), ImageNet (<xref rid="B39" ref-type="bibr">Russakovsky <italic>et al.</italic>, 2015</xref>), etc., contribute to the development of genuinely useful DL methods; however, most of them lack heterogeneity of the covered domains and are limited in data size. Even combining them one could not possibly prepare their network/method to generalize well (enough) on unseen domains that vastly differ from the pool they covered. On the contrary, such an adaptation ability can be achieved if the target domain is represented in the training data, as proposed in <xref rid="B23" ref-type="bibr">Hollandi <italic>et al.</italic>, 2020</xref>, where synthetic training examples are generated automatically in the target domain via image style transfer.</p>
    <p>Eventually, similar DL approaches’ performance can only be increased over a certain level if we provide more training examples. The proposed software tool was created for this purpose: the expert can more quickly and easily create a new annotated dataset in their desired domain and feed the examples to DL methods with ease. The user-friendly functions included in the plugin help organize data and support annotation, for example, multiple annotation types, editing, classes, etc. Additionally, a batch exporter is provided offering different export formats matching typical DL models’; supported annotation and export types are visualized in <xref ref-type="fig" rid="F1">Figure 1</xref>; open-source code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/spreka/annotatorj">https://github.com/spreka/annotatorj</ext-link> under GNU GPLv3 license.</p>
    <fig id="F1" position="float">
      <label>FIGURE 1:</label>
      <caption>
        <p>Annotation types. The top row displays our supported types of annotation: instance, semantic, and bounding box (noted as “bbox” in the figure) based on the same objects of interest, in this case nuclei, shown in red. Instances mark the object contours, semantic overlay shows the regions (area) covered, while bounding boxes are the smallest enclosing rectangles around the object borders. Export options are shown in the bottom row: multilabel, binary, multilayer images, and coordinates in a text file. Lines mark the supported export options for each annotation type by colors: orange for instance, green for semantic, and blue for bounding box. Dashed lines indicate additional export options for semantics that should be used carefully.</p>
      </caption>
      <graphic xlink:href="mbc-31-2179-g001"/>
    </fig>
    <p>We implemented the tool as an ImageJ (<xref rid="B2" ref-type="bibr">Abramoff <italic>et al.</italic>, 2004</xref>, <xref rid="B42" ref-type="bibr">Schneider <italic>et al.</italic>, 2012</xref>) plugin because ImageJ is frequently used by bioimage analysts, providing a familiar environment for users. While other software also provide a means to support annotation, for example, by machine learning–based pixel classification (see a detailed comparison in <italic>Materials and Methods</italic>), AnnotatorJ is a lightweight, free, open-source, cross-platform alternative. It can be easily installed via its ImageJ update site at <ext-link ext-link-type="uri" xlink:href="https://sites.imagej.net/Spreka/">https://sites.imagej.net/Spreka/</ext-link> or run as a standalone ImageJ instance containing the plugin.</p>
    <p>In AnnotatorJ we initialize annotations with DL presegmentation using U-Net to suggest contours from as little as a quickly drawn line over the object (see Supplemental Material and <xref ref-type="fig" rid="F2">Figure 2</xref>). U-Net predicts pixels belonging to the target class with the highest probability within a small bounding box (a rectangle) around the initially drawn contour; then a fine approximation of the true object boundary is calculated from connected pixels; this is referred to as the suggested contour. The user then manually refines the contour to create a pixel-perfect annotation of the object.</p>
    <fig id="F2" position="float">
      <label>FIGURE 2:</label>
      <caption>
        <p>Contour assist mode of AnnotatorJ. The blocks show the order of steps; the given tool needed is automatically selected. User interactions are marked with orange arrows, and automatic steps with blue. 1) Initialize the contour with a lazily drawn line; 2) the suggested contour appears (a window is shown until processing completes), brush selection tool is selected automatically; 3) refine the contour as needed; 4) accept it by pressing the key “q” or reject with “Ctrl” + “delete.” Accepting adds the ROI to ROI Manager with a numbered label. See also Supplemental Material for a demo video (figure2.mov).</p>
      </caption>
      <graphic xlink:href="mbc-31-2179-g002"/>
    </fig>
    <fig id="d41e419" position="anchor">
      <caption>
        <title>Movie S1</title>
        <p>A short demonstration of Contour assist is provided as the video figure2.mov; available in higher resolution at <ext-link ext-link-type="uri" xlink:href="https://drive.google.com/file/d/1dk3FrX-KIhTpaNYSKv-zVsoAoVUGtEYp/view?usp=sharing">https://drive.google.com/file/d/1dk3FrX-KIhTpaNYSKv-zVsoAoVUGtEYp/view?usp=sharing</ext-link>.</p>
      </caption>
      <media xlink:href="mbc-31-2179-s003.mov" mimetype="video" mime-subtype="quicktime">
        <alt-text>Movie S1</alt-text>
      </media>
    </fig>
  </sec>
  <sec sec-type="results">
    <title>RESULTS AND DISCUSSION</title>
    <sec>
      <title>Performance evaluation</title>
      <p>We quantitatively evaluated annotation performance and speed in AnnotatorJ (see <xref ref-type="fig" rid="F3">Figures 3</xref> and <xref ref-type="fig" rid="F4">4</xref>) with the help of three annotators who had experience in cellular compartment annotation. Both annotation accuracy and time were measured on the same two test sets: a nucleus and a cytoplasm image set (see also Supplemental Figure S1 and Supplemental Material). Both test sets contained images of various experimental conditions, including fluorescently labeled and brightfield-stained samples, tissue section, and cell culture images. We compared the effectiveness of our plugin using <italic>Contour assist</italic> mode to only allowing the use of <italic>Automatic adding</italic>. Even though the latter is also a functionality of AnnotatorJ, it ensured that the measured annotation times correspond to a single object each. Without this option the user must press the key “t” after every contour drawn to add it to the region of interest (ROI) list, which can be unintendedly missed, increasing its time as the same contour must be drawn again.</p>
      <fig id="F3" position="float">
        <label>FIGURE 3:</label>
        <caption>
          <p>Annotation times on nucleus images. AnnotatorJ was tested on sample microscopy images (both fluorescent and brightfield, as well as cell culture and tissue section images); annotation time was measured on a per-object (nucleus) level. Bars represent the mean annotation times on the test image set; error bars show SEM. Orange corresponds to <italic>Contour assist</italic> mode and blue to only allowing the <italic>Automatic adding</italic> option. (A) Nucleus test set annotation times. (B) Example cell culture test images. (C) Example histopathology images. Images shown in B and C are 256 × 256 crops of original images. Some images are courtesy of Kerstin Elisabeth Dörner, Andreas Mund, Viktor Honti, and Hella Bolck.</p>
        </caption>
        <graphic xlink:href="mbc-31-2179-g003"/>
      </fig>
      <fig id="F4" position="float">
        <label>FIGURE 4:</label>
        <caption>
          <p>Annotation accuracies. Annotations created in the same test as the times measured in <xref ref-type="fig" rid="F3">Figure 3</xref> were evaluated using mean IoU scores for the nucleus test set. Error bars show SEM; colors are as in <xref ref-type="fig" rid="F3">Figure 3</xref>. (A) Nucleus test set accuracies. (B) Example contours drawn by our expert annotators. Inset highlighted in orange is zoomed in showing the original image; annotations are marked in red, green, and blue corresponding to experts #1–3, respectively, and ground truth contours (in magenta) are overlayed on the original image for comparison.</p>
        </caption>
        <graphic xlink:href="mbc-31-2179-g004"/>
      </fig>
      <p>For the annotation time test presented in <xref ref-type="fig" rid="F3">Figure 3</xref> we measured the time passed between adding new objects to the annotated object set in ROI Manager for each object, then averaged the times for each image and each annotator, respectively. Time was measured in the Java implementation of the plugin in milliseconds. <xref ref-type="fig" rid="F3">Figures 3</xref> and <xref ref-type="fig" rid="F4">4</xref> show SEM error bars for each mean measurement (see Supplemental Material for details).</p>
      <p>In the case of annotating cell nuclei, results confirm that hand-annotation tasks can be significantly accelerated using our tool. Each of the three annotators were faster by using <italic>Contour assist</italic>; two of them nearly double their speed.</p>
      <p>To ensure efficient usage of our plugin in annotation assistance, we also evaluated the accuracies achieved in each test case by calculating mean intersection over union (IoU) scores of the annotations as segmentations compared with ground truth masks previously created by different expert annotators. We used the mean IoU score defined in the Data Science Bowl 2018 competition (https://www.kaggle.com/c/data-science-bowl-2018/overview/evaluation) and in <xref rid="B23" ref-type="bibr">Hollandi <italic>et al.</italic>, 2020</xref>: <disp-formula><graphic xlink:href="mbc-31-2179-e001.jpg" position="anchor" mimetype="image"/><label>(<italic>1</italic>)</label></disp-formula></p>
      <p>IoU determines the overlapping pixels of the segmented mask with the ground truth mask (intersection) compared with their union. The IoU score is calculated at 10 different thresholds from 0.5 to 0.95 with 0.05 steps; at each threshold true positive (TP), false positive (FP), and false negative (FN) objects are counted. An object is considered TP if its IoU is greater than the given threshold <italic>t</italic>. IoU scores calculated at all 10 thresholds were finally averaged to yield a single IoU score for a given image in the test set.</p>
      <p>An arbitrarily small ε = 10<sup>–40</sup> value was added to the denominators for numerical stability. Equation 1 is a modified version of mean average precision (mAP) typically used to describe the accuracy of instance segmentation approaches. Precision is formulated as <disp-formula><graphic xlink:href="mbc-31-2179-e002.jpg" position="anchor" mimetype="image"/><label>(<italic>2</italic>)</label></disp-formula></p>
      <p>Nucleus and cytoplasm image segmentation accuracies were averaged over the test sets, respectively. We compared our annotators using and not using <italic>Contour assist</italic> mode (<xref ref-type="fig" rid="F4">Figure 4</xref>). The results show greater interexpert than intraexpert differences, allowing us to conclude that the annotations created in AnnototarJ are nearly as accurate as freehand annotations.</p>
    </sec>
    <sec>
      <title>Export evaluation</title>
      <p>As the training data annotation process for deep learning applications requires the annotated objects to be exported in a manner that DL models can load them, which typically covers the four types of export options offered in AnnotatorJExporter, it is also important to investigate the efficiency of export. We measured export times similarly to annotation times. For the baseline results, each object defined by their ROI was copied to a new empty image, then filled and saved to create a segmentation mask image file. Exportation from AnnotatorJExporter was significantly faster and only required a few clicks: it took four orders of magnitude less time to export the annotations (∼60 ms). Export times reported correspond to a randomly selected expert so that computer hardware specifications remain the same.</p>
    </sec>
    <sec>
      <title>Comparison to other tools and software packages</title>
      <p>The desire to collect annotated datasets has arisen with the growing popularity and availability of application-specific DL methods. Object classification on natural images (photos) and face recognition are frequently used examples of such applications in computer vision. We discuss some of the available software tools created for image annotation tasks and compare their feature scope in the following table (<xref rid="T1" ref-type="table">Table 1</xref>; see also Supplemental Table S1) and in the Supplemental Material.</p>
      <table-wrap id="T1" position="float">
        <label>TABLE 1:</label>
        <caption>
          <p>Comparison of annotation software tools.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead valign="bottom">
            <tr>
              <th colspan="2" rowspan="2" align="left">Feature</th>
              <th colspan="8" align="center" rowspan="1">Tool</th>
            </tr>
            <tr>
              <th align="center" rowspan="1" colspan="1">LabelImg</th>
              <th align="center" rowspan="1" colspan="1">Lionbridge.AI</th>
              <th align="center" rowspan="1" colspan="1">Hive</th>
              <th align="center" rowspan="1" colspan="1">VGG Image Annotator</th>
              <th align="center" rowspan="1" colspan="1">Diffgram</th>
              <th align="center" rowspan="1" colspan="1">CytoMine</th>
              <th align="center" rowspan="1" colspan="1">SlideRunner</th>
              <th align="center" rowspan="1" colspan="1">AnnotatorJ</th>
            </tr>
          </thead>
          <tbody valign="top">
            <tr>
              <td align="left" rowspan="1" colspan="1">Open source</td>
              <td align="center" rowspan="1" colspan="1"/>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">×</td>
              <td align="center" rowspan="1" colspan="1">×</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Cross-platform</td>
              <td align="center" rowspan="1" colspan="1"/>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">service</td>
              <td align="center" rowspan="1" colspan="1">service</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">Ubuntu</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Implementation</td>
              <td align="center" rowspan="1" colspan="1"/>
              <td align="center" rowspan="1" colspan="1">Python</td>
              <td align="center" rowspan="1" colspan="1">N/A</td>
              <td align="center" rowspan="1" colspan="1">N/A</td>
              <td align="center" rowspan="1" colspan="1">web</td>
              <td align="center" rowspan="1" colspan="1">Python, web</td>
              <td align="center" rowspan="1" colspan="1">Docker, web</td>
              <td align="center" rowspan="1" colspan="1">Python</td>
              <td align="center" rowspan="1" colspan="1">Java</td>
            </tr>
            <tr>
              <td rowspan="6" align="right" colspan="1">Annotation</td>
              <td align="left" rowspan="1" colspan="1">Bounding box</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Freehand ROI</td>
              <td align="center" rowspan="1" colspan="1">×</td>
              <td align="center" rowspan="1" colspan="1">x</td>
              <td align="center" rowspan="1" colspan="1">N/A</td>
              <td align="center" rowspan="1" colspan="1">×</td>
              <td align="center" rowspan="1" colspan="1">×</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">×</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Polygonal region</td>
              <td align="center" rowspan="1" colspan="1">×</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓ (ImageJ)</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Semantic</td>
              <td align="center" rowspan="1" colspan="1">×</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">×</td>
              <td align="center" rowspan="1" colspan="1">×</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">×</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Single click<sup>a</sup></td>
              <td align="center" rowspan="1" colspan="1">×</td>
              <td align="center" rowspan="1" colspan="1">×</td>
              <td align="center" rowspan="1" colspan="1">×</td>
              <td align="center" rowspan="1" colspan="1">×</td>
              <td align="center" rowspan="1" colspan="1">×</td>
              <td align="center" rowspan="1" colspan="1">✓ (magic wand)</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓ (drag)</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Edit selection</td>
              <td align="center" rowspan="1" colspan="1">x (drag)</td>
              <td align="center" rowspan="1" colspan="1">N/A</td>
              <td align="center" rowspan="1" colspan="1">N/A</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">N/A</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
            </tr>
            <tr>
              <td align="right" rowspan="1" colspan="1">Class option</td>
              <td align="left" rowspan="1" colspan="1"/>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
            </tr>
            <tr>
              <td rowspan="2" align="right" colspan="1">DL</td>
              <td align="left" rowspan="1" colspan="1">Support</td>
              <td align="center" rowspan="1" colspan="1">×</td>
              <td align="center" rowspan="1" colspan="1">AI assist</td>
              <td align="center" rowspan="1" colspan="1">N/A</td>
              <td align="center" rowspan="1" colspan="1">×</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
              <td align="center" rowspan="1" colspan="1">×</td>
              <td align="center" rowspan="1" colspan="1">✓</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Model import</td>
              <td align="center" rowspan="1" colspan="1">×</td>
              <td align="center" rowspan="1" colspan="1">N/A</td>
              <td align="center" rowspan="1" colspan="1">N/A</td>
              <td align="center" rowspan="1" colspan="1">×</td>
              <td align="center" rowspan="1" colspan="1">Tensorflow</td>
              <td align="center" rowspan="1" colspan="1">N/A</td>
              <td align="center" rowspan="1" colspan="1">×</td>
              <td align="center" rowspan="1" colspan="1">Keras, DL4J</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn>
            <p><sup>a</sup>Bounding box drawing with a single click and drag is not considered a single click annotation.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>We collected our list of methods to compare following <xref rid="B29" ref-type="bibr">Morikawa, 2019</xref> and “The best image annotation platforms”, 2018. While there certainly is a considerable amount of annotation tools for object detection purposes, most of them are not open source. We included Lionbridge.AI (https://lionbridge.ai/services/image-annotation/) and Hive (https://thehive.ai/), two service-based solutions, because of their wide functionality and artificial intelligence support. Both of them work in a project-management way and outsource the annotation task to enable fast and accurate results. Their main application spectra cover more general object detection tasks like classification of traffic video frames. LabelImg (<ext-link ext-link-type="uri" xlink:href="https://github.com/tzutalin/labelImg">https://github.com/tzutalin/labelImg</ext-link>), on the other hand, as well as the following tools, is open source but offers a narrower range of annotation options and lacks machine learning support making it a lightweight but free alternative. VGG Image Annotator (<xref rid="B14" ref-type="bibr">Dutta and Zisserman, 2019</xref>) comes on a web-based platform, therefore making it very easy for the user to become familiarized with the software. It enables multiple types of annotation with class definition. Diffgram (https://diffgram.com/) is available both online and as a locally installable version (Python) and adds DL support which speeds up the annotation process significantly; that is, provided the intended object classes are already trained and the DL predictions only need minor edit. A similar, also web-based approach is provided by supervise.ly (https://supervise.ly/; see the Supplemental Material), which is free for research purposes. Even though web-hosted services offer a convenient solution for training new models (if supported), handling sensitive clinical data may be problematic. Hence, locally installable software is more desirable in biological and medical applications. A software closer to the bioimage analyst community is CytoMine (<xref rid="B28" ref-type="bibr">Marée <italic>et al.</italic>, 2016</xref>; <xref rid="B38" ref-type="bibr">Rubens <italic>et al.</italic>, 2019</xref>), a more general image processing tool with a lot of annotation options that also provides DL support and has a web interface. SlideRunner (<xref rid="B7" ref-type="bibr">Aubreville <italic>et al.</italic>, 2018</xref>) was created for large tissue section (slide) annotation specifically, but similar to others it does not integrate machine learning methods to help annotation and rather focuses on the classification task.</p>
      <p>AnnotatorJ, on the other hand, as an ImageJ (Fiji) plugin should provide a familiar environment for bioimage annotators to work in. It offers all the functionality available in similar tools (such as different annotation options: bounding box, polygon, freehand drawing, semantic segmentation, and editing them) while it also incorporates support for a popular DL model, U-Net. Furthermore, any user-trained Keras model can be loaded into the plugin with ease because of the DL4J framework, extending its use cases to general object annotation tasks (see Supplemental Figure S2 and Supplemental Material). Due to its open-source implementation, the users can modify or extend the plugin to even better fit their needs. Additionally, as an ImageJ plugin it requires no software installation, can be downloaded inside ImageJ/Fiji (via its update site, https://sites.imagej.net/Spreka/), or run as a standalone ImageJ instance with this plugin.</p>
      <p>We also briefly discuss ilastik (<xref rid="B44" ref-type="bibr">Sommer <italic>et al.</italic>, 2011</xref>; <xref rid="B9" ref-type="bibr">Berg <italic>et al.</italic>, 2019</xref>) and Suite2p (<xref rid="B33" ref-type="bibr">Pachitariu <italic>et al.</italic>, 2017</xref>) in the Supplemental Material because they are not primarily intended for annotation purposes. Two ImageJ plugins that offer manual annotation and machine learning–generated outputs, Trainable Weka Segmentation (<xref rid="B4" ref-type="bibr">Arganda-Carreras <italic>et al.</italic>, 2017</xref>) and LabKit (<xref rid="B6" ref-type="bibr">Arzt, 2017</xref>), are also detailed in the Supplemental Material.</p>
      <p>We presented an ImageJ plugin, AnnotatorJ, for convenient and fast annotation and labeling of objects on digital images. Multiple export options are also offered in the plugin.</p>
      <p>We tested the efficiency of our plugin with three experts on two test sets comprising nucleus and cytoplasm images. We found that our plugin accelerates the hand-annotation process on average and offers up to four orders of magnitude faster export. By integrating the DL4J Java framework for U-Net contour suggestion in <italic>Contour assist</italic> mode any class of object can be annotated easily: the users can load their own custom models for the target class.</p>
    </sec>
  </sec>
  <sec sec-type="materials|methods">
    <title>MATERIALS AND METHODS</title>
    <sec>
      <title>Motivation</title>
      <p>We propose AnnotatorJ, an ImageJ (<xref rid="B2" ref-type="bibr">Abramoff <italic>et al.</italic>, 2004</xref>, <xref rid="B42" ref-type="bibr">Schneider <italic>et al.</italic>, 2012</xref>) plugin for the annotation and export of cellular compartments that can be used to boost DL models’ performance. The plugin is mainly intended for bioimage annotation but could possibly be used to annotate any type of object on images (see Supplemental Figure S2 for a general example). During development we kept in mind that the intended user should be able to get comfortable with the software very quickly and quicken the otherwise truly time-consuming and exhausting process of manually annotating single cells or their compartments (such as individual nucleoli, lipid droplets, nucleus, or cytoplasm).</p>
      <p>The performance of DL segmentation methods is significantly influenced by both the training data size and its quality. Should we feed automatically segmented objects to the network, errors present in the original data will be propagated through the network during training and bias the performance, hence such training data should always be avoided. Hand-annotated and curated data, however, will minimize the initial error boosting the expected performance increase on the target domain to which the annotated data belongs. NucleAIzer (<xref rid="B23" ref-type="bibr">Hollandi <italic>et al.</italic>, 2020</xref>) showed an increase in nucleus segmentation accuracy when a DL model was trained on synthetic images generated from ground truth annotations instead of presegmented masks.</p>
    </sec>
    <sec>
      <title>Features</title>
      <p>AnnotatorJ helps organize the input and output files by automatically creating folders and matching file names to the selected type and class of annotation. Currently, the supported annotation types are 1) instance, 2) semantic, and 3) bounding box (see <xref ref-type="fig" rid="F1">Figure 1</xref>). Each of these are typical inputs of DL networks; instance annotation provides individual objects separated by their boundaries (useful in the case of, e.g., clumped cells of cancerous regions) and can be used to provide training data for instance segmentation networks such as Mask R-CNN (<xref rid="B21" ref-type="bibr">He <italic>et al.</italic>, 2017</xref>). Semantic annotation means foreground–background separation of the image without distinguishing individual objects (foreground); a typical architecture using such segmentations is U-Net (<xref rid="B37" ref-type="bibr">Ronneberger <italic>et al.</italic>, 2015</xref>). And finally, bounding box annotation is done by identifying the object’s bounding rectangle, and is generally used in object detection networks (like YOLO [ <xref rid="B35" ref-type="bibr">Redmon <italic>et al.</italic>, 2016</xref>] or R-CNN [ <xref rid="B18" ref-type="bibr">Girshick <italic>et al.</italic>, 2014</xref>]).</p>
      <p>Semantic annotation is done by painting areas on the image overlay. All necessary tools to operate a given function of the plugin are selected automatically. Contour or overlay colors can be selected from the plugin window. For a detailed description and user guide please see the documentation of the tool (available at <ext-link ext-link-type="uri" xlink:href="https://github.com/spreka/annotatorj">https://github.com/spreka/annotatorj</ext-link> repository).</p>
      <p>Annotations can be saved to default or user-defined “classes” corresponding to biological phenotypes (e.g., normal or cancerous) or object classes—used as in DL terminology (such as person, chair, bicycle, etc.), and later exported in a batch by class. Phenotypic differentiation of objects can be supported by loading a previously annotated class’s objects for comparison as overlay to the image and toggling their appearance by a checkbox.</p>
      <p>We use the default ImageJ ROI Manager to handle instance annotations as individual objects. Annotated objects can be added to the ROI list automatically (without the bound keystroke “t” as defined by ROI Manager) when the user releases the mouse button used to draw the contour by checking its option in the main window of the plugin. This ensures that no annotation drawn is missing from the ROI list.</p>
      <p>Contour editing is also possible in our plugin using “Edit mode” (by selecting its checkbox) in which the user can select any already annotated object on the image by clicking on it, then proceed to edit the contour and either apply modifications with the shortcut “Ctrl” + “q,” discard them with “escape,” or delete the contour with “Ctrl” + “delete.” The given object selected for edit is highlighted in inverse contour color.</p>
      <p>Object-based classification is also possible in “Class mode” (via its checkbox): similarly to “Edit mode,” ROIs can be assigned to a class by clicking on them on the image which will also update the selected ROI’s contour to the current class’s color. New classes can be added and removed, and their class color can be changed. A default class can be set for all unassigned objects on the image. Upon export (using either the quick export button “[^]” in the main window or the exporter plugin) masks are saved by classes.</p>
      <p>In the options (button “…” in the main window) the user can select to use either U-Net or a classical region-growing method to initialize the contour around the object marked. Currently only instance annotation can be assisted.</p>
    </sec>
    <sec>
      <title>Contour suggestion using U-Net</title>
      <p>Our annotation helper feature “<italic>Contour assist</italic>” (see <xref ref-type="fig" rid="F2">Figure 2</xref>) allows the user to work on initialized object boundaries by roughly marking an object’s location on the image which is converted to a well-defined object contour via weighted thresholding after a U-Net (<xref rid="B37" ref-type="bibr">Ronneberger <italic>et al.</italic>, 2015</xref>) model trained on nucleus or other compartment data predicts the region covered by the object. We refer to this as the <italic>suggested contour</italic> and expect the user to refine the boundaries to match the object border precisely. The suggested contour can be further optimized by applying <italic>active contour</italic> (AC; <xref rid="B24" ref-type="bibr">Kass <italic>et al.</italic>, 1988</xref>) to it. We aim to avoid fully automatic annotation (as previously argued) by only enabling one object suggestion at a time and requiring manual interaction to either refine, accept, or reject the suggested contour. These operations are bound to keyboard shortcuts for convenience (see <xref ref-type="fig" rid="F2">Figure 2</xref>). When using the <italic>Contour assist</italic> function automatic adding of objects is not available to encourage the user to manually validate and correct the suggested contour as needed.</p>
      <p>In <xref ref-type="fig" rid="F2">Figure 2</xref> we demonstrate <italic>Contour assist</italic> using a U-Net model trained on versatile microscopy images of nuclei in Keras and on a fluorescent microscopy image of a cell culture where the target objects, nuclei, are labeled with DAPI (in blue). This model is provided at <ext-link ext-link-type="uri" xlink:href="https://github.com/spreka/annotatorj/releases/tag/v0.0.2-model">https://github.com/spreka/annotatorj/releases/tag/v0.0.2-model</ext-link> in the open-source code repository of the plugin.</p>
      <p>Contour suggestions can be efficiently used for proper initialization of object annotation, saving valuable time for the expert annotator by suggesting a nearly perfect object contour that only needs refinement (as shown in <xref ref-type="fig" rid="F2">Figure 2</xref>). Using a U-Net model accurate enough for the target object class, the expert can focus on those image regions where the model is rather uncertain (e.g., around the edges of an object or the separating line between adjacent objects) and fine-tune the contour accurately while sparing considerable effort on more obvious regions (like an isolated object on simple background) by accepting the suggested contour after marginal correction.</p>
      <p>The framework of the chosen U-Net implementation, DL4J (available at <ext-link ext-link-type="uri" xlink:href="http://deeplearning4j.org/">http://deeplearning4j.org/</ext-link> or <ext-link ext-link-type="uri" xlink:href="https://github.com/eclipse/deeplearning4j">https://github.com/eclipse/deeplearning4j</ext-link>), supports Keras model import, hence custom, application-specific models can be loaded in the plugin easily by either training them in DL4J (Java) or Python (Keras) and saving the trained weights and model configuration in .h5 and .json files. This vastly extends the possible fields of application for the plugin to general object detection or segmentation tasks (see Supplemental Material and Supplemental Figures S2 and S3).</p>
    </sec>
    <sec>
      <title>Exporter</title>
      <p>The annotation tool is supplemented by an exporter, AnnotatorJExporter plugin, also available in the package. It was optimized for the batch export of annotations created by our annotation tool. For consistency, one class of objects can be exported at a time. We offer four export options: 1) multilabeled, 2) multilayered, 3) semantic images, and 4) coordinates (see <xref ref-type="fig" rid="F1">Figure 1</xref>). Instance annotations are typically expected to be exported as multilabeled (instance-aware) or multilayered (stack) grayscale images, the latter of which is useful for handling overlapping objects such as cytoplasms in cell culture images. Semantic images are binary foreground–background images of the target objects while coordinates (top-left corner [<italic>x</italic>,<italic>y</italic>] of the bounding rectangle appended by its width and height in pixels) can be useful training data for object detection applications including astrocyte localization (<xref rid="B45" ref-type="bibr">Suleymanova <italic>et al.</italic>, 2018</xref>) or in a broader aspect, face detection (<xref rid="B47" ref-type="bibr">Taigman <italic>et al.</italic>, 2014</xref>). All export options are supported for semantic annotation; however, we note that in instance-aware options (multilabeled or multilayered mask and coordinates) only such objects are distinguished whose contours do not touch on the annotation image.</p>
    </sec>
    <sec>
      <title>OpSeF compatibility</title>
      <p>OpSeF (Open Segmentation Framework; <xref rid="B34" ref-type="bibr">Rasse <italic>et al.</italic>, 2020</xref>) is an interactive python notebook-based framework (available at <ext-link ext-link-type="uri" xlink:href="https://github.com/trasse/OpSeF-IV">https://github.com/trasse/OpSeF-IV</ext-link>) that allows users to easily try different DL segmentation methods in customizable pipelines. We extended AnnotatorJ to support the data structure and format used in OpSeF to allow seamless integration in these pipelines, so users can manually modify, create, or classify objects found by OpSeF in AnnotatorJ, then export the results in a compatible format for further use in the former software. A user guide is provided in the documentation of <ext-link ext-link-type="uri" xlink:href="https://github.com/trasse/OpSeF-IV">https://github.com/trasse/OpSeF-IV</ext-link>.</p>
    </sec>
    <sec>
      <title>ImageJ</title>
      <p>ImageJ (or Fiji: Fiji is just ImageJ; <xref rid="B40" ref-type="bibr">Schindelin <italic>et al.</italic>, 2012</xref>) is an open-source, cross-platform image analysis software tool in Java that has been successfully applied in numerous bioimage analysis tasks (segmentation [ <xref rid="B26" ref-type="bibr">Legland <italic>et al.</italic>, 2016</xref>; <xref rid="B4" ref-type="bibr">Arganda-Carreras <italic>et al.</italic>, 2017</xref>], particle analysis [ <xref rid="B2" ref-type="bibr">Abramoff <italic>et al.</italic> 2004</xref>], etc.) and is supported by a broad range of community, comprising of biomage analyst end users and developers as well. It provides a convenient framework for new developers to create their custom plugins and share them with the community. Many typical image analysis pipelines have already been implemented as a plugin, for example, U-Net segmentation plugin (<xref rid="B16" ref-type="bibr">Falk <italic>et al.</italic>, 2019</xref>) or StarDist segmentation plugin (<xref rid="B41" ref-type="bibr">Schmidt <italic>et al.</italic>, 2018</xref>).</p>
    </sec>
    <sec>
      <title>U-Net implementation</title>
      <p>We used the DL4J (<ext-link ext-link-type="uri" xlink:href="http://deeplearning4j.org/">http://deeplearning4j.org/</ext-link>) implementation of U-Net in Java. DL4J enables building and training custom DL networks, preparing input data for efficient handling and supports both GPU and CPU computation throughout its ND4J library.</p>
      <p>The architecture of U-Net was first developed by Ronneberger <italic>et al.</italic> (<xref rid="B37" ref-type="bibr">Ronneberger <italic>et al.</italic>, 2015</xref>) and was designed to learn medical image segmentation on a small training set when a limited amount of labeled data is available, which is often the case in biological contexts. To handle touching objects as often is the case in nuclei segmentation, it uses a weighted cross entropy loss function to enhance the object-separating background pixels.</p>
    </sec>
    <sec>
      <title>Region growing</title>
      <p>A classical image processing algorithm, region growing (<xref rid="B20" ref-type="bibr">Haralick and Shapiro, 1985</xref>; <xref rid="B3" ref-type="bibr">Adams and Bischof, 1994</xref>) starts from initial seed points or objects and expands the regions towards the object boundaries based on the intensity changes on the image and constraints on distance or shape. We used our own implementation of this algorithm.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data">
      <media xlink:href="mbc-31-2179-s001.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data">
      <media xlink:href="mbc-31-2179-s002.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <fn-group>
    <fn fn-type="other">
      <p>This article was published online ahead of print in MBoC in Press (<ext-link ext-link-type="uri" xlink:href="http://www.molbiolcell.org/cgi/doi/10.1091/mbc.E20-02-0156">http://www.molbiolcell.org/cgi/doi/10.1091/mbc.E20-02-0156</ext-link>). on July 22, 2020</p>
    </fn>
  </fn-group>
  <ack>
    <p>R.H., N.M., A.D., and P.H. acknowledge support from the LENDULET-BIOMAG grant (Grant no. 2018-342), from the European Regional Development Funds (GINOP-2.3.2-15-2016-00006, GINOP-2.3.2-15-2016-00026, and GINOP-2.3.2-15-2016-00037), from the H2020-discovAIR (874656), and from Chan Zuckerberg Initiative, Seed Networks for the HCA-DVP. We thank Krisztián Koós for the technical help, and Máté Görbe and Tamás Monostori for testing the plugin.</p>
  </ack>
  <glossary>
    <title>Abbreviations used:</title>
    <def-list>
      <def-item>
        <term id="G1">DL</term>
        <def>
          <p>deep learning</p>
        </def>
      </def-item>
      <def-item>
        <term id="G2">DL4J</term>
        <def>
          <p>Deeplearning4j</p>
        </def>
      </def-item>
      <def-item>
        <term id="G3">IoU</term>
        <def>
          <p>intersection over union</p>
        </def>
      </def-item>
      <def-item>
        <term id="G4">ROI</term>
        <def>
          <p>region of interest.</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <ref-list>
    <title>REFERENCES</title>
    <ref id="B1">
      <mixed-citation publication-type="other"><collab>The best image annotation platforms for computer vision</collab> (<year>2018, October 30</year>). <source><italic>+ an honest review of each</italic></source>, <comment><ext-link ext-link-type="uri" xlink:href="https://hackernoon.com/the-best-image-annotation-platforms-for-computer-vision-an-honest-review-of-each-dac7f565fea">https://hackernoon.com/the-best-image-annotation-platforms-for-computer-vision-an-honest-review-of-each-dac7f565fea</ext-link></comment>.</mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abramoff</surname><given-names>MD</given-names></name><name><surname>Magalhaes</surname><given-names>PJ</given-names></name><name><surname>Ram</surname><given-names>SJ</given-names></name></person-group> (<year>2004</year>). <article-title>Image processing with ImageJ</article-title>. <source><italic>Biophoton Int</italic></source>, , <fpage>36</fpage>–<lpage>42</lpage>.</mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adams</surname><given-names>R</given-names></name><name><surname>Bischof</surname><given-names>L</given-names></name></person-group> (<year>1994</year>). <article-title>Seeded region growing</article-title>, <source><italic>IEEE Trans Pattern Anal Mach Intell</italic></source> , <fpage>641</fpage>–<lpage>647</lpage>.</mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arganda-Carreras</surname><given-names>I</given-names></name><name><surname>Kaynig</surname><given-names>V</given-names></name><name><surname>Rueden</surname><given-names>C</given-names></name><name><surname>Eliceiri</surname><given-names>KW</given-names></name><name><surname>Schindelin</surname><given-names>J</given-names></name><name><surname>Cardona</surname><given-names>A</given-names></name><name><surname>Sebastian Seung</surname><given-names>H</given-names></name></person-group> (<year>2017</year>). <article-title>Trainable Weka segmentation: a machine learning tool for microscopy pixel classification</article-title>. <source><italic>Bioinformatics</italic></source> , <fpage>2424</fpage>–<lpage>2426</lpage>.<pub-id pub-id-type="pmid">28369169</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arganda-Carreras</surname><given-names>I</given-names></name><name><surname>Turaga</surname><given-names>SC</given-names></name><name><surname>Berger</surname><given-names>DR</given-names></name><name><surname>Cires˛an</surname><given-names>D</given-names></name><name><surname>Giusti</surname><given-names>A</given-names></name><name><surname>Gambardella</surname><given-names>LM</given-names></name><name><surname>Schmidhuber</surname><given-names>J</given-names></name><name><surname>Laptev</surname><given-names>D</given-names></name><name><surname>Dwivedi</surname><given-names>S</given-names></name><name><surname>Buhmann</surname><given-names>JM</given-names></name></person-group>, <italic>et al.</italic> (<year>2015</year>). <article-title>Crowdsourcing the creation of image segmentation algorithms for connectomics</article-title>. <source><italic>Front Neuroanat</italic></source> , <fpage>142</fpage>.<pub-id pub-id-type="pmid">26594156</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Arzt</surname><given-names>M</given-names></name></person-group> (<year>2017</year>). <comment><ext-link ext-link-type="uri" xlink:href="https://imagej.net/Labkit">https://imagej.net/Labkit</ext-link></comment>.</mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Aubreville</surname><given-names>M</given-names></name><name><surname>Bertram</surname><given-names>C</given-names></name><name><surname>Klopfleisch</surname><given-names>R</given-names></name><name><surname>Maier</surname><given-names>A</given-names></name></person-group> (<year>2018</year>). <article-title>SlideRunner</article-title>. In: <source><italic>Bildverarbeitung für die Medizin 2018</italic></source>, <publisher-loc>Heidelberg, Berlin</publisher-loc>: <publisher-name>Informatik aktuell, Springer Vieweg</publisher-name>, pp.<fpage> 309</fpage>–<lpage>314</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-662-56537-7_81</pub-id>.</mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badrinarayanan</surname><given-names>V</given-names></name><name><surname>Kendall</surname><given-names>A</given-names></name><name><surname>Cipolla</surname><given-names>R</given-names></name></person-group> (<year>2017</year>). <article-title>SegNet: a deep convolutional encoder-decoder architecture for image segmentation</article-title>. <source><italic>IEEE Trans Pattern Anal Mach Intell</italic></source> , <fpage>2481</fpage>–<lpage>2495</lpage>.<pub-id pub-id-type="pmid">28060704</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berg</surname><given-names>S</given-names></name><name><surname>Kutra</surname><given-names>D</given-names></name><name><surname>Kroeger</surname><given-names>T</given-names></name><name><surname>Straehle</surname><given-names>CN</given-names></name><name><surname>Kausler</surname><given-names> BX </given-names></name><name><surname>Haubold</surname><given-names>C</given-names></name><name><surname>Schiegg</surname><given-names>M</given-names></name><name><surname>Ales</surname><given-names>J</given-names></name><name><surname>Beier</surname><given-names>T</given-names></name><name><surname>Rudy</surname><given-names>M</given-names></name></person-group>, <italic>et al.</italic> (<year>2019</year>). <article-title>ilastik: interactive machine learning for (bio)image analysis</article-title>. <source><italic>Nat Methods</italic></source> , <fpage>1226</fpage>–<lpage>1232</lpage>.<pub-id pub-id-type="pmid">31570887</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caicedo</surname><given-names>JC</given-names></name><name><surname>Roth</surname><given-names>J</given-names></name><name><surname>Goodman</surname><given-names>A</given-names></name><name><surname>Becker</surname><given-names>T</given-names></name><name><surname>Karhohs</surname><given-names>KW</given-names></name><name><surname>Broisin</surname><given-names>M</given-names></name><name><surname>Molnar</surname><given-names>C,</given-names></name><name><surname>Becker</surname><given-names>T</given-names></name><name><surname>Karhohs</surname><given-names>KW</given-names></name><name><surname>Broisin</surname><given-names>M</given-names></name></person-group>, <italic>et al.</italic> (<year>2019</year>). <article-title>Evaluation of deep learning strategies for nucleus segmentation in fluorescence images</article-title>. <source><italic>Cytometry A</italic></source> , <fpage>952</fpage>–<lpage>965</lpage>.<pub-id pub-id-type="pmid">31313519</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><collab>Cancer Genome Atlas Research Network</collab> (<year>2008</year>). <article-title>Comprehensive genomic characterization defines human glioblastoma genes and core pathways</article-title>. <source><italic>Nature</italic></source> , <fpage>1061</fpage>–<lpage>1068</lpage>.</mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coelho</surname><given-names>LP</given-names></name><name><surname>Shariff</surname><given-names>A</given-names></name><name><surname>Murphy</surname><given-names>RF</given-names></name></person-group> (<year>2009</year>). <article-title>Nuclear segmentation in microscope cell images: a hand-segmented dataset and comparison of algorithms</article-title>. <source><italic>2009 IEEE International Symposium on Biomedical Imaging: From Nano to Macro.</italic></source>
<comment>Available at</comment>
<pub-id pub-id-type="doi">10.1109/isbi.2009.5193098</pub-id>.</mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Cordts</surname><given-names>M</given-names></name><name><surname>Omran</surname><given-names>M</given-names></name><name><surname>Ramos</surname><given-names>S</given-names></name><name><surname>Rehfeld</surname><given-names>T</given-names></name><name><surname>Enzweiler</surname><given-names>M</given-names></name><name><surname>Benenson</surname><given-names>R</given-names></name><name><surname>Franke</surname><given-names>U</given-names></name><name><surname>Roth</surname><given-names>S</given-names></name><name><surname>Schiele</surname><given-names>B</given-names></name></person-group>, <italic>et al.</italic> (<year>2016</year>). <article-title>The cityscapes dataset for semantic urban scene understanding</article-title>. In <comment>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</comment>
<comment>Available at</comment>
<pub-id pub-id-type="doi">10.1109/cvpr.2016.350</pub-id>.</mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Dutta</surname><given-names>A</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group> (<year>2019</year>). <article-title>The VIA annotation software for images, audio and video</article-title>. In <comment>Proceedings of the 27th ACM International Conference on Multimedia - MM ‘19. Available at</comment>
<pub-id pub-id-type="doi">10.1145/3343031.3350535</pub-id>.</mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><collab>Eclipse Deeplearning4j Development Team</collab>. <article-title>Deeplearning4j: open-source distributed deep learning for the JVM</article-title>, <source><italic>Apache Software Foundation License 2.0</italic></source>. <comment><ext-link ext-link-type="uri" xlink:href="http://deeplearning4j.org">http://deeplearning4j.org</ext-link></comment></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Falk</surname><given-names>T</given-names></name><name><surname>Mai</surname><given-names>D</given-names></name><name><surname>Bensch</surname><given-names>R</given-names></name><name><surname>Çiçek</surname><given-names>Ö</given-names></name><name><surname>Abdulkadir</surname><given-names>A</given-names></name><name><surname>Marrakchi</surname><given-names>Y</given-names></name><name><surname>Böhm</surname><given-names>A</given-names></name><name><surname>Deubner</surname><given-names>J</given-names></name><name><surname>Jäckel</surname><given-names>Z</given-names></name><name><surname>Seiwald</surname><given-names>K</given-names></name></person-group>, <italic>et al.</italic> (<year>2019</year>). <article-title>U-Net: deep learning for cell counting, detection, and morphometry</article-title>. <source><italic>Nat Methods</italic></source> , <fpage>67</fpage>–<lpage>70</lpage>.<pub-id pub-id-type="pmid">30559429</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Frank</surname><given-names>E</given-names></name><name><surname>Hall</surname><given-names>MA</given-names></name><name><surname>Witten</surname><given-names>IH</given-names></name></person-group> (<year>2016</year>). <article-title>“The WEKA Workbench”, online appendix for Data Mining: Practical Machine Learning Tools and Techniques</article-title>, <edition>4th ed.</edition>, <source><italic>Morgan Kaufmann</italic></source>.</mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Girshick</surname><given-names>R</given-names></name><name><surname>Donahue</surname><given-names>J</given-names></name><name><surname>Darrell</surname><given-names>T</given-names></name><name><surname>Malik</surname><given-names>J</given-names></name></person-group> (<year>2014</year>). <article-title>Rich feature hierarchies for accurate object detection and semantic segmentation</article-title>. In <comment>2014 IEEE Conference on Computer Vision and Pattern Recognition. Available at</comment>
<pub-id pub-id-type="doi">10.1109/cvpr.2014.81</pub-id>.</mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grigorescu</surname><given-names>S</given-names></name><name><surname>Trasnea</surname><given-names>B</given-names></name><name><surname>Cocias</surname><given-names>T</given-names></name><name><surname>Macesanu</surname><given-names>G</given-names></name></person-group> (<year>2019</year>). <article-title>A survey of deep learning techniques for autonomous driving</article-title>. <source><italic>J Field Rob</italic></source> , <fpage>362</fpage>–<lpage>386</lpage>.</mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haralick</surname><given-names>RM</given-names></name><name><surname>Shapiro</surname><given-names>LG</given-names></name></person-group> (<year>1985</year>). <article-title>Image segmentation techniques</article-title>. <source><italic>Applications of Artificial Intelligence II</italic></source>. <comment>Available at</comment>
<pub-id pub-id-type="doi">10.1117/12.948400</pub-id>.</mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Gkioxari</surname><given-names>G</given-names></name><name><surname>Dollar</surname><given-names>P</given-names></name><name><surname>Girshick</surname><given-names>R</given-names></name></person-group> (<year>2017</year>). <article-title>Mask R-CNN</article-title>. In <comment>2017 IEEE International Conference on Computer Vision (ICCV)</comment>
<comment>Available at</comment>
<pub-id pub-id-type="doi">10.1109/iccv.2017.322</pub-id>.</mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>G</given-names></name><name><surname>Deng</surname><given-names>L</given-names></name><name><surname>Yu</surname><given-names>D</given-names></name><name><surname>Dahl</surname><given-names>G</given-names></name><name><surname>Mohamed</surname><given-names>A-R</given-names></name><name><surname>Jaitly</surname><given-names>N</given-names></name><name><surname>Senior</surname><given-names>A</given-names></name><name><surname>Vanhoucke</surname><given-names>V</given-names></name><name><surname>Nguyen</surname><given-names>P</given-names></name><name><surname>Sainath</surname><given-names>TN</given-names></name></person-group>, <italic>et al.</italic> (<year>2012</year>). <article-title>Deep neural networks for acoustic modeling in speech recognition: the shared views of four research groups</article-title>. <source><italic>IEEE Signal Process Mag</italic></source> , <fpage>82</fpage>–<lpage>97</lpage>.</mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hollandi</surname><given-names>R</given-names></name><name><surname>Szkalisity</surname><given-names>A</given-names></name><name><surname>Toth</surname><given-names>T</given-names></name><name><surname>Tasnadi</surname><given-names>E</given-names></name><name><surname>Molnar</surname><given-names>C</given-names></name><name><surname>Mathe</surname><given-names>B</given-names></name><name><surname>Grexa</surname><given-names>I</given-names></name><name><surname>Molnar</surname><given-names>J</given-names></name><name><surname>Balind</surname><given-names>A</given-names></name><name><surname>Gorbe</surname><given-names>M</given-names></name></person-group>, <italic>et al.</italic> (<year>2020</year>). <article-title>nucleAIzer: a parameter-free deep learning framework for nucleus segmentation using image style transfer</article-title>. <source><italic>Cell Syst</italic></source> , <fpage>453</fpage>–<lpage>458</lpage>.<comment>e6</comment>.</mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kass</surname><given-names>M</given-names></name><name><surname>Witkin</surname><given-names>A</given-names></name><name><surname>Terzopoulos</surname><given-names>D</given-names></name></person-group> (<year>1988</year>). <article-title>Snakes: active contour models</article-title>. <source><italic>Int J Comput Vis</italic></source> , <fpage>321</fpage>–<lpage>331</lpage>.</mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>N</given-names></name><name><surname>Verma</surname><given-names>R</given-names></name><name><surname>Sharma</surname><given-names>S</given-names></name><name><surname>Bhargava</surname><given-names>S</given-names></name><name><surname>Vahadane</surname><given-names>A</given-names></name><name><surname>Sethi</surname><given-names>A</given-names></name></person-group> (<year>2017</year>). <article-title>A dataset and a technique for generalized nuclear segmentation for computational pathology</article-title>. <source><italic>IEEE Trans Med Imaging</italic></source> , <fpage>1550</fpage>–<lpage>1560</lpage>.<pub-id pub-id-type="pmid">28287963</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Legland</surname><given-names>D</given-names></name><name><surname>Arganda-Carreras</surname><given-names>I</given-names></name><name><surname>Andrey</surname><given-names>P</given-names></name></person-group> (<year>2016</year>). <article-title>MorphoLibJ: integrated library and plugins for mathematical morphology with ImageJ</article-title>. <source><italic>Bioinformatics</italic></source> , <fpage>3532</fpage>–<lpage>3534</lpage>.<pub-id pub-id-type="pmid">27412086</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ljosa</surname><given-names>V</given-names></name><name><surname>Sokolnicki</surname><given-names>KL</given-names></name><name><surname>Carpenter</surname><given-names>AE</given-names></name></person-group> (<year>2012</year>). <article-title>Annotated high-throughput microscopy image sets for validation</article-title>. <source><italic>Nat Methods</italic></source> , <fpage>637</fpage>.<pub-id pub-id-type="pmid">22743765</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marée</surname><given-names>R</given-names></name><name><surname>Rollus</surname><given-names>L</given-names></name><name><surname>Stévens</surname><given-names>B</given-names></name><name><surname>Hoyoux</surname><given-names>R</given-names></name><name><surname>Louppe</surname><given-names>G</given-names></name><name><surname>Vandaele</surname><given-names>R</given-names></name><name><surname>Begon</surname><given-names>J-M</given-names></name><name><surname>Kainz</surname><given-names>P</given-names></name><name><surname>Geurts</surname><given-names>P</given-names></name><name><surname>Wehenkel</surname><given-names>L</given-names></name></person-group>, <italic>et al.</italic> (<year>2016</year>). <article-title>Collaborative analysis of multi-gigapixel imaging data using Cytomine</article-title>. <source><italic>Bioinformatics</italic></source> , <fpage>1395</fpage>–<lpage>1401</lpage>.<pub-id pub-id-type="pmid">26755625</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Morikawa</surname><given-names>R</given-names></name></person-group> (<comment>July 18</comment>, <year>2019</year>). <article-title>24 best image annotation tools for computer vision</article-title>. <comment>Available at</comment>
<comment><ext-link ext-link-type="uri" xlink:href="https://lionbridge.ai/articles/image-annotation-tools-for-computer-vision/">https://lionbridge.ai/articles/image-annotation-tools-for-computer-vision/</ext-link></comment>.</mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moshkov</surname><given-names>N</given-names></name><name><surname>Mathe</surname><given-names>B</given-names></name><name><surname>Kertesz-Farkas</surname><given-names>A</given-names></name><name><surname>Hollandi</surname><given-names>R</given-names></name><name><surname>Horvath</surname><given-names>P</given-names></name></person-group> (<year>2020</year>). <article-title>Test-time augmentation for deep learning-based cell segmentation on microscopy images</article-title>. <source><italic>Sci Rep</italic></source> , <fpage>5068</fpage>.<pub-id pub-id-type="pmid">32193485</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naylor</surname><given-names>P</given-names></name><name><surname>Lae</surname><given-names>M</given-names></name><name><surname>Reyal</surname><given-names>F</given-names></name><name><surname>Walter</surname><given-names>T</given-names></name></person-group> (<year>2017</year>). <article-title>Nuclei segmentation in histopathology images using deep neural networks</article-title>. In <source><italic>2017 IEEE 14th International Symposium on Biomedical Imaging</italic></source>
<comment>(ISBI 2017)</comment>
<comment>Available at</comment>
<pub-id pub-id-type="doi">10.1109/isbi.2017.7950669</pub-id>.</mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naylor</surname><given-names>P</given-names></name><name><surname>Lae</surname><given-names>M</given-names></name><name><surname>Reyal</surname><given-names>F</given-names></name><name><surname>Walter</surname><given-names>T</given-names></name></person-group> (<year>2019</year>). <article-title>Segmentation of nuclei in histopathology images by deep regression of the distance map</article-title>. <source><italic>IEEE Trans Med Imaging</italic></source> , <fpage>448</fpage>–<lpage>459</lpage>.<pub-id pub-id-type="pmid">30716022</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Dipoppa</surname><given-names>M</given-names></name><name><surname>Schröder</surname><given-names>S</given-names></name><name><surname>Rossi</surname><given-names>LF</given-names></name><name><surname>Dalgleish</surname><given-names>H</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name></person-group>, (<year>2017</year>). <article-title>Suite2p: beyond 10,000 neurons with standard two-photon microscopy</article-title>. <source><italic>BioRxiv</italic></source>. <pub-id pub-id-type="doi">10.1101/061507</pub-id>.</mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rasse</surname><given-names>TM</given-names></name><name><surname>Hollandi</surname><given-names>R</given-names></name><name><surname>Horvath</surname><given-names>P</given-names></name></person-group> (<year>2020</year>). <article-title>OpSeF IV: open source Python framework for segmentation of biomedical images</article-title>. <source><italic>BioRxiv</italic></source>. <pub-id pub-id-type="doi">10.1101/2020.04.29.068023</pub-id><comment><ext-link ext-link-type="uri" xlink:href="&lt;/bib"/></comment></mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Redmon</surname><given-names>J</given-names></name><name><surname>Divvala</surname><given-names>S</given-names></name><name><surname>Girshick</surname><given-names>R</given-names></name><name><surname>Farhadi</surname><given-names>A</given-names></name></person-group> (<year>2016</year>). <article-title>You only look once: unified, real-time object detection</article-title>. In <comment>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</comment>
<comment>Available at</comment>
<pub-id pub-id-type="doi">10.1109/cvpr.2016.91</pub-id>.</mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name>Redmon J</string-name><string-name>Farhadi A</string-name></person-group> (2018). YOLOv3: an incremental improvement. arXiv https://arxiv.org/abs/1804.02767.</mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>O</given-names></name><name><surname>Fischer</surname><given-names>P</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name></person-group> (<year>2015</year>). <article-title>U-Net: convolutional networks for biomedical image segmentation</article-title>. In <source><italic>Lecture Notes in Computer Science,</italic></source>
<publisher-name>Springer</publisher-name>, Vol. , <fpage>234</fpage>–<lpage>241</lpage>.</mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubens</surname><given-names>U</given-names></name><name><surname>Hoyoux</surname><given-names>R</given-names></name><name><surname>Vanosmael</surname><given-names>L</given-names></name><name><surname>Ouras</surname><given-names>M</given-names></name><name><surname>Tasset</surname><given-names>M</given-names></name><name><surname>Hamilton</surname><given-names>C</given-names></name><name><surname>Longuespée</surname><given-names>R</given-names></name><name><surname>Marée</surname><given-names>R</given-names></name></person-group> (<year>2019</year>). <article-title>Cytomine: toward an open and collaborative software platform for digital pathology bridged to molecular investigations</article-title>. <source><italic>Prot Clin Appl</italic></source> , <fpage>1800057</fpage>.</mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russakovsky</surname><given-names>O</given-names></name><name><surname>Deng</surname><given-names>J</given-names></name><name><surname>Su</surname><given-names>H</given-names></name><name><surname>Krause</surname><given-names>J</given-names></name><name><surname>Satheesh</surname><given-names>S</given-names></name><name><surname>Ma</surname><given-names>S</given-names></name><name><surname>Huang</surname><given-names>Z</given-names></name><name><surname>Karpathy</surname><given-names>A</given-names></name><name><surname>Khosla</surname><given-names>A</given-names></name><name><surname>Bernstein</surname><given-names>M</given-names></name></person-group>, <italic>et al.</italic> (<year>2015</year>). <article-title>ImageNet large scale visual recognition challenge</article-title>. <source><italic>Int J Comput Vis</italic></source>, , <fpage>211</fpage>–<lpage>252</lpage>.</mixed-citation>
    </ref>
    <ref id="B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schindelin</surname><given-names>J</given-names></name><name><surname>Arganda-Carreras</surname><given-names>I</given-names></name><name><surname>Frise</surname><given-names>E</given-names></name><name><surname>Kaynig</surname><given-names>V</given-names></name><name><surname>Longair</surname><given-names>M</given-names></name><name><surname>Pietzsch</surname><given-names>T</given-names></name><name><surname>Preibisch</surname><given-names>S</given-names></name><name><surname>Rueden</surname><given-names>C</given-names></name><name><surname>Saalfeld</surname><given-names>S</given-names></name><name><surname>Schmid</surname><given-names>B</given-names></name></person-group>, <italic>et al.</italic> (<year>2012</year>). <article-title>Fiji: an open-source platform for biological-image analysis</article-title>. <source><italic>Nat Methods</italic></source> , <fpage>676</fpage>–<lpage>682</lpage>.<pub-id pub-id-type="pmid">22743772</pub-id></mixed-citation>
    </ref>
    <ref id="B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt</surname><given-names>U</given-names></name><name><surname>Weigert</surname><given-names>M</given-names></name><name><surname>Broaddus</surname><given-names>C</given-names></name><name><surname>Myers</surname><given-names>G</given-names></name></person-group> (<year>2018</year>). <article-title>Cell detection with star-convex polygons</article-title>. In <source><italic>Lecture Notes in Computer Science</italic></source>, <comment>Springer</comment>, <fpage>265</fpage>–<lpage>273</lpage>. <comment>Crossref. Web</comment>.</mixed-citation>
    </ref>
    <ref id="B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider</surname><given-names>CA</given-names></name><name><surname>Rasband</surname><given-names>WS</given-names></name><name><surname>Eliceiri</surname><given-names>KW</given-names></name></person-group> (<year>2012</year>). <article-title>NIH image to ImageJ: 25 years of image analysis</article-title>. <source><italic>Nat Methods</italic></source> , <fpage>671</fpage>–<lpage>675</lpage>.<pub-id pub-id-type="pmid">22930834</pub-id></mixed-citation>
    </ref>
    <ref id="B43">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Schroff</surname><given-names>F</given-names></name><name><surname>Kalenichenko</surname><given-names>D</given-names></name><name><surname>Philbin</surname><given-names>J</given-names></name></person-group> (<year>2015</year>). <article-title>FaceNet: a unified embedding for face recognition and clustering</article-title>. In <comment>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</comment>, pp. <fpage>815</fpage>–<lpage>823</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2015.7298682</pub-id>.</mixed-citation>
    </ref>
    <ref id="B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sommer</surname><given-names>C</given-names></name><name><surname>Straehle</surname><given-names>C</given-names></name><name><surname>Kothe</surname><given-names>U</given-names></name><name><surname>Hamprecht</surname><given-names>FA</given-names></name></person-group> (<year>2011</year>). <article-title>Ilastik: interactive learning and segmentation toolkit</article-title>. In <source><italic>IEEE International Symposium on Biomedical Imaging: From Nano to Macro</italic></source> , <fpage>230</fpage>–<lpage>233</lpage>.</mixed-citation>
    </ref>
    <ref id="B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suleymanova</surname><given-names>I</given-names></name><name><surname>Balassa</surname><given-names>T</given-names></name><name><surname>Tripathi</surname><given-names>S</given-names></name><name><surname>Molnar</surname><given-names>C</given-names></name><name><surname>Saarma</surname><given-names>M</given-names></name><name><surname>Sidorova</surname><given-names>Y</given-names></name><name><surname>Horvath</surname><given-names>P</given-names></name></person-group> (<year>2018</year>). <article-title>A deep convolutional neural network approach for astrocyte detection</article-title>. <source><italic>Sci Rep</italic></source> , <fpage>12878</fpage>.<pub-id pub-id-type="pmid">30150631</pub-id></mixed-citation>
    </ref>
    <ref id="B46">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Tang</surname><given-names>X</given-names></name></person-group>, (<year>2014</year>). <article-title>Deep learning face representation from predicting 10,000 classes</article-title>. In <comment>2014 IEEE Conference on Computer Vision and Pattern Recognition</comment>, pp. <fpage>1891–-1898</fpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2014.244</pub-id>.</mixed-citation>
    </ref>
    <ref id="B47">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Taigman</surname><given-names>Y</given-names></name><name><surname>Yang</surname><given-names>M</given-names></name><name><surname>Ranzato</surname><given-names>M’A</given-names></name><name><surname>Wolf</surname><given-names>L</given-names></name></person-group> (<year>2014</year>). <article-title>DeepFace: closing the gap to human-level performance in face verification</article-title>. In <comment>2014 IEEE Conference on Computer Vision and Pattern Recognition. Available at</comment>
<pub-id pub-id-type="doi">10.1109/cvpr.2014.220</pub-id>.</mixed-citation>
    </ref>
  </ref-list>
</back>
