<?DTDIdentifier.IdentifierValue http://null/schema/dtds/document/fulltext/xcr/xocs-article.xsd?>
<?DTDIdentifier.IdentifierType schema?>
<?SourceDTD.DTDName xocs-article.xsd?>
<?SourceDTD.Version 1.0?>
<?ConverterInfo.XSLTName ftrr2jats.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Comput Biol Med</journal-id>
    <journal-id journal-id-type="iso-abbrev">Comput Biol Med</journal-id>
    <journal-title-group>
      <journal-title>Computers in Biology and Medicine</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">0010-4825</issn>
    <issn pub-type="epub">1879-0534</issn>
    <publisher>
      <publisher-name>Elsevier Ltd.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8048393</article-id>
    <article-id pub-id-type="pii">S0010-4825(21)00169-4</article-id>
    <article-id pub-id-type="doi">10.1016/j.compbiomed.2021.104375</article-id>
    <article-id pub-id-type="publisher-id">104375</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>CovidXrayNet: Optimizing data augmentation and CNN hyperparameters for improved COVID-19 detection from CXR</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au1">
        <name>
          <surname>Monshi</surname>
          <given-names>Maram Mahmoud A.</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
        <xref rid="aff2" ref-type="aff">b</xref>
        <xref rid="cor1" ref-type="corresp">∗</xref>
      </contrib>
      <contrib contrib-type="author" id="au2">
        <name>
          <surname>Poon</surname>
          <given-names>Josiah</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author" id="au3">
        <name>
          <surname>Chung</surname>
          <given-names>Vera</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author" id="au4">
        <name>
          <surname>Monshi</surname>
          <given-names>Fahad Mahmoud</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">c</xref>
      </contrib>
      <aff id="aff1"><label>a</label>School of Computer Science, The University of Sydney, Camperdown, NSW, 2006, Australia</aff>
      <aff id="aff2"><label>b</label>Department of Information Technology, Taif University, Taif, 26571, Saudi Arabia</aff>
      <aff id="aff3"><label>c</label>Radiology and Medical Imaging Department, King Saud University Medical City, Riyadh, 12746, Saudi Arabia</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>∗</label>Corresponding author. School of Computer Science, the University of Sydney, Camperdown, NSW, 2006, Australia.</corresp>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>15</day>
      <month>4</month>
      <year>2021</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="ppub">
      <month>6</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>15</day>
      <month>4</month>
      <year>2021</year>
    </pub-date>
    <volume>133</volume>
    <fpage>104375</fpage>
    <lpage>104375</lpage>
    <history>
      <date date-type="received">
        <day>27</day>
        <month>12</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>31</day>
        <month>3</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>1</day>
        <month>4</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2021 Elsevier Ltd. All rights reserved.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Elsevier Ltd</copyright-holder>
      <license>
        <license-p>Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.</license-p>
      </license>
    </permissions>
    <abstract id="abs0010">
      <p>To mitigate the spread of the current coronavirus disease 2019 (COVID-19) pandemic, it is crucial to have an effective screening of infected patients to be isolated and treated. Chest X-Ray (CXR) radiological imaging coupled with Artificial Intelligence (AI) applications, in particular Convolutional Neural Network (CNN), can speed the COVID-19 diagnostic process. In this paper, we optimize the data augmentation and the CNN hyperparameters for detecting COVID-19 from CXRs in terms of validation accuracy. This optimization increases the accuracy of the popular CNN architectures such as the Visual Geometry Group network (VGG-19) and the Residual Neural Network (ResNet-50), by 11.93% and 4.97%, respectively. We then proposed CovidXrayNet model that is based on EfficientNet-B0 and our optimization results. We evaluated CovidXrayNet on two datasets, including our generated balanced COVIDcxr dataset (960 CXRs) and the benchmark COVIDx dataset (15,496 CXRs). With only 30 epochs of training, CovidXrayNet achieves state-of-the-art accuracy of 95.82% on the COVIDx dataset in the three-class classification task (COVID-19, normal or pneumonia). The CovidXRayNet model, the COVIDcxr dataset, and several optimization experiments are publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/MaramMonshi/CovidXrayNet" id="intref0010">https://github.com/MaramMonshi/CovidXrayNet</ext-link>.</p>
    </abstract>
    <kwd-group id="kwrds0010">
      <title>Keywords</title>
      <kwd>Chest X-Ray</kwd>
      <kwd>Convolutional neural network</kwd>
      <kwd>COVID-19</kwd>
      <kwd>Data augmentation</kwd>
      <kwd>Hyperparameters</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <label>1</label>
    <title>Introduction</title>
    <p id="p0035">Coronavirus disease 2019 (COVID-19), caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), became a global pandemic in less than four months after first appearing in December 2019 in Wuhan, China. It has since reached 127.34 million confirmed cases and over 2.78 million deaths worldwide, as of March 30, 2021 [<xref rid="bib1" ref-type="bibr">1</xref>]. This caused devastating issues in public health and the global economy. COVID-19 patients may have one or more of the following symptoms: fever, cough, sore throat, headache, fatigue, muscle pain, and shortness of breath [<xref rid="bib2" ref-type="bibr">2</xref>]. Early detection of positive COVID-19 cases is the most critical factor in slowing the spread of this pandemic.</p>
    <p id="p0040">The golden standard for diagnosing COVID-19 patients is the reverse transcriptase-polymerase chain reaction (RT-PCR) testing, which detects SARS-CoV-2 through collected respiratory specimens of nasopharyngeal or oropharyngeal swabs [<xref rid="bib3" ref-type="bibr">3</xref>]. However, RT-PCR testing is time-consuming, laborious, and shows poor sensitivity [<xref rid="bib4" ref-type="bibr">4</xref>]. Alternatively, chest radiography imaging, including computed tomography (CT) or chest X-ray, may be examined by a radiologist to inspect any visual indicators linked to SARS-CoV-2 [<xref rid="bib5" ref-type="bibr">5</xref>]. Whereas CT scans have greater image details, CXR images are more accessible, portable, and offer rapid triaging. CXR imaging is more accessible in most healthcare systems than CT scanners that require expensive equipment and maintenance. The portability of the CXR system reduces the risk of COVID-19 transmission by performing the exams within the isolation room, which is not possible with the fixed CT scanners. Importantly, CXR allows rapid triaging of suspected COVID-19 cases in most affected countries like the USA, Spain, and Italy where they have run out of both capacity and PC-RCT testing supplies [<xref rid="bib6" ref-type="bibr">6</xref>]. Combining laboratory results with radiological image features can speed the process of COVID-19 detection.</p>
    <p id="p0045">Artificial Intelligence (AI) applications coupled with chest radiological imaging can speed the COVID-19 diagnosing process. Deep Learning (DL), in particular, enables AI-based models to achieve accurate results without manual feature extraction [<xref rid="bib7" ref-type="bibr">7</xref>]. For example, a Convolution Neural Network (CNN), which is a supervised DL approach, has recently gained popularity among the research community of AI in medicine. For COVID-19 detection from chest x-ray images, CNN produced the best classification accuracy compared to other classification techniques, such as Artificial Neural Network (<funding-source id="gs2">ANN</funding-source>), Support Vector Machine (SVM) and K-Nearest Neighbor (KNN) [<xref rid="bib8" ref-type="bibr">8</xref>].</p>
    <p id="p0050">Typically, the CNN model is created by combining one or more of the following: a convolution layer, a pooling layer, and a fully connected layer that extracts features from the input, minimizes the size for computational performance, and classifies an image, respectively. Simultaneously, the CNN model adjusts its internal parameters to achieve a specific task, like classifying chest X-rays [<xref rid="bib9" ref-type="bibr">9</xref>,<xref rid="bib10" ref-type="bibr">10</xref>]. The performance of such CNN models can be improved in various ways, including optimizing data augmentation and CNN hyperparameters.</p>
    <sec id="sec1.1">
      <label>1.1</label>
      <title>Data augmentation</title>
      <p id="p0055">A method that artificially inflates the original training set <inline-formula><mml:math id="M1" altimg="si11.svg"><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:math></inline-formula> with label preserving transformations is a data augmentation method. It can be mapped as <inline-formula><mml:math id="M2" altimg="si1.svg"><mml:mrow><mml:mi>φ</mml:mi><mml:mo>:</mml:mo><mml:mi>S</mml:mi><mml:mo>↦</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M3" altimg="si12.svg"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> is the augmented set of <inline-formula><mml:math id="M4" altimg="si11.svg"><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:math></inline-formula>. The label preserving transformation means that if image <inline-formula><mml:math id="M5" altimg="si2.svg"><mml:mrow><mml:mi>x</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">⊂</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:math></inline-formula>, then <inline-formula><mml:math id="M6" altimg="si3.svg"><mml:mrow><mml:mi>φ</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">⊂</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:math></inline-formula> [<xref rid="bib11" ref-type="bibr">11</xref>]. Hence, the artificially enlarged training set is defined as <inline-formula><mml:math id="M7" altimg="si4.svg"><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mi>S</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">∪</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M8" altimg="si5.svg"><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> consists of <inline-formula><mml:math id="M9" altimg="si11.svg"><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:math></inline-formula> and the corresponding transformations denoted by <inline-formula><mml:math id="M10" altimg="si15.svg"><mml:mrow><mml:mi mathvariant="normal">φ</mml:mi></mml:mrow></mml:math></inline-formula>. Resizing, flipping, and zooming are examples of data augmentation methods.</p>
      <p id="p0060">Data augmentation improves CNN performance [<xref rid="bib12" ref-type="bibr">12</xref>], prevents over-fitting [<xref rid="bib11" ref-type="bibr">11</xref>], and is easy to implement [<xref rid="bib13" ref-type="bibr">13</xref>]. Training a CNN on limited data, such as COVID-19 data, inhibits its ability to generalize results to unseen data due to the over-fitting issue. However, inflating the dataset by using data augmentation methods adds more invariant cases and thus prevents over-fitting. In addition, generic methods are easy to implement and computationally inexpensive. Several recent works have proved the benefit of data augmentation in improving CNN-based models for various DL applications [<xref rid="bib12" ref-type="bibr">12</xref>]. [<xref rid="bib11" ref-type="bibr">11</xref>]. [<xref rid="bib13" ref-type="bibr">13</xref>]. However, limited existing methods specifically address data augmentation in detecting COVID-19 from chest x-rays. A shortcoming of existing studies is the limited amount of data augmentation methods evaluated. As such, this is the investigative scope of our paper because data augmentation leads to positive results when training CNN on limited data but only with suitable augmentation techniques for each dataset [<xref rid="bib14" ref-type="bibr">14</xref>].</p>
      <p id="p0065">As there is an endless array of mappings <inline-formula><mml:math id="M11" altimg="si6.svg"><mml:mrow><mml:mi>φ</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, we examine common data augmentation methods, including resizing, flipping, rotating, zooming, warping, lighting and normalizing. Our investigative space was determined by consultations with practical radiologist and research on common techniques in the literature. From a radiologist's perspective, the use of portable devices, that minimize the infection control issues of COVID-19 result in low-quality CXRs and incorrect rotation. From the literature's perspective, researchers tend to apply resizing, zooming, warping, and lighting to increase the number of cases to handle the issue of limited COVID-19 data.</p>
    </sec>
    <sec id="sec1.2">
      <label>1.2</label>
      <title>CNN hyperparameters</title>
      <p id="p0070">CNN hyperparameter optimization, on the other hand, aims to find the optimal combination of values that must be selected for a given dataset before the training starts in a reasonable amount of time (e.g., the number of epochs). Deep learning practitioners aim to identify such values through automatic software, such as Optuna [<xref rid="bib15" ref-type="bibr">15</xref>], or through a trial and error method. For example, Nishio et al. [<xref rid="bib16" ref-type="bibr">16</xref>] utilize Optuna to implement Bayesian optimization in segmenting the lungs from severely abnormal CXRs.</p>
    </sec>
    <sec id="sec1.3">
      <label>1.3</label>
      <title>Contribution</title>
      <p id="p0075">The main contribution of this study is the implementation of the CovidXrayNet model, which improves the detection rate of COVID-19 from CXRs, by means of optimizing the data augmentation pipeline and CNN hyperparameters. To the best of the authors’ knowledge, CovidXrayNet is one of the first models that demonstrates the effects of data augmentation pipelines on CXR quality while also investigating several CNN hyperparameters. This in turn may significantly enhance the accuracy of CNN in diagnosing COVID-19. In addition, we introduce COVIDcxr, a balanced and complete dataset that consists of CXRs and the associated tabular data.</p>
      <p id="p0080">In this paper, we use a three-class classification (“COVID-19″, “pneumonia”, “normal”) because these three automatic predictions can help doctors to quickly triage patients for RT-PCR testing for COVID-19 diagnosis confirmation and choose the suitable treatment plan based on the presence and cause of infection (i.e., COVID-19 infection or non-COVID-19 infection). We investigate data augmentation on the COVID-19 CXR classification task to observe the differences between them in terms of the model's accuracy. We also explain and visualize the chosen data augmentation techniques on CXR, (including resizing, flipping, rotating, zooming, warping, lighting, and normalizing) to understand what happens behind the scenes.</p>
    </sec>
  </sec>
  <sec id="sec2">
    <label>2</label>
    <title>Related work</title>
    <p id="p0085">A growing number of research publications have demonstrated the compelling ability of deep learning with CNNs to automatically detect COVID-19 from chest X-ray images.</p>
    <sec id="sec2.1">
      <label>2.1</label>
      <title>Dataset</title>
      <p id="p0090"><xref rid="tbl1" ref-type="table">Table 1</xref> outlines the public datasets of COVID-19 CXR. Currently, the largest and most popular dataset among researchers is COVIDx. However, COVIDx is unbalanced as the number of cases in the COVID-19 class (589) is far less than pneumonia (6,056) and no-finding (8,851). This may cause a sharp increase and decrease in the loss values while training a DL model. To address this issue, Bridge et al. [<xref rid="bib17" ref-type="bibr">17</xref>] proposed the generalized extreme value (GEV) as an alternative to the common sigmoid activation function. They proved that GEV distribution improves the performance of COVID-19 classification from unbalanced datasets.<table-wrap position="float" id="tbl1"><label>Table 1</label><caption><p>Dataset of COVID-19 CXR.</p></caption><alt-text id="alttext0050">Table 1</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Dataset</th><th>Description</th></tr></thead><tbody><tr><td align="left">Figure 1 COVID-19 Chest X-Ray Dataset Initiative<xref rid="tbl1fna" ref-type="table-fn">a</xref></td><td align="left">56 CXR, metadata &amp; clinical notes</td></tr><tr><td align="left">ActualMed COVID-19 Chest X-Ray Dataset Initiative<xref rid="tbl1fnb" ref-type="table-fn">b</xref></td><td align="left">239 CXR, metadata &amp; clinical notes</td></tr><tr><td align="left">covid-19-ct-cxr<xref rid="tbl1fnc" ref-type="table-fn">c</xref> [<xref rid="bib18" ref-type="bibr">18</xref>]</td><td align="left">263 CXR and relevant text</td></tr><tr><td align="left">COVID-19 image data collection<xref rid="tblfnd" ref-type="table-fn">d</xref> [<xref rid="bib19" ref-type="bibr">19</xref>]</td><td align="left">654 CXR, metadata &amp; clinical notes</td></tr><tr><td align="left">COVID-19 radiography database<xref rid="tbl1fne" ref-type="table-fn">e</xref></td><td align="left">219 COVID-19, 1341 normal &amp; 1345 Pneumonia CXR</td></tr><tr><td align="left">COVIDx<xref rid="tbl1fnf" ref-type="table-fn">f</xref> [<xref rid="bib20" ref-type="bibr">20</xref>]</td><td align="left">13917 CXR for training &amp; 1579 CXR for testing</td></tr></tbody></table><table-wrap-foot><fn id="tbl1fna"><label>a</label><p id="ntpara0010"><ext-link ext-link-type="uri" xlink:href="https://github.com/agchung/Figure1-COVID-chestxray-dataset" id="intref0025">https://github.com/agchung/Figure1-COVID-chestxray-dataset</ext-link>.</p></fn></table-wrap-foot><table-wrap-foot><fn id="tbl1fnb"><label>b</label><p id="ntpara0015"><ext-link ext-link-type="uri" xlink:href="https://github.com/agchung/Actualmed-COVID-chestxray-dataset" id="intref0030">https://github.com/agchung/Actualmed-COVID-chestxray-dataset</ext-link>.</p></fn></table-wrap-foot><table-wrap-foot><fn id="tbl1fnc"><label>c</label><p id="ntpara0020"><ext-link ext-link-type="uri" xlink:href="https://github.com/ncbi-nlp/COVID-19-CT-CXR" id="intref0035">https://github.com/ncbi-nlp/COVID-19-CT-CXR</ext-link>.</p></fn></table-wrap-foot><table-wrap-foot><fn id="tblfnd"><label>d</label><p id="ntpara0025"><ext-link ext-link-type="uri" xlink:href="https://github.com/ieee8023/covid-chestxray-dataset" id="intref0040">https://github.com/ieee8023/covid-chestxray-dataset</ext-link>.</p></fn></table-wrap-foot><table-wrap-foot><fn id="tbl1fne"><label>e</label><p id="ntpara0030"><ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/tawsifurrahman/covid19-radiography-database" id="intref0045">https://www.kaggle.com/tawsifurrahman/covid19-radiography-database</ext-link>.</p></fn></table-wrap-foot><table-wrap-foot><fn id="tbl1fnf"><label>f</label><p id="ntpara0035"><ext-link ext-link-type="uri" xlink:href="https://github.com/lindawangg/COVID-Net" id="intref0050">https://github.com/lindawangg/COVID-Net</ext-link>.</p></fn></table-wrap-foot></table-wrap></p>
      <p id="p0095">These COVID-19 CXR datasets are constantly updated with new images added by researchers around the world. Nevertheless, none of these datasets provides complete metadata for all patients.</p>
    </sec>
    <sec id="sec2.2">
      <label>2.2</label>
      <title>Models</title>
      <p id="p0100"><xref rid="tbl2" ref-type="table">Table 2</xref> summarizes the proposed CNN based models in the literature, which can be grouped into binary classification (i.e., COVID-19 or normal), and multi-class classification (COVID-19, pneumonia or normal).<table-wrap position="float" id="tbl2"><label>Table 2</label><caption><p>Models for detecting COVID-19 from CXR.</p></caption><alt-text id="alttext0055">Table 2</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Classification</th><th>Model</th><th>Acc(%)</th><th>Repositories/Datasets</th><th>COVID-19</th><th>Pneumonia</th><th>Normal</th></tr></thead><tbody><tr><td align="left" rowspan="4">Binary</td><td align="left">COVIDX-Net [<xref rid="bib21" ref-type="bibr">21</xref>]</td><td align="left">90.00</td><td align="left">COVID-19 image data collection</td><td align="left">25</td><td align="left">_</td><td align="left">25</td></tr><tr><td align="left">CovXNet [<xref rid="bib22" ref-type="bibr">22</xref>]</td><td align="left">97.40</td><td align="left">Guangzhou Medical Center in China &amp; Sylhet Medical College in Bangladesh</td><td align="left">305</td><td align="left">_</td><td align="left">305</td></tr><tr><td align="left">ResNet-50 [<xref rid="bib23" ref-type="bibr">23</xref>]</td><td align="left">98.00</td><td align="left">COVID-19 image data collection &amp; Kaggle</td><td align="left">50</td><td align="left">_</td><td align="left">50</td></tr><tr><td align="left">DarkCovidNet [<xref rid="bib24" ref-type="bibr">24</xref>]</td><td align="left">98.08</td><td align="left">COVID-19 image data collection &amp; ChestXray-14</td><td align="left">125</td><td align="left">_</td><td align="left">500</td></tr><tr><td align="left" rowspan="6">Multi-class</td><td align="left">VGG-16 [<xref rid="bib25" ref-type="bibr">25</xref>]</td><td align="left">83.68</td><td align="left">COVID-19 image data collection &amp; Radiological Society of North America (RSNA)</td><td align="left">215</td><td align="left">533</td><td align="left">500</td></tr><tr><td align="left">DarkCovidNet [<xref rid="bib24" ref-type="bibr">24</xref>]</td><td align="left">87.02</td><td align="left">COVID-19 image data collection &amp; ChestXray-14</td><td align="left">125</td><td align="left">500</td><td align="left">500</td></tr><tr><td align="left">CovXNet [<xref rid="bib22" ref-type="bibr">22</xref>]</td><td align="left">90.30</td><td align="left">Guangzhou Medical Center in China &amp; Sylhet Medical College in Bangladesh</td><td align="left">305</td><td align="left">305-Viral 305-Bacterial</td><td align="left">305</td></tr><tr><td align="left">COVID-Net [<xref rid="bib20" ref-type="bibr">20</xref>]</td><td align="left">93.30</td><td align="left">COVIDx</td><td align="left">53</td><td align="left">5526</td><td align="left">8066</td></tr><tr><td align="left">MobileNet-v2 [<xref rid="bib26" ref-type="bibr">26</xref>]</td><td align="left">94.72</td><td align="left">COVID-19 image data collection, Radiological Society of North America (RSNA), Radiopaedia, Italian Society of Medical &amp; Interventional Radiology (SIRM) &amp; Kermany dataset</td><td align="left">224</td><td align="left">700</td><td align="left">504</td></tr><tr><td align="left">CNN-SVM [<xref rid="bib27" ref-type="bibr">27</xref>]</td><td align="left">95.33</td><td align="left">COVID-19 image data collection, COVID-19 radiography database &amp; Kermany dataset</td><td align="left">127</td><td align="left">127</td><td align="left">127</td></tr></tbody></table></table-wrap></p>
      <p id="p0105">For the binary classification, COVIDX-Net [<xref rid="bib21" ref-type="bibr">21</xref>] achieves 90% with only 25 CXRs for COVID-19 and 25 CXRs for normal patients. Another balanced dataset with 305 cases in each class was used to train the CovXNet model, resulting in 97.40% accuracy [<xref rid="bib22" ref-type="bibr">22</xref>]. With a combination of ResNet50, InceptionV3, and Inception-ResNetV2, Narin et al. [<xref rid="bib23" ref-type="bibr">23</xref>] model achieves 98% accuracy. The 50 healthy CXRs in this study, however, belong to children (one to five years olds) from a Kaggle repository [<xref rid="bib28" ref-type="bibr">28</xref>]. Using a larger but unbalanced dataset of 1125 images, DarkCovidNet [<xref rid="bib24" ref-type="bibr">24</xref>] achieves 98.08% accuracy. Beyond the classification task, Wang et al. [<xref rid="bib29" ref-type="bibr">29</xref>] localize the pulmonary location coordinates of COVID-19 (i.e., left lung, right lung, or both [bi-pulmonary]), using a residual attention network [<xref rid="bib30" ref-type="bibr">30</xref>].</p>
      <p id="p0110">For the three-class problem, Nishio et al. [<xref rid="bib25" ref-type="bibr">25</xref>] achieve 83.68% accuracy using a VGG-16 based model with a combination of data augmentation methods. By starting with a real-time object detection system, named “you only look one” (YOLO), which is based on the Darknet-19 [<xref rid="bib31" ref-type="bibr">31</xref>] classifier, DarkCovidNet achieves 87.02% accuracy [<xref rid="bib24" ref-type="bibr">24</xref>]. However, this result could be biased due to the small number of COVID-19 cases (125), compared to 500 pneumonia cases and 500 normal cases. To compensate for this issue, Mahmud et al. [<xref rid="bib22" ref-type="bibr">22</xref>] transfer training from a large dataset of normal cases and viral/bacterial pneumonia cases to a small balanced COVID-19 dataset, achieving 90.3% accuracy for their CovXNet model. Further, COVID-Net [<xref rid="bib20" ref-type="bibr">20</xref>] leveraged the generative synthesis [<xref rid="bib32" ref-type="bibr">32</xref>] to determine the optimal design, where COVID-19 sensitivity and positive predictive value (PPV) are at or above 80%. Conversely, Oh et al. [<xref rid="bib33" ref-type="bibr">33</xref>] proposed a patch-based CNN method that may handle the issue of small datasets, as it uses only 11.6 million trainable parameters on COVIDx. Note that the COVID-Net team is releasing enhanced versions of COVID-Net through a GitHub repository, and this paper refers to the COVID-Net-CXR3-B version. Apostolopoulos and Mpesiana [<xref rid="bib26" ref-type="bibr">26</xref>] achieve a 93.48% performance by transferring the learning of MobileNet v2 [<xref rid="bib34" ref-type="bibr">34</xref>]. They conclude that MobileNet v2 is better than VGG-19 [<xref rid="bib35" ref-type="bibr">35</xref>] for this particular COVID-19 classification task, as it has the fewest instances of False Negatives. <funding-source id="gs3">Furthermore</funding-source>, Sethy et al. [<xref rid="bib27" ref-type="bibr">27</xref>] add a support vector machine (SVM) to classify the features obtained from CNN models and achieved 95.33% accuracy.</p>
      <p id="p0115"><xref rid="tbl3" ref-type="table">Table 3</xref> and <xref rid="tbl4" ref-type="table">Table 4</xref>
outline the data augmentation and the CNN hyperparameters in recent proposed models, respectively. Nishio et al. [<xref rid="bib25" ref-type="bibr">25</xref>] show that combining multiple data augmentation techniques is more effective than only using one or not using any data augmentation in detecting COVID-19 from CXRs. They utilize a random search [<xref rid="bib36" ref-type="bibr">36</xref>] to select the optimal VGG-16 hyperparameters and data augmentation methods, including conventional method and mixup [<xref rid="bib37" ref-type="bibr">37</xref>]. This resulted in an increase in their model's accuracy from the initial 78.72%–83.68%. However, this approach of hyperparameter tuning is hard to achieve with complex networks such as EfficientNet [<xref rid="bib38" ref-type="bibr">38</xref>] due to the large number of trainable parameters.<table-wrap position="float" id="tbl3"><label>Table 3</label><caption><p>Data augmentation for detecting COVID-19 from CXR.</p></caption><alt-text id="alttext0060">Table 3</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Model</th><th>Software</th><th>Norm.</th><th>Size</th><th>Flip</th><th>Rotate</th><th>Zoom</th><th>Light</th><th>Extra</th></tr></thead><tbody><tr><td align="left" rowspan="2">VGG-16 [<xref rid="bib25" ref-type="bibr">25</xref>]</td><td align="left">Keras, Tenserflow</td><td align="left">_</td><td align="left">220*220</td><td align="left">HORIZ</td><td align="left">15</td><td align="left">85-</td><td align="left">_</td><td align="left">shear transformation</td></tr><tr><td/><td/><td/><td/><td/><td align="left">115%</td><td/><td align="left">mixup: 0.1</td></tr><tr><td align="left">DarkCovidNet [<xref rid="bib24" ref-type="bibr">24</xref>]</td><td align="left">fastai v1, Pytorch</td><td align="left">yes</td><td align="left">256*256</td><td align="left">_</td><td align="left">_</td><td align="left">_</td><td align="left">_</td><td align="left">defult values of fastai</td></tr><tr><td align="left" rowspan="2">CovXNet [<xref rid="bib22" ref-type="bibr">22</xref>]</td><td align="left">Keras, Tenserflow</td><td align="left">yes</td><td align="left">uniform</td><td align="left">_</td><td align="left">30</td><td align="left">0.2</td><td align="left">_</td><td align="left">rescale: 1/255</td></tr><tr><td/><td/><td/><td/><td/><td/><td/><td align="left">shift: 0.1</td></tr><tr><td align="left">COVID-Net [<xref rid="bib20" ref-type="bibr">20</xref>]</td><td align="left">Keras, Tenserflow</td><td align="left">yes</td><td align="left">480*480</td><td align="left">HORIZ</td><td align="left">yes</td><td align="left">yes</td><td align="left">_</td><td align="left">intensity shift</td></tr><tr><td align="left">MobileNet-v2 [<xref rid="bib26" ref-type="bibr">26</xref>]</td><td align="left">_</td><td align="left">_</td><td align="left">200*266</td><td align="left">_</td><td align="left">_</td><td align="left">_</td><td align="left">_</td><td align="left">blackground: 1:1.5</td></tr></tbody></table></table-wrap><table-wrap position="float" id="tbl4"><label>Table 4</label><caption><p>CNN hyperparameters for detecting COVID-19 from CXR.</p></caption><alt-text id="alttext0065">Table 4</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Model</th><th>CNN</th><th>Pretrained</th><th>Optimizer</th><th>Learning Rate</th><th>Loss Function</th><th>Epoch</th><th>Batch</th></tr></thead><tbody><tr><td align="left">VGG-16 [<xref rid="bib25" ref-type="bibr">25</xref>]</td><td align="left">VGG-16</td><td align="left">yes</td><td align="left">Adam</td><td align="left">1e-4</td><td align="left">cross entropy</td><td align="left">100</td><td align="left">8</td></tr><tr><td align="left">DarkCovidNet [<xref rid="bib24" ref-type="bibr">24</xref>]</td><td align="left">YOLO DarkNet-19</td><td/><td/><td align="left">3e-3</td><td/><td align="left">100</td><td align="left">32</td></tr><tr><td align="left">CovXNet [<xref rid="bib22" ref-type="bibr">22</xref>]</td><td align="left">CovXNet</td><td/><td/><td align="left">1e-3</td><td/><td align="left">70</td><td align="left">128</td></tr><tr><td align="left">COVID-Net [<xref rid="bib20" ref-type="bibr">20</xref>]</td><td align="left">COVID-Net</td><td/><td/><td align="left">2e-4 &amp; lr policy</td><td align="left">_</td><td align="left">22</td><td align="left">64</td></tr><tr><td align="left">MobileNet v2 [<xref rid="bib26" ref-type="bibr">26</xref>]</td><td align="left">MobileNet v2</td><td/><td/><td align="left">_</td><td align="left">_</td><td align="left">10</td><td align="left">64</td></tr></tbody></table></table-wrap></p>
      <p id="p0120">In terms of optimizing CNN hyperparameters, existing models used pre-trained architectures on ImageNet, Adam optimizer [<xref rid="bib39" ref-type="bibr">39</xref>], epochs that ranged from 10 to 100, and a batch size of 8, 32, 64 or 128. Notably, several proposed architectures apply few arbitrary transformers to the X-rays based on random choices rather than well-justified motives. For instance, Ozturk et al. [<xref rid="bib24" ref-type="bibr">24</xref>] apply the default values in the fastai v1 library. However, selecting the optimal CNN hyperparameters and data augmentation methods improves the robustness of CNN models [<xref rid="bib13" ref-type="bibr">13</xref>].</p>
    </sec>
  </sec>
  <sec id="sec3">
    <label>3</label>
    <title>Proposed model</title>
    <sec id="sec3.1">
      <label>3.1</label>
      <title>Dataset</title>
      <p id="p0125">We trained CovidXrayNet on two datasets called COVIDcxr and COVIDx (refer to <xref rid="fig1" ref-type="fig">Fig. 1</xref>
). Both datasets contain three classes of CXR: COVID-19 viral infection, pneumonia (i.e., non-COVID-19 infection such as viral and bacterial), and normal (i.e, no infection).<fig id="fig1"><label>Fig. 1</label><caption><p>Dataset distribution.</p></caption><alt-text id="alttext0010">Fig. 1</alt-text><graphic xlink:href="gr1_lrg"/></fig></p>
      <p id="p0130">COVIDcxr is the dataset of COVID-19 that we have generated from two open-source repositories: ChestX-Ray14 [<xref rid="bib40" ref-type="bibr">40</xref>] and COVID-19 image data collection [<xref rid="bib19" ref-type="bibr">19</xref>] with the associated tabular data (i.e., gender, sex, and view) for each patient. It is comprised of 960 CXR images. Our aim was to create a balanced, unbiased, and complete COVID-19 CXR dataset. We randomly selected 320 no-finding and 320 pneumonia from ChestX-Ray14, and 320 COVID-19 from the COVID-19 image data collection, along with complete metadata. There are 568 male and 392 female cases, and the average age of these subjects is about 56 years.</p>
      <p id="p0135">COVIDx [<xref rid="bib20" ref-type="bibr">20</xref>] is the largest public dataset in terms of presented positive COVID-19 cases. It includes 15,496 CXRs generated from five public datasets, where three of them— COVID-19 Image Data Collection, Figure 1 COVID-19 Chest X-Ray Dataset Initiative, and ActualMed COVID-19 Chest X-Ray Dataset Initiative can be downloaded from the GitHub repository, and two datasets—RSNA Pneumonia Detection challenge dataset and COVID-19 radiography database can be obtained from Kaggle. Note that COVIDx is expanding on a regular basis with the addition of new patient records for training while maintaining the same test dataset for consistency. We employed COVIDx-v3 in this research.</p>
    </sec>
    <sec id="sec3.2">
      <label>3.2</label>
      <title>CovidXrayNet architecture</title>
      <p id="p0140">The overall structure of our proposed CovidXrayNet model, which classifies a CXR into either “COVID-19″, “normal”, or “pneumonia”, is presented in <xref rid="fig2" ref-type="fig">Fig. 2</xref>
. Before feeding the CXRs to the pre-trained EfficientNet-B0 along with the optimized CNN hyperparameters, we performed several augmentation techniques on the data.<fig id="fig2"><label>Fig. 2</label><caption><p>CovidXrayNet structure.</p></caption><alt-text id="alttext0015">Fig. 2</alt-text><graphic xlink:href="gr2_lrg"/></fig></p>
      <sec id="sec3.2.1">
        <label>3.2.1</label>
        <title>Data augmentation</title>
        <p id="p0145">First, we performed several deliberate data augmentations, based on extensive experiments on the COVIDcxr dataset using ResNet18 as it can be seen in <xref rid="tbl5" ref-type="table">Table 5</xref>
. <xref rid="fig3" ref-type="fig">Fig. 3</xref>
plots all transformer techniques against each other to observe the differences between them.<table-wrap position="float" id="tbl5"><label>Table 5</label><caption><p>Pipeline for Data Augmentation on CXR. For each independent parameter, we trained ResNet-18 on COVIDcxr for 30 epochs to examine the effects of various transformers on COVID-19 CXR classification.</p></caption><alt-text id="alttext0070">Table 5</alt-text><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Independent Parameter</th><th colspan="2">Resize<hr/></th><th rowspan="2">Rotate</th><th rowspan="2">Zoom</th><th rowspan="2">Wrap</th><th rowspan="2">Light</th><th rowspan="2">Extra</th><th colspan="3">(%)<hr/></th></tr><tr><th>Size</th><th>Method</th><th>Acc</th><th>AUC</th><th>F1</th></tr></thead><tbody><tr><td align="left" rowspan="12"><bold>Resize</bold></td><td align="left">224*224</td><td align="left">crop</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">none</td><td align="left">78.12</td><td align="left">90.84</td><td align="left">78.04</td></tr><tr><td/><td align="left">pad</td><td/><td/><td/><td/><td/><td align="left">79.68</td><td align="left">90.66</td><td align="left">79.39</td></tr><tr><td/><td align="left">squish</td><td/><td/><td/><td/><td/><td align="left">74.47</td><td align="left">88.63</td><td align="left">74.47</td></tr><tr><td align="left">256*256</td><td align="left">crop</td><td/><td/><td/><td/><td/><td align="left">79.16</td><td align="left">92.12</td><td align="left">79.12</td></tr><tr><td/><td align="left">pad</td><td/><td/><td/><td/><td/><td align="left">76.04</td><td align="left">90.62</td><td align="left">75.55</td></tr><tr><td/><td align="left">squish</td><td/><td/><td/><td/><td/><td align="left">78.12</td><td align="left">90.15</td><td align="left">78.22</td></tr><tr><td align="left"><bold>480*480</bold></td><td align="left">crop</td><td/><td/><td/><td/><td/><td align="left">80.72</td><td align="left">94.42</td><td align="left">80.65</td></tr><tr><td/><td align="left">pad</td><td/><td/><td/><td/><td/><td align="left">82.81</td><td align="left"><bold>94.67</bold></td><td align="left">82.86</td></tr><tr><td/><td align="left"><bold>squish</bold></td><td/><td/><td/><td/><td/><td align="left"><bold>83.85</bold></td><td align="left">94.14</td><td align="left"><bold>83.95</bold></td></tr><tr><td align="left">512*512</td><td align="left">crop</td><td/><td/><td/><td/><td/><td align="left">80.72</td><td align="left">93.35</td><td align="left">80.78</td></tr><tr><td/><td align="left">pad</td><td/><td/><td/><td/><td/><td align="left">78.64</td><td align="left">93.22</td><td align="left">78.62</td></tr><tr><td/><td align="left">squish</td><td/><td/><td/><td/><td/><td align="left">77.08</td><td align="left">92.67</td><td align="left">77.22</td></tr><tr><td align="left" rowspan="5"><bold>Rotate</bold></td><td align="left">480*480</td><td align="left">squish</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">none</td><td align="left">83.85</td><td align="left">94.14</td><td align="left">83.95</td></tr><tr><td/><td/><td align="left">10</td><td/><td/><td/><td/><td align="left">85.93</td><td align="left">95.73</td><td align="left">85.95</td></tr><tr><td/><td/><td align="left"><bold>20</bold></td><td/><td/><td/><td/><td align="left"><bold>86.45</bold></td><td align="left"><bold>96.48</bold></td><td align="left">86.56</td></tr><tr><td/><td/><td align="left">30</td><td/><td/><td/><td/><td align="left">86.45</td><td align="left">95.97</td><td align="left"><bold>86.58</bold></td></tr><tr><td/><td/><td align="left">50</td><td/><td/><td/><td/><td align="left">84.89</td><td align="left">95.72</td><td align="left">85.03</td></tr><tr><td align="left" rowspan="5"><bold>Zoom</bold></td><td align="left">480*480</td><td align="left">squish</td><td align="left">0</td><td align="left">1</td><td align="left">0</td><td align="left">0</td><td align="left">none</td><td align="left">83.85</td><td align="left">94.14</td><td align="left">83.95</td></tr><tr><td/><td/><td/><td align="left"><bold>1.2</bold></td><td/><td/><td/><td align="left"><bold>85.41</bold></td><td align="left"><bold>95.86</bold></td><td align="left"><bold>85.48</bold></td></tr><tr><td/><td/><td/><td align="left">1.3</td><td/><td/><td/><td align="left">82.29</td><td align="left">95.77</td><td align="left">82.37</td></tr><tr><td/><td/><td/><td align="left">1.4</td><td/><td/><td/><td align="left">84.37</td><td align="left">95.60</td><td align="left">84.45</td></tr><tr><td/><td/><td/><td align="left">1.5</td><td/><td/><td/><td align="left">81.25</td><td align="left">95.29</td><td align="left">81.21</td></tr><tr><td align="left" rowspan="4"><bold>Warp</bold></td><td align="left">480*480</td><td align="left">squish</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">none</td><td align="left">83.85</td><td align="left">94.14</td><td align="left">83.95</td></tr><tr><td/><td/><td/><td/><td align="left">0.1</td><td/><td/><td align="left">84.37</td><td align="left">95.36</td><td align="left">84.42</td></tr><tr><td/><td/><td/><td/><td align="left"><bold>0.2</bold></td><td/><td/><td align="left"><bold>85.41</bold></td><td align="left">96.33</td><td align="left"><bold>85.50</bold></td></tr><tr><td/><td/><td/><td/><td align="left">0.3</td><td/><td/><td align="left">84.89</td><td align="left"><bold>96.34</bold></td><td align="left">84.94</td></tr><tr><td align="left" rowspan="6"><bold>Lighting</bold></td><td align="left">480*480</td><td align="left">squish</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">none</td><td align="left">83.85</td><td align="left">94.14</td><td align="left">83.95</td></tr><tr><td/><td/><td/><td/><td/><td align="left">0.1</td><td/><td align="left">81.77</td><td align="left">93.12</td><td align="left">81.93</td></tr><tr><td/><td/><td/><td/><td/><td align="left">0.2</td><td/><td align="left">83.85</td><td align="left">94.34</td><td align="left">83.91</td></tr><tr><td/><td/><td/><td/><td/><td align="left"><bold>0.3</bold></td><td/><td align="left"><bold>85.41</bold></td><td align="left">95.10</td><td align="left"><bold>85.46</bold></td></tr><tr><td/><td/><td/><td/><td/><td align="left">0.4</td><td/><td align="left">82.81</td><td align="left">95.34</td><td align="left">82.97</td></tr><tr><td/><td/><td/><td/><td/><td align="left">0.5</td><td/><td align="left">84.37</td><td align="left"><bold>95.89</bold></td><td align="left">84.46</td></tr><tr><td align="left"><bold>Flip (dihedral)</bold></td><td align="left">480*480</td><td align="left">squish</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">flip</td><td align="left">83.85</td><td align="left"><bold>95.69</bold></td><td align="left">83.81</td></tr><tr><td align="left"><bold>Mixup (0.4)</bold></td><td/><td/><td/><td/><td/><td/><td align="left">mixup</td><td align="left">83.33</td><td align="left">94.88</td><td align="left">83.29</td></tr><tr><td align="left"><bold>Erasing (random)</bold></td><td/><td/><td/><td/><td/><td/><td align="left">erase</td><td align="left">80.72</td><td align="left">94.11</td><td align="left">80.91</td></tr><tr><td align="left"><bold>Normalize (imagenet)</bold></td><td/><td/><td/><td/><td/><td/><td align="left"><bold>norm</bold></td><td align="left"><bold>83.85</bold></td><td align="left">94.14</td><td align="left"><bold>83.95</bold></td></tr><tr><td align="left" rowspan="4"><bold>Multiple Param (pipline)</bold></td><td align="left">480*480</td><td align="left">squish</td><td align="left">20</td><td align="left">1.2</td><td align="left">0.2</td><td align="left">0.3</td><td align="left">flip</td><td align="left">81.77</td><td align="left">95.70</td><td align="left">81.69</td></tr><tr><td align="left">480*480</td><td align="left">squish</td><td align="left">20</td><td align="left">1.2</td><td align="left">0.2</td><td align="left">0.3</td><td align="left">mixup</td><td align="left">82.81</td><td align="left">95.86</td><td align="left">82.48</td></tr><tr><td align="left">480*480</td><td align="left">squish</td><td align="left">20</td><td align="left">1.2</td><td align="left">0.2</td><td align="left">0.3</td><td align="left">flip, norm</td><td align="left">81.77</td><td align="left">95.70</td><td align="left">81.69</td></tr><tr><td align="left"><bold>480*480</bold></td><td align="left"><bold>squish</bold></td><td align="left"><bold>20</bold></td><td align="left"><bold>1.2</bold></td><td align="left"><bold>0.2</bold></td><td align="left"><bold>0.3</bold></td><td align="left"><bold>norm</bold></td><td align="left"><bold>88.02</bold></td><td align="left"><bold>96.20</bold></td><td align="left"><bold>88.14</bold></td></tr></tbody></table></table-wrap><fig id="fig3"><label>Fig. 3</label><caption><p>Visualizing Data Augmentation Effects on CXR. The CXR is for a 25-year-old COVID-19-positive female taken from the COVID-19 image data collection.</p></caption><alt-text id="alttext0020">Fig. 3</alt-text><graphic xlink:href="gr3_lrg"/></fig></p>
        <p id="p0150">At the item transformation level, we resized each CXR to 480<italic>x</italic>480 pixels by squishing the CXR on the horizontal axis on the Central Processing Unit (CPU). This constricts the ribcage towards the center while keeping all the parts of the CXR. Our method is different from the common approach in the literature, which resizes each CXR to the same aspect ratio to set the smallest dimension to a specified size and then arbitrarily crops it on the other dimension, as illustrated in <xref rid="fig4" ref-type="fig">Fig. 4</xref>
. This cropping method may erase important CXR details from the edges of the image. Resizing all CXRs to a fixed size is a prerequisite data augmentation for classifying them using CNN.<fig id="fig4"><label>Fig. 4</label><caption><p>Resizing Method. We propose to squish a 480*480 pixel CXR rather than cropping it to maintain important CXR details from the edges of the image.</p></caption><alt-text id="alttext0025">Fig. 4</alt-text><graphic xlink:href="gr4_lrg"/></fig></p>
        <p id="p0155">At the batch transformation level, we applied a group of optimized augmentation parameters on a Graphical Processing Unit (GPU) to minimize the number of computation and lossy operations. We used pipeline to compose the best transformers’ values together. A series of experiments on the COVIDcxr dataset, with a fixed seed, was used to find the best combination of choices and orders of data augmentation that ensured the ResNet-18 gave the best accuracy, as recorded in <xref rid="tbl5" ref-type="table">Table 5</xref>.</p>
        <p id="p0160">We applied a random rotation with a maximum of 20° and 75% probability to overcome the incorrect rotation of some of the acquired images. Such low-quality CXRs are the result of using portable devices that minimize the infection control issues of COVID-19 [<xref rid="bib41" ref-type="bibr">41</xref>]. In addition, it is not uncommon, especially for anteroposterior (AP) supine CXRs, for the patient to be rotated, which makes interpretation difficult. In addition to rotating CXRs, we also applied zooming, warping, and lighting as we relied on data augmentation to handle the issue of limited COVID-19 data through increasing the number of cases [<xref rid="bib12" ref-type="bibr">12</xref>] and, hence, preventing overfitting. With a 75% probability, we zoomed the CXRs by a scale of 1.2, lighted by a scale of 0.3, and warped by a magnitude of 0.2. Warping and ightening augmentation may handle situations when patients face the X-ray device at different angles and in various lighting rooms. We attempted to apply the random erasing [<xref rid="bib42" ref-type="bibr">42</xref>] and the mixup [<xref rid="bib37" ref-type="bibr">37</xref>] techniques, however, we did not notice improved performance.</p>
      </sec>
      <sec id="sec3.2.2">
        <label>3.2.2</label>
        <title>CNN architectures and hyperparameters</title>
        <p id="p0165">Second, we replaced the head of EfficientNet-B0 with a head suitable for the three-class classification and trained it for 30 epochs. To compensate for the small dataset, we perform transfer learning with the pre-trained weights from ImageNet. Then, we fine-tuned EfficientNet-B0 using one NVIDIA Tesla V100. EfficientNet scales the CovidXrayNets’ width and depth according to the size of 480<italic>x</italic>480 pixels, which results in substantially less computational power used and fewer parameters with high performance compared to other CNN architectures.</p>
        <p id="p0170"><xref rid="tbl6" ref-type="table">Table 6</xref> presents the performance of the optimized data augmentation on two datasets COVIDcxr (small and balanced dataset) and COVIDx (large and unbalanced dataset) using the benchmark deep neural network architectures including: VGG-16, VGG-19 [<xref rid="bib35" ref-type="bibr">35</xref>], ResNet-18, ResNet-34, ResNet-50 [<xref rid="bib43" ref-type="bibr">43</xref>], and EfficientNet-B0 [<xref rid="bib38" ref-type="bibr">38</xref>]. Among various CNN architectures, EfficientNet-B0 accomplishes the best results in classifying COVID-19 from the COVIDcxr and COVIDx datasets based on various evaluation metrics such as accuracy, precision, recall and F1 scores. Please refer to Section <xref rid="sec3.3" ref-type="sec">3.3</xref> for more details about these evaluation metrics.<table-wrap position="float" id="tbl6"><label>Table 6</label><caption><p>CNN Architectures on COVIDx and COVIDcxr. We trained the popular CNN architectures on both datasets for 30 epochs using the optimized data augmentation pipeline.</p></caption><alt-text id="alttext0075">Table 6</alt-text><table frame="hsides" rules="groups"><thead><tr><th>CNN</th><th>Dataset</th><th>Acc (%)</th><th>AUC (%)</th><th>MCC (%)</th><th>Precision (%)</th><th>Recall (%)</th><th>F1 (%)</th></tr></thead><tbody><tr><td align="left">VGG-16</td><td align="left">COVIDcxr</td><td align="left">80.73</td><td align="left">94.68</td><td align="left">72.29</td><td align="left">82.03</td><td align="left">81.35</td><td align="left">80.53</td></tr><tr><td align="left">VGG-19</td><td/><td align="left">84.90</td><td align="left">95.67</td><td align="left">77.74</td><td align="left">85.31</td><td align="left">85.26</td><td align="left">84.92</td></tr><tr><td align="left">ResNet-18</td><td/><td align="left">85.94</td><td align="left">96.72</td><td align="left">79.40</td><td align="left">86.84</td><td align="left">86.31</td><td align="left">86.14</td></tr><tr><td align="left">ResNet-34</td><td/><td align="left">79.69</td><td align="left">94.91</td><td align="left">70.02</td><td align="left">80.26</td><td align="left">80.03</td><td align="left">79.70</td></tr><tr><td align="left">ResNet-50</td><td/><td align="left">82.81</td><td align="left">95.90</td><td align="left">75.31</td><td align="left">84.90</td><td align="left">83.29</td><td align="left">83.12</td></tr><tr><td align="left"><bold>EfficientNet-B0</bold></td><td/><td align="left"><bold>88.02</bold></td><td align="left">_</td><td align="left"><bold>82.01</bold></td><td align="left"><bold>87.98</bold></td><td align="left"><bold>88.03</bold></td><td align="left"><bold>88.00</bold></td></tr><tr><td align="left">VGG-16</td><td align="left">COVIDx</td><td align="left">93.41</td><td align="left">98.70</td><td align="left">87.74</td><td align="left">94.40</td><td align="left">89.41</td><td align="left">91.61</td></tr><tr><td align="left">VGG-19</td><td/><td align="left">93.60</td><td align="left">98.55</td><td align="left">88.06</td><td align="left">95.29</td><td align="left">85.53</td><td align="left">89.24</td></tr><tr><td align="left">ResNet-18</td><td/><td align="left">93.29</td><td align="left">98.86</td><td align="left">87.48</td><td align="left">95.03</td><td align="left">86.73</td><td align="left">90.05</td></tr><tr><td align="left">ResNet-34</td><td/><td align="left">94.74</td><td align="left">99.10</td><td align="left">90.19</td><td align="left">95.85</td><td align="left">89.95</td><td align="left">92.53</td></tr><tr><td align="left">ResNet50</td><td/><td align="left">95.12</td><td align="left">99.22</td><td align="left">90.92</td><td align="left">96.08</td><td align="left">91.76</td><td align="left">93.72</td></tr><tr><td align="left"><bold>EfficientNet-B0</bold></td><td/><td align="left"><bold>95.69</bold></td><td align="left">_</td><td align="left"><bold>92.01</bold></td><td align="left"><bold>96.24</bold></td><td align="left"><bold>94.76</bold></td><td align="left"><bold>95.48</bold></td></tr></tbody></table></table-wrap></p>
        <p id="p0175">EfficientNet introduced a new and simple compound scaling technique to scale the number of layers <inline-formula><mml:math id="M12" altimg="si16.svg"><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow></mml:math></inline-formula>, the number of channels <inline-formula><mml:math id="M13" altimg="si17.svg"><mml:mrow><mml:mi mathvariant="normal">β</mml:mi></mml:mrow></mml:math></inline-formula>, and the number of pixels <inline-formula><mml:math id="M14" altimg="si18.svg"><mml:mrow><mml:mi mathvariant="normal">γ</mml:mi></mml:mrow></mml:math></inline-formula> in an image, which represent the CNN width, depth, and resolution, respectively [<xref rid="bib38" ref-type="bibr">38</xref>], as depicted in Eq. <xref rid="fd1" ref-type="disp-formula">(1)</xref>. This technique uses a compound coefficient <inline-formula><mml:math id="M15" altimg="si18.svg"><mml:mrow><mml:mi mathvariant="normal">φ</mml:mi></mml:mrow></mml:math></inline-formula>, which defines the amount of available resources to determine how to scale <inline-formula><mml:math id="M16" altimg="si16.svg"><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M17" altimg="si17.svg"><mml:mrow><mml:mi mathvariant="normal">β</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="M18" altimg="si18.svg"><mml:mrow><mml:mi mathvariant="normal">γ</mml:mi></mml:mrow></mml:math></inline-formula>. The constraint <inline-formula><mml:math id="M19" altimg="si81.svg"><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo linebreak="badbreak">⋅</mml:mo><mml:msup><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo linebreak="badbreak">⋅</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">≈</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> is applied in order to make sure that the total floating-point operations per second (FLOPS) do not exceed <inline-formula><mml:math id="M20" altimg="si9.svg"><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mi>φ</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. CovidXrayNet is based on the baseline network EfficientNet-B0, where the optimal values are <inline-formula><mml:math id="M21" altimg="si10.svg"><mml:mrow><mml:mi>α</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1.2</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M22" altimg="si11.svg"><mml:mrow><mml:mi>β</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1.1</mml:mn></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="M23" altimg="si12.svg"><mml:mrow><mml:mi>γ</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1.15</mml:mn></mml:mrow></mml:math></inline-formula>. Using this multi-objective neural architecture search, we optimize both accuracy and FLOPS. Although the original EfficientNet-B0 uses the standard input size 224<italic>x</italic>224, it perfectly handles the 480<italic>x</italic>480 CXR pixels.<disp-formula id="fd1"><label>(1)</label><mml:math id="M24" altimg="si11.svg" alttext="Equation 1."><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mtext>depth:</mml:mtext><mml:mi>d</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mi>φ</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext>width:</mml:mtext><mml:mi>w</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:msup><mml:mi>β</mml:mi><mml:mi>φ</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext>resolution:</mml:mtext><mml:mi>r</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mi>φ</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext>s.t.</mml:mtext><mml:mi>α</mml:mi><mml:mo linebreak="badbreak">⋅</mml:mo><mml:msup><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo linebreak="badbreak">⋅</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo linebreak="badbreak">≈</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>α</mml:mi><mml:mo linebreak="badbreak">≥</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo linebreak="badbreak">≥</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>γ</mml:mi><mml:mo linebreak="badbreak">≥</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
</p>
        <p id="p0180">Furthermore, we have studied various CNN hyperparameters on COVIDcxr and COVIDx, including the loss function, the number of epochs, and the batch size, as demonstrated in <xref rid="tbl7" ref-type="table">Table 7</xref>
and <xref rid="tbl8" ref-type="table">Table 8</xref>
, respectively. Based on this trial and error method, we selected the optimal hyperparameters for EfficientNet-B0 on the COVIDx dataset including the label smoothing [<xref rid="bib44" ref-type="bibr">44</xref>] of the cross-entropy loss function, 30 epochs, and a batch size of 32. Label smoothing for our three-class problem is presented in Eq. <xref rid="fd2" ref-type="disp-formula">(2)</xref>, where <inline-formula><mml:math id="M25" altimg="si11.svg"><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>ε</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the prediction of the correct class, and <inline-formula><mml:math id="M26" altimg="si121.svg"><mml:mrow><mml:mi mathvariant="normal">ε</mml:mi></mml:mrow></mml:math></inline-formula> is the prediction of the other two classes. In this formula, <inline-formula><mml:math id="M27" altimg="si15.svg"><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> donates the standard cross-entropy loss of <inline-formula><mml:math id="M28" altimg="si131.svg"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M29" altimg="si121.svg"><mml:mrow><mml:mi mathvariant="normal">ε</mml:mi></mml:mrow></mml:math></inline-formula> is a small positive number, <inline-formula><mml:math id="M30" altimg="si222.svg"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula> is the correct class, and <inline-formula><mml:math id="M31" altimg="si132.svg"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> is the number of classes. This regularization technique improved CovidXrayNet performance and robustness by computing cross entropy with a weighted mixture of the hard targets from the COVIDx dataset using the uniform distribution.<disp-formula id="fd2"><label>(2)</label><mml:math id="M32" altimg="si16.svg" alttext="Equation 2."><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.25em"/><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>y</mml:mi><mml:mspace width="0.25em"/><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>ε</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">+</mml:mo><mml:mi>ε</mml:mi><mml:mo>∑</mml:mo><mml:mfrac><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mi>N</mml:mi></mml:mfrac></mml:mrow></mml:math></disp-formula>
<table-wrap position="float" id="tbl7"><label>Table 7</label><caption><p>Optimizing CNN hyperparameters using COVIDcxr. For each independent parameter, we trained several architectures on COVIDcxr to examine the effects of various hyperparameters on the accuracy of COVID-19 CXR classification.</p></caption><alt-text id="alttext0080">Table 7</alt-text><table frame="hsides" rules="groups"><thead><tr><th>CNN</th><th>Epoch</th><th>Batch Size</th><th>Loss Function</th><th>Acc (%)</th><th>MCC (%)</th><th>F1 (%)</th></tr></thead><tbody><tr><td align="left" rowspan="7"><bold>VGG-16</bold></td><td align="left">10</td><td align="left">32</td><td align="left"><bold>Cross Entropy</bold></td><td align="left">77.08</td><td align="left">68.15</td><td align="left">76.26</td></tr><tr><td align="left">20</td><td/><td/><td align="left">77.60</td><td align="left">66.71</td><td align="left">77.43</td></tr><tr><td align="left">30</td><td/><td/><td align="left">80.73</td><td align="left">72.29</td><td align="left">80.53</td></tr><tr><td align="left">40</td><td/><td/><td align="left">83.33</td><td align="left">75.51</td><td align="left">83.31</td></tr><tr><td align="left"><bold>30</bold></td><td align="left"><bold>8</bold></td><td/><td align="left"><bold>85.42</bold></td><td align="left"><bold>79.20</bold></td><td align="left"><bold>85.43</bold></td></tr><tr><td/><td align="left">16</td><td/><td align="left">84.38</td><td align="left">76.61</td><td align="left">84.27</td></tr><tr><td/><td align="left">32</td><td align="left">Label Smoothing</td><td align="left">79.17</td><td align="left">69.62</td><td align="left">78.91</td></tr><tr><td align="left" rowspan="7"><bold>VGG-19</bold></td><td align="left">10</td><td align="left">32</td><td align="left">Cross Entropy</td><td align="left">78.65</td><td align="left">68.38</td><td align="left">78.89</td></tr><tr><td align="left">20</td><td/><td/><td align="left">82.81</td><td align="left">74.25</td><td align="left">82.96</td></tr><tr><td align="left">30</td><td/><td/><td align="left">84.90</td><td align="left">77.74</td><td align="left">84.92</td></tr><tr><td align="left">40</td><td/><td/><td align="left">84.38</td><td align="left">76.66</td><td align="left">84.35</td></tr><tr><td align="left"><bold>30</bold></td><td align="left">8</td><td/><td align="left">84.90</td><td align="left">78.36</td><td align="left">84.96</td></tr><tr><td/><td align="left">16</td><td/><td align="left">82.81</td><td align="left">74.90</td><td align="left">82.74</td></tr><tr><td/><td align="left"><bold>32</bold></td><td align="left"><bold>Label Smoothing</bold></td><td align="left"><bold>85.42</bold></td><td align="left"><bold>78.30</bold></td><td align="left"><bold>85.51</bold></td></tr><tr><td align="left" rowspan="7"><bold>ResNet-18</bold></td><td align="left">10</td><td align="left"><bold>32</bold></td><td align="left"><bold>Cross Entropy</bold></td><td align="left">81.25</td><td align="left">73.69</td><td align="left">81.25</td></tr><tr><td align="left">20</td><td/><td/><td align="left">82.29</td><td align="left">74.21</td><td align="left">82.45</td></tr><tr><td align="left"><bold>30</bold></td><td/><td/><td align="left"><bold>85.94</bold></td><td align="left"><bold>79.40</bold></td><td align="left"><bold>86.14</bold></td></tr><tr><td align="left">40</td><td/><td/><td align="left">85.42</td><td align="left">78.16</td><td align="left">85.37</td></tr><tr><td align="left">30</td><td align="left">8</td><td/><td align="left">81.25</td><td align="left">73.56</td><td align="left">81.39</td></tr><tr><td/><td align="left">16</td><td/><td align="left">82.29</td><td align="left">74.20</td><td align="left">82.37</td></tr><tr><td/><td align="left">32</td><td align="left">Label Smoothing</td><td align="left">84.38</td><td align="left">76.95</td><td align="left">84.46</td></tr><tr><td align="left" rowspan="7"><bold>ResNet-34</bold></td><td align="left">10</td><td align="left">32</td><td align="left"><bold>Cross Entropy</bold></td><td align="left">81.25</td><td align="left">72.10</td><td align="left">81.20</td></tr><tr><td align="left">20</td><td/><td/><td align="left">81.25</td><td align="left">71.93</td><td align="left">80.91</td></tr><tr><td align="left">30</td><td/><td/><td align="left">79.69</td><td align="left">70.02</td><td align="left">79.70</td></tr><tr><td align="left">40</td><td/><td/><td align="left">81.25</td><td align="left">71.94</td><td align="left">81.23</td></tr><tr><td align="left"><bold>30</bold></td><td align="left"><bold>8</bold></td><td/><td align="left"><bold>86.46</bold></td><td align="left"><bold>80.12</bold></td><td align="left"><bold>86.54</bold></td></tr><tr><td/><td align="left">16</td><td/><td align="left">85.94</td><td align="left">79.00</td><td align="left">85.87</td></tr><tr><td/><td align="left">32</td><td align="left">Label Smoothing</td><td align="left">83.85</td><td align="left">76.08</td><td align="left">83.85</td></tr><tr><td align="left" rowspan="7"><bold>ResNet-50</bold></td><td align="left">10</td><td align="left">32</td><td align="left"><bold>Cross Entropy</bold></td><td align="left">81.77</td><td align="left">73.18</td><td align="left">82.09</td></tr><tr><td align="left">20</td><td/><td/><td align="left">84.90</td><td align="left">77.32</td><td align="left">84.93</td></tr><tr><td align="left">30</td><td/><td/><td align="left">82.81</td><td align="left">75.31</td><td align="left">83.12</td></tr><tr><td align="left">40</td><td/><td/><td align="left">85.42</td><td align="left">78.12</td><td align="left">85.45</td></tr><tr><td align="left"><bold>30</bold></td><td align="left">8</td><td/><td align="left">86.46</td><td align="left">80.49</td><td align="left">86.52</td></tr><tr><td/><td align="left"><bold>16</bold></td><td/><td align="left"><bold>86.98</bold></td><td align="left"><bold>80.84</bold></td><td align="left"><bold>87.16</bold></td></tr><tr><td/><td align="left">32</td><td align="left">Label Smoothing</td><td align="left">83.85</td><td align="left">76.21</td><td align="left">84.05</td></tr><tr><td align="left" rowspan="7"><bold>EfficientNet-B0</bold></td><td align="left">10</td><td align="left">32</td><td align="left">Cross Entropy</td><td align="left">83.33</td><td align="left">75.36</td><td align="left">83.65</td></tr><tr><td align="left">20</td><td/><td/><td align="left">84.38</td><td align="left">76.67</td><td align="left">84.41</td></tr><tr><td align="left">30</td><td/><td/><td align="left">88.02</td><td align="left">82.01</td><td align="left">88.00</td></tr><tr><td align="left">40</td><td/><td/><td align="left">85.42</td><td align="left">78.10</td><td align="left">85.42</td></tr><tr><td align="left"><bold>30</bold></td><td align="left">8</td><td/><td align="left">88.02</td><td align="left">82.06</td><td align="left">87.89</td></tr><tr><td/><td align="left">16</td><td/><td align="left">86.98</td><td align="left">80.45</td><td align="left">86.99</td></tr><tr><td/><td align="left"><bold>32</bold></td><td align="left"><bold>Label Smoothing</bold></td><td align="left"><bold>88.54</bold></td><td align="left"><bold>82.83</bold></td><td align="left"><bold>88.62</bold></td></tr></tbody></table></table-wrap><table-wrap position="float" id="tbl8"><label>Table 8</label><caption><p>Optimizing CNN hyperparameters using COVIDx. For each independent parameter, we trained several architectures on COVIDx to examine the effects of various hyperparameters on the accuracy of COVID-19 CXR classification.</p></caption><alt-text id="alttext0085">Table 8</alt-text><table frame="hsides" rules="groups"><thead><tr><th>CNN</th><th>Epoch</th><th>Batch Size</th><th>Loss Function</th><th>Acc (%)</th><th>MCC (%)</th><th>F1 (%)</th></tr></thead><tbody><tr><td align="left" rowspan="7"><bold>VGG-16</bold></td><td align="left">10</td><td align="left">32</td><td align="left"><bold>Cross Entropy</bold></td><td align="left">92.08</td><td align="left">85.20</td><td align="left">86.99</td></tr><tr><td align="left">20</td><td/><td/><td align="left">93.35</td><td align="left">87.56</td><td align="left">90.10</td></tr><tr><td align="left">30</td><td/><td/><td align="left">93.41</td><td align="left">87.74</td><td align="left">91.61</td></tr><tr><td align="left">40</td><td/><td/><td align="left">94.24</td><td align="left">89.25</td><td align="left">91.99</td></tr><tr><td align="left"><bold>30</bold></td><td align="left">8</td><td/><td align="left">93.86</td><td align="left">88.56</td><td align="left">91.03</td></tr><tr><td/><td align="left"><bold>16</bold></td><td/><td align="left"><bold>94.30</bold></td><td align="left"><bold>89.38</bold></td><td align="left"><bold>92.00</bold></td></tr><tr><td/><td align="left">32</td><td align="left">Label Smoothing</td><td align="left">94.05</td><td align="left">88.88</td><td align="left">91.35</td></tr><tr><td align="left" rowspan="7"><bold>VGG-19</bold></td><td align="left">10</td><td align="left">32</td><td align="left"><bold>Cross Entropy</bold></td><td align="left">92.53</td><td align="left">86.04</td><td align="left">87.29</td></tr><tr><td align="left">20</td><td/><td/><td align="left">93.98</td><td align="left">88.77</td><td align="left">91.57</td></tr><tr><td align="left">30</td><td/><td/><td align="left">93.60</td><td align="left">88.06</td><td align="left">89.24</td></tr><tr><td align="left">40</td><td/><td/><td align="left">93.29</td><td align="left">87.46</td><td align="left">88.72</td></tr><tr><td align="left"><bold>30</bold></td><td align="left">8</td><td/><td align="left">94.49</td><td align="left">89.73</td><td align="left">92.14</td></tr><tr><td/><td align="left"><bold>16</bold></td><td/><td align="left"><bold>94.93</bold></td><td align="left"><bold>90.56</bold></td><td align="left"><bold>92.79</bold></td></tr><tr><td/><td align="left">32</td><td align="left">Label Smoothing</td><td align="left">93.79</td><td align="left">88.40</td><td align="left">90.10</td></tr><tr><td align="left" rowspan="7"><bold>ResNet-18</bold></td><td align="left">10</td><td align="left">32</td><td align="left"><bold>Cross Entropy</bold></td><td align="left">93.10</td><td align="left">87.08</td><td align="left">88.43</td></tr><tr><td align="left">20</td><td/><td/><td align="left">93.60</td><td align="left">88.06</td><td align="left">90.07</td></tr><tr><td align="left">30</td><td/><td/><td align="left">93.29</td><td align="left">87.48</td><td align="left">90.05</td></tr><tr><td align="left">40</td><td/><td/><td align="left">93.86</td><td align="left">88.53</td><td align="left">90.87</td></tr><tr><td align="left"><bold>30</bold></td><td align="left">8</td><td/><td align="left">94.17</td><td align="left">89.11</td><td align="left">91.17</td></tr><tr><td/><td align="left"><bold>16</bold></td><td/><td align="left"><bold>94.43</bold></td><td align="left"><bold>89.60</bold></td><td align="left"><bold>92.49</bold></td></tr><tr><td/><td align="left">32</td><td align="left">Label Smoothing</td><td align="left">94.30</td><td align="left">89.35</td><td align="left">91.58</td></tr><tr><td align="left" rowspan="7"><bold>ResNet-34</bold></td><td align="left">10</td><td align="left">32</td><td align="left"><bold>Cross Entropy</bold></td><td align="left">94.05</td><td align="left">88.89</td><td align="left">91.41</td></tr><tr><td align="left">20</td><td/><td/><td align="left">94.62</td><td align="left">89.97</td><td align="left">93.32</td></tr><tr><td align="left">30</td><td/><td/><td align="left">94.74</td><td align="left">90.19</td><td align="left">92.53</td></tr><tr><td align="left">40</td><td/><td/><td align="left">94.43</td><td align="left">89.63</td><td align="left">93.38</td></tr><tr><td align="left"><bold>30</bold></td><td align="left">8</td><td/><td align="left">94.87</td><td align="left">90.44</td><td align="left">92.43</td></tr><tr><td/><td align="left"><bold>16</bold></td><td/><td align="left"><bold>95.31</bold></td><td align="left"><bold>91.28</bold></td><td align="left"><bold>94.50</bold></td></tr><tr><td/><td align="left">32</td><td align="left">Label Smoothing</td><td align="left">94.62</td><td align="left">89.96</td><td align="left">92.50</td></tr><tr><td align="left" rowspan="7"><bold>ResNet-50</bold></td><td align="left">10</td><td align="left">32</td><td align="left"><bold>Cross Entropy</bold></td><td align="left">94.93</td><td align="left">90.55</td><td align="left">92.62</td></tr><tr><td align="left">20</td><td/><td/><td align="left">94.81</td><td align="left">90.34</td><td align="left">93.37</td></tr><tr><td align="left">30</td><td/><td/><td align="left">95.12</td><td align="left">90.92</td><td align="left">93.72</td></tr><tr><td align="left">40</td><td/><td/><td align="left">94.81</td><td align="left">90.35</td><td align="left">93.14</td></tr><tr><td align="left"><bold>30</bold></td><td align="left">8</td><td/><td align="left">93.03</td><td align="left">87.01</td><td align="left">91.99</td></tr><tr><td/><td align="left"><bold>16</bold></td><td/><td align="left"><bold>95.57</bold></td><td align="left"><bold>91.76</bold></td><td align="left"><bold>95.35</bold></td></tr><tr><td/><td align="left">32</td><td align="left">Label Smoothing</td><td align="left">95.12</td><td align="left">90.91</td><td align="left">93.36</td></tr><tr><td align="left" rowspan="7"><bold>EfficientNet-B0</bold></td><td align="left">10</td><td align="left">32</td><td align="left">Cross Entropy</td><td align="left">95.69</td><td align="left">91.99</td><td align="left">94.52</td></tr><tr><td align="left">20</td><td/><td/><td align="left">95.19</td><td align="left">91.02</td><td align="left">93.38</td></tr><tr><td align="left">30</td><td/><td/><td align="left">95.69</td><td align="left">92.01</td><td align="left">95.48</td></tr><tr><td align="left">40</td><td/><td/><td align="left">95.00</td><td align="left">90.72</td><td align="left">95.00</td></tr><tr><td align="left"><bold>30</bold></td><td align="left">8</td><td/><td align="left">94.68</td><td align="left">90.16</td><td align="left">93.25</td></tr><tr><td/><td align="left">16</td><td/><td align="left">95.38</td><td align="left">91.40</td><td align="left">94.88</td></tr><tr><td/><td align="left"><bold>32</bold></td><td align="left"><bold>Label Smoothing</bold></td><td align="left"><bold>95.82</bold></td><td align="left"><bold>92.24</bold></td><td align="left"><bold>96.16</bold></td></tr></tbody></table></table-wrap></p>
        <p id="p0185">We fine-tuned CovidXrayNet using the one-cycle policy [<xref rid="bib45" ref-type="bibr">45</xref>] and discriminative learning rates [<xref rid="bib46" ref-type="bibr">46</xref>]. Equation <xref rid="fd3" ref-type="disp-formula">(3)</xref> defines this discriminative fine-tuning technique, where CovidXrayNet's parameters <inline-formula><mml:math id="M33" altimg="si1.svg"><mml:mrow><mml:mi mathvariant="normal">θ</mml:mi></mml:mrow></mml:math></inline-formula> are split into <inline-formula><mml:math id="M34" altimg="si17.svg"><mml:mrow><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mi>L</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the learning rates <inline-formula><mml:math id="M35" altimg="si1.svg"><mml:mrow><mml:mi mathvariant="normal">η</mml:mi></mml:mrow></mml:math></inline-formula> are split into <inline-formula><mml:math id="M36" altimg="si18.svg"><mml:mrow><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:msup><mml:mi>η</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>η</mml:mi><mml:mi>L</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at time step “<inline-formula><mml:math id="M37" altimg="si134.svg"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>” for the number of layers “<inline-formula><mml:math id="M38" altimg="si133.svg"><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula>”. Using this function, we start with a learning rate of <inline-formula><mml:math id="M39" altimg="si16.svg"><mml:mrow><mml:mn>2</mml:mn><mml:mi>e</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>, then automatically adjust this value for both COVIDx and COVIDcxr datasets, where the gradient of the CovidXrayNet's objective function is <inline-formula><mml:math id="M40" altimg="si17.svg"><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mi>l</mml:mi></mml:msup></mml:msub><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula>.<disp-formula id="fd3"><label>(3)</label><mml:math id="M41" altimg="si18.svg" alttext="Equation 3."><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>t</mml:mi><mml:mi>l</mml:mi></mml:msubsup><mml:mo linebreak="badbreak">=</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:mo linebreak="goodbreak">−</mml:mo><mml:msup><mml:mi>η</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:mo linebreak="goodbreak">×</mml:mo><mml:msub><mml:mo>∇</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mi>l</mml:mi></mml:msup></mml:msub><mml:mi>j</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
</p>
      </sec>
    </sec>
    <sec id="sec3.3">
      <label>3.3</label>
      <title>CovidXrayNet evaluation</title>
      <p id="p0190">We computed the accuracy, macro average precision, macro average recall, and macro F1 score of the CovidXrayNet in distinguishing between the three classes (“COVID-19″, “pneumonia”, “normal”). Equations <xref rid="fd4" ref-type="disp-formula">(4)</xref>, <xref rid="fd5" ref-type="disp-formula">(5)</xref>, <xref rid="fd6" ref-type="disp-formula">(6)</xref>, <xref rid="fd7" ref-type="disp-formula">(7)</xref> explain these metrics for a generic class <inline-formula><mml:math id="M42" altimg="si1.svg"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M43" altimg="si22.svg"><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula> refers to True Positives classifications, <inline-formula><mml:math id="M44" altimg="si23.svg"><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> denotes False Negatives classifications, <inline-formula><mml:math id="M45" altimg="si24.svg"><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> presents True Negatives classification, and <inline-formula><mml:math id="M46" altimg="si25.svg"><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula> means False Positives classifications. In the Macro approach, all classes are considered as basic elements of the calculation [<xref rid="bib47" ref-type="bibr">47</xref>] (i.e., each class has the same weight in the average regardless of its size).<disp-formula id="fd4"><label>(4)</label><mml:math id="M47" altimg="si26.svg" alttext="Equation 4."><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mi>K</mml:mi></mml:mfrac></mml:mrow></mml:math></disp-formula>
<disp-formula id="fd5"><label>(5)</label><mml:math id="M48" altimg="si27.svg" alttext="Equation 5."><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mi>K</mml:mi></mml:mfrac></mml:mrow></mml:math></disp-formula>
<disp-formula id="fd6"><label>(6)</label><mml:math id="M49" altimg="si28.svg" alttext="Equation 6."><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mi>K</mml:mi></mml:mfrac></mml:mrow></mml:math></disp-formula>
<disp-formula id="fd7"><label>(7)</label><mml:math id="M50" altimg="si29.svg" alttext="Equation 7."><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mn>1</mml:mn><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>2</mml:mn><mml:mo linebreak="goodbreak">×</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">×</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msubsup><mml:mi>n</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:msubsup><mml:mi>l</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
</p>
      <p id="p0195">Moreover, we used the Area Under the Receiver Operating Characteristic Curve (AUC) [<xref rid="bib48" ref-type="bibr">48</xref>], and Matthews correlation coefficient (MCC) [<xref rid="bib49" ref-type="bibr">49</xref>]. AUC for multi-class is defined in Eq. <xref rid="fd8" ref-type="disp-formula">(8)</xref>, where <inline-formula><mml:math id="M51" altimg="si30.svg"><mml:mrow><mml:mi>A</mml:mi><mml:mi>U</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the area under the class reference <inline-formula><mml:math id="M52" altimg="si31.svg"><mml:mrow><mml:mi>R</mml:mi><mml:mi>O</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula> curve for the positive class <inline-formula><mml:math id="M53" altimg="si32.svg"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. This implementation of AUC score is simple and fast but it is sensitive to class distributions and error costs. MCC, on the other hand, is a good indicator of total unbalanced prediction models as defined in Eq. <xref rid="fd9" ref-type="disp-formula">(9)</xref>, where “<inline-formula><mml:math id="M54" altimg="si143.svg"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:math></inline-formula>” represents all correctly predicted cases, “<inline-formula><mml:math id="M55" altimg="si1.svg"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula>” represents all cases, “<inline-formula><mml:math id="M56" altimg="si33.svg"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>” is the number of instances that class “<inline-formula><mml:math id="M57" altimg="si1.svg"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula>” was predicted to be, and “<inline-formula><mml:math id="M58" altimg="si34.svg"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>” is the number of instances that class “<inline-formula><mml:math id="M59" altimg="si154.svg"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula>” truly occurred. Since accuracy depends mostly on the number of samples in each class, CNN-based models perform seemingly well in the imbalanced datasets, such as COVIDx. This may result in an inaccurate conclusion. Therefore, the combination of multiple evaluation matrices should be the criterion for selecting the best model.<disp-formula id="fd8"><label>(8)</label><mml:math id="M60" altimg="si35.svg" alttext="Equation 8."><mml:mrow><mml:mi>A</mml:mi><mml:mi>U</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>ε</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:munder><mml:mi>A</mml:mi><mml:mi>U</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">×</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="fd9"><label>(9)</label><mml:math id="M61" altimg="si36.svg" alttext="Equation 9."><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi>c</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mi>s</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mi>k</mml:mi><mml:mi>K</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo linebreak="badbreak">×</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:msqrt><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo linebreak="badbreak">−</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mi>k</mml:mi><mml:mi>K</mml:mi></mml:msubsup><mml:msubsup><mml:mi>p</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo linebreak="badbreak">−</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mi>k</mml:mi><mml:mi>K</mml:mi></mml:msubsup><mml:msubsup><mml:mi>t</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:math></disp-formula>
</p>
    </sec>
  </sec>
  <sec id="sec4">
    <label>4</label>
    <title>Experiment</title>
    <sec id="sec4.1">
      <label>4.1</label>
      <title>Implementation</title>
      <p id="p0200">We used PyTorch software [<xref rid="bib50" ref-type="bibr">50</xref>], fastai library [<xref rid="bib51" ref-type="bibr">51</xref>], an n1-highmem-8 (8 vCPUs, 52 GB memory) machine, and one NVIDIA Tesla V100 GPU. Fastai is a deep learning library that enables the implementation of CovidXrayNet with its unique ability to join several transformers inside a pipeline that manages the minimum number of computations and lossy operations.</p>
    </sec>
    <sec id="sec4.2">
      <label>4.2</label>
      <title>Result</title>
      <sec id="sec4.2.1">
        <label>4.2.1</label>
        <title>Quantitative evaluation</title>
        <p id="p0205">In order to evaluate our proposed data augmentation pipeline, we compared the reported results of VGG-19 and ResNet-50 in the COVID-Net paper [<xref rid="bib20" ref-type="bibr">20</xref>] with our results on the COVIDx dataset as recorded in <xref rid="tbl9" ref-type="table">Table 9</xref>
. With only 30 epochs of learning cycles, the accuracy of VGG-19 increased by 11.93%, while the accuracy of ResNet-50 improved by 4.97%. This clearly indicates the effect of our proposed method on enhancing the accuracy of COVID-19 classification from CXRs.<table-wrap position="float" id="tbl9"><label>Table 9</label><caption><p>Comparing our Optimised Data Augmentation Pipeline and CNN Hyperparameters with Benchmark. Both papers used VGG19 and ResNet50 on the COVIDx dataset but with different transformers and hyperparameters.</p></caption><alt-text id="alttext0090">Table 9</alt-text><table frame="hsides" rules="groups"><thead><tr><th>CNN</th><th>Paper</th><th>Parameters (M)</th><th>Acc (%)</th><th>AUC (%)</th><th>MCC (%)</th><th>F1 (%)</th></tr></thead><tbody><tr><td align="left" rowspan="2">VGG-19</td><td align="left">COVID-Net [<xref rid="bib20" ref-type="bibr">20</xref>]</td><td align="left">20</td><td align="left">83.00</td><td align="left">_</td><td align="left">_</td><td align="left">_</td></tr><tr><td align="left"><bold>CovidXrayNet</bold></td><td/><td align="left"><bold>94.93</bold></td><td align="left"><bold>98.69</bold></td><td align="left"><bold>90.56</bold></td><td align="left"><bold>92.79</bold></td></tr><tr><td align="left" rowspan="2">ResNet-50</td><td align="left">COVID-Net [<xref rid="bib20" ref-type="bibr">20</xref>]</td><td align="left">25</td><td align="left">90.60</td><td align="left">_</td><td align="left">_</td><td align="left">_</td></tr><tr><td align="left"><bold>CovidXrayNet</bold></td><td/><td align="left"><bold>95.57</bold></td><td align="left"><bold>99.29</bold></td><td align="left"><bold>91.76</bold></td><td align="left"><bold>95.35</bold></td></tr></tbody></table></table-wrap></p>
        <p id="p0210"><xref rid="tbl10" ref-type="table">Table 10</xref> compares CovidXrayNet to other studies in the literature that are based on three-class classification. We achieved better accuracy (95.82%) over the remainder of the models, including DarkCovidNet (87.02%), COVID-Net (93.30%), and MobileNet v2 (93.48%). Further, the F1 score for CovidXrayNet (96.16%) is higher than DarkCovidNet (87.37%) and the precision score of CovidXrayNet (96.93%) is better than DarkCovidNet (89.96%). Significantly, the overall sensitivity of CovidXrayNet is 95.43%. Our reported results are reproducible. We have used the same test dataset as COVID-Net.<table-wrap position="float" id="tbl10"><label>Table 10</label><caption><p>Comparing CovidXrayNet with Benchmark. All models are based on three-class COVID-19 classification. COVID-Net and CovidXrayNet employed the COVIDx dataset.</p></caption><alt-text id="alttext0095">Table 10</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Model</th><th>Acc (%)</th><th>MCC (%)</th><th>Precision (%)</th><th>Recall (%)</th><th>F1 (%)</th></tr></thead><tbody><tr><td align="left">DarkCovidNet [<xref rid="bib24" ref-type="bibr">24</xref>]</td><td align="left">87.02</td><td align="left">_</td><td align="left">89.96</td><td align="left">_</td><td align="left">87.37</td></tr><tr><td align="left">COVID-Net [<xref rid="bib20" ref-type="bibr">20</xref>]</td><td align="left">93.30</td><td align="left">_</td><td align="left">_</td><td align="left">_</td><td align="left">_</td></tr><tr><td align="left">MobileNet v2 [<xref rid="bib26" ref-type="bibr">26</xref>]</td><td align="left">93.48</td><td align="left">_</td><td align="left">_</td><td align="left">_</td><td align="left">_</td></tr><tr><td align="left"><bold>CovidXrayNet</bold></td><td align="left"><bold>95.82</bold></td><td align="left"><bold>92.24</bold></td><td align="left"><bold>96.93</bold></td><td align="left"><bold>95.43</bold></td><td align="left"><bold>96.16</bold></td></tr></tbody></table></table-wrap></p>
      </sec>
      <sec id="sec4.2.2">
        <label>4.2.2</label>
        <title>Qualitative evaluation</title>
        <p id="p0215">We have ensured the robustness of CovidXrayNet by sharing its top prediction errors and actual labels with expert radiologists (refer to <xref rid="fig5" ref-type="fig">Fig. 5</xref>
). CovidXrayNet classified four COVID-19 patients as pneumonia. Since COVID-19 is a subset of pneumonia diseases, the diagnosis is correct but the interpretation is not. For this reason, CovidXrayNet can only offer a second opinion to the radiologist in the clinical setting.<fig id="fig5"><label>Fig. 5</label><caption><p>Top prediction errors generated by CovidXrayNet on COVIDx test dataset.</p></caption><alt-text id="alttext0030">Fig. 5</alt-text><graphic xlink:href="gr5_lrg"/></fig></p>
      </sec>
    </sec>
  </sec>
  <sec id="sec5">
    <label>5</label>
    <title>Discussion</title>
    <p id="p0220">The rapid spread of the COVID-19 pandemic along with the limited number of RT-PCR test kits and qualified radiologists, has necessitated the need for accurate automated detection systems. CXR is one of the main imaging methods that are fast, non-invasive, affordable, and possibly able to be completed at bedside to monitor the progression of COVID-19 infection. However, radiologists with expertise in CXR interpretation may not be available at every institution.</p>
    <sec id="sec5.1">
      <label>5.1</label>
      <title>Optimization in deep learning</title>
      <p id="p0225">We aim to implement an AI model, CovidXrayNet, that can identify COVID-19 infection based on CXRs. CovidXrayNet optimizes data augmentation to enable CNN models to observe visual features that are not noticeable to the radiologist's eye. With data augmentation, CNN models will generalize better results. However, the implications of choosing efficient and effective augmentation techniques depend on the dataset at hand. Using CXR with COVID-19 datasets, we performed a separate search phase that was computationally expensive. Recent work, such as RandAugment [<xref rid="bib52" ref-type="bibr">52</xref>] and AutoAugment [<xref rid="bib53" ref-type="bibr">53</xref>], suggests removing the need for a search phase to reduce the parameter space for data augmentation. However, incorrect choices in the COVID-19 classification task may lead to erasing or diluting vital features.</p>
      <p id="p0230">Notably, individual data augmentation methods yielded a minor increased task performance as seen in <xref rid="tbl5" ref-type="table">Table 5</xref>. For example, the optimal warping value improves the classification task accuracy by only %1.56. However, a combination of these optimized methods (i.e., our proposed data augmentation pipeline and CNN hayperparameters) has increased the performance significantly, as can be seen in <xref rid="tbl9" ref-type="table">Table 9</xref>. It increases the accuracy of the popular CNN architectures such as VGG-19 and ResNet-50, by 11.93% and 4.97%, respectively.</p>
      <p id="p0235">We find that EfficientNet-B0 performs well for COVID-19 CXR classification with the following data augmentation pipeline: squishing the CXR to 480<italic>x</italic>480 pixels, rotating by 20°, zooming by 1.2 scale, warping by 0.2 magnitude, lighting by 0.3 scale, and normalizing. Also, the label smoothing cross-entropy loss function, at the batch size of 32 with 30 epochs, increases the accuracy of CovidXrayNet on the COVIDx dataset. EfficientNet is rapidly becoming the deep learning practitioners' choice over ResNet for many classification tasks. It allows practitioners to use the minimum FLOPS while achieving the best possible accuracy by compound scaling the network's depth, width, and input resolution.</p>
    </sec>
    <sec id="sec5.2">
      <label>5.2</label>
      <title>Limitation and future direction</title>
      <p id="p0240">While CovidXrayNet performs well as a whole (see <xref rid="fig6" ref-type="fig">Fig. 6</xref>
), it misidentified four patients with COVID-19 as having pneumonia, and one patient with COVID-19 as being normal (refer to the confusion matrix in <xref rid="fig7" ref-type="fig">Fig. 7</xref>
). However, it is important to limit the number of missed COVID-19 patients to be isolated as well as the number of false-positive COVID-19 patients to avoid unnecessary burden for the clinical sites. Therefore, CovidXrayNet is still at a research stage and is not suitable for direct clinical diagnosis. It can be built upon and optimized with additional data augmentation and better CNN hyperparameters.<fig id="fig6"><label>Fig. 6</label><caption><p>Randomly generated results for CovidXrayNet on COVIDx test dataset.</p></caption><alt-text id="alttext0035">Fig. 6</alt-text><graphic xlink:href="gr6_lrg"/></fig><fig id="fig7"><label>Fig. 7</label><caption><p>Confusion matrix for CovidXrayNet on COVIDx test dataset.</p></caption><alt-text id="alttext0040">Fig. 7</alt-text><graphic xlink:href="gr7_lrg"/></fig></p>
      <p id="p0245">Without conducting a proper clinical study, the achieved accuracy of CovidXrayNet (95.82%) on the COVIDx dataset does not indicate that CovidXrayNet is sufficient for detecting COVID-19 from CXR. Our aim is to empower this research wave through our optimized data augmentation pipeline and CNN hyperparameters. Therefore, we are releasing the source code of CovidXrayNet to enable researchers to reproduce the results and experiment on different datasets.</p>
      <p id="p0250">As there is an endless array of transformation, our work evaluates common augmentation techniques in the CXR classification literature (i.e., resize value, resize method, rotate, zoom, warp, light, flip and normalize), recent proposed methods (i.e., mixup and random erasing), and combinations of these methods. Future research can enhance our model with de-noising or segmentation steps. In addition, the proposed data augmentation pipeline was tested only on the three-class classification task (“COVID-19″, “normal” or “pneumonia”). Researchers may investigate the effects of the proposed technique on detecting other common CXR observations including atelectasis, cardiomegaly, consolidation, edema, enlarged cardiomediastinum, fracture, lung lesion, lung opacity, pleural effusion, pleural other, pneumonia and pneumothorax.</p>
      <p id="p0255">Designing a fair testing protocol could be highly challenging. Different datasets were merged with large differences among them in order to respond to the global challenge of quickly identifying COVID-19 [<xref rid="bib19" ref-type="bibr">19</xref>]. COVIDx and COVIDcxr datsets were collected from public sources. They were also indirectly collected from hospitals and physicians. For the COVIDx, we tested our model in the official split recommended by the COVIDx paper to allow for future comparison. For the COVIDcxr dataset, we will release the dataset generation scripts. Future research should assess the validity of the available testing protocol by validating the COVID-19 CXRs with clinical experts and determining the ground truth.</p>
      <p id="p0260">COVIDcxr is suitable for building a single neural network based on both images (CXR) and tabular data (sex, age, and view), as can be seen in <xref rid="fig8" ref-type="fig">Fig. 8</xref>
. However, we did not observe better performance for such a model than a linear model with embedding. Even though a multi-modal network, with multiple input modalities, receives more information, it is often prone to over-fitting [<xref rid="bib54" ref-type="bibr">54</xref>]. Future research may explore training multi-modal classification networks based on the COVIDcxr dataset using various CNN architectures and hyperparameters.<fig id="fig8"><label>Fig. 8</label><caption><p>Data Loader from COVIDcxr that Combines both Tabular Data and CXR.</p></caption><alt-text id="alttext0045">Fig. 8</alt-text><graphic xlink:href="gr8_lrg"/></fig></p>
    </sec>
  </sec>
  <sec id="sec6">
    <label>6</label>
    <title>Conclusion</title>
    <p id="p0265">We have demonstrated that optimizing data augmentation and CNN hyperparameters result in outstanding effects on the automatic extraction of features from CXR related to the diagnosis of COVID-19. CovidXrayNet only requires 30 learning cycles to process a CXR yet achieves 95.82% accuracy on the COVIDx dataset.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Declaration of competing interest</title>
    <p id="p0270">The authors declare no competing interests.</p>
  </sec>
</body>
<back>
  <ref-list id="cebib0010">
    <title>References</title>
    <ref id="bib1">
      <label>1</label>
      <element-citation publication-type="other" id="sref1">
        <article-title>WHO coronavirus (COVID-19) dashboard — WHO coronavirus disease (COVID-19) dashboard</article-title>
        <comment>[Online]. Available:</comment>
        <ext-link ext-link-type="uri" xlink:href="https://covid19.who.int/" id="intref0015">https://covid19.who.int/</ext-link>
      </element-citation>
    </ref>
    <ref id="bib2">
      <label>2</label>
      <element-citation publication-type="journal" id="sref2">
        <person-group person-group-type="author">
          <name>
            <surname>Singhal</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>A review of coronavirus disease-2019 (COVID-19)</article-title>
        <source>Indian J. Pediatr.</source>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>6</lpage>
      </element-citation>
    </ref>
    <ref id="bib3">
      <label>3</label>
      <element-citation publication-type="journal" id="sref3">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>W.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Detection of SARS-CoV-2 in different types of clinical specimens</article-title>
        <source>Jama</source>
        <volume>323</volume>
        <issue>18</issue>
        <year>2020</year>
        <fpage>1843</fpage>
        <lpage>1844</lpage>
        <pub-id pub-id-type="pmid">32159775</pub-id>
      </element-citation>
    </ref>
    <ref id="bib4">
      <label>4</label>
      <element-citation publication-type="book" id="sref4">
        <person-group person-group-type="author">
          <name>
            <surname>West</surname>
            <given-names>C.P.</given-names>
          </name>
          <name>
            <surname>Montori</surname>
            <given-names>V.M.</given-names>
          </name>
          <name>
            <surname>Sampathkumar</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <part-title>COVID-19 testing: the threat of false-negative results</part-title>
        <series>In Mayo Clinic Proceedings</series>
        <volume>vol. 95</volume>
        <year>2020</year>
        <publisher-name>Elsevier</publisher-name>
        <fpage>1127</fpage>
        <lpage>1129</lpage>
        <comment>6</comment>
      </element-citation>
    </ref>
    <ref id="bib5">
      <label>5</label>
      <element-citation publication-type="journal" id="sref5">
        <person-group person-group-type="author">
          <name>
            <surname>Ng</surname>
            <given-names>M.-Y.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Imaging profile of the COVID-19 infection: radiologic findings and literature review</article-title>
        <source>Radiology: Cardiothoracic Imag.</source>
        <volume>2</volume>
        <issue>1</issue>
        <year>2020</year>
        <object-id pub-id-type="publisher-id">e200034</object-id>
      </element-citation>
    </ref>
    <ref id="bib6">
      <label>6</label>
      <element-citation publication-type="book" id="sref6">
        <person-group person-group-type="author">
          <name>
            <surname>Rubin</surname>
            <given-names>G.D.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>The Role of Chest Imaging in Patient Management during the COVID-19 Pandemic: a Multinational Consensus Statement from the Fleischner Society</part-title>
        <year>2020</year>
        <publisher-name>
          <italic>Chest</italic>
        </publisher-name>
      </element-citation>
    </ref>
    <ref id="bib7">
      <label>7</label>
      <element-citation publication-type="journal" id="sref7">
        <person-group person-group-type="author">
          <name>
            <surname>LeCun</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <source>“Deep learning,” <italic>nature</italic></source>
        <volume>521</volume>
        <issue>7553</issue>
        <year>2015</year>
        <fpage>436</fpage>
        <lpage>444</lpage>
      </element-citation>
    </ref>
    <ref id="bib8">
      <label>8</label>
      <element-citation publication-type="journal" id="sref8">
        <person-group person-group-type="author">
          <name>
            <surname>Ahsan</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Based</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Haider</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Kowalski</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>COVID-19 detection from chest X-ray images using feature fusion and deep learning</article-title>
        <source>Sensors</source>
        <volume>21</volume>
        <issue>4</issue>
        <year>2021</year>
        <fpage>1480</fpage>
        <pub-id pub-id-type="pmid">33672585</pub-id>
      </element-citation>
    </ref>
    <ref id="bib9">
      <label>9</label>
      <element-citation publication-type="book" id="sref9">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Summers</surname>
            <given-names>R.M.</given-names>
          </name>
        </person-group>
        <part-title>Tienet: text-image embedding network for common thorax disease classification and reporting in chest x-rays</part-title>
        <source>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>
        <year>2018</year>
        <fpage>9049</fpage>
        <lpage>9058</lpage>
      </element-citation>
    </ref>
    <ref id="bib10">
      <label>10</label>
      <element-citation publication-type="book" id="sref10">
        <person-group person-group-type="author">
          <name>
            <surname>Monshi</surname>
            <given-names>M.M.A.</given-names>
          </name>
          <name>
            <surname>Poon</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Chung</surname>
            <given-names>V.</given-names>
          </name>
        </person-group>
        <part-title>Convolutional neural network to detect thorax diseases from multi-view chest X-rays</part-title>
        <source>International Conference on Neural Information Processing</source>
        <year>2019</year>
        <publisher-name>Springer</publisher-name>
        <fpage>148</fpage>
        <lpage>158</lpage>
      </element-citation>
    </ref>
    <ref id="bib11">
      <label>11</label>
      <element-citation publication-type="journal" id="sref11">
        <person-group person-group-type="author">
          <name>
            <surname>Shorten</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Khoshgoftaar</surname>
            <given-names>T.M.</given-names>
          </name>
        </person-group>
        <article-title>A survey on image data augmentation for deep learning</article-title>
        <source>J. Big Data</source>
        <volume>6</volume>
        <issue>1</issue>
        <year>2019</year>
        <fpage>1</fpage>
        <lpage>48</lpage>
      </element-citation>
    </ref>
    <ref id="bib12">
      <label>12</label>
      <element-citation publication-type="book" id="sref12">
        <person-group person-group-type="author">
          <name>
            <surname>Calderon-Ramirez</surname>
            <given-names>S.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>Correcting Data Imbalance for Semi-supervised Covid-19 Detection Using X-Ray Chest Images</part-title>
        <year>2020</year>
        <comment>
          <italic>arXiv preprint arXiv:2008.08496</italic>
        </comment>
      </element-citation>
    </ref>
    <ref id="bib13">
      <label>13</label>
      <element-citation publication-type="book" id="sref13">
        <person-group person-group-type="author">
          <name>
            <surname>Taylor</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Nitschke</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <part-title>Improving Deep Learning Using Generic Data Augmentation</part-title>
        <year>2017</year>
        <comment>
          <italic>arXiv preprint arXiv:1708.06020</italic>
        </comment>
      </element-citation>
    </ref>
    <ref id="bib14">
      <label>14</label>
      <element-citation publication-type="journal" id="sref14">
        <person-group person-group-type="author">
          <name>
            <surname>Rodrigues</surname>
            <given-names>L.F.</given-names>
          </name>
          <name>
            <surname>Naldi</surname>
            <given-names>M.C.</given-names>
          </name>
          <name>
            <surname>Mari</surname>
            <given-names>J.F.</given-names>
          </name>
        </person-group>
        <article-title>Comparing convolutional neural networks and preprocessing techniques for HEp-2 cell classification in immunofluorescence images</article-title>
        <source>Comput. Biol. Med.</source>
        <volume>116</volume>
        <year>2020</year>
        <fpage>103542</fpage>
        <pub-id pub-id-type="pmid">31790962</pub-id>
      </element-citation>
    </ref>
    <ref id="bib15">
      <label>15</label>
      <element-citation publication-type="book" id="sref15">
        <person-group person-group-type="author">
          <name>
            <surname>Akiba</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Sano</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Yanase</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Ohta</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Koyama</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <part-title>Optuna: a next-generation hyperparameter optimization framework</part-title>
        <source>In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</source>
        <year>2019</year>
        <fpage>2623</fpage>
        <lpage>2631</lpage>
      </element-citation>
    </ref>
    <ref id="bib16">
      <label>16</label>
      <element-citation publication-type="journal" id="sref16">
        <person-group person-group-type="author">
          <name>
            <surname>Nishio</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Fujimoto</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Togashi</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Lung segmentation on chest X-ray images in patients with severe abnormal findings using deep learning</article-title>
        <source>Int. J. Imag. Syst. Technol.</source>
        <year>2020</year>
        <comment>ISSN: 0899-9457</comment>
      </element-citation>
    </ref>
    <ref id="bib17">
      <label>17</label>
      <element-citation publication-type="journal" id="sref17">
        <person-group person-group-type="author">
          <name>
            <surname>Bridge</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Meng</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Du</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Introducing the GEV activation function for highly unbalanced data to develop COVID-19 diagnostic models</article-title>
        <source>IEEE J. Biomed. Health Inform.</source>
        <volume>24</volume>
        <issue>10</issue>
        <year>2020</year>
        <fpage>2776</fpage>
        <lpage>2786</lpage>
        <pub-id pub-id-type="pmid">32750973</pub-id>
      </element-citation>
    </ref>
    <ref id="bib18">
      <label>18</label>
      <element-citation publication-type="book" id="sref18">
        <person-group person-group-type="author">
          <name>
            <surname>Peng</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>Y.-X.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Summers</surname>
            <given-names>R.M.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <part-title>COVID-19-CT-CXR: a Freely Accessible and Weakly Labeled Chest X-Ray and CT Image Collection on COVID-19 from Biomedical Literature</part-title>
        <year>2020</year>
        <comment>
          <italic>arXiv preprint arXiv:2006.06177</italic>
        </comment>
      </element-citation>
    </ref>
    <ref id="bib19">
      <label>19</label>
      <element-citation publication-type="book" id="sref19">
        <person-group person-group-type="author">
          <name>
            <surname>Cohen</surname>
            <given-names>J.P.</given-names>
          </name>
          <name>
            <surname>Morrison</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Dao</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Roth</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Duong</surname>
            <given-names>T.Q.</given-names>
          </name>
          <name>
            <surname>Ghassemi</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <part-title>COVID-19 Image Data Collection: Prospective Predictions Are the Future</part-title>
        <year>2020</year>
        <comment>
          <italic>arXiv preprint arXiv:2006.11988</italic>
        </comment>
      </element-citation>
    </ref>
    <ref id="bib20">
      <label>20</label>
      <element-citation publication-type="journal" id="sref20">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Z.Q.</given-names>
          </name>
          <name>
            <surname>Wong</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Covid-net: a tailored deep convolutional neural network design for detection of covid-19 cases from chest x-ray images</article-title>
        <source>Sci. Rep.</source>
        <volume>10</volume>
        <issue>1</issue>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="pmid">31913322</pub-id>
      </element-citation>
    </ref>
    <ref id="bib21">
      <label>21</label>
      <element-citation publication-type="book" id="sref21">
        <person-group person-group-type="author">
          <name>
            <surname>Hemdan</surname>
            <given-names>E.E.-D.</given-names>
          </name>
          <name>
            <surname>Shouman</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Karar</surname>
            <given-names>M.E.</given-names>
          </name>
        </person-group>
        <part-title>Covidx-net: A Framework of Deep Learning Classifiers to Diagnose Covid-19 in X-Ray Images</part-title>
        <year>2020</year>
        <comment>
          <italic>arXiv preprint arXiv:2003.11055</italic>
        </comment>
      </element-citation>
    </ref>
    <ref id="bib22">
      <label>22</label>
      <element-citation publication-type="journal" id="sref22">
        <person-group person-group-type="author">
          <name>
            <surname>Mahmud</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Rahman</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Fattah</surname>
            <given-names>S.A.</given-names>
          </name>
        </person-group>
        <article-title>CovXNet: a multi-dilation convolutional neural network for automatic COVID-19 and other pneumonia detection from chest X-ray images with transferable multi-receptive feature optimization</article-title>
        <source>Comput. Biol. Med.</source>
        <volume>122</volume>
        <year>2020</year>
        <fpage>103869</fpage>
        <pub-id pub-id-type="pmid">32658740</pub-id>
      </element-citation>
    </ref>
    <ref id="bib23">
      <label>23</label>
      <element-citation publication-type="book" id="sref23">
        <person-group person-group-type="author">
          <name>
            <surname>Narin</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Kaya</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Pamuk</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <part-title>Automatic Detection of Coronavirus Disease (Covid-19) Using X-Ray Images and Deep Convolutional Neural Networks</part-title>
        <year>2020</year>
        <comment>
          <italic>arXiv preprint arXiv:2003.10849</italic>
        </comment>
      </element-citation>
    </ref>
    <ref id="bib24">
      <label>24</label>
      <element-citation publication-type="journal" id="sref24">
        <person-group person-group-type="author">
          <name>
            <surname>Ozturk</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Talo</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Yildirim</surname>
            <given-names>E.A.</given-names>
          </name>
          <name>
            <surname>Baloglu</surname>
            <given-names>U.B.</given-names>
          </name>
          <name>
            <surname>Yildirim</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Acharya</surname>
            <given-names>U.R.</given-names>
          </name>
        </person-group>
        <article-title>Automated detection of COVID-19 cases using deep neural networks with X-ray images</article-title>
        <source>Comput. Biol. Med.</source>
        <year>2020</year>
        <fpage>103792</fpage>
        <pub-id pub-id-type="pmid">32568675</pub-id>
      </element-citation>
    </ref>
    <ref id="bib25">
      <label>25</label>
      <element-citation publication-type="journal" id="sref25">
        <person-group person-group-type="author">
          <name>
            <surname>Nishio</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Noguchi</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Matsuo</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Murakami</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Automatic classification between COVID-19 pneumonia, non-COVID-19 pneumonia, and the healthy on chest X-ray image: combination of data augmentation methods</article-title>
        <source>Sci. Rep.</source>
        <volume>10</volume>
        <issue>1</issue>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>6</lpage>
        <pub-id pub-id-type="pmid">31913322</pub-id>
      </element-citation>
    </ref>
    <ref id="bib26">
      <label>26</label>
      <element-citation publication-type="journal" id="sref26">
        <person-group person-group-type="author">
          <name>
            <surname>Apostolopoulos</surname>
            <given-names>I.D.</given-names>
          </name>
          <name>
            <surname>Mpesiana</surname>
            <given-names>T.A.</given-names>
          </name>
        </person-group>
        <article-title>Covid-19: automatic detection from x-ray images utilizing transfer learning with convolutional neural networks</article-title>
        <source>Phys. Eng. Sci. Med.</source>
        <volume>1</volume>
        <year>2020</year>
      </element-citation>
    </ref>
    <ref id="bib27">
      <label>27</label>
      <element-citation publication-type="journal" id="sref27">
        <person-group person-group-type="author">
          <name>
            <surname>Sethy</surname>
            <given-names>P.K.</given-names>
          </name>
          <name>
            <surname>Behera</surname>
            <given-names>S.K.</given-names>
          </name>
        </person-group>
        <article-title>Detection of coronavirus disease (covid-19) based on deep features</article-title>
        <source>Preprints</source>
        <volume>2020030300</volume>
        <year>2020</year>
        <fpage>2020</fpage>
      </element-citation>
    </ref>
    <ref id="bib28">
      <label>28</label>
      <element-citation publication-type="book" id="sref28">
        <person-group person-group-type="author">
          <name>
            <surname>Mooney</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <part-title>Chest X-Ray Images (Pneumonia)</part-title>
        <year>2018</year>
        <publisher-name>tanggal akses</publisher-name>
        <comment>
          <italic>Online]</italic>
        </comment>
        <ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com" id="intref0020">https://www.kaggle.com</ext-link>
        <comment>/paultimothymooney/chest-xray-pneumonia</comment>
      </element-citation>
    </ref>
    <ref id="bib29">
      <label>29</label>
      <element-citation publication-type="journal" id="sref29">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Hou</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>Automatically discriminating and localizing COVID-19 from community-acquired pneumonia on chest X-rays</article-title>
        <source>Pattern Recogn.</source>
        <year>2020</year>
        <fpage>107613</fpage>
      </element-citation>
    </ref>
    <ref id="bib30">
      <label>30</label>
      <element-citation publication-type="book" id="sref30">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Qian</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <part-title>Residual attention network for image classification</part-title>
        <source>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>
        <year>2017</year>
        <fpage>3156</fpage>
        <lpage>3164</lpage>
      </element-citation>
    </ref>
    <ref id="bib31">
      <label>31</label>
      <element-citation publication-type="book" id="sref31">
        <person-group person-group-type="author">
          <name>
            <surname>Redmon</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Farhadi</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <part-title>YOLO9000: better, faster, stronger,</part-title>
        <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>
        <year>2017</year>
        <fpage>7263</fpage>
        <lpage>7271</lpage>
      </element-citation>
    </ref>
    <ref id="bib32">
      <label>32</label>
      <element-citation publication-type="book" id="sref32">
        <person-group person-group-type="author">
          <name>
            <surname>Wong</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Shafiee</surname>
            <given-names>M.J.</given-names>
          </name>
          <name>
            <surname>Chwyl</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <part-title>Ferminets: Learning Generative Machines to Generate Efficient Neural Networks via Generative Synthesis</part-title>
        <year>2018</year>
        <comment>
          <italic>arXiv preprint arXiv:1809.05989</italic>
        </comment>
      </element-citation>
    </ref>
    <ref id="bib33">
      <label>33</label>
      <element-citation publication-type="journal" id="sref33">
        <person-group person-group-type="author">
          <name>
            <surname>Oh</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Ye</surname>
            <given-names>J.C.</given-names>
          </name>
        </person-group>
        <article-title>Deep learning COVID-19 features on CXR using limited training data sets</article-title>
        <source>IEEE Trans. Med. Imag.</source>
        <issue>1–1</issue>
        <year>2020</year>
      </element-citation>
    </ref>
    <ref id="bib34">
      <label>34</label>
      <element-citation publication-type="book" id="sref34">
        <person-group person-group-type="author">
          <name>
            <surname>Howard</surname>
            <given-names>A.G.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>Mobilenets: Efficient Convolutional Neural Networks for Mobile Vision Applications</part-title>
        <year>2017</year>
        <comment>
          <italic>arXiv preprint arXiv:1704.04861</italic>
        </comment>
      </element-citation>
    </ref>
    <ref id="bib35">
      <label>35</label>
      <element-citation publication-type="book" id="sref35">
        <person-group person-group-type="author">
          <name>
            <surname>Simonyan</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Zisserman</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <part-title>Very Deep Convolutional Networks for Large-Scale Image Recognition</part-title>
        <year>2014</year>
        <comment>
          <italic>arXiv preprint arXiv:1409.1556</italic>
        </comment>
      </element-citation>
    </ref>
    <ref id="bib36">
      <label>36</label>
      <element-citation publication-type="journal" id="sref36">
        <person-group person-group-type="author">
          <name>
            <surname>Bergstra</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Random search for hyper-parameter optimization</article-title>
        <source>J. Mach. Learn. Res.</source>
        <volume>13</volume>
        <issue>2</issue>
        <year>2012</year>
      </element-citation>
    </ref>
    <ref id="bib37">
      <label>37</label>
      <element-citation publication-type="book" id="sref37">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Cisse</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Dauphin</surname>
            <given-names>Y.N.</given-names>
          </name>
          <name>
            <surname>Lopez-Paz</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <part-title>Mixup: beyond Empirical Risk Minimization</part-title>
        <year>2017</year>
        <comment>
          <italic>arXiv preprint arXiv:1710.09412</italic>
        </comment>
      </element-citation>
    </ref>
    <ref id="bib38">
      <label>38</label>
      <element-citation publication-type="book" id="sref38">
        <person-group person-group-type="author">
          <name>
            <surname>Tan</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Le</surname>
            <given-names>Q.V.</given-names>
          </name>
          <name>
            <surname>Efficientnet</surname>
            <given-names>“</given-names>
          </name>
        </person-group>
        <part-title>Rethinking Model Scaling for Convolutional Neural Networks</part-title>
        <year>2019</year>
        <comment>
          <italic>arXiv preprint arXiv:1905.11946</italic>
        </comment>
      </element-citation>
    </ref>
    <ref id="bib39">
      <label>39</label>
      <element-citation publication-type="book" id="sref39">
        <person-group person-group-type="author">
          <name>
            <surname>Kingma</surname>
            <given-names>D.P.</given-names>
          </name>
          <name>
            <surname>Ba</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Adam</surname>
            <given-names>“</given-names>
          </name>
        </person-group>
        <part-title>A Method for Stochastic Optimization</part-title>
        <year>2014</year>
        <comment>
          <italic>arXiv preprint arXiv:1412.6980</italic>
        </comment>
      </element-citation>
    </ref>
    <ref id="bib40">
      <label>40</label>
      <element-citation publication-type="book" id="sref40">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Bagheri</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Summers</surname>
            <given-names>R.M.</given-names>
          </name>
        </person-group>
        <part-title>Chestx-ray8: hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</part-title>
        <source>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>
        <year>2017</year>
        <fpage>2097</fpage>
        <lpage>2106</lpage>
      </element-citation>
    </ref>
    <ref id="bib41">
      <label>41</label>
      <element-citation publication-type="book" id="sref41">
        <person-group person-group-type="author">
          <name>
            <surname>de Moura</surname>
            <given-names>J.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>Deep Convolutional Approaches for the Analysis of Covid-19 Using Chest X-Ray Images from Portable Devices</part-title>
        <year>2020</year>
        <publisher-name>
          <italic>medRxiv</italic>
        </publisher-name>
      </element-citation>
    </ref>
    <ref id="bib42">
      <label>42</label>
      <element-citation publication-type="journal" id="sref42">
        <person-group person-group-type="author">
          <name>
            <surname>Zhong</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Kang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Random erasing data augmentation</article-title>
        <source>in <italic>AAAI</italic></source>
        <year>2020</year>
        <fpage>13 001</fpage>
        <lpage>13 008</lpage>
      </element-citation>
    </ref>
    <ref id="bib43">
      <label>43</label>
      <element-citation publication-type="book" id="sref43">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <part-title>Deep residual learning for image recognition</part-title>
        <source>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>
        <year>2016</year>
        <fpage>770</fpage>
        <lpage>778</lpage>
      </element-citation>
    </ref>
    <ref id="bib44">
      <label>44</label>
      <element-citation publication-type="book" id="sref44">
        <person-group person-group-type="author">
          <name>
            <surname>Szegedy</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Vanhoucke</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Ioffe</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Shlens</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Wojna</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <part-title>Rethinking the inception architecture for computer vision</part-title>
        <source>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>
        <year>2016</year>
        <fpage>2818</fpage>
        <lpage>2826</lpage>
      </element-citation>
    </ref>
    <ref id="bib45">
      <label>45</label>
      <element-citation publication-type="book" id="sref45">
        <person-group person-group-type="author">
          <name>
            <surname>Smith</surname>
            <given-names>L.N.</given-names>
          </name>
        </person-group>
        <part-title>“A Disciplined Approach to Neural Network Hyper-Parameters: Part 1–learning Rate, Batch Size, Momentum, and Weight Decay</part-title>
        <year>2018</year>
        <comment>
          <italic>arXiv preprint arXiv:1803.09820</italic>
        </comment>
      </element-citation>
    </ref>
    <ref id="bib46">
      <label>46</label>
      <element-citation publication-type="book" id="sref46">
        <person-group person-group-type="author">
          <name>
            <surname>Howard</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Ruder</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <part-title>Universal Language Model Fine-Tuning for Text Classification</part-title>
        <year>2018</year>
        <comment><italic>arXiv preprint arXiv:1801.06146</italic>,</comment>
      </element-citation>
    </ref>
    <ref id="bib47">
      <label>47</label>
      <element-citation publication-type="book" id="sref47">
        <person-group person-group-type="author">
          <name>
            <surname>Grandini</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Bagli</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Visani</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <part-title>Metrics for Multi-Class Classification: an Overview</part-title>
        <year>2020</year>
        <comment>
          <italic>arXiv preprint arXiv:2008.05756</italic>
        </comment>
      </element-citation>
    </ref>
    <ref id="bib48">
      <label>48</label>
      <element-citation publication-type="journal" id="sref48">
        <person-group person-group-type="author">
          <name>
            <surname>Fawcett</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>An introduction to ROC analysis</article-title>
        <source>Pattern Recogn. Lett.</source>
        <volume>27</volume>
        <issue>8</issue>
        <year>2006</year>
        <fpage>861</fpage>
        <lpage>874</lpage>
      </element-citation>
    </ref>
    <ref id="bib49">
      <label>49</label>
      <element-citation publication-type="journal" id="sref49">
        <person-group person-group-type="author">
          <name>
            <surname>Gorodkin</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Comparing two K-category assignments by a K-category correlation coefficient</article-title>
        <source>Comput. Biol. Chem.</source>
        <volume>28</volume>
        <issue>5–6</issue>
        <year>2004</year>
        <fpage>367</fpage>
        <lpage>374</lpage>
        <pub-id pub-id-type="pmid">15556477</pub-id>
      </element-citation>
    </ref>
    <ref id="bib50">
      <label>50</label>
      <element-citation publication-type="book" id="sref50">
        <person-group person-group-type="author">
          <name>
            <surname>Ketkar</surname>
            <given-names>N.</given-names>
          </name>
        </person-group>
        <part-title>Introduction to pytorch</part-title>
        <source>In Deep Learning with python</source>
        <year>2017</year>
        <publisher-name>Springer</publisher-name>
        <fpage>195</fpage>
        <lpage>208</lpage>
      </element-citation>
    </ref>
    <ref id="bib51">
      <label>51</label>
      <element-citation publication-type="journal" id="sref51">
        <person-group person-group-type="author">
          <name>
            <surname>Howard</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Gugger</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Fastai</surname>
            <given-names>“</given-names>
          </name>
        </person-group>
        <article-title>A layered API for deep learning</article-title>
        <source>Information</source>
        <volume>11</volume>
        <issue>2</issue>
        <year>2020</year>
        <fpage>108</fpage>
      </element-citation>
    </ref>
    <ref id="bib52">
      <label>52</label>
      <element-citation publication-type="book" id="sref52">
        <person-group person-group-type="author">
          <name>
            <surname>Cubuk</surname>
            <given-names>E.D.</given-names>
          </name>
          <name>
            <surname>Zoph</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Shlens</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Le</surname>
            <given-names>Q.V.</given-names>
          </name>
          <name>
            <surname>Randaugment</surname>
            <given-names>“</given-names>
          </name>
        </person-group>
        <part-title>Practical automated data augmentation with a reduced search space</part-title>
        <source>In <italic>Proceedings Of the IEEE/CVF Conference On Computer Vision And Pattern Recognition Workshops</italic></source>
        <year>2020</year>
        <fpage>702</fpage>
        <lpage>703</lpage>
      </element-citation>
    </ref>
    <ref id="bib53">
      <label>53</label>
      <element-citation publication-type="book" id="sref53">
        <person-group person-group-type="author">
          <name>
            <surname>Lim</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <part-title>Fast Autoaugment</part-title>
        <year>2019</year>
        <comment>
          <italic>arXiv preprint arXiv:1905.00397</italic>
        </comment>
      </element-citation>
    </ref>
    <ref id="bib54">
      <label>54</label>
      <element-citation publication-type="book" id="sref54">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Tran</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Feiszli</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <part-title>What makes training multi-modal classification networks hard?</part-title>
        <source>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source>
        <year>2020</year>
        <fpage>12695</fpage>
        <lpage>12705</lpage>
      </element-citation>
    </ref>
  </ref-list>
  <ack id="ack0010">
    <title>Acknowledgement</title>
    <p id="p0275">This material is based upon work supported by <funding-source id="gs1">Google Cloud COVID-19 research credits program</funding-source>.</p>
  </ack>
</back>
