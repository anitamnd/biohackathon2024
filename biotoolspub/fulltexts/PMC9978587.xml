<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9978587</article-id>
    <article-id pub-id-type="pmid">36794913</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btad094</article-id>
    <article-id pub-id-type="publisher-id">btad094</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Paper</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Sequence Analysis</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>PFresGO: an attention mechanism-based deep-learning approach for protein annotation by integrating gene ontology inter-relationships</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Pan</surname>
          <given-names>Tong</given-names>
        </name>
        <aff><institution>Department of Biochemistry and Molecular Biology, Biomedicine Discovery Institute, Monash University</institution>, Melbourne, VIC 3800, <country country="AU">Australia</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-1847-754X</contrib-id>
        <name>
          <surname>Li</surname>
          <given-names>Chen</given-names>
        </name>
        <aff><institution>Department of Biochemistry and Molecular Biology, Biomedicine Discovery Institute, Monash University</institution>, Melbourne, VIC 3800, <country country="AU">Australia</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Bi</surname>
          <given-names>Yue</given-names>
        </name>
        <aff><institution>Department of Biochemistry and Molecular Biology, Biomedicine Discovery Institute, Monash University</institution>, Melbourne, VIC 3800, <country country="AU">Australia</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Zhikang</given-names>
        </name>
        <aff><institution>Department of Biochemistry and Molecular Biology, Biomedicine Discovery Institute, Monash University</institution>, Melbourne, VIC 3800, <country country="AU">Australia</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-4423-1690</contrib-id>
        <name>
          <surname>Gasser</surname>
          <given-names>Robin B</given-names>
        </name>
        <aff><institution>Department of Veterinary Biosciences, Melbourne Veterinary School, The University of Melbourne</institution>, Parkville, VIC 3010, <country country="AU">Australia</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Purcell</surname>
          <given-names>Anthony W</given-names>
        </name>
        <aff><institution>Department of Biochemistry and Molecular Biology, Biomedicine Discovery Institute, Monash University</institution>, Melbourne, VIC 3800, <country country="AU">Australia</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-9763-797X</contrib-id>
        <name>
          <surname>Akutsu</surname>
          <given-names>Tatsuya</given-names>
        </name>
        <aff><institution>Bioinformatics Center, Institute for Chemical Research, Kyoto University</institution>, Uji 611-0011, <country country="JP">Japan</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Webb</surname>
          <given-names>Geoffrey I</given-names>
        </name>
        <aff><institution>Monash Data Futures Institute, Monash University</institution>, Melbourne, VIC 3800, <country country="AU">Australia</country></aff>
        <xref rid="btad094-cor1" ref-type="corresp"/>
        <!--geoff.webb@monash.edu-->
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-2989-308X</contrib-id>
        <name>
          <surname>Imoto</surname>
          <given-names>Seiya</given-names>
        </name>
        <aff><institution>Division of Health Medical Intelligence, Human Genome Center, Institute of Medical Science, The University of Tokyo, Minato-ku</institution>, Tokyo 108-8639, <country country="JP">Japan</country></aff>
        <aff><institution>Collaborative Research Institute for Innovative Microbiology, The University of Tokyo, Bunkyo-ku</institution>, Tokyo 113-8657, <country country="JP">Japan</country></aff>
        <xref rid="btad094-cor1" ref-type="corresp"/>
        <!--imoto@ims.u-tokyo.ac.jp-->
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-8031-9086</contrib-id>
        <name>
          <surname>Song</surname>
          <given-names>Jiangning</given-names>
        </name>
        <aff><institution>Department of Biochemistry and Molecular Biology, Biomedicine Discovery Institute, Monash University</institution>, Melbourne, VIC 3800, <country country="AU">Australia</country></aff>
        <aff><institution>Bioinformatics Center, Institute for Chemical Research, Kyoto University</institution>, Uji 611-0011, <country country="JP">Japan</country></aff>
        <aff><institution>Monash Data Futures Institute, Monash University</institution>, Melbourne, VIC 3800, <country country="AU">Australia</country></aff>
        <xref rid="btad094-cor1" ref-type="corresp"/>
        <!--jiangning.song@monash.edu-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Martelli</surname>
          <given-names>Pier Luigi</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btad094-cor1">To whom correspondence should be addressed. <email>jiangning.song@monash.edu</email> or <email>geoff.webb@monash.edu</email> or <email>imoto@ims.u-tokyo.ac.jp</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>3</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2023-02-16">
      <day>16</day>
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>16</day>
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <volume>39</volume>
    <issue>3</issue>
    <elocation-id>btad094</elocation-id>
    <history>
      <date date-type="received">
        <day>21</day>
        <month>10</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>10</day>
        <month>2</month>
        <year>2023</year>
      </date>
      <date date-type="editorial-decision">
        <day>13</day>
        <month>2</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>15</day>
        <month>2</month>
        <year>2023</year>
      </date>
      <date date-type="corrected-typeset">
        <day>01</day>
        <month>3</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Â© The Author(s) 2023. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btad094.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>The rapid accumulation of high-throughput sequence data demands the development of effective and efficient data-driven computational methods to functionally annotate proteins. However, most current approaches used for functional annotation simply focus on the use of protein-level information but ignore inter-relationships among annotations.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>Here, we established PFresGO, an attention-based deep-learning approach that incorporates hierarchical structures in Gene Ontology (GO) graphs and advances in natural language processing algorithms for the functional annotation of proteins. PFresGO employs a self-attention operation to capture the inter-relationships of GO terms, updates its embedding accordingly and uses a cross-attention operation to project protein representations and GO embedding into a common latent space to identify global protein sequence patterns and local functional residues. We demonstrate that PFresGO consistently achieves superior performance across GO categories when compared with âstate-of-the-artâ methods. Importantly, we show that PFresGO can identify functionally important residues in protein sequences by assessing the distribution of attention weightings. PFresGO should serve as an effective tool for the accurate functional annotation of proteins and functional domains within proteins.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>PFresGO is available for academic purposes at <ext-link xlink:href="https://github.com/BioColLab/PFresGO" ext-link-type="uri">https://github.com/BioColLab/PFresGO</ext-link>.</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p>Supplementary data are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Major Inter-Disciplinary Research</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="8"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Proteins are indispensable macromolecules that play fundamental roles in many activities and biological functions in living cells, such as maintaining normal metabolism, transporting nutrients, transducing signals and catalyzing biochemistry interactions (<xref rid="btad094-B17" ref-type="bibr">Lee <italic toggle="yes">et al.</italic>, 2007</xref>). To infer general or specific functions of proteins and to establish their relationships, standardized classification schemes (<xref rid="btad094-B21" ref-type="bibr">Ouzounis <italic toggle="yes">et al.</italic>, 2003</xref>), such as the enzyme classification (EC) System (<xref rid="btad094-B1" ref-type="bibr">Bairoch, 2000</xref>), Kyoto Encyclopedia of Genes and Genomes (KEGG) (<xref rid="btad094-B14" ref-type="bibr">Kanehisa <italic toggle="yes">et al.</italic>, 2021</xref>) and Gene Ontology (GO) (<xref rid="btad094-B11" ref-type="bibr">The Gene Ontology Consortium, 2008</xref>) have been developed. To date, GO is a widely accepted and used system for the functional annotation of proteins (i.e. gene products). GO terms are organized hierarchically in a directed acyclic graph (DAG), according to protein relationships and are divided into three non-overlapping branches, namely molecular function (MF), biological process (BP) and cellular component (CC).</p>
    <p>The rapid accumulation of protein datasets through the use of genomic, transcriptomic and proteomic techniques has resulted in an exponential growth in demand for high-throughput and reliable functional annotation of such datasets (<xref rid="btad094-B13" ref-type="bibr">Hasin <italic toggle="yes">et al.</italic>, 2017</xref>). For example, the UniProt database (<xref rid="btad094-B26" ref-type="bibr">The UniProt Consortium, 2021</xref>) contains more than 200 million protein sequences, but &lt;1% of these entries have been fully annotated (<xref rid="btad094-B12" ref-type="bibr">Gligorijevic <italic toggle="yes">et al.</italic>, 2021</xref>), which relates to major limitations (in terms of throughput, time and cost) associated with the conventional approach of annotating proteins using laboratory-based methods and information from published literature. To circumvent these constraints, computational methods have been established to predict the functions of proteins represented in large datasets (<xref rid="btad094-B24" ref-type="bibr">Sharma <italic toggle="yes">et al.</italic>, 2022</xref>). These include homology-based, machine-learning-based and deep-learning-based methods. Homology-based methods rely on the comparison of protein sequences using, for example, BLAST (<xref rid="btad094-B28" ref-type="bibr">Ye <italic toggle="yes">et al.</italic>, 2006</xref>), because evolutionarily related proteins tend to have similar functions, although minor mutations can significantly alter protein structure and function.</p>
    <p>Compared with homology-based methods, conventional machine-learning approaches, such as support vector machines (<xref rid="btad094-B2" ref-type="bibr">Cai <italic toggle="yes">et al.</italic>, 2003</xref>) and random forest (<xref rid="btad094-B4" ref-type="bibr">Chen and Ishwaran, 2012</xref>), and deep-learning-based approaches (<xref rid="btad094-B22" ref-type="bibr">Sapoval <italic toggle="yes">et al.</italic>, 2022</xref>) are reported to exhibit a superior prediction performance. Most deep-learning methods treat the annotation of protein function as a multi-label prediction task, where protein information is used as the model input and the predicted GO terms represent outputs, disregarding the correlations of GO labels. Although GO terms and their hierarchical structure have been measured based on semantic similarity (<xref rid="btad094-B7" ref-type="bibr">Edera <italic toggle="yes">et al.</italic>, 2022</xref>) and applied in various studies, there are limited studies that explicitly account for the GO term inter-relationships. DeepGO (<xref rid="btad094-B16" ref-type="bibr">Kulmanov <italic toggle="yes">et al.</italic>, 2018</xref>) constructed a deep-learning classification model that resembled the structure and dependencies between GO classes to refine features on each distinction present in the GO. Another tool, DEEPred (<xref rid="btad094-B25" ref-type="bibr">Sureyya Rifaioglu <italic toggle="yes">et al.</italic>, 2019</xref>), applied a stack of multi-task feed-forward networks according to the inheritance relationships of the GO system for protein function prediction. DeeProtGO (<xref rid="btad094-B19" ref-type="bibr">Merino <italic toggle="yes">et al.</italic>, 2022</xref>), which is a feed-forward deep neural network for predicting GO terms, integrated the GO knowledge represented by means of normalized co-occurrence vectors. Meanwhile, DeepGOZero (<xref rid="btad094-B15" ref-type="bibr">Kulmanov and Hoehndorf, 2022</xref>) combined a model-theoretic approach for learning ontology embedding, using the axioms of the GO to constrain function prediction. TALE (<xref rid="btad094-B3" ref-type="bibr">Cao and Shen, 2021</xref>) employed a transformer-based deep-learning model with a joint embedding of sequence inputs and hierarchical function labels. While it remains a great challenge regarding how to effectively capture the GO term inter-relationships, a recent study (<xref rid="btad094-B6" ref-type="bibr">Duong <italic toggle="yes">et al.</italic>, 2020</xref>) shows that incorporating the hierarchical structure of GO graphs can enable the annotation model to emphasize on the GO label distribution, thereby benefitting the final prediction.</p>
    <p>In this article, we propose a novel, attention-based approach, termed PFresGO, for protein function annotation by leveraging both protein residual-level representations and GO architecture. PFresGO uses sequence information of the query proteins as input; it takes the protein sequence embedding encoded by the pre-trained language model as well as GO terms embedding as inputs and delivers a probability of protein function by calculating the correlation between protein features and individual GO terms via an attention mechanism. Our findings show that PFresGO performs better than existing methods across all GO categories and benefits markedly from the incorporation of GO hierarchical structure information. The interpretation of annotation results is enhanced through the location of functionally relevant residues/domains in protein sequences via the analysis of attention weights.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Functional annotation data and GO graphs</title>
      <p>We employed the curated dataset from the study of DeepFRIâa graph convolutional network (<xref rid="btad094-B12" ref-type="bibr">Gligorijevic <italic toggle="yes">et al.</italic>, 2021</xref>). This dataset of 36Â 641 protein sequences provided the coverage of 2752 GO terms across MF (<italic toggle="yes">n</italic>â=â489 terms), BP (1943) and CC (320), with each GO term linked to &gt;50 non-redundant Protein Data Bank (PDB) chains. This dataset was further divided into training (â¼80%; 29Â 902 sequences), validation (â¼10%; 3323 sequences) and test (â¼10%; 3416 sequences) datasets for the training, optimization and evaluation of the model, respectively. For the test dataset, only proteins with at least one trusted functional annotation in each of the three GO categories was selected. All annotations represented in the test dataset were experimentally validated, and the maximum length of protein chains was limited to 1000. CD-HIT (<xref rid="btad094-B10" ref-type="bibr">Fu <italic toggle="yes">et al.</italic>, 2012</xref>) was applied to ensure that there were no redundant PDB chains between training and test datasets using varying sequence identity thresholds. The relationship among GO terms was illustrated as a DAG. A filtered version (based on the dataset) of the GO.obo format file describing the hierarchical related structure of GO terms was downloaded from the GO resource website (<ext-link xlink:href="http://geneontology.org/" ext-link-type="uri">http://geneontology.org/</ext-link>; data version: 1 June 2020) (<xref rid="btad094-B151" ref-type="bibr">Day-Richter <italic toggle="yes">et al.</italic>, 2007</xref>), which ensures consistency in annotation with previous works for impartial comparisons.</p>
    </sec>
    <sec>
      <title>2.2 Input features</title>
      <sec>
        <title>2.2.1 Protein sequence embedding</title>
        <p>Given a protein sequence <italic toggle="yes">S</italic> with <italic toggle="yes">l</italic> residues, we first used one-hot embedding to represent the protein sequence <italic toggle="yes">S</italic>. Specifically, each residue in the protein sequence was embedded into a 26-dimension vector (including the 20 standard amino acids, 5 non-standard amino acids and 1 padding symbol). The one-hot encoding procedure was followed by a fully connected layer with hidden dimension <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> to generate an embedding matrix <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula>. We also utilized a deep-learning language model ProtT5 (<xref rid="btad094-B9" ref-type="bibr">Elnaggar <italic toggle="yes">et al.</italic>, 2021</xref>), which had been pre-trained on datasets comprising 393 billion amino acids, to encode the protein sequence <italic toggle="yes">S</italic> with <italic toggle="yes">l</italic> residues into the residue-level protein sequence feature embedding <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula>, where <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is 1024 by default. The encoded residue-level feature vector comprises the information of individual residue and its immediate context, and constraints of protein global structure and protein function.</p>
      </sec>
      <sec>
        <title>2.2.2 GO term embedding</title>
        <p>Gene ontology (GO) is a commonly used classification scheme in terms of annotating protein functions. Here, we applied the pre-trained model Anc2vec (<xref rid="btad094-B7" ref-type="bibr">Edera <italic toggle="yes">et al.</italic>, 2022</xref>) to generate the compact GO term embedding as the initial input of PFresGO. Anc2vec is a neural network-based protocol that considers the preservation of ontological uniqueness, ancestorsâ hierarchy and sub-ontology membership to embed GO terms. More specifically, each GO term <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is embedded into a <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> dimensional label representation vector, where <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is the predefined hidden dimension.</p>
      </sec>
    </sec>
    <sec>
      <title>2.3 The autoencoder module</title>
      <p>The autoencoder module (<xref rid="btad094-B20" ref-type="bibr">Ng, 2011</xref>) was used to reduce the high-dimension residue-level protein data to feature vectors of hidden dimension <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>. The module comprises two submodules, including an encoder submodule and a decoder submodule; each is composed of two layers of neurons. The encoder submodule transfers the high-dimension input data into a low-dimensional latent space, while the decoder submodule converts the low-dimensional vector back to the original space, reversely. The dimension-reduced feature vector in the latent space is represented as compressed low-dimension embedding of the original input.</p>
      <p>The output of the encoder submodule can be computed using
<disp-formula id="E1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">en</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi mathvariant="normal">ReLU</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">en</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>Ã</mml:mo><mml:msubsup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">en</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">en</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">en</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">en</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi mathvariant="normal">Â </mml:mi></mml:math></inline-formula>denote the learned weights and the bias of <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> th encoder layer. The rectified linear unit (ReLU) is a non-linear activate function <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mi mathvariant="normal">ReLU</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">max</mml:mi><mml:mo>â¡</mml:mo><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula>. The output of the previous encoder layer <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">en</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> serves as the input of the following <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mi mathvariant="normal">Â </mml:mi></mml:math></inline-formula>th encoder layer. Specifically, the initial input to the encoder submodule is the residue-level protein sequence features, i.e. <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">en</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula>, where <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1024</mml:mn></mml:math></inline-formula>. The number of neurons in the second encoder layer is the predefined hidden dimension <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>.</p>
      <p>The decoder submodule takes the reduced dimension embedding <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">en</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> as input and aims to recover the embedded feature vector into the original dimension. The output of the decoder submodule can be computed as:
<disp-formula id="E2"><label>(2)</label><mml:math id="M2" display="block" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">de</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi mathvariant="normal">ReLU</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">de</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>Ã</mml:mo><mml:msubsup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">de</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">de</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">de</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">de</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>Â </mml:mo></mml:math></inline-formula>represent the learned weights and the bias of <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>th decoder layer, respectively. The decoder module consists of two neural network layers. The output of the previous decoder layer <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">de</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> is used as the input of the following decoder layer. The final optimization goal of the autoencoder is to minimize the reconstruction error (squared error) between the initial encoder input and the reconstructed decoder output:
<disp-formula id="E3"><label>(3)</label><mml:math id="M3" display="block" overflow="scroll"><mml:mi mathvariant="normal">loss</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="false">â</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy="false">â</mml:mo><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">en</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msubsup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">de</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula></p>
      <p>Given the protein feature vector <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>Ã</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, we computed the encoder submodule output <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula> as the compressed residue-level embedding, which is then added with <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula> for the final protein residue-level embedding <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mi>E</mml:mi><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula>.</p>
    </sec>
    <sec>
      <title>2.4 The multi-head attention module</title>
      <p>The functional annotation of proteins is a multi-label classification task. The prediction algorithm should, therefore, consider the relationships among GO terms. Theoretically, proteins perform specific biological functions relying on spatially aggregated functional residues, such as ligand-binding sites of proteins and catalytic residues in enzymes (<xref rid="btad094-B18" ref-type="bibr">Lichtarge <italic toggle="yes">et al.</italic>, 1996</xref>). We then expect PFresGO to be able to dynamically focus on functional residues to capture the relationship among GO terms and key functional regions within protein sequences, thereby enabling the final predictions of protein functions. With this goal in mind, we integrated two multi-head attention operations to enable PFresGO to simultaneously capture relevant feature projections from multi-subspaces. The main principle of the multi-head attention mechanism is to calculate the scaled dot-product attention as follows:
<disp-formula id="E4"><label>(4)</label><mml:math id="M4" display="block" overflow="scroll"><mml:mi mathvariant="normal">Attention</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="normal">softmax</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:mo>Ã</mml:mo><mml:msup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msqrt><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mi>V</mml:mi><mml:mo>,</mml:mo></mml:math></disp-formula>where <italic toggle="yes">Q</italic>, <italic toggle="yes">K</italic> and <italic toggle="yes">V</italic> refer to the query, key and value matrix transformed from the attention layer input, respectively, and <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>Â </mml:mo></mml:math></inline-formula>represents a constant of the key dimension as a scalar factor.</p>
      <p>The first multi-head attention operation encourages the model to automatically capture the correlations between GO terms and then update the GO term embedding accordingly. Given an input of GO term embedding <inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:mi>G</mml:mi><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula>, the updated GO term embedding <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mo>Â¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> can be calculated as
<disp-formula id="E5"><label>(5)</label><mml:math id="M5" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="normal">head</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Attention</mml:mi><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula><disp-formula id="E6"><label>(6)</label><mml:math id="M6" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Contact</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">head</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>â¦</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">head</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:math></disp-formula><disp-formula id="E7"><label>(7)</label><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mo>Â¯</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">LN</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="normal">head</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> indicates the <italic toggle="yes">i</italic>th attention head, with <italic toggle="yes">n</italic> heads in total. The learned weights <inline-formula id="IE31"><mml:math id="IM31" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msubsup><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula>, <inline-formula id="IE32"><mml:math id="IM32" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula id="IE33"><mml:math id="IM33" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msubsup><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula> are used to project the input GO term embedding <inline-formula id="IE34"><mml:math id="IM34" display="inline" overflow="scroll"><mml:mi>G</mml:mi><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula> into the corresponding query matrix <inline-formula id="IE35"><mml:math id="IM35" display="inline" overflow="scroll"><mml:mi>Q</mml:mi><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula>, the key matrix <inline-formula id="IE36"><mml:math id="IM36" display="inline" overflow="scroll"><mml:mi>K</mml:mi><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula>, and the value matrix <inline-formula id="IE37"><mml:math id="IM37" display="inline" overflow="scroll"><mml:mi>V</mml:mi><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula>, respectively. The <italic toggle="yes">n</italic> attention matrix computed based on <italic toggle="yes">Q</italic>, <italic toggle="yes">K</italic> and <italic toggle="yes">V</italic> are then concatenated and multiplied for the final output matrix <inline-formula id="IE38"><mml:math id="IM38" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msubsup><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula>, to obtain the updated GO term embedding <inline-formula id="IE39"><mml:math id="IM39" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula>. Then, a residual connection as well as a layer normalization procedure, was applied to obtain <inline-formula id="IE40"><mml:math id="IM40" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mo>Â¯</mml:mo></mml:mover></mml:mrow><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula>.</p>
      <p>Another multi-head attention mechanism was applied, where the model takes GO terms as a query to detect specific protein features important for protein function annotation. The protein embedding is zero-padded if the protein chain consists of &lt;1000 residues. Given a zero-padded protein feature embedding <inline-formula id="IE41"><mml:math id="IM41" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mo>Â¯</mml:mo></mml:mover></mml:mrow><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mi mathvariant="normal">Â </mml:mi><mml:mo>(</mml:mo><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>1000</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula>, we first calculated the attention between the protein feature and GO labels:.
<disp-formula id="E8"><label>(8)</label><mml:math id="M8" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="normal">head</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Attention</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mo>Â¯</mml:mo></mml:mover></mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mo>Â¯</mml:mo></mml:mover></mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mo>Â¯</mml:mo></mml:mover></mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula><disp-formula id="E9"><label>(9)</label><mml:math id="M9" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Contact</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">head</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>â¦</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">head</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:math></disp-formula><disp-formula id="E10"><label>(10)</label><mml:math id="M10" display="block" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">LN</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mo>Â¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula id="IE42"><mml:math id="IM42" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="normal">head</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> indicates the <italic toggle="yes">i</italic>th attention head, with <italic toggle="yes">n</italic> heads in total. Similarly, the learned weights <inline-formula id="IE43"><mml:math id="IM43" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msubsup><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula>, <inline-formula id="IE44"><mml:math id="IM44" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula id="IE45"><mml:math id="IM45" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msubsup><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula> are used to project the input GO term embedding <inline-formula id="IE46"><mml:math id="IM46" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mo>Â¯</mml:mo></mml:mover></mml:mrow><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula> into the corresponding query matrix <inline-formula id="IE47"><mml:math id="IM47" display="inline" overflow="scroll"><mml:mi>Q</mml:mi><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula>, and project the residue-level protein feature embedding <inline-formula id="IE48"><mml:math id="IM48" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mo>Â¯</mml:mo></mml:mover></mml:mrow><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula> into the key matrix <inline-formula id="IE49"><mml:math id="IM49" display="inline" overflow="scroll"><mml:mi>K</mml:mi><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula>, and the value matrix <inline-formula id="IE50"><mml:math id="IM50" display="inline" overflow="scroll"><mml:mi>V</mml:mi><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula>, respectively. The <italic toggle="yes">n</italic> attention matrix computed based on <italic toggle="yes">Q</italic>, <italic toggle="yes">K</italic> and <italic toggle="yes">V</italic> were then concatenated and multiplied for the final output matrix <inline-formula id="IE51"><mml:math id="IM51" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msup><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula>, to obtain the updated GO term embedding <inline-formula id="IE52"><mml:math id="IM52" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula>. Again, a residual connection and layer normalization were applied to acquire <inline-formula id="IE53"><mml:math id="IM53" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula>. A feed-forward layer is followed to take the <inline-formula id="IE54"><mml:math id="IM54" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> dimensional embedding <inline-formula id="IE55"><mml:math id="IM55" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> as input and perform two point-wise dense layers to obtain <inline-formula id="IE56"><mml:math id="IM56" display="inline" overflow="scroll"><mml:mi mathvariant="normal">FF</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>Ã</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>:
<disp-formula id="E11"><label>(11)</label><mml:math id="M11" display="block" overflow="scroll"><mml:mi mathvariant="normal">FF</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="normal">ReLU</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula id="IE57"><mml:math id="IM57" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula id="IE58"><mml:math id="IM58" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula id="IE59"><mml:math id="IM59" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula id="IE60"><mml:math id="IM60" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> are learnable weights and biases of two dense layers, respectively. Here, we linked two multi-head attention modules for MF and CC protein function annotation. For BP term, only one multi-head attention module is applied considering the memory limitation.</p>
    </sec>
    <sec>
      <title>2.5 The GO term prediction module</title>
      <p>This module computes the probability of each GO term. It formulates the multi-label task of protein function annotation as a binary classification task. Specifically, it projects the individual GO term embedding feature into a probability value. In the first step, we performed a global pooling on the resulted vector <inline-formula id="IE61"><mml:math id="IM61" display="inline" overflow="scroll"><mml:mi mathvariant="normal">FF</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mfenced></mml:math></inline-formula> by summing over the last dimension:
<disp-formula id="E12"><label>(12)</label><mml:math id="M12" display="block" overflow="scroll"><mml:msup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">pool</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:munderover><mml:mrow><mml:mi mathvariant="normal">FF</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula></p>
      <p>We then computed the final probability distribution utilizing a fully connected layer with the <italic toggle="yes">sigmoid</italic> activation function from this pooled representation. The <italic toggle="yes">m</italic>-dimension output vector stands for the predicted probability of <italic toggle="yes">m</italic> GO terms:
<disp-formula id="E13"><label>(13)</label><mml:math id="M13" display="block" overflow="scroll"><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">sigmoid</mml:mi><mml:mo>(</mml:mo><mml:mi>W</mml:mi><mml:mo>Ã</mml:mo><mml:msup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">pool</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:math></disp-formula></p>
      <p>Given the true protein function GO label and the predicted probabilities, we minimized the binary cross-entropy loss to optimize the above process:
<disp-formula id="E14"><label>(14)</label><mml:math id="M14" display="block" overflow="scroll"><mml:mi mathvariant="script" class="calligraphy">L</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo stretchy="false">â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">â</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mi mathvariant="normal">GO</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>â¡</mml:mo><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>where <italic toggle="yes">N</italic> represents the total number of protein chains, |GO| is the total number of GO terms, <inline-formula id="IE62"><mml:math id="IM62" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula id="IE63"><mml:math id="IM63" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represents the true value and the predicted probability of GO term <italic toggle="yes">j</italic> for protein chain <italic toggle="yes">i</italic>.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 PFresGO annotates protein function using GO term inter-relationships</title>
      <p>First, we describe how PFresGO performs function annotations for a query protein. In brief, PFresGO contains three critical mechanisms to facilitate protein function prediction using GO terms, including a pre-trained protein language model, a GO inter-relationship self-attention model and a multi-head cross-attention mechanism. The architecture of PFresGO is illustrated in <xref rid="btad094-F1" ref-type="fig">FigureÂ 1</xref>. We utilized a pre-trained protein language model, ProtT5 (<xref rid="btad094-B9" ref-type="bibr">Elnaggar <italic toggle="yes">et al.</italic>, 2021</xref>), to encode informative protein sequence embedding, which is a novel natural-language-based model trained on &gt;390 billion amino acids. We learned a compact representation for embedded protein vectors using an autoencoder to reduce these vectors to a hidden dimension, which was then added with the projected one-hot embedding of protein sequences as the final protein feature representation at the amino acid residue level.</p>
      <fig position="float" id="btad094-F1">
        <label>Fig. 1.</label>
        <caption>
          <p>The architecture of PFresGO. The pre-trained language model (<xref rid="btad094-B9" ref-type="bibr">Elnaggar <italic toggle="yes">et al.</italic>, 2021</xref>) encodes amino acid sequences into protein feature embedding. An autoencoder module reduces the protein embedding to the hidden dimension d0, which adds with the projected one-hot sequence embedding to produce the residue-level protein representation. A self-attention operation is utilized to explore the relationships between GO term representations generated by Anc2vec (<xref rid="btad094-B7" ref-type="bibr">Edera <italic toggle="yes">et al.</italic>, 2022</xref>), a deep neural network for constructing GO term embedding and further updates the representations accordingly. A cross-attention operation is used to detect the correlation between the protein features and protein functions by taking gene ontology embedding as a query to detect related protein information, which is followed by linear layers to output the final GO term probability</p>
        </caption>
        <graphic xlink:href="btad094f1" position="float"/>
      </fig>
      <p>To learn the inter-relationships of GO terms, we initially applied the deep-learning-based Anc2vec algorithm (<xref rid="btad094-B7" ref-type="bibr">Edera <italic toggle="yes">et al.</italic>, 2022</xref>) to generate a compact GO term embedding, according to the hierarchical structure of the GO graph. A multi-head self-attention operation was then used to capture the inherent semantic relations of GO terms automatically and to update the GO term embedding accordingly. An âAdd and Normâ operation was then conducted to facilitate and stabilize the algorithm training process. We then applied a multi-head cross-attention operation to project residue-level protein representations and GO embedding into a common latent space, where GO terms act as a query to detect the global protein sequence patterns as well as local functional residues. The resultant vectors were processed by the âAdd and Normâ operation and then fed into a feed-forward module constituting two fully connected layers. The final dense layer, in which the number of neurons equals the number of GO term labels, serves as the output layer and computes the probability of each protein function term. A detailed description of PFresGO implementation and optimization is provided in <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref>.</p>
    </sec>
    <sec>
      <title>3.2 PFresGO outperforms existing methods across all GO categories</title>
      <p>We compared PFresGO to six previously proposed approaches: one sequence identity-based search method BLAST (<xref rid="btad094-B28" ref-type="bibr">Ye <italic toggle="yes">et al.</italic>, 2006</xref>), one protein domain-based function transfer approach FunFams (<xref rid="btad094-B5" ref-type="bibr">Das <italic toggle="yes">et al.</italic>, 2015</xref>) and four state-of-the-art deep-learning-based approaches DeepGO (<xref rid="btad094-B16" ref-type="bibr">Kulmanov <italic toggle="yes">et al.</italic>, 2018</xref>), DeepFRI (<xref rid="btad094-B12" ref-type="bibr">Gligorijevic <italic toggle="yes">et al.</italic>, 2021</xref>), TALE+ (<xref rid="btad094-B3" ref-type="bibr">Cao and Shen, 2021</xref>) and DeepGOZero (<xref rid="btad094-B15" ref-type="bibr">Kulmanov and Hoehndorf, 2022</xref>). Of these, BLAST has been extensively applied as the âstandardâ sequence-based method in many studies. In addition, DeepFRI was applied for structure-based comparison of the text dataset. <xref rid="sup1" ref-type="supplementary-material">Supplementary Sections S2 and S4</xref> describe details regarding performance measures and the comparison of distinct approaches.</p>
      <p>The performance comparison results are provided in <xref rid="btad094-T1" ref-type="table">TableÂ 1</xref>. Compared with the other methods assessed, PFresGO achieved a remarkable performance with <inline-formula id="IE64"><mml:math id="IM64" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> values of 0.6917, 0.5678 and 0.6737 for MF, BP and CC, respectively, while DeepGOZero (i.e. 0.7191) outperformed the other methods in terms of <inline-formula id="IE65"><mml:math id="IM65" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for MF. In relation to AUPRC, PFresGO performed favorably compared with the other methods, achieving values of 0.6017, 0.2934 and 0.3612 for MF, BP and CC, compared with 0.1357, 0.0674 and 0.0973 for BLAST, respectively. PFresGO consistently outperformed other methods in terms of AUROC for MF and CC, and achieved a comparable AUROC value (0.8394) to that of DeepFRI (i.e. 0.8578) for BP. As for <inline-formula id="IE66"><mml:math id="IM66" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, FunFams outperformed the competing methods for BP and CC, while PFresGO was the top three best predictor for MF and CC. Taken together, the performance values in <xref rid="btad094-T1" ref-type="table">TableÂ 1</xref> show the effectiveness of the proposed deep-learning strategy in PFresGO for the annotation of protein function.</p>
      <table-wrap position="float" id="btad094-T1">
        <label>Table 1.</label>
        <caption>
          <p>Performance comparison of PFresGO and state-of-the-art methods for protein function prediction on the independent test dataset</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Method</th>
              <th rowspan="1" colspan="1">GO category</th>
              <th rowspan="1" colspan="1">
                <inline-formula id="IE67">
                  <mml:math id="IM67" display="inline" overflow="scroll">
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>F</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi mathvariant="normal">max</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:math>
                </inline-formula>
              </th>
              <th rowspan="1" colspan="1">AUPRC</th>
              <th rowspan="1" colspan="1">AUROC</th>
              <th rowspan="1" colspan="1">
                <inline-formula id="IE68">
                  <mml:math id="IM68" display="inline" overflow="scroll">
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi mathvariant="normal">min</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:math>
                </inline-formula>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="3" colspan="1">BLAST</td>
              <td rowspan="1" colspan="1">MF</td>
              <td rowspan="1" colspan="1">0.3282</td>
              <td rowspan="1" colspan="1">0.1357</td>
              <td rowspan="1" colspan="1">0.7114</td>
              <td rowspan="1" colspan="1">5.5119</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">BP</td>
              <td rowspan="1" colspan="1">0.3358</td>
              <td rowspan="1" colspan="1">0.0674</td>
              <td rowspan="1" colspan="1">0.6450</td>
              <td rowspan="1" colspan="1">49.0130</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">CC</td>
              <td rowspan="1" colspan="1">0.4478</td>
              <td rowspan="1" colspan="1">0.0973</td>
              <td rowspan="1" colspan="1">0.6650</td>
              <td rowspan="1" colspan="1">6.4769</td>
            </tr>
            <tr>
              <td rowspan="3" colspan="1">DeepGO</td>
              <td rowspan="1" colspan="1">MF</td>
              <td rowspan="1" colspan="1">0.5772</td>
              <td rowspan="1" colspan="1">0.3911</td>
              <td rowspan="1" colspan="1">0.8599</td>
              <td rowspan="1" colspan="1">4.1592</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">BP</td>
              <td rowspan="1" colspan="1">0.4934</td>
              <td rowspan="1" colspan="1">0.1821</td>
              <td rowspan="1" colspan="1">0.8080</td>
              <td rowspan="1" colspan="1">44.4762</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">CC</td>
              <td rowspan="1" colspan="1">0.5941</td>
              <td rowspan="1" colspan="1">0.2627</td>
              <td rowspan="1" colspan="1">0.8617</td>
              <td rowspan="1" colspan="1">5.6486</td>
            </tr>
            <tr>
              <td rowspan="3" colspan="1">FunFams</td>
              <td rowspan="1" colspan="1">MF</td>
              <td rowspan="1" colspan="1">0.5721</td>
              <td rowspan="1" colspan="1">0.3671</td>
              <td rowspan="1" colspan="1">0.7506</td>
              <td rowspan="1" colspan="1">3.8517</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">BP</td>
              <td rowspan="1" colspan="1">0.4997</td>
              <td rowspan="1" colspan="1">0.2600</td>
              <td rowspan="1" colspan="1">0.7091</td>
              <td rowspan="1" colspan="1">
                <bold>38.9009</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">CC</td>
              <td rowspan="1" colspan="1">0.6265</td>
              <td rowspan="1" colspan="1">0.2882</td>
              <td rowspan="1" colspan="1">0.7677</td>
              <td rowspan="1" colspan="1">
                <bold>4.7833</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="3" colspan="1">DeepFRI</td>
              <td rowspan="1" colspan="1">MF</td>
              <td rowspan="1" colspan="1">0.6246</td>
              <td rowspan="1" colspan="1">0.4949</td>
              <td rowspan="1" colspan="1">0.9147</td>
              <td rowspan="1" colspan="1">3.7344</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">BP</td>
              <td rowspan="1" colspan="1">0.5402</td>
              <td rowspan="1" colspan="1">0.2612</td>
              <td rowspan="1" colspan="1"><bold>0</bold>.<bold>8578</bold></td>
              <td rowspan="1" colspan="1">41.9820</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">CC</td>
              <td rowspan="1" colspan="1">0.6126</td>
              <td rowspan="1" colspan="1">0.2744</td>
              <td rowspan="1" colspan="1">0.8837</td>
              <td rowspan="1" colspan="1">5.4917</td>
            </tr>
            <tr>
              <td rowspan="3" colspan="1">TALE+</td>
              <td rowspan="1" colspan="1">MF</td>
              <td rowspan="1" colspan="1">0.6624</td>
              <td rowspan="1" colspan="1">0.5642</td>
              <td rowspan="1" colspan="1">0.8844</td>
              <td rowspan="1" colspan="1">3.2205</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">BP</td>
              <td rowspan="1" colspan="1">0.5539</td>
              <td rowspan="1" colspan="1">
                <bold>0.3021</bold>
              </td>
              <td rowspan="1" colspan="1">0.8105</td>
              <td rowspan="1" colspan="1">39.9229</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">CC</td>
              <td rowspan="1" colspan="1">0.6099</td>
              <td rowspan="1" colspan="1">0.3251</td>
              <td rowspan="1" colspan="1">0.8486</td>
              <td rowspan="1" colspan="1">5.3235</td>
            </tr>
            <tr>
              <td rowspan="3" colspan="1">DeepGOZero</td>
              <td rowspan="1" colspan="1">MF</td>
              <td rowspan="1" colspan="1">
                <bold>0.7191</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.6144</bold>
              </td>
              <td rowspan="1" colspan="1">0.8925</td>
              <td rowspan="1" colspan="1">
                <bold>3.0187</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">BP</td>
              <td rowspan="1" colspan="1">0.5645</td>
              <td rowspan="1" colspan="1">0.2944</td>
              <td rowspan="1" colspan="1">0.7682</td>
              <td rowspan="1" colspan="1">40.9241</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">CC</td>
              <td rowspan="1" colspan="1">0.5341</td>
              <td rowspan="1" colspan="1">0.3146</td>
              <td rowspan="1" colspan="1">0.7381</td>
              <td rowspan="1" colspan="1">5.4340</td>
            </tr>
            <tr>
              <td rowspan="3" colspan="1">PFresGO</td>
              <td rowspan="1" colspan="1">MF</td>
              <td rowspan="1" colspan="1">0.6917</td>
              <td rowspan="1" colspan="1">0.6017</td>
              <td rowspan="1" colspan="1"><bold>0</bold>.<bold>9247</bold></td>
              <td rowspan="1" colspan="1">3.5600</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">BP</td>
              <td rowspan="1" colspan="1">
                <bold>0.5678</bold>
              </td>
              <td rowspan="1" colspan="1">0.2934</td>
              <td rowspan="1" colspan="1">0.8394</td>
              <td rowspan="1" colspan="1">41.3265</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">CC</td>
              <td rowspan="1" colspan="1"><bold>0</bold>.<bold>6737</bold></td>
              <td rowspan="1" colspan="1">
                <bold>0.3612</bold>
              </td>
              <td rowspan="1" colspan="1"><bold>0</bold>.<bold>8841</bold></td>
              <td rowspan="1" colspan="1">5.1916</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <p><italic toggle="yes">Note</italic>: The best performance on MF, BP and CC categories has been bolded.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>3.3 Incorporating GO term inter-relationships improves functional annotation</title>
      <p>The inter-relationships of GO terms are incorporated into PFresGO via two attention-based operations: the first operation automatically captures hierarchical information about GO graphs and updates the embedding accordingly; the second operation takes the embedding of each GO term as a query to explore potentially important protein features in the same latent space for the prediction of individual protein function terms.</p>
      <p>To thoroughly delineate the effectiveness of incorporating GO terms inter-relationships, we built PFresGO_Seq by only feeding the extracted protein feature representation into a dense output layer. We compared the AUPRC values of PFresGO_Seq and PFresGO, as well as the other two baseline deep-learning methodsâDeepGO and DeepFRIâacross all GO categories on the test dataset (<xref rid="btad094-F2" ref-type="fig">Fig.Â 2</xref>). Although PFresGO_Seq had a comparable performance to DeepGO for BP and to DeepFRI for CC ontology, PFresGO significantly outperformed all other methods assessed for all three GO categories (MF, BP and CC). These results show that PFresGO largely benefits from the strategy of incorporating GO term inter-relationships to functionally annotate proteins.</p>
      <fig position="float" id="btad094-F2">
        <label>Fig. 2.</label>
        <caption>
          <p>Precision-recall curves of methods DeepGO, DeepFRI, PFresGO and PFresGO_Seq on MF, BP and CC terms</p>
        </caption>
        <graphic xlink:href="btad094f2" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.4 PFresGO shows superior performance in annotating protein function with different sequence identities and GO specificities</title>
      <p>Here, we initially evaluated the ability of PFresGO to predictive using protein sequences with different identities, especially in the case of novel protein sequences with low sequence identities compared to the training dataset. We split the test dataset into five different groups with varying sequence identity thresholds: 30%, 40%, 50%, 70% and 95%, which are the maximum identity values of test sequences compared to the training dataset. We compared PFresGO with other models, including BLAST, DeepGO, FunFams, TALE+ and DeepFRI, using <inline-formula id="IE69"><mml:math id="IM69" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and AUPRC on the same test datasets, split by sequence identity thresholds.</p>
      <p>As shown in <xref rid="btad094-F3" ref-type="fig">FigureÂ 3a</xref> and b, the <inline-formula id="IE70"><mml:math id="IM70" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> values of all methods improved with increased sequence identity for all three GO categories, while PFresGO consistently outperformed other methods, regardless of the sequence identity. PFresGO also has higher AUPRC values for both MF and CC for all sequence identity thresholds, even when the test proteins shared â¤30% identity with the training dataset. Although FunFams achieved a higher AUPRC value for BP for proteins sharing &lt;40% identity to the training dataset, PFresGO outperformed other methods, achieving a higher AUPRC score for sequence identities ranging from 50% to 95%.</p>
      <fig position="float" id="btad094-F3">
        <label>Fig. 3.</label>
        <caption>
          <p>Performance comparison of (<bold>a</bold>) <italic toggle="yes">F</italic><sub>max</sub>, (<bold>b</bold>) AUPRC on varying sequence identity and (<bold>c</bold>) distributions of AUPRC scores on varying IC across MF, BP and CC categories among different methods: Blast, DeepGO, FunFams, DeepFRI, TALE+ and PFresGO</p>
        </caption>
        <graphic xlink:href="btad094f3" position="float"/>
      </fig>
      <p>Subsequently, we investigated the performance of PFresGO when annotating GO terms with a high specificity. Here, we evaluated the specificity of GO terms according to their Shannon Information Content (IC).
<disp-formula id="E15"><label>(15)</label><mml:math id="M15" display="block" overflow="scroll"><mml:mi mathvariant="normal">IC</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">GO</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="normal">Prob</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">GO</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:math></disp-formula></p>
      <p>A higher IC value of the GO term corresponds to a higher specificity (i.e. a rarer occurrence). We separately stratified the GO terms within the MF, BP and CC categories into three groups according to their IC values and then compared the performance of PFresGO with that of other methods using these GO terms with distinct IC values. We recorded the AUPRC values of different methods across all IC cutoffs (<xref rid="btad094-F3" ref-type="fig">Fig.Â 3c</xref>). Although all methods consistently showed sound performance when predicting protein GO terms for lower IC values, the high AUPRC value of PFresGO provided evidence of a marked advantage of integrating GO term relationships in the training process. PFresGO outperformed other methods in terms of AUPRC for GO terms, with high specificity (i.e. IC &gt;10) for MF and CC branches; its performance was comparable with FunFams for GO terms with high specificity (IC &gt;10) on the BP branch. PFresGO showed strong scalability and generalizability for functional annotation of novel query proteins with a limited sequence identity to those in training sets, and usually for annotation of GO terms with high specificity.</p>
      <p>Furthermore, we analyzed the performance of PFresGO in cases where the proteins in test set did not share any homologous domains with those in training set. More specifically, we applied the ECOD classifier (file: âecod.latest.domains.txtâ, version: â20221014â) (<xref rid="btad094-B23" ref-type="bibr">Schaeffer <italic toggle="yes">et al.</italic>, 2017</xref>) to rigorously divide the training and test sets to eliminate most if not all evolutionary relationships (i.e. the H level). Please refer to the <xref rid="sup1" ref-type="supplementary-material">Supplementary Section S9</xref> for detailed results. It can be seen that the performance of PFresGO dropped across all GO branches, indicating the use of protein domains as analytic units could improve the protein function annotation.</p>
    </sec>
    <sec>
      <title>3.5 PFresGO locates residues linked to protein function annotation</title>
      <p>As it has been reported that spatially aggregated functional residues play critical roles in protein functions (<xref rid="btad094-B18" ref-type="bibr">Lichtarge <italic toggle="yes">et al.</italic>, 1996</xref>), we assessed the ability of PFresGO to infer the location(s) of residues linked to protein function. Our hypothesis here is that PFresGO is capable of focusing more on protein residues that make more important contributions to protein function annotation with higher attention weights, and accordingly, such functionally important resides can be identified via the analysis of attention weights assigned by PFresGO. In <xref rid="btad094-F4" ref-type="fig">FigureÂ 4a</xref>, we illustrate an example of the visualization of averaged attention weights, which indicates that PFresGO correctly identified the sites in rat Î±-parvalbumin (PDB: 1S3P; Chain A) linked to âcalcium ion bindingâ (GO: 0005509). The grey line corresponds to the varying attention weights along the protein sequence, while the red dots represent the experimentally validated calcium-binding sites annotated in BioLip (<xref rid="btad094-B27" ref-type="bibr">Yang <italic toggle="yes">et al.</italic>, 2013</xref>). In <xref rid="btad094-F4" ref-type="fig">FigureÂ 4b</xref>, we provide another example where PFresGO correctly identified most sites of lactose operon repressor (PDB: 2PE5, Chain B) associated with DNA binding (GO: 0003677). We plotted the ROC curves for four examples of proteins with known functional residues to measure the consistency between important residues identified by PFresGO with the genuine protein functional residues annotated in BioLip in <xref rid="btad094-F4" ref-type="fig">FigureÂ 4c</xref>. Specifically, we calculated the attention weights of the proteins rat Î±-parvalbumin (PDB: 1S3P; Chain A), lactose operon repressor (PDB: 2PE5; Chain B), glutathione S-transferase (PDB: 2J9H; Chain A) and a putative cytochrome (PDB: 4RM4; Chain A) for the terms âcalcium ion bindingâ (GO: 0005509), âDNA bindingâ (GO: 0003677), âglutathione transferase activityâ (GO: 0004364) and âheme bindingâ (GO: 0020037), and compared them with the binary representation of function sites retrieved from BioLip. Despite the lack of functionally active sites or related information in the training process, the functional sites inferred by the attention weights and those within BioLip are highly correlated (<xref rid="btad094-F4" ref-type="fig">Fig.Â 4c</xref>).</p>
      <fig position="float" id="btad094-F4">
        <label>Fig. 4.</label>
        <caption>
          <p>PFresGO locates functional residues based on attention weights: (<bold>a</bold>) attention weights of rat Î±-parvalbumin (PDB: 1S3P, Chain A) with function calcium ion binding (GO: 0005509), the dots correspond to calcium-binding residues annotated in BioLip; (<bold>b</bold>) attention weights of lactose operon repressor (PDB: 2PE5, Chain B) with function DNA binding (GO: 0003677); (<bold>c</bold>) ROC curves of residues identified by attention weights and functional residues of protein examples retrieved from BioLip; and (<bold>d</bold>) an example of the percentage of attention on binding sites. The left, medium and right bars show the percentage of attention of every head in attention Layer 1, Layer 2 and the maximum percentage of each head, respectively</p>
        </caption>
        <graphic xlink:href="btad094f4" position="float"/>
      </fig>
      <p>To explore how the attention weights of every head align with known protein functional residues, we defined the following function to compute the percentage of high-confidence attentions that are indicative of protein functional residues:
<disp-formula id="E16"><label>(16)</label><mml:math id="M16" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mo>Î±</mml:mo></mml:mrow></mml:msub><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">A</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>Î±</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mo>Î¸</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">A</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>Î±</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mo>Î¸</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula id="IE71"><mml:math id="IM71" display="inline" overflow="scroll"><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> is an indicator function that returns 1 if the <italic toggle="yes">i</italic>th residue in the protein sequence <italic toggle="yes">X</italic> is annotated as a functional site in the BioLip database; otherwise returns 0, <inline-formula id="IE72"><mml:math id="IM72" display="inline" overflow="scroll"><mml:mo>Î¸</mml:mo></mml:math></inline-formula> (<inline-formula id="IE73"><mml:math id="IM73" display="inline" overflow="scroll"><mml:mo>Î¸</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>) represents a threshold used for filtering out the high-confidence residues, and <inline-formula id="IE74"><mml:math id="IM74" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">A</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>Î±</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mo>Î¸</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> indicates the attention weights of the high-confidence residues (<inline-formula id="IE75"><mml:math id="IM75" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mo>Î±</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mo>Î¸</mml:mo></mml:math></inline-formula>).</p>
      <p><xref rid="btad094-F4" ref-type="fig">FigureÂ 4d</xref> shows the proportion of attention weights for protein rat Î±-parvalbumin for two attention layers of PFresGO. The first head in attention layer 1 almost paid all of its attention to the functional residues and ignored other general residues. Further, the fifth head in attention Layer 1 and the eighth head in attention Layer 2 paid &gt; 80% of attention to functional residues. Please refer to the <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref> for the attention percentage analysis on other cases. Considering that functional sites of protein are often evolutionarily conserved to sustain function across the tree-of-life, our analysis demonstrates that PFresGO can accurately infer protein function at a residue level.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Discussion and conclusion</title>
    <p>In this study, we established PFresGOâan attention-based deep-learning method to tackle the multi-label protein function annotation challenge utilizing both the protein sequence information and hierarchical GO structures. PFresGO requires no information other than protein sequences for functional annotation, which is particularly convenient for newly identified proteins. Our evaluation of an independent test dataset showed that PFresGO achieved a superior prediction performance compared with current, âstate-of-the-artâ sequence-based methods, and importantly, structure-guided approaches for all GO categories, indicating that the incorporation of hierarchical structures of GO graphs for the prediction of protein functions is effective. Importantly, PFresGO functionally annotates proteins with no requirement for multiple sequence alignment (MSA) (<xref rid="btad094-B8" ref-type="bibr">Edgar and Batzoglou, 2006</xref>). Although MSA has been routinely used to support protein structure and functional modeling, the inference of protein homology through sequence alignment alone is not feasible on a genome-wide scale. Circumventing the computational bottleneck imposed by MSA, PFresGO annotates proteins by identifying sequence patterns and functional residues, and the findings here show that PFresGO consistently achieves confident annotation results.</p>
    <p>We demonstrated the effectiveness of integrating a pre-trained deep-learning language model and the hierarchical structure of GO terms for function annotation. On the other hand, there is a caveat when engaging the attention-based mechanism, the use of which can result in substantial memory consumption, which can further limit the number of GO terms that can be annotated; however, the annotation performance using GO terms of high specificity significantly benefits from the integration of structure information from GO graphs. Significantly, PFresGO can also infer functional sites in protein by assessing the attention weightings of individual amino acid residues. A case study showed that the distribution of attention weights along a protein sequence is readily interpretable in relation to functionally relevant amino acid residues or domains. We conducted a multiple attention analysis of the functions of select proteins and illustrated that important residues identified by PFresGO with high attention weights accord well experimental data in the BioLip database. Based on these findings, we expect that PFresGO will serve as a useful tool for the functional annotation of proteins and the identification of functional sites in proteins, which will be beneficial given the ever-expanding genomic, proteomic and transcriptomic datasets.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btad094_Supplementary_Data</label>
      <media xlink:href="btad094_supplementary_data.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <sec>
    <title>Funding</title>
    <p>This work was supported by Major Inter-Disciplinary Research (IDR) projects awarded by Monash University and a Grant from the International Joint Usage/Research Center, Institute of Medical Science, The University of Tokyo. A.W.P. is a National Health and Medical Research Council of Australia (NHMRC) Investigator Fellow [APP2016596].</p>
    <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
  </sec>
  <sec sec-type="data-availability">
    <title>Data availability</title>
    <p>All data underlying this work, including source code, is freely available at <ext-link xlink:href="https://github.com/BioColLab/PFresGO" ext-link-type="uri">https://github.com/BioColLab/PFresGO</ext-link>.</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btad094-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bairoch</surname><given-names>A.</given-names></string-name></person-group> (<year>2000</year>) <article-title>The ENZYME database in 2000</article-title>. <source>Nucleic Acids Res</source>., <volume>28</volume>, <fpage>304</fpage>â<lpage>305</lpage>.<pub-id pub-id-type="pmid">10592255</pub-id></mixed-citation>
    </ref>
    <ref id="btad094-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cai</surname><given-names>C.Z.</given-names></string-name></person-group><etal>et al</etal> (<year>2003</year>) <article-title>Protein function classification via support vector machine approach</article-title>. <source>Math. Biosci</source>., <volume>185</volume>, <fpage>111</fpage>â<lpage>122</lpage>.<pub-id pub-id-type="pmid">12941532</pub-id></mixed-citation>
    </ref>
    <ref id="btad094-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cao</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Shen</surname><given-names>Y.</given-names></string-name></person-group> (<year>2021</year>) <article-title>TALE: transformer-based protein function annotation with joint sequenceâlabel embedding</article-title>. <source>Bioinformatics</source>, <volume>37</volume>, <fpage>2825</fpage>â<lpage>2833</lpage>.<pub-id pub-id-type="pmid">33755048</pub-id></mixed-citation>
    </ref>
    <ref id="btad094-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>X.</given-names></string-name>, <string-name><surname>Ishwaran</surname><given-names>H.</given-names></string-name></person-group> (<year>2012</year>) <article-title>Random forests for genomic data analysis</article-title>. <source>Genomics</source>, <volume>99</volume>, <fpage>323</fpage>â<lpage>329</lpage>.<pub-id pub-id-type="pmid">22546560</pub-id></mixed-citation>
    </ref>
    <ref id="btad094-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Das</surname><given-names>S.</given-names></string-name></person-group><etal>et al</etal> (<year>2015</year>) <article-title>Functional classification of CATH superfamilies: a domain-based approach for protein function annotation</article-title>. <source>Bioinformatics</source>, <volume>31</volume>, <fpage>3460</fpage>â<lpage>3467</lpage>.<pub-id pub-id-type="pmid">26139634</pub-id></mixed-citation>
    </ref>
    <ref id="btad094-B151">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Day-Richter</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2007</year>) <article-title>OBO-Editâan ontology editor for biologists</article-title>. <source>Bioinformatics</source>, <volume>23</volume>, <fpage>2198</fpage>â<lpage>2200</lpage>.<pub-id pub-id-type="pmid">17545183</pub-id></mixed-citation>
    </ref>
    <ref id="btad094-B6">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Duong</surname><given-names>D.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) Annotating Gene Ontology terms for protein sequences with the Transformer model. bioRxiv.</mixed-citation>
    </ref>
    <ref id="btad094-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Edera</surname><given-names>A.A.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>Anc2vec: embedding gene ontology terms by preserving ancestors relationships</article-title>. <source>Brief. Bioinform</source>., <volume>23</volume>.</mixed-citation>
    </ref>
    <ref id="btad094-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Edgar</surname><given-names>R.C.</given-names></string-name>, <string-name><surname>Batzoglou</surname><given-names>S.</given-names></string-name></person-group> (<year>2006</year>) <article-title>Multiple sequence alignment</article-title>. <source>Curr. Opin. Struct. Biol</source>., <volume>16</volume>, <fpage>368</fpage>â<lpage>373</lpage>.<pub-id pub-id-type="pmid">16679011</pub-id></mixed-citation>
    </ref>
    <ref id="btad094-B9">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Elnaggar</surname><given-names>A.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) ProtTrans: towards cracking the language of lifeâs code through self-supervised learning. bioRxiv.</mixed-citation>
    </ref>
    <ref id="btad094-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fu</surname><given-names>L.</given-names></string-name></person-group><etal>et al</etal> (<year>2012</year>) <article-title>CD-HIT: accelerated for clustering the next-generation sequencing data</article-title>. <source>Bioinformatics</source>, <volume>28</volume>, <fpage>3150</fpage>â<lpage>3152</lpage>.<pub-id pub-id-type="pmid">23060610</pub-id></mixed-citation>
    </ref>
    <ref id="btad094-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gligorijevic</surname><given-names>V.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Structure-based protein function prediction using graph convolutional networks</article-title>. <source>Nat. Commun</source>., <volume>12</volume>, <fpage>3168</fpage>.<pub-id pub-id-type="pmid">34039967</pub-id></mixed-citation>
    </ref>
    <ref id="btad094-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hasin</surname><given-names>Y.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) <article-title>Multi-omics approaches to disease</article-title>. <source>Genome Biol</source>., <volume>18</volume>, <fpage>83</fpage>.<pub-id pub-id-type="pmid">28476144</pub-id></mixed-citation>
    </ref>
    <ref id="btad094-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kanehisa</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>KEGG: integrating viruses and cellular organisms</article-title>. <source>Nucleic Acids Res</source>., <volume>49</volume>, <fpage>D545</fpage>â<lpage>D551</lpage>.<pub-id pub-id-type="pmid">33125081</pub-id></mixed-citation>
    </ref>
    <ref id="btad094-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kulmanov</surname><given-names>M.</given-names></string-name>, <string-name><surname>Hoehndorf</surname><given-names>R.</given-names></string-name></person-group> (<year>2022</year>) <article-title>DeepGOZero: improving protein function prediction from sequence and zero-shot learning based on ontology axioms</article-title>. <source>Bioinformatics</source>, <volume>38</volume>, <fpage>i238</fpage>â<lpage>i245</lpage>.<pub-id pub-id-type="pmid">35758802</pub-id></mixed-citation>
    </ref>
    <ref id="btad094-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kulmanov</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>DeepGO: predicting protein functions from sequence and interactions using a deep ontology-aware classifier</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>660</fpage>â<lpage>668</lpage>.<pub-id pub-id-type="pmid">29028931</pub-id></mixed-citation>
    </ref>
    <ref id="btad094-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname><given-names>D.</given-names></string-name></person-group><etal>et al</etal> (<year>2007</year>) <article-title>Predicting protein function from sequence and structure</article-title>. <source>Nat. Rev. Mol. Cell Biol</source>., <volume>8</volume>, <fpage>995</fpage>â<lpage>1005</lpage>.<pub-id pub-id-type="pmid">18037900</pub-id></mixed-citation>
    </ref>
    <ref id="btad094-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lichtarge</surname><given-names>O.</given-names></string-name></person-group><etal>et al</etal> (<year>1996</year>) <article-title>An evolutionary trace method defines binding surfaces common to protein families</article-title>. <source>J. Mol. Biol</source>., <volume>257</volume>, <fpage>342</fpage>â<lpage>358</lpage>.<pub-id pub-id-type="pmid">8609628</pub-id></mixed-citation>
    </ref>
    <ref id="btad094-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Merino</surname><given-names>G.A.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>Hierarchical deep learning for predicting GO annotations by integrating protein knowledge</article-title>. <source>Bioinformatics</source>, <volume>38</volume>, <fpage>4488</fpage>â<lpage>4496</lpage>.<pub-id pub-id-type="pmid">35929781</pub-id></mixed-citation>
    </ref>
    <ref id="btad094-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ng</surname><given-names>A.</given-names></string-name></person-group> et al. (<year>2011</year>) Sparse Autoencoder<italic toggle="yes">.</italic>Â <italic toggle="yes">CS294A Lecture Notes</italic>, <volume>72</volume>, <fpage>1</fpage>â<lpage>19</lpage>.</mixed-citation>
    </ref>
    <ref id="btad094-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ouzounis</surname><given-names>C.A.</given-names></string-name></person-group><etal>et al</etal> (<year>2003</year>) <article-title>Classification schemes for protein structure and function</article-title>. <source>Nat. Rev. Genet</source>., <volume>4</volume>, <fpage>508</fpage>â<lpage>519</lpage>.<pub-id pub-id-type="pmid">12838343</pub-id></mixed-citation>
    </ref>
    <ref id="btad094-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sapoval</surname><given-names>N.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>Current progress and open challenges for applying deep learning across the biosciences</article-title>. <source>Nat. Commun</source>., <volume>13</volume>, <fpage>1728</fpage>.<pub-id pub-id-type="pmid">35365602</pub-id></mixed-citation>
    </ref>
    <ref id="btad094-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schaeffer</surname><given-names>R.D.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) <article-title>ECOD: new developments in the evolutionary classification of domains</article-title>. <source>Nucleic Acids Res</source>., <volume>45</volume>, <fpage>D296</fpage>â<lpage>D302</lpage>.<pub-id pub-id-type="pmid">27899594</pub-id></mixed-citation>
    </ref>
    <ref id="btad094-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sharma</surname><given-names>V.S.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>PCfun: a hybrid computational framework for systematic characterization of protein complex function</article-title>. <source>Brief. Bioinform</source>., <volume>23, bbac239</volume>.</mixed-citation>
    </ref>
    <ref id="btad094-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sureyya Rifaioglu</surname><given-names>A.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>DEEPred: automated protein function prediction with multi-task feed-forward deep neural networks</article-title>. <source>Sci. Rep</source>., <volume>9</volume>, <fpage>7344</fpage>.<pub-id pub-id-type="pmid">31089211</pub-id></mixed-citation>
    </ref>
    <ref id="btad094-B25">
      <mixed-citation publication-type="journal"><collab>The Gene Ontology Consortium</collab>. (<year>2008</year>) <article-title>The gene ontology project in 2008</article-title>. <source>Nucleic Acids Res</source>., <volume>36(Database issue)</volume>, <fpage>D440</fpage>â<lpage>D444</lpage>.<pub-id pub-id-type="pmid">17984083</pub-id></mixed-citation>
    </ref>
    <ref id="btad094-B26">
      <mixed-citation publication-type="journal"><collab>The UniProt Consortium</collab>. (<year>2021</year>) <article-title>UniProt: the universal protein knowledgebase in 2021</article-title>. <source>Nucleic Acids Res</source>., <volume>49</volume>, <fpage>D480</fpage>â<lpage>D489</lpage>.<pub-id pub-id-type="pmid">33237286</pub-id></mixed-citation>
    </ref>
    <ref id="btad094-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yang</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2013</year>) <article-title>BioLiP: a semi-manually curated database for biologically relevant ligand-protein interactions</article-title>. <source>Nucleic Acids Res</source>., <volume>41</volume>, <fpage>D1096</fpage>â<lpage>D1103</lpage>.<pub-id pub-id-type="pmid">23087378</pub-id></mixed-citation>
    </ref>
    <ref id="btad094-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ye</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2006</year>) <article-title>BLAST: improvements for better sequence analysis</article-title>. <source>Nucleic Acids Res</source>., <volume>34</volume>, <fpage>W6</fpage>â<lpage>W9</lpage>.<pub-id pub-id-type="pmid">16845079</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
