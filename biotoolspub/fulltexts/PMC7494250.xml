<?OLF?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?covid-19-tdm?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Neural Comput Appl</journal-id>
    <journal-id journal-id-type="iso-abbrev">Neural Comput Appl</journal-id>
    <journal-title-group>
      <journal-title>Neural Computing &amp; Applications</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">0941-0643</issn>
    <issn pub-type="epub">1433-3058</issn>
    <publisher>
      <publisher-name>Springer London</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7494250</article-id>
    <article-id pub-id-type="pmid">32958982</article-id>
    <article-id pub-id-type="publisher-id">5335</article-id>
    <article-id pub-id-type="doi">10.1007/s00521-020-05335-2</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>S.I.: Data Fusion in the era of Data Science</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>TermInformer: unsupervised term mining and analysis in biomedical literature</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2851-4260</contrib-id>
        <name>
          <surname>Tiwari</surname>
          <given-names>Prayag</given-names>
        </name>
        <address>
          <email>prayag.tiwari@unipd.it</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Uprety</surname>
          <given-names>Sagar</given-names>
        </name>
        <address>
          <email>sagar.uprety@open.ac.uk</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Dehdashti</surname>
          <given-names>Shahram</given-names>
        </name>
        <address>
          <email>shahram.dehdashti@qut.edu.au</email>
        </address>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5906-9422</contrib-id>
        <name>
          <surname>Hossain</surname>
          <given-names>M. Shamim</given-names>
        </name>
        <address>
          <email>mshossain@ksu.edu.sa</email>
        </address>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.5608.b</institution-id><institution-id institution-id-type="ISNI">0000 0004 1757 3470</institution-id><institution>Department of Information Engineering, </institution><institution>University of Padova, </institution></institution-wrap>Padua, Italy </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.10837.3d</institution-id><institution-id institution-id-type="ISNI">0000000096069301</institution-id><institution>The Open University, </institution></institution-wrap>London, UK </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.1024.7</institution-id><institution-id institution-id-type="ISNI">0000000089150953</institution-id><institution>School of Information Systems, Science and Engineering Faculty, </institution><institution>Queensland University of Technology, </institution></institution-wrap>Brisbane, Australia </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.56302.32</institution-id><institution-id institution-id-type="ISNI">0000 0004 1773 5396</institution-id><institution>Department of Software Engineering, </institution><institution>College of Computer and Information Sciences, King Saud University, </institution></institution-wrap>Riyadh, 11543 Saudi Arabia </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>16</day>
      <month>9</month>
      <year>2020</year>
    </pub-date>
    <fpage>1</fpage>
    <lpage>14</lpage>
    <history>
      <date date-type="received">
        <day>17</day>
        <month>6</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>2</day>
        <month>9</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© Springer-Verlag London Ltd., part of Springer Nature 2020</copyright-statement>
      <license>
        <license-p>This article is made available via the PMC Open Access Subset for unrestricted research re-use and secondary analysis in any form or by any means with acknowledgement of the original source. These permissions are granted for the duration of the World Health Organization (WHO) declaration of COVID-19 as a global pandemic.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Terminology is the most basic information that researchers and literature analysis systems need to understand. Mining terms and revealing the semantic relationships between terms can help biomedical researchers find solutions to some major health problems and motivate researchers to explore innovative biomedical research issues. However, how to mine terms from biomedical literature remains a challenge. At present, the research on text segmentation in natural language processing (NLP) technology has not been well applied in the biomedical field. Named entity recognition models usually require a large amount of training corpus, and the types of entities that the model can recognize are limited. Besides, dictionary-based methods mainly use pre-established vocabularies to match the text. However, this method can only match terms in a specific field, and the process of collecting terms is time-consuming and labour-intensive. Many scenarios faced in the field of biomedical research are unsupervised, i.e. unlabelled corpora, and the system may not have much prior knowledge. This paper proposes the TermInformer project, which aims to mine the meaning of terms in an open fashion by calculating terms and find solutions to some of the significant problems in our society. We propose an unsupervised method that can automatically mine terms in the text without relying on external resources. Our method can generally be applied to any document data. Combined with the word vector training algorithm, we can obtain reusable term embeddings, which can be used in any NLP downstream application. This paper compares term embeddings with existing word embeddings. The results show that our method can better reflect the semantic relationship between terms. Finally, we use the proposed method to find potential factors and treatments for lung cancer, breast cancer, and coronavirus.</p>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Term mining</kwd>
      <kwd>Unsupervised learning</kwd>
      <kwd>Term embeddings</kwd>
      <kwd>Sequence labelling</kwd>
      <kwd>GloVe</kwd>
      <kwd>Biomedical literature</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">Term mining aims to mine terms from unstructured documents. Terminology is usually composed of multiple words and describes concepts in a particular domain that forms the basis of the domain. Unsupervised term mining aims to use automated algorithms to mine terms in the literature without relying on external resources and human intervention. Therefore, the method has a wide range of applications and can be used to process text data in any field. Existing research usually uses dictionary-based methods and supervised machine learning methods to mine terms in the literature. However, the dictionary-based method’s limitation is that the method can only match terms in a specific field, and terms in the vocabulary are often different from expressions in the literature. For example, there are more than 5 ways in the literature to mention the disease “ischemic stroke”. Terms in the literature contain many variants, so dictionary-based methods require constant vocabulary maintenance, which is costly and time-consuming.</p>
    <p id="Par3">The named entity recognition method has achieved good results in terms of recognition. However, the named entity recognition model requires a large number of labelled corpora for training and can only identify predefined types of terms, which is not suitable for this open term mining problem. Corpus annotation in the biomedical field is very challenging because of the high requirements for the knowledge background of the annotators, and the labelling process relies heavily on the knowledge of domain experts and annotation standards. Therefore, the method of manually labelling the term corpus for named entity recognition is very time-consuming and costly. In addition, using deep learning models to identify terms can also lead to increased computation costs. Based on these problems, this paper proposes an unsupervised term mining method. This method can automatically mine terms from the literature without the need to annotate the corpus manually, so it can work with word embedding training algorithms to produce term-level embeddings, not just word embeddings. We propose a multi-length term mining algorithm. Using this algorithm, we can fully mine terms from the text without requiring any external resources.</p>
    <p id="Par4">Existing term representation methods usually represent a term by average word embeddings [<xref ref-type="bibr" rid="CR1">1</xref>]. However, this method cannot obtain the semantics of the term. This method only represents the term by each compound word. There are essential differences between terms and words, so this poses a limitation of term research. Faced with this problem, we apply the proposed method to train term embeddings. Based on the existing word embedding algorithm GloVe [<xref ref-type="bibr" rid="CR2">2</xref>], we trained the mined terms and found that the term representation can better represent the semantic relationship between terms. To evaluate the performance of the method, we created two datasets of different sizes. We compared the term representation composed of the original word embeddings with the term representation obtained based on our method. We observe that our method improves the representation of terms, can better characterize the relationships between terms, and better calculate the similarity between terms. This method can potentially be applied to any biomedical text mining system. The proposed method can also be used to build a term correlation graph. Finally, we explore the factors and treatments for lung and breast cancer using the proposed method. The results show that our method can find some key information for these diseases from the literature.</p>
    <sec id="Sec2">
      <title>Contribution</title>
      <p id="Par5">The main contributions of this paper can be summarized below: <list list-type="order"><list-item><p id="Par6">We propose an unsupervised term mining algorithm. The proposed method can be applied to any biomedical corpus, and the process can mine terms without manual annotation and can be used with word embedding training algorithms, which may become a scheme for solving term representation problems.</p></list-item><list-item><p id="Par7">The proposed method improves the existing term representation based on word embeddings, and the obtained term embeddings can better characterize the relationship between terms.</p></list-item><list-item><p id="Par8">This paper creates two-term mining datasets to evaluate the performance of the method.</p></list-item></list></p>
    </sec>
    <sec id="Sec3">
      <title>Organization</title>
      <p id="Par9">The rest of the paper is organized as follows. Related works are discussed in Sect. <xref rid="Sec4" ref-type="sec">2</xref>. Section <xref rid="Sec5" ref-type="sec">3</xref> describes the method proposed in this paper, followed by the proposed algorithm. Experiment results are explained in Sect. <xref rid="Sec11" ref-type="sec">4</xref> consisting of experimental results, including dataset description, evaluation metric, analysis of term similarity, analysis of term relationship. Finally, we discuss the conclusion and possible future work in Sect. <xref rid="Sec17" ref-type="sec">5</xref>.</p>
    </sec>
  </sec>
  <sec id="Sec4">
    <title>Related work</title>
    <p id="Par10">Named entity recognition models are widely used for recognizing biomedical terms. Settles et al. [<xref ref-type="bibr" rid="CR3">3</xref>] use conditional random fields (CRF) [<xref ref-type="bibr" rid="CR4">4</xref>] to recognize the gene and protein mentions in biomedical abstracts. Leaman et al. [<xref ref-type="bibr" rid="CR5">5</xref>] propose the BANNER framework to recognize biomedical entities, which aims to improve the generalization ability for this task. Habibi et al. [<xref ref-type="bibr" rid="CR6">6</xref>] use the LSTM-CRF model to recognize genes, chemicals, and diseases mentions. Tang et al. [<xref ref-type="bibr" rid="CR7">7</xref>] study three different types of word representation methods and analyze their performance for biomedical NER on JNLPBA and BioCreAtIvE II BNER tasks. Yao et al. [<xref ref-type="bibr" rid="CR8">8</xref>] propose a deep learning model that consists of multiple CNN layers and achieves improvement on the GENIA dataset. Wang et al. [<xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR10">10</xref>] propose a multitask learning approach to recognize biomedical entities by using training data collectively consisting of distinct types of entities. Yoon et al. [<xref ref-type="bibr" rid="CR11">11</xref>] propose CollaboNet to resolve the issue due to lack of data and entity-type misclassification by using the integration of multiple NER frameworks. Cho et al. [<xref ref-type="bibr" rid="CR12">12</xref>] use BiLSTM and CRF to propose contextual LSTM networks with CRF to capture all the contextual information.</p>
    <p id="Par11">Lafferty et al. [<xref ref-type="bibr" rid="CR13">13</xref>] use conditional random fields (CRF) to build probabilistic models for sequence labelling problem. Nadeau et al. [<xref ref-type="bibr" rid="CR14">14</xref>] investigate the feature engineering-based NER models and systems. Collobert et al. [<xref ref-type="bibr" rid="CR15">15</xref>] use CNN to solve may NLP tasks. Lample et al. [<xref ref-type="bibr" rid="CR16">16</xref>] adopt LSTM-CRF to solve the sequence labelling problem. Chiu et al. [<xref ref-type="bibr" rid="CR17">17</xref>] propose to use bidirectional LSTM-CNNs to resolve the sequence labelling problem. Ma et al. [<xref ref-type="bibr" rid="CR18">18</xref>] use lstm-cnns-crf model to recognize entities. Akbik et al. [<xref ref-type="bibr" rid="CR19">19</xref>] propose to use character-level language modelling to improve performance.</p>
    <p id="Par12">Pre-trained language models, such as ELMo [<xref ref-type="bibr" rid="CR20">20</xref>] and BERT [<xref ref-type="bibr" rid="CR21">21</xref>], have also been applied in the clinical NLP field. Beltagy et al. [<xref ref-type="bibr" rid="CR22">22</xref>] train the SciBERT to enhance downstream NLP tasks. Alsentzer et al. [<xref ref-type="bibr" rid="CR23">23</xref>] train the BERT model based on both clinical notes and discharge summaries. Lee et al. [<xref ref-type="bibr" rid="CR24">24</xref>] proposed BioBERT, a model that retrains BERT on PubMed and PMC corpora, which can improve the results of downstream BioBLP tasks. These studies focus on word-level representations without considering the term representation. Context-dependent representations make the same word have different representations in different sentences. However, this paper aims to obtain context-independent representations, so we do not adopt these methods.</p>
  </sec>
  <sec id="Sec5">
    <title>Methods</title>
    <sec id="Sec6">
      <title>Multi-length term mining algorithm</title>
      <p id="Par13">This section explains this algorithm. After statistical analysis, we found that terms composed of 2, 3, 4, and 5 words are the more common, so we mainly mine terms of the above length. Word vectors can directly represent terms with only one word. The input of the algorithm is the original text, and the output is the mined terms. We do not need to use any external resources to apply it to any corpus in this process. We first perform word segmentation and part-of-speech tagging on the original text and then mine the terms.</p>
      <p id="Par14">As shown in Algorithm 1, the first line of the algorithm is mainly to initialize 4 dictionaries for storing terms of different lengths and input the corpora. The method then starts processing each sample, that is, each article. The fourth line performs word segmentation for these articles. Word segmentation is to divide these articles into a sequence of words and identify the punctuations, which can prevent the words and punctuations from being connected, resulting in an irregular vocabulary and inaccurate words.<graphic position="anchor" xlink:href="521_2020_5335_Figa_HTML" id="MO180"/></p>
      <p id="Par15">The fifth line is mainly used for part-of-speech (POS) tagging of words. We adopt the LSTM-CRF neural network for POS tagging. The detailed mechanism of this model will be introduced in Sect. <xref rid="Sec7" ref-type="sec">3.2</xref>. The POS features help in improving the term mining in the algorithm. Lines 6 to 9 are used to mine terms of different lengths. We mainly focus on terms composed of 2, 3, 4, and 5 words.</p>
      <p id="Par16">The term extraction algorithm is described in Algorithm 2. The second line splits the input word sequence into phrases of a certain length. The third line is to extract the POS tags of the phrase from the recognized POS tag sequence. Inline 4, this algorithm matches the POS of the target phrase with our predefined rule. This rule considers the extraction of medical terms where the first two words are adjective, and noun, respectively, and the last term is also a noun. If the phrase matches successfully, the phrase will be treated as a potential term and added to the dictionary. Then, this algorithm counts how often the term appears. If the term has appeared in the dictionary, increase the term frequency by 1. If the term does not appear in the dictionary, the term has a frequency of 1. The purpose of counting term frequency is to extract meaningful terms. By setting thresholds, we have the flexibility to mine a certain number of terms. Terms are often repeated in the literature, and if a phrase appears only once, we do not consider it as a term. If we set a higher threshold, it means that we can get more confident terms, which also demonstrates that these terms are more common.<graphic position="anchor" xlink:href="521_2020_5335_Figb_HTML" id="MO200"/></p>
      <p id="Par17">In the following, we briefly discuss the asymptotic complexity of our approach. The algorithm describes the processing of a large number of samples, where the number of samples is related to the corpus’s size. Therefore, we analyze the term mining process for only one sample. The algorithm first performs word segmentation and then performs part-of-speech tagging and then calls Algorithm 2. Since the complexity of Algorithm 2 is <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathcal {O}(n)$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mi mathvariant="script">O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="521_2020_5335_Article_IEq1.gif"/></alternatives></inline-formula>, the complexity of Algorithm 1 mainly depends on the two steps of word segmentation and part-of-speech tagging. Therefore, our algorithm is approximately equal to the complexity of segmentation and part-of-speech tagging. The proposed method does not significantly increase computational costs.</p>
    </sec>
    <sec id="Sec7">
      <title>LSTM-CRF</title>
      <p id="Par18">This subsection introduces the LSTM-CRF sequence labelling model for POS tagging, as shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. This model does not depend on feature engineering; instead, it adopts the words and characters as input. This design can increase the generality for processing any dataset. Then, textual input is projected to the word embeddings, which is a way to encode the prior knowledge of semantics into a dense vector representation [<xref ref-type="bibr" rid="CR25">25</xref>–<xref ref-type="bibr" rid="CR27">27</xref>].<fig id="Fig1"><label>Fig. 1</label><caption><p>LSTM-CRF network overview</p></caption><graphic xlink:href="521_2020_5335_Fig1_HTML" id="MO3"/></fig></p>
      <sec id="Sec8">
        <title>Long short-term memory</title>
        <p id="Par19">The long short-term memory (LSTM) network is a kind of recurrent neural network (RNN). It uses the LSTM unit [<xref ref-type="bibr" rid="CR28">28</xref>] to solve the exploding and vanishing gradient problems encountered in the traditional RNNs. The formulations of LSTM unit are as follows.<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} i_t&amp;=\sigma (W_i h_{t-1}+U_i x_t +b_i) \end{aligned}$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="521_2020_5335_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} f_t&amp;=\sigma (W_f h_{t-1}+U_f x_t +b_f) \end{aligned}$$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="521_2020_5335_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \tilde{c_t}&amp;=\tanh (W_c h_{t-1}+U_c x_t +b_c) \end{aligned}$$\end{document}</tex-math><mml:math id="M8" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mover accent="true"><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mo>tanh</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="521_2020_5335_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} c_t&amp;=f_t \odot c_{t-1}+i_t \odot \tilde{c_t} \end{aligned}$$\end{document}</tex-math><mml:math id="M10" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>⊙</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="521_2020_5335_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} o_t&amp;=\sigma (W_o h_{t-1}+U_o x_t +b_o) \end{aligned}$$\end{document}</tex-math><mml:math id="M12" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="521_2020_5335_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} h_t&amp;=o_t \odot tanh(c_t) \end{aligned}$$\end{document}</tex-math><mml:math id="M14" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>⊙</mml:mo><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="521_2020_5335_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq2"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma (\cdot )$$\end{document}</tex-math><mml:math id="M16"><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="521_2020_5335_Article_IEq2.gif"/></alternatives></inline-formula> is a sigmoid activation function. <inline-formula id="IEq3"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_t$$\end{document}</tex-math><mml:math id="M18"><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2020_5335_Article_IEq3.gif"/></alternatives></inline-formula> represents the input vector at time step t, and <inline-formula id="IEq4"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_t$$\end{document}</tex-math><mml:math id="M20"><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2020_5335_Article_IEq4.gif"/></alternatives></inline-formula> denotes the hidden state containing the context information in the former time steps. <italic>W</italic> and <italic>b</italic> are weight and bias parameters. <inline-formula id="IEq5"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i_t$$\end{document}</tex-math><mml:math id="M22"><mml:msub><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2020_5335_Article_IEq5.gif"/></alternatives></inline-formula>, <inline-formula id="IEq6"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f_t$$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2020_5335_Article_IEq6.gif"/></alternatives></inline-formula>, <inline-formula id="IEq7"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c_t$$\end{document}</tex-math><mml:math id="M26"><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2020_5335_Article_IEq7.gif"/></alternatives></inline-formula> and <inline-formula id="IEq8"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$o_t$$\end{document}</tex-math><mml:math id="M28"><mml:msub><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2020_5335_Article_IEq8.gif"/></alternatives></inline-formula> are the input gate vector, forget gate vector, cell state, and output gate vector, respectively. However, the hidden state in a forward LSTM can only capture the context information in the left side of current step [<xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR18">18</xref>]. Bidirectional LSTM (Bi-LSTM) has an operation to reverse the order of input sequence and concatenate the hidden state in each time step, which can capture the context information from the left and right side of the current step.</p>
      </sec>
      <sec id="Sec9">
        <title>Conditional random field</title>
        <p id="Par20">The conditional random field (CRF) layer performs the label sequence prediction [<xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR18">18</xref>]. This model can be more effective than classifying each word independently because the word label is determined not only by itself but also by the context.<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} p(y|z;W,b)&amp;=\frac{\prod _{i=1}^{n}\exp (W_{y_{i-1}y_i}^Tz_i+b_{y_{i-1}y_i})}{\sum _{y'\in Y(z)}\prod _{i=1}^{n}\exp (W_{y'_{i-1}y'_i}^Tz_i+b_{y'_{i-1}y'_i})} \end{aligned}$$\end{document}</tex-math><mml:math id="M30" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>z</mml:mi><mml:mo>;</mml:mo><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mo>exp</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:mi>Y</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mo>exp</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="521_2020_5335_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} L(W,b)&amp;=\sum \limits _{i}\log p(y|z;W,b) \end{aligned}$$\end{document}</tex-math><mml:math id="M32" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mo>log</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>z</mml:mi><mml:mo>;</mml:mo><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="521_2020_5335_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} y^*&amp;=\arg \max \limits _{y\in Y(z)}p(y|z;W,b) \end{aligned}$$\end{document}</tex-math><mml:math id="M34" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mi>y</mml:mi><mml:mo>∗</mml:mo></mml:msup></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mo>arg</mml:mo><mml:munder><mml:mo movablelimits="false">max</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>z</mml:mi><mml:mo>;</mml:mo><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="521_2020_5335_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq9"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\{[z_i,y_i]\}, i=1,2...n$$\end{document}</tex-math><mml:math id="M36"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo stretchy="false">}</mml:mo><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="521_2020_5335_Article_IEq9.gif"/></alternatives></inline-formula> denotes a set of words <italic>z</italic> with a label sequence <italic>y</italic>. <italic>W</italic> and <italic>b</italic> are weight and bias parameters. <italic>p</italic>(<italic>y</italic>|<italic>z</italic>; <italic>W</italic>, <italic>b</italic>) is the probability of label sequence <italic>y</italic> over all possible sequences <italic>Y</italic>(<italic>z</italic>) on the input <italic>z</italic>. In training stage, the objective is to maximize the log-likelihood <italic>L</italic>(<italic>W</italic>, <italic>b</italic>). In prediction, the decoder will find the optimal label sequence as Eq. <xref rid="Equ9" ref-type="">9</xref> by Viterbi algorithm [<xref ref-type="bibr" rid="CR29">29</xref>].</p>
      </sec>
    </sec>
    <sec id="Sec10">
      <title>Term embedding</title>
      <p id="Par21">Word vector approaches aim to project a large vector space of words into a much lower-dimensional space and generate dense representations. It has made significant contributions to enhance various NLP techniques and has been widely used in various downstream NLP tasks like sentiment analysis, document classification, etc., to achieve improved results. The word representations are computable and have actively promoted the development of deep learning NLP. Existing word vector training methods mainly embed words into a fixed-length vector, but cannot be directly used to learn term vector in sentences. For terms, the word embeddings are not enough to represent the overall meaning of the term. Existing methods have limitations on the problem of term representation.</p>
      <p id="Par22">Existing methods usually use average or max pooling of all word vectors to represent a multi-word term. The problem is that the term representation obtained by this method will make the term most similar to each composed word. This method cannot reflect the relationship between different terms, but only the relationship between words contained in the term. This method belongs to a word-level learning method and cannot be used in the term level. Based on the above problems, we use the multi-length term mining algorithm proposed in Sect. <xref rid="Sec6" ref-type="sec">3.1</xref> together with the word vector learning algorithm to fundamentally alleviate the problem of term representation and learn term embeddings. We use the GloVe algorithm to train term embeddings. However, the major difference is that our vocabulary consists of terms mined using the algorithms described above. This enables the GloVe algorithm to learn vectors for terms rather than individual words. For example, it will learn one single vector for the compound term “lung cancer”. GloVe will learn vectors for the two terms “lung” and “cancer”, and then one has to average them to obtain the vector for “lung cancer”. The training objective for GloVe embeddings is shown in equation (<xref rid="Equ10" ref-type="">10</xref>).<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} J=\sum _{i,j=1}^{V}f(X_{i,j})(w_i^T\tilde{w}_j+b_i+\tilde{b}_j-\log {X_{ij}})^2 \end{aligned}$$\end{document}</tex-math><mml:math id="M38" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>J</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>V</mml:mi></mml:munderover><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>b</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mo>log</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="521_2020_5335_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq10"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X_{ij}$$\end{document}</tex-math><mml:math id="M40"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="521_2020_5335_Article_IEq10.gif"/></alternatives></inline-formula> represents the co-occurrence frequency of the word<inline-formula id="IEq11"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{i}$$\end{document}</tex-math><mml:math id="M42"><mml:msub><mml:mrow/><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2020_5335_Article_IEq11.gif"/></alternatives></inline-formula> and word<inline-formula id="IEq12"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{j}$$\end{document}</tex-math><mml:math id="M44"><mml:msub><mml:mrow/><mml:mi>j</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2020_5335_Article_IEq12.gif"/></alternatives></inline-formula>. <inline-formula id="IEq13"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$w_i$$\end{document}</tex-math><mml:math id="M46"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2020_5335_Article_IEq13.gif"/></alternatives></inline-formula> and <inline-formula id="IEq14"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$w_j$$\end{document}</tex-math><mml:math id="M48"><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2020_5335_Article_IEq14.gif"/></alternatives></inline-formula> denote vector representations of the word<inline-formula id="IEq15"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{i}$$\end{document}</tex-math><mml:math id="M50"><mml:msub><mml:mrow/><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2020_5335_Article_IEq15.gif"/></alternatives></inline-formula> and word<inline-formula id="IEq16"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{j}$$\end{document}</tex-math><mml:math id="M52"><mml:msub><mml:mrow/><mml:mi>j</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2020_5335_Article_IEq16.gif"/></alternatives></inline-formula>, respectively. <inline-formula id="IEq17"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$b_i$$\end{document}</tex-math><mml:math id="M54"><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2020_5335_Article_IEq17.gif"/></alternatives></inline-formula> and <inline-formula id="IEq18"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$b_j$$\end{document}</tex-math><mml:math id="M56"><mml:msub><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2020_5335_Article_IEq18.gif"/></alternatives></inline-formula> are the bias value for the word<inline-formula id="IEq19"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{i}$$\end{document}</tex-math><mml:math id="M58"><mml:msub><mml:mrow/><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2020_5335_Article_IEq19.gif"/></alternatives></inline-formula> and word<inline-formula id="IEq20"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{j}$$\end{document}</tex-math><mml:math id="M60"><mml:msub><mml:mrow/><mml:mi>j</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2020_5335_Article_IEq20.gif"/></alternatives></inline-formula>, respectively. <inline-formula id="IEq21"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f(X_{i,j})$$\end{document}</tex-math><mml:math id="M62"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="521_2020_5335_Article_IEq21.gif"/></alternatives></inline-formula> is the weighting factor defined in equation (<xref rid="Equ11" ref-type="">11</xref>).<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} f(x)={\left\{ \begin{array}{ll}(x/x_{\max })^\alpha ,&amp;{} {\text{ if } }\; x&lt;x_{\max } \\ 1,&amp;{} {\text{ otherwise }} \end{array}\right. } \end{aligned}$$\end{document}</tex-math><mml:math id="M64" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mo movablelimits="true">max</mml:mo></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>α</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>if</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow><mml:mspace width="0.277778em"/><mml:mi>x</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mo movablelimits="true">max</mml:mo></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>otherwise</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="521_2020_5335_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq22"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_{\max }$$\end{document}</tex-math><mml:math id="M66"><mml:msub><mml:mi>x</mml:mi><mml:mo movablelimits="true">max</mml:mo></mml:msub></mml:math><inline-graphic xlink:href="521_2020_5335_Article_IEq22.gif"/></alternatives></inline-formula> and <inline-formula id="IEq23"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><mml:math id="M68"><mml:mi>α</mml:mi></mml:math><inline-graphic xlink:href="521_2020_5335_Article_IEq23.gif"/></alternatives></inline-formula> are hyperparameters. These two parameters are set to <inline-formula id="IEq24"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_{\max }=100$$\end{document}</tex-math><mml:math id="M70"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mo movablelimits="true">max</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="521_2020_5335_Article_IEq24.gif"/></alternatives></inline-formula> and <inline-formula id="IEq25"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha =0.75$$\end{document}</tex-math><mml:math id="M72"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.75</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="521_2020_5335_Article_IEq25.gif"/></alternatives></inline-formula>.</p>
    </sec>
  </sec>
  <sec id="Sec11">
    <title>Results</title>
    <p id="Par23">In this section, we conduct experiments on two datasets. We first introduce the datasets, evaluation methods. Then, we compare the mined term-based embeddings with the word embedding-based method. Finally, we analyze the experimental results.</p>
    <sec id="Sec12">
      <title>Experimental settings</title>
      <sec id="Sec13">
        <title>Dataset</title>
        <p id="Par24"><bold>PubMed-10K</bold> We randomly sampled 10k abstracts from PubMed. This dataset contains 91k sentences. We mine more than 42k potential terms from this dataset. The term statistics can be found in the first row in Table <xref rid="Tab1" ref-type="table">1</xref>.</p>
        <p id="Par25"><bold>PubMed-100K</bold> We randomly sampled 100k abstracts from PubMed. This dataset contains 0.94M sentences, which is larger than the first dataset, so we can compare the performance on different data scales, and we can mine 0.35M potential terms from this dataset. The term statistics can be found in the second row in Table <xref rid="Tab1" ref-type="table">1</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Number of terms mined on different datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Term length</th><th align="left">2 words</th><th align="left">3 words</th><th align="left">4 words</th><th align="left">5 words</th><th align="left">Total</th></tr></thead><tbody><tr><td align="left">PubMed-10K</td><td char="." align="char">34,362</td><td char="." align="char">7304</td><td char="." align="char">1090</td><td char="." align="char">129</td><td char="." align="char">42,885</td></tr><tr><td align="left">PubMed-100K</td><td char="." align="char">268,890</td><td char="." align="char">72,612</td><td char="." align="char">11,267</td><td char="." align="char">1,545</td><td char="." align="char">354,314</td></tr></tbody></table></table-wrap></p>
      </sec>
    </sec>
    <sec id="Sec14">
      <title>Evaluation</title>
      <p id="Par26">We mainly analyze the mined terms and their semantic representation capacity through manual evaluation and visualization. We use cosine similarity to calculate similar terms for each term, thereby reflecting the learned term embeddings’ performance.</p>
    </sec>
    <sec id="Sec15">
      <title>Analysis of term similarity</title>
      <p id="Par27">We compared with the word vector-based method, as shown in Figs. <xref rid="Fig2" ref-type="fig">2</xref>, <xref rid="Fig3" ref-type="fig">3</xref>, <xref rid="Fig4" ref-type="fig">4</xref> and <xref rid="Fig5" ref-type="fig">5</xref>. For the baseline method, we used the most common way to represent a term. That is, for each word contained in the term, we averaged their embeddings to represent the term. Different from this method, the proposed method directly learns the term embeddings. <fig id="Fig2"><label>Fig. 2</label><caption><p>Visualization of term embeddings where each term consists of 2 words, and <bold>a</bold> and <bold>b</bold> are based on our method and word embeddings</p></caption><graphic xlink:href="521_2020_5335_Fig2_HTML" id="MO15"/></fig><fig id="Fig3"><label>Fig. 3</label><caption><p>Visualization of term embeddings where each term consists of 5 words, and <bold>a</bold> and <bold>b</bold> are based on our method and word embeddings</p></caption><graphic xlink:href="521_2020_5335_Fig3_HTML" id="MO16"/></fig><fig id="Fig4"><label>Fig. 4</label><caption><p>Visualization of term embeddings where each term consists of 3 words, and <bold>a</bold> and <bold>b</bold> are based on our method and word embeddings</p></caption><graphic xlink:href="521_2020_5335_Fig4_HTML" id="MO17"/></fig><fig id="Fig5"><label>Fig. 5</label><caption><p>Visualization of term embeddings where each term consists of 4 words, and <bold>a</bold> and <bold>b</bold> are based on our method and word embeddings</p></caption><graphic xlink:href="521_2020_5335_Fig5_HTML" id="MO18"/></fig></p>
      <p id="Par28">As shown in Figs <xref rid="Fig6" ref-type="fig">6</xref> and <xref rid="Fig7" ref-type="fig">7</xref>, the first column is our method, and the second column is the baseline method. We observe the proposed method can find closely related terms. The word vector-based method finds mainly insignificant words or phrases related to only one word of the term. This limitation is because the word-vector-based method can only find other words similar to a word in this phrase, and the computing process is to maintain the semantics of each word instead of the entire phrase. Each word that makes up a term is used to represent the term, so the most similar representation is usually a word in the term. However, this kind of information does not generate much value, so we removed the words contained in the term. Unlike word vector-based methods, our models can find abbreviations of some terms.</p>
      <p id="Par29">As shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>, we observe that “chronic obstructive pulmonary disease” is closely related to the abbreviation “copd”. “liquid chromatography-tandem mass spectrometry” is closely related to the abbreviation “ls-ms”. As shown in Fig. <xref rid="Fig7" ref-type="fig">7</xref>, “toll-like receptor” is closely related to different types of “tlr”. For “human immunodeficiency virus”, we find the abbreviation “hiv”. Abbreviations can be found for almost all the terms based on our method, while the baseline method does not find abbreviations. This shows that our model has achieved better results in expressing the true semantics of terms, and we have performed experiments on two datasets, respectively. We found that when the dataset is larger, there are more term names contained in the dataset. Due to many candidate terms, each term is more likely to find related terms. When a corpus contains fewer samples, this corpus also contains fewer terms, so each term may not find similar terms. However, some phrases related to the term can be founded. It can be seen that our algorithm achieves better results on corpora of different sizes.<fig id="Fig6"><label>Fig. 6</label><caption><p>The most closely related terms base on the PubMed-10k dataset</p></caption><graphic xlink:href="521_2020_5335_Fig6_HTML" id="MO19"/></fig><fig id="Fig7"><label>Fig. 7</label><caption><p>The most closely related terms base on the PubMed-100k dataset</p></caption><graphic xlink:href="521_2020_5335_Fig7_HTML" id="MO20"/></fig></p>
    </sec>
    <sec id="Sec16">
      <title>Analysis of term relationship</title>
      <p id="Par30">In this subsection, we analyze the difference between terms of various lengths for learning term embeddings. The baseline is the most commonly used term representation method based on word vectors. We apply principal component analysis (PCA) [<xref ref-type="bibr" rid="CR30">30</xref>] to reduce the dimension of the learned term, which is convenient for a visualization based on low dimensions to observe and measure the semantic similarity between terms. As shown in Figs. <xref rid="Fig8" ref-type="fig">8</xref> and <xref rid="Fig9" ref-type="fig">9</xref>, we found that term embeddings learned by our method can better reflect the relationship between terms. For example, disease-related terms are relatively close, but the word vector-based term representation does not reveal the semantic relationship between terms well. The baseline method mainly retains the words’ similarity, so this method cannot get the term similarity very well. We further find that the longer the term, the less accurate the term relationship based on the word vector method, and the more obvious the need for term embedding. So the proposed method can learn a reasonable representation for the longer term.<fig id="Fig8"><label>Fig. 8</label><caption><p>Visualization of cancer and <bold>a</bold> and <bold>b</bold> are based on our method and word embeddings</p></caption><graphic xlink:href="521_2020_5335_Fig8_HTML" id="MO21"/></fig><fig id="Fig9"><label>Fig. 9</label><caption><p>Visualization of lung disease and <bold>a</bold> and <bold>b</bold> are based on our method and word embeddings</p></caption><graphic xlink:href="521_2020_5335_Fig9_HTML" id="MO22"/></fig></p>
      <p id="Par31">We analyze the performance of the proposed method on different types of terms. As shown in Fig. <xref rid="Fig8" ref-type="fig">8</xref>, for cancer terms, our method can achieve a more accurate semantic distribution, so that semantically related terms are closer. For example, we found “non-small cell lung cancer” is closely related to “colorectal cancer”. There is potential relation between “epithelial ovarian cancer” and “neck cancer”, “early breast cancer” and “head-and-neck cancer”. This characteristic helps find related treatment schemes from other terms. However, the word vector-based method makes the terms mixed.</p>
      <p id="Par32">Figure <xref rid="Fig10" ref-type="fig">10</xref> shows the terms related to chronic diseases. Our method brings together similar diseases. We find some potential links between chronic diseases, such as “chronic pancreatitis” and “chronic heart failure”, “chronic rhinosinusitis” and “chronic lymphocytic leukemia”. The baseline method does not generate this effect, so our method’s advantage is that it can compare similar chronic diseases to find treatment options.<fig id="Fig10"><label>Fig. 10</label><caption><p>Visualization of chronic diseases and <bold>a</bold> and <bold>b</bold> are based on our method and word embeddings</p></caption><graphic xlink:href="521_2020_5335_Fig10_HTML" id="MO23"/></fig></p>
      <p id="Par33">Terms related to drugs and treatments are shown in Fig. <xref rid="Fig11" ref-type="fig">11</xref>. Our method can put related drugs and treatment methods together, which helps medical researchers choose the corresponding treatment plan and recommend more treatment plans. We observe “neoadjuvant chemotherapy” and “cancer immunotherapy” are closely related. The baseline method has no obvious semantic characteristics.<fig id="Fig11"><label>Fig. 11</label><caption><p>Visualization of drug and therapy, and <bold>a</bold> and <bold>b</bold> are based on our method and word embeddings</p></caption><graphic xlink:href="521_2020_5335_Fig11_HTML" id="MO24"/></fig></p>
      <p id="Par34">Figure <xref rid="Fig9" ref-type="fig">9</xref> shows the terms related to lung diseases. Our method reveals the relationship between lung diseases. The baseline method does not reveal this relationship. Therefore, our method can be further used to find drugs to treat lung diseases.</p>
      <p id="Par35">We visualize factors and treatments closely related to lung cancer, breast cancer, and coronavirus, as shown in Figs. <xref rid="Fig12" ref-type="fig">12</xref>, <xref rid="Fig13" ref-type="fig">13</xref>, and <xref rid="Fig14" ref-type="fig">14</xref>, respectively. We observe the lung cancer is closely related to “antiretroviral therapy, radiation therapy, gene therapy, adjuvant chemotherapy, prognostic factor, nuclear factor, targeted therapy, photodynamic therapy”. The breast cancer is closely related to “prognostic factor, antiretroviral therapy, adjuvant chemotherapy, radiation therapy, gene therapy, photodynamic therapy”. “tumour necrosis factor, epidermal growth factor receptor, nuclear factor, targeted therapy, combination therapy” has more inspiration for the treatment of breast cancer. The coronavirus is closely related to “vascular endothelial growth factor”, “cell therapy”, “replacement therapy”, “neoadjuvant chemotherapy”, “drug development”, “combination therapy”, “photodynamic therapy”, etc. These results show that our method can learn term embeddings from a large-scale corpus to generate inspiration for diseases’ treatment. <fig id="Fig12"><label>Fig. 12</label><caption><p>Visualization of factors for breast cancer (blue dot) (color figure online)</p></caption><graphic xlink:href="521_2020_5335_Fig12_HTML" id="MO25"/></fig><fig id="Fig13"><label>Fig. 13</label><caption><p>Visualization of factors for lung cancer (blue dot) (color figure online)</p></caption><graphic xlink:href="521_2020_5335_Fig13_HTML" id="MO26"/></fig><fig id="Fig14"><label>Fig. 14</label><caption><p>Visualization of factors for coronavirus (blue dot) (color figure online)</p></caption><graphic xlink:href="521_2020_5335_Fig14_HTML" id="MO27"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec17">
    <title>Conclusion</title>
    <p id="Par36">In this paper, an unsupervised term mining method has been proposed for mining terms from a biomedical corpus. We have combined the term mining method with existing word vector training methods to learn term embeddings to capture the semantic similarity between terms. The proposed method can identify term variations and improve term representations. It is to be noted that the proposed method can be applied across domains without the need for external resources. We also analyzed the distribution of diseases and treatments based on the learned term embeddings, which can be used to explore treatment schemes for some challenging diseases. Extensive computer simulations were conducted to determine the effectiveness of the proposed method. PubMed—10K and PubMed—100K datasets were used for experimentation (see Table <xref rid="Tab1" ref-type="table">1</xref>). A comprehensive evaluation was carried out through visualization of term embeddings, which used cosine similarity to determine similar terms to each term. The visualization helped to demonstrate the performance of the proposed method and serve as a way to explore treatments for novel diseases. The application of the proposed model can be applicable in several domains [<xref ref-type="bibr" rid="CR31">31</xref>–<xref ref-type="bibr" rid="CR54">54</xref>] for the future work.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>Prayag Tiwari received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 721321. M. Shamim Hossain extends his appreciation to the Researchers Supporting Project number (RSP-2020/32), King Saud University, Riyadh, Saudi Arabia for funding this work.</p>
  </ack>
  <notes notes-type="data-availability">
    <sec id="FPar100">
      <title>Code Availability</title>
      <p><ext-link ext-link-type="uri" xlink:href="https://github.com/prayagtiwari/TermInformer">https://github.com/prayagtiwari/TermInformer</ext-link>.</p>
    </sec>
  </notes>
  <notes notes-type="ethics">
    <title>Compliance with ethical standards</title>
    <notes id="FPar1" notes-type="COI-statement">
      <title>Conflict of interest</title>
      <p id="Par38">The authors declare that they have no conflict of interest.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Pandey</surname>
            <given-names>HM</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Jin</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>A distant supervision method based on paradigmatic relations for learning word embeddings</article-title>
        <source>Neural Comput Appl</source>
        <year>2019</year>
        <volume>2019</volume>
        <fpage>1</fpage>
        <lpage>10</lpage>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <mixed-citation publication-type="other">Pennington J, Socher R, Manning CD (2014) Glove: global vectors for word representation. In: Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) pp 1532–1543</mixed-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <mixed-citation publication-type="other">Settles B (2004) Biomedical named entity recognition using conditional random fields and rich feature sets. In: Proceedings of the international joint workshop on natural language processing in biomedicine and its applications (NLPBA/BioNLP), pp 107–110</mixed-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <mixed-citation publication-type="other">Wallach HM (2004) Conditional random fields: an introduction. Technical Reports (CIS), p 22</mixed-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <mixed-citation publication-type="other">Leaman R, Gonzalez G (2008) BANNER: an executable survey of advances in biomedical named entity recognition. In: Biocomputing 2008, pp 652–663</mixed-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Habibi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Weber</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Neves</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wiegandt</surname>
            <given-names>DL</given-names>
          </name>
          <name>
            <surname>Leser</surname>
            <given-names>U</given-names>
          </name>
        </person-group>
        <article-title>Deep learning with word embeddings improves biomedical named entity recognition</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>33</volume>
        <issue>14</issue>
        <fpage>i37</fpage>
        <lpage>i48</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx228</pub-id>
        <pub-id pub-id-type="pmid">28881963</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tang</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Evaluating word representation features in biomedical named entity recognition tasks</article-title>
        <source>BioMed Res Int</source>
        <year>2014</year>
        <volume>2014</volume>
        <fpage>240403</fpage>
        <pub-id pub-id-type="doi">10.1155/2014/240403</pub-id>
        <pub-id pub-id-type="pmid">24729964</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yao</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Anwar</surname>
            <given-names>MW</given-names>
          </name>
        </person-group>
        <article-title>Biomedical named entity recognition based on deep neutral network</article-title>
        <source>Int J Hybrid Inf Technol</source>
        <year>2015</year>
        <volume>8</volume>
        <issue>8</issue>
        <fpage>279</fpage>
        <lpage>288</lpage>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zitnik</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Shang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Langlotz</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Cross-type biomedical named entity recognition with deep multi-task learning</article-title>
        <source>Bioinformatics</source>
        <year>2019</year>
        <volume>35</volume>
        <issue>10</issue>
        <fpage>1745</fpage>
        <lpage>1752</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty869</pub-id>
        <pub-id pub-id-type="pmid">30307536</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Lyu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Dong</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Multitask learning for biomedical named entity recognition with cross-sharing structure</article-title>
        <source>BMC Bioinform</source>
        <year>2019</year>
        <volume>20</volume>
        <issue>1</issue>
        <fpage>427</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-019-3000-5</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yoon</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>So</surname>
            <given-names>CH</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Collabonet: collaboration of deep neural networks for biomedical named entity recognition</article-title>
        <source>BMC Bioinform</source>
        <year>2019</year>
        <volume>20</volume>
        <issue>10</issue>
        <fpage>249</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-019-2813-6</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cho</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Biomedical named entity recognition using deep neural networks with contextual information</article-title>
        <source>BMC Bioinform</source>
        <year>2019</year>
        <volume>20</volume>
        <issue>1</issue>
        <fpage>735</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-019-3321-4</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Lafferty JD, McCallum A, Pereira FC (2001) Conditional random fields: probabilistic models for segmenting and labeling sequence data. In: Proceedings of the eighteenth international conference on machine learning, pp. 282–289</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nadeau</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Sekine</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>A survey of named entity recognition and classification</article-title>
        <source>Lingvist Investig</source>
        <year>2007</year>
        <volume>30</volume>
        <issue>1</issue>
        <fpage>3</fpage>
        <lpage>26</lpage>
        <pub-id pub-id-type="doi">10.1075/li.30.1.03nad</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Collobert</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Weston</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Bottou</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Karlen</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kavukcuoglu</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Kuksa</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Natural language processing (almost) from scratch</article-title>
        <source>J Mach Learn Res</source>
        <year>2011</year>
        <volume>12</volume>
        <fpage>2493</fpage>
        <lpage>2537</lpage>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Lample G, Ballesteros M, Subramanian S, Kawakami K, Dyer C (2016) Neural architectures for named entity recognition. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1603.01360">arXiv:1603.01360</ext-link></mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chiu</surname>
            <given-names>JP</given-names>
          </name>
          <name>
            <surname>Nichols</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Named entity recognition with bidirectional LSTM-CNNs</article-title>
        <source>Trans Assoc Comput Linguist</source>
        <year>2016</year>
        <volume>4</volume>
        <fpage>357</fpage>
        <lpage>370</lpage>
        <pub-id pub-id-type="doi">10.1162/tacl_a_00104</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Ma X, Hovy E (2016) End-to-end sequence labeling via bi-directional lstm-cnns-crf. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1603.01354">arXiv:1603.01354</ext-link></mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Akbik A, Blythe D, Vollgraf R (2018) Contextual string embeddings for sequence labeling. In: Proceedings of the 27th international conference on computational linguistics, pp 1638–1649</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Peters ME, Neumann M, Iyyer M, Gardner M, Clark C, Lee K, Zettlemoyer L (2018) Deep contextualized word representations. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1802.05365">arXiv:1802.05365</ext-link></mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Devlin J, Chang MW, Lee K, Toutanova K (2018) Bert: pre-training of deep bidirectional transformers for language understanding. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1810.04805">arXiv:1810.04805</ext-link></mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Beltagy I, Lo K, Cohan A (2019) SciBERT: a pretrained language model for scientific text. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1903.10676">arXiv:1903.10676</ext-link></mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Alsentzer E, Murphy JR, Boag W, Weng WH, Jin D, Naumann T, McDermott M (2019) Publicly available clinical BERT embeddings. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1904.03323">arXiv:1904.03323</ext-link></mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Yoon</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>So</surname>
            <given-names>CH</given-names>
          </name>
          <name>
            <surname>Kang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>BioBERT: a pre-trained biomedical language representation model for biomedical text mining</article-title>
        <source>Bioinformatics</source>
        <year>2020</year>
        <volume>36</volume>
        <issue>4</issue>
        <fpage>1234</fpage>
        <lpage>1240</lpage>
        <pub-id pub-id-type="pmid">31501885</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Ducharme</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Vincent</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Jauvin</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>A neural probabilistic language model</article-title>
        <source>J Mach Learn Res</source>
        <year>2003</year>
        <volume>3</volume>
        <fpage>1137</fpage>
        <lpage>1155</lpage>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Mikolov T, Chen K, Corrado G, Dean J (2013) Efficient estimation of word representations in vector space. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1301.3781">arXiv:1301.3781</ext-link></mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bojanowski</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Grave</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Joulin</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Mikolov</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Enriching word vectors with subword information</article-title>
        <source>Trans Assoc Comput Linguist</source>
        <year>2017</year>
        <volume>5</volume>
        <fpage>135</fpage>
        <lpage>146</lpage>
        <pub-id pub-id-type="doi">10.1162/tacl_a_00051</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hochreiter</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Schmidhuber</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Long short-term memory</article-title>
        <source>Neural Comput</source>
        <year>1997</year>
        <volume>9</volume>
        <issue>8</issue>
        <fpage>1735</fpage>
        <lpage>1780</lpage>
        <pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id>
        <pub-id pub-id-type="pmid">9377276</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Forney</surname>
            <given-names>GD</given-names>
          </name>
        </person-group>
        <article-title>The viterbi algorithm</article-title>
        <source>Proc IEEE</source>
        <year>1973</year>
        <volume>61</volume>
        <issue>3</issue>
        <fpage>268</fpage>
        <lpage>278</lpage>
        <pub-id pub-id-type="doi">10.1109/PROC.1973.9030</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wold</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Esbensen</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Geladi</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Principal component analysis</article-title>
        <source>Chemom Intell Lab Syst</source>
        <year>1987</year>
        <volume>2</volume>
        <issue>1–3</issue>
        <fpage>37</fpage>
        <lpage>52</lpage>
        <pub-id pub-id-type="doi">10.1016/0169-7439(87)80084-9</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sodhro</surname>
            <given-names>AH</given-names>
          </name>
          <name>
            <surname>Pirbhulal</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sangaiah</surname>
            <given-names>AK</given-names>
          </name>
        </person-group>
        <article-title>Convergence of IoT and product lifecycle management in medical health care</article-title>
        <source>Future Gen Comput Syst</source>
        <year>2018</year>
        <volume>86</volume>
        <fpage>380</fpage>
        <lpage>391</lpage>
        <pub-id pub-id-type="doi">10.1016/j.future.2018.03.052</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sodhro</surname>
            <given-names>AH</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Sangaiah</surname>
            <given-names>AK</given-names>
          </name>
          <name>
            <surname>Baik</surname>
            <given-names>SW</given-names>
          </name>
        </person-group>
        <article-title>Mobile edge computing based QoS optimization in medical healthcare applications</article-title>
        <source>Int J Inf Manag</source>
        <year>2019</year>
        <volume>45</volume>
        <fpage>308</fpage>
        <lpage>318</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ijinfomgt.2018.08.004</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sodhro</surname>
            <given-names>AH</given-names>
          </name>
          <name>
            <surname>Pirbhulal</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Qaraqe</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lohano</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sodhro</surname>
            <given-names>GH</given-names>
          </name>
          <name>
            <surname>Junejo</surname>
            <given-names>NUR</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Power control algorithms for media transmission in remote healthcare systems</article-title>
        <source>IEEE Access</source>
        <year>2018</year>
        <volume>6</volume>
        <fpage>42384</fpage>
        <lpage>42393</lpage>
        <pub-id pub-id-type="doi">10.1109/ACCESS.2018.2859205</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sodhro</surname>
            <given-names>AH</given-names>
          </name>
          <name>
            <surname>Malokani</surname>
            <given-names>AS</given-names>
          </name>
          <name>
            <surname>Sodhro</surname>
            <given-names>GH</given-names>
          </name>
          <name>
            <surname>Muzammal</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Zongwei</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>An adaptive QoS computation for medical data processing in intelligent healthcare applications</article-title>
        <source>Neural Comput Appl</source>
        <year>2020</year>
        <volume>32</volume>
        <issue>3</issue>
        <fpage>723</fpage>
        <lpage>734</lpage>
        <pub-id pub-id-type="doi">10.1007/s00521-018-3931-1</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tiwari</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Qian</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Gupta</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Khanna</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Rodrigues</surname>
            <given-names>JJ</given-names>
          </name>
          <name>
            <surname>de Albuquerque</surname>
            <given-names>VHC</given-names>
          </name>
        </person-group>
        <article-title>Detection of subtype blood cells using deep learning</article-title>
        <source>Cogn Syst Res</source>
        <year>2018</year>
        <volume>52</volume>
        <fpage>1036</fpage>
        <lpage>1044</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cogsys.2018.08.022</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Qian J, Tiwari P, Gochhayat SP, Pandey HM (2020) A noble double dictionary based ECG compression technique for IoTH. IEEE Internet Things J. 10.1109/JIOT.2020.2974678</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jaiswal</surname>
            <given-names>AK</given-names>
          </name>
          <name>
            <surname>Kumar</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gupta</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Khanna</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Rodrigues</surname>
            <given-names>JJ</given-names>
          </name>
        </person-group>
        <article-title>Identifying pneumonia in chest X-rays: a deep learning approach</article-title>
        <source>Measurement</source>
        <year>2019</year>
        <volume>145</volume>
        <fpage>511</fpage>
        <lpage>518</lpage>
        <pub-id pub-id-type="doi">10.1016/j.measurement.2019.05.076</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rodrigues</surname>
            <given-names>MB</given-names>
          </name>
          <name>
            <surname>Da Nóbrega</surname>
            <given-names>RVM</given-names>
          </name>
          <name>
            <surname>Alves</surname>
            <given-names>SSA</given-names>
          </name>
          <name>
            <surname>Rebouças Filho</surname>
            <given-names>PP</given-names>
          </name>
          <name>
            <surname>Duarte</surname>
            <given-names>JBF</given-names>
          </name>
          <name>
            <surname>Sangaiah</surname>
            <given-names>AK</given-names>
          </name>
          <name>
            <surname>De Albuquerque</surname>
            <given-names>VHC</given-names>
          </name>
        </person-group>
        <article-title>Health of things algorithms for malignancy level classification of lung nodules</article-title>
        <source>IEEE Access</source>
        <year>2018</year>
        <volume>6</volume>
        <fpage>18592</fpage>
        <lpage>18601</lpage>
        <pub-id pub-id-type="doi">10.1109/ACCESS.2018.2817614</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Piccialli</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Casolla</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Cuomo</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Giampaolo</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Di Cola</surname>
            <given-names>VS</given-names>
          </name>
        </person-group>
        <article-title>Decision making in IoT environment through unsupervised learning</article-title>
        <source>IEEE Intell Syst</source>
        <year>2019</year>
        <volume>35</volume>
        <issue>1</issue>
        <fpage>27</fpage>
        <lpage>35</lpage>
        <pub-id pub-id-type="doi">10.1109/MIS.2019.2944783</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Casolla</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Cuomo</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Di Cola</surname>
            <given-names>VS</given-names>
          </name>
          <name>
            <surname>Piccialli</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Exploring unsupervised learning techniques for the Internet of Things</article-title>
        <source>IEEE Trans Ind Inform</source>
        <year>2019</year>
        <volume>16</volume>
        <issue>4</issue>
        <fpage>2621</fpage>
        <lpage>2628</lpage>
        <pub-id pub-id-type="doi">10.1109/TII.2019.2941142</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Piccialli</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Cuomo</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>di Cola</surname>
            <given-names>VS</given-names>
          </name>
          <name>
            <surname>Casolla</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>A machine learning approach for IoT cultural data</article-title>
        <source>J Ambient Intell Hum Comput</source>
        <year>2019</year>
        <volume>2019</volume>
        <fpage>1</fpage>
        <lpage>12</lpage>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ahmad</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Jabbar</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ahmad</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Piccialli</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Jeon</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>A sustainable solution to support data security in high bandwidth healthcare remote locations by using TCP CUBIC mechanism</article-title>
        <source>IEEE Trans Sustain Comput</source>
        <year>2020</year>
        <volume>5</volume>
        <issue>2</issue>
        <fpage>249</fpage>
        <lpage>259</lpage>
        <pub-id pub-id-type="doi">10.1109/TSUSC.2018.2841998</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Alexandridis</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Zilic</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Pang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Jeon</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Piccialli</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>A blockchain-based eHealthcare system interoperating with WBANs</article-title>
        <source>Future Gen Comput Syst</source>
        <year>2020</year>
        <volume>110</volume>
        <fpage>675</fpage>
        <lpage>685</lpage>
        <pub-id pub-id-type="doi">10.1016/j.future.2019.09.049</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Qureshi</surname>
            <given-names>KN</given-names>
          </name>
          <name>
            <surname>Din</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Jeon</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Piccialli</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>An accurate and dynamic predictive model for a smart M-Health system using machine learning</article-title>
        <source>Inf Sci</source>
        <year>2020</year>
        <volume>538</volume>
        <fpage>486</fpage>
        <lpage>502</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ins.2020.06.025</pub-id>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tiwari</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Melucci</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Towards a quantum-inspired binary classifier</article-title>
        <source>IEEE Access</source>
        <year>2019</year>
        <volume>7</volume>
        <fpage>42354</fpage>
        <lpage>42372</lpage>
        <pub-id pub-id-type="doi">10.1109/ACCESS.2019.2904624</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Tiwari</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Garg</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Bruza</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Structural block driven enhanced convolutional neural representation for relation extraction</article-title>
        <source>Appl Soft Comput</source>
        <year>2020</year>
        <volume>86</volume>
        <fpage>105913</fpage>
        <pub-id pub-id-type="doi">10.1016/j.asoc.2019.105913</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <mixed-citation publication-type="other">Tiwari P, Melucci M (2018) Towards a quantum-inspired framework for binary classification. In: Proceedings of the 27th ACM international conference on information and knowledge management, pp 1815–1818</mixed-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <mixed-citation publication-type="other">Tiwari P, Melucci M (2019) Binary classifier inspired by quantum theory. In: Proceedings of the AAAI conference on artificial intelligence, vol 33, pp 10051–10052</mixed-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <mixed-citation publication-type="other">Tiwari P, Melucci M (2018) Multi-class classification model inspired by quantum detection theory. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1810.04491">arXiv:1810.04491</ext-link></mixed-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Aujla</surname>
            <given-names>GS</given-names>
          </name>
          <name>
            <surname>Chaudhary</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Kaur</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Garg</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kumar</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Ranjan</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>SAFE: SDN-assisted framework for edge-cloud interplay in secure healthcare ecosystem</article-title>
        <source>IEEE Trans Ind Inform</source>
        <year>2018</year>
        <volume>15</volume>
        <issue>1</issue>
        <fpage>469</fpage>
        <lpage>480</lpage>
        <pub-id pub-id-type="doi">10.1109/TII.2018.2866917</pub-id>
      </element-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <mixed-citation publication-type="other">Rathee G, Garg S, Kaddoum G, Choi BJ (2020) A decision-making model for securing IoT devices in smart industries. IEEE Trans Ind Inform. 10.1109/TII.2020.3005252</mixed-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Garg</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kaur</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Kumar</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Rodrigues</surname>
            <given-names>JJ</given-names>
          </name>
        </person-group>
        <article-title>Hybrid deep-learning-based anomaly detection scheme for suspicious flow detection in SDN: a social multimedia perspective</article-title>
        <source>IEEE Trans Multimed</source>
        <year>2019</year>
        <volume>21</volume>
        <issue>3</issue>
        <fpage>566</fpage>
        <lpage>578</lpage>
        <pub-id pub-id-type="doi">10.1109/TMM.2019.2893549</pub-id>
      </element-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Garg</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kaur</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Kumar</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Kaddoum</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Zomaya</surname>
            <given-names>AY</given-names>
          </name>
          <name>
            <surname>Ranjan</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>A hybrid deep learning-based model for anomaly detection in cloud datacenter networks</article-title>
        <source>IEEE Trans Netw Serv Manag</source>
        <year>2019</year>
        <volume>16</volume>
        <issue>3</issue>
        <fpage>924</fpage>
        <lpage>935</lpage>
        <pub-id pub-id-type="doi">10.1109/TNSM.2019.2927886</pub-id>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fang</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Sang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Hossain</surname>
            <given-names>MS</given-names>
          </name>
        </person-group>
        <article-title>Relational user attribute inference in social media</article-title>
        <source>IEEE Trans Multimed</source>
        <year>2015</year>
        <volume>17</volume>
        <issue>7</issue>
        <fpage>1031</fpage>
        <lpage>1044</lpage>
        <pub-id pub-id-type="doi">10.1109/TMM.2015.2430819</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
