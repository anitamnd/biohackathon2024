<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Database (Oxford)</journal-id>
    <journal-id journal-id-type="iso-abbrev">Database (Oxford)</journal-id>
    <journal-id journal-id-type="publisher-id">databa</journal-id>
    <journal-title-group>
      <journal-title>Database: The Journal of Biological Databases and Curation</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1758-0463</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
      <publisher-loc>UK</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">11184448</article-id>
    <article-id pub-id-type="pmid">38554132</article-id>
    <article-id pub-id-type="doi">10.1093/database/baae008</article-id>
    <article-id pub-id-type="publisher-id">baae008</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Article</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI00960</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Visualization and exploration of linked data using virtual reality</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-6108-5552</contrib-id>
        <name>
          <surname>Kellmann</surname>
          <given-names>Alexander J</given-names>
        </name>
        <aff><institution content-type="department">Department of Genetics, University of Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff>
        <aff><institution content-type="department">Department of Genetics, University Medical Center Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Postema</surname>
          <given-names>Max</given-names>
        </name>
        <aff><institution content-type="department">Department of Genetics, University Medical Center Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>de Keijser</surname>
          <given-names>Joris</given-names>
        </name>
        <aff><institution content-type="department">Department of Genetics, University Medical Center Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Svetachov</surname>
          <given-names>Pjotr</given-names>
        </name>
        <aff><institution content-type="department">Center of information technology, University of Groningen</institution>, Nettelbosje 1, Groningen, Groningen 9747 AJ, <country country="NL">The Netherlands</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-2294-593X</contrib-id>
        <name>
          <surname>Wilson</surname>
          <given-names>Rebecca C</given-names>
        </name>
        <aff><institution content-type="department">Public Health, Policy &amp; Systems, University of Liverpool</institution>, Block B, 1st Floor, Waterhouse Building, 1-5 Dover Street, Liverpool L69 3GL, <country country="GB">United Kingdom</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-2440-3993</contrib-id>
        <name>
          <surname>van Enckevort</surname>
          <given-names>Esther J</given-names>
        </name>
        <aff><institution content-type="department">Department of Genetics, University of Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff>
        <aff><institution content-type="department">Department of Genetics, University Medical Center Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-0979-3401</contrib-id>
        <name>
          <surname>Swertz</surname>
          <given-names>Morris A</given-names>
        </name>
        <!--m.a.swertz@umcg.nl-->
        <aff><institution content-type="department">Department of Genetics, University of Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff>
        <aff><institution content-type="department">Department of Genetics, University Medical Center Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff>
        <xref rid="COR0001" ref-type="corresp"/>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="COR0001">*Corresponding author: Tel: +31 50 3617100; Fax: +31 50 3617231; Email: <email xlink:href="m.a.swertz@umcg.nl">m.a.swertz@umcg.nl</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2024-02-23">
      <day>23</day>
      <month>2</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>23</day>
      <month>2</month>
      <year>2024</year>
    </pub-date>
    <volume>2024</volume>
    <elocation-id>baae008</elocation-id>
    <history>
      <date date-type="accepted">
        <day>25</day>
        <month>1</month>
        <year>2024</year>
      </date>
      <date date-type="rev-recd">
        <day>18</day>
        <month>12</month>
        <year>2023</year>
      </date>
      <date date-type="received">
        <day>27</day>
        <month>6</month>
        <year>2023</year>
      </date>
      <date date-type="editorial-decision">
        <day>21</day>
        <month>12</month>
        <year>2023</year>
      </date>
      <date date-type="corrected-typeset">
        <day>23</day>
        <month>2</month>
        <year>2024</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2024. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2024</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="baae008.pdf"/>
    <abstract>
      <title>Abstract</title>
      <p>In this report, we analyse the use of virtual reality (VR) as a method to navigate and explore complex knowledge graphs. Over the past few decades, linked data technologies [Resource Description Framework (RDF) and Web Ontology Language (OWL)] have shown to be valuable to encode such graphs and many tools have emerged to interactively visualize RDF. However, as knowledge graphs get larger, most of these tools struggle with the limitations of 2D screens or 3D projections. Therefore, in this paper, we evaluate the use of VR to visually explore SPARQL Protocol and RDF Query Language (SPARQL) (construct) queries, including a series of tutorial videos that demonstrate the power of VR (see Graph2VR tutorial playlist: <ext-link xlink:href="https://www.youtube.com/playlist?list=PLRQCsKSUyhNIdUzBNRTmE-_JmuiOEZbdH" ext-link-type="uri">https://www.youtube.com/playlist?list=PLRQCsKSUyhNIdUzBNRTmE-_JmuiOEZbdH</ext-link>). We first review existing methods for Linked Data visualization and then report the creation of a prototype, Graph2VR. Finally, we report a first evaluation of the use of VR for exploring linked data graphs. Our results show that most participants enjoyed testing Graph2VR and found it to be a useful tool for graph exploration and data discovery. The usability study also provides valuable insights for potential future improvements to Linked Data visualization in VR.</p>
    </abstract>
    <counts>
      <page-count count="15"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec id="s1">
    <title>Introduction</title>
    <p>In recent years Linked Data has increased in popularity for representing complex analysis results, i.e. ‘hairballs’ of scientific knowledge, in particular in light of the desire to increase the FAIRness of research results (<xref rid="R2" ref-type="bibr">2</xref>, <xref rid="R3" ref-type="bibr">3</xref>).</p>
    <p>In 2009, Sir Tim Berners Lee, who is also known as the ‘inventor of the World Wide Web’ because he invented the Hypertext Transfer Protocol protocol and Hypertext markup language for web pages, gave a famous Ted talk about Linked Data in which he described how incompatible data formats and documentation systems make it necessary to examine each data element in order to create something new (<xref rid="R4" ref-type="bibr">4</xref>, <xref rid="R5" ref-type="bibr">5</xref>). Berners Lee suggested that uploading unadulterated raw data to the web as Linked Data would make it easier to combine, link and reuse existing data. Since then, shared vocabularies and ontologies have increasingly been used to structure data, and chemical and biological registries, as well as governments, have started using Linked Data to handle the large amounts of data they store (<xref rid="R6" ref-type="bibr">6–9</xref>).</p>
    <p>The Resource Description Framework (RDF) developed by the World Wide Web Consortium is a standard for describing data on the web in a machine-readable format (<xref rid="R10" ref-type="bibr">10</xref>). The RDF data model describes information as subject–predicate–object relationships called triples. This kind of data can be queried using the SPARQL query language; however, the powers of SPARQL and linked data are not readily accessible to users unfamiliar with SPARQL. Graphical tools provide visual user interfaces to support the user in visually exploring and accessing this kind of data. These triples can be used to create a network graph visualization by representing the subject and object of each triple as nodes and the predicates as edges between them. Nodes that represent the same resource are merged in the visual representation.</p>
    <p>Immersive technologies such as virtual reality (VR) and augmented reality (AR) are increasingly used in health and life sciences for a variety of applications, including therapeutics, training, simulation of real-world scenarios and data analysis for, e.g. genomics and medical imaging (<xref rid="R11" ref-type="bibr">11–15</xref>). Immersive analytics has established applications for abstract and multidimensional data in this domain (see (<xref rid="R12" ref-type="bibr">12</xref>) for a review). A limited number of applications exist to explore knowledge graphs in VR (<xref rid="R16" ref-type="bibr">16–18</xref>), but these are merely visualizations and do not offer many options for users to query and interact with the data.</p>
    <p>We hypothesize that VR will allow users to more readily explore, compare and query large knowledge graphs using a gesture-driven interface that requires less technical expertise. In this VR context, users can use ontologies to search, order and filter data to their needs. Instead of writing SPARQL queries, the user can expand existing connections and define patterns in the data interactively using the VR controllers. Overall, VR adds a third dimension and an open space that can help users perform complex data analysis.</p>
    <p>In this paper, to test this hypothesis, we first review recent visualization methods and tools used for the exploration and analysis of semantic web knowledge graphs and identify best practice methods for visualizing and interacting with SPARQL query results. We then select methods and materials and implement an experimental VR prototype to explore Linked Data and Graph2VR, evaluate its usability and investigate the human and VR environment factors that could enhance the exploration and analysis of semantic web knowledge graphs.</p>
  </sec>
  <sec id="s2">
    <title>Related work</title>
    <p>As a basis for the Graph2VR experiment, we reviewed existing tools that provide best practice methods for addressing specific challenges when working with (large) graph databases. Our review included tools for visualizing graphs in 2D, 3D and VR; those working with ontologies, SPARQL and graph databases; backends for graph databases and general ways to visualize data in VR (<xref rid="s10" ref-type="sec">Supplementary Appendix Table A1</xref>).</p>
    <p>Later, we describe some notable tools, LODLive, GraphDB, Vasturiano, Toran, Visual Notation for OWL ontologies (VOWL), Gruff and Tarsier (<xref rid="R18" ref-type="bibr">18–25</xref>), that all provide rich graph exploration functionality, enumerate functional challenges and visualization aspects and highlight their implementation in Graph2VR. We also refer the reader to some overview papers about tools to visualize and interact with ontologies, Linked Data or graph databases in general, which have been tested and described, e.g. in the following papers. (<xref rid="R6" ref-type="bibr">6</xref>, <xref rid="R26" ref-type="bibr">26</xref>, <xref rid="R27" ref-type="bibr">27</xref>).</p>
    <p>In ‘LodLive’, the user is first asked for a SPARQL Endpoint and a Unique Resource Identifier (URI). Each node is represented as a circle in LodLive, and outgoing connections are represented as small nodes around it (see <xref rid="F1" ref-type="fig">Figure 1</xref>). Each node offers some options in the form of a menu that includes ‘i’ for information, a button to focus on the node and to close other relationships, a button to open a link to the URI to show an online resource, a button to expand all relationships around that node and a button to remove the selected node. Once a node is added, existing connections to other open nodes are also displayed. However, beyond opening new connections and a comprehensive info panel that shows information about the respective node in a flexible way that makes use of different kinds of semantic annotations, integrating images and even google maps, the query possibilities in LodLive are quite limited.</p>
    <fig position="float" id="F1" fig-type="figure">
      <label>Figure 1.</label>
      <caption>
        <p>A screenshot of LodLive (<xref rid="R19" ref-type="bibr">19</xref>): the graph can be expanded by clicking on the small clouds or circles around the node.</p>
      </caption>
      <graphic xlink:href="baae008f1" position="float"/>
    </fig>
    <p>‘GraphDB’ and ‘Metaphactory’ are commercial software tools (<xref rid="R20" ref-type="bibr">20</xref>, <xref rid="R28" ref-type="bibr">28</xref>). They use an internal database and offer an autocomplete search function to find URIs. The user is thus not required to know the precise URI, which makes the tools quite user-friendly. GraphDB also offers an option to collapse all nodes around a specific node instead of only allowing users to delete specific nodes (<xref rid="F2" ref-type="fig">Figure 2</xref>).</p>
    <fig position="float" id="F2" fig-type="figure">
      <label>Figure 2.</label>
      <caption>
        <p>A small ‘Visual Graph’ in GraphDB (<xref rid="R20" ref-type="bibr">20</xref>). The node in the middle has a menu with the option to collapse connections around that node.</p>
      </caption>
      <graphic xlink:href="baae008f2" position="float"/>
    </fig>
    <p>The Visual Notation for OWL Ontologies ‘VOWL’ (<xref rid="R22" ref-type="bibr">22</xref>, <xref rid="R23" ref-type="bibr">23</xref>) adds use of colour-blind-friendly colours and symbols to visualize different types of nodes and edges. Two examples of the implementation of VOWL are a plugin for Protégé and WebVOWL (<xref rid="R29" ref-type="bibr">29</xref>, <xref rid="R30" ref-type="bibr">30</xref>). WebVOWL is an online visualization for ontologies. By default, WebVOWL starts with a subgraph of the Friend of a Friend Ontology (FOAF) ontology, as shown in <xref rid="F3" ref-type="fig">Figure 3</xref>, but users can upload their own, owl files. QueryVOWL, another VOWL tool, can be used to query a SPARQL Endpoint using a textual search to create a visual graph as output (<xref rid="R31" ref-type="bibr">31</xref>).</p>
    <fig position="float" id="F3" fig-type="figure">
      <label>Figure 3.</label>
      <caption>
        <p>Screenshot of WebVOWL showing parts of the FOAF ontology (<xref rid="R32" ref-type="bibr">32</xref>).</p>
      </caption>
      <graphic xlink:href="baae008f3" position="float"/>
    </fig>
    <p>Gruff is a commercial tool that can use its internal graph database (Allegro graph), SPARQL Endpoints and Neo4j. After selecting a database, users can ‘Display some sample triples’ from that database without any prior knowledge about the content, which is very convenient for new users. Gruff offers a visual query view to describe the necessary relations using variables and searching for nodes. These relations can contain specific URIs, such as constant nodes and variables. The relationships created in query view are then translated into an editable SPARQL query, which can be altered to add additional filters or other specific SPARQL commands. This translation from a visualization to an editable SPARQL query is convenient and makes Gruff stand out as a tool. However, screen size is a limiting factor for Gruff. When zooming out, the text becomes too small to read and is hidden. The user can move the window of visible nodes in all directions. Instead of using labels, Gruff uses colours and shapes to indicate similar nodes and repeating edges.</p>
    <fig position="float" id="F4" fig-type="figure">
      <label>Figure 4.</label>
      <caption>
        <p>Gruff allows users to build queries visually and shows the SPARQL query, so it can be modified (4(A)). The results can be displayed as a visual graph (4(B))(<xref rid="R24" ref-type="bibr">24</xref>). (A) Screenshot of a query built-in Gruff’s ‘Graphical Query View’. (B) Screenshot of the resulting graph in Gruff’s ‘Graph View’. When zoomed out, the texts disappear, but the colours still indicate similar nodes and edges.</p>
      </caption>
      <graphic xlink:href="baae008f4" position="float"/>
    </fig>
    <p>Finally, ‘Tarsier’ is a tool that makes use of a 3D representation of SPARQL queries. Filters allow users to select nodes that fulfil specific criteria and shift them to another ‘semantic plane’. This tool was built with the intention to help new students to learn SPARQL and understand how filters work (<xref rid="F5" ref-type="fig">Figure 5</xref>).</p>
    <fig position="float" id="F5" fig-type="figure">
      <label>Figure 5.</label>
      <caption>
        <p>Tarsier filter data and can shift datapoints to different semantic planes accordingly. (This screenshot was taken from a video from the authors of Tarsier (<xref rid="R33" ref-type="bibr">33</xref>)).</p>
      </caption>
      <graphic xlink:href="baae008f5" position="float"/>
    </fig>
  </sec>
  <sec id="s3">
    <title>Materials and Methods</title>
    <p>Based on the analysis of existing tools and approaches, we prioritized a list of key features for Graph2VR, which are summarized in <xref rid="T1" ref-type="table">Table 1</xref>. We aimed to combine some of the strengths of the different methods from different tools in one VR application that enables immersive 3D visualization that allows interaction with and manipulation of data. Graph2VR is an extensive usable prototype that still has a few issues that need to be resolved. It is extendable and demonstrates how a VR application can be used to visualize and interact with Linked Data. A more detailed description of its features is present in the user manual in (<xref rid="R34" ref-type="bibr">34</xref>) and in five tutorial videos (<xref rid="R1" ref-type="bibr">1</xref>). Later, we describe the methodological considerations for these features in detail, grouped by initialization, visualization, navigation and data analysis.</p>
    <table-wrap position="float" id="T1">
      <label>Table 1.</label>
      <caption>
        <p>Key features of Graph2VR</p>
      </caption>
      <table frame="hsides" rules="groups">
        <colgroup span="1">
          <col align="left" span="1"/>
          <col align="left" span="1"/>
        </colgroup>
        <thead>
          <tr>
            <th valign="bottom" align="left" rowspan="1" colspan="1">Category</th>
            <th valign="bottom" align="left" rowspan="1" colspan="1">Description</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left" rowspan="1" colspan="1">Initialization</td>
            <td align="left" rowspan="1" colspan="1">A configuration file is used to preconfigure which SPARQL Endpoint to start from and additional ones to switch to. Graph exploration usually starts from either a single URI or a small graph, which can also be specified in the configuration file. Image predicates, predicates that get suggested for new connections and colour coding can be adjusted in this file. We adopted the VOWL colour schema as a reasonable default.</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Visualization</td>
            <td align="left" rowspan="1" colspan="1">To visualize Linked Data, nodes are represented as spheres and the edges between them as arrows. The colour schema is used to represent different kinds of nodes or their current status (e.g. being selected or part of a query). A choice of graph layout (3D, 2D, hierarchical and class hierarchy) for the visualization will give the most flexibility for the user. Labels and images for nodes and labels for edges are preloaded when the graph is expanded.</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Interaction</td>
            <td align="left" rowspan="1" colspan="1">Graph2VR provides features for interacting with the visualized Linked Data. The user can grab and move single nodes or the whole graph. Once grabbed, the whole graph can be moved, rotated and scaled. There are four different graph layout options: 3D, 2D, hierarchical and class hierarchy. The user has different ways to navigate in the VR environment. Graph2VR is a room-scale VR application, so looking around and taking a few steps can help to move small distances. For larger distances, the user can either teleport or fly. Pinning nodes prevents them from being affected by the layout algorithm so that they can be restructured manually. The user can create new nodes, edges and graphs; drag and drop nodes and graphs; and convert nodes or edges to variables to create queries. New nodes can be added by searching for them in the database or by spawning a new variable node. After selecting single nodes or edges, the circle menu shows options for the selected node or edge for further interaction.</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Data analysis</td>
            <td align="left" rowspan="1" colspan="1">Graph2VR provides features for querying SPARQL Endpoints, including graph expansion. In addition, the nodes and edges can be used to generate custom queries visually. Triples can be selected to be part of a SPARQL query. A predefined set of commands, including selecting triples, creating variables, OrderBy and Limit, can be used to create a SPARQL query. Options that affect a whole triple can be found in the edge menu.</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <sec id="s3-s1">
      <title>Initialization</title>
      <p>In many visualization tools, graph exploration starts from either a small subgraph (bottom-up or local view) or a global hierarchy (top-down or global view) in which zoom and filters are used to request more details (<xref rid="R35" ref-type="bibr">35</xref>). Starting from a subgraph, either an overview graph or a start node, the user can expand the graph by opening further connections. Commonly, tools that work with SPARQL need the Uniform Resource Locator of a SPARQL Endpoint to determine to which graph database their requests should be sent. Additionally, they need a URI, SPARQL query or keyword to know where to start the graph exploration. The graph can then be expanded further to explore it incrementally. In Graph2VR, one can start with an initially provided SPARQL query and explore from there.</p>
    </sec>
    <sec id="s3-s2">
      <title>Visualization</title>
      <p>This section summarizes methods around visualization, in particular use of layout, colour and information display.</p>
      <sec id="s3-s2-s1">
        <title>Layout</title>
        <p>In general, the layout of graphs in the tools is either static or is restructured over time to increase the distance between the nodes and the readability. The Fruchterman–Reingold algorithm is a force-directed graph algorithm (<xref rid="R36" ref-type="bibr">36</xref>). The regular runtime of the algorithm has a runtime of <inline-formula id="ILM0001"><tex-math notation="LaTeX" id="ILM0001-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal{O}(|N|^2 + |E|)$\end{document}</tex-math></inline-formula>, where <italic toggle="yes">N</italic> is the number of nodes and <italic toggle="yes">E</italic> is the number of edges (<xref rid="R37" ref-type="bibr">37</xref>). The algorithm also works in three dimensions. As the graphs get larger, the number of node–node interactions increases. For larger graphs, other algorithms like the Barnes–Hut algorithm scale better. The Barnes–Hut algorithm combines the gravity centre of nodes that are further away so that fewer calculations need to be executed (<xref rid="R38" ref-type="bibr">38</xref>, <xref rid="R39" ref-type="bibr">39</xref>). There are, of course, many more algorithms like Kamada–Kawai that could be used to handle even more nodes in a graph at once (<xref rid="R36" ref-type="bibr">36</xref>, <xref rid="R40" ref-type="bibr">40</xref>). In a 3D representation, we can use the third dimension to compare a stack of 2D layers. In the literature, this kind of representation is known as semantic planes (<xref rid="R25" ref-type="bibr">25</xref>, <xref rid="R41" ref-type="bibr">41</xref>), and Tarsier already makes use of it. Tarsier runs on a local server and can be accessed via its web interface. It allows the user to apply filters, then takes all the nodes that fulfil the selection criteria and shifts them to another semantic plane. A stack of different planes of information would allow a user to compare them without losing their internal structure (e.g. a tree structure). In this way, the user can, e.g. compare data and annotate the similarity between two resources. For comparisons, different predicates can be used to describe the kinds of connections between different nodes. In their Scientific Lens paper, Batchelor et al. name four predicates with decreasing similarity levels to compare similar entities that could be used to compare different entities with similar meanings: ‘owl: sameAs’, ‘skos: exactMatch’, ‘skos: closeMatch’ and ‘rdfs: seeAlso’ (<xref rid="R42" ref-type="bibr">42</xref>). In the final version of Graph2VR, we added those as predefined predicates so that a user can quickly adjust the predicate of an edge. In Graph2VR, results can be shown as a series of semantic planes representing the different results that match a given query pattern (see section Demonstration Data). These planes are useful for comparing different 2D structures and creating new connections. For example, multiple 2D layers of class hierarchies allow users to compare between different ontologies, which can be used to find similarities and differences.</p>
      </sec>
      <sec id="s3-s2-s2">
        <title>Colour</title>
        <p>The colouring of nodes and edges also varies from tool to tool. LodLive uses random colours, whereas Gruff reuses the same colour for the same types of nodes and edges. The VOWL colour scheme defines the colour of each node based on its properties, using specific colours for classes, variables, blank nodes and literals. It has also been designed to be colour-blind-friendly and understandable when printed in black and white (<xref rid="R32" ref-type="bibr">32</xref>). VOWL recommends using specific shapes or patterns to indicate different attributes, e.g. a ring around a node to represent a class. For the 3D environment, the colours can be reused, but some forms need to be adjusted. In 3D, for example, a circle around a sphere to indicate a class could be represented as either a circle or a sphere. Using different forms and shapes would also be a viable option to represent an object in Linked Data. In Graph2VR, we primarily apply the VOWL schema but have not yet implemented all of it. We also add some colours for variables and selected triples. Further details can be found in the manual (<xref rid="R34" ref-type="bibr">34</xref>). As graphs become larger through expansion, it becomes more important to be able to quickly identify different types of nodes, i.e. is the node a URI, a literal, a blank node or a variable? Some tools do not distinguish, displaying all nodes in the same or a custom colour. One example of such a tool is ‘Noda’, an application sold on Steam that allows the user to interact with 3D mindmaps or graph structures by adding nodes and edges with different sizes, symbols, colours and labels (<xref rid="R43" ref-type="bibr">43</xref>). Gruff colours the same kinds of connections in the same randomly chosen colour, whereas the VOWL colour scheme can help users make distinctions quickly (<xref rid="R23" ref-type="bibr">23</xref>). In Graph2VR, parts of the VOWL schema are applied, making literals (strings and numbers) yellow, URIs blue and classes light blue (only if the class relationship is expanded). Blank nodes are dark. When a node or edge is selected using the laser, it turns red. When converted to a variable, nodes and edges turn green. Nodes and edges glow white when the user hovers over them with the laser. If triples are part of the selection for a query, they shine in a bright yellow colour. While common edges are black with a black arrowhead, the arrowhead changes the colour for the ‘rdfs: subClassOf’ relationship to a white arrowhead.</p>
      </sec>
      <sec id="s3-s2-s3">
        <title>Information display</title>
        <p>One useful feature in tools like LODLive or WebVOWL is an information box containing essential information about a selected node, although which information is displayed varies. Typical information provided is the URI, label, class relationship and sometimes comments. SPARQL offers the ‘describe’ function to request some basic information, but the results can be quite extensive. Therefore, it is better to restrict the information to a selection of predicates.</p>
        <p>To visualize query results, we parsed the results and displayed the URIs or, if findable, the labels of the nodes and edges in the graph. To further improve readability, the labels above the nodes are continuously rotated towards the user, as are the images. This can be seen in the first part of the Video tutorial (<xref rid="R1" ref-type="bibr">1</xref>). In contrast, the texts above the edges always sit on top of the connecting lines and are readable from both sides of the edges. Since URIs are often long strings of characters, the labels of the URIs are displayed instead (if available). The complete URI appears when the user hovers over the edge with the laser.</p>
      </sec>
    </sec>
    <sec id="s3-s3">
      <title>Interaction</title>
      <p>This section summarizes features for interaction with the visualization, in particular methods for controls, navigation, zooming and rotation and menus in VR.</p>
      <sec id="s3-s3-s1">
        <title>Controls</title>
        <p>To enable users to reorder nodes, we implemented a gesture-driven interface that allows the user to drag and drop them using the controllers. The user grabs a node with both hands while pressing the grip button. Grabbing a node while pressing the trigger button allows new links between two nodes to be created.</p>
        <p>When multiple graphs exist in VR, it can be challenging to grab and manipulate the desired graph. To differentiate between different graphs, we implemented a sphere around all the nodes of a graph that is only visible from the outside of the graph, to avoid cluttering the view. This sphere determines the size of a graph and allows the user to interact with a specific graph. When a SPARQL query is executed and multiple results are created in different layers (semantic planes), each layer is generated as a separate graph. Grabbing and moving a graph enable the user to work with a specific subgraph. To remove less interesting graphs, we made it possible to delete sibling graphs (all the other graphs created by the same query aside from the current one) or all child graphs of a particular graph. When the semantic planes are displayed next to each other, we first tested drawing flat planes around the 2D visualizations. Those planes were transparent, similar to the sphere surrounding the 3D version of the graph. However, as multiple partially transparent 2D layers behind each other turned out to be more distracting than useful, we turned off the planes by default.</p>
      </sec>
      <sec id="s3-s3-s2">
        <title>Navigation</title>
        <p>We added a platform to Graph2VR to make orientation easier as it feels more natural for a user to stand and walk on a surface than to float in empty space. Common ways to move in room-scaled VR applications are moving in the room, walking with a thumbstick or trackpad, teleportation and, in some cases, flying. In Noda, users can drag themselves through the room. In GraphXR, users can rotate the whole graph. Finally, in 3D Force Graphs, the user can fly through a universe of huge nodes (<xref rid="R17" ref-type="bibr">17</xref>, <xref rid="R18" ref-type="bibr">18</xref>, <xref rid="R43" ref-type="bibr">43</xref>). To give the user maximum freedom for navigation, we implemented both flying and teleportation and, if the user looks around or moves, this also happens in the virtual environment.</p>
      </sec>
      <sec id="s3-s3-s3">
        <title>Zoom and rotation</title>
        <p>Graph2VR offers several ways to interact with a graph. When using the grip buttons of the VR controller on a node, the user can drag and drop a node close to the controller. Here, we were inspired by functions in the game/tech demo Toran (<xref rid="F6" ref-type="fig">Figure 6(A)</xref>) (<xref rid="R21" ref-type="bibr">21</xref>). In that demo, a graph is embedded in a round sphere that can be freely rotated and zoomed (<xref rid="F6" ref-type="fig">Figure 6(B)</xref>). The user can also create new connections between nodes within the sphere. As this also suits working with graphs, we implemented similar ways to interact with the graphs in Graph2VR. Both controllers’ grip buttons must be pressed close to the graph simultaneously to grab the whole graph, which can then be dragged around and rotated freely. When the controllers are moved together/away from each other, the graph is scaled down/up. It is possible to grab a node and the whole graph simultaneously. This results in scaling the entire graph; only the grabbed node does not scale with the graph, which can be used to scale specific nodes up or down. It is also possible to work with multiple graphs in Graph2VR, and each can have a different size.</p>
        <fig position="float" id="F6" fig-type="figure">
          <label>Figure 6.</label>
          <caption>
            <p>The sphere in Toran (6(A)) was the inspiration for having a sphere and for how to rotate, zoom and drag&amp;drop a graph (<xref rid="R21" ref-type="bibr">21</xref>). For comparison, see the sphere in Graph2VR (6(B)). (A) Screenshot of Toran, a transparent sphere surrounds the game elements. (B) Screenshot of rotating a graph in Graph2VR. A transparent sphere surrounds the graph.</p>
          </caption>
          <graphic xlink:href="baae008f6" position="float"/>
        </fig>
      </sec>
    </sec>
    <sec id="s3-s4">
      <title>Circle menu</title>
      <p>VR has not yet evolved to have standard control archetypes comparable to those used in 2D user interfaces. Nonetheless, users do need a system that allows them to quickly choose from sets of options. We were inspired to create a circular menu by the ‘Aesthethic Hover UI’ asset (<xref rid="R44" ref-type="bibr">44</xref>). Here, we first attempted to reuse the HoverUIKit, but it used depreciated packages, so we ultimately built our own circle menu. Most 2D tools like LOD Live, Gruff, GraphDB and Metaphactory show their menu and search options next to the nodes (<xref rid="R19" ref-type="bibr">19</xref>, <xref rid="R20" ref-type="bibr">20</xref>, <xref rid="R24" ref-type="bibr">24</xref>, <xref rid="R45" ref-type="bibr">45</xref>), but VR offers a wider range of options. We therefore considered having the menu directly next to the node, on the left arm, on a virtual display or panel or static in front of the head as in a helmet display. After some testing, we determined that a menu with many options within a hairball of nodes would not be the appropriate solution and that the menu was best readable on the left controller. This circular menu has several submenus and shows the options that can be applied in the current context. When a node is selected, the menu displays options that can be applied to that node (<xref rid="F7" ref-type="fig">Figure 7(A)</xref>). When selecting an edge, a menu for the edge is shown. In the node menu are further submenus that show all incoming or outgoing relations from or to the selected node. To improve the usability of the menu, we added some icons to the options in the circle menu. Click sounds indicate that buttons have been pressed, and white circles around newly spawned nodes that disappear within 2 s denote changes within the application. For new Graph2VR users, we have added a short help menu describing the main functionalities.
</p>
      <fig position="float" id="F7" fig-type="figure">
        <label>Figure 7.</label>
        <caption>
          <p>The circle menu in 7(A) shows different options based on the context. There are submenus that show more details, such as the outgoing connections in 7(B). (A) The circle menu after clicking on a node. (B) Circle menu displays multiple outgoing connections.</p>
        </caption>
        <graphic xlink:href="baae008f7" position="float"/>
      </fig>
    </sec>
    <sec id="s3-s5">
      <title>Data analysis</title>
      <p>This section summarizes methods for graph manipulation, in particular query generation, search, and save and load.</p>
      <sec id="s3-s5-s1">
        <title>Query generation</title>
        <p>In SPARQL, Select queries result in tabular answers, while Construct queries describe graph structures. Therefore, Graph2VR uses Select queries to populate the menu, e.g. with the incoming and outgoing connections of a node, and Construct queries to create graph structures. Nodes with the same URI are combined into one node in the visualization to generate a network graph instead of a list of triples. The submenu for incoming (and outgoing) connections lists the predicates pointing to (or from) the current node. They are then grouped by their predicates, and the number of available triples for each predicate is displayed next to the entry. When there are more predicates than can be displayed at once, a circular scrollbar can be used to scroll through the menu (<xref rid="F7" ref-type="fig">Figure 7(B)</xref>). If a node with the same URI is added to a graph, it will be merged with the existing node. To restrict the number of results when expanding the graph (e.g. if one node has thousands of connections of the same type), there is a limit on how many connections to open (default is 25). This default limit can be adjusted using the limit slider below the menu (<xref rid="F7" ref-type="fig">Figure 7(B)</xref>). The slider is not a linear scale but has fixed amounts of nodes.</p>
        <p>To improve predicate readability, predicate labels are shown instead of the URI. If no label is available, the URI is shortened, but it is still possible to see the entire URI when hovering over the menu with the pointer. The predicates are ordered alphabetically by URI, not by the shortened version shown, which may confuse users when the base URI changes and the alphabetic order starts from the beginning again. When there are multiple connections between the same nodes, or in case of a self-reference, the straight edges are replaced by bent arrows to avoid overlapping texts.</p>
      </sec>
      <sec id="s3-s5-s2">
        <title>Node removal</title>
        <p>Once nodes are established, the user should have the option to remove them from the visualization. We implemented two different ways to do so. The simplest is the ‘close’ option, which removes the node and all triples containing this node from the visualization (but not the database). A more complex way to reduce the size of the graph is the ‘collapse’ option. This removes all leaf nodes around the selected node, while the node itself remains. Leaf nodes are nodes with no other connections within the visualization (but not necessarily in the underlying graph database). This helps reduce the number of nodes while preserving the graph structure.</p>
        <p>For each node, the user has the option to remove it or to remove the surrounding leaf nodes (incoming, outgoing or both). When a node is deleted, the edges around it and its leaf nodes are also removed because there would be no triple left that contains them. This could lead to many single nodes that the user would have to remove manually. To prevent accidental removal when collapsing a selected node (when no triples are left containing it), and to keep newly created nodes that are not yet part of a triple, we decided to keep single nodes so that a user can connect them or start a new exploration from there.</p>
        <p>When queries are sent in Graph2VR, the response will be displayed in new graphs that we call child graphs. To remove these again, each graph has the option to remove a whole graph at once. When a query creates multiple child graphs, it would be too much effort to remove those one-by-one. We therefore included an option to delete all child graphs from the original graph or the operation to delete all sibling graphs. Removing all child graphs will remove all child graphs of the respective graph. Each of the graphs also has the option to remove its sibling graphs, which will remove all unmodified sibling graphs created by the same query, leaving only the selected graph and modified graphs. If the last graph has been removed, it is possible to create a new graph by creating a new node, loading some saved data or using the search function(s).</p>
      </sec>
      <sec id="s3-s5-s3">
        <title>Search</title>
        <p>Graph2VR was intended to be more interactive than just a visualization. To be able to create some SPARQL queries visually, we added ways to create and rename variables or search for keywords. This required a text input system. We therefore reused a virtual keyboard from Unity’s asset store, VRKeys, that uses VR controllers as drumsticks to enter text (<xref rid="R46" ref-type="bibr">46</xref>). This approach to entering text in VR has two advantages over other applications, which mainly use a laser with a point-and-click system. Drumming the keyboard does not use an additional key, so it does not interfere with our controls, and it can be done with two hands simultaneously. We also implemented both a global search in the settings menu and a context-specific search that can be accessed from the node menu for variable nodes. In addition to the keyword, this second search function takes all the selected triples into account. Only results that match the selected variable in the given context and the keyword are shown. Both search functions attempt to perform an autocomplete search on the search term. Depending on the settings, the search can either be triggered with every keystroke on the VR keyboard or by pressing the return key.</p>
      </sec>
      <sec id="s3-s5-s4">
        <title>Save and load</title>
        <p>Graph2VR has two different ways to save and load data to a file. The first is to save the whole state of the application, including all the nodes, edges, their positions, labels, graphs, etc. Even images that have been loaded are saved, so they can be reloaded even if they are no longer on the internet. To prevent saved files from becoming too large, the image resolution is scaled down if it is too large. The quicksave option offers one save slot that will be overwritten every time. Alternatively, the user can specify a filename for a save state, allowing multiple save states. Loading a save state will overwrite the current session. Another way of saving and loading is to save triples as ntriples. This is a standard format that can also be read from other applications. In contrast to the first save option, this only saves the triples, and positions, images and single nodes are not stored. When loading an ntriples file, all the triples are automatically added to the current scene. In contrast to loading a save state, loading an ntriples file does not overwrite the current scene. Instead, all the triples are added as an additional graph. We strongly advise users not to load large ntriples files because Graph2VR could become slow or unresponsive. We could load about 5000 triples at once during our tests, but the framerate dropped to around five frames per second. For larger ntriples files, we recommend loading them into a SPARQL endpoint, e.g. a Virtuoso server, and accessing only the relevant parts from there (<xref rid="R47" ref-type="bibr">47</xref>).</p>
      </sec>
    </sec>
  </sec>
  <sec id="s4">
    <title>Implementation</title>
    <p>Based on the methodological basis, we implemented a Graph2VR prototype in Unity (version 2 021.2f1) (<xref rid="R48" ref-type="bibr">48</xref>). This section summarizes implementation details, in particular use of Quest 2 VR, how to best represent RDF in unity, implementation of the layout algorithms and performance optimizations.</p>
    <sec id="s4-s1">
      <title>Standalone on Quest 2 VR headset</title>
      <p>Graph2VR was designed to run as a standalone version on the Quest 2 VR headset, but it also supports the HTC Vive headset. We recommend using the Quest 2 headset because its higher resolution provides better readability. Graph2VR can be compiled as a Windows application (.exe) or as a standalone application (.apk) for the Meta Quest 2 VR headset. Our Graph2VR implementation process started with the Free Unity WebXR Exporter from the Unity Asset store (<xref rid="R49" ref-type="bibr">49</xref>). This template includes a desert background and some objects and models for the VR controllers. To be able to select nodes, a laser pointer was added to the right controller. We used a sample dataset in a local Virtuoso server via docker as a database (<xref rid="R50" ref-type="bibr">50</xref>). To access the server, a modified DotNetRDF (version 2.6) is used (<xref rid="R51" ref-type="bibr">51</xref>, <xref rid="R52" ref-type="bibr">52</xref>). At some point, DotNetRDF checks whether a specific interface is present, but even if it was present, the test failed in the Quest 2 standalone application. We resolved this by removing this check and recompiling the DotNetRDF library. One of the tougher decisions during the implementation process was whether to use SteamVR or OpenXR (<xref rid="R53" ref-type="bibr">53</xref>, <xref rid="R54" ref-type="bibr">54</xref>). SteamVR, as commercial software, supports many VR headsets out of the box, provides 3D controller models and is, in general, easier to use due to its more abstract controller bindings. On the other hand, OpenXR would allow us to build a standalone application for the Quest2 headset that does not require a connection to a personal computer with a strong graphics card. Ultimately, we chose OpenXR and were able to create a standalone application for the Quest2.</p>
    </sec>
    <sec id="s4-s2">
      <title>RDF representation</title>
      <p>One issue we had to resolve was whether to use the representation of graphs provided by DotNetRDF, using iNodes and iGraphs, or whether to use the Unity in-memory representation for the nodes and edges. Unity-based prefabs can help to circumvent potential inconsistencies of two representations of the same dataset. Keeping both representations would be more error-prone but would allow reuse of more iGraph functionalities. The DotNetRDF representation has certain advantages when parsing and reusing the query results. Many functionalities were already present and utilizable. The disadvantage of this representation was that both the internal and the visual would need updates when adapting the graph. Additionally, while iGraphs in DotNetRDF support triples, adding a single node to our graphs, e.g. when a user adds a new node, led to differences between iGraph and visual representation. Consequently, the decision was made to rely on the unity-based representation to allow single nodes and enable step-by-step generation of new triples without consistency issues.</p>
    </sec>
    <sec id="s4-s3">
      <title>Layout algorithms</title>
      <p>Layout algorithms help to reorder the graph to improve readability. We implemented four layout algorithms: 3D force-directed, 2D force-directed, hierarchical view and class hierarchy. For the force-directed 3D layout, we used the Fruchterman Reingold Algorithm (<xref rid="R37" ref-type="bibr">37</xref>), which uses repulsive forces between nodes and attractive forces along the edges. Over time, the ‘temperature’ cools down, the forces get weaker, the adjustments in the graph get smaller and smaller and there can be a cut-off value. In Graph2VR, layout algorithms can be switched via the Graph operations menu. Inspired by Gephi, we modularized the layout algorithm, so the user can swap to other layout algorithms (<xref rid="R55" ref-type="bibr">55</xref>). For the 2D layout, we used a simple layout heuristic, simply ordering the nodes sequentially in a plane aided by minimal force direction. While the force-directed Fruchterman Reingold algorithm forms circles in 2D or spheres of nodes in 3D, the ‘Hierarchical View’ layout orders the nodes alternating in horizontal and vertical stacks (<xref rid="F8" ref-type="fig">Figure 8</xref>). This makes it easier to read the labels of the nodes and find a specific node. Additionally, the nodes are sorted alphabetically in this layout. New outgoing nodes are usually added to the expanded node’s right side. However, an exception is the rdfs: subClassOf relationship, which points to the left.</p>
      <fig position="float" id="F8" fig-type="figure">
        <label>Figure 8.</label>
        <caption>
          <p>The hierarchical layout adds new triples, alternating vertical and horizontal on the ‘right’ side. Only the rdfs: SubClassOf relations are pointing to the ‘left’ side. They have a white arrowhead as specified in the VOWL schema.</p>
        </caption>
        <graphic xlink:href="baae008f8" position="float"/>
      </fig>
      <fig position="float" id="F9" fig-type="figure">
        <label>Figure 9.</label>
        <caption>
          <p>Comparison of the 3D class hierarchy in Graph2VR (9(A)) with the 2D class hierarchy in Protégé (9(B)) showing the same class hierarchy. (A) The class hierarchy in Graph2VR can display subclasses and individuals in a single 3D tree structure. The individuals and their attributes point in the third dimension. (B) The class hierarchy in Protégé is a tree structure starting at owl: thing. The individuals are displayed in a separate panel after selecting one of the classes.</p>
        </caption>
        <graphic xlink:href="baae008f9" position="float"/>
      </fig>
      <p>Besides the ‘Hierarchical View’, we also added a ‘Class Hierarchy’ layout. The basic idea of this layout was to create a class hierarchy, like the tree structure in Protégé (<xref rid="R29" ref-type="bibr">29</xref>), but in three dimensions. The base of this layout is a 2D class hierarchy based on the rdfs: subClassOf predicate. Each of the classes (or subclasses) can contain multiple individuals of that class. We can use the extra dimension to display the individuals in a list orthogonal to the 2D class hierarchy based on the rdf: type predicate. See <xref rid="F9" ref-type="fig">Figure 9(A)</xref>.
</p>
      <p>Finally, a pin function was added to pin certain nodes to their current position. A pin prevents these nodes from being affected by the layout algorithm, but they can still be dragged around manually. This can have interesting effects when some nodes are pinned and layout algorithms are applied only to parts of the graph. It can be helpful to have a class hierarchy for the classes, pin it and continue exploring the individuals, e.g. with a force-directed algorithm.</p>
    </sec>
    <sec id="s4-s4">
      <title>Performance optimizations</title>
      <p>To prevent the whole application from stuttering while SPARQL queries are executed, we put them into separate threads. This way, Graph2VR will not stutter even if a query takes several seconds to be executed. The disadvantage of this is that the order of results from different queries is not determined. There is a race condition between the different queries, with the faster result displayed first. This may affect the search function when ‘search on key press’ is activated as it fires a query on each key press. This can be quite fast compared to the variation in execution times of the search queries, potentially leading to a situation where search results for an older but slower query overwrite newer results. To speed-up free-text search queries, we used the bif: contains command. This command is not supported by every SPARQL server but is supported by e.g. OpenLink’s Virtuoso (<xref rid="R56" ref-type="bibr">56</xref>). The bif: contains command triggers a search function on a preindexed internal Structured query language table. If such an index exists, this command can be used to speed up the free-text search. If the index does not exist, the server will most likely return empty results (<xref rid="R57" ref-type="bibr">57</xref>). During our tests, the DBpedia SPARQL endpoint supported the bif: contains command. One restriction of the bif: contains command (in Virtuoso) is that it only supports words with at least four letters and needs to be enclosed in brackets if it contains spaces. This is a problem if the search term consists of multiple connected words that belong together (e.g. names with ‘Mc’ at the beginning or ‘van’ in the middle). We did overcome this issue by separating the words but replacing the spaces of words with fewer than four letters with a star (any character) so that the whole name can be used as a single search term.</p>
    </sec>
  </sec>
  <sec id="s5">
    <title>Results</title>
    <sec id="s5-s1">
      <title>Application overview</title>
      <p>A SPARQL query can be used to define the initial graph. A combination of URIs, literals and variables in SPARQL is used to define query patterns and to request results that match this pattern from the database. Traditionally, formulating a SPARQL query involved manually searching for relevant URIs and predicates, a process that could take several minutes, especially for complex queries involving multiple triples. In Graph2VR, it is just a matter of expanding the graph by clicking on the desired predicate and expanding the graph and converting existing nodes (or edges) in the graph to variables. If necessary, new nodes and edges can be spawned. Once the relevant triples in the graph are selected, the results can be requested.</p>
      <p>We implemented two different ways to display the query results. They can either be displayed as a single result graph containing all the triples merged into one new graph or as a series of separate result graphs stacked behind each other. These result graphs are independent graphs. To make sure that not all of them are moved, scaled and rotated at the same time, only one graph (the closest) can be grabbed at the same time. The relevant distance is the distance between the middle point of a graph and the left controller. There are also options to dispose of those stacks of graphs.</p>
      <p>The idea of displaying parts of the results as stacked 2D projections had already been used in previous applications. Tarsier, for example, uses them to separate nodes that do or do not meet user-defined filter criteria into different planes (<xref rid="R25" ref-type="bibr">25</xref>). In the current version of Graph2VR, we did not implement a filter function, but the Tarsier approach might be an excellent way to do so in the future.</p>
      <p>SPARQL has several commands to modify a query. The limit slider was already introduced to limit the number of nodes when expanding the graph (<xref rid="F7" ref-type="fig">Figure 7(B)</xref>). It also can be used to limit the number of result graphs when sending a query. Another modifier is the ORDER BY command, which is used to order the stacked result graphs either descending or ascending. We added this menu option so that the variable can be selected by clicking on it, and there is a small adjacent button (ASC/DESC) to adjust the order.</p>
      <p>Graph2VR can connect to local and online SPARQL Endpoints. If databases are preconfigured, users can switch between them via the circle menu.</p>
    </sec>
    <sec id="s5-s2">
      <title>Demonstration data</title>
      <p>For testing purposes, we provided an easy-to-understand data example from DBpedia, which is one of the best-known public sources for Linked Data and is based on Wikipedia (<xref rid="R58" ref-type="bibr">58</xref>).</p>
      <p>In contrast to Wikipedia, it is possible to query data in DBpedia using SPARQL queries. One of the examples from a lecture about the semantic web was how to ask DBpedia for the second-highest mountain in a certain country, e.g. Australia (<xref rid="R59" ref-type="bibr">59</xref>). While it is challenging to find this information simply by reading Wikipedia articles, writing a SPARQL query to get this information from DBpedia is relatively easy. Since all the information is represented as triples, a user only needs to find out which predicates are used to encode height (dbo: elevation), location (dbp: location) and the fact that it should be a mountain (rdf: type dbo: mountain). The prefixes (res, dbo, dbp and rdf) are the abbreviations of the base URIs and are defined first. To find out which URIs need to be used, a user could open an entry about any mountain, look up the respective predicates and use that information to build their SPARQL query. The following example query is not about the second-highest mountain in Australia but rather about the highest mountains in DBpedia and their location. The mountains are ordered in descending order by height:</p>
      <p>
        <disp-quote>
          <preformat position="float" xml:space="preserve">PREFIX res: &lt;<ext-link xlink:href="http://dbpedia.org/resource/" ext-link-type="uri">http://dbpedia.org/resource/</ext-link>&gt;PREFIX dbo: &lt;<ext-link xlink:href="http://dbpedia.org/ontology/" ext-link-type="uri">http://dbpedia.org/ontology/</ext-link>&gt;PREFIX dbp: &lt;<ext-link xlink:href="http://dbpedia.org/property/" ext-link-type="uri">http://dbpedia.org/property/</ext-link>&gt;PREFIX rdf: &lt;<ext-link xlink:href="http://www.w3.org/1999/02/22-rdf-syntax-ns" ext-link-type="uri">http://www.w3.org/1999/02/22-rdf-syntax-ns</ext-link>&gt;
SELECT ?Mountain ?location ?heightWhere {?Mountain rdf: type dbo: Mountain.?Mountain dbp: location ?location.?Mountain dbo: elevation ?height.}ORDER BY DESC(?height)Limit 25</preformat>
        </disp-quote>
      </p>
      <p>Within Graph2VR, this search can be done visually without manually looking up all the URIs for the predicates. All outgoing predicates are listed in the menu, so the user only needs to select them.</p>
      <fig position="float" id="F10" fig-type="figure">
        <label>Figure 10.</label>
        <caption>
          <p>Creating a query pattern in Graph2VR (10(A)) and requesting the result graphs (10(B)). (A) Query patterns can be created visually in Graph2VR by selecting the triples in the graph that should be part of the query. The query can be modified using the Language settings, Order By options and the Query Limit slider. (B) After rotating the graph and clicking on ‘Request similar patterns’, the results are displayed in a stack of multiple independent graphs.</p>
        </caption>
        <graphic xlink:href="baae008f10" position="float"/>
      </fig>
      <p>Within Graph2VR, it is possible to select the relevant triples, add those to the query pattern, transform the respective nodes into variables and then send the query to the SPARQL Endpoint. Besides the limit, the example query is the same one used as the example for Gruff (<xref rid="F4" ref-type="fig">Figure 4</xref>). This task might still be tricky for a number of reasons: not every mountain’s height is represented using the same predicate, some mountains might be missing in the database or there might be multiple instances encoding the same mountain. However, this is a limitation of DBpedia data, not Graph2VR.</p>
      <p>In many cases, DBpedia uses language tags that indicate the language used for the literal. Just asking for all labels results in many labels in different languages. We therefore added a language filter feature to Graph2VR that can be set via the settings menu. This allows users to request only labels with a specific language tag or no language tag at all. Once set, this is applied to every query. This may have consequences for the results, as it will not be indicated when the content is unavailable in the desired language or without a language tag. However, being able to set these conditions in the menu is more convenient than writing a SPARQL query to do so.</p>
    </sec>
    <sec id="s5-s3">
      <title>Usability survey</title>
      <p>To assess usability, we conducted a user evaluation with 34 participants. We were interested in how useful the application was for sensemaking, whether users enjoy the experience in VR and what influences their perceptions. By ‘sensemaking’, we mean the ability of the participant to answer questions or get understandable content from the semantic graph using the Graph2VR interface. We adopted this term from the Tarsier paper, which reused it from another earlier paper (<xref rid="R25" ref-type="bibr">25</xref>, <xref rid="R60" ref-type="bibr">60</xref>) . Some participants already had some experience with in GraphDB, SPARQL or Linked Data, but others did not. We guided all first-time users through the application, told them about their options and let them test the different features. Then, we asked the users a series of questions, described in <xref rid="s10" ref-type="sec">Supplementary Appendix B</xref>. The results are summarized in <xref rid="F11 F12" ref-type="fig">Figures 11 and 12</xref> with data from the usability survey available in <xref rid="s10" ref-type="sec">Supplementary Appendix B</xref>.</p>
      <fig position="float" id="F11" fig-type="figure">
        <label>Figure 11.</label>
        <caption>
          <p>Results usability questionnaire questions 9–15.</p>
        </caption>
        <graphic xlink:href="baae008f11" position="float"/>
      </fig>
      <fig position="float" id="F12" fig-type="figure">
        <label>Figure 12.</label>
        <caption>
          <p>Results usability questionnaire questions 19 and 20.</p>
        </caption>
        <graphic xlink:href="baae008f12" position="float"/>
      </fig>
    </sec>
    <sec id="s5-s4">
      <title>Evaluation results</title>
      <p>We identified several factors that influenced how much participants enjoyed testing Graph2VR and how effective they found the tool. One hypothesis was that people who are more used to playing computer games, especially if they have experience with VR glasses, might have an easier time using our application. It was uncertain whether age would have any effect, but we could imagine that younger participants, as digital natives, might find it easier. So there might be a weak correlation. To differentiate between enjoyment and effectiveness, both questions were asked after the VR experience and next to each other. We assumed that people who had a better experience, with fewer bugs, would rate their enjoyment as well as the tool’s effectivity for sensemaking more highly. We also expected a worse rating when the application had problems like poor readability or functions not working. An intuitive user interface makes the application more effective and enjoyable, whereas high complexity can make the application more interesting for experts but more difficult to use for beginners. During our studies, we determined that most participants found navigation and exploration quite easy, while the query-building seemed more challenging. Creating and modifying queries seemed to be noticeably easier for the ‘expert users’ who had at least some experience with Linked Data and writing SPARQL queries. Finally, we looked at the correlations between participants’ answers to the different questions. In addition to the Pearson correlation, <italic toggle="yes">P</italic>-values were calculated to determine the significance of those correlations. The complete correlation matrix is given in <xref rid="s10" ref-type="sec">Supplementary Appendix Table A6</xref>. In <xref rid="T2" ref-type="table">Table 2</xref>, we summarize the statistically significant correlations.</p>
      <table-wrap position="float" id="T2">
        <label>Table 2.</label>
        <caption>
          <p>Interpretation of the statistically significant correlations of the usability study results ordered by p-values.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col align="left" span="1"/>
          </colgroup>
          <tbody>
            <tr>
              <td align="left" rowspan="1" colspan="1">
                <inline-graphic xlink:href="baae008fa1.jpg"/>
              </td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>The questions, results and a more detailed evaluation can be found in Supplementary Appendix B. The findings are mostly as expected: having played video games more often is positively correlated with more intuitive navigation in the virtual world. Most participants also preferred flying to teleportation. We also found a positive correlation between navigation with gesture controls, requesting query results and exploring further details. We were curious whether age would have any significant effect on the experience of Graph2VR. Most participants (26/34) were between 30 and 50 years old. The sample size of people outside that age group was too small and probably not representative enough to draw any statistically significant conclusions. At the end of the experiment, we asked participants how much they enjoyed the experience and how effective Graph2VR is for sensemaking. Since both questions were asked at the same moment after the test, it is not a surprise that the answers are highly correlated. What is interesting is which factors are most relevant. For enjoying the experience, there is a strong correlation between the gesture controls and the graph exploration, as well as the exploration of further details. The exploration of further details is the second-to-last question during the VR session. A positive rating for this question implies that it was possible to request further results and that no major bug prevented this. When a problem occurred, the rating and the overall impression decreased. As Graph2VR was still under active development during the user study, some of the issues were fixed over time. Examples of this are the Limit slider that intersected with the scrollbar around the circle menu, nodes that were hard to select when they were zoomed too large and a layout option that led to many nodes being stapled at the same position.</p>
      <p>Setting up the query, selecting the relevant triples, setting the order (and optionally the limit) and requesting the results seemed to be the most complex part of the study for many participants, especially those with no previous experience with SPARQL. Query-building was also the most complex and error-prone part as we were still developing features that interfered with this process. It also takes users time to really understand how this feature works, especially for individuals who did not have any experience with SPARQL queries. Setting up SPARQL query patterns (like in Gruff) is one of the most relevant features of Graph2VR. While it seemed to already be complex for inexperienced users, users who already had some experience were asking us to implement even more SPARQL commands.</p>
    </sec>
  </sec>
  <sec id="s6">
    <title>Discussion</title>
    <p>Graph2VR is a prototype VR application for visualizing and exploring Linked Data in the form of 3D graphs. After exploring and testing multiple existing tools, we used Unity to create a user interface and DotNetRDF to connect to SPARQL Endpoints. We then tested our tool with a local Virtuoso server and DBpedia’s publicly accessible SPARQL Endpoint. To test the application and obtain feedback and suggestions for improvements, we conducted a usability study with 34 individuals who had never tried Graph2VR before.</p>
    <sec id="s6-s1">
      <title>User feedback</title>
      <p>Most testers were impressed and somewhat overwhelmed at the beginning, especially if they had never used a VR headset before. During the study, we guided them through the different functionalities and explained what they could do. During the tests, we noted their comments. After their VR session, we asked them to give feedback. This yielded valuable feedback from participants, including comments like ‘It felt good to be inside the world of the database’, ‘This is very fun to use and a great way of organizing and querying data’ and ‘I would love to try it out with other ontologies like Orphanet and connect it to applications like the RD-Connect Sample Catalogue (once that is in EMX2 so we can connect it)’ For explanation: The Entity Model eXtensible (EMX) is an internal metadata format of Molgenis, the current version is version 2 (<xref rid="R61" ref-type="bibr">61</xref>).</p>
      <p>We explicitly asked the testers about specific problems they experienced and any suggestions they had for the application. It takes some time to explore all the different functionalities that we have built into Graph2VR. One tester mentioned, [I] “need to get used to the tool, but after that, it is good :)”. Another tester found the movement in space initially challenging: ‘moving in space was a bit hard in the beginning, but started to feel more natural along the way’. The most commonly mentioned issue among the testers was the readability of the text in the application. While the menu and nodes and edges nearby were legible, the labels of distant nodes and edges were hard to read. This problem is related to the resolution of the VR headset, which needs to display texts that are further away in just a couple of pixels. One general critique was that the VR headset is still quite heavy. After long test sessions, some people felt somewhat dizzy or tired. That was not unexpected. When using VR glasses, especially for the first time, manufacturers recommend taking breaks at least every 30 min. Our test sessions (including some introduction and filling in the questionnaire) took 40–60 min, on average.</p>
      <p>We also asked more advanced users some additional questions, such as how Graph2VR compares to other conventional tools. Some of the experts mentioned that typing SPARQL queries by hand would still be faster than using the virtual keyboard and selecting the triples one-by-one. Graph2VR is somewhat limited by not having certain keywords, like Optional and Subqueries. Nevertheless, these users saw the potential that 3D visualization offers: ‘[Graph2VR is] much more fun, also allows much more data to be visualized, opens new possibilities’.</p>
      <p>In all, we received constructive feedback, and we have already fixed several of the issues mentioned by testers. For example:</p>
      <list list-type="bullet">
        <list-item>
          <p>The scrollbar no longer overlaps with the Limit slider.</p>
        </list-item>
        <list-item>
          <p>There was an issue with selecting nodes when the graph was scaled too big.</p>
        </list-item>
        <list-item>
          <p>Some testers asked for more visual feedback when clicking a button or to recognize when new nodes spawn. Both have been added.</p>
        </list-item>
        <list-item>
          <p>As we had at least two left-handed testers, we decided to add an option for left-handed people. It is not perfect as the menu still points to the right, but we have already received some positive feedback.</p>
        </list-item>
      </list>
      <p>Other issues persist and need to be addressed in future versions:</p>
      <list list-type="bullet">
        <list-item>
          <p>When the trigger is pressed while not pointing at any menu item, node,or edge, the menu should close. When scrolling down the scrollbar of the circle menu with the laser, it is easy to slip down the scrollbar, causing the menu to close. The circular scrollbar should be used instead via the slider knob on the scrollbar, a small ball that can be grabbed and then moved around.</p>
        </list-item>
        <list-item>
          <p>Some testers mentioned that the movement speed for flying in Graph2VR was quite fast and highly responsive, making it difficult to control the movement. This could be addressed in the future by adding an option to adjust the movement speed.</p>
        </list-item>
        <list-item>
          <p>When a graph bumps into the ‘floor’, the nodes collide with the floor and the graph deforms, which might result in a flat graph. This can also happen when a query is sent and a stack of result graphs is requested as the new graphs are spawned in the looking direction and might collide with the platform. One way to fix this is to trigger a layout algorithm.</p>
        </list-item>
      </list>
      <p>Overall, user study feedback was valuable for identifying areas of improvement. Participants’ comments complemented their scores and elucidated specific strengths and weaknesses of Graph2VR. For example, several participants noted the difficulty of reading node and edge labels in VR, and this was also reflected in lower scores for readability. Participants with previous VR experience found navigation easier, while those who had experience with SPARQL queries found it easier to create the query patterns.</p>
    </sec>
    <sec id="s6-s2">
      <title>Limitations of VR</title>
      <p>One of our expectations was that the almost unlimited space in VR and 3D could help users visualize more nodes at once. Based on the reviewers’ comments, we can now confirm this. However, one ongoing challenge in VR is the readability due to a limited resolution of the node and edge labels. To improve readability, we increased the font size of texts when hovering on them, but they remain hard to read when the graph is too small or the text is too far away. We also replaced the URIs of nodes and edges with their labels. If no label is available, Graph2VR displays the Compact URI, a shortened version of the URI in which the namespace is replaced by a prefix to shorten the URI. In addition to the textual representation, images, colours and shapes can also help to differentiate between different types of nodes. We had originally planned to allow federated queries for requesting data from multiple SPARQL endpoints at once, but this has not yet been implemented.</p>
    </sec>
    <sec id="s6-s3">
      <title>Ideas for future development</title>
      <p>In the current version of Graph2VR, the colour schema is derived from the VOWL specification (<xref rid="R30" ref-type="bibr">30</xref>). But not all the colours and shapes from VOWL are implemented yet.</p>
      <p>In the future, we would like to add further SPARQL commands to the application. Important SPARQL commands or keywords such as <italic toggle="yes">OPTIONAL</italic>, <italic toggle="yes">SERVICE</italic> (for federated queries), <italic toggle="yes">UNION</italic>, <italic toggle="yes">BIND</italic>, <italic toggle="yes">OFFSET</italic>, <italic toggle="yes">DISTINCT</italic>, <italic toggle="yes">COUNT</italic>, <italic toggle="yes">GROUP BY</italic> and <italic toggle="yes">MINUS</italic>, as well as <italic toggle="yes">FILTER</italic>, are supported by DotNetRDF but have not yet been implemented in the Graph2VR interface. In addition, an ‘undo’ button would be desirable, especially to restore accidentally deleted elements or graphs. Many test users found Graph2VR already quite complex, mainly because they were using it for the first time and did not have much experience with graph databases. Adding new functionalities will only increase the complexity further, so it might be a good idea to create a wizard to lead users through the query-building process in order to make this process easier. Another feature that should be implemented in the future is the ability to log and export some of the queries that have been performed. In addition, it should be possible to export the latest SPARQL query to use, e.g. in a browser. To find a specific object in the menu of outgoing nodes, another layer of entries would be helpful. This could be triggered when clicking the count of objects instead of the object itself. A similar feature would be to display all the existing connections between two nodes when setting the predicate for a new connection between two nodes. Finally, Graph2VR could be transformed into an augmented reality application in the future. This would especially be interesting as a multi-user application.</p>
    </sec>
  </sec>
  <sec id="s7">
    <title>Conclusion</title>
    <p>We developed Graph2VR, a prototype VR application for visualizing and exploring Linked Data in the form of 3D graphs, and conducted a usability study with 34 testers, which provided valuable feedback on the tool’s usability and areas for improvement. We believe that Graph2VR represents a novel and engaging way of visualizing and exploring Linked Data. The overall user experience reported during the usability study was positive, especially among more experienced users. While there are still some limitations and issues to be addressed, we are confident that, with further development and refinement, VR will provide tools for working with large Linked Data graphs.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>baae008_Supp</label>
      <media xlink:href="baae008_supp.zip"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>We would like to thank Gert-Jan Verheij, who leads the visualization group at the Center for Information Technology (CIT) and organizes the XRHub, where we presented Graph2VR. We thank our colleagues Prof. Dr Isabel Fortier and Dr Tina Wey from the Maelstrom Institute in Montreal for providing us with the REACH data as a test dataset. Of course, we thank all participants of our usability study, especially the expert users who gave us critical input and feedback on how to improve the application. Last but not least, we thank Kate Mc Intyre for editing this manuscript.</p>
  </ack>
  <sec sec-type="data-availability" id="s8">
    <title>Data availability</title>
    <p>The source code of Graph2VR is openly available on GitHub <ext-link xlink:href="http://github.com/molgenis/Graph2VR" ext-link-type="uri">http://github.com/molgenis/Graph2VR</ext-link>, under LGPL v3 license. The technical manual can be found in the attachments. The questions from the usability study, as well as the anonymous answers of the participants, can also be found in the attachments. A technical user manual for Graph2VR (version 1) can be found here: <ext-link xlink:href="https://doi.org/10.5281/zenodo.8040594" ext-link-type="uri">https://doi.org/10.5281/zenodo.8040594</ext-link>. A Graph2VR tutorial playlist is available on YouTube (<xref rid="R1" ref-type="bibr">1</xref>): <ext-link xlink:href="https://www.youtube.com/playlist?list=PLRQCsKSUyhNIdUzBNRTmE-_JmuiOEZbdH" ext-link-type="uri">https://www.youtube.com/playlist? list=PLRQCsKSUyhNIdUzBNRTmE-_JmuiOEZbdH</ext-link>.</p>
  </sec>
  <sec id="s9">
    <title>Author contributions</title>
    <p><bold>Alexander Kellmann</bold> contributed to conceptualization, software, writing—original draft, visualization, investigation, data curation, formal analysis</p>
    <p>Max Postema contributed to software and visualization</p>
    <p>Pjotr Svetachov contributed to software and visualization</p>
    <p>Joris de Keijser contributed to software and visualization</p>
    <p>Becca Wilson: contributed to methodology, writing—original draft, writing—review and editing, suspervision</p>
    <p>Esther van Enckevort contributed to supervision, writing—review and editing</p>
    <p>Morris A. Swertz contributed to supervision, writing—review and editing</p>
  </sec>
  <sec id="s10">
    <title>Supplementary Material</title>
    <p><xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref> is available at <italic toggle="yes">Database</italic> online.</p>
  </sec>
  <sec id="s11">
    <title>Funding</title>
    <p>EUCAN-Connect, a federated FAIR platform enabling large-scale analysis of high-value cohort data connecting Europe and Canada in personalized health, which is funded by the European Union’s Horizon 2020 research and innovation programme under grant agreement No 824 989; and a UKRI Innovation Fellowship with HDR UK (MR/S003959/2; R.C.W.).</p>
  </sec>
  <sec sec-type="COI-statement" id="s12">
    <title>Conflict of interest statement</title>
    <p>The authors declare no conflict of interest.</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="R1">
      <label>1.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kellmann</surname><given-names>A.</given-names></string-name> and <string-name><surname>Postema</surname><given-names>M.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Graph2VR tutorial part 1 – 5</article-title>. <ext-link xlink:href="https://www.youtube.com/playlist?list=PLRQCsKSUyhNIdUzBNRTmE-_JmuiOEZbdH" ext-link-type="uri">https://www.youtube.com/playlist?list=PLRQCsKSUyhNIdUzBNRTmE-_JmuiOEZbdH</ext-link> (<comment>4 December 2023, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wilkinson</surname><given-names>M.D.</given-names></string-name>, <string-name><surname>Dumontier</surname><given-names>M.</given-names></string-name>, <string-name><surname>Aalbersberg</surname><given-names>I.J.J.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2016</year>) <article-title>The FAIR Guiding Principles for scientific data management and stewardship</article-title>. <source><italic toggle="yes">Sci. Data.</italic></source>, <volume>3</volume>, <page-range>160018</page-range>.</mixed-citation>
    </ref>
    <ref id="R3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Croset</surname><given-names>S.</given-names></string-name>, <string-name><surname>Rupp</surname><given-names>J.</given-names></string-name> and <string-name><surname>Romacker</surname><given-names>M.</given-names></string-name></person-group> (<year>2016</year>) <article-title>Flexible data integration and curation using a graph-based approach</article-title>. <source><italic toggle="yes">Bioinformatics</italic></source>, <volume>32</volume>, <fpage>918</fpage>–<lpage>925</lpage>.<pub-id pub-id-type="pmid">26556384</pub-id>
</mixed-citation>
    </ref>
    <ref id="R4">
      <label>4.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Berners-Lee</surname><given-names>T.</given-names></string-name> and <string-name><surname>Fischetti</surname><given-names>M.</given-names></string-name></person-group> (<year>2000</year>) <source><italic toggle="yes">Weaving the Web: The Original Design and Ultimate Destiny of the World Wide Web by its inventor.</italic></source>  <publisher-name>HarperCollins</publisher-name>, <publisher-loc>San Francisco, CA</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="R5">
      <label>5.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Berners-Lee</surname><given-names>T.</given-names></string-name></person-group> (<year>2009</year>) <article-title>The next web</article-title>. <ext-link xlink:href="https://www.ted.com/talks/tim_berners_lee_the_next_web/transcript" ext-link-type="uri">https://www.ted.com/talks/tim_berners_lee_the_next_web/transcript</ext-link>.</mixed-citation>
    </ref>
    <ref id="R6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Desimoni</surname><given-names>F.</given-names></string-name> and <string-name><surname>Po</surname><given-names>L.</given-names></string-name></person-group> (<year>2020</year>) <article-title>Empirical evaluation of Linked Data visualization tools</article-title>. <source><italic toggle="yes">Future Gener. Comput. Syst.</italic></source>, <volume>112</volume>, <fpage>258</fpage>–<lpage>282</lpage>.</mixed-citation>
    </ref>
    <ref id="R7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Callahan</surname><given-names>A.</given-names></string-name>, <string-name><surname>Cruz-Toledo</surname><given-names>J.</given-names></string-name> and <string-name><surname>Dumontier</surname><given-names>M.</given-names></string-name></person-group> (<year>2013</year>) <article-title>Ontology-based querying with Bio2RDF’s linked open ata</article-title>. <source><italic toggle="yes">J. Biomed. Semant.</italic></source>, <volume>4 Suppl 1</volume>, <fpage>1</fpage>–<lpage>13</lpage>.</mixed-citation>
    </ref>
    <ref id="R8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fu</surname><given-names>G.</given-names></string-name>, <string-name><surname>Batchelor</surname><given-names>C.</given-names></string-name>, <string-name><surname>Dumontier</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2015</year>) <article-title>PubChemRDF: Towards the semantic annotation of PubChem compound and substance databases</article-title>. <source><italic toggle="yes">J. Cheminformatics</italic></source>, <volume>7</volume>, <page-range>34</page-range>.</mixed-citation>
    </ref>
    <ref id="R9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Galgonek</surname><given-names>J.</given-names></string-name>, <string-name><surname>Hurt</surname><given-names>T.</given-names></string-name>, <string-name><surname>Michlíková</surname><given-names>V.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2016</year>) <article-title>Advanced SPARQL querying in small molecule databases</article-title>. <source><italic toggle="yes">J. Cheminformatics</italic></source>, <volume>8</volume>, <page-range>31</page-range>.</mixed-citation>
    </ref>
    <ref id="R10">
      <label>10.</label>
      <mixed-citation publication-type="other">(<year>2015</year>) <article-title>RDF - Semantic Web Standards</article-title>. <ext-link xlink:href="https://www.w3.org/RDF/" ext-link-type="uri">https://www.w3.org/RDF/</ext-link> (<comment>12 December 2022, date last accepted</comment>).</mixed-citation>
    </ref>
    <ref id="R11">
      <label>11.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Coffey</surname><given-names>D.</given-names></string-name><string-name><surname>Malbraaten</surname><given-names>N.</given-names></string-name><string-name><surname>Le</surname><given-names>T.</given-names></string-name></person-group>  <etal>et al.</etal> (<year>2011</year>) <part-title>Slice WIM</part-title>. In <person-group person-group-type="editor"><string-name><surname>Garland</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Wang</surname>, <given-names>R.</given-names></string-name></person-group> (<edition>eds.</edition>), <source>Symposium on Interactive 3D Graphics and Games</source>. <publisher-name>ACM</publisher-name>, <publisher-loc>New York, NY, USA</publisher-loc>, <fpage>pp. 191</fpage>–<lpage>198</lpage>.</mixed-citation>
    </ref>
    <ref id="R12">
      <label>12.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Czauderna</surname><given-names>T.</given-names></string-name><string-name><surname>Haga</surname><given-names>J.</given-names></string-name><string-name><surname>Kim</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al.</etal> (<year>2018</year>) <part-title>Immersive Analytics Applications in Life and Health Sciences</part-title>. In <person-group person-group-type="editor"><string-name><surname>Marriott</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Schreiber</surname>, <given-names>F.</given-names></string-name> and <string-name><surname>Dwyer</surname>, <given-names>T.</given-names></string-name></person-group>  <etal>et al</etal>. (<edition>eds.</edition>), <source>Immersive Analytics</source>. <publisher-name>Springer International Publishing</publisher-name>, <publisher-loc>Cham</publisher-loc>, <volume>611190</volume>, <fpage>pp. 289</fpage>–<lpage>330</lpage>.</mixed-citation>
    </ref>
    <ref id="R13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lau</surname><given-names>C.W.</given-names></string-name>, <string-name><surname>Qu</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Draper</surname><given-names>D.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2022</year>) <article-title>Virtual reality for the observation of oncology models (VROOM): immersive analytics for oncology patient cohorts</article-title>. <source><italic toggle="yes">Sci. Rep.</italic></source>, <volume>12</volume>, <page-range>11337</page-range>.</mixed-citation>
    </ref>
    <ref id="R14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Qu</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Lau</surname><given-names>C.W.</given-names></string-name>, <string-name><surname>Simoff</surname><given-names>S.J.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2022</year>) <article-title>Review of innovative immersive technologies for healthcare applications</article-title>. <source><italic toggle="yes">Innovations in Digital Health, Diagnostics, and Biomarkers</italic></source>, <volume>2</volume>, <fpage>27</fpage>–<lpage>39</lpage>.</mixed-citation>
    </ref>
    <ref id="R15">
      <label>15.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>McCrae</surname><given-names>J.P.</given-names></string-name></person-group> (<year>2022</year>) <article-title>The Linked Open Data Cloud</article-title>. <ext-link xlink:href="https://lod-cloud.net/" ext-link-type="uri">https://lod-cloud.net/</ext-link> (<comment>13 January 2023, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R16">
      <label>16.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Deligiannidis</surname><given-names>L.</given-names></string-name><string-name><surname>Sheth</surname><given-names>A.P.</given-names></string-name><string-name><surname>Aleman-Meza</surname><given-names>B.</given-names></string-name></person-group> (<year>2006</year>) <part-title>Semantic Analytics Visualization</part-title>. In <person-group person-group-type="editor"><string-name><surname>Hutchison</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Kanade</surname>, <given-names>T.</given-names></string-name> and <string-name><surname>Kittler</surname>, <given-names>J.</given-names></string-name></person-group>  <etal>et al</etal>. (eds.), <source>Intelligence and Security Informatics</source>. <publisher-name>Springer Berlin Heidelberg</publisher-name>, <publisher-loc>Berlin, Heidelberg</publisher-loc>, <volume>3975</volume>, <fpage>pp. 48</fpage>–<lpage>59</lpage>.</mixed-citation>
    </ref>
    <ref id="R17">
      <label>17.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>Sony Green &amp; Robert Allison</collab></person-group>. (<year>2019</year>) <article-title>KINEVIZ GraphXR: How to GraphXR: for GraphXR v2.2.1</article-title>. <ext-link xlink:href="https://static1.squarespace.com/static/5c58b86e8dfc8c2d0d700050/t/5df2b6134e0d57635d14df4b/1576187456752/How+to+GraphXR.pdf" ext-link-type="uri">https://static1.squarespace.com/static/5c58b86e8dfc8c2d0d700050/t/5df2b6134e0d57635d14df4b/1576187456752/How+to+GraphXR.pdf</ext-link> (<comment>24 August 2020, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R18">
      <label>18.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Asturiano</surname><given-names>V.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2021</year>) <article-title>vasturiano/3d-force-graph</article-title>. <ext-link xlink:href="https://github.com/vasturiano/3d-force-graph/" ext-link-type="uri">https://github.com/vasturiano/3d-force-graph/</ext-link> (<comment>18 February 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R19">
      <label>19.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Camarda</surname><given-names>D.V.</given-names></string-name>, <string-name><surname>Mazzini</surname><given-names>S.</given-names></string-name> and <string-name><surname>Antonuccio</surname><given-names>A</given-names></string-name></person-group>. (<year>2012</year>) <article-title>LodLive, exploring the web of data</article-title>. <italic toggle="yes">Proceedings of the 8th International Conference on Semantic Systems</italic>. <publisher-name>ACM Digital Library</publisher-name>, <conf-loc>New York, NY, USA</conf-loc>. <page-range>p. 197</page-range>.</mixed-citation>
    </ref>
    <ref id="R20">
      <label>20.</label>
      <mixed-citation publication-type="other">(<year>2020</year>) <article-title>GraphDB</article-title>. <ext-link xlink:href="https://graphdb.ontotext.com/" ext-link-type="uri">https://graphdb.ontotext.com/</ext-link> (<comment>5 March 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R21">
      <label>21.</label>
      <mixed-citation publication-type="other">(<year>2022</year>) <article-title>Toran (VR tech demo on steam)</article-title>. <ext-link xlink:href="https://store.steampowered.com/app/720300/Toran/" ext-link-type="uri">https://store.steampowered.com/app/720300/Toran/</ext-link> (<comment>9 March 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R22">
      <label>22.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>OWL Web Ontology Language Overview</collab></person-group>. (<year>2009</year>) <ext-link xlink:href="https://www.w3.org/TR/owl-features/" ext-link-type="uri">https://www.w3.org/TR/owl-features/</ext-link> (<comment>19 May 2017, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lohmann</surname><given-names>S.</given-names></string-name>, <string-name><surname>Negru</surname><given-names>S.</given-names></string-name>, <string-name><surname>Haag</surname><given-names>F.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2016</year>) <article-title>Visualizing Ontologies with VOWL</article-title>. <source><italic toggle="yes">Semantic Web</italic></source>, <volume>7</volume>, <fpage>399</fpage>–<lpage>419</lpage>.</mixed-citation>
    </ref>
    <ref id="R24">
      <label>24.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>Franz Inc</collab></person-group>. (<year>2020</year>) <article-title>Gruff</article-title>. <ext-link xlink:href="https://allegrograph.com/products/gruff/" ext-link-type="uri">https://allegrograph.com/products/gruff/</ext-link> (<comment>3 November 2020, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Viola</surname><given-names>F.</given-names></string-name>, <string-name><surname>Roffia</surname><given-names>L.</given-names></string-name>, <string-name><surname>Antoniazzi</surname><given-names>F.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2018</year>) <article-title>Interactive 3D Exploration of RDF Graphs through Semantic Planes</article-title>. <source><italic toggle="yes">Future Internet</italic></source>, <volume>10</volume>, <fpage>1</fpage>–<lpage>30</lpage>.</mixed-citation>
    </ref>
    <ref id="R26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dudáš</surname><given-names>M.</given-names></string-name>, <string-name><surname>Lohmann</surname><given-names>S.</given-names></string-name>, <string-name><surname>Svátek</surname><given-names>V.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2018</year>) <article-title>Ontology visualization methods and tools: a survey of the state of the art</article-title>. <source><italic toggle="yes">Knowl. Eng. Rev.</italic></source>, <page-range>33</page-range>.</mixed-citation>
    </ref>
    <ref id="R27">
      <label>27.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Lanzenberger</surname><given-names>M.</given-names></string-name>, <string-name><surname>Sampson</surname><given-names>J.</given-names></string-name> and <string-name><surname>Rester</surname><given-names>M</given-names></string-name></person-group>. (<year>2009</year>) <article-title>Visualization in Ontology Tools</article-title>. <italic toggle="yes">International Conference on Complex, Intelligent and Software Intensive Systems</italic>. <publisher-name>IEEE</publisher-name>, <conf-loc>Piscataway, NJ</conf-loc>. <fpage>pp. 705</fpage>–<lpage>711</lpage>.</mixed-citation>
    </ref>
    <ref id="R28">
      <label>28.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Palma</surname><given-names>R.</given-names></string-name></person-group> (<year>2021</year>) <article-title>A Knowledge Graph for the Agri-Food Sector</article-title>. <ext-link xlink:href="https://blog.metaphacts.com/a-knowledge-graph-for-the-agri-food-sector" ext-link-type="uri">https://blog.metaphacts.com/a-knowledge-graph-for-the-agri-food-sector</ext-link> (<comment>8 March 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Musen</surname><given-names>M.A.</given-names></string-name></person-group> (<year>2015</year>) <article-title>The protégé Project: a look back and a look forward</article-title>. <source><italic toggle="yes">AI matters</italic></source>, <volume>1</volume>, <fpage>4</fpage>–<lpage>12</lpage>.<pub-id pub-id-type="pmid">27239556</pub-id>
</mixed-citation>
    </ref>
    <ref id="R30">
      <label>30.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Steffen Lohmann</surname><given-names>S.N.</given-names></string-name></person-group> (<year>2019</year>) <article-title>VOWL: Visual Notation for OWL Ontologies</article-title>. <ext-link xlink:href="http://vowl.visualdataweb.org/" ext-link-type="uri">http://vowl.visualdataweb.org/</ext-link> (<comment>13 January 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R31">
      <label>31.</label>
      <mixed-citation publication-type="other">(<year>2017</year>) <article-title>QueryVOWL</article-title>. <ext-link xlink:href="http://vowl.visualdataweb.org/queryvowl/queryvowl.html" ext-link-type="uri">http://vowl.visualdataweb.org/queryvowl/queryvowl.html</ext-link> (<comment>13 January 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R32">
      <label>32.</label>
      <mixed-citation publication-type="other">(<year>2019</year>) <article-title>WebVOWL</article-title>. <ext-link xlink:href="http://www.visualdataweb.de/webvowl/#" ext-link-type="uri">http://www.visualdataweb.de/webvowl/#</ext-link> (<comment>13 January 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R33">
      <label>33.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><given-names>Fabio</given-names><surname>V.</surname></string-name></person-group>  <etal>et al</etal>. (<year>2018</year>) <article-title>Tarsier – exploring DBpedia</article-title>. <ext-link xlink:href="https://www.youtube.com/watch?v=OgoxFWAb1vQ" ext-link-type="uri">https://www.youtube.com/watch?v=OgoxFWAb1vQ</ext-link> (<comment>2 December 2022, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R34">
      <label>34.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kellmann</surname><given-names>A.J.</given-names></string-name>, <string-name><surname>Postema</surname><given-names>M.</given-names></string-name>, <string-name><surname>Keijser</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2023</year>) <article-title>Graph2VR Manual</article-title>. <source><italic toggle="yes">Zenodo</italic></source>, <fpage>1</fpage>–<lpage>20</lpage>.</mixed-citation>
    </ref>
    <ref id="R35">
      <label>35.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Pienta</surname><given-names>R.</given-names></string-name>, <string-name><surname>Abello</surname><given-names>J.</given-names></string-name>, <string-name><surname>Kahng</surname><given-names>M</given-names></string-name></person-group>. <etal>et al.</etal> (<year>2015</year>) <article-title>Scalable graph exploration and visualization: Sensemaking challenges and opportunities</article-title>. In <italic toggle="yes">2015 International Conference on Big Data and Smart Computing (BIGCOMP)</italic>, <conf-loc>Jeju, Republic of Korea</conf-loc>, <conf-date>09.02.2015-11.02.2015</conf-date>. <publisher-name>IEEE</publisher-name>, <fpage>pp. 271</fpage>–<lpage>278</lpage>.</mixed-citation>
    </ref>
    <ref id="R36">
      <label>36.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Sund</surname><given-names>D.</given-names></string-name></person-group> (<year>2016</year>) <article-title>Comparison of Visualization Algorithms for Graphs and Implementation of Visualization Algorithm for Multi-Touch table using JavaFX</article-title>, <source>Bachelor Thesis</source>, <publisher-name>Linköping</publisher-name>.</mixed-citation>
    </ref>
    <ref id="R37">
      <label>37.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fruchterman</surname><given-names>T.M.J.</given-names></string-name> and <string-name><surname>Reingold</surname><given-names>E.M.</given-names></string-name></person-group> (<year>1991</year>) <article-title>Graph drawing by force-directed placement</article-title>. <source><italic toggle="yes">Softw. - Pract. Exp.</italic></source>, <volume>21</volume>, <fpage>1129</fpage>–<lpage>1164</lpage>.</mixed-citation>
    </ref>
    <ref id="R38">
      <label>38.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barnes</surname><given-names>J.</given-names></string-name> and <string-name><surname>Hut</surname><given-names>P.</given-names></string-name></person-group> (<year>1986</year>) <article-title>A hierarchical O(N log N) force-calculation algorithm</article-title>. <source><italic toggle="yes">Nature</italic></source>, <volume>324</volume>, <fpage>446</fpage>–<lpage>449</lpage>.</mixed-citation>
    </ref>
    <ref id="R39">
      <label>39.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Swinehart</surname><given-names>C.</given-names></string-name></person-group> (<year>2011</year>) <article-title>The Barnes-Hut Algorithm</article-title>. <ext-link xlink:href="http://arborjs.org/docs/barnes-hut" ext-link-type="uri">http://arborjs.org/docs/barnes-hut</ext-link> (<comment>3 December 2020, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R40">
      <label>40.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kamada</surname><given-names>T.</given-names></string-name> and <string-name><surname>Kawai</surname><given-names>S.</given-names></string-name></person-group> (<year>1989</year>) <article-title>An algorithm for drawing general undirected graphs</article-title>. <source><italic toggle="yes">Inf. Process. Lett.</italic></source>, <volume>31</volume>, <fpage>7</fpage>–<lpage>15</lpage>.</mixed-citation>
    </ref>
    <ref id="R41">
      <label>41.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Teyseyre</surname><given-names>A.R.</given-names></string-name> and <string-name><surname>Campo</surname><given-names>M.R.</given-names></string-name></person-group> (<year>2009</year>) <article-title>An overview of 3D software visualization</article-title>. <source><italic toggle="yes">IEEE Trans. Vis. Comput. Graph.</italic></source>, <volume>15</volume>, <fpage>87</fpage>–<lpage>105</lpage>.<pub-id pub-id-type="pmid">19008558</pub-id>
</mixed-citation>
    </ref>
    <ref id="R42">
      <label>42.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Batchelor</surname><given-names>C.</given-names></string-name><string-name><surname>Brenninkmeijer</surname><given-names>C.Y.A.</given-names></string-name><string-name><surname>Chichester</surname><given-names>C.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2014</year>) <part-title>Scientific Lenses to Support Multiple Views over Linked Chemistry Data</part-title>. <source><italic toggle="yes">The Semantic Web - ISWC 2014.</italic></source> In: <person-group person-group-type="editor"><string-name><surname>Mika</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Tudorache</surname>, <given-names>T.</given-names></string-name> and <string-name><surname>Bernstein</surname>, <given-names>A.</given-names></string-name></person-group> (eds.), <publisher-name>Springer International Publishing</publisher-name>, <publisher-loc>Cham</publisher-loc>, <fpage>98</fpage>–<lpage>113</lpage>.</mixed-citation>
    </ref>
    <ref id="R43">
      <label>43.</label>
      <mixed-citation publication-type="other">(<year>2021</year>) <article-title>Noda (by Coding Leap - Steam link)</article-title>. <ext-link xlink:href="https://store.steampowered.com/app/578060/Noda/" ext-link-type="uri">https://store.steampowered.com/app/578060/Noda/</ext-link> (<comment>11 May 2021, dare last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R44">
      <label>44.</label>
      <mixed-citation publication-type="other">(<year>2020</year>) <article-title>aestheticinteractive/Hover-UI-Kit</article-title>. <ext-link xlink:href="https://github.com/aestheticinteractive/Hover-UI-Kit" ext-link-type="uri">https://github.com/aestheticinteractive/Hover-UI-Kit</ext-link> (<comment>10 Decemebr 2020, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R45">
      <label>45.</label>
      <mixed-citation publication-type="other">(<year>2021</year>) <article-title>Getting Started with metaphactory</article-title>. <ext-link xlink:href="https://help.metaphacts.com/resource/Help:Start" ext-link-type="uri">https://help.metaphacts.com/resource/Help:Start</ext-link> (<comment>5 March 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R46">
      <label>46.</label>
      <mixed-citation publication-type="other">(<year>2022</year>) <article-title>VRKeys|Input Management|Unity Asset Store</article-title>. <ext-link xlink:href="https://assetstore.unity.com/packages/tools/input-management/vrkeys-99222" ext-link-type="uri">https://assetstore.unity.com/packages/tools/input-management/vrkeys-99222</ext-link> (<comment>24 March 2022, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R47">
      <label>47.</label>
      <mixed-citation publication-type="other">(<year>2021</year>) <article-title>OpenLink Software: Virtuoso Homepage</article-title>. <ext-link xlink:href="https://virtuoso.openlinksw.com/" ext-link-type="uri">https://virtuoso.openlinksw.com/</ext-link> (<comment>25 February 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R48">
      <label>48.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>Unity Technologies</collab></person-group>.<article-title>Unity</article-title>. <ext-link xlink:href="https://unity.com/" ext-link-type="uri">https://unity.com/</ext-link> (<comment>24 February 2022, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R49">
      <label>49.</label>
      <mixed-citation publication-type="other">(<year>2022</year>) <article-title>MozillaReality/unity-webxr-export: assets for creating WebXR-enabled Unity3D projects</article-title>. <ext-link xlink:href="https://github.com/mozillareality/unity-webxr-export" ext-link-type="uri">https://github.com/mozillareality/unity-webxr-export</ext-link> (<comment>28 September 2022, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R50">
      <label>50.</label>
      <mixed-citation publication-type="other"><article-title>tenforce/virtuoso</article-title>. <ext-link xlink:href="https://hub.docker.com/r/tenforce/virtuoso/" ext-link-type="uri">https://hub.docker.com/r/tenforce/virtuoso/</ext-link> (<comment>3 March 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R51">
      <label>51.</label>
      <mixed-citation publication-type="other">(<year>2020</year>) <article-title>DotNetRDF</article-title>. <ext-link xlink:href="https://www.dotnetrdf.org/" ext-link-type="uri">https://www.dotnetrdf.org/</ext-link> (<comment>15 September 2020, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R52">
      <label>52.</label>
      <mixed-citation publication-type="other">(<year>2020</year>) <article-title>dotnetrdf/dotnetrdf</article-title>. <ext-link xlink:href="https://github.com/dotnetrdf/dotnetrdf/wiki/UserGuide-Querying-With-SPARQL" ext-link-type="uri">https://github.com/dotnetrdf/dotnetrdf/wiki/UserGuide-Querying-With-SPARQL</ext-link> (<comment>12 August 2020, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R53">
      <label>53.</label>
      <mixed-citation publication-type="other">(<year>2020</year>) <article-title>Release SteamVR Unity Plugin v2.6.0b4 - SDK 1.13.10 - ValveSoftware/steamvr_unity_plugin</article-title>. <ext-link xlink:href="https://github.com/ValveSoftware/steamvr_unity_plugin/releases/tag/2.6.0b4" ext-link-type="uri">https://github.com/ValveSoftware/steamvr_unity_plugin/releases/tag/2.6.0b4</ext-link> (<comment>12 August 2020, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R54">
      <label>54.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>The Khronos Group</collab></person-group>. (<year>2016</year>) <article-title>OpenXR - High-performance access to AR and VR – collectively known as XR – platforms and devices</article-title>. <ext-link xlink:href="https://www.khronos.org/openxr/" ext-link-type="uri">https://www.khronos.org/openxr/</ext-link> (<comment>8 March 2022, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R55">
      <label>55.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Bastian</given-names><surname>M.</surname></string-name>, <string-name><given-names>Heymann</given-names><surname>S.</surname></string-name>, and <string-name><given-names>Jacomy</given-names><surname>M.</surname></string-name></person-group> (<year>2009</year>) <article-title>Gephi: An Open Source Software for Exploring and Manipulating Networks</article-title>. <source>Proceedings of the International AAAI Conference on Web and Social Media</source>, <volume>3</volume>, <fpage>361</fpage>–<lpage>362</lpage>.</mixed-citation>
    </ref>
    <ref id="R56">
      <label>56.</label>
      <mixed-citation publication-type="other">(<year>2022</year>) <article-title>Using Full Text Search in SPARQL</article-title>. <ext-link xlink:href="https://docs.openlinksw.com/virtuoso/rdfsparqlrulefulltext/" ext-link-type="uri">https://docs.openlinksw.com/virtuoso/rdfsparqlrulefulltext/</ext-link> (<comment>2 December 2022, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R57">
      <label>57.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Bast</surname><given-names>H.</given-names></string-name>, <string-name><surname>Kalmbach</surname><given-names>J.</given-names></string-name>, <string-name><surname>Klumpp</surname><given-names>T</given-names></string-name></person-group>. <etal>et al.</etal> (<year>2022</year>) <article-title>Efficient and Effective SPARQL Autocompletion on Very Large Knowledge Graphs</article-title>. <italic toggle="yes">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management. ACM</italic>. <conf-loc>New York, NY, USA</conf-loc>. <fpage>pp. 2893</fpage>–<lpage>2902</lpage>.</mixed-citation>
    </ref>
    <ref id="R58">
      <label>58.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Auer</surname><given-names>S.</given-names></string-name>, <string-name><surname>Bizer</surname><given-names>C.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2007</year>) <part-title>DBpedia: A Nucleus for a Web of Open Data</part-title>. <source><italic toggle="yes">The Semantic Web.</italic></source>, Vol. <volume>4825</volume>, <series>Lecture Notes in Computer Science</series>, <publisher-loc>Berlin, Heidelberg</publisher-loc>, <publisher-name>Springer</publisher-name>, pp. <fpage>722</fpage>–<lpage>735</lpage>.</mixed-citation>
    </ref>
    <ref id="R59">
      <label>59.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Cimiano</surname><given-names>P.</given-names></string-name></person-group> (<year>2015</year>) <source><italic toggle="yes">Personal Communication During the “Semantic Web” Lecture at the</italic></source>  <publisher-name>University of Bielefeld</publisher-name>.</mixed-citation>
    </ref>
    <ref id="R60">
      <label>60.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Motta</surname><given-names>E.</given-names></string-name>, <string-name><surname>Mulholland</surname><given-names>P.</given-names></string-name> and <string-name><surname>Peroni</surname><given-names>S</given-names></string-name></person-group>. (<year>2011</year>) <article-title>A Novel Approach to Visualizing and Navigating Ontologies</article-title>. <volume>7031</volume>, <fpage>470</fpage>–<lpage>483</lpage>.</mixed-citation>
    </ref>
    <ref id="R61">
      <label>61.</label>
      <mixed-citation publication-type="other">(<year>2024</year>) <article-title>Molgenis 6 user documentation</article-title>. <part-title>Formats</part-title>. <ext-link xlink:href="https://molgenis.gitbooks.io/molgenis/content/v/6.0/user_documentation/guide-upload.html" ext-link-type="uri">https://molgenis.gitbooks.io/molgenis/content/v/6.0/user_documentation/guide-upload.html</ext-link>.</mixed-citation>
    </ref>
    <ref id="R62">
      <label>62.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pirch</surname><given-names>S.</given-names></string-name>, <string-name><surname>Müller</surname><given-names>F.</given-names></string-name>, <string-name><surname>Iofinova</surname><given-names>E.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2021</year>) <article-title>The VRNetzer platform enables interactive network analysis in Virtual Reality</article-title>. <source><italic toggle="yes">Nat. Commun.</italic></source>, <volume>12</volume>, <page-range>2432</page-range>.</mixed-citation>
    </ref>
    <ref id="R63">
      <label>63.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Dooley</surname><given-names>D.</given-names></string-name>, <string-name><surname>Nguyen</surname><given-names>M.</given-names></string-name> and <string-name><surname>Hsiao</surname><given-names>W.</given-names></string-name></person-group> (<year>2023</year>) <article-title>3D Visualization of Application Ontology Class Hierarchies</article-title>. <ext-link xlink:href="http://genepio.org/ontotrek" ext-link-type="uri">http://genepio.org/ontotrek</ext-link> (<comment>19 May 2020, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R64">
      <label>64.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>Kineviz inc</collab></person-group>. (<year>2022</year>) <article-title>Login</article-title>. <ext-link xlink:href="https://graphxr.kineviz.com/login" ext-link-type="uri">https://graphxr.kineviz.com/login</ext-link> (<comment>8 March 2022, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R65">
      <label>65.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>McVeigh-Schultz</surname><given-names>J.</given-names></string-name></person-group> (<year>2018</year>) <source><italic toggle="yes">Immersive Human Networks: An Exploration of How VR Network Analysis Can Transform Sensemaking and Help Organizations Become More Agile INSTITUTE FOR THE FUTURE 201 Hamilton Avenue</italic></source>, <publisher-loc>Palo Alto, CA 94301</publisher-loc>, <ext-link xlink:href="https://www.iftf.org/fileadmin/user_upload/images/More_Projects_Images/IFTF_Immersive_Human_Networks_FINAL_READER_100918__1_.pdf" ext-link-type="uri">https://www.iftf.org/fileadmin/user_upload/images/More_Projects_Images/IFTF_Immersive_Human_Networks_FINAL_READER_100918__1_.pdf</ext-link>.</mixed-citation>
    </ref>
    <ref id="R66">
      <label>66.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Capece</surname><given-names>N.</given-names></string-name>, <string-name><surname>Erra</surname><given-names>U.</given-names></string-name> and <string-name><surname>Grippa</surname><given-names>J</given-names></string-name></person-group>. (<year>2018</year>) <article-title>Graphvr: A virtual reality tool for the exploration of graphs with htc vive system</article-title>. In: <italic toggle="yes">2018 22nd International Conference Information Visualisation (iV 2018)</italic>. <publisher-name>IEEE</publisher-name>, <conf-loc>Piscataway, NJ</conf-loc>, <fpage>pp. 448</fpage>–<lpage>453</lpage>.</mixed-citation>
    </ref>
    <ref id="R67">
      <label>67.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Panahi</surname><given-names>A.</given-names></string-name></person-group> (<year>2017</year>) <article-title>Big data visualization platform for mixed reality</article-title>, <publisher-name>VCU Libraries</publisher-name>, <pub-id pub-id-type="doi">10.25772/6MDD-2B85</pub-id>.</mixed-citation>
    </ref>
    <ref id="R68">
      <label>68.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Radics</surname><given-names>P.J.</given-names></string-name>, <string-name><surname>Polys</surname><given-names>N.F.</given-names></string-name>, <string-name><surname>Neuman</surname><given-names>S.P.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2015</year>) <article-title>OSNAP! Introducing the open semantic network analysis platform</article-title>. <source><italic toggle="yes">Visualization and Data Analysis 2015</italic></source>, <volume>9397</volume>, <fpage>38</fpage>–<lpage>52</lpage>.</mixed-citation>
    </ref>
    <ref id="R69">
      <label>69.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>OpenGraphiti: Data Visualization Framework</collab></person-group>. (<year>2014</year>) <ext-link xlink:href="https://www.opengraphiti.com/" ext-link-type="uri">https://www.opengraphiti.com/</ext-link> (<comment>7 March 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R70">
      <label>70.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>GitHub</collab></person-group>. (<year>2014</year>) <article-title>opendns/dataviz</article-title>, <ext-link xlink:href="https://github.com/opendns/dataviz" ext-link-type="uri">https://github.com/opendns/dataviz</ext-link> (<comment>7 March 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R71">
      <label>71.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Bremer</surname><given-names>E.</given-names></string-name> and <collab>Haylyn</collab></person-group>. (<year>2014</year>) <ext-link xlink:href="http://haylyn.io/" ext-link-type="uri">http://haylyn.io/</ext-link> (<comment>6 January 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R72">
      <label>72.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>LodLive – browsing the Web of Data</collab></person-group>. (<year>2012</year>) <ext-link xlink:href="http://lodlive.it/" ext-link-type="uri">http://lodlive.it/</ext-link> (<comment>15 Septemeber 2019, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R73">
      <label>73.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Philipp</surname>  <given-names>H.</given-names></string-name>, <string-name><surname>Steffen</surname>  <given-names>L.</given-names></string-name>, <string-name><surname>Timo</surname>  <given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2010</year>) <article-title>RelFinder – Visual Data Web</article-title>. <ext-link xlink:href="http://www.visualdataweb.org/relfinder.php" ext-link-type="uri">http://www.visualdataweb.org/relfinder.php</ext-link> (<comment>25 February 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R74">
      <label>74.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pavlopoulos</surname><given-names>G.A.</given-names></string-name>, <string-name><surname>O’Donoghue</surname><given-names>S.I.</given-names></string-name>, <string-name><surname>Satagopam</surname><given-names>V.P.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2008</year>) <article-title>Arena3D: visualization of biological networks in 3D</article-title>. <source><italic toggle="yes">BMC Syst. Biol.</italic></source>, <volume>2</volume>, <page-range>104</page-range>.</mixed-citation>
    </ref>
    <ref id="R75">
      <label>75.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Secrier</surname><given-names>M.</given-names></string-name>, <string-name><surname>Pavlopoulos</surname><given-names>G.A.</given-names></string-name>, <string-name><surname>Aerts</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2012</year>) <article-title>Arena3D: visualizing time-driven phenotypic differences in biological systems</article-title>. <source><italic toggle="yes">BMC Bioinformatics</italic></source>, <volume>13</volume>, <page-range>45</page-range>.</mixed-citation>
    </ref>
    <ref id="R76">
      <label>76.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koutrouli</surname><given-names>M.</given-names></string-name>, <string-name><surname>Karatzas</surname><given-names>E.</given-names></string-name>, <string-name><surname>Paez-Espino</surname><given-names>D.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2020</year>) <article-title>A Guide to Conquer the Biological Network Era Using Graph Theory</article-title>. <source><italic toggle="yes">Front. bioeng. biotechnol.</italic></source>, <volume>8</volume>, <page-range>34</page-range>.</mixed-citation>
    </ref>
    <ref id="R77">
      <label>77.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><collab>Ramanathan Somasundaram</collab></person-group>. (<year>2007</year>) <article-title>ONTOSELF: A 3D ONTOLOGY VISUALIZATION TOOL</article-title>, <source>Master Thesis</source>, <publisher-loc>Oxford, Ohio</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="R78">
      <label>78.</label>
      <mixed-citation publication-type="other"><article-title>The Interactorium - Systems Biology Initiative</article-title>. <ext-link xlink:href="https://www.systemsbiology.org.au/software/the-interactorium/" ext-link-type="uri">https://www.systemsbiology.org.au/software/the-interactorium/</ext-link> (<comment>25 May 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R79">
      <label>79.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Bosca</surname><given-names>A.</given-names></string-name>, <string-name><surname>Bonino</surname><given-names>D.</given-names></string-name> and <string-name><surname>Pellegrino</surname><given-names>P</given-names></string-name></person-group>. (<year>2005</year>) <article-title>OntoSphere: more than a 3D ontology visualization tool</article-title>. <italic toggle="yes">Semantic Web Applications and Perspectives, Proceedings of the 2nd Italian Semantic Web Workshop</italic>, <conf-loc>Trento, Italy</conf-loc>, <conf-date>December 14-16, 2005</conf-date>.</mixed-citation>
    </ref>
    <ref id="R80">
      <label>80.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Reski</surname>  <given-names>N.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Change your Perspective</article-title>. <source>Master’s Thesis</source>, <publisher-name>Institutionen för medieteknik, Linnéuniversitetet, Växjö</publisher-name>. <ext-link xlink:href="https://www-diva-portal-org.proxy-ub.rug.nl/smash/get/diva2:861573/FULLTEXT01.pdf" ext-link-type="uri">https://www-diva-portal-org.proxy-ub.rug.nl/smash/get/diva2:861573/FULLTEXT01.pdf</ext-link> (<comment>10 December 2020, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R81">
      <label>81.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Zainab</surname><given-names>S.S.</given-names></string-name>, <string-name><surname>Saleem</surname><given-names>M.</given-names></string-name>, <string-name><surname>Mehmood</surname><given-names>Q.</given-names></string-name></person-group>  <etal>et al.</etal> (<year>2015</year>) <article-title>FedViz: A Visual Interface for SPARQL Queries Formulation and Execution</article-title>. <source>In VOILA@ISWC 2015</source>. <ext-link xlink:href="http://svn.aksw.org/papers/2015/ISWC-VOILA_FedViz/public.pdf" ext-link-type="uri">http://svn.aksw.org/papers/2015/ISWC-VOILA_FedViz/public.pdf</ext-link>.</mixed-citation>
    </ref>
    <ref id="R82">
      <label>82.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Pattie</surname><given-names>C.</given-names></string-name>, <string-name><surname>Wilson</surname><given-names>B.</given-names></string-name>, <string-name><surname>Mullally</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al.</etal>  <article-title>Investigating individual differences influencing the understanding of statistical concepts in immersive data visualisations</article-title>, <source>Authors’ version from 2020 – unpublished work</source>.</mixed-citation>
    </ref>
  </ref-list>
</back>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Database (Oxford)</journal-id>
    <journal-id journal-id-type="iso-abbrev">Database (Oxford)</journal-id>
    <journal-id journal-id-type="publisher-id">databa</journal-id>
    <journal-title-group>
      <journal-title>Database: The Journal of Biological Databases and Curation</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1758-0463</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
      <publisher-loc>UK</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">11184448</article-id>
    <article-id pub-id-type="pmid">38554132</article-id>
    <article-id pub-id-type="doi">10.1093/database/baae008</article-id>
    <article-id pub-id-type="publisher-id">baae008</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Article</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI00960</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Visualization and exploration of linked data using virtual reality</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-6108-5552</contrib-id>
        <name>
          <surname>Kellmann</surname>
          <given-names>Alexander J</given-names>
        </name>
        <aff><institution content-type="department">Department of Genetics, University of Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff>
        <aff><institution content-type="department">Department of Genetics, University Medical Center Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Postema</surname>
          <given-names>Max</given-names>
        </name>
        <aff><institution content-type="department">Department of Genetics, University Medical Center Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>de Keijser</surname>
          <given-names>Joris</given-names>
        </name>
        <aff><institution content-type="department">Department of Genetics, University Medical Center Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Svetachov</surname>
          <given-names>Pjotr</given-names>
        </name>
        <aff><institution content-type="department">Center of information technology, University of Groningen</institution>, Nettelbosje 1, Groningen, Groningen 9747 AJ, <country country="NL">The Netherlands</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-2294-593X</contrib-id>
        <name>
          <surname>Wilson</surname>
          <given-names>Rebecca C</given-names>
        </name>
        <aff><institution content-type="department">Public Health, Policy &amp; Systems, University of Liverpool</institution>, Block B, 1st Floor, Waterhouse Building, 1-5 Dover Street, Liverpool L69 3GL, <country country="GB">United Kingdom</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-2440-3993</contrib-id>
        <name>
          <surname>van Enckevort</surname>
          <given-names>Esther J</given-names>
        </name>
        <aff><institution content-type="department">Department of Genetics, University of Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff>
        <aff><institution content-type="department">Department of Genetics, University Medical Center Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-0979-3401</contrib-id>
        <name>
          <surname>Swertz</surname>
          <given-names>Morris A</given-names>
        </name>
        <!--m.a.swertz@umcg.nl-->
        <aff><institution content-type="department">Department of Genetics, University of Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff>
        <aff><institution content-type="department">Department of Genetics, University Medical Center Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff>
        <xref rid="COR0001" ref-type="corresp"/>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="COR0001">*Corresponding author: Tel: +31 50 3617100; Fax: +31 50 3617231; Email: <email xlink:href="m.a.swertz@umcg.nl">m.a.swertz@umcg.nl</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2024-02-23">
      <day>23</day>
      <month>2</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>23</day>
      <month>2</month>
      <year>2024</year>
    </pub-date>
    <volume>2024</volume>
    <elocation-id>baae008</elocation-id>
    <history>
      <date date-type="accepted">
        <day>25</day>
        <month>1</month>
        <year>2024</year>
      </date>
      <date date-type="rev-recd">
        <day>18</day>
        <month>12</month>
        <year>2023</year>
      </date>
      <date date-type="received">
        <day>27</day>
        <month>6</month>
        <year>2023</year>
      </date>
      <date date-type="editorial-decision">
        <day>21</day>
        <month>12</month>
        <year>2023</year>
      </date>
      <date date-type="corrected-typeset">
        <day>23</day>
        <month>2</month>
        <year>2024</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2024. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2024</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="baae008.pdf"/>
    <abstract>
      <title>Abstract</title>
      <p>In this report, we analyse the use of virtual reality (VR) as a method to navigate and explore complex knowledge graphs. Over the past few decades, linked data technologies [Resource Description Framework (RDF) and Web Ontology Language (OWL)] have shown to be valuable to encode such graphs and many tools have emerged to interactively visualize RDF. However, as knowledge graphs get larger, most of these tools struggle with the limitations of 2D screens or 3D projections. Therefore, in this paper, we evaluate the use of VR to visually explore SPARQL Protocol and RDF Query Language (SPARQL) (construct) queries, including a series of tutorial videos that demonstrate the power of VR (see Graph2VR tutorial playlist: <ext-link xlink:href="https://www.youtube.com/playlist?list=PLRQCsKSUyhNIdUzBNRTmE-_JmuiOEZbdH" ext-link-type="uri">https://www.youtube.com/playlist?list=PLRQCsKSUyhNIdUzBNRTmE-_JmuiOEZbdH</ext-link>). We first review existing methods for Linked Data visualization and then report the creation of a prototype, Graph2VR. Finally, we report a first evaluation of the use of VR for exploring linked data graphs. Our results show that most participants enjoyed testing Graph2VR and found it to be a useful tool for graph exploration and data discovery. The usability study also provides valuable insights for potential future improvements to Linked Data visualization in VR.</p>
    </abstract>
    <counts>
      <page-count count="15"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec id="s1">
    <title>Introduction</title>
    <p>In recent years Linked Data has increased in popularity for representing complex analysis results, i.e. ‘hairballs’ of scientific knowledge, in particular in light of the desire to increase the FAIRness of research results (<xref rid="R2" ref-type="bibr">2</xref>, <xref rid="R3" ref-type="bibr">3</xref>).</p>
    <p>In 2009, Sir Tim Berners Lee, who is also known as the ‘inventor of the World Wide Web’ because he invented the Hypertext Transfer Protocol protocol and Hypertext markup language for web pages, gave a famous Ted talk about Linked Data in which he described how incompatible data formats and documentation systems make it necessary to examine each data element in order to create something new (<xref rid="R4" ref-type="bibr">4</xref>, <xref rid="R5" ref-type="bibr">5</xref>). Berners Lee suggested that uploading unadulterated raw data to the web as Linked Data would make it easier to combine, link and reuse existing data. Since then, shared vocabularies and ontologies have increasingly been used to structure data, and chemical and biological registries, as well as governments, have started using Linked Data to handle the large amounts of data they store (<xref rid="R6" ref-type="bibr">6–9</xref>).</p>
    <p>The Resource Description Framework (RDF) developed by the World Wide Web Consortium is a standard for describing data on the web in a machine-readable format (<xref rid="R10" ref-type="bibr">10</xref>). The RDF data model describes information as subject–predicate–object relationships called triples. This kind of data can be queried using the SPARQL query language; however, the powers of SPARQL and linked data are not readily accessible to users unfamiliar with SPARQL. Graphical tools provide visual user interfaces to support the user in visually exploring and accessing this kind of data. These triples can be used to create a network graph visualization by representing the subject and object of each triple as nodes and the predicates as edges between them. Nodes that represent the same resource are merged in the visual representation.</p>
    <p>Immersive technologies such as virtual reality (VR) and augmented reality (AR) are increasingly used in health and life sciences for a variety of applications, including therapeutics, training, simulation of real-world scenarios and data analysis for, e.g. genomics and medical imaging (<xref rid="R11" ref-type="bibr">11–15</xref>). Immersive analytics has established applications for abstract and multidimensional data in this domain (see (<xref rid="R12" ref-type="bibr">12</xref>) for a review). A limited number of applications exist to explore knowledge graphs in VR (<xref rid="R16" ref-type="bibr">16–18</xref>), but these are merely visualizations and do not offer many options for users to query and interact with the data.</p>
    <p>We hypothesize that VR will allow users to more readily explore, compare and query large knowledge graphs using a gesture-driven interface that requires less technical expertise. In this VR context, users can use ontologies to search, order and filter data to their needs. Instead of writing SPARQL queries, the user can expand existing connections and define patterns in the data interactively using the VR controllers. Overall, VR adds a third dimension and an open space that can help users perform complex data analysis.</p>
    <p>In this paper, to test this hypothesis, we first review recent visualization methods and tools used for the exploration and analysis of semantic web knowledge graphs and identify best practice methods for visualizing and interacting with SPARQL query results. We then select methods and materials and implement an experimental VR prototype to explore Linked Data and Graph2VR, evaluate its usability and investigate the human and VR environment factors that could enhance the exploration and analysis of semantic web knowledge graphs.</p>
  </sec>
  <sec id="s2">
    <title>Related work</title>
    <p>As a basis for the Graph2VR experiment, we reviewed existing tools that provide best practice methods for addressing specific challenges when working with (large) graph databases. Our review included tools for visualizing graphs in 2D, 3D and VR; those working with ontologies, SPARQL and graph databases; backends for graph databases and general ways to visualize data in VR (<xref rid="s10" ref-type="sec">Supplementary Appendix Table A1</xref>).</p>
    <p>Later, we describe some notable tools, LODLive, GraphDB, Vasturiano, Toran, Visual Notation for OWL ontologies (VOWL), Gruff and Tarsier (<xref rid="R18" ref-type="bibr">18–25</xref>), that all provide rich graph exploration functionality, enumerate functional challenges and visualization aspects and highlight their implementation in Graph2VR. We also refer the reader to some overview papers about tools to visualize and interact with ontologies, Linked Data or graph databases in general, which have been tested and described, e.g. in the following papers. (<xref rid="R6" ref-type="bibr">6</xref>, <xref rid="R26" ref-type="bibr">26</xref>, <xref rid="R27" ref-type="bibr">27</xref>).</p>
    <p>In ‘LodLive’, the user is first asked for a SPARQL Endpoint and a Unique Resource Identifier (URI). Each node is represented as a circle in LodLive, and outgoing connections are represented as small nodes around it (see <xref rid="F1" ref-type="fig">Figure 1</xref>). Each node offers some options in the form of a menu that includes ‘i’ for information, a button to focus on the node and to close other relationships, a button to open a link to the URI to show an online resource, a button to expand all relationships around that node and a button to remove the selected node. Once a node is added, existing connections to other open nodes are also displayed. However, beyond opening new connections and a comprehensive info panel that shows information about the respective node in a flexible way that makes use of different kinds of semantic annotations, integrating images and even google maps, the query possibilities in LodLive are quite limited.</p>
    <fig position="float" id="F1" fig-type="figure">
      <label>Figure 1.</label>
      <caption>
        <p>A screenshot of LodLive (<xref rid="R19" ref-type="bibr">19</xref>): the graph can be expanded by clicking on the small clouds or circles around the node.</p>
      </caption>
      <graphic xlink:href="baae008f1" position="float"/>
    </fig>
    <p>‘GraphDB’ and ‘Metaphactory’ are commercial software tools (<xref rid="R20" ref-type="bibr">20</xref>, <xref rid="R28" ref-type="bibr">28</xref>). They use an internal database and offer an autocomplete search function to find URIs. The user is thus not required to know the precise URI, which makes the tools quite user-friendly. GraphDB also offers an option to collapse all nodes around a specific node instead of only allowing users to delete specific nodes (<xref rid="F2" ref-type="fig">Figure 2</xref>).</p>
    <fig position="float" id="F2" fig-type="figure">
      <label>Figure 2.</label>
      <caption>
        <p>A small ‘Visual Graph’ in GraphDB (<xref rid="R20" ref-type="bibr">20</xref>). The node in the middle has a menu with the option to collapse connections around that node.</p>
      </caption>
      <graphic xlink:href="baae008f2" position="float"/>
    </fig>
    <p>The Visual Notation for OWL Ontologies ‘VOWL’ (<xref rid="R22" ref-type="bibr">22</xref>, <xref rid="R23" ref-type="bibr">23</xref>) adds use of colour-blind-friendly colours and symbols to visualize different types of nodes and edges. Two examples of the implementation of VOWL are a plugin for Protégé and WebVOWL (<xref rid="R29" ref-type="bibr">29</xref>, <xref rid="R30" ref-type="bibr">30</xref>). WebVOWL is an online visualization for ontologies. By default, WebVOWL starts with a subgraph of the Friend of a Friend Ontology (FOAF) ontology, as shown in <xref rid="F3" ref-type="fig">Figure 3</xref>, but users can upload their own, owl files. QueryVOWL, another VOWL tool, can be used to query a SPARQL Endpoint using a textual search to create a visual graph as output (<xref rid="R31" ref-type="bibr">31</xref>).</p>
    <fig position="float" id="F3" fig-type="figure">
      <label>Figure 3.</label>
      <caption>
        <p>Screenshot of WebVOWL showing parts of the FOAF ontology (<xref rid="R32" ref-type="bibr">32</xref>).</p>
      </caption>
      <graphic xlink:href="baae008f3" position="float"/>
    </fig>
    <p>Gruff is a commercial tool that can use its internal graph database (Allegro graph), SPARQL Endpoints and Neo4j. After selecting a database, users can ‘Display some sample triples’ from that database without any prior knowledge about the content, which is very convenient for new users. Gruff offers a visual query view to describe the necessary relations using variables and searching for nodes. These relations can contain specific URIs, such as constant nodes and variables. The relationships created in query view are then translated into an editable SPARQL query, which can be altered to add additional filters or other specific SPARQL commands. This translation from a visualization to an editable SPARQL query is convenient and makes Gruff stand out as a tool. However, screen size is a limiting factor for Gruff. When zooming out, the text becomes too small to read and is hidden. The user can move the window of visible nodes in all directions. Instead of using labels, Gruff uses colours and shapes to indicate similar nodes and repeating edges.</p>
    <fig position="float" id="F4" fig-type="figure">
      <label>Figure 4.</label>
      <caption>
        <p>Gruff allows users to build queries visually and shows the SPARQL query, so it can be modified (4(A)). The results can be displayed as a visual graph (4(B))(<xref rid="R24" ref-type="bibr">24</xref>). (A) Screenshot of a query built-in Gruff’s ‘Graphical Query View’. (B) Screenshot of the resulting graph in Gruff’s ‘Graph View’. When zoomed out, the texts disappear, but the colours still indicate similar nodes and edges.</p>
      </caption>
      <graphic xlink:href="baae008f4" position="float"/>
    </fig>
    <p>Finally, ‘Tarsier’ is a tool that makes use of a 3D representation of SPARQL queries. Filters allow users to select nodes that fulfil specific criteria and shift them to another ‘semantic plane’. This tool was built with the intention to help new students to learn SPARQL and understand how filters work (<xref rid="F5" ref-type="fig">Figure 5</xref>).</p>
    <fig position="float" id="F5" fig-type="figure">
      <label>Figure 5.</label>
      <caption>
        <p>Tarsier filter data and can shift datapoints to different semantic planes accordingly. (This screenshot was taken from a video from the authors of Tarsier (<xref rid="R33" ref-type="bibr">33</xref>)).</p>
      </caption>
      <graphic xlink:href="baae008f5" position="float"/>
    </fig>
  </sec>
  <sec id="s3">
    <title>Materials and Methods</title>
    <p>Based on the analysis of existing tools and approaches, we prioritized a list of key features for Graph2VR, which are summarized in <xref rid="T1" ref-type="table">Table 1</xref>. We aimed to combine some of the strengths of the different methods from different tools in one VR application that enables immersive 3D visualization that allows interaction with and manipulation of data. Graph2VR is an extensive usable prototype that still has a few issues that need to be resolved. It is extendable and demonstrates how a VR application can be used to visualize and interact with Linked Data. A more detailed description of its features is present in the user manual in (<xref rid="R34" ref-type="bibr">34</xref>) and in five tutorial videos (<xref rid="R1" ref-type="bibr">1</xref>). Later, we describe the methodological considerations for these features in detail, grouped by initialization, visualization, navigation and data analysis.</p>
    <table-wrap position="float" id="T1">
      <label>Table 1.</label>
      <caption>
        <p>Key features of Graph2VR</p>
      </caption>
      <table frame="hsides" rules="groups">
        <colgroup span="1">
          <col align="left" span="1"/>
          <col align="left" span="1"/>
        </colgroup>
        <thead>
          <tr>
            <th valign="bottom" align="left" rowspan="1" colspan="1">Category</th>
            <th valign="bottom" align="left" rowspan="1" colspan="1">Description</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left" rowspan="1" colspan="1">Initialization</td>
            <td align="left" rowspan="1" colspan="1">A configuration file is used to preconfigure which SPARQL Endpoint to start from and additional ones to switch to. Graph exploration usually starts from either a single URI or a small graph, which can also be specified in the configuration file. Image predicates, predicates that get suggested for new connections and colour coding can be adjusted in this file. We adopted the VOWL colour schema as a reasonable default.</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Visualization</td>
            <td align="left" rowspan="1" colspan="1">To visualize Linked Data, nodes are represented as spheres and the edges between them as arrows. The colour schema is used to represent different kinds of nodes or their current status (e.g. being selected or part of a query). A choice of graph layout (3D, 2D, hierarchical and class hierarchy) for the visualization will give the most flexibility for the user. Labels and images for nodes and labels for edges are preloaded when the graph is expanded.</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Interaction</td>
            <td align="left" rowspan="1" colspan="1">Graph2VR provides features for interacting with the visualized Linked Data. The user can grab and move single nodes or the whole graph. Once grabbed, the whole graph can be moved, rotated and scaled. There are four different graph layout options: 3D, 2D, hierarchical and class hierarchy. The user has different ways to navigate in the VR environment. Graph2VR is a room-scale VR application, so looking around and taking a few steps can help to move small distances. For larger distances, the user can either teleport or fly. Pinning nodes prevents them from being affected by the layout algorithm so that they can be restructured manually. The user can create new nodes, edges and graphs; drag and drop nodes and graphs; and convert nodes or edges to variables to create queries. New nodes can be added by searching for them in the database or by spawning a new variable node. After selecting single nodes or edges, the circle menu shows options for the selected node or edge for further interaction.</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Data analysis</td>
            <td align="left" rowspan="1" colspan="1">Graph2VR provides features for querying SPARQL Endpoints, including graph expansion. In addition, the nodes and edges can be used to generate custom queries visually. Triples can be selected to be part of a SPARQL query. A predefined set of commands, including selecting triples, creating variables, OrderBy and Limit, can be used to create a SPARQL query. Options that affect a whole triple can be found in the edge menu.</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <sec id="s3-s1">
      <title>Initialization</title>
      <p>In many visualization tools, graph exploration starts from either a small subgraph (bottom-up or local view) or a global hierarchy (top-down or global view) in which zoom and filters are used to request more details (<xref rid="R35" ref-type="bibr">35</xref>). Starting from a subgraph, either an overview graph or a start node, the user can expand the graph by opening further connections. Commonly, tools that work with SPARQL need the Uniform Resource Locator of a SPARQL Endpoint to determine to which graph database their requests should be sent. Additionally, they need a URI, SPARQL query or keyword to know where to start the graph exploration. The graph can then be expanded further to explore it incrementally. In Graph2VR, one can start with an initially provided SPARQL query and explore from there.</p>
    </sec>
    <sec id="s3-s2">
      <title>Visualization</title>
      <p>This section summarizes methods around visualization, in particular use of layout, colour and information display.</p>
      <sec id="s3-s2-s1">
        <title>Layout</title>
        <p>In general, the layout of graphs in the tools is either static or is restructured over time to increase the distance between the nodes and the readability. The Fruchterman–Reingold algorithm is a force-directed graph algorithm (<xref rid="R36" ref-type="bibr">36</xref>). The regular runtime of the algorithm has a runtime of <inline-formula id="ILM0001"><tex-math notation="LaTeX" id="ILM0001-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal{O}(|N|^2 + |E|)$\end{document}</tex-math></inline-formula>, where <italic toggle="yes">N</italic> is the number of nodes and <italic toggle="yes">E</italic> is the number of edges (<xref rid="R37" ref-type="bibr">37</xref>). The algorithm also works in three dimensions. As the graphs get larger, the number of node–node interactions increases. For larger graphs, other algorithms like the Barnes–Hut algorithm scale better. The Barnes–Hut algorithm combines the gravity centre of nodes that are further away so that fewer calculations need to be executed (<xref rid="R38" ref-type="bibr">38</xref>, <xref rid="R39" ref-type="bibr">39</xref>). There are, of course, many more algorithms like Kamada–Kawai that could be used to handle even more nodes in a graph at once (<xref rid="R36" ref-type="bibr">36</xref>, <xref rid="R40" ref-type="bibr">40</xref>). In a 3D representation, we can use the third dimension to compare a stack of 2D layers. In the literature, this kind of representation is known as semantic planes (<xref rid="R25" ref-type="bibr">25</xref>, <xref rid="R41" ref-type="bibr">41</xref>), and Tarsier already makes use of it. Tarsier runs on a local server and can be accessed via its web interface. It allows the user to apply filters, then takes all the nodes that fulfil the selection criteria and shifts them to another semantic plane. A stack of different planes of information would allow a user to compare them without losing their internal structure (e.g. a tree structure). In this way, the user can, e.g. compare data and annotate the similarity between two resources. For comparisons, different predicates can be used to describe the kinds of connections between different nodes. In their Scientific Lens paper, Batchelor et al. name four predicates with decreasing similarity levels to compare similar entities that could be used to compare different entities with similar meanings: ‘owl: sameAs’, ‘skos: exactMatch’, ‘skos: closeMatch’ and ‘rdfs: seeAlso’ (<xref rid="R42" ref-type="bibr">42</xref>). In the final version of Graph2VR, we added those as predefined predicates so that a user can quickly adjust the predicate of an edge. In Graph2VR, results can be shown as a series of semantic planes representing the different results that match a given query pattern (see section Demonstration Data). These planes are useful for comparing different 2D structures and creating new connections. For example, multiple 2D layers of class hierarchies allow users to compare between different ontologies, which can be used to find similarities and differences.</p>
      </sec>
      <sec id="s3-s2-s2">
        <title>Colour</title>
        <p>The colouring of nodes and edges also varies from tool to tool. LodLive uses random colours, whereas Gruff reuses the same colour for the same types of nodes and edges. The VOWL colour scheme defines the colour of each node based on its properties, using specific colours for classes, variables, blank nodes and literals. It has also been designed to be colour-blind-friendly and understandable when printed in black and white (<xref rid="R32" ref-type="bibr">32</xref>). VOWL recommends using specific shapes or patterns to indicate different attributes, e.g. a ring around a node to represent a class. For the 3D environment, the colours can be reused, but some forms need to be adjusted. In 3D, for example, a circle around a sphere to indicate a class could be represented as either a circle or a sphere. Using different forms and shapes would also be a viable option to represent an object in Linked Data. In Graph2VR, we primarily apply the VOWL schema but have not yet implemented all of it. We also add some colours for variables and selected triples. Further details can be found in the manual (<xref rid="R34" ref-type="bibr">34</xref>). As graphs become larger through expansion, it becomes more important to be able to quickly identify different types of nodes, i.e. is the node a URI, a literal, a blank node or a variable? Some tools do not distinguish, displaying all nodes in the same or a custom colour. One example of such a tool is ‘Noda’, an application sold on Steam that allows the user to interact with 3D mindmaps or graph structures by adding nodes and edges with different sizes, symbols, colours and labels (<xref rid="R43" ref-type="bibr">43</xref>). Gruff colours the same kinds of connections in the same randomly chosen colour, whereas the VOWL colour scheme can help users make distinctions quickly (<xref rid="R23" ref-type="bibr">23</xref>). In Graph2VR, parts of the VOWL schema are applied, making literals (strings and numbers) yellow, URIs blue and classes light blue (only if the class relationship is expanded). Blank nodes are dark. When a node or edge is selected using the laser, it turns red. When converted to a variable, nodes and edges turn green. Nodes and edges glow white when the user hovers over them with the laser. If triples are part of the selection for a query, they shine in a bright yellow colour. While common edges are black with a black arrowhead, the arrowhead changes the colour for the ‘rdfs: subClassOf’ relationship to a white arrowhead.</p>
      </sec>
      <sec id="s3-s2-s3">
        <title>Information display</title>
        <p>One useful feature in tools like LODLive or WebVOWL is an information box containing essential information about a selected node, although which information is displayed varies. Typical information provided is the URI, label, class relationship and sometimes comments. SPARQL offers the ‘describe’ function to request some basic information, but the results can be quite extensive. Therefore, it is better to restrict the information to a selection of predicates.</p>
        <p>To visualize query results, we parsed the results and displayed the URIs or, if findable, the labels of the nodes and edges in the graph. To further improve readability, the labels above the nodes are continuously rotated towards the user, as are the images. This can be seen in the first part of the Video tutorial (<xref rid="R1" ref-type="bibr">1</xref>). In contrast, the texts above the edges always sit on top of the connecting lines and are readable from both sides of the edges. Since URIs are often long strings of characters, the labels of the URIs are displayed instead (if available). The complete URI appears when the user hovers over the edge with the laser.</p>
      </sec>
    </sec>
    <sec id="s3-s3">
      <title>Interaction</title>
      <p>This section summarizes features for interaction with the visualization, in particular methods for controls, navigation, zooming and rotation and menus in VR.</p>
      <sec id="s3-s3-s1">
        <title>Controls</title>
        <p>To enable users to reorder nodes, we implemented a gesture-driven interface that allows the user to drag and drop them using the controllers. The user grabs a node with both hands while pressing the grip button. Grabbing a node while pressing the trigger button allows new links between two nodes to be created.</p>
        <p>When multiple graphs exist in VR, it can be challenging to grab and manipulate the desired graph. To differentiate between different graphs, we implemented a sphere around all the nodes of a graph that is only visible from the outside of the graph, to avoid cluttering the view. This sphere determines the size of a graph and allows the user to interact with a specific graph. When a SPARQL query is executed and multiple results are created in different layers (semantic planes), each layer is generated as a separate graph. Grabbing and moving a graph enable the user to work with a specific subgraph. To remove less interesting graphs, we made it possible to delete sibling graphs (all the other graphs created by the same query aside from the current one) or all child graphs of a particular graph. When the semantic planes are displayed next to each other, we first tested drawing flat planes around the 2D visualizations. Those planes were transparent, similar to the sphere surrounding the 3D version of the graph. However, as multiple partially transparent 2D layers behind each other turned out to be more distracting than useful, we turned off the planes by default.</p>
      </sec>
      <sec id="s3-s3-s2">
        <title>Navigation</title>
        <p>We added a platform to Graph2VR to make orientation easier as it feels more natural for a user to stand and walk on a surface than to float in empty space. Common ways to move in room-scaled VR applications are moving in the room, walking with a thumbstick or trackpad, teleportation and, in some cases, flying. In Noda, users can drag themselves through the room. In GraphXR, users can rotate the whole graph. Finally, in 3D Force Graphs, the user can fly through a universe of huge nodes (<xref rid="R17" ref-type="bibr">17</xref>, <xref rid="R18" ref-type="bibr">18</xref>, <xref rid="R43" ref-type="bibr">43</xref>). To give the user maximum freedom for navigation, we implemented both flying and teleportation and, if the user looks around or moves, this also happens in the virtual environment.</p>
      </sec>
      <sec id="s3-s3-s3">
        <title>Zoom and rotation</title>
        <p>Graph2VR offers several ways to interact with a graph. When using the grip buttons of the VR controller on a node, the user can drag and drop a node close to the controller. Here, we were inspired by functions in the game/tech demo Toran (<xref rid="F6" ref-type="fig">Figure 6(A)</xref>) (<xref rid="R21" ref-type="bibr">21</xref>). In that demo, a graph is embedded in a round sphere that can be freely rotated and zoomed (<xref rid="F6" ref-type="fig">Figure 6(B)</xref>). The user can also create new connections between nodes within the sphere. As this also suits working with graphs, we implemented similar ways to interact with the graphs in Graph2VR. Both controllers’ grip buttons must be pressed close to the graph simultaneously to grab the whole graph, which can then be dragged around and rotated freely. When the controllers are moved together/away from each other, the graph is scaled down/up. It is possible to grab a node and the whole graph simultaneously. This results in scaling the entire graph; only the grabbed node does not scale with the graph, which can be used to scale specific nodes up or down. It is also possible to work with multiple graphs in Graph2VR, and each can have a different size.</p>
        <fig position="float" id="F6" fig-type="figure">
          <label>Figure 6.</label>
          <caption>
            <p>The sphere in Toran (6(A)) was the inspiration for having a sphere and for how to rotate, zoom and drag&amp;drop a graph (<xref rid="R21" ref-type="bibr">21</xref>). For comparison, see the sphere in Graph2VR (6(B)). (A) Screenshot of Toran, a transparent sphere surrounds the game elements. (B) Screenshot of rotating a graph in Graph2VR. A transparent sphere surrounds the graph.</p>
          </caption>
          <graphic xlink:href="baae008f6" position="float"/>
        </fig>
      </sec>
    </sec>
    <sec id="s3-s4">
      <title>Circle menu</title>
      <p>VR has not yet evolved to have standard control archetypes comparable to those used in 2D user interfaces. Nonetheless, users do need a system that allows them to quickly choose from sets of options. We were inspired to create a circular menu by the ‘Aesthethic Hover UI’ asset (<xref rid="R44" ref-type="bibr">44</xref>). Here, we first attempted to reuse the HoverUIKit, but it used depreciated packages, so we ultimately built our own circle menu. Most 2D tools like LOD Live, Gruff, GraphDB and Metaphactory show their menu and search options next to the nodes (<xref rid="R19" ref-type="bibr">19</xref>, <xref rid="R20" ref-type="bibr">20</xref>, <xref rid="R24" ref-type="bibr">24</xref>, <xref rid="R45" ref-type="bibr">45</xref>), but VR offers a wider range of options. We therefore considered having the menu directly next to the node, on the left arm, on a virtual display or panel or static in front of the head as in a helmet display. After some testing, we determined that a menu with many options within a hairball of nodes would not be the appropriate solution and that the menu was best readable on the left controller. This circular menu has several submenus and shows the options that can be applied in the current context. When a node is selected, the menu displays options that can be applied to that node (<xref rid="F7" ref-type="fig">Figure 7(A)</xref>). When selecting an edge, a menu for the edge is shown. In the node menu are further submenus that show all incoming or outgoing relations from or to the selected node. To improve the usability of the menu, we added some icons to the options in the circle menu. Click sounds indicate that buttons have been pressed, and white circles around newly spawned nodes that disappear within 2 s denote changes within the application. For new Graph2VR users, we have added a short help menu describing the main functionalities.
</p>
      <fig position="float" id="F7" fig-type="figure">
        <label>Figure 7.</label>
        <caption>
          <p>The circle menu in 7(A) shows different options based on the context. There are submenus that show more details, such as the outgoing connections in 7(B). (A) The circle menu after clicking on a node. (B) Circle menu displays multiple outgoing connections.</p>
        </caption>
        <graphic xlink:href="baae008f7" position="float"/>
      </fig>
    </sec>
    <sec id="s3-s5">
      <title>Data analysis</title>
      <p>This section summarizes methods for graph manipulation, in particular query generation, search, and save and load.</p>
      <sec id="s3-s5-s1">
        <title>Query generation</title>
        <p>In SPARQL, Select queries result in tabular answers, while Construct queries describe graph structures. Therefore, Graph2VR uses Select queries to populate the menu, e.g. with the incoming and outgoing connections of a node, and Construct queries to create graph structures. Nodes with the same URI are combined into one node in the visualization to generate a network graph instead of a list of triples. The submenu for incoming (and outgoing) connections lists the predicates pointing to (or from) the current node. They are then grouped by their predicates, and the number of available triples for each predicate is displayed next to the entry. When there are more predicates than can be displayed at once, a circular scrollbar can be used to scroll through the menu (<xref rid="F7" ref-type="fig">Figure 7(B)</xref>). If a node with the same URI is added to a graph, it will be merged with the existing node. To restrict the number of results when expanding the graph (e.g. if one node has thousands of connections of the same type), there is a limit on how many connections to open (default is 25). This default limit can be adjusted using the limit slider below the menu (<xref rid="F7" ref-type="fig">Figure 7(B)</xref>). The slider is not a linear scale but has fixed amounts of nodes.</p>
        <p>To improve predicate readability, predicate labels are shown instead of the URI. If no label is available, the URI is shortened, but it is still possible to see the entire URI when hovering over the menu with the pointer. The predicates are ordered alphabetically by URI, not by the shortened version shown, which may confuse users when the base URI changes and the alphabetic order starts from the beginning again. When there are multiple connections between the same nodes, or in case of a self-reference, the straight edges are replaced by bent arrows to avoid overlapping texts.</p>
      </sec>
      <sec id="s3-s5-s2">
        <title>Node removal</title>
        <p>Once nodes are established, the user should have the option to remove them from the visualization. We implemented two different ways to do so. The simplest is the ‘close’ option, which removes the node and all triples containing this node from the visualization (but not the database). A more complex way to reduce the size of the graph is the ‘collapse’ option. This removes all leaf nodes around the selected node, while the node itself remains. Leaf nodes are nodes with no other connections within the visualization (but not necessarily in the underlying graph database). This helps reduce the number of nodes while preserving the graph structure.</p>
        <p>For each node, the user has the option to remove it or to remove the surrounding leaf nodes (incoming, outgoing or both). When a node is deleted, the edges around it and its leaf nodes are also removed because there would be no triple left that contains them. This could lead to many single nodes that the user would have to remove manually. To prevent accidental removal when collapsing a selected node (when no triples are left containing it), and to keep newly created nodes that are not yet part of a triple, we decided to keep single nodes so that a user can connect them or start a new exploration from there.</p>
        <p>When queries are sent in Graph2VR, the response will be displayed in new graphs that we call child graphs. To remove these again, each graph has the option to remove a whole graph at once. When a query creates multiple child graphs, it would be too much effort to remove those one-by-one. We therefore included an option to delete all child graphs from the original graph or the operation to delete all sibling graphs. Removing all child graphs will remove all child graphs of the respective graph. Each of the graphs also has the option to remove its sibling graphs, which will remove all unmodified sibling graphs created by the same query, leaving only the selected graph and modified graphs. If the last graph has been removed, it is possible to create a new graph by creating a new node, loading some saved data or using the search function(s).</p>
      </sec>
      <sec id="s3-s5-s3">
        <title>Search</title>
        <p>Graph2VR was intended to be more interactive than just a visualization. To be able to create some SPARQL queries visually, we added ways to create and rename variables or search for keywords. This required a text input system. We therefore reused a virtual keyboard from Unity’s asset store, VRKeys, that uses VR controllers as drumsticks to enter text (<xref rid="R46" ref-type="bibr">46</xref>). This approach to entering text in VR has two advantages over other applications, which mainly use a laser with a point-and-click system. Drumming the keyboard does not use an additional key, so it does not interfere with our controls, and it can be done with two hands simultaneously. We also implemented both a global search in the settings menu and a context-specific search that can be accessed from the node menu for variable nodes. In addition to the keyword, this second search function takes all the selected triples into account. Only results that match the selected variable in the given context and the keyword are shown. Both search functions attempt to perform an autocomplete search on the search term. Depending on the settings, the search can either be triggered with every keystroke on the VR keyboard or by pressing the return key.</p>
      </sec>
      <sec id="s3-s5-s4">
        <title>Save and load</title>
        <p>Graph2VR has two different ways to save and load data to a file. The first is to save the whole state of the application, including all the nodes, edges, their positions, labels, graphs, etc. Even images that have been loaded are saved, so they can be reloaded even if they are no longer on the internet. To prevent saved files from becoming too large, the image resolution is scaled down if it is too large. The quicksave option offers one save slot that will be overwritten every time. Alternatively, the user can specify a filename for a save state, allowing multiple save states. Loading a save state will overwrite the current session. Another way of saving and loading is to save triples as ntriples. This is a standard format that can also be read from other applications. In contrast to the first save option, this only saves the triples, and positions, images and single nodes are not stored. When loading an ntriples file, all the triples are automatically added to the current scene. In contrast to loading a save state, loading an ntriples file does not overwrite the current scene. Instead, all the triples are added as an additional graph. We strongly advise users not to load large ntriples files because Graph2VR could become slow or unresponsive. We could load about 5000 triples at once during our tests, but the framerate dropped to around five frames per second. For larger ntriples files, we recommend loading them into a SPARQL endpoint, e.g. a Virtuoso server, and accessing only the relevant parts from there (<xref rid="R47" ref-type="bibr">47</xref>).</p>
      </sec>
    </sec>
  </sec>
  <sec id="s4">
    <title>Implementation</title>
    <p>Based on the methodological basis, we implemented a Graph2VR prototype in Unity (version 2 021.2f1) (<xref rid="R48" ref-type="bibr">48</xref>). This section summarizes implementation details, in particular use of Quest 2 VR, how to best represent RDF in unity, implementation of the layout algorithms and performance optimizations.</p>
    <sec id="s4-s1">
      <title>Standalone on Quest 2 VR headset</title>
      <p>Graph2VR was designed to run as a standalone version on the Quest 2 VR headset, but it also supports the HTC Vive headset. We recommend using the Quest 2 headset because its higher resolution provides better readability. Graph2VR can be compiled as a Windows application (.exe) or as a standalone application (.apk) for the Meta Quest 2 VR headset. Our Graph2VR implementation process started with the Free Unity WebXR Exporter from the Unity Asset store (<xref rid="R49" ref-type="bibr">49</xref>). This template includes a desert background and some objects and models for the VR controllers. To be able to select nodes, a laser pointer was added to the right controller. We used a sample dataset in a local Virtuoso server via docker as a database (<xref rid="R50" ref-type="bibr">50</xref>). To access the server, a modified DotNetRDF (version 2.6) is used (<xref rid="R51" ref-type="bibr">51</xref>, <xref rid="R52" ref-type="bibr">52</xref>). At some point, DotNetRDF checks whether a specific interface is present, but even if it was present, the test failed in the Quest 2 standalone application. We resolved this by removing this check and recompiling the DotNetRDF library. One of the tougher decisions during the implementation process was whether to use SteamVR or OpenXR (<xref rid="R53" ref-type="bibr">53</xref>, <xref rid="R54" ref-type="bibr">54</xref>). SteamVR, as commercial software, supports many VR headsets out of the box, provides 3D controller models and is, in general, easier to use due to its more abstract controller bindings. On the other hand, OpenXR would allow us to build a standalone application for the Quest2 headset that does not require a connection to a personal computer with a strong graphics card. Ultimately, we chose OpenXR and were able to create a standalone application for the Quest2.</p>
    </sec>
    <sec id="s4-s2">
      <title>RDF representation</title>
      <p>One issue we had to resolve was whether to use the representation of graphs provided by DotNetRDF, using iNodes and iGraphs, or whether to use the Unity in-memory representation for the nodes and edges. Unity-based prefabs can help to circumvent potential inconsistencies of two representations of the same dataset. Keeping both representations would be more error-prone but would allow reuse of more iGraph functionalities. The DotNetRDF representation has certain advantages when parsing and reusing the query results. Many functionalities were already present and utilizable. The disadvantage of this representation was that both the internal and the visual would need updates when adapting the graph. Additionally, while iGraphs in DotNetRDF support triples, adding a single node to our graphs, e.g. when a user adds a new node, led to differences between iGraph and visual representation. Consequently, the decision was made to rely on the unity-based representation to allow single nodes and enable step-by-step generation of new triples without consistency issues.</p>
    </sec>
    <sec id="s4-s3">
      <title>Layout algorithms</title>
      <p>Layout algorithms help to reorder the graph to improve readability. We implemented four layout algorithms: 3D force-directed, 2D force-directed, hierarchical view and class hierarchy. For the force-directed 3D layout, we used the Fruchterman Reingold Algorithm (<xref rid="R37" ref-type="bibr">37</xref>), which uses repulsive forces between nodes and attractive forces along the edges. Over time, the ‘temperature’ cools down, the forces get weaker, the adjustments in the graph get smaller and smaller and there can be a cut-off value. In Graph2VR, layout algorithms can be switched via the Graph operations menu. Inspired by Gephi, we modularized the layout algorithm, so the user can swap to other layout algorithms (<xref rid="R55" ref-type="bibr">55</xref>). For the 2D layout, we used a simple layout heuristic, simply ordering the nodes sequentially in a plane aided by minimal force direction. While the force-directed Fruchterman Reingold algorithm forms circles in 2D or spheres of nodes in 3D, the ‘Hierarchical View’ layout orders the nodes alternating in horizontal and vertical stacks (<xref rid="F8" ref-type="fig">Figure 8</xref>). This makes it easier to read the labels of the nodes and find a specific node. Additionally, the nodes are sorted alphabetically in this layout. New outgoing nodes are usually added to the expanded node’s right side. However, an exception is the rdfs: subClassOf relationship, which points to the left.</p>
      <fig position="float" id="F8" fig-type="figure">
        <label>Figure 8.</label>
        <caption>
          <p>The hierarchical layout adds new triples, alternating vertical and horizontal on the ‘right’ side. Only the rdfs: SubClassOf relations are pointing to the ‘left’ side. They have a white arrowhead as specified in the VOWL schema.</p>
        </caption>
        <graphic xlink:href="baae008f8" position="float"/>
      </fig>
      <fig position="float" id="F9" fig-type="figure">
        <label>Figure 9.</label>
        <caption>
          <p>Comparison of the 3D class hierarchy in Graph2VR (9(A)) with the 2D class hierarchy in Protégé (9(B)) showing the same class hierarchy. (A) The class hierarchy in Graph2VR can display subclasses and individuals in a single 3D tree structure. The individuals and their attributes point in the third dimension. (B) The class hierarchy in Protégé is a tree structure starting at owl: thing. The individuals are displayed in a separate panel after selecting one of the classes.</p>
        </caption>
        <graphic xlink:href="baae008f9" position="float"/>
      </fig>
      <p>Besides the ‘Hierarchical View’, we also added a ‘Class Hierarchy’ layout. The basic idea of this layout was to create a class hierarchy, like the tree structure in Protégé (<xref rid="R29" ref-type="bibr">29</xref>), but in three dimensions. The base of this layout is a 2D class hierarchy based on the rdfs: subClassOf predicate. Each of the classes (or subclasses) can contain multiple individuals of that class. We can use the extra dimension to display the individuals in a list orthogonal to the 2D class hierarchy based on the rdf: type predicate. See <xref rid="F9" ref-type="fig">Figure 9(A)</xref>.
</p>
      <p>Finally, a pin function was added to pin certain nodes to their current position. A pin prevents these nodes from being affected by the layout algorithm, but they can still be dragged around manually. This can have interesting effects when some nodes are pinned and layout algorithms are applied only to parts of the graph. It can be helpful to have a class hierarchy for the classes, pin it and continue exploring the individuals, e.g. with a force-directed algorithm.</p>
    </sec>
    <sec id="s4-s4">
      <title>Performance optimizations</title>
      <p>To prevent the whole application from stuttering while SPARQL queries are executed, we put them into separate threads. This way, Graph2VR will not stutter even if a query takes several seconds to be executed. The disadvantage of this is that the order of results from different queries is not determined. There is a race condition between the different queries, with the faster result displayed first. This may affect the search function when ‘search on key press’ is activated as it fires a query on each key press. This can be quite fast compared to the variation in execution times of the search queries, potentially leading to a situation where search results for an older but slower query overwrite newer results. To speed-up free-text search queries, we used the bif: contains command. This command is not supported by every SPARQL server but is supported by e.g. OpenLink’s Virtuoso (<xref rid="R56" ref-type="bibr">56</xref>). The bif: contains command triggers a search function on a preindexed internal Structured query language table. If such an index exists, this command can be used to speed up the free-text search. If the index does not exist, the server will most likely return empty results (<xref rid="R57" ref-type="bibr">57</xref>). During our tests, the DBpedia SPARQL endpoint supported the bif: contains command. One restriction of the bif: contains command (in Virtuoso) is that it only supports words with at least four letters and needs to be enclosed in brackets if it contains spaces. This is a problem if the search term consists of multiple connected words that belong together (e.g. names with ‘Mc’ at the beginning or ‘van’ in the middle). We did overcome this issue by separating the words but replacing the spaces of words with fewer than four letters with a star (any character) so that the whole name can be used as a single search term.</p>
    </sec>
  </sec>
  <sec id="s5">
    <title>Results</title>
    <sec id="s5-s1">
      <title>Application overview</title>
      <p>A SPARQL query can be used to define the initial graph. A combination of URIs, literals and variables in SPARQL is used to define query patterns and to request results that match this pattern from the database. Traditionally, formulating a SPARQL query involved manually searching for relevant URIs and predicates, a process that could take several minutes, especially for complex queries involving multiple triples. In Graph2VR, it is just a matter of expanding the graph by clicking on the desired predicate and expanding the graph and converting existing nodes (or edges) in the graph to variables. If necessary, new nodes and edges can be spawned. Once the relevant triples in the graph are selected, the results can be requested.</p>
      <p>We implemented two different ways to display the query results. They can either be displayed as a single result graph containing all the triples merged into one new graph or as a series of separate result graphs stacked behind each other. These result graphs are independent graphs. To make sure that not all of them are moved, scaled and rotated at the same time, only one graph (the closest) can be grabbed at the same time. The relevant distance is the distance between the middle point of a graph and the left controller. There are also options to dispose of those stacks of graphs.</p>
      <p>The idea of displaying parts of the results as stacked 2D projections had already been used in previous applications. Tarsier, for example, uses them to separate nodes that do or do not meet user-defined filter criteria into different planes (<xref rid="R25" ref-type="bibr">25</xref>). In the current version of Graph2VR, we did not implement a filter function, but the Tarsier approach might be an excellent way to do so in the future.</p>
      <p>SPARQL has several commands to modify a query. The limit slider was already introduced to limit the number of nodes when expanding the graph (<xref rid="F7" ref-type="fig">Figure 7(B)</xref>). It also can be used to limit the number of result graphs when sending a query. Another modifier is the ORDER BY command, which is used to order the stacked result graphs either descending or ascending. We added this menu option so that the variable can be selected by clicking on it, and there is a small adjacent button (ASC/DESC) to adjust the order.</p>
      <p>Graph2VR can connect to local and online SPARQL Endpoints. If databases are preconfigured, users can switch between them via the circle menu.</p>
    </sec>
    <sec id="s5-s2">
      <title>Demonstration data</title>
      <p>For testing purposes, we provided an easy-to-understand data example from DBpedia, which is one of the best-known public sources for Linked Data and is based on Wikipedia (<xref rid="R58" ref-type="bibr">58</xref>).</p>
      <p>In contrast to Wikipedia, it is possible to query data in DBpedia using SPARQL queries. One of the examples from a lecture about the semantic web was how to ask DBpedia for the second-highest mountain in a certain country, e.g. Australia (<xref rid="R59" ref-type="bibr">59</xref>). While it is challenging to find this information simply by reading Wikipedia articles, writing a SPARQL query to get this information from DBpedia is relatively easy. Since all the information is represented as triples, a user only needs to find out which predicates are used to encode height (dbo: elevation), location (dbp: location) and the fact that it should be a mountain (rdf: type dbo: mountain). The prefixes (res, dbo, dbp and rdf) are the abbreviations of the base URIs and are defined first. To find out which URIs need to be used, a user could open an entry about any mountain, look up the respective predicates and use that information to build their SPARQL query. The following example query is not about the second-highest mountain in Australia but rather about the highest mountains in DBpedia and their location. The mountains are ordered in descending order by height:</p>
      <p>
        <disp-quote>
          <preformat position="float" xml:space="preserve">PREFIX res: &lt;<ext-link xlink:href="http://dbpedia.org/resource/" ext-link-type="uri">http://dbpedia.org/resource/</ext-link>&gt;PREFIX dbo: &lt;<ext-link xlink:href="http://dbpedia.org/ontology/" ext-link-type="uri">http://dbpedia.org/ontology/</ext-link>&gt;PREFIX dbp: &lt;<ext-link xlink:href="http://dbpedia.org/property/" ext-link-type="uri">http://dbpedia.org/property/</ext-link>&gt;PREFIX rdf: &lt;<ext-link xlink:href="http://www.w3.org/1999/02/22-rdf-syntax-ns" ext-link-type="uri">http://www.w3.org/1999/02/22-rdf-syntax-ns</ext-link>&gt;
SELECT ?Mountain ?location ?heightWhere {?Mountain rdf: type dbo: Mountain.?Mountain dbp: location ?location.?Mountain dbo: elevation ?height.}ORDER BY DESC(?height)Limit 25</preformat>
        </disp-quote>
      </p>
      <p>Within Graph2VR, this search can be done visually without manually looking up all the URIs for the predicates. All outgoing predicates are listed in the menu, so the user only needs to select them.</p>
      <fig position="float" id="F10" fig-type="figure">
        <label>Figure 10.</label>
        <caption>
          <p>Creating a query pattern in Graph2VR (10(A)) and requesting the result graphs (10(B)). (A) Query patterns can be created visually in Graph2VR by selecting the triples in the graph that should be part of the query. The query can be modified using the Language settings, Order By options and the Query Limit slider. (B) After rotating the graph and clicking on ‘Request similar patterns’, the results are displayed in a stack of multiple independent graphs.</p>
        </caption>
        <graphic xlink:href="baae008f10" position="float"/>
      </fig>
      <p>Within Graph2VR, it is possible to select the relevant triples, add those to the query pattern, transform the respective nodes into variables and then send the query to the SPARQL Endpoint. Besides the limit, the example query is the same one used as the example for Gruff (<xref rid="F4" ref-type="fig">Figure 4</xref>). This task might still be tricky for a number of reasons: not every mountain’s height is represented using the same predicate, some mountains might be missing in the database or there might be multiple instances encoding the same mountain. However, this is a limitation of DBpedia data, not Graph2VR.</p>
      <p>In many cases, DBpedia uses language tags that indicate the language used for the literal. Just asking for all labels results in many labels in different languages. We therefore added a language filter feature to Graph2VR that can be set via the settings menu. This allows users to request only labels with a specific language tag or no language tag at all. Once set, this is applied to every query. This may have consequences for the results, as it will not be indicated when the content is unavailable in the desired language or without a language tag. However, being able to set these conditions in the menu is more convenient than writing a SPARQL query to do so.</p>
    </sec>
    <sec id="s5-s3">
      <title>Usability survey</title>
      <p>To assess usability, we conducted a user evaluation with 34 participants. We were interested in how useful the application was for sensemaking, whether users enjoy the experience in VR and what influences their perceptions. By ‘sensemaking’, we mean the ability of the participant to answer questions or get understandable content from the semantic graph using the Graph2VR interface. We adopted this term from the Tarsier paper, which reused it from another earlier paper (<xref rid="R25" ref-type="bibr">25</xref>, <xref rid="R60" ref-type="bibr">60</xref>) . Some participants already had some experience with in GraphDB, SPARQL or Linked Data, but others did not. We guided all first-time users through the application, told them about their options and let them test the different features. Then, we asked the users a series of questions, described in <xref rid="s10" ref-type="sec">Supplementary Appendix B</xref>. The results are summarized in <xref rid="F11 F12" ref-type="fig">Figures 11 and 12</xref> with data from the usability survey available in <xref rid="s10" ref-type="sec">Supplementary Appendix B</xref>.</p>
      <fig position="float" id="F11" fig-type="figure">
        <label>Figure 11.</label>
        <caption>
          <p>Results usability questionnaire questions 9–15.</p>
        </caption>
        <graphic xlink:href="baae008f11" position="float"/>
      </fig>
      <fig position="float" id="F12" fig-type="figure">
        <label>Figure 12.</label>
        <caption>
          <p>Results usability questionnaire questions 19 and 20.</p>
        </caption>
        <graphic xlink:href="baae008f12" position="float"/>
      </fig>
    </sec>
    <sec id="s5-s4">
      <title>Evaluation results</title>
      <p>We identified several factors that influenced how much participants enjoyed testing Graph2VR and how effective they found the tool. One hypothesis was that people who are more used to playing computer games, especially if they have experience with VR glasses, might have an easier time using our application. It was uncertain whether age would have any effect, but we could imagine that younger participants, as digital natives, might find it easier. So there might be a weak correlation. To differentiate between enjoyment and effectiveness, both questions were asked after the VR experience and next to each other. We assumed that people who had a better experience, with fewer bugs, would rate their enjoyment as well as the tool’s effectivity for sensemaking more highly. We also expected a worse rating when the application had problems like poor readability or functions not working. An intuitive user interface makes the application more effective and enjoyable, whereas high complexity can make the application more interesting for experts but more difficult to use for beginners. During our studies, we determined that most participants found navigation and exploration quite easy, while the query-building seemed more challenging. Creating and modifying queries seemed to be noticeably easier for the ‘expert users’ who had at least some experience with Linked Data and writing SPARQL queries. Finally, we looked at the correlations between participants’ answers to the different questions. In addition to the Pearson correlation, <italic toggle="yes">P</italic>-values were calculated to determine the significance of those correlations. The complete correlation matrix is given in <xref rid="s10" ref-type="sec">Supplementary Appendix Table A6</xref>. In <xref rid="T2" ref-type="table">Table 2</xref>, we summarize the statistically significant correlations.</p>
      <table-wrap position="float" id="T2">
        <label>Table 2.</label>
        <caption>
          <p>Interpretation of the statistically significant correlations of the usability study results ordered by p-values.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col align="left" span="1"/>
          </colgroup>
          <tbody>
            <tr>
              <td align="left" rowspan="1" colspan="1">
                <inline-graphic xlink:href="baae008fa1.jpg"/>
              </td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>The questions, results and a more detailed evaluation can be found in Supplementary Appendix B. The findings are mostly as expected: having played video games more often is positively correlated with more intuitive navigation in the virtual world. Most participants also preferred flying to teleportation. We also found a positive correlation between navigation with gesture controls, requesting query results and exploring further details. We were curious whether age would have any significant effect on the experience of Graph2VR. Most participants (26/34) were between 30 and 50 years old. The sample size of people outside that age group was too small and probably not representative enough to draw any statistically significant conclusions. At the end of the experiment, we asked participants how much they enjoyed the experience and how effective Graph2VR is for sensemaking. Since both questions were asked at the same moment after the test, it is not a surprise that the answers are highly correlated. What is interesting is which factors are most relevant. For enjoying the experience, there is a strong correlation between the gesture controls and the graph exploration, as well as the exploration of further details. The exploration of further details is the second-to-last question during the VR session. A positive rating for this question implies that it was possible to request further results and that no major bug prevented this. When a problem occurred, the rating and the overall impression decreased. As Graph2VR was still under active development during the user study, some of the issues were fixed over time. Examples of this are the Limit slider that intersected with the scrollbar around the circle menu, nodes that were hard to select when they were zoomed too large and a layout option that led to many nodes being stapled at the same position.</p>
      <p>Setting up the query, selecting the relevant triples, setting the order (and optionally the limit) and requesting the results seemed to be the most complex part of the study for many participants, especially those with no previous experience with SPARQL. Query-building was also the most complex and error-prone part as we were still developing features that interfered with this process. It also takes users time to really understand how this feature works, especially for individuals who did not have any experience with SPARQL queries. Setting up SPARQL query patterns (like in Gruff) is one of the most relevant features of Graph2VR. While it seemed to already be complex for inexperienced users, users who already had some experience were asking us to implement even more SPARQL commands.</p>
    </sec>
  </sec>
  <sec id="s6">
    <title>Discussion</title>
    <p>Graph2VR is a prototype VR application for visualizing and exploring Linked Data in the form of 3D graphs. After exploring and testing multiple existing tools, we used Unity to create a user interface and DotNetRDF to connect to SPARQL Endpoints. We then tested our tool with a local Virtuoso server and DBpedia’s publicly accessible SPARQL Endpoint. To test the application and obtain feedback and suggestions for improvements, we conducted a usability study with 34 individuals who had never tried Graph2VR before.</p>
    <sec id="s6-s1">
      <title>User feedback</title>
      <p>Most testers were impressed and somewhat overwhelmed at the beginning, especially if they had never used a VR headset before. During the study, we guided them through the different functionalities and explained what they could do. During the tests, we noted their comments. After their VR session, we asked them to give feedback. This yielded valuable feedback from participants, including comments like ‘It felt good to be inside the world of the database’, ‘This is very fun to use and a great way of organizing and querying data’ and ‘I would love to try it out with other ontologies like Orphanet and connect it to applications like the RD-Connect Sample Catalogue (once that is in EMX2 so we can connect it)’ For explanation: The Entity Model eXtensible (EMX) is an internal metadata format of Molgenis, the current version is version 2 (<xref rid="R61" ref-type="bibr">61</xref>).</p>
      <p>We explicitly asked the testers about specific problems they experienced and any suggestions they had for the application. It takes some time to explore all the different functionalities that we have built into Graph2VR. One tester mentioned, [I] “need to get used to the tool, but after that, it is good :)”. Another tester found the movement in space initially challenging: ‘moving in space was a bit hard in the beginning, but started to feel more natural along the way’. The most commonly mentioned issue among the testers was the readability of the text in the application. While the menu and nodes and edges nearby were legible, the labels of distant nodes and edges were hard to read. This problem is related to the resolution of the VR headset, which needs to display texts that are further away in just a couple of pixels. One general critique was that the VR headset is still quite heavy. After long test sessions, some people felt somewhat dizzy or tired. That was not unexpected. When using VR glasses, especially for the first time, manufacturers recommend taking breaks at least every 30 min. Our test sessions (including some introduction and filling in the questionnaire) took 40–60 min, on average.</p>
      <p>We also asked more advanced users some additional questions, such as how Graph2VR compares to other conventional tools. Some of the experts mentioned that typing SPARQL queries by hand would still be faster than using the virtual keyboard and selecting the triples one-by-one. Graph2VR is somewhat limited by not having certain keywords, like Optional and Subqueries. Nevertheless, these users saw the potential that 3D visualization offers: ‘[Graph2VR is] much more fun, also allows much more data to be visualized, opens new possibilities’.</p>
      <p>In all, we received constructive feedback, and we have already fixed several of the issues mentioned by testers. For example:</p>
      <list list-type="bullet">
        <list-item>
          <p>The scrollbar no longer overlaps with the Limit slider.</p>
        </list-item>
        <list-item>
          <p>There was an issue with selecting nodes when the graph was scaled too big.</p>
        </list-item>
        <list-item>
          <p>Some testers asked for more visual feedback when clicking a button or to recognize when new nodes spawn. Both have been added.</p>
        </list-item>
        <list-item>
          <p>As we had at least two left-handed testers, we decided to add an option for left-handed people. It is not perfect as the menu still points to the right, but we have already received some positive feedback.</p>
        </list-item>
      </list>
      <p>Other issues persist and need to be addressed in future versions:</p>
      <list list-type="bullet">
        <list-item>
          <p>When the trigger is pressed while not pointing at any menu item, node,or edge, the menu should close. When scrolling down the scrollbar of the circle menu with the laser, it is easy to slip down the scrollbar, causing the menu to close. The circular scrollbar should be used instead via the slider knob on the scrollbar, a small ball that can be grabbed and then moved around.</p>
        </list-item>
        <list-item>
          <p>Some testers mentioned that the movement speed for flying in Graph2VR was quite fast and highly responsive, making it difficult to control the movement. This could be addressed in the future by adding an option to adjust the movement speed.</p>
        </list-item>
        <list-item>
          <p>When a graph bumps into the ‘floor’, the nodes collide with the floor and the graph deforms, which might result in a flat graph. This can also happen when a query is sent and a stack of result graphs is requested as the new graphs are spawned in the looking direction and might collide with the platform. One way to fix this is to trigger a layout algorithm.</p>
        </list-item>
      </list>
      <p>Overall, user study feedback was valuable for identifying areas of improvement. Participants’ comments complemented their scores and elucidated specific strengths and weaknesses of Graph2VR. For example, several participants noted the difficulty of reading node and edge labels in VR, and this was also reflected in lower scores for readability. Participants with previous VR experience found navigation easier, while those who had experience with SPARQL queries found it easier to create the query patterns.</p>
    </sec>
    <sec id="s6-s2">
      <title>Limitations of VR</title>
      <p>One of our expectations was that the almost unlimited space in VR and 3D could help users visualize more nodes at once. Based on the reviewers’ comments, we can now confirm this. However, one ongoing challenge in VR is the readability due to a limited resolution of the node and edge labels. To improve readability, we increased the font size of texts when hovering on them, but they remain hard to read when the graph is too small or the text is too far away. We also replaced the URIs of nodes and edges with their labels. If no label is available, Graph2VR displays the Compact URI, a shortened version of the URI in which the namespace is replaced by a prefix to shorten the URI. In addition to the textual representation, images, colours and shapes can also help to differentiate between different types of nodes. We had originally planned to allow federated queries for requesting data from multiple SPARQL endpoints at once, but this has not yet been implemented.</p>
    </sec>
    <sec id="s6-s3">
      <title>Ideas for future development</title>
      <p>In the current version of Graph2VR, the colour schema is derived from the VOWL specification (<xref rid="R30" ref-type="bibr">30</xref>). But not all the colours and shapes from VOWL are implemented yet.</p>
      <p>In the future, we would like to add further SPARQL commands to the application. Important SPARQL commands or keywords such as <italic toggle="yes">OPTIONAL</italic>, <italic toggle="yes">SERVICE</italic> (for federated queries), <italic toggle="yes">UNION</italic>, <italic toggle="yes">BIND</italic>, <italic toggle="yes">OFFSET</italic>, <italic toggle="yes">DISTINCT</italic>, <italic toggle="yes">COUNT</italic>, <italic toggle="yes">GROUP BY</italic> and <italic toggle="yes">MINUS</italic>, as well as <italic toggle="yes">FILTER</italic>, are supported by DotNetRDF but have not yet been implemented in the Graph2VR interface. In addition, an ‘undo’ button would be desirable, especially to restore accidentally deleted elements or graphs. Many test users found Graph2VR already quite complex, mainly because they were using it for the first time and did not have much experience with graph databases. Adding new functionalities will only increase the complexity further, so it might be a good idea to create a wizard to lead users through the query-building process in order to make this process easier. Another feature that should be implemented in the future is the ability to log and export some of the queries that have been performed. In addition, it should be possible to export the latest SPARQL query to use, e.g. in a browser. To find a specific object in the menu of outgoing nodes, another layer of entries would be helpful. This could be triggered when clicking the count of objects instead of the object itself. A similar feature would be to display all the existing connections between two nodes when setting the predicate for a new connection between two nodes. Finally, Graph2VR could be transformed into an augmented reality application in the future. This would especially be interesting as a multi-user application.</p>
    </sec>
  </sec>
  <sec id="s7">
    <title>Conclusion</title>
    <p>We developed Graph2VR, a prototype VR application for visualizing and exploring Linked Data in the form of 3D graphs, and conducted a usability study with 34 testers, which provided valuable feedback on the tool’s usability and areas for improvement. We believe that Graph2VR represents a novel and engaging way of visualizing and exploring Linked Data. The overall user experience reported during the usability study was positive, especially among more experienced users. While there are still some limitations and issues to be addressed, we are confident that, with further development and refinement, VR will provide tools for working with large Linked Data graphs.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>baae008_Supp</label>
      <media xlink:href="baae008_supp.zip"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>We would like to thank Gert-Jan Verheij, who leads the visualization group at the Center for Information Technology (CIT) and organizes the XRHub, where we presented Graph2VR. We thank our colleagues Prof. Dr Isabel Fortier and Dr Tina Wey from the Maelstrom Institute in Montreal for providing us with the REACH data as a test dataset. Of course, we thank all participants of our usability study, especially the expert users who gave us critical input and feedback on how to improve the application. Last but not least, we thank Kate Mc Intyre for editing this manuscript.</p>
  </ack>
  <sec sec-type="data-availability" id="s8">
    <title>Data availability</title>
    <p>The source code of Graph2VR is openly available on GitHub <ext-link xlink:href="http://github.com/molgenis/Graph2VR" ext-link-type="uri">http://github.com/molgenis/Graph2VR</ext-link>, under LGPL v3 license. The technical manual can be found in the attachments. The questions from the usability study, as well as the anonymous answers of the participants, can also be found in the attachments. A technical user manual for Graph2VR (version 1) can be found here: <ext-link xlink:href="https://doi.org/10.5281/zenodo.8040594" ext-link-type="uri">https://doi.org/10.5281/zenodo.8040594</ext-link>. A Graph2VR tutorial playlist is available on YouTube (<xref rid="R1" ref-type="bibr">1</xref>): <ext-link xlink:href="https://www.youtube.com/playlist?list=PLRQCsKSUyhNIdUzBNRTmE-_JmuiOEZbdH" ext-link-type="uri">https://www.youtube.com/playlist? list=PLRQCsKSUyhNIdUzBNRTmE-_JmuiOEZbdH</ext-link>.</p>
  </sec>
  <sec id="s9">
    <title>Author contributions</title>
    <p><bold>Alexander Kellmann</bold> contributed to conceptualization, software, writing—original draft, visualization, investigation, data curation, formal analysis</p>
    <p>Max Postema contributed to software and visualization</p>
    <p>Pjotr Svetachov contributed to software and visualization</p>
    <p>Joris de Keijser contributed to software and visualization</p>
    <p>Becca Wilson: contributed to methodology, writing—original draft, writing—review and editing, suspervision</p>
    <p>Esther van Enckevort contributed to supervision, writing—review and editing</p>
    <p>Morris A. Swertz contributed to supervision, writing—review and editing</p>
  </sec>
  <sec id="s10">
    <title>Supplementary Material</title>
    <p><xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref> is available at <italic toggle="yes">Database</italic> online.</p>
  </sec>
  <sec id="s11">
    <title>Funding</title>
    <p>EUCAN-Connect, a federated FAIR platform enabling large-scale analysis of high-value cohort data connecting Europe and Canada in personalized health, which is funded by the European Union’s Horizon 2020 research and innovation programme under grant agreement No 824 989; and a UKRI Innovation Fellowship with HDR UK (MR/S003959/2; R.C.W.).</p>
  </sec>
  <sec sec-type="COI-statement" id="s12">
    <title>Conflict of interest statement</title>
    <p>The authors declare no conflict of interest.</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="R1">
      <label>1.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kellmann</surname><given-names>A.</given-names></string-name> and <string-name><surname>Postema</surname><given-names>M.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Graph2VR tutorial part 1 – 5</article-title>. <ext-link xlink:href="https://www.youtube.com/playlist?list=PLRQCsKSUyhNIdUzBNRTmE-_JmuiOEZbdH" ext-link-type="uri">https://www.youtube.com/playlist?list=PLRQCsKSUyhNIdUzBNRTmE-_JmuiOEZbdH</ext-link> (<comment>4 December 2023, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wilkinson</surname><given-names>M.D.</given-names></string-name>, <string-name><surname>Dumontier</surname><given-names>M.</given-names></string-name>, <string-name><surname>Aalbersberg</surname><given-names>I.J.J.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2016</year>) <article-title>The FAIR Guiding Principles for scientific data management and stewardship</article-title>. <source><italic toggle="yes">Sci. Data.</italic></source>, <volume>3</volume>, <page-range>160018</page-range>.</mixed-citation>
    </ref>
    <ref id="R3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Croset</surname><given-names>S.</given-names></string-name>, <string-name><surname>Rupp</surname><given-names>J.</given-names></string-name> and <string-name><surname>Romacker</surname><given-names>M.</given-names></string-name></person-group> (<year>2016</year>) <article-title>Flexible data integration and curation using a graph-based approach</article-title>. <source><italic toggle="yes">Bioinformatics</italic></source>, <volume>32</volume>, <fpage>918</fpage>–<lpage>925</lpage>.<pub-id pub-id-type="pmid">26556384</pub-id>
</mixed-citation>
    </ref>
    <ref id="R4">
      <label>4.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Berners-Lee</surname><given-names>T.</given-names></string-name> and <string-name><surname>Fischetti</surname><given-names>M.</given-names></string-name></person-group> (<year>2000</year>) <source><italic toggle="yes">Weaving the Web: The Original Design and Ultimate Destiny of the World Wide Web by its inventor.</italic></source>  <publisher-name>HarperCollins</publisher-name>, <publisher-loc>San Francisco, CA</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="R5">
      <label>5.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Berners-Lee</surname><given-names>T.</given-names></string-name></person-group> (<year>2009</year>) <article-title>The next web</article-title>. <ext-link xlink:href="https://www.ted.com/talks/tim_berners_lee_the_next_web/transcript" ext-link-type="uri">https://www.ted.com/talks/tim_berners_lee_the_next_web/transcript</ext-link>.</mixed-citation>
    </ref>
    <ref id="R6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Desimoni</surname><given-names>F.</given-names></string-name> and <string-name><surname>Po</surname><given-names>L.</given-names></string-name></person-group> (<year>2020</year>) <article-title>Empirical evaluation of Linked Data visualization tools</article-title>. <source><italic toggle="yes">Future Gener. Comput. Syst.</italic></source>, <volume>112</volume>, <fpage>258</fpage>–<lpage>282</lpage>.</mixed-citation>
    </ref>
    <ref id="R7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Callahan</surname><given-names>A.</given-names></string-name>, <string-name><surname>Cruz-Toledo</surname><given-names>J.</given-names></string-name> and <string-name><surname>Dumontier</surname><given-names>M.</given-names></string-name></person-group> (<year>2013</year>) <article-title>Ontology-based querying with Bio2RDF’s linked open ata</article-title>. <source><italic toggle="yes">J. Biomed. Semant.</italic></source>, <volume>4 Suppl 1</volume>, <fpage>1</fpage>–<lpage>13</lpage>.</mixed-citation>
    </ref>
    <ref id="R8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fu</surname><given-names>G.</given-names></string-name>, <string-name><surname>Batchelor</surname><given-names>C.</given-names></string-name>, <string-name><surname>Dumontier</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2015</year>) <article-title>PubChemRDF: Towards the semantic annotation of PubChem compound and substance databases</article-title>. <source><italic toggle="yes">J. Cheminformatics</italic></source>, <volume>7</volume>, <page-range>34</page-range>.</mixed-citation>
    </ref>
    <ref id="R9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Galgonek</surname><given-names>J.</given-names></string-name>, <string-name><surname>Hurt</surname><given-names>T.</given-names></string-name>, <string-name><surname>Michlíková</surname><given-names>V.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2016</year>) <article-title>Advanced SPARQL querying in small molecule databases</article-title>. <source><italic toggle="yes">J. Cheminformatics</italic></source>, <volume>8</volume>, <page-range>31</page-range>.</mixed-citation>
    </ref>
    <ref id="R10">
      <label>10.</label>
      <mixed-citation publication-type="other">(<year>2015</year>) <article-title>RDF - Semantic Web Standards</article-title>. <ext-link xlink:href="https://www.w3.org/RDF/" ext-link-type="uri">https://www.w3.org/RDF/</ext-link> (<comment>12 December 2022, date last accepted</comment>).</mixed-citation>
    </ref>
    <ref id="R11">
      <label>11.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Coffey</surname><given-names>D.</given-names></string-name><string-name><surname>Malbraaten</surname><given-names>N.</given-names></string-name><string-name><surname>Le</surname><given-names>T.</given-names></string-name></person-group>  <etal>et al.</etal> (<year>2011</year>) <part-title>Slice WIM</part-title>. In <person-group person-group-type="editor"><string-name><surname>Garland</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Wang</surname>, <given-names>R.</given-names></string-name></person-group> (<edition>eds.</edition>), <source>Symposium on Interactive 3D Graphics and Games</source>. <publisher-name>ACM</publisher-name>, <publisher-loc>New York, NY, USA</publisher-loc>, <fpage>pp. 191</fpage>–<lpage>198</lpage>.</mixed-citation>
    </ref>
    <ref id="R12">
      <label>12.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Czauderna</surname><given-names>T.</given-names></string-name><string-name><surname>Haga</surname><given-names>J.</given-names></string-name><string-name><surname>Kim</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al.</etal> (<year>2018</year>) <part-title>Immersive Analytics Applications in Life and Health Sciences</part-title>. In <person-group person-group-type="editor"><string-name><surname>Marriott</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Schreiber</surname>, <given-names>F.</given-names></string-name> and <string-name><surname>Dwyer</surname>, <given-names>T.</given-names></string-name></person-group>  <etal>et al</etal>. (<edition>eds.</edition>), <source>Immersive Analytics</source>. <publisher-name>Springer International Publishing</publisher-name>, <publisher-loc>Cham</publisher-loc>, <volume>611190</volume>, <fpage>pp. 289</fpage>–<lpage>330</lpage>.</mixed-citation>
    </ref>
    <ref id="R13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lau</surname><given-names>C.W.</given-names></string-name>, <string-name><surname>Qu</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Draper</surname><given-names>D.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2022</year>) <article-title>Virtual reality for the observation of oncology models (VROOM): immersive analytics for oncology patient cohorts</article-title>. <source><italic toggle="yes">Sci. Rep.</italic></source>, <volume>12</volume>, <page-range>11337</page-range>.</mixed-citation>
    </ref>
    <ref id="R14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Qu</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Lau</surname><given-names>C.W.</given-names></string-name>, <string-name><surname>Simoff</surname><given-names>S.J.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2022</year>) <article-title>Review of innovative immersive technologies for healthcare applications</article-title>. <source><italic toggle="yes">Innovations in Digital Health, Diagnostics, and Biomarkers</italic></source>, <volume>2</volume>, <fpage>27</fpage>–<lpage>39</lpage>.</mixed-citation>
    </ref>
    <ref id="R15">
      <label>15.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>McCrae</surname><given-names>J.P.</given-names></string-name></person-group> (<year>2022</year>) <article-title>The Linked Open Data Cloud</article-title>. <ext-link xlink:href="https://lod-cloud.net/" ext-link-type="uri">https://lod-cloud.net/</ext-link> (<comment>13 January 2023, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R16">
      <label>16.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Deligiannidis</surname><given-names>L.</given-names></string-name><string-name><surname>Sheth</surname><given-names>A.P.</given-names></string-name><string-name><surname>Aleman-Meza</surname><given-names>B.</given-names></string-name></person-group> (<year>2006</year>) <part-title>Semantic Analytics Visualization</part-title>. In <person-group person-group-type="editor"><string-name><surname>Hutchison</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Kanade</surname>, <given-names>T.</given-names></string-name> and <string-name><surname>Kittler</surname>, <given-names>J.</given-names></string-name></person-group>  <etal>et al</etal>. (eds.), <source>Intelligence and Security Informatics</source>. <publisher-name>Springer Berlin Heidelberg</publisher-name>, <publisher-loc>Berlin, Heidelberg</publisher-loc>, <volume>3975</volume>, <fpage>pp. 48</fpage>–<lpage>59</lpage>.</mixed-citation>
    </ref>
    <ref id="R17">
      <label>17.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>Sony Green &amp; Robert Allison</collab></person-group>. (<year>2019</year>) <article-title>KINEVIZ GraphXR: How to GraphXR: for GraphXR v2.2.1</article-title>. <ext-link xlink:href="https://static1.squarespace.com/static/5c58b86e8dfc8c2d0d700050/t/5df2b6134e0d57635d14df4b/1576187456752/How+to+GraphXR.pdf" ext-link-type="uri">https://static1.squarespace.com/static/5c58b86e8dfc8c2d0d700050/t/5df2b6134e0d57635d14df4b/1576187456752/How+to+GraphXR.pdf</ext-link> (<comment>24 August 2020, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R18">
      <label>18.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Asturiano</surname><given-names>V.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2021</year>) <article-title>vasturiano/3d-force-graph</article-title>. <ext-link xlink:href="https://github.com/vasturiano/3d-force-graph/" ext-link-type="uri">https://github.com/vasturiano/3d-force-graph/</ext-link> (<comment>18 February 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R19">
      <label>19.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Camarda</surname><given-names>D.V.</given-names></string-name>, <string-name><surname>Mazzini</surname><given-names>S.</given-names></string-name> and <string-name><surname>Antonuccio</surname><given-names>A</given-names></string-name></person-group>. (<year>2012</year>) <article-title>LodLive, exploring the web of data</article-title>. <italic toggle="yes">Proceedings of the 8th International Conference on Semantic Systems</italic>. <publisher-name>ACM Digital Library</publisher-name>, <conf-loc>New York, NY, USA</conf-loc>. <page-range>p. 197</page-range>.</mixed-citation>
    </ref>
    <ref id="R20">
      <label>20.</label>
      <mixed-citation publication-type="other">(<year>2020</year>) <article-title>GraphDB</article-title>. <ext-link xlink:href="https://graphdb.ontotext.com/" ext-link-type="uri">https://graphdb.ontotext.com/</ext-link> (<comment>5 March 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R21">
      <label>21.</label>
      <mixed-citation publication-type="other">(<year>2022</year>) <article-title>Toran (VR tech demo on steam)</article-title>. <ext-link xlink:href="https://store.steampowered.com/app/720300/Toran/" ext-link-type="uri">https://store.steampowered.com/app/720300/Toran/</ext-link> (<comment>9 March 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R22">
      <label>22.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>OWL Web Ontology Language Overview</collab></person-group>. (<year>2009</year>) <ext-link xlink:href="https://www.w3.org/TR/owl-features/" ext-link-type="uri">https://www.w3.org/TR/owl-features/</ext-link> (<comment>19 May 2017, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lohmann</surname><given-names>S.</given-names></string-name>, <string-name><surname>Negru</surname><given-names>S.</given-names></string-name>, <string-name><surname>Haag</surname><given-names>F.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2016</year>) <article-title>Visualizing Ontologies with VOWL</article-title>. <source><italic toggle="yes">Semantic Web</italic></source>, <volume>7</volume>, <fpage>399</fpage>–<lpage>419</lpage>.</mixed-citation>
    </ref>
    <ref id="R24">
      <label>24.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>Franz Inc</collab></person-group>. (<year>2020</year>) <article-title>Gruff</article-title>. <ext-link xlink:href="https://allegrograph.com/products/gruff/" ext-link-type="uri">https://allegrograph.com/products/gruff/</ext-link> (<comment>3 November 2020, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Viola</surname><given-names>F.</given-names></string-name>, <string-name><surname>Roffia</surname><given-names>L.</given-names></string-name>, <string-name><surname>Antoniazzi</surname><given-names>F.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2018</year>) <article-title>Interactive 3D Exploration of RDF Graphs through Semantic Planes</article-title>. <source><italic toggle="yes">Future Internet</italic></source>, <volume>10</volume>, <fpage>1</fpage>–<lpage>30</lpage>.</mixed-citation>
    </ref>
    <ref id="R26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dudáš</surname><given-names>M.</given-names></string-name>, <string-name><surname>Lohmann</surname><given-names>S.</given-names></string-name>, <string-name><surname>Svátek</surname><given-names>V.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2018</year>) <article-title>Ontology visualization methods and tools: a survey of the state of the art</article-title>. <source><italic toggle="yes">Knowl. Eng. Rev.</italic></source>, <page-range>33</page-range>.</mixed-citation>
    </ref>
    <ref id="R27">
      <label>27.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Lanzenberger</surname><given-names>M.</given-names></string-name>, <string-name><surname>Sampson</surname><given-names>J.</given-names></string-name> and <string-name><surname>Rester</surname><given-names>M</given-names></string-name></person-group>. (<year>2009</year>) <article-title>Visualization in Ontology Tools</article-title>. <italic toggle="yes">International Conference on Complex, Intelligent and Software Intensive Systems</italic>. <publisher-name>IEEE</publisher-name>, <conf-loc>Piscataway, NJ</conf-loc>. <fpage>pp. 705</fpage>–<lpage>711</lpage>.</mixed-citation>
    </ref>
    <ref id="R28">
      <label>28.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Palma</surname><given-names>R.</given-names></string-name></person-group> (<year>2021</year>) <article-title>A Knowledge Graph for the Agri-Food Sector</article-title>. <ext-link xlink:href="https://blog.metaphacts.com/a-knowledge-graph-for-the-agri-food-sector" ext-link-type="uri">https://blog.metaphacts.com/a-knowledge-graph-for-the-agri-food-sector</ext-link> (<comment>8 March 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Musen</surname><given-names>M.A.</given-names></string-name></person-group> (<year>2015</year>) <article-title>The protégé Project: a look back and a look forward</article-title>. <source><italic toggle="yes">AI matters</italic></source>, <volume>1</volume>, <fpage>4</fpage>–<lpage>12</lpage>.<pub-id pub-id-type="pmid">27239556</pub-id>
</mixed-citation>
    </ref>
    <ref id="R30">
      <label>30.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Steffen Lohmann</surname><given-names>S.N.</given-names></string-name></person-group> (<year>2019</year>) <article-title>VOWL: Visual Notation for OWL Ontologies</article-title>. <ext-link xlink:href="http://vowl.visualdataweb.org/" ext-link-type="uri">http://vowl.visualdataweb.org/</ext-link> (<comment>13 January 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R31">
      <label>31.</label>
      <mixed-citation publication-type="other">(<year>2017</year>) <article-title>QueryVOWL</article-title>. <ext-link xlink:href="http://vowl.visualdataweb.org/queryvowl/queryvowl.html" ext-link-type="uri">http://vowl.visualdataweb.org/queryvowl/queryvowl.html</ext-link> (<comment>13 January 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R32">
      <label>32.</label>
      <mixed-citation publication-type="other">(<year>2019</year>) <article-title>WebVOWL</article-title>. <ext-link xlink:href="http://www.visualdataweb.de/webvowl/#" ext-link-type="uri">http://www.visualdataweb.de/webvowl/#</ext-link> (<comment>13 January 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R33">
      <label>33.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><given-names>Fabio</given-names><surname>V.</surname></string-name></person-group>  <etal>et al</etal>. (<year>2018</year>) <article-title>Tarsier – exploring DBpedia</article-title>. <ext-link xlink:href="https://www.youtube.com/watch?v=OgoxFWAb1vQ" ext-link-type="uri">https://www.youtube.com/watch?v=OgoxFWAb1vQ</ext-link> (<comment>2 December 2022, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R34">
      <label>34.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kellmann</surname><given-names>A.J.</given-names></string-name>, <string-name><surname>Postema</surname><given-names>M.</given-names></string-name>, <string-name><surname>Keijser</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2023</year>) <article-title>Graph2VR Manual</article-title>. <source><italic toggle="yes">Zenodo</italic></source>, <fpage>1</fpage>–<lpage>20</lpage>.</mixed-citation>
    </ref>
    <ref id="R35">
      <label>35.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Pienta</surname><given-names>R.</given-names></string-name>, <string-name><surname>Abello</surname><given-names>J.</given-names></string-name>, <string-name><surname>Kahng</surname><given-names>M</given-names></string-name></person-group>. <etal>et al.</etal> (<year>2015</year>) <article-title>Scalable graph exploration and visualization: Sensemaking challenges and opportunities</article-title>. In <italic toggle="yes">2015 International Conference on Big Data and Smart Computing (BIGCOMP)</italic>, <conf-loc>Jeju, Republic of Korea</conf-loc>, <conf-date>09.02.2015-11.02.2015</conf-date>. <publisher-name>IEEE</publisher-name>, <fpage>pp. 271</fpage>–<lpage>278</lpage>.</mixed-citation>
    </ref>
    <ref id="R36">
      <label>36.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Sund</surname><given-names>D.</given-names></string-name></person-group> (<year>2016</year>) <article-title>Comparison of Visualization Algorithms for Graphs and Implementation of Visualization Algorithm for Multi-Touch table using JavaFX</article-title>, <source>Bachelor Thesis</source>, <publisher-name>Linköping</publisher-name>.</mixed-citation>
    </ref>
    <ref id="R37">
      <label>37.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fruchterman</surname><given-names>T.M.J.</given-names></string-name> and <string-name><surname>Reingold</surname><given-names>E.M.</given-names></string-name></person-group> (<year>1991</year>) <article-title>Graph drawing by force-directed placement</article-title>. <source><italic toggle="yes">Softw. - Pract. Exp.</italic></source>, <volume>21</volume>, <fpage>1129</fpage>–<lpage>1164</lpage>.</mixed-citation>
    </ref>
    <ref id="R38">
      <label>38.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barnes</surname><given-names>J.</given-names></string-name> and <string-name><surname>Hut</surname><given-names>P.</given-names></string-name></person-group> (<year>1986</year>) <article-title>A hierarchical O(N log N) force-calculation algorithm</article-title>. <source><italic toggle="yes">Nature</italic></source>, <volume>324</volume>, <fpage>446</fpage>–<lpage>449</lpage>.</mixed-citation>
    </ref>
    <ref id="R39">
      <label>39.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Swinehart</surname><given-names>C.</given-names></string-name></person-group> (<year>2011</year>) <article-title>The Barnes-Hut Algorithm</article-title>. <ext-link xlink:href="http://arborjs.org/docs/barnes-hut" ext-link-type="uri">http://arborjs.org/docs/barnes-hut</ext-link> (<comment>3 December 2020, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R40">
      <label>40.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kamada</surname><given-names>T.</given-names></string-name> and <string-name><surname>Kawai</surname><given-names>S.</given-names></string-name></person-group> (<year>1989</year>) <article-title>An algorithm for drawing general undirected graphs</article-title>. <source><italic toggle="yes">Inf. Process. Lett.</italic></source>, <volume>31</volume>, <fpage>7</fpage>–<lpage>15</lpage>.</mixed-citation>
    </ref>
    <ref id="R41">
      <label>41.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Teyseyre</surname><given-names>A.R.</given-names></string-name> and <string-name><surname>Campo</surname><given-names>M.R.</given-names></string-name></person-group> (<year>2009</year>) <article-title>An overview of 3D software visualization</article-title>. <source><italic toggle="yes">IEEE Trans. Vis. Comput. Graph.</italic></source>, <volume>15</volume>, <fpage>87</fpage>–<lpage>105</lpage>.<pub-id pub-id-type="pmid">19008558</pub-id>
</mixed-citation>
    </ref>
    <ref id="R42">
      <label>42.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Batchelor</surname><given-names>C.</given-names></string-name><string-name><surname>Brenninkmeijer</surname><given-names>C.Y.A.</given-names></string-name><string-name><surname>Chichester</surname><given-names>C.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2014</year>) <part-title>Scientific Lenses to Support Multiple Views over Linked Chemistry Data</part-title>. <source><italic toggle="yes">The Semantic Web - ISWC 2014.</italic></source> In: <person-group person-group-type="editor"><string-name><surname>Mika</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Tudorache</surname>, <given-names>T.</given-names></string-name> and <string-name><surname>Bernstein</surname>, <given-names>A.</given-names></string-name></person-group> (eds.), <publisher-name>Springer International Publishing</publisher-name>, <publisher-loc>Cham</publisher-loc>, <fpage>98</fpage>–<lpage>113</lpage>.</mixed-citation>
    </ref>
    <ref id="R43">
      <label>43.</label>
      <mixed-citation publication-type="other">(<year>2021</year>) <article-title>Noda (by Coding Leap - Steam link)</article-title>. <ext-link xlink:href="https://store.steampowered.com/app/578060/Noda/" ext-link-type="uri">https://store.steampowered.com/app/578060/Noda/</ext-link> (<comment>11 May 2021, dare last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R44">
      <label>44.</label>
      <mixed-citation publication-type="other">(<year>2020</year>) <article-title>aestheticinteractive/Hover-UI-Kit</article-title>. <ext-link xlink:href="https://github.com/aestheticinteractive/Hover-UI-Kit" ext-link-type="uri">https://github.com/aestheticinteractive/Hover-UI-Kit</ext-link> (<comment>10 Decemebr 2020, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R45">
      <label>45.</label>
      <mixed-citation publication-type="other">(<year>2021</year>) <article-title>Getting Started with metaphactory</article-title>. <ext-link xlink:href="https://help.metaphacts.com/resource/Help:Start" ext-link-type="uri">https://help.metaphacts.com/resource/Help:Start</ext-link> (<comment>5 March 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R46">
      <label>46.</label>
      <mixed-citation publication-type="other">(<year>2022</year>) <article-title>VRKeys|Input Management|Unity Asset Store</article-title>. <ext-link xlink:href="https://assetstore.unity.com/packages/tools/input-management/vrkeys-99222" ext-link-type="uri">https://assetstore.unity.com/packages/tools/input-management/vrkeys-99222</ext-link> (<comment>24 March 2022, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R47">
      <label>47.</label>
      <mixed-citation publication-type="other">(<year>2021</year>) <article-title>OpenLink Software: Virtuoso Homepage</article-title>. <ext-link xlink:href="https://virtuoso.openlinksw.com/" ext-link-type="uri">https://virtuoso.openlinksw.com/</ext-link> (<comment>25 February 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R48">
      <label>48.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>Unity Technologies</collab></person-group>.<article-title>Unity</article-title>. <ext-link xlink:href="https://unity.com/" ext-link-type="uri">https://unity.com/</ext-link> (<comment>24 February 2022, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R49">
      <label>49.</label>
      <mixed-citation publication-type="other">(<year>2022</year>) <article-title>MozillaReality/unity-webxr-export: assets for creating WebXR-enabled Unity3D projects</article-title>. <ext-link xlink:href="https://github.com/mozillareality/unity-webxr-export" ext-link-type="uri">https://github.com/mozillareality/unity-webxr-export</ext-link> (<comment>28 September 2022, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R50">
      <label>50.</label>
      <mixed-citation publication-type="other"><article-title>tenforce/virtuoso</article-title>. <ext-link xlink:href="https://hub.docker.com/r/tenforce/virtuoso/" ext-link-type="uri">https://hub.docker.com/r/tenforce/virtuoso/</ext-link> (<comment>3 March 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R51">
      <label>51.</label>
      <mixed-citation publication-type="other">(<year>2020</year>) <article-title>DotNetRDF</article-title>. <ext-link xlink:href="https://www.dotnetrdf.org/" ext-link-type="uri">https://www.dotnetrdf.org/</ext-link> (<comment>15 September 2020, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R52">
      <label>52.</label>
      <mixed-citation publication-type="other">(<year>2020</year>) <article-title>dotnetrdf/dotnetrdf</article-title>. <ext-link xlink:href="https://github.com/dotnetrdf/dotnetrdf/wiki/UserGuide-Querying-With-SPARQL" ext-link-type="uri">https://github.com/dotnetrdf/dotnetrdf/wiki/UserGuide-Querying-With-SPARQL</ext-link> (<comment>12 August 2020, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R53">
      <label>53.</label>
      <mixed-citation publication-type="other">(<year>2020</year>) <article-title>Release SteamVR Unity Plugin v2.6.0b4 - SDK 1.13.10 - ValveSoftware/steamvr_unity_plugin</article-title>. <ext-link xlink:href="https://github.com/ValveSoftware/steamvr_unity_plugin/releases/tag/2.6.0b4" ext-link-type="uri">https://github.com/ValveSoftware/steamvr_unity_plugin/releases/tag/2.6.0b4</ext-link> (<comment>12 August 2020, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R54">
      <label>54.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>The Khronos Group</collab></person-group>. (<year>2016</year>) <article-title>OpenXR - High-performance access to AR and VR – collectively known as XR – platforms and devices</article-title>. <ext-link xlink:href="https://www.khronos.org/openxr/" ext-link-type="uri">https://www.khronos.org/openxr/</ext-link> (<comment>8 March 2022, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R55">
      <label>55.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Bastian</given-names><surname>M.</surname></string-name>, <string-name><given-names>Heymann</given-names><surname>S.</surname></string-name>, and <string-name><given-names>Jacomy</given-names><surname>M.</surname></string-name></person-group> (<year>2009</year>) <article-title>Gephi: An Open Source Software for Exploring and Manipulating Networks</article-title>. <source>Proceedings of the International AAAI Conference on Web and Social Media</source>, <volume>3</volume>, <fpage>361</fpage>–<lpage>362</lpage>.</mixed-citation>
    </ref>
    <ref id="R56">
      <label>56.</label>
      <mixed-citation publication-type="other">(<year>2022</year>) <article-title>Using Full Text Search in SPARQL</article-title>. <ext-link xlink:href="https://docs.openlinksw.com/virtuoso/rdfsparqlrulefulltext/" ext-link-type="uri">https://docs.openlinksw.com/virtuoso/rdfsparqlrulefulltext/</ext-link> (<comment>2 December 2022, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R57">
      <label>57.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Bast</surname><given-names>H.</given-names></string-name>, <string-name><surname>Kalmbach</surname><given-names>J.</given-names></string-name>, <string-name><surname>Klumpp</surname><given-names>T</given-names></string-name></person-group>. <etal>et al.</etal> (<year>2022</year>) <article-title>Efficient and Effective SPARQL Autocompletion on Very Large Knowledge Graphs</article-title>. <italic toggle="yes">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management. ACM</italic>. <conf-loc>New York, NY, USA</conf-loc>. <fpage>pp. 2893</fpage>–<lpage>2902</lpage>.</mixed-citation>
    </ref>
    <ref id="R58">
      <label>58.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Auer</surname><given-names>S.</given-names></string-name>, <string-name><surname>Bizer</surname><given-names>C.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2007</year>) <part-title>DBpedia: A Nucleus for a Web of Open Data</part-title>. <source><italic toggle="yes">The Semantic Web.</italic></source>, Vol. <volume>4825</volume>, <series>Lecture Notes in Computer Science</series>, <publisher-loc>Berlin, Heidelberg</publisher-loc>, <publisher-name>Springer</publisher-name>, pp. <fpage>722</fpage>–<lpage>735</lpage>.</mixed-citation>
    </ref>
    <ref id="R59">
      <label>59.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Cimiano</surname><given-names>P.</given-names></string-name></person-group> (<year>2015</year>) <source><italic toggle="yes">Personal Communication During the “Semantic Web” Lecture at the</italic></source>  <publisher-name>University of Bielefeld</publisher-name>.</mixed-citation>
    </ref>
    <ref id="R60">
      <label>60.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Motta</surname><given-names>E.</given-names></string-name>, <string-name><surname>Mulholland</surname><given-names>P.</given-names></string-name> and <string-name><surname>Peroni</surname><given-names>S</given-names></string-name></person-group>. (<year>2011</year>) <article-title>A Novel Approach to Visualizing and Navigating Ontologies</article-title>. <volume>7031</volume>, <fpage>470</fpage>–<lpage>483</lpage>.</mixed-citation>
    </ref>
    <ref id="R61">
      <label>61.</label>
      <mixed-citation publication-type="other">(<year>2024</year>) <article-title>Molgenis 6 user documentation</article-title>. <part-title>Formats</part-title>. <ext-link xlink:href="https://molgenis.gitbooks.io/molgenis/content/v/6.0/user_documentation/guide-upload.html" ext-link-type="uri">https://molgenis.gitbooks.io/molgenis/content/v/6.0/user_documentation/guide-upload.html</ext-link>.</mixed-citation>
    </ref>
    <ref id="R62">
      <label>62.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pirch</surname><given-names>S.</given-names></string-name>, <string-name><surname>Müller</surname><given-names>F.</given-names></string-name>, <string-name><surname>Iofinova</surname><given-names>E.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2021</year>) <article-title>The VRNetzer platform enables interactive network analysis in Virtual Reality</article-title>. <source><italic toggle="yes">Nat. Commun.</italic></source>, <volume>12</volume>, <page-range>2432</page-range>.</mixed-citation>
    </ref>
    <ref id="R63">
      <label>63.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Dooley</surname><given-names>D.</given-names></string-name>, <string-name><surname>Nguyen</surname><given-names>M.</given-names></string-name> and <string-name><surname>Hsiao</surname><given-names>W.</given-names></string-name></person-group> (<year>2023</year>) <article-title>3D Visualization of Application Ontology Class Hierarchies</article-title>. <ext-link xlink:href="http://genepio.org/ontotrek" ext-link-type="uri">http://genepio.org/ontotrek</ext-link> (<comment>19 May 2020, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R64">
      <label>64.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>Kineviz inc</collab></person-group>. (<year>2022</year>) <article-title>Login</article-title>. <ext-link xlink:href="https://graphxr.kineviz.com/login" ext-link-type="uri">https://graphxr.kineviz.com/login</ext-link> (<comment>8 March 2022, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R65">
      <label>65.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>McVeigh-Schultz</surname><given-names>J.</given-names></string-name></person-group> (<year>2018</year>) <source><italic toggle="yes">Immersive Human Networks: An Exploration of How VR Network Analysis Can Transform Sensemaking and Help Organizations Become More Agile INSTITUTE FOR THE FUTURE 201 Hamilton Avenue</italic></source>, <publisher-loc>Palo Alto, CA 94301</publisher-loc>, <ext-link xlink:href="https://www.iftf.org/fileadmin/user_upload/images/More_Projects_Images/IFTF_Immersive_Human_Networks_FINAL_READER_100918__1_.pdf" ext-link-type="uri">https://www.iftf.org/fileadmin/user_upload/images/More_Projects_Images/IFTF_Immersive_Human_Networks_FINAL_READER_100918__1_.pdf</ext-link>.</mixed-citation>
    </ref>
    <ref id="R66">
      <label>66.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Capece</surname><given-names>N.</given-names></string-name>, <string-name><surname>Erra</surname><given-names>U.</given-names></string-name> and <string-name><surname>Grippa</surname><given-names>J</given-names></string-name></person-group>. (<year>2018</year>) <article-title>Graphvr: A virtual reality tool for the exploration of graphs with htc vive system</article-title>. In: <italic toggle="yes">2018 22nd International Conference Information Visualisation (iV 2018)</italic>. <publisher-name>IEEE</publisher-name>, <conf-loc>Piscataway, NJ</conf-loc>, <fpage>pp. 448</fpage>–<lpage>453</lpage>.</mixed-citation>
    </ref>
    <ref id="R67">
      <label>67.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Panahi</surname><given-names>A.</given-names></string-name></person-group> (<year>2017</year>) <article-title>Big data visualization platform for mixed reality</article-title>, <publisher-name>VCU Libraries</publisher-name>, <pub-id pub-id-type="doi">10.25772/6MDD-2B85</pub-id>.</mixed-citation>
    </ref>
    <ref id="R68">
      <label>68.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Radics</surname><given-names>P.J.</given-names></string-name>, <string-name><surname>Polys</surname><given-names>N.F.</given-names></string-name>, <string-name><surname>Neuman</surname><given-names>S.P.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2015</year>) <article-title>OSNAP! Introducing the open semantic network analysis platform</article-title>. <source><italic toggle="yes">Visualization and Data Analysis 2015</italic></source>, <volume>9397</volume>, <fpage>38</fpage>–<lpage>52</lpage>.</mixed-citation>
    </ref>
    <ref id="R69">
      <label>69.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>OpenGraphiti: Data Visualization Framework</collab></person-group>. (<year>2014</year>) <ext-link xlink:href="https://www.opengraphiti.com/" ext-link-type="uri">https://www.opengraphiti.com/</ext-link> (<comment>7 March 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R70">
      <label>70.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>GitHub</collab></person-group>. (<year>2014</year>) <article-title>opendns/dataviz</article-title>, <ext-link xlink:href="https://github.com/opendns/dataviz" ext-link-type="uri">https://github.com/opendns/dataviz</ext-link> (<comment>7 March 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R71">
      <label>71.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Bremer</surname><given-names>E.</given-names></string-name> and <collab>Haylyn</collab></person-group>. (<year>2014</year>) <ext-link xlink:href="http://haylyn.io/" ext-link-type="uri">http://haylyn.io/</ext-link> (<comment>6 January 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R72">
      <label>72.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>LodLive – browsing the Web of Data</collab></person-group>. (<year>2012</year>) <ext-link xlink:href="http://lodlive.it/" ext-link-type="uri">http://lodlive.it/</ext-link> (<comment>15 Septemeber 2019, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R73">
      <label>73.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Philipp</surname>  <given-names>H.</given-names></string-name>, <string-name><surname>Steffen</surname>  <given-names>L.</given-names></string-name>, <string-name><surname>Timo</surname>  <given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2010</year>) <article-title>RelFinder – Visual Data Web</article-title>. <ext-link xlink:href="http://www.visualdataweb.org/relfinder.php" ext-link-type="uri">http://www.visualdataweb.org/relfinder.php</ext-link> (<comment>25 February 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R74">
      <label>74.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pavlopoulos</surname><given-names>G.A.</given-names></string-name>, <string-name><surname>O’Donoghue</surname><given-names>S.I.</given-names></string-name>, <string-name><surname>Satagopam</surname><given-names>V.P.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2008</year>) <article-title>Arena3D: visualization of biological networks in 3D</article-title>. <source><italic toggle="yes">BMC Syst. Biol.</italic></source>, <volume>2</volume>, <page-range>104</page-range>.</mixed-citation>
    </ref>
    <ref id="R75">
      <label>75.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Secrier</surname><given-names>M.</given-names></string-name>, <string-name><surname>Pavlopoulos</surname><given-names>G.A.</given-names></string-name>, <string-name><surname>Aerts</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2012</year>) <article-title>Arena3D: visualizing time-driven phenotypic differences in biological systems</article-title>. <source><italic toggle="yes">BMC Bioinformatics</italic></source>, <volume>13</volume>, <page-range>45</page-range>.</mixed-citation>
    </ref>
    <ref id="R76">
      <label>76.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koutrouli</surname><given-names>M.</given-names></string-name>, <string-name><surname>Karatzas</surname><given-names>E.</given-names></string-name>, <string-name><surname>Paez-Espino</surname><given-names>D.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2020</year>) <article-title>A Guide to Conquer the Biological Network Era Using Graph Theory</article-title>. <source><italic toggle="yes">Front. bioeng. biotechnol.</italic></source>, <volume>8</volume>, <page-range>34</page-range>.</mixed-citation>
    </ref>
    <ref id="R77">
      <label>77.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><collab>Ramanathan Somasundaram</collab></person-group>. (<year>2007</year>) <article-title>ONTOSELF: A 3D ONTOLOGY VISUALIZATION TOOL</article-title>, <source>Master Thesis</source>, <publisher-loc>Oxford, Ohio</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="R78">
      <label>78.</label>
      <mixed-citation publication-type="other"><article-title>The Interactorium - Systems Biology Initiative</article-title>. <ext-link xlink:href="https://www.systemsbiology.org.au/software/the-interactorium/" ext-link-type="uri">https://www.systemsbiology.org.au/software/the-interactorium/</ext-link> (<comment>25 May 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R79">
      <label>79.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Bosca</surname><given-names>A.</given-names></string-name>, <string-name><surname>Bonino</surname><given-names>D.</given-names></string-name> and <string-name><surname>Pellegrino</surname><given-names>P</given-names></string-name></person-group>. (<year>2005</year>) <article-title>OntoSphere: more than a 3D ontology visualization tool</article-title>. <italic toggle="yes">Semantic Web Applications and Perspectives, Proceedings of the 2nd Italian Semantic Web Workshop</italic>, <conf-loc>Trento, Italy</conf-loc>, <conf-date>December 14-16, 2005</conf-date>.</mixed-citation>
    </ref>
    <ref id="R80">
      <label>80.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Reski</surname>  <given-names>N.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Change your Perspective</article-title>. <source>Master’s Thesis</source>, <publisher-name>Institutionen för medieteknik, Linnéuniversitetet, Växjö</publisher-name>. <ext-link xlink:href="https://www-diva-portal-org.proxy-ub.rug.nl/smash/get/diva2:861573/FULLTEXT01.pdf" ext-link-type="uri">https://www-diva-portal-org.proxy-ub.rug.nl/smash/get/diva2:861573/FULLTEXT01.pdf</ext-link> (<comment>10 December 2020, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R81">
      <label>81.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Zainab</surname><given-names>S.S.</given-names></string-name>, <string-name><surname>Saleem</surname><given-names>M.</given-names></string-name>, <string-name><surname>Mehmood</surname><given-names>Q.</given-names></string-name></person-group>  <etal>et al.</etal> (<year>2015</year>) <article-title>FedViz: A Visual Interface for SPARQL Queries Formulation and Execution</article-title>. <source>In VOILA@ISWC 2015</source>. <ext-link xlink:href="http://svn.aksw.org/papers/2015/ISWC-VOILA_FedViz/public.pdf" ext-link-type="uri">http://svn.aksw.org/papers/2015/ISWC-VOILA_FedViz/public.pdf</ext-link>.</mixed-citation>
    </ref>
    <ref id="R82">
      <label>82.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Pattie</surname><given-names>C.</given-names></string-name>, <string-name><surname>Wilson</surname><given-names>B.</given-names></string-name>, <string-name><surname>Mullally</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al.</etal>  <article-title>Investigating individual differences influencing the understanding of statistical concepts in immersive data visualisations</article-title>, <source>Authors’ version from 2020 – unpublished work</source>.</mixed-citation>
    </ref>
  </ref-list>
</back>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Database (Oxford)</journal-id>
    <journal-id journal-id-type="iso-abbrev">Database (Oxford)</journal-id>
    <journal-id journal-id-type="publisher-id">databa</journal-id>
    <journal-title-group>
      <journal-title>Database: The Journal of Biological Databases and Curation</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1758-0463</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
      <publisher-loc>UK</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">11184448</article-id>
    <article-id pub-id-type="pmid">38554132</article-id>
    <article-id pub-id-type="doi">10.1093/database/baae008</article-id>
    <article-id pub-id-type="publisher-id">baae008</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Article</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI00960</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Visualization and exploration of linked data using virtual reality</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-6108-5552</contrib-id>
        <name>
          <surname>Kellmann</surname>
          <given-names>Alexander J</given-names>
        </name>
        <aff><institution content-type="department">Department of Genetics, University of Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff>
        <aff><institution content-type="department">Department of Genetics, University Medical Center Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Postema</surname>
          <given-names>Max</given-names>
        </name>
        <aff><institution content-type="department">Department of Genetics, University Medical Center Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>de Keijser</surname>
          <given-names>Joris</given-names>
        </name>
        <aff><institution content-type="department">Department of Genetics, University Medical Center Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Svetachov</surname>
          <given-names>Pjotr</given-names>
        </name>
        <aff><institution content-type="department">Center of information technology, University of Groningen</institution>, Nettelbosje 1, Groningen, Groningen 9747 AJ, <country country="NL">The Netherlands</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-2294-593X</contrib-id>
        <name>
          <surname>Wilson</surname>
          <given-names>Rebecca C</given-names>
        </name>
        <aff><institution content-type="department">Public Health, Policy &amp; Systems, University of Liverpool</institution>, Block B, 1st Floor, Waterhouse Building, 1-5 Dover Street, Liverpool L69 3GL, <country country="GB">United Kingdom</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-2440-3993</contrib-id>
        <name>
          <surname>van Enckevort</surname>
          <given-names>Esther J</given-names>
        </name>
        <aff><institution content-type="department">Department of Genetics, University of Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff>
        <aff><institution content-type="department">Department of Genetics, University Medical Center Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-0979-3401</contrib-id>
        <name>
          <surname>Swertz</surname>
          <given-names>Morris A</given-names>
        </name>
        <!--m.a.swertz@umcg.nl-->
        <aff><institution content-type="department">Department of Genetics, University of Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff>
        <aff><institution content-type="department">Department of Genetics, University Medical Center Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff>
        <xref rid="COR0001" ref-type="corresp"/>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="COR0001">*Corresponding author: Tel: +31 50 3617100; Fax: +31 50 3617231; Email: <email xlink:href="m.a.swertz@umcg.nl">m.a.swertz@umcg.nl</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2024-02-23">
      <day>23</day>
      <month>2</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>23</day>
      <month>2</month>
      <year>2024</year>
    </pub-date>
    <volume>2024</volume>
    <elocation-id>baae008</elocation-id>
    <history>
      <date date-type="accepted">
        <day>25</day>
        <month>1</month>
        <year>2024</year>
      </date>
      <date date-type="rev-recd">
        <day>18</day>
        <month>12</month>
        <year>2023</year>
      </date>
      <date date-type="received">
        <day>27</day>
        <month>6</month>
        <year>2023</year>
      </date>
      <date date-type="editorial-decision">
        <day>21</day>
        <month>12</month>
        <year>2023</year>
      </date>
      <date date-type="corrected-typeset">
        <day>23</day>
        <month>2</month>
        <year>2024</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2024. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2024</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="baae008.pdf"/>
    <abstract>
      <title>Abstract</title>
      <p>In this report, we analyse the use of virtual reality (VR) as a method to navigate and explore complex knowledge graphs. Over the past few decades, linked data technologies [Resource Description Framework (RDF) and Web Ontology Language (OWL)] have shown to be valuable to encode such graphs and many tools have emerged to interactively visualize RDF. However, as knowledge graphs get larger, most of these tools struggle with the limitations of 2D screens or 3D projections. Therefore, in this paper, we evaluate the use of VR to visually explore SPARQL Protocol and RDF Query Language (SPARQL) (construct) queries, including a series of tutorial videos that demonstrate the power of VR (see Graph2VR tutorial playlist: <ext-link xlink:href="https://www.youtube.com/playlist?list=PLRQCsKSUyhNIdUzBNRTmE-_JmuiOEZbdH" ext-link-type="uri">https://www.youtube.com/playlist?list=PLRQCsKSUyhNIdUzBNRTmE-_JmuiOEZbdH</ext-link>). We first review existing methods for Linked Data visualization and then report the creation of a prototype, Graph2VR. Finally, we report a first evaluation of the use of VR for exploring linked data graphs. Our results show that most participants enjoyed testing Graph2VR and found it to be a useful tool for graph exploration and data discovery. The usability study also provides valuable insights for potential future improvements to Linked Data visualization in VR.</p>
    </abstract>
    <counts>
      <page-count count="15"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec id="s1">
    <title>Introduction</title>
    <p>In recent years Linked Data has increased in popularity for representing complex analysis results, i.e. ‘hairballs’ of scientific knowledge, in particular in light of the desire to increase the FAIRness of research results (<xref rid="R2" ref-type="bibr">2</xref>, <xref rid="R3" ref-type="bibr">3</xref>).</p>
    <p>In 2009, Sir Tim Berners Lee, who is also known as the ‘inventor of the World Wide Web’ because he invented the Hypertext Transfer Protocol protocol and Hypertext markup language for web pages, gave a famous Ted talk about Linked Data in which he described how incompatible data formats and documentation systems make it necessary to examine each data element in order to create something new (<xref rid="R4" ref-type="bibr">4</xref>, <xref rid="R5" ref-type="bibr">5</xref>). Berners Lee suggested that uploading unadulterated raw data to the web as Linked Data would make it easier to combine, link and reuse existing data. Since then, shared vocabularies and ontologies have increasingly been used to structure data, and chemical and biological registries, as well as governments, have started using Linked Data to handle the large amounts of data they store (<xref rid="R6" ref-type="bibr">6–9</xref>).</p>
    <p>The Resource Description Framework (RDF) developed by the World Wide Web Consortium is a standard for describing data on the web in a machine-readable format (<xref rid="R10" ref-type="bibr">10</xref>). The RDF data model describes information as subject–predicate–object relationships called triples. This kind of data can be queried using the SPARQL query language; however, the powers of SPARQL and linked data are not readily accessible to users unfamiliar with SPARQL. Graphical tools provide visual user interfaces to support the user in visually exploring and accessing this kind of data. These triples can be used to create a network graph visualization by representing the subject and object of each triple as nodes and the predicates as edges between them. Nodes that represent the same resource are merged in the visual representation.</p>
    <p>Immersive technologies such as virtual reality (VR) and augmented reality (AR) are increasingly used in health and life sciences for a variety of applications, including therapeutics, training, simulation of real-world scenarios and data analysis for, e.g. genomics and medical imaging (<xref rid="R11" ref-type="bibr">11–15</xref>). Immersive analytics has established applications for abstract and multidimensional data in this domain (see (<xref rid="R12" ref-type="bibr">12</xref>) for a review). A limited number of applications exist to explore knowledge graphs in VR (<xref rid="R16" ref-type="bibr">16–18</xref>), but these are merely visualizations and do not offer many options for users to query and interact with the data.</p>
    <p>We hypothesize that VR will allow users to more readily explore, compare and query large knowledge graphs using a gesture-driven interface that requires less technical expertise. In this VR context, users can use ontologies to search, order and filter data to their needs. Instead of writing SPARQL queries, the user can expand existing connections and define patterns in the data interactively using the VR controllers. Overall, VR adds a third dimension and an open space that can help users perform complex data analysis.</p>
    <p>In this paper, to test this hypothesis, we first review recent visualization methods and tools used for the exploration and analysis of semantic web knowledge graphs and identify best practice methods for visualizing and interacting with SPARQL query results. We then select methods and materials and implement an experimental VR prototype to explore Linked Data and Graph2VR, evaluate its usability and investigate the human and VR environment factors that could enhance the exploration and analysis of semantic web knowledge graphs.</p>
  </sec>
  <sec id="s2">
    <title>Related work</title>
    <p>As a basis for the Graph2VR experiment, we reviewed existing tools that provide best practice methods for addressing specific challenges when working with (large) graph databases. Our review included tools for visualizing graphs in 2D, 3D and VR; those working with ontologies, SPARQL and graph databases; backends for graph databases and general ways to visualize data in VR (<xref rid="s10" ref-type="sec">Supplementary Appendix Table A1</xref>).</p>
    <p>Later, we describe some notable tools, LODLive, GraphDB, Vasturiano, Toran, Visual Notation for OWL ontologies (VOWL), Gruff and Tarsier (<xref rid="R18" ref-type="bibr">18–25</xref>), that all provide rich graph exploration functionality, enumerate functional challenges and visualization aspects and highlight their implementation in Graph2VR. We also refer the reader to some overview papers about tools to visualize and interact with ontologies, Linked Data or graph databases in general, which have been tested and described, e.g. in the following papers. (<xref rid="R6" ref-type="bibr">6</xref>, <xref rid="R26" ref-type="bibr">26</xref>, <xref rid="R27" ref-type="bibr">27</xref>).</p>
    <p>In ‘LodLive’, the user is first asked for a SPARQL Endpoint and a Unique Resource Identifier (URI). Each node is represented as a circle in LodLive, and outgoing connections are represented as small nodes around it (see <xref rid="F1" ref-type="fig">Figure 1</xref>). Each node offers some options in the form of a menu that includes ‘i’ for information, a button to focus on the node and to close other relationships, a button to open a link to the URI to show an online resource, a button to expand all relationships around that node and a button to remove the selected node. Once a node is added, existing connections to other open nodes are also displayed. However, beyond opening new connections and a comprehensive info panel that shows information about the respective node in a flexible way that makes use of different kinds of semantic annotations, integrating images and even google maps, the query possibilities in LodLive are quite limited.</p>
    <fig position="float" id="F1" fig-type="figure">
      <label>Figure 1.</label>
      <caption>
        <p>A screenshot of LodLive (<xref rid="R19" ref-type="bibr">19</xref>): the graph can be expanded by clicking on the small clouds or circles around the node.</p>
      </caption>
      <graphic xlink:href="baae008f1" position="float"/>
    </fig>
    <p>‘GraphDB’ and ‘Metaphactory’ are commercial software tools (<xref rid="R20" ref-type="bibr">20</xref>, <xref rid="R28" ref-type="bibr">28</xref>). They use an internal database and offer an autocomplete search function to find URIs. The user is thus not required to know the precise URI, which makes the tools quite user-friendly. GraphDB also offers an option to collapse all nodes around a specific node instead of only allowing users to delete specific nodes (<xref rid="F2" ref-type="fig">Figure 2</xref>).</p>
    <fig position="float" id="F2" fig-type="figure">
      <label>Figure 2.</label>
      <caption>
        <p>A small ‘Visual Graph’ in GraphDB (<xref rid="R20" ref-type="bibr">20</xref>). The node in the middle has a menu with the option to collapse connections around that node.</p>
      </caption>
      <graphic xlink:href="baae008f2" position="float"/>
    </fig>
    <p>The Visual Notation for OWL Ontologies ‘VOWL’ (<xref rid="R22" ref-type="bibr">22</xref>, <xref rid="R23" ref-type="bibr">23</xref>) adds use of colour-blind-friendly colours and symbols to visualize different types of nodes and edges. Two examples of the implementation of VOWL are a plugin for Protégé and WebVOWL (<xref rid="R29" ref-type="bibr">29</xref>, <xref rid="R30" ref-type="bibr">30</xref>). WebVOWL is an online visualization for ontologies. By default, WebVOWL starts with a subgraph of the Friend of a Friend Ontology (FOAF) ontology, as shown in <xref rid="F3" ref-type="fig">Figure 3</xref>, but users can upload their own, owl files. QueryVOWL, another VOWL tool, can be used to query a SPARQL Endpoint using a textual search to create a visual graph as output (<xref rid="R31" ref-type="bibr">31</xref>).</p>
    <fig position="float" id="F3" fig-type="figure">
      <label>Figure 3.</label>
      <caption>
        <p>Screenshot of WebVOWL showing parts of the FOAF ontology (<xref rid="R32" ref-type="bibr">32</xref>).</p>
      </caption>
      <graphic xlink:href="baae008f3" position="float"/>
    </fig>
    <p>Gruff is a commercial tool that can use its internal graph database (Allegro graph), SPARQL Endpoints and Neo4j. After selecting a database, users can ‘Display some sample triples’ from that database without any prior knowledge about the content, which is very convenient for new users. Gruff offers a visual query view to describe the necessary relations using variables and searching for nodes. These relations can contain specific URIs, such as constant nodes and variables. The relationships created in query view are then translated into an editable SPARQL query, which can be altered to add additional filters or other specific SPARQL commands. This translation from a visualization to an editable SPARQL query is convenient and makes Gruff stand out as a tool. However, screen size is a limiting factor for Gruff. When zooming out, the text becomes too small to read and is hidden. The user can move the window of visible nodes in all directions. Instead of using labels, Gruff uses colours and shapes to indicate similar nodes and repeating edges.</p>
    <fig position="float" id="F4" fig-type="figure">
      <label>Figure 4.</label>
      <caption>
        <p>Gruff allows users to build queries visually and shows the SPARQL query, so it can be modified (4(A)). The results can be displayed as a visual graph (4(B))(<xref rid="R24" ref-type="bibr">24</xref>). (A) Screenshot of a query built-in Gruff’s ‘Graphical Query View’. (B) Screenshot of the resulting graph in Gruff’s ‘Graph View’. When zoomed out, the texts disappear, but the colours still indicate similar nodes and edges.</p>
      </caption>
      <graphic xlink:href="baae008f4" position="float"/>
    </fig>
    <p>Finally, ‘Tarsier’ is a tool that makes use of a 3D representation of SPARQL queries. Filters allow users to select nodes that fulfil specific criteria and shift them to another ‘semantic plane’. This tool was built with the intention to help new students to learn SPARQL and understand how filters work (<xref rid="F5" ref-type="fig">Figure 5</xref>).</p>
    <fig position="float" id="F5" fig-type="figure">
      <label>Figure 5.</label>
      <caption>
        <p>Tarsier filter data and can shift datapoints to different semantic planes accordingly. (This screenshot was taken from a video from the authors of Tarsier (<xref rid="R33" ref-type="bibr">33</xref>)).</p>
      </caption>
      <graphic xlink:href="baae008f5" position="float"/>
    </fig>
  </sec>
  <sec id="s3">
    <title>Materials and Methods</title>
    <p>Based on the analysis of existing tools and approaches, we prioritized a list of key features for Graph2VR, which are summarized in <xref rid="T1" ref-type="table">Table 1</xref>. We aimed to combine some of the strengths of the different methods from different tools in one VR application that enables immersive 3D visualization that allows interaction with and manipulation of data. Graph2VR is an extensive usable prototype that still has a few issues that need to be resolved. It is extendable and demonstrates how a VR application can be used to visualize and interact with Linked Data. A more detailed description of its features is present in the user manual in (<xref rid="R34" ref-type="bibr">34</xref>) and in five tutorial videos (<xref rid="R1" ref-type="bibr">1</xref>). Later, we describe the methodological considerations for these features in detail, grouped by initialization, visualization, navigation and data analysis.</p>
    <table-wrap position="float" id="T1">
      <label>Table 1.</label>
      <caption>
        <p>Key features of Graph2VR</p>
      </caption>
      <table frame="hsides" rules="groups">
        <colgroup span="1">
          <col align="left" span="1"/>
          <col align="left" span="1"/>
        </colgroup>
        <thead>
          <tr>
            <th valign="bottom" align="left" rowspan="1" colspan="1">Category</th>
            <th valign="bottom" align="left" rowspan="1" colspan="1">Description</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left" rowspan="1" colspan="1">Initialization</td>
            <td align="left" rowspan="1" colspan="1">A configuration file is used to preconfigure which SPARQL Endpoint to start from and additional ones to switch to. Graph exploration usually starts from either a single URI or a small graph, which can also be specified in the configuration file. Image predicates, predicates that get suggested for new connections and colour coding can be adjusted in this file. We adopted the VOWL colour schema as a reasonable default.</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Visualization</td>
            <td align="left" rowspan="1" colspan="1">To visualize Linked Data, nodes are represented as spheres and the edges between them as arrows. The colour schema is used to represent different kinds of nodes or their current status (e.g. being selected or part of a query). A choice of graph layout (3D, 2D, hierarchical and class hierarchy) for the visualization will give the most flexibility for the user. Labels and images for nodes and labels for edges are preloaded when the graph is expanded.</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Interaction</td>
            <td align="left" rowspan="1" colspan="1">Graph2VR provides features for interacting with the visualized Linked Data. The user can grab and move single nodes or the whole graph. Once grabbed, the whole graph can be moved, rotated and scaled. There are four different graph layout options: 3D, 2D, hierarchical and class hierarchy. The user has different ways to navigate in the VR environment. Graph2VR is a room-scale VR application, so looking around and taking a few steps can help to move small distances. For larger distances, the user can either teleport or fly. Pinning nodes prevents them from being affected by the layout algorithm so that they can be restructured manually. The user can create new nodes, edges and graphs; drag and drop nodes and graphs; and convert nodes or edges to variables to create queries. New nodes can be added by searching for them in the database or by spawning a new variable node. After selecting single nodes or edges, the circle menu shows options for the selected node or edge for further interaction.</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Data analysis</td>
            <td align="left" rowspan="1" colspan="1">Graph2VR provides features for querying SPARQL Endpoints, including graph expansion. In addition, the nodes and edges can be used to generate custom queries visually. Triples can be selected to be part of a SPARQL query. A predefined set of commands, including selecting triples, creating variables, OrderBy and Limit, can be used to create a SPARQL query. Options that affect a whole triple can be found in the edge menu.</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <sec id="s3-s1">
      <title>Initialization</title>
      <p>In many visualization tools, graph exploration starts from either a small subgraph (bottom-up or local view) or a global hierarchy (top-down or global view) in which zoom and filters are used to request more details (<xref rid="R35" ref-type="bibr">35</xref>). Starting from a subgraph, either an overview graph or a start node, the user can expand the graph by opening further connections. Commonly, tools that work with SPARQL need the Uniform Resource Locator of a SPARQL Endpoint to determine to which graph database their requests should be sent. Additionally, they need a URI, SPARQL query or keyword to know where to start the graph exploration. The graph can then be expanded further to explore it incrementally. In Graph2VR, one can start with an initially provided SPARQL query and explore from there.</p>
    </sec>
    <sec id="s3-s2">
      <title>Visualization</title>
      <p>This section summarizes methods around visualization, in particular use of layout, colour and information display.</p>
      <sec id="s3-s2-s1">
        <title>Layout</title>
        <p>In general, the layout of graphs in the tools is either static or is restructured over time to increase the distance between the nodes and the readability. The Fruchterman–Reingold algorithm is a force-directed graph algorithm (<xref rid="R36" ref-type="bibr">36</xref>). The regular runtime of the algorithm has a runtime of <inline-formula id="ILM0001"><tex-math notation="LaTeX" id="ILM0001-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal{O}(|N|^2 + |E|)$\end{document}</tex-math></inline-formula>, where <italic toggle="yes">N</italic> is the number of nodes and <italic toggle="yes">E</italic> is the number of edges (<xref rid="R37" ref-type="bibr">37</xref>). The algorithm also works in three dimensions. As the graphs get larger, the number of node–node interactions increases. For larger graphs, other algorithms like the Barnes–Hut algorithm scale better. The Barnes–Hut algorithm combines the gravity centre of nodes that are further away so that fewer calculations need to be executed (<xref rid="R38" ref-type="bibr">38</xref>, <xref rid="R39" ref-type="bibr">39</xref>). There are, of course, many more algorithms like Kamada–Kawai that could be used to handle even more nodes in a graph at once (<xref rid="R36" ref-type="bibr">36</xref>, <xref rid="R40" ref-type="bibr">40</xref>). In a 3D representation, we can use the third dimension to compare a stack of 2D layers. In the literature, this kind of representation is known as semantic planes (<xref rid="R25" ref-type="bibr">25</xref>, <xref rid="R41" ref-type="bibr">41</xref>), and Tarsier already makes use of it. Tarsier runs on a local server and can be accessed via its web interface. It allows the user to apply filters, then takes all the nodes that fulfil the selection criteria and shifts them to another semantic plane. A stack of different planes of information would allow a user to compare them without losing their internal structure (e.g. a tree structure). In this way, the user can, e.g. compare data and annotate the similarity between two resources. For comparisons, different predicates can be used to describe the kinds of connections between different nodes. In their Scientific Lens paper, Batchelor et al. name four predicates with decreasing similarity levels to compare similar entities that could be used to compare different entities with similar meanings: ‘owl: sameAs’, ‘skos: exactMatch’, ‘skos: closeMatch’ and ‘rdfs: seeAlso’ (<xref rid="R42" ref-type="bibr">42</xref>). In the final version of Graph2VR, we added those as predefined predicates so that a user can quickly adjust the predicate of an edge. In Graph2VR, results can be shown as a series of semantic planes representing the different results that match a given query pattern (see section Demonstration Data). These planes are useful for comparing different 2D structures and creating new connections. For example, multiple 2D layers of class hierarchies allow users to compare between different ontologies, which can be used to find similarities and differences.</p>
      </sec>
      <sec id="s3-s2-s2">
        <title>Colour</title>
        <p>The colouring of nodes and edges also varies from tool to tool. LodLive uses random colours, whereas Gruff reuses the same colour for the same types of nodes and edges. The VOWL colour scheme defines the colour of each node based on its properties, using specific colours for classes, variables, blank nodes and literals. It has also been designed to be colour-blind-friendly and understandable when printed in black and white (<xref rid="R32" ref-type="bibr">32</xref>). VOWL recommends using specific shapes or patterns to indicate different attributes, e.g. a ring around a node to represent a class. For the 3D environment, the colours can be reused, but some forms need to be adjusted. In 3D, for example, a circle around a sphere to indicate a class could be represented as either a circle or a sphere. Using different forms and shapes would also be a viable option to represent an object in Linked Data. In Graph2VR, we primarily apply the VOWL schema but have not yet implemented all of it. We also add some colours for variables and selected triples. Further details can be found in the manual (<xref rid="R34" ref-type="bibr">34</xref>). As graphs become larger through expansion, it becomes more important to be able to quickly identify different types of nodes, i.e. is the node a URI, a literal, a blank node or a variable? Some tools do not distinguish, displaying all nodes in the same or a custom colour. One example of such a tool is ‘Noda’, an application sold on Steam that allows the user to interact with 3D mindmaps or graph structures by adding nodes and edges with different sizes, symbols, colours and labels (<xref rid="R43" ref-type="bibr">43</xref>). Gruff colours the same kinds of connections in the same randomly chosen colour, whereas the VOWL colour scheme can help users make distinctions quickly (<xref rid="R23" ref-type="bibr">23</xref>). In Graph2VR, parts of the VOWL schema are applied, making literals (strings and numbers) yellow, URIs blue and classes light blue (only if the class relationship is expanded). Blank nodes are dark. When a node or edge is selected using the laser, it turns red. When converted to a variable, nodes and edges turn green. Nodes and edges glow white when the user hovers over them with the laser. If triples are part of the selection for a query, they shine in a bright yellow colour. While common edges are black with a black arrowhead, the arrowhead changes the colour for the ‘rdfs: subClassOf’ relationship to a white arrowhead.</p>
      </sec>
      <sec id="s3-s2-s3">
        <title>Information display</title>
        <p>One useful feature in tools like LODLive or WebVOWL is an information box containing essential information about a selected node, although which information is displayed varies. Typical information provided is the URI, label, class relationship and sometimes comments. SPARQL offers the ‘describe’ function to request some basic information, but the results can be quite extensive. Therefore, it is better to restrict the information to a selection of predicates.</p>
        <p>To visualize query results, we parsed the results and displayed the URIs or, if findable, the labels of the nodes and edges in the graph. To further improve readability, the labels above the nodes are continuously rotated towards the user, as are the images. This can be seen in the first part of the Video tutorial (<xref rid="R1" ref-type="bibr">1</xref>). In contrast, the texts above the edges always sit on top of the connecting lines and are readable from both sides of the edges. Since URIs are often long strings of characters, the labels of the URIs are displayed instead (if available). The complete URI appears when the user hovers over the edge with the laser.</p>
      </sec>
    </sec>
    <sec id="s3-s3">
      <title>Interaction</title>
      <p>This section summarizes features for interaction with the visualization, in particular methods for controls, navigation, zooming and rotation and menus in VR.</p>
      <sec id="s3-s3-s1">
        <title>Controls</title>
        <p>To enable users to reorder nodes, we implemented a gesture-driven interface that allows the user to drag and drop them using the controllers. The user grabs a node with both hands while pressing the grip button. Grabbing a node while pressing the trigger button allows new links between two nodes to be created.</p>
        <p>When multiple graphs exist in VR, it can be challenging to grab and manipulate the desired graph. To differentiate between different graphs, we implemented a sphere around all the nodes of a graph that is only visible from the outside of the graph, to avoid cluttering the view. This sphere determines the size of a graph and allows the user to interact with a specific graph. When a SPARQL query is executed and multiple results are created in different layers (semantic planes), each layer is generated as a separate graph. Grabbing and moving a graph enable the user to work with a specific subgraph. To remove less interesting graphs, we made it possible to delete sibling graphs (all the other graphs created by the same query aside from the current one) or all child graphs of a particular graph. When the semantic planes are displayed next to each other, we first tested drawing flat planes around the 2D visualizations. Those planes were transparent, similar to the sphere surrounding the 3D version of the graph. However, as multiple partially transparent 2D layers behind each other turned out to be more distracting than useful, we turned off the planes by default.</p>
      </sec>
      <sec id="s3-s3-s2">
        <title>Navigation</title>
        <p>We added a platform to Graph2VR to make orientation easier as it feels more natural for a user to stand and walk on a surface than to float in empty space. Common ways to move in room-scaled VR applications are moving in the room, walking with a thumbstick or trackpad, teleportation and, in some cases, flying. In Noda, users can drag themselves through the room. In GraphXR, users can rotate the whole graph. Finally, in 3D Force Graphs, the user can fly through a universe of huge nodes (<xref rid="R17" ref-type="bibr">17</xref>, <xref rid="R18" ref-type="bibr">18</xref>, <xref rid="R43" ref-type="bibr">43</xref>). To give the user maximum freedom for navigation, we implemented both flying and teleportation and, if the user looks around or moves, this also happens in the virtual environment.</p>
      </sec>
      <sec id="s3-s3-s3">
        <title>Zoom and rotation</title>
        <p>Graph2VR offers several ways to interact with a graph. When using the grip buttons of the VR controller on a node, the user can drag and drop a node close to the controller. Here, we were inspired by functions in the game/tech demo Toran (<xref rid="F6" ref-type="fig">Figure 6(A)</xref>) (<xref rid="R21" ref-type="bibr">21</xref>). In that demo, a graph is embedded in a round sphere that can be freely rotated and zoomed (<xref rid="F6" ref-type="fig">Figure 6(B)</xref>). The user can also create new connections between nodes within the sphere. As this also suits working with graphs, we implemented similar ways to interact with the graphs in Graph2VR. Both controllers’ grip buttons must be pressed close to the graph simultaneously to grab the whole graph, which can then be dragged around and rotated freely. When the controllers are moved together/away from each other, the graph is scaled down/up. It is possible to grab a node and the whole graph simultaneously. This results in scaling the entire graph; only the grabbed node does not scale with the graph, which can be used to scale specific nodes up or down. It is also possible to work with multiple graphs in Graph2VR, and each can have a different size.</p>
        <fig position="float" id="F6" fig-type="figure">
          <label>Figure 6.</label>
          <caption>
            <p>The sphere in Toran (6(A)) was the inspiration for having a sphere and for how to rotate, zoom and drag&amp;drop a graph (<xref rid="R21" ref-type="bibr">21</xref>). For comparison, see the sphere in Graph2VR (6(B)). (A) Screenshot of Toran, a transparent sphere surrounds the game elements. (B) Screenshot of rotating a graph in Graph2VR. A transparent sphere surrounds the graph.</p>
          </caption>
          <graphic xlink:href="baae008f6" position="float"/>
        </fig>
      </sec>
    </sec>
    <sec id="s3-s4">
      <title>Circle menu</title>
      <p>VR has not yet evolved to have standard control archetypes comparable to those used in 2D user interfaces. Nonetheless, users do need a system that allows them to quickly choose from sets of options. We were inspired to create a circular menu by the ‘Aesthethic Hover UI’ asset (<xref rid="R44" ref-type="bibr">44</xref>). Here, we first attempted to reuse the HoverUIKit, but it used depreciated packages, so we ultimately built our own circle menu. Most 2D tools like LOD Live, Gruff, GraphDB and Metaphactory show their menu and search options next to the nodes (<xref rid="R19" ref-type="bibr">19</xref>, <xref rid="R20" ref-type="bibr">20</xref>, <xref rid="R24" ref-type="bibr">24</xref>, <xref rid="R45" ref-type="bibr">45</xref>), but VR offers a wider range of options. We therefore considered having the menu directly next to the node, on the left arm, on a virtual display or panel or static in front of the head as in a helmet display. After some testing, we determined that a menu with many options within a hairball of nodes would not be the appropriate solution and that the menu was best readable on the left controller. This circular menu has several submenus and shows the options that can be applied in the current context. When a node is selected, the menu displays options that can be applied to that node (<xref rid="F7" ref-type="fig">Figure 7(A)</xref>). When selecting an edge, a menu for the edge is shown. In the node menu are further submenus that show all incoming or outgoing relations from or to the selected node. To improve the usability of the menu, we added some icons to the options in the circle menu. Click sounds indicate that buttons have been pressed, and white circles around newly spawned nodes that disappear within 2 s denote changes within the application. For new Graph2VR users, we have added a short help menu describing the main functionalities.
</p>
      <fig position="float" id="F7" fig-type="figure">
        <label>Figure 7.</label>
        <caption>
          <p>The circle menu in 7(A) shows different options based on the context. There are submenus that show more details, such as the outgoing connections in 7(B). (A) The circle menu after clicking on a node. (B) Circle menu displays multiple outgoing connections.</p>
        </caption>
        <graphic xlink:href="baae008f7" position="float"/>
      </fig>
    </sec>
    <sec id="s3-s5">
      <title>Data analysis</title>
      <p>This section summarizes methods for graph manipulation, in particular query generation, search, and save and load.</p>
      <sec id="s3-s5-s1">
        <title>Query generation</title>
        <p>In SPARQL, Select queries result in tabular answers, while Construct queries describe graph structures. Therefore, Graph2VR uses Select queries to populate the menu, e.g. with the incoming and outgoing connections of a node, and Construct queries to create graph structures. Nodes with the same URI are combined into one node in the visualization to generate a network graph instead of a list of triples. The submenu for incoming (and outgoing) connections lists the predicates pointing to (or from) the current node. They are then grouped by their predicates, and the number of available triples for each predicate is displayed next to the entry. When there are more predicates than can be displayed at once, a circular scrollbar can be used to scroll through the menu (<xref rid="F7" ref-type="fig">Figure 7(B)</xref>). If a node with the same URI is added to a graph, it will be merged with the existing node. To restrict the number of results when expanding the graph (e.g. if one node has thousands of connections of the same type), there is a limit on how many connections to open (default is 25). This default limit can be adjusted using the limit slider below the menu (<xref rid="F7" ref-type="fig">Figure 7(B)</xref>). The slider is not a linear scale but has fixed amounts of nodes.</p>
        <p>To improve predicate readability, predicate labels are shown instead of the URI. If no label is available, the URI is shortened, but it is still possible to see the entire URI when hovering over the menu with the pointer. The predicates are ordered alphabetically by URI, not by the shortened version shown, which may confuse users when the base URI changes and the alphabetic order starts from the beginning again. When there are multiple connections between the same nodes, or in case of a self-reference, the straight edges are replaced by bent arrows to avoid overlapping texts.</p>
      </sec>
      <sec id="s3-s5-s2">
        <title>Node removal</title>
        <p>Once nodes are established, the user should have the option to remove them from the visualization. We implemented two different ways to do so. The simplest is the ‘close’ option, which removes the node and all triples containing this node from the visualization (but not the database). A more complex way to reduce the size of the graph is the ‘collapse’ option. This removes all leaf nodes around the selected node, while the node itself remains. Leaf nodes are nodes with no other connections within the visualization (but not necessarily in the underlying graph database). This helps reduce the number of nodes while preserving the graph structure.</p>
        <p>For each node, the user has the option to remove it or to remove the surrounding leaf nodes (incoming, outgoing or both). When a node is deleted, the edges around it and its leaf nodes are also removed because there would be no triple left that contains them. This could lead to many single nodes that the user would have to remove manually. To prevent accidental removal when collapsing a selected node (when no triples are left containing it), and to keep newly created nodes that are not yet part of a triple, we decided to keep single nodes so that a user can connect them or start a new exploration from there.</p>
        <p>When queries are sent in Graph2VR, the response will be displayed in new graphs that we call child graphs. To remove these again, each graph has the option to remove a whole graph at once. When a query creates multiple child graphs, it would be too much effort to remove those one-by-one. We therefore included an option to delete all child graphs from the original graph or the operation to delete all sibling graphs. Removing all child graphs will remove all child graphs of the respective graph. Each of the graphs also has the option to remove its sibling graphs, which will remove all unmodified sibling graphs created by the same query, leaving only the selected graph and modified graphs. If the last graph has been removed, it is possible to create a new graph by creating a new node, loading some saved data or using the search function(s).</p>
      </sec>
      <sec id="s3-s5-s3">
        <title>Search</title>
        <p>Graph2VR was intended to be more interactive than just a visualization. To be able to create some SPARQL queries visually, we added ways to create and rename variables or search for keywords. This required a text input system. We therefore reused a virtual keyboard from Unity’s asset store, VRKeys, that uses VR controllers as drumsticks to enter text (<xref rid="R46" ref-type="bibr">46</xref>). This approach to entering text in VR has two advantages over other applications, which mainly use a laser with a point-and-click system. Drumming the keyboard does not use an additional key, so it does not interfere with our controls, and it can be done with two hands simultaneously. We also implemented both a global search in the settings menu and a context-specific search that can be accessed from the node menu for variable nodes. In addition to the keyword, this second search function takes all the selected triples into account. Only results that match the selected variable in the given context and the keyword are shown. Both search functions attempt to perform an autocomplete search on the search term. Depending on the settings, the search can either be triggered with every keystroke on the VR keyboard or by pressing the return key.</p>
      </sec>
      <sec id="s3-s5-s4">
        <title>Save and load</title>
        <p>Graph2VR has two different ways to save and load data to a file. The first is to save the whole state of the application, including all the nodes, edges, their positions, labels, graphs, etc. Even images that have been loaded are saved, so they can be reloaded even if they are no longer on the internet. To prevent saved files from becoming too large, the image resolution is scaled down if it is too large. The quicksave option offers one save slot that will be overwritten every time. Alternatively, the user can specify a filename for a save state, allowing multiple save states. Loading a save state will overwrite the current session. Another way of saving and loading is to save triples as ntriples. This is a standard format that can also be read from other applications. In contrast to the first save option, this only saves the triples, and positions, images and single nodes are not stored. When loading an ntriples file, all the triples are automatically added to the current scene. In contrast to loading a save state, loading an ntriples file does not overwrite the current scene. Instead, all the triples are added as an additional graph. We strongly advise users not to load large ntriples files because Graph2VR could become slow or unresponsive. We could load about 5000 triples at once during our tests, but the framerate dropped to around five frames per second. For larger ntriples files, we recommend loading them into a SPARQL endpoint, e.g. a Virtuoso server, and accessing only the relevant parts from there (<xref rid="R47" ref-type="bibr">47</xref>).</p>
      </sec>
    </sec>
  </sec>
  <sec id="s4">
    <title>Implementation</title>
    <p>Based on the methodological basis, we implemented a Graph2VR prototype in Unity (version 2 021.2f1) (<xref rid="R48" ref-type="bibr">48</xref>). This section summarizes implementation details, in particular use of Quest 2 VR, how to best represent RDF in unity, implementation of the layout algorithms and performance optimizations.</p>
    <sec id="s4-s1">
      <title>Standalone on Quest 2 VR headset</title>
      <p>Graph2VR was designed to run as a standalone version on the Quest 2 VR headset, but it also supports the HTC Vive headset. We recommend using the Quest 2 headset because its higher resolution provides better readability. Graph2VR can be compiled as a Windows application (.exe) or as a standalone application (.apk) for the Meta Quest 2 VR headset. Our Graph2VR implementation process started with the Free Unity WebXR Exporter from the Unity Asset store (<xref rid="R49" ref-type="bibr">49</xref>). This template includes a desert background and some objects and models for the VR controllers. To be able to select nodes, a laser pointer was added to the right controller. We used a sample dataset in a local Virtuoso server via docker as a database (<xref rid="R50" ref-type="bibr">50</xref>). To access the server, a modified DotNetRDF (version 2.6) is used (<xref rid="R51" ref-type="bibr">51</xref>, <xref rid="R52" ref-type="bibr">52</xref>). At some point, DotNetRDF checks whether a specific interface is present, but even if it was present, the test failed in the Quest 2 standalone application. We resolved this by removing this check and recompiling the DotNetRDF library. One of the tougher decisions during the implementation process was whether to use SteamVR or OpenXR (<xref rid="R53" ref-type="bibr">53</xref>, <xref rid="R54" ref-type="bibr">54</xref>). SteamVR, as commercial software, supports many VR headsets out of the box, provides 3D controller models and is, in general, easier to use due to its more abstract controller bindings. On the other hand, OpenXR would allow us to build a standalone application for the Quest2 headset that does not require a connection to a personal computer with a strong graphics card. Ultimately, we chose OpenXR and were able to create a standalone application for the Quest2.</p>
    </sec>
    <sec id="s4-s2">
      <title>RDF representation</title>
      <p>One issue we had to resolve was whether to use the representation of graphs provided by DotNetRDF, using iNodes and iGraphs, or whether to use the Unity in-memory representation for the nodes and edges. Unity-based prefabs can help to circumvent potential inconsistencies of two representations of the same dataset. Keeping both representations would be more error-prone but would allow reuse of more iGraph functionalities. The DotNetRDF representation has certain advantages when parsing and reusing the query results. Many functionalities were already present and utilizable. The disadvantage of this representation was that both the internal and the visual would need updates when adapting the graph. Additionally, while iGraphs in DotNetRDF support triples, adding a single node to our graphs, e.g. when a user adds a new node, led to differences between iGraph and visual representation. Consequently, the decision was made to rely on the unity-based representation to allow single nodes and enable step-by-step generation of new triples without consistency issues.</p>
    </sec>
    <sec id="s4-s3">
      <title>Layout algorithms</title>
      <p>Layout algorithms help to reorder the graph to improve readability. We implemented four layout algorithms: 3D force-directed, 2D force-directed, hierarchical view and class hierarchy. For the force-directed 3D layout, we used the Fruchterman Reingold Algorithm (<xref rid="R37" ref-type="bibr">37</xref>), which uses repulsive forces between nodes and attractive forces along the edges. Over time, the ‘temperature’ cools down, the forces get weaker, the adjustments in the graph get smaller and smaller and there can be a cut-off value. In Graph2VR, layout algorithms can be switched via the Graph operations menu. Inspired by Gephi, we modularized the layout algorithm, so the user can swap to other layout algorithms (<xref rid="R55" ref-type="bibr">55</xref>). For the 2D layout, we used a simple layout heuristic, simply ordering the nodes sequentially in a plane aided by minimal force direction. While the force-directed Fruchterman Reingold algorithm forms circles in 2D or spheres of nodes in 3D, the ‘Hierarchical View’ layout orders the nodes alternating in horizontal and vertical stacks (<xref rid="F8" ref-type="fig">Figure 8</xref>). This makes it easier to read the labels of the nodes and find a specific node. Additionally, the nodes are sorted alphabetically in this layout. New outgoing nodes are usually added to the expanded node’s right side. However, an exception is the rdfs: subClassOf relationship, which points to the left.</p>
      <fig position="float" id="F8" fig-type="figure">
        <label>Figure 8.</label>
        <caption>
          <p>The hierarchical layout adds new triples, alternating vertical and horizontal on the ‘right’ side. Only the rdfs: SubClassOf relations are pointing to the ‘left’ side. They have a white arrowhead as specified in the VOWL schema.</p>
        </caption>
        <graphic xlink:href="baae008f8" position="float"/>
      </fig>
      <fig position="float" id="F9" fig-type="figure">
        <label>Figure 9.</label>
        <caption>
          <p>Comparison of the 3D class hierarchy in Graph2VR (9(A)) with the 2D class hierarchy in Protégé (9(B)) showing the same class hierarchy. (A) The class hierarchy in Graph2VR can display subclasses and individuals in a single 3D tree structure. The individuals and their attributes point in the third dimension. (B) The class hierarchy in Protégé is a tree structure starting at owl: thing. The individuals are displayed in a separate panel after selecting one of the classes.</p>
        </caption>
        <graphic xlink:href="baae008f9" position="float"/>
      </fig>
      <p>Besides the ‘Hierarchical View’, we also added a ‘Class Hierarchy’ layout. The basic idea of this layout was to create a class hierarchy, like the tree structure in Protégé (<xref rid="R29" ref-type="bibr">29</xref>), but in three dimensions. The base of this layout is a 2D class hierarchy based on the rdfs: subClassOf predicate. Each of the classes (or subclasses) can contain multiple individuals of that class. We can use the extra dimension to display the individuals in a list orthogonal to the 2D class hierarchy based on the rdf: type predicate. See <xref rid="F9" ref-type="fig">Figure 9(A)</xref>.
</p>
      <p>Finally, a pin function was added to pin certain nodes to their current position. A pin prevents these nodes from being affected by the layout algorithm, but they can still be dragged around manually. This can have interesting effects when some nodes are pinned and layout algorithms are applied only to parts of the graph. It can be helpful to have a class hierarchy for the classes, pin it and continue exploring the individuals, e.g. with a force-directed algorithm.</p>
    </sec>
    <sec id="s4-s4">
      <title>Performance optimizations</title>
      <p>To prevent the whole application from stuttering while SPARQL queries are executed, we put them into separate threads. This way, Graph2VR will not stutter even if a query takes several seconds to be executed. The disadvantage of this is that the order of results from different queries is not determined. There is a race condition between the different queries, with the faster result displayed first. This may affect the search function when ‘search on key press’ is activated as it fires a query on each key press. This can be quite fast compared to the variation in execution times of the search queries, potentially leading to a situation where search results for an older but slower query overwrite newer results. To speed-up free-text search queries, we used the bif: contains command. This command is not supported by every SPARQL server but is supported by e.g. OpenLink’s Virtuoso (<xref rid="R56" ref-type="bibr">56</xref>). The bif: contains command triggers a search function on a preindexed internal Structured query language table. If such an index exists, this command can be used to speed up the free-text search. If the index does not exist, the server will most likely return empty results (<xref rid="R57" ref-type="bibr">57</xref>). During our tests, the DBpedia SPARQL endpoint supported the bif: contains command. One restriction of the bif: contains command (in Virtuoso) is that it only supports words with at least four letters and needs to be enclosed in brackets if it contains spaces. This is a problem if the search term consists of multiple connected words that belong together (e.g. names with ‘Mc’ at the beginning or ‘van’ in the middle). We did overcome this issue by separating the words but replacing the spaces of words with fewer than four letters with a star (any character) so that the whole name can be used as a single search term.</p>
    </sec>
  </sec>
  <sec id="s5">
    <title>Results</title>
    <sec id="s5-s1">
      <title>Application overview</title>
      <p>A SPARQL query can be used to define the initial graph. A combination of URIs, literals and variables in SPARQL is used to define query patterns and to request results that match this pattern from the database. Traditionally, formulating a SPARQL query involved manually searching for relevant URIs and predicates, a process that could take several minutes, especially for complex queries involving multiple triples. In Graph2VR, it is just a matter of expanding the graph by clicking on the desired predicate and expanding the graph and converting existing nodes (or edges) in the graph to variables. If necessary, new nodes and edges can be spawned. Once the relevant triples in the graph are selected, the results can be requested.</p>
      <p>We implemented two different ways to display the query results. They can either be displayed as a single result graph containing all the triples merged into one new graph or as a series of separate result graphs stacked behind each other. These result graphs are independent graphs. To make sure that not all of them are moved, scaled and rotated at the same time, only one graph (the closest) can be grabbed at the same time. The relevant distance is the distance between the middle point of a graph and the left controller. There are also options to dispose of those stacks of graphs.</p>
      <p>The idea of displaying parts of the results as stacked 2D projections had already been used in previous applications. Tarsier, for example, uses them to separate nodes that do or do not meet user-defined filter criteria into different planes (<xref rid="R25" ref-type="bibr">25</xref>). In the current version of Graph2VR, we did not implement a filter function, but the Tarsier approach might be an excellent way to do so in the future.</p>
      <p>SPARQL has several commands to modify a query. The limit slider was already introduced to limit the number of nodes when expanding the graph (<xref rid="F7" ref-type="fig">Figure 7(B)</xref>). It also can be used to limit the number of result graphs when sending a query. Another modifier is the ORDER BY command, which is used to order the stacked result graphs either descending or ascending. We added this menu option so that the variable can be selected by clicking on it, and there is a small adjacent button (ASC/DESC) to adjust the order.</p>
      <p>Graph2VR can connect to local and online SPARQL Endpoints. If databases are preconfigured, users can switch between them via the circle menu.</p>
    </sec>
    <sec id="s5-s2">
      <title>Demonstration data</title>
      <p>For testing purposes, we provided an easy-to-understand data example from DBpedia, which is one of the best-known public sources for Linked Data and is based on Wikipedia (<xref rid="R58" ref-type="bibr">58</xref>).</p>
      <p>In contrast to Wikipedia, it is possible to query data in DBpedia using SPARQL queries. One of the examples from a lecture about the semantic web was how to ask DBpedia for the second-highest mountain in a certain country, e.g. Australia (<xref rid="R59" ref-type="bibr">59</xref>). While it is challenging to find this information simply by reading Wikipedia articles, writing a SPARQL query to get this information from DBpedia is relatively easy. Since all the information is represented as triples, a user only needs to find out which predicates are used to encode height (dbo: elevation), location (dbp: location) and the fact that it should be a mountain (rdf: type dbo: mountain). The prefixes (res, dbo, dbp and rdf) are the abbreviations of the base URIs and are defined first. To find out which URIs need to be used, a user could open an entry about any mountain, look up the respective predicates and use that information to build their SPARQL query. The following example query is not about the second-highest mountain in Australia but rather about the highest mountains in DBpedia and their location. The mountains are ordered in descending order by height:</p>
      <p>
        <disp-quote>
          <preformat position="float" xml:space="preserve">PREFIX res: &lt;<ext-link xlink:href="http://dbpedia.org/resource/" ext-link-type="uri">http://dbpedia.org/resource/</ext-link>&gt;PREFIX dbo: &lt;<ext-link xlink:href="http://dbpedia.org/ontology/" ext-link-type="uri">http://dbpedia.org/ontology/</ext-link>&gt;PREFIX dbp: &lt;<ext-link xlink:href="http://dbpedia.org/property/" ext-link-type="uri">http://dbpedia.org/property/</ext-link>&gt;PREFIX rdf: &lt;<ext-link xlink:href="http://www.w3.org/1999/02/22-rdf-syntax-ns" ext-link-type="uri">http://www.w3.org/1999/02/22-rdf-syntax-ns</ext-link>&gt;
SELECT ?Mountain ?location ?heightWhere {?Mountain rdf: type dbo: Mountain.?Mountain dbp: location ?location.?Mountain dbo: elevation ?height.}ORDER BY DESC(?height)Limit 25</preformat>
        </disp-quote>
      </p>
      <p>Within Graph2VR, this search can be done visually without manually looking up all the URIs for the predicates. All outgoing predicates are listed in the menu, so the user only needs to select them.</p>
      <fig position="float" id="F10" fig-type="figure">
        <label>Figure 10.</label>
        <caption>
          <p>Creating a query pattern in Graph2VR (10(A)) and requesting the result graphs (10(B)). (A) Query patterns can be created visually in Graph2VR by selecting the triples in the graph that should be part of the query. The query can be modified using the Language settings, Order By options and the Query Limit slider. (B) After rotating the graph and clicking on ‘Request similar patterns’, the results are displayed in a stack of multiple independent graphs.</p>
        </caption>
        <graphic xlink:href="baae008f10" position="float"/>
      </fig>
      <p>Within Graph2VR, it is possible to select the relevant triples, add those to the query pattern, transform the respective nodes into variables and then send the query to the SPARQL Endpoint. Besides the limit, the example query is the same one used as the example for Gruff (<xref rid="F4" ref-type="fig">Figure 4</xref>). This task might still be tricky for a number of reasons: not every mountain’s height is represented using the same predicate, some mountains might be missing in the database or there might be multiple instances encoding the same mountain. However, this is a limitation of DBpedia data, not Graph2VR.</p>
      <p>In many cases, DBpedia uses language tags that indicate the language used for the literal. Just asking for all labels results in many labels in different languages. We therefore added a language filter feature to Graph2VR that can be set via the settings menu. This allows users to request only labels with a specific language tag or no language tag at all. Once set, this is applied to every query. This may have consequences for the results, as it will not be indicated when the content is unavailable in the desired language or without a language tag. However, being able to set these conditions in the menu is more convenient than writing a SPARQL query to do so.</p>
    </sec>
    <sec id="s5-s3">
      <title>Usability survey</title>
      <p>To assess usability, we conducted a user evaluation with 34 participants. We were interested in how useful the application was for sensemaking, whether users enjoy the experience in VR and what influences their perceptions. By ‘sensemaking’, we mean the ability of the participant to answer questions or get understandable content from the semantic graph using the Graph2VR interface. We adopted this term from the Tarsier paper, which reused it from another earlier paper (<xref rid="R25" ref-type="bibr">25</xref>, <xref rid="R60" ref-type="bibr">60</xref>) . Some participants already had some experience with in GraphDB, SPARQL or Linked Data, but others did not. We guided all first-time users through the application, told them about their options and let them test the different features. Then, we asked the users a series of questions, described in <xref rid="s10" ref-type="sec">Supplementary Appendix B</xref>. The results are summarized in <xref rid="F11 F12" ref-type="fig">Figures 11 and 12</xref> with data from the usability survey available in <xref rid="s10" ref-type="sec">Supplementary Appendix B</xref>.</p>
      <fig position="float" id="F11" fig-type="figure">
        <label>Figure 11.</label>
        <caption>
          <p>Results usability questionnaire questions 9–15.</p>
        </caption>
        <graphic xlink:href="baae008f11" position="float"/>
      </fig>
      <fig position="float" id="F12" fig-type="figure">
        <label>Figure 12.</label>
        <caption>
          <p>Results usability questionnaire questions 19 and 20.</p>
        </caption>
        <graphic xlink:href="baae008f12" position="float"/>
      </fig>
    </sec>
    <sec id="s5-s4">
      <title>Evaluation results</title>
      <p>We identified several factors that influenced how much participants enjoyed testing Graph2VR and how effective they found the tool. One hypothesis was that people who are more used to playing computer games, especially if they have experience with VR glasses, might have an easier time using our application. It was uncertain whether age would have any effect, but we could imagine that younger participants, as digital natives, might find it easier. So there might be a weak correlation. To differentiate between enjoyment and effectiveness, both questions were asked after the VR experience and next to each other. We assumed that people who had a better experience, with fewer bugs, would rate their enjoyment as well as the tool’s effectivity for sensemaking more highly. We also expected a worse rating when the application had problems like poor readability or functions not working. An intuitive user interface makes the application more effective and enjoyable, whereas high complexity can make the application more interesting for experts but more difficult to use for beginners. During our studies, we determined that most participants found navigation and exploration quite easy, while the query-building seemed more challenging. Creating and modifying queries seemed to be noticeably easier for the ‘expert users’ who had at least some experience with Linked Data and writing SPARQL queries. Finally, we looked at the correlations between participants’ answers to the different questions. In addition to the Pearson correlation, <italic toggle="yes">P</italic>-values were calculated to determine the significance of those correlations. The complete correlation matrix is given in <xref rid="s10" ref-type="sec">Supplementary Appendix Table A6</xref>. In <xref rid="T2" ref-type="table">Table 2</xref>, we summarize the statistically significant correlations.</p>
      <table-wrap position="float" id="T2">
        <label>Table 2.</label>
        <caption>
          <p>Interpretation of the statistically significant correlations of the usability study results ordered by p-values.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col align="left" span="1"/>
          </colgroup>
          <tbody>
            <tr>
              <td align="left" rowspan="1" colspan="1">
                <inline-graphic xlink:href="baae008fa1.jpg"/>
              </td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>The questions, results and a more detailed evaluation can be found in Supplementary Appendix B. The findings are mostly as expected: having played video games more often is positively correlated with more intuitive navigation in the virtual world. Most participants also preferred flying to teleportation. We also found a positive correlation between navigation with gesture controls, requesting query results and exploring further details. We were curious whether age would have any significant effect on the experience of Graph2VR. Most participants (26/34) were between 30 and 50 years old. The sample size of people outside that age group was too small and probably not representative enough to draw any statistically significant conclusions. At the end of the experiment, we asked participants how much they enjoyed the experience and how effective Graph2VR is for sensemaking. Since both questions were asked at the same moment after the test, it is not a surprise that the answers are highly correlated. What is interesting is which factors are most relevant. For enjoying the experience, there is a strong correlation between the gesture controls and the graph exploration, as well as the exploration of further details. The exploration of further details is the second-to-last question during the VR session. A positive rating for this question implies that it was possible to request further results and that no major bug prevented this. When a problem occurred, the rating and the overall impression decreased. As Graph2VR was still under active development during the user study, some of the issues were fixed over time. Examples of this are the Limit slider that intersected with the scrollbar around the circle menu, nodes that were hard to select when they were zoomed too large and a layout option that led to many nodes being stapled at the same position.</p>
      <p>Setting up the query, selecting the relevant triples, setting the order (and optionally the limit) and requesting the results seemed to be the most complex part of the study for many participants, especially those with no previous experience with SPARQL. Query-building was also the most complex and error-prone part as we were still developing features that interfered with this process. It also takes users time to really understand how this feature works, especially for individuals who did not have any experience with SPARQL queries. Setting up SPARQL query patterns (like in Gruff) is one of the most relevant features of Graph2VR. While it seemed to already be complex for inexperienced users, users who already had some experience were asking us to implement even more SPARQL commands.</p>
    </sec>
  </sec>
  <sec id="s6">
    <title>Discussion</title>
    <p>Graph2VR is a prototype VR application for visualizing and exploring Linked Data in the form of 3D graphs. After exploring and testing multiple existing tools, we used Unity to create a user interface and DotNetRDF to connect to SPARQL Endpoints. We then tested our tool with a local Virtuoso server and DBpedia’s publicly accessible SPARQL Endpoint. To test the application and obtain feedback and suggestions for improvements, we conducted a usability study with 34 individuals who had never tried Graph2VR before.</p>
    <sec id="s6-s1">
      <title>User feedback</title>
      <p>Most testers were impressed and somewhat overwhelmed at the beginning, especially if they had never used a VR headset before. During the study, we guided them through the different functionalities and explained what they could do. During the tests, we noted their comments. After their VR session, we asked them to give feedback. This yielded valuable feedback from participants, including comments like ‘It felt good to be inside the world of the database’, ‘This is very fun to use and a great way of organizing and querying data’ and ‘I would love to try it out with other ontologies like Orphanet and connect it to applications like the RD-Connect Sample Catalogue (once that is in EMX2 so we can connect it)’ For explanation: The Entity Model eXtensible (EMX) is an internal metadata format of Molgenis, the current version is version 2 (<xref rid="R61" ref-type="bibr">61</xref>).</p>
      <p>We explicitly asked the testers about specific problems they experienced and any suggestions they had for the application. It takes some time to explore all the different functionalities that we have built into Graph2VR. One tester mentioned, [I] “need to get used to the tool, but after that, it is good :)”. Another tester found the movement in space initially challenging: ‘moving in space was a bit hard in the beginning, but started to feel more natural along the way’. The most commonly mentioned issue among the testers was the readability of the text in the application. While the menu and nodes and edges nearby were legible, the labels of distant nodes and edges were hard to read. This problem is related to the resolution of the VR headset, which needs to display texts that are further away in just a couple of pixels. One general critique was that the VR headset is still quite heavy. After long test sessions, some people felt somewhat dizzy or tired. That was not unexpected. When using VR glasses, especially for the first time, manufacturers recommend taking breaks at least every 30 min. Our test sessions (including some introduction and filling in the questionnaire) took 40–60 min, on average.</p>
      <p>We also asked more advanced users some additional questions, such as how Graph2VR compares to other conventional tools. Some of the experts mentioned that typing SPARQL queries by hand would still be faster than using the virtual keyboard and selecting the triples one-by-one. Graph2VR is somewhat limited by not having certain keywords, like Optional and Subqueries. Nevertheless, these users saw the potential that 3D visualization offers: ‘[Graph2VR is] much more fun, also allows much more data to be visualized, opens new possibilities’.</p>
      <p>In all, we received constructive feedback, and we have already fixed several of the issues mentioned by testers. For example:</p>
      <list list-type="bullet">
        <list-item>
          <p>The scrollbar no longer overlaps with the Limit slider.</p>
        </list-item>
        <list-item>
          <p>There was an issue with selecting nodes when the graph was scaled too big.</p>
        </list-item>
        <list-item>
          <p>Some testers asked for more visual feedback when clicking a button or to recognize when new nodes spawn. Both have been added.</p>
        </list-item>
        <list-item>
          <p>As we had at least two left-handed testers, we decided to add an option for left-handed people. It is not perfect as the menu still points to the right, but we have already received some positive feedback.</p>
        </list-item>
      </list>
      <p>Other issues persist and need to be addressed in future versions:</p>
      <list list-type="bullet">
        <list-item>
          <p>When the trigger is pressed while not pointing at any menu item, node,or edge, the menu should close. When scrolling down the scrollbar of the circle menu with the laser, it is easy to slip down the scrollbar, causing the menu to close. The circular scrollbar should be used instead via the slider knob on the scrollbar, a small ball that can be grabbed and then moved around.</p>
        </list-item>
        <list-item>
          <p>Some testers mentioned that the movement speed for flying in Graph2VR was quite fast and highly responsive, making it difficult to control the movement. This could be addressed in the future by adding an option to adjust the movement speed.</p>
        </list-item>
        <list-item>
          <p>When a graph bumps into the ‘floor’, the nodes collide with the floor and the graph deforms, which might result in a flat graph. This can also happen when a query is sent and a stack of result graphs is requested as the new graphs are spawned in the looking direction and might collide with the platform. One way to fix this is to trigger a layout algorithm.</p>
        </list-item>
      </list>
      <p>Overall, user study feedback was valuable for identifying areas of improvement. Participants’ comments complemented their scores and elucidated specific strengths and weaknesses of Graph2VR. For example, several participants noted the difficulty of reading node and edge labels in VR, and this was also reflected in lower scores for readability. Participants with previous VR experience found navigation easier, while those who had experience with SPARQL queries found it easier to create the query patterns.</p>
    </sec>
    <sec id="s6-s2">
      <title>Limitations of VR</title>
      <p>One of our expectations was that the almost unlimited space in VR and 3D could help users visualize more nodes at once. Based on the reviewers’ comments, we can now confirm this. However, one ongoing challenge in VR is the readability due to a limited resolution of the node and edge labels. To improve readability, we increased the font size of texts when hovering on them, but they remain hard to read when the graph is too small or the text is too far away. We also replaced the URIs of nodes and edges with their labels. If no label is available, Graph2VR displays the Compact URI, a shortened version of the URI in which the namespace is replaced by a prefix to shorten the URI. In addition to the textual representation, images, colours and shapes can also help to differentiate between different types of nodes. We had originally planned to allow federated queries for requesting data from multiple SPARQL endpoints at once, but this has not yet been implemented.</p>
    </sec>
    <sec id="s6-s3">
      <title>Ideas for future development</title>
      <p>In the current version of Graph2VR, the colour schema is derived from the VOWL specification (<xref rid="R30" ref-type="bibr">30</xref>). But not all the colours and shapes from VOWL are implemented yet.</p>
      <p>In the future, we would like to add further SPARQL commands to the application. Important SPARQL commands or keywords such as <italic toggle="yes">OPTIONAL</italic>, <italic toggle="yes">SERVICE</italic> (for federated queries), <italic toggle="yes">UNION</italic>, <italic toggle="yes">BIND</italic>, <italic toggle="yes">OFFSET</italic>, <italic toggle="yes">DISTINCT</italic>, <italic toggle="yes">COUNT</italic>, <italic toggle="yes">GROUP BY</italic> and <italic toggle="yes">MINUS</italic>, as well as <italic toggle="yes">FILTER</italic>, are supported by DotNetRDF but have not yet been implemented in the Graph2VR interface. In addition, an ‘undo’ button would be desirable, especially to restore accidentally deleted elements or graphs. Many test users found Graph2VR already quite complex, mainly because they were using it for the first time and did not have much experience with graph databases. Adding new functionalities will only increase the complexity further, so it might be a good idea to create a wizard to lead users through the query-building process in order to make this process easier. Another feature that should be implemented in the future is the ability to log and export some of the queries that have been performed. In addition, it should be possible to export the latest SPARQL query to use, e.g. in a browser. To find a specific object in the menu of outgoing nodes, another layer of entries would be helpful. This could be triggered when clicking the count of objects instead of the object itself. A similar feature would be to display all the existing connections between two nodes when setting the predicate for a new connection between two nodes. Finally, Graph2VR could be transformed into an augmented reality application in the future. This would especially be interesting as a multi-user application.</p>
    </sec>
  </sec>
  <sec id="s7">
    <title>Conclusion</title>
    <p>We developed Graph2VR, a prototype VR application for visualizing and exploring Linked Data in the form of 3D graphs, and conducted a usability study with 34 testers, which provided valuable feedback on the tool’s usability and areas for improvement. We believe that Graph2VR represents a novel and engaging way of visualizing and exploring Linked Data. The overall user experience reported during the usability study was positive, especially among more experienced users. While there are still some limitations and issues to be addressed, we are confident that, with further development and refinement, VR will provide tools for working with large Linked Data graphs.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>baae008_Supp</label>
      <media xlink:href="baae008_supp.zip"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>We would like to thank Gert-Jan Verheij, who leads the visualization group at the Center for Information Technology (CIT) and organizes the XRHub, where we presented Graph2VR. We thank our colleagues Prof. Dr Isabel Fortier and Dr Tina Wey from the Maelstrom Institute in Montreal for providing us with the REACH data as a test dataset. Of course, we thank all participants of our usability study, especially the expert users who gave us critical input and feedback on how to improve the application. Last but not least, we thank Kate Mc Intyre for editing this manuscript.</p>
  </ack>
  <sec sec-type="data-availability" id="s8">
    <title>Data availability</title>
    <p>The source code of Graph2VR is openly available on GitHub <ext-link xlink:href="http://github.com/molgenis/Graph2VR" ext-link-type="uri">http://github.com/molgenis/Graph2VR</ext-link>, under LGPL v3 license. The technical manual can be found in the attachments. The questions from the usability study, as well as the anonymous answers of the participants, can also be found in the attachments. A technical user manual for Graph2VR (version 1) can be found here: <ext-link xlink:href="https://doi.org/10.5281/zenodo.8040594" ext-link-type="uri">https://doi.org/10.5281/zenodo.8040594</ext-link>. A Graph2VR tutorial playlist is available on YouTube (<xref rid="R1" ref-type="bibr">1</xref>): <ext-link xlink:href="https://www.youtube.com/playlist?list=PLRQCsKSUyhNIdUzBNRTmE-_JmuiOEZbdH" ext-link-type="uri">https://www.youtube.com/playlist? list=PLRQCsKSUyhNIdUzBNRTmE-_JmuiOEZbdH</ext-link>.</p>
  </sec>
  <sec id="s9">
    <title>Author contributions</title>
    <p><bold>Alexander Kellmann</bold> contributed to conceptualization, software, writing—original draft, visualization, investigation, data curation, formal analysis</p>
    <p>Max Postema contributed to software and visualization</p>
    <p>Pjotr Svetachov contributed to software and visualization</p>
    <p>Joris de Keijser contributed to software and visualization</p>
    <p>Becca Wilson: contributed to methodology, writing—original draft, writing—review and editing, suspervision</p>
    <p>Esther van Enckevort contributed to supervision, writing—review and editing</p>
    <p>Morris A. Swertz contributed to supervision, writing—review and editing</p>
  </sec>
  <sec id="s10">
    <title>Supplementary Material</title>
    <p><xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref> is available at <italic toggle="yes">Database</italic> online.</p>
  </sec>
  <sec id="s11">
    <title>Funding</title>
    <p>EUCAN-Connect, a federated FAIR platform enabling large-scale analysis of high-value cohort data connecting Europe and Canada in personalized health, which is funded by the European Union’s Horizon 2020 research and innovation programme under grant agreement No 824 989; and a UKRI Innovation Fellowship with HDR UK (MR/S003959/2; R.C.W.).</p>
  </sec>
  <sec sec-type="COI-statement" id="s12">
    <title>Conflict of interest statement</title>
    <p>The authors declare no conflict of interest.</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="R1">
      <label>1.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kellmann</surname><given-names>A.</given-names></string-name> and <string-name><surname>Postema</surname><given-names>M.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Graph2VR tutorial part 1 – 5</article-title>. <ext-link xlink:href="https://www.youtube.com/playlist?list=PLRQCsKSUyhNIdUzBNRTmE-_JmuiOEZbdH" ext-link-type="uri">https://www.youtube.com/playlist?list=PLRQCsKSUyhNIdUzBNRTmE-_JmuiOEZbdH</ext-link> (<comment>4 December 2023, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wilkinson</surname><given-names>M.D.</given-names></string-name>, <string-name><surname>Dumontier</surname><given-names>M.</given-names></string-name>, <string-name><surname>Aalbersberg</surname><given-names>I.J.J.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2016</year>) <article-title>The FAIR Guiding Principles for scientific data management and stewardship</article-title>. <source><italic toggle="yes">Sci. Data.</italic></source>, <volume>3</volume>, <page-range>160018</page-range>.</mixed-citation>
    </ref>
    <ref id="R3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Croset</surname><given-names>S.</given-names></string-name>, <string-name><surname>Rupp</surname><given-names>J.</given-names></string-name> and <string-name><surname>Romacker</surname><given-names>M.</given-names></string-name></person-group> (<year>2016</year>) <article-title>Flexible data integration and curation using a graph-based approach</article-title>. <source><italic toggle="yes">Bioinformatics</italic></source>, <volume>32</volume>, <fpage>918</fpage>–<lpage>925</lpage>.<pub-id pub-id-type="pmid">26556384</pub-id>
</mixed-citation>
    </ref>
    <ref id="R4">
      <label>4.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Berners-Lee</surname><given-names>T.</given-names></string-name> and <string-name><surname>Fischetti</surname><given-names>M.</given-names></string-name></person-group> (<year>2000</year>) <source><italic toggle="yes">Weaving the Web: The Original Design and Ultimate Destiny of the World Wide Web by its inventor.</italic></source>  <publisher-name>HarperCollins</publisher-name>, <publisher-loc>San Francisco, CA</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="R5">
      <label>5.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Berners-Lee</surname><given-names>T.</given-names></string-name></person-group> (<year>2009</year>) <article-title>The next web</article-title>. <ext-link xlink:href="https://www.ted.com/talks/tim_berners_lee_the_next_web/transcript" ext-link-type="uri">https://www.ted.com/talks/tim_berners_lee_the_next_web/transcript</ext-link>.</mixed-citation>
    </ref>
    <ref id="R6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Desimoni</surname><given-names>F.</given-names></string-name> and <string-name><surname>Po</surname><given-names>L.</given-names></string-name></person-group> (<year>2020</year>) <article-title>Empirical evaluation of Linked Data visualization tools</article-title>. <source><italic toggle="yes">Future Gener. Comput. Syst.</italic></source>, <volume>112</volume>, <fpage>258</fpage>–<lpage>282</lpage>.</mixed-citation>
    </ref>
    <ref id="R7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Callahan</surname><given-names>A.</given-names></string-name>, <string-name><surname>Cruz-Toledo</surname><given-names>J.</given-names></string-name> and <string-name><surname>Dumontier</surname><given-names>M.</given-names></string-name></person-group> (<year>2013</year>) <article-title>Ontology-based querying with Bio2RDF’s linked open ata</article-title>. <source><italic toggle="yes">J. Biomed. Semant.</italic></source>, <volume>4 Suppl 1</volume>, <fpage>1</fpage>–<lpage>13</lpage>.</mixed-citation>
    </ref>
    <ref id="R8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fu</surname><given-names>G.</given-names></string-name>, <string-name><surname>Batchelor</surname><given-names>C.</given-names></string-name>, <string-name><surname>Dumontier</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2015</year>) <article-title>PubChemRDF: Towards the semantic annotation of PubChem compound and substance databases</article-title>. <source><italic toggle="yes">J. Cheminformatics</italic></source>, <volume>7</volume>, <page-range>34</page-range>.</mixed-citation>
    </ref>
    <ref id="R9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Galgonek</surname><given-names>J.</given-names></string-name>, <string-name><surname>Hurt</surname><given-names>T.</given-names></string-name>, <string-name><surname>Michlíková</surname><given-names>V.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2016</year>) <article-title>Advanced SPARQL querying in small molecule databases</article-title>. <source><italic toggle="yes">J. Cheminformatics</italic></source>, <volume>8</volume>, <page-range>31</page-range>.</mixed-citation>
    </ref>
    <ref id="R10">
      <label>10.</label>
      <mixed-citation publication-type="other">(<year>2015</year>) <article-title>RDF - Semantic Web Standards</article-title>. <ext-link xlink:href="https://www.w3.org/RDF/" ext-link-type="uri">https://www.w3.org/RDF/</ext-link> (<comment>12 December 2022, date last accepted</comment>).</mixed-citation>
    </ref>
    <ref id="R11">
      <label>11.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Coffey</surname><given-names>D.</given-names></string-name><string-name><surname>Malbraaten</surname><given-names>N.</given-names></string-name><string-name><surname>Le</surname><given-names>T.</given-names></string-name></person-group>  <etal>et al.</etal> (<year>2011</year>) <part-title>Slice WIM</part-title>. In <person-group person-group-type="editor"><string-name><surname>Garland</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Wang</surname>, <given-names>R.</given-names></string-name></person-group> (<edition>eds.</edition>), <source>Symposium on Interactive 3D Graphics and Games</source>. <publisher-name>ACM</publisher-name>, <publisher-loc>New York, NY, USA</publisher-loc>, <fpage>pp. 191</fpage>–<lpage>198</lpage>.</mixed-citation>
    </ref>
    <ref id="R12">
      <label>12.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Czauderna</surname><given-names>T.</given-names></string-name><string-name><surname>Haga</surname><given-names>J.</given-names></string-name><string-name><surname>Kim</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al.</etal> (<year>2018</year>) <part-title>Immersive Analytics Applications in Life and Health Sciences</part-title>. In <person-group person-group-type="editor"><string-name><surname>Marriott</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Schreiber</surname>, <given-names>F.</given-names></string-name> and <string-name><surname>Dwyer</surname>, <given-names>T.</given-names></string-name></person-group>  <etal>et al</etal>. (<edition>eds.</edition>), <source>Immersive Analytics</source>. <publisher-name>Springer International Publishing</publisher-name>, <publisher-loc>Cham</publisher-loc>, <volume>611190</volume>, <fpage>pp. 289</fpage>–<lpage>330</lpage>.</mixed-citation>
    </ref>
    <ref id="R13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lau</surname><given-names>C.W.</given-names></string-name>, <string-name><surname>Qu</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Draper</surname><given-names>D.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2022</year>) <article-title>Virtual reality for the observation of oncology models (VROOM): immersive analytics for oncology patient cohorts</article-title>. <source><italic toggle="yes">Sci. Rep.</italic></source>, <volume>12</volume>, <page-range>11337</page-range>.</mixed-citation>
    </ref>
    <ref id="R14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Qu</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Lau</surname><given-names>C.W.</given-names></string-name>, <string-name><surname>Simoff</surname><given-names>S.J.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2022</year>) <article-title>Review of innovative immersive technologies for healthcare applications</article-title>. <source><italic toggle="yes">Innovations in Digital Health, Diagnostics, and Biomarkers</italic></source>, <volume>2</volume>, <fpage>27</fpage>–<lpage>39</lpage>.</mixed-citation>
    </ref>
    <ref id="R15">
      <label>15.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>McCrae</surname><given-names>J.P.</given-names></string-name></person-group> (<year>2022</year>) <article-title>The Linked Open Data Cloud</article-title>. <ext-link xlink:href="https://lod-cloud.net/" ext-link-type="uri">https://lod-cloud.net/</ext-link> (<comment>13 January 2023, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R16">
      <label>16.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Deligiannidis</surname><given-names>L.</given-names></string-name><string-name><surname>Sheth</surname><given-names>A.P.</given-names></string-name><string-name><surname>Aleman-Meza</surname><given-names>B.</given-names></string-name></person-group> (<year>2006</year>) <part-title>Semantic Analytics Visualization</part-title>. In <person-group person-group-type="editor"><string-name><surname>Hutchison</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Kanade</surname>, <given-names>T.</given-names></string-name> and <string-name><surname>Kittler</surname>, <given-names>J.</given-names></string-name></person-group>  <etal>et al</etal>. (eds.), <source>Intelligence and Security Informatics</source>. <publisher-name>Springer Berlin Heidelberg</publisher-name>, <publisher-loc>Berlin, Heidelberg</publisher-loc>, <volume>3975</volume>, <fpage>pp. 48</fpage>–<lpage>59</lpage>.</mixed-citation>
    </ref>
    <ref id="R17">
      <label>17.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>Sony Green &amp; Robert Allison</collab></person-group>. (<year>2019</year>) <article-title>KINEVIZ GraphXR: How to GraphXR: for GraphXR v2.2.1</article-title>. <ext-link xlink:href="https://static1.squarespace.com/static/5c58b86e8dfc8c2d0d700050/t/5df2b6134e0d57635d14df4b/1576187456752/How+to+GraphXR.pdf" ext-link-type="uri">https://static1.squarespace.com/static/5c58b86e8dfc8c2d0d700050/t/5df2b6134e0d57635d14df4b/1576187456752/How+to+GraphXR.pdf</ext-link> (<comment>24 August 2020, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R18">
      <label>18.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Asturiano</surname><given-names>V.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2021</year>) <article-title>vasturiano/3d-force-graph</article-title>. <ext-link xlink:href="https://github.com/vasturiano/3d-force-graph/" ext-link-type="uri">https://github.com/vasturiano/3d-force-graph/</ext-link> (<comment>18 February 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R19">
      <label>19.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Camarda</surname><given-names>D.V.</given-names></string-name>, <string-name><surname>Mazzini</surname><given-names>S.</given-names></string-name> and <string-name><surname>Antonuccio</surname><given-names>A</given-names></string-name></person-group>. (<year>2012</year>) <article-title>LodLive, exploring the web of data</article-title>. <italic toggle="yes">Proceedings of the 8th International Conference on Semantic Systems</italic>. <publisher-name>ACM Digital Library</publisher-name>, <conf-loc>New York, NY, USA</conf-loc>. <page-range>p. 197</page-range>.</mixed-citation>
    </ref>
    <ref id="R20">
      <label>20.</label>
      <mixed-citation publication-type="other">(<year>2020</year>) <article-title>GraphDB</article-title>. <ext-link xlink:href="https://graphdb.ontotext.com/" ext-link-type="uri">https://graphdb.ontotext.com/</ext-link> (<comment>5 March 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R21">
      <label>21.</label>
      <mixed-citation publication-type="other">(<year>2022</year>) <article-title>Toran (VR tech demo on steam)</article-title>. <ext-link xlink:href="https://store.steampowered.com/app/720300/Toran/" ext-link-type="uri">https://store.steampowered.com/app/720300/Toran/</ext-link> (<comment>9 March 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R22">
      <label>22.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>OWL Web Ontology Language Overview</collab></person-group>. (<year>2009</year>) <ext-link xlink:href="https://www.w3.org/TR/owl-features/" ext-link-type="uri">https://www.w3.org/TR/owl-features/</ext-link> (<comment>19 May 2017, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lohmann</surname><given-names>S.</given-names></string-name>, <string-name><surname>Negru</surname><given-names>S.</given-names></string-name>, <string-name><surname>Haag</surname><given-names>F.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2016</year>) <article-title>Visualizing Ontologies with VOWL</article-title>. <source><italic toggle="yes">Semantic Web</italic></source>, <volume>7</volume>, <fpage>399</fpage>–<lpage>419</lpage>.</mixed-citation>
    </ref>
    <ref id="R24">
      <label>24.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>Franz Inc</collab></person-group>. (<year>2020</year>) <article-title>Gruff</article-title>. <ext-link xlink:href="https://allegrograph.com/products/gruff/" ext-link-type="uri">https://allegrograph.com/products/gruff/</ext-link> (<comment>3 November 2020, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Viola</surname><given-names>F.</given-names></string-name>, <string-name><surname>Roffia</surname><given-names>L.</given-names></string-name>, <string-name><surname>Antoniazzi</surname><given-names>F.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2018</year>) <article-title>Interactive 3D Exploration of RDF Graphs through Semantic Planes</article-title>. <source><italic toggle="yes">Future Internet</italic></source>, <volume>10</volume>, <fpage>1</fpage>–<lpage>30</lpage>.</mixed-citation>
    </ref>
    <ref id="R26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dudáš</surname><given-names>M.</given-names></string-name>, <string-name><surname>Lohmann</surname><given-names>S.</given-names></string-name>, <string-name><surname>Svátek</surname><given-names>V.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2018</year>) <article-title>Ontology visualization methods and tools: a survey of the state of the art</article-title>. <source><italic toggle="yes">Knowl. Eng. Rev.</italic></source>, <page-range>33</page-range>.</mixed-citation>
    </ref>
    <ref id="R27">
      <label>27.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Lanzenberger</surname><given-names>M.</given-names></string-name>, <string-name><surname>Sampson</surname><given-names>J.</given-names></string-name> and <string-name><surname>Rester</surname><given-names>M</given-names></string-name></person-group>. (<year>2009</year>) <article-title>Visualization in Ontology Tools</article-title>. <italic toggle="yes">International Conference on Complex, Intelligent and Software Intensive Systems</italic>. <publisher-name>IEEE</publisher-name>, <conf-loc>Piscataway, NJ</conf-loc>. <fpage>pp. 705</fpage>–<lpage>711</lpage>.</mixed-citation>
    </ref>
    <ref id="R28">
      <label>28.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Palma</surname><given-names>R.</given-names></string-name></person-group> (<year>2021</year>) <article-title>A Knowledge Graph for the Agri-Food Sector</article-title>. <ext-link xlink:href="https://blog.metaphacts.com/a-knowledge-graph-for-the-agri-food-sector" ext-link-type="uri">https://blog.metaphacts.com/a-knowledge-graph-for-the-agri-food-sector</ext-link> (<comment>8 March 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Musen</surname><given-names>M.A.</given-names></string-name></person-group> (<year>2015</year>) <article-title>The protégé Project: a look back and a look forward</article-title>. <source><italic toggle="yes">AI matters</italic></source>, <volume>1</volume>, <fpage>4</fpage>–<lpage>12</lpage>.<pub-id pub-id-type="pmid">27239556</pub-id>
</mixed-citation>
    </ref>
    <ref id="R30">
      <label>30.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Steffen Lohmann</surname><given-names>S.N.</given-names></string-name></person-group> (<year>2019</year>) <article-title>VOWL: Visual Notation for OWL Ontologies</article-title>. <ext-link xlink:href="http://vowl.visualdataweb.org/" ext-link-type="uri">http://vowl.visualdataweb.org/</ext-link> (<comment>13 January 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R31">
      <label>31.</label>
      <mixed-citation publication-type="other">(<year>2017</year>) <article-title>QueryVOWL</article-title>. <ext-link xlink:href="http://vowl.visualdataweb.org/queryvowl/queryvowl.html" ext-link-type="uri">http://vowl.visualdataweb.org/queryvowl/queryvowl.html</ext-link> (<comment>13 January 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R32">
      <label>32.</label>
      <mixed-citation publication-type="other">(<year>2019</year>) <article-title>WebVOWL</article-title>. <ext-link xlink:href="http://www.visualdataweb.de/webvowl/#" ext-link-type="uri">http://www.visualdataweb.de/webvowl/#</ext-link> (<comment>13 January 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R33">
      <label>33.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><given-names>Fabio</given-names><surname>V.</surname></string-name></person-group>  <etal>et al</etal>. (<year>2018</year>) <article-title>Tarsier – exploring DBpedia</article-title>. <ext-link xlink:href="https://www.youtube.com/watch?v=OgoxFWAb1vQ" ext-link-type="uri">https://www.youtube.com/watch?v=OgoxFWAb1vQ</ext-link> (<comment>2 December 2022, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R34">
      <label>34.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kellmann</surname><given-names>A.J.</given-names></string-name>, <string-name><surname>Postema</surname><given-names>M.</given-names></string-name>, <string-name><surname>Keijser</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2023</year>) <article-title>Graph2VR Manual</article-title>. <source><italic toggle="yes">Zenodo</italic></source>, <fpage>1</fpage>–<lpage>20</lpage>.</mixed-citation>
    </ref>
    <ref id="R35">
      <label>35.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Pienta</surname><given-names>R.</given-names></string-name>, <string-name><surname>Abello</surname><given-names>J.</given-names></string-name>, <string-name><surname>Kahng</surname><given-names>M</given-names></string-name></person-group>. <etal>et al.</etal> (<year>2015</year>) <article-title>Scalable graph exploration and visualization: Sensemaking challenges and opportunities</article-title>. In <italic toggle="yes">2015 International Conference on Big Data and Smart Computing (BIGCOMP)</italic>, <conf-loc>Jeju, Republic of Korea</conf-loc>, <conf-date>09.02.2015-11.02.2015</conf-date>. <publisher-name>IEEE</publisher-name>, <fpage>pp. 271</fpage>–<lpage>278</lpage>.</mixed-citation>
    </ref>
    <ref id="R36">
      <label>36.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Sund</surname><given-names>D.</given-names></string-name></person-group> (<year>2016</year>) <article-title>Comparison of Visualization Algorithms for Graphs and Implementation of Visualization Algorithm for Multi-Touch table using JavaFX</article-title>, <source>Bachelor Thesis</source>, <publisher-name>Linköping</publisher-name>.</mixed-citation>
    </ref>
    <ref id="R37">
      <label>37.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fruchterman</surname><given-names>T.M.J.</given-names></string-name> and <string-name><surname>Reingold</surname><given-names>E.M.</given-names></string-name></person-group> (<year>1991</year>) <article-title>Graph drawing by force-directed placement</article-title>. <source><italic toggle="yes">Softw. - Pract. Exp.</italic></source>, <volume>21</volume>, <fpage>1129</fpage>–<lpage>1164</lpage>.</mixed-citation>
    </ref>
    <ref id="R38">
      <label>38.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barnes</surname><given-names>J.</given-names></string-name> and <string-name><surname>Hut</surname><given-names>P.</given-names></string-name></person-group> (<year>1986</year>) <article-title>A hierarchical O(N log N) force-calculation algorithm</article-title>. <source><italic toggle="yes">Nature</italic></source>, <volume>324</volume>, <fpage>446</fpage>–<lpage>449</lpage>.</mixed-citation>
    </ref>
    <ref id="R39">
      <label>39.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Swinehart</surname><given-names>C.</given-names></string-name></person-group> (<year>2011</year>) <article-title>The Barnes-Hut Algorithm</article-title>. <ext-link xlink:href="http://arborjs.org/docs/barnes-hut" ext-link-type="uri">http://arborjs.org/docs/barnes-hut</ext-link> (<comment>3 December 2020, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R40">
      <label>40.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kamada</surname><given-names>T.</given-names></string-name> and <string-name><surname>Kawai</surname><given-names>S.</given-names></string-name></person-group> (<year>1989</year>) <article-title>An algorithm for drawing general undirected graphs</article-title>. <source><italic toggle="yes">Inf. Process. Lett.</italic></source>, <volume>31</volume>, <fpage>7</fpage>–<lpage>15</lpage>.</mixed-citation>
    </ref>
    <ref id="R41">
      <label>41.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Teyseyre</surname><given-names>A.R.</given-names></string-name> and <string-name><surname>Campo</surname><given-names>M.R.</given-names></string-name></person-group> (<year>2009</year>) <article-title>An overview of 3D software visualization</article-title>. <source><italic toggle="yes">IEEE Trans. Vis. Comput. Graph.</italic></source>, <volume>15</volume>, <fpage>87</fpage>–<lpage>105</lpage>.<pub-id pub-id-type="pmid">19008558</pub-id>
</mixed-citation>
    </ref>
    <ref id="R42">
      <label>42.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Batchelor</surname><given-names>C.</given-names></string-name><string-name><surname>Brenninkmeijer</surname><given-names>C.Y.A.</given-names></string-name><string-name><surname>Chichester</surname><given-names>C.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2014</year>) <part-title>Scientific Lenses to Support Multiple Views over Linked Chemistry Data</part-title>. <source><italic toggle="yes">The Semantic Web - ISWC 2014.</italic></source> In: <person-group person-group-type="editor"><string-name><surname>Mika</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Tudorache</surname>, <given-names>T.</given-names></string-name> and <string-name><surname>Bernstein</surname>, <given-names>A.</given-names></string-name></person-group> (eds.), <publisher-name>Springer International Publishing</publisher-name>, <publisher-loc>Cham</publisher-loc>, <fpage>98</fpage>–<lpage>113</lpage>.</mixed-citation>
    </ref>
    <ref id="R43">
      <label>43.</label>
      <mixed-citation publication-type="other">(<year>2021</year>) <article-title>Noda (by Coding Leap - Steam link)</article-title>. <ext-link xlink:href="https://store.steampowered.com/app/578060/Noda/" ext-link-type="uri">https://store.steampowered.com/app/578060/Noda/</ext-link> (<comment>11 May 2021, dare last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R44">
      <label>44.</label>
      <mixed-citation publication-type="other">(<year>2020</year>) <article-title>aestheticinteractive/Hover-UI-Kit</article-title>. <ext-link xlink:href="https://github.com/aestheticinteractive/Hover-UI-Kit" ext-link-type="uri">https://github.com/aestheticinteractive/Hover-UI-Kit</ext-link> (<comment>10 Decemebr 2020, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R45">
      <label>45.</label>
      <mixed-citation publication-type="other">(<year>2021</year>) <article-title>Getting Started with metaphactory</article-title>. <ext-link xlink:href="https://help.metaphacts.com/resource/Help:Start" ext-link-type="uri">https://help.metaphacts.com/resource/Help:Start</ext-link> (<comment>5 March 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R46">
      <label>46.</label>
      <mixed-citation publication-type="other">(<year>2022</year>) <article-title>VRKeys|Input Management|Unity Asset Store</article-title>. <ext-link xlink:href="https://assetstore.unity.com/packages/tools/input-management/vrkeys-99222" ext-link-type="uri">https://assetstore.unity.com/packages/tools/input-management/vrkeys-99222</ext-link> (<comment>24 March 2022, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R47">
      <label>47.</label>
      <mixed-citation publication-type="other">(<year>2021</year>) <article-title>OpenLink Software: Virtuoso Homepage</article-title>. <ext-link xlink:href="https://virtuoso.openlinksw.com/" ext-link-type="uri">https://virtuoso.openlinksw.com/</ext-link> (<comment>25 February 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R48">
      <label>48.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>Unity Technologies</collab></person-group>.<article-title>Unity</article-title>. <ext-link xlink:href="https://unity.com/" ext-link-type="uri">https://unity.com/</ext-link> (<comment>24 February 2022, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R49">
      <label>49.</label>
      <mixed-citation publication-type="other">(<year>2022</year>) <article-title>MozillaReality/unity-webxr-export: assets for creating WebXR-enabled Unity3D projects</article-title>. <ext-link xlink:href="https://github.com/mozillareality/unity-webxr-export" ext-link-type="uri">https://github.com/mozillareality/unity-webxr-export</ext-link> (<comment>28 September 2022, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R50">
      <label>50.</label>
      <mixed-citation publication-type="other"><article-title>tenforce/virtuoso</article-title>. <ext-link xlink:href="https://hub.docker.com/r/tenforce/virtuoso/" ext-link-type="uri">https://hub.docker.com/r/tenforce/virtuoso/</ext-link> (<comment>3 March 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R51">
      <label>51.</label>
      <mixed-citation publication-type="other">(<year>2020</year>) <article-title>DotNetRDF</article-title>. <ext-link xlink:href="https://www.dotnetrdf.org/" ext-link-type="uri">https://www.dotnetrdf.org/</ext-link> (<comment>15 September 2020, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R52">
      <label>52.</label>
      <mixed-citation publication-type="other">(<year>2020</year>) <article-title>dotnetrdf/dotnetrdf</article-title>. <ext-link xlink:href="https://github.com/dotnetrdf/dotnetrdf/wiki/UserGuide-Querying-With-SPARQL" ext-link-type="uri">https://github.com/dotnetrdf/dotnetrdf/wiki/UserGuide-Querying-With-SPARQL</ext-link> (<comment>12 August 2020, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R53">
      <label>53.</label>
      <mixed-citation publication-type="other">(<year>2020</year>) <article-title>Release SteamVR Unity Plugin v2.6.0b4 - SDK 1.13.10 - ValveSoftware/steamvr_unity_plugin</article-title>. <ext-link xlink:href="https://github.com/ValveSoftware/steamvr_unity_plugin/releases/tag/2.6.0b4" ext-link-type="uri">https://github.com/ValveSoftware/steamvr_unity_plugin/releases/tag/2.6.0b4</ext-link> (<comment>12 August 2020, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R54">
      <label>54.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>The Khronos Group</collab></person-group>. (<year>2016</year>) <article-title>OpenXR - High-performance access to AR and VR – collectively known as XR – platforms and devices</article-title>. <ext-link xlink:href="https://www.khronos.org/openxr/" ext-link-type="uri">https://www.khronos.org/openxr/</ext-link> (<comment>8 March 2022, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R55">
      <label>55.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Bastian</given-names><surname>M.</surname></string-name>, <string-name><given-names>Heymann</given-names><surname>S.</surname></string-name>, and <string-name><given-names>Jacomy</given-names><surname>M.</surname></string-name></person-group> (<year>2009</year>) <article-title>Gephi: An Open Source Software for Exploring and Manipulating Networks</article-title>. <source>Proceedings of the International AAAI Conference on Web and Social Media</source>, <volume>3</volume>, <fpage>361</fpage>–<lpage>362</lpage>.</mixed-citation>
    </ref>
    <ref id="R56">
      <label>56.</label>
      <mixed-citation publication-type="other">(<year>2022</year>) <article-title>Using Full Text Search in SPARQL</article-title>. <ext-link xlink:href="https://docs.openlinksw.com/virtuoso/rdfsparqlrulefulltext/" ext-link-type="uri">https://docs.openlinksw.com/virtuoso/rdfsparqlrulefulltext/</ext-link> (<comment>2 December 2022, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R57">
      <label>57.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Bast</surname><given-names>H.</given-names></string-name>, <string-name><surname>Kalmbach</surname><given-names>J.</given-names></string-name>, <string-name><surname>Klumpp</surname><given-names>T</given-names></string-name></person-group>. <etal>et al.</etal> (<year>2022</year>) <article-title>Efficient and Effective SPARQL Autocompletion on Very Large Knowledge Graphs</article-title>. <italic toggle="yes">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management. ACM</italic>. <conf-loc>New York, NY, USA</conf-loc>. <fpage>pp. 2893</fpage>–<lpage>2902</lpage>.</mixed-citation>
    </ref>
    <ref id="R58">
      <label>58.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Auer</surname><given-names>S.</given-names></string-name>, <string-name><surname>Bizer</surname><given-names>C.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2007</year>) <part-title>DBpedia: A Nucleus for a Web of Open Data</part-title>. <source><italic toggle="yes">The Semantic Web.</italic></source>, Vol. <volume>4825</volume>, <series>Lecture Notes in Computer Science</series>, <publisher-loc>Berlin, Heidelberg</publisher-loc>, <publisher-name>Springer</publisher-name>, pp. <fpage>722</fpage>–<lpage>735</lpage>.</mixed-citation>
    </ref>
    <ref id="R59">
      <label>59.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Cimiano</surname><given-names>P.</given-names></string-name></person-group> (<year>2015</year>) <source><italic toggle="yes">Personal Communication During the “Semantic Web” Lecture at the</italic></source>  <publisher-name>University of Bielefeld</publisher-name>.</mixed-citation>
    </ref>
    <ref id="R60">
      <label>60.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Motta</surname><given-names>E.</given-names></string-name>, <string-name><surname>Mulholland</surname><given-names>P.</given-names></string-name> and <string-name><surname>Peroni</surname><given-names>S</given-names></string-name></person-group>. (<year>2011</year>) <article-title>A Novel Approach to Visualizing and Navigating Ontologies</article-title>. <volume>7031</volume>, <fpage>470</fpage>–<lpage>483</lpage>.</mixed-citation>
    </ref>
    <ref id="R61">
      <label>61.</label>
      <mixed-citation publication-type="other">(<year>2024</year>) <article-title>Molgenis 6 user documentation</article-title>. <part-title>Formats</part-title>. <ext-link xlink:href="https://molgenis.gitbooks.io/molgenis/content/v/6.0/user_documentation/guide-upload.html" ext-link-type="uri">https://molgenis.gitbooks.io/molgenis/content/v/6.0/user_documentation/guide-upload.html</ext-link>.</mixed-citation>
    </ref>
    <ref id="R62">
      <label>62.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pirch</surname><given-names>S.</given-names></string-name>, <string-name><surname>Müller</surname><given-names>F.</given-names></string-name>, <string-name><surname>Iofinova</surname><given-names>E.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2021</year>) <article-title>The VRNetzer platform enables interactive network analysis in Virtual Reality</article-title>. <source><italic toggle="yes">Nat. Commun.</italic></source>, <volume>12</volume>, <page-range>2432</page-range>.</mixed-citation>
    </ref>
    <ref id="R63">
      <label>63.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Dooley</surname><given-names>D.</given-names></string-name>, <string-name><surname>Nguyen</surname><given-names>M.</given-names></string-name> and <string-name><surname>Hsiao</surname><given-names>W.</given-names></string-name></person-group> (<year>2023</year>) <article-title>3D Visualization of Application Ontology Class Hierarchies</article-title>. <ext-link xlink:href="http://genepio.org/ontotrek" ext-link-type="uri">http://genepio.org/ontotrek</ext-link> (<comment>19 May 2020, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R64">
      <label>64.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>Kineviz inc</collab></person-group>. (<year>2022</year>) <article-title>Login</article-title>. <ext-link xlink:href="https://graphxr.kineviz.com/login" ext-link-type="uri">https://graphxr.kineviz.com/login</ext-link> (<comment>8 March 2022, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R65">
      <label>65.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>McVeigh-Schultz</surname><given-names>J.</given-names></string-name></person-group> (<year>2018</year>) <source><italic toggle="yes">Immersive Human Networks: An Exploration of How VR Network Analysis Can Transform Sensemaking and Help Organizations Become More Agile INSTITUTE FOR THE FUTURE 201 Hamilton Avenue</italic></source>, <publisher-loc>Palo Alto, CA 94301</publisher-loc>, <ext-link xlink:href="https://www.iftf.org/fileadmin/user_upload/images/More_Projects_Images/IFTF_Immersive_Human_Networks_FINAL_READER_100918__1_.pdf" ext-link-type="uri">https://www.iftf.org/fileadmin/user_upload/images/More_Projects_Images/IFTF_Immersive_Human_Networks_FINAL_READER_100918__1_.pdf</ext-link>.</mixed-citation>
    </ref>
    <ref id="R66">
      <label>66.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Capece</surname><given-names>N.</given-names></string-name>, <string-name><surname>Erra</surname><given-names>U.</given-names></string-name> and <string-name><surname>Grippa</surname><given-names>J</given-names></string-name></person-group>. (<year>2018</year>) <article-title>Graphvr: A virtual reality tool for the exploration of graphs with htc vive system</article-title>. In: <italic toggle="yes">2018 22nd International Conference Information Visualisation (iV 2018)</italic>. <publisher-name>IEEE</publisher-name>, <conf-loc>Piscataway, NJ</conf-loc>, <fpage>pp. 448</fpage>–<lpage>453</lpage>.</mixed-citation>
    </ref>
    <ref id="R67">
      <label>67.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Panahi</surname><given-names>A.</given-names></string-name></person-group> (<year>2017</year>) <article-title>Big data visualization platform for mixed reality</article-title>, <publisher-name>VCU Libraries</publisher-name>, <pub-id pub-id-type="doi">10.25772/6MDD-2B85</pub-id>.</mixed-citation>
    </ref>
    <ref id="R68">
      <label>68.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Radics</surname><given-names>P.J.</given-names></string-name>, <string-name><surname>Polys</surname><given-names>N.F.</given-names></string-name>, <string-name><surname>Neuman</surname><given-names>S.P.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2015</year>) <article-title>OSNAP! Introducing the open semantic network analysis platform</article-title>. <source><italic toggle="yes">Visualization and Data Analysis 2015</italic></source>, <volume>9397</volume>, <fpage>38</fpage>–<lpage>52</lpage>.</mixed-citation>
    </ref>
    <ref id="R69">
      <label>69.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>OpenGraphiti: Data Visualization Framework</collab></person-group>. (<year>2014</year>) <ext-link xlink:href="https://www.opengraphiti.com/" ext-link-type="uri">https://www.opengraphiti.com/</ext-link> (<comment>7 March 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R70">
      <label>70.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>GitHub</collab></person-group>. (<year>2014</year>) <article-title>opendns/dataviz</article-title>, <ext-link xlink:href="https://github.com/opendns/dataviz" ext-link-type="uri">https://github.com/opendns/dataviz</ext-link> (<comment>7 March 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R71">
      <label>71.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Bremer</surname><given-names>E.</given-names></string-name> and <collab>Haylyn</collab></person-group>. (<year>2014</year>) <ext-link xlink:href="http://haylyn.io/" ext-link-type="uri">http://haylyn.io/</ext-link> (<comment>6 January 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R72">
      <label>72.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>LodLive – browsing the Web of Data</collab></person-group>. (<year>2012</year>) <ext-link xlink:href="http://lodlive.it/" ext-link-type="uri">http://lodlive.it/</ext-link> (<comment>15 Septemeber 2019, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R73">
      <label>73.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Philipp</surname>  <given-names>H.</given-names></string-name>, <string-name><surname>Steffen</surname>  <given-names>L.</given-names></string-name>, <string-name><surname>Timo</surname>  <given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2010</year>) <article-title>RelFinder – Visual Data Web</article-title>. <ext-link xlink:href="http://www.visualdataweb.org/relfinder.php" ext-link-type="uri">http://www.visualdataweb.org/relfinder.php</ext-link> (<comment>25 February 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R74">
      <label>74.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pavlopoulos</surname><given-names>G.A.</given-names></string-name>, <string-name><surname>O’Donoghue</surname><given-names>S.I.</given-names></string-name>, <string-name><surname>Satagopam</surname><given-names>V.P.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2008</year>) <article-title>Arena3D: visualization of biological networks in 3D</article-title>. <source><italic toggle="yes">BMC Syst. Biol.</italic></source>, <volume>2</volume>, <page-range>104</page-range>.</mixed-citation>
    </ref>
    <ref id="R75">
      <label>75.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Secrier</surname><given-names>M.</given-names></string-name>, <string-name><surname>Pavlopoulos</surname><given-names>G.A.</given-names></string-name>, <string-name><surname>Aerts</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2012</year>) <article-title>Arena3D: visualizing time-driven phenotypic differences in biological systems</article-title>. <source><italic toggle="yes">BMC Bioinformatics</italic></source>, <volume>13</volume>, <page-range>45</page-range>.</mixed-citation>
    </ref>
    <ref id="R76">
      <label>76.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koutrouli</surname><given-names>M.</given-names></string-name>, <string-name><surname>Karatzas</surname><given-names>E.</given-names></string-name>, <string-name><surname>Paez-Espino</surname><given-names>D.</given-names></string-name></person-group>  <etal>et al</etal>. (<year>2020</year>) <article-title>A Guide to Conquer the Biological Network Era Using Graph Theory</article-title>. <source><italic toggle="yes">Front. bioeng. biotechnol.</italic></source>, <volume>8</volume>, <page-range>34</page-range>.</mixed-citation>
    </ref>
    <ref id="R77">
      <label>77.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><collab>Ramanathan Somasundaram</collab></person-group>. (<year>2007</year>) <article-title>ONTOSELF: A 3D ONTOLOGY VISUALIZATION TOOL</article-title>, <source>Master Thesis</source>, <publisher-loc>Oxford, Ohio</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="R78">
      <label>78.</label>
      <mixed-citation publication-type="other"><article-title>The Interactorium - Systems Biology Initiative</article-title>. <ext-link xlink:href="https://www.systemsbiology.org.au/software/the-interactorium/" ext-link-type="uri">https://www.systemsbiology.org.au/software/the-interactorium/</ext-link> (<comment>25 May 2021, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R79">
      <label>79.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Bosca</surname><given-names>A.</given-names></string-name>, <string-name><surname>Bonino</surname><given-names>D.</given-names></string-name> and <string-name><surname>Pellegrino</surname><given-names>P</given-names></string-name></person-group>. (<year>2005</year>) <article-title>OntoSphere: more than a 3D ontology visualization tool</article-title>. <italic toggle="yes">Semantic Web Applications and Perspectives, Proceedings of the 2nd Italian Semantic Web Workshop</italic>, <conf-loc>Trento, Italy</conf-loc>, <conf-date>December 14-16, 2005</conf-date>.</mixed-citation>
    </ref>
    <ref id="R80">
      <label>80.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Reski</surname>  <given-names>N.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Change your Perspective</article-title>. <source>Master’s Thesis</source>, <publisher-name>Institutionen för medieteknik, Linnéuniversitetet, Växjö</publisher-name>. <ext-link xlink:href="https://www-diva-portal-org.proxy-ub.rug.nl/smash/get/diva2:861573/FULLTEXT01.pdf" ext-link-type="uri">https://www-diva-portal-org.proxy-ub.rug.nl/smash/get/diva2:861573/FULLTEXT01.pdf</ext-link> (<comment>10 December 2020, date last accessed</comment>).</mixed-citation>
    </ref>
    <ref id="R81">
      <label>81.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Zainab</surname><given-names>S.S.</given-names></string-name>, <string-name><surname>Saleem</surname><given-names>M.</given-names></string-name>, <string-name><surname>Mehmood</surname><given-names>Q.</given-names></string-name></person-group>  <etal>et al.</etal> (<year>2015</year>) <article-title>FedViz: A Visual Interface for SPARQL Queries Formulation and Execution</article-title>. <source>In VOILA@ISWC 2015</source>. <ext-link xlink:href="http://svn.aksw.org/papers/2015/ISWC-VOILA_FedViz/public.pdf" ext-link-type="uri">http://svn.aksw.org/papers/2015/ISWC-VOILA_FedViz/public.pdf</ext-link>.</mixed-citation>
    </ref>
    <ref id="R82">
      <label>82.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Pattie</surname><given-names>C.</given-names></string-name>, <string-name><surname>Wilson</surname><given-names>B.</given-names></string-name>, <string-name><surname>Mullally</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al.</etal>  <article-title>Investigating individual differences influencing the understanding of statistical concepts in immersive data visualisations</article-title>, <source>Authors’ version from 2020 – unpublished work</source>.</mixed-citation>
    </ref>
  </ref-list>
</back>
