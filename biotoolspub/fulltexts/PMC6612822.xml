<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6612822</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btz394</article-id>
    <article-id pub-id-type="publisher-id">btz394</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Ismb/Eccb 2019 Conference Proceedings</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Bioinformatics of Microbes and Microbiomes</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>TADA: phylogenetic augmentation of microbiome samples enhances phenotype classification</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Sayyari</surname>
          <given-names>Erfan</given-names>
        </name>
        <xref ref-type="aff" rid="btz394-aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kawas</surname>
          <given-names>Ban</given-names>
        </name>
        <xref ref-type="aff" rid="btz394-aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Mirarab</surname>
          <given-names>Siavash</given-names>
        </name>
        <xref ref-type="aff" rid="btz394-aff1">1</xref>
        <xref ref-type="corresp" rid="btz394-cor1"/>
        <!--<email>smirarabbaygi@eng.ucsd.edu</email>-->
      </contrib>
    </contrib-group>
    <aff id="btz394-aff1"><label>1</label>Department of Electrical and Computer Engineering, University of California, San Diego, La Jolla, CA, USA</aff>
    <aff id="btz394-aff2"><label>2</label>IBM Research—Almaden Research Center, San Jose, CA, USA</aff>
    <author-notes>
      <corresp id="btz394-cor1">To whom correspondence should be addressed. <email>smirarabbaygi@eng.ucsd.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2019-07-05">
      <day>05</day>
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>05</day>
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>35</volume>
    <issue>14</issue>
    <fpage>i31</fpage>
    <lpage>i40</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2019. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2019</copyright-year>
      <license license-type="cc-by-nc" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btz394.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Learning associations of traits with the microbial composition of a set of samples is a fundamental goal in microbiome studies. Recently, machine learning methods have been explored for this goal, with some promise. However, in comparison to other fields, microbiome data are high-dimensional and not abundant; leading to a high-dimensional low-sample-size under-determined system. Moreover, microbiome data are often unbalanced and biased. Given such training data, machine learning methods often fail to perform a classification task with sufficient accuracy. Lack of signal is especially problematic when classes are represented in an unbalanced way in the training data; with some classes under-represented. The presence of inter-correlations among subsets of observations further compounds these issues. As a result, machine learning methods have had only limited success in predicting many traits from microbiome. Data augmentation consists of building synthetic samples and adding them to the training data and is a technique that has proved helpful for many machine learning tasks.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>In this paper, we propose a new data augmentation technique for classifying phenotypes based on the microbiome. Our algorithm, called TADA, uses available data and a statistical generative model to create new samples augmenting existing ones, addressing issues of low-sample-size. In generating new samples, TADA takes into account phylogenetic relationships between microbial species. On two real datasets, we show that adding these synthetic samples to the training set improves the accuracy of downstream classification, especially when the training data have an unbalanced representation of classes.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>TADA is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/tada-alg/TADA">https://github.com/tada-alg/TADA</ext-link>.</p>
      </sec>
      <sec id="s4">
        <title>Supplementary information</title>
        <p><xref ref-type="supplementary-material" rid="sup1">Supplementary data</xref> are available at <italic>Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">IBM Research AI through the AI Horizons Network</named-content>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">National Science Foundation</named-content>
          <named-content content-type="funder-identifier">10.13039/100000001</named-content>
        </funding-source>
        <award-id>IIS-1565862</award-id>
        <award-id>III-1845967</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="10"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Understanding the impact of the composition of the microbiome on clinically-relevant traits is a major promise of microbiome profiling <xref rid="btz394-B38" ref-type="bibr">National Research Council (US) Committee on Metagenomics: Challenges and Functional Applications, 2007</xref>] using both 16S (<xref rid="btz394-B20" ref-type="bibr">Gill <italic>et al.</italic>, 2006</xref>) and metagenomic sampling (<xref rid="btz394-B50" ref-type="bibr">Venter <italic>et al.</italic>, 2004</xref>). The goal is to understand how the composition of species, or genes, in a microbial community such as human gut impacts phenotypes of interest such as obesity (e.g. <xref rid="btz394-B49" ref-type="bibr">Turnbaugh <italic>et al.</italic>, 2007</xref>). The relationship between microbial composition and traits, however, is complex and hugely variable, from person to person (<xref rid="btz394-B13" ref-type="bibr">Dave <italic>et al.</italic>, 2012</xref>) and from one time to another (<xref rid="btz394-B10" ref-type="bibr">Caporaso <italic>et al.</italic>, 2011</xref>; <xref rid="btz394-B18" ref-type="bibr">Flores <italic>et al.</italic>, 2014</xref>). As a result, microbial communities have been hard to model (<xref rid="btz394-B52" ref-type="bibr">Waldor <italic>et al.</italic>, 2015</xref>) using traditional sample differentiation methods (<xref rid="btz394-B26" ref-type="bibr">Langille <italic>et al.</italic>, 2013</xref>; <xref rid="btz394-B41" ref-type="bibr">Paulson <italic>et al.</italic>, 2013</xref>).</p>
    <p>Machine learning (ML) methods have proved capable of capturing complex relationships in many fields, such as vision and speech recognition. As a result, researchers have pointed out the potential of ML models to capture complexities of the microbiome (<xref rid="btz394-B24" ref-type="bibr">Knights <italic>et al.</italic>, 2011</xref>). Many researchers (e.g. <xref rid="btz394-B44" ref-type="bibr">Saulnier <italic>et al.</italic>, 2011</xref>; <xref rid="btz394-B46" ref-type="bibr">Statnikov <italic>et al.</italic>, 2013</xref>) have formulated understanding microbiome as a classification task: given is a set of samples, each consisting of a set of sequences from various microorganisms, and each sample is labeled by a trait of interest (e.g. lean or obese); a model is learned to predict these labels and classify unlabeled (new) samples. Some studies have shown promise in achieving an accurate classification of clinically-relevant traits using microbiome (e.g. <xref rid="btz394-B1" ref-type="bibr">Aagaard <italic>et al.</italic>, 2012</xref>; <xref rid="btz394-B8" ref-type="bibr">Beck and Foster, 2014</xref>; <xref rid="btz394-B17" ref-type="bibr">Feng <italic>et al.</italic>, 2015</xref>).</p>
    <p>The number of samples available for training an ML algorithm has tremendous effects on the accuracy of the model. Tuning a large number of parameters of a classifier or regression method using a small dataset can lead to overfitting and poor generalization to new samples. Impacts of overfitting are particularly severe when we have an unbalanced distribution of class labels or hidden confounding factors in training datasets (e.g. <xref rid="btz394-B12" ref-type="bibr">Chawla <italic>et al.</italic>, 2002</xref>, <xref rid="btz394-B11" ref-type="bibr">2010</xref>; <xref rid="btz394-B25" ref-type="bibr">Kubat and Matwin, 1997</xref>).</p>
    <p>The number of microbiome samples, compared to applications like vision and speech recognition, is relatively small. For example, ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2010) involved the classification of 1.2 million high-resolution images into 1000 different classes (<xref rid="btz394-B43" ref-type="bibr">Russakovsky <italic>et al.</italic>, 2015</xref>), whereas, one of the largest microbiome datasets, the American Gut Project (AGP) (<xref rid="btz394-B34" ref-type="bibr">McDonald <italic>et al.</italic>, 2018</xref>), includes 14<italic> </italic>794 samples, has only self-reported labels, and is heterogeneous (e.g. only 1942 samples are omnivores of age between 20 and 80 with no self-reported disease or antibiotic usage). For classifying specific traits, AGP has even fewer samples [e.g. only 262 samples report having inflammatory bowel disease (IBD)]. Moreover, the representation of traits of interest is often not balanced, and the distribution of the labels often is not even close to the larger population (e.g. targeted datasets are often over-represented in the diseased state and short on healthy samples). Biases are further compounded by the natural variability of microbiome and auto-correlation between labels due to hidden or nuisance variables, which abound. These difficulties have led to diminished hope for the generalization of methods (<xref rid="btz394-B48" ref-type="bibr">Sze and Schloss, 2016</xref>).</p>
    <p>Perhaps the ultimate goal should be gathering more (and less biased) labeled samples for training, a task that will progress only slowly, especially given difficulties of combining datasets gathered with various lab protocols (<xref rid="btz394-B27" ref-type="bibr">Leek <italic>et al.</italic>, 2010</xref>; <xref rid="btz394-B53" ref-type="bibr">Weiss <italic>et al.</italic>, 2014</xref>). An alternative that has been explored extensively in recent years by the ML community is data augmentation. The idea is to create artificial labeled samples algorithmically and add them to the training data. For example, two widely-used methods, SMOTE (<xref rid="btz394-B12" ref-type="bibr">Chawla <italic>et al.</italic>, 2002</xref>) and ADASYN (<xref rid="btz394-B22" ref-type="bibr">He <italic>et al.</italic>, 2008</xref>) seek to reduce biases introduced by unbalanced distributions of labels using a <italic>k</italic>-NN clustering of samples and combining points in the same cluster. Beyond these generic methods, which do not seek to capture domain knowledge, augmentation has the potential to combine the power of black-box ML models and biologically-motivated generative statistical models.</p>
    <p>In this paper, we propose a new data augmentation technique for microbiome data, called Tree-based Associative Data Augmentation (TADA). The main ideas behind TADA are 2-fold. (i) Each observed sample captures the underlying microbiome only imperfectly, and hence, a variation of the sample could have easily been observed, (ii) such variations are constrained by the phylogenetic relationships between species (<xref rid="btz394-B32" ref-type="bibr">Matsen, 2015</xref>), which underlie the sequence similarity and microbial diversity (<xref rid="btz394-B40" ref-type="bibr">O’Dwyer <italic>et al.</italic>, 2012</xref>; <xref rid="btz394-B51" ref-type="bibr">von Mering <italic>et al.</italic>, 2007</xref>). Thus, TADA generates new samples while considering the evolutionary relationships between organisms. Furthermore, we do not stop at just increasing the number of samples. As we will show, it is crucial to deal with unbalances and biases in the training data. In deciding what samples to add, TADA can also remove unbalances in the data with respect to both observed and hidden variables (which we seek to approximate using clustering). We test TADA on two datasets with various biases added to the training dataset. We show that two leading ML models (random forests and neural networks) fail to perform well on unbalanced and biased samples. We also show that data augmentation improves the accuracy, marginally but meaningfully for balanced datasets and dramatically in the presence of unbalanced training sets.</p>
  </sec>
  <sec>
    <title>2 The TADA method</title>
    <sec>
      <title>2.1 Background and notations</title>
      <p>The training data used in microbiome classification is an operational-taxonomic-unit (OTU) table X. The rows of the table correspond to a set of <italic>m</italic> samples, often one per individual, <inline-formula id="IE1"><mml:math id="IM1"><mml:mi mathvariant="script">S</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:math></inline-formula> and the columns correspond to features. Features can be defined in various ways, but for simplicity, we focus on a specific form. Our features are a set of <italic>n</italic> OTUs (e.g. representing species)<inline-formula id="IE2"><mml:math id="IM2"><mml:mo> </mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:math></inline-formula>. Each cell of the matrix gives the number of times an OTU is observed in a sample. The counts in each row can also be normalized so that they add up to one. In addition to the OTU table, we need a class label <italic>y<sub>i</sub></italic> for each sample <italic>s<sub>i</sub></italic>. The class labels correspond to phenotypes (e.g. healthy versus diseased or lean versus obese) that we seek to classify using the microbiome.</p>
      <p>The OTUs have a corresponding sequence, for example from the marker genes like 16S rRNA. These sequences may be obtained using a number of approaches, including the traditional OTU picking methods (<xref rid="btz394-B15" ref-type="bibr">Edgar, 2010</xref>; <xref rid="btz394-B45" ref-type="bibr">Schloss and Handelsman, 2005</xref>) or sub-operational-taxonomic-unit methods (<xref rid="btz394-B5" ref-type="bibr">Amir <italic>et al.</italic>, 2017</xref>; <xref rid="btz394-B9" ref-type="bibr">Callahan <italic>et al.</italic>, 2016</xref>; <xref rid="btz394-B16" ref-type="bibr">Edgar, 2016</xref>). Depending on the method, the exact meaning of OTUs changes; however, they always correspond (at least in approximation) to microorganisms that constitute the sample.</p>
    </sec>
    <sec>
      <title>2.2 Generative model used in TADA</title>
      <p>Data augmentation seeks to add to the training set new samples that could have been seen but are not seen. TADA achieves this using a generative model to create synthetic samples distributed around existing samples. TADA models two types of variations.
<list list-type="bullet"><list-item><p><italic>True variation (TV)</italic>. From one individual to another, even among those with the same phenotype, the true proportions of different OTUs in the microbiome change. These variations may be due to confounding factors (i.e. hidden variables) or natural biological variation among people. Moreover, the microbial composition for each person may also change through time. Thus, samples have true biological variation.</p></list-item><list-item><p><italic>Sampling variation (SV).</italic> Environmental sequencing takes a random (but not necessarily uniformly random) subsample of the true diversity, creating additional variation around the true proportions. Moreover, sequencing adds errors and ambiguity that further increase variation.</p></list-item></list></p>
      <p>Of the two forms of variation, true variation is much harder to model statistically. Confounding factors are mostly unknown as are the source of natural or temporal variations. However, a major source of inter-correlation, the phylogenetic structure, <italic>can</italic> be inferred and modeled.</p>
      <p><italic>Phylogenetic structure.</italic> Microorganisms that make up a sample are all descendants from a common ancestor, as captured by their phylogenetic tree. The shared evolutionary history creates a dependence between OTUs, and a phylogenetic tree can represent the relationships (in its topology) as well as the distance between the species. Close phylogenetic relationships between OTUs corresponds to closeness in the sequence space and perhaps also in functional roles. Both forms of variation are likely influenced by the phylogeny. True variation can be phylogenetic because phylogenetically similar organisms may interchange easily, though we note that this is far from a universal rule; strain-variation may have a large impact on the function. Sampling variation is impacted because algorithms for creating OTU tables are prone to merge or confuse OTUs that are close phylogenetically.</p>
      <p>TADA uses an inferred binary phylogenetic tree (details are given in Section 3.4), called<inline-formula id="IE3"><mml:math id="IM3"><mml:mi mathvariant="script"> </mml:mi><mml:mi mathvariant="script">T</mml:mi></mml:math></inline-formula>, with leaves labeled by OTUs <inline-formula id="IE4"><mml:math id="IM4"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="btz394-F1">Fig. 1a</xref>). We index internal nodes of <inline-formula id="IE5"><mml:math id="IM5"><mml:mi mathvariant="script">T</mml:mi></mml:math></inline-formula> from 1 (for the root) to <italic>n </italic>−<italic> </italic>1 and refer to the length of the edge above node <italic>u</italic> by <italic>t<sub>u</sub></italic>. Using a simple <italic>O</italic>(<italic>n</italic>) algorithm (Supplementary Algorithm S1), we compute <italic>d<sub>u</sub></italic>: the average length of the path from each leaf under the left child of <italic>u</italic> to each leaf under the right child of <italic>u</italic>.
</p>
      <fig id="btz394-F1" orientation="portrait" position="float">
        <label>Fig. 1.</label>
        <caption>
          <p>(<bold>a</bold>) A phylogeny <inline-formula id="IE6"><mml:math id="IM6"><mml:mi mathvariant="script">T</mml:mi></mml:math></inline-formula> with branch lengths (<italic>t<sub>u</sub></italic>), OTUs at leaves (<italic>o<sub>i</sub></italic>) and internal node indices. (<bold>b</bold>) The hierarchical graphical model used to generate new samples. (<bold>c</bold>) The augmentation procedure. First, each sample is mapped to the phylogeny, then we estimate parameters of the model for each sample <italic>s<sub>i</sub></italic> (or a collection of samples; see <xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S2</xref>), and then generate new samples using the generative model. The augmented samples are concatenated with the original samples for training the classifier (e.g. RF or NN)</p>
        </caption>
        <graphic xlink:href="btz394f1"/>
      </fig>
      <sec>
        <title>2.2.1 Generative model: the base model</title>
        <p>We design a hierarchical generative model to capture both sources of variation and the phylogenetic auto-correlation. The model has three sets of parameters: (i) the phylogeny, <inline-formula id="IE7"><mml:math id="IM7"><mml:mi mathvariant="script">T</mml:mi></mml:math></inline-formula>, and its branch lengths (and, thus, <italic>d<sub>u</sub>’</italic>s), with the two nodes below each node <italic>u</italic> arbitrarily labeled as <italic>left</italic> (<italic>l</italic>) and <italic>right</italic> (<italic>r</italic>), (ii) a set<inline-formula id="IE8"><mml:math id="IM8"><mml:mi mathvariant="script"> </mml:mi><mml:mi mathvariant="script">M</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">μ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">μ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>}</mml:mo><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">μ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>, each corresponding to an internal node of the phylogenetic tree, (iii) the total sequence count <italic>N</italic>. In addition to these parameters, we define for each node, a value <inline-formula id="IE9"><mml:math id="IM9"><mml:msub><mml:mrow><mml:mo>ν</mml:mo></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> where <italic>f</italic> can be any monotonically increasing function.</p>
        <p>Our generative hierarchical model (<xref ref-type="fig" rid="btz394-F1">Fig. 1b</xref>) is defined recursively, starting at the root and traversing the tree top–down. Algorithm 1 shows this model generates <italic>q</italic> individuals and <italic>k</italic> new samples for each individual (<italic>k </italic>×<italic> q</italic> in total), each with <italic>N</italic> sequences. The true variation is modeled using a Beta distribution and the sample variation using a Binomial distribution. We use the μ,ν parameterization of the Beta distribution (as opposed to the standard α, β parameterization). For each node <italic>u</italic>, we have the parameter μ<sub><italic>u</italic></sub>, which gives the population-wide portion of sequences under the node <italic>u</italic> that fall under the left subtree of <italic>u</italic>. A draw from the Beta distribution gives us<inline-formula id="IE10"><mml:math id="IM10"><mml:mi mathvariant="normal"> </mml:mi><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>: the true portion of sequences that go to the left subtree in the underlying microbiome. Then, a draw from the Binomial distribution gives the actual observed count and models the variation due to sampling (sequencing) around the true proportion <inline-formula id="IE11"><mml:math id="IM11"><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>.</p>
        <p>In this model, the true variance is inversely proportional to the square root of phylogenetic distance. In the parameterization of the Beta distribution used here, the mean is μ and the variance is<inline-formula id="IE13"><mml:math id="IM12"><mml:mi mathvariant="normal"> </mml:mi><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">μ</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi mathvariant="normal">μ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">ν</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:math></inline-formula>. By setting the ν parameter of Beta to a monotonically increasing function of <italic>d<sub>u</sub></italic>, we make sure that the variance increases closer to the tips of the tree (where <italic>d<sub>u</sub></italic> is small), and decreases toward the root (where <italic>d<sub>u</sub></italic> is high). The choice of the exact function <italic>f</italic> (see Section 3.4) is arbitrary. However, the fact that variance should be higher closer to tips has a biological justification. Closer to the leaves of the tree, microbial organisms become more similar and therefore more likely to be able to replace each other in an environment or be confused with each other. Conversely, the microbial composition becomes more stable close to the root of the tree.
</p>
        <p>
          <boxed-text id="btz394-BOX1" position="float" orientation="portrait">
            <label>Algorithm 1</label>
            <caption>
              <p> TADA sample generation procedure</p>
            </caption>
            <p>1: <bold>for</bold> individual <inline-formula id="IE14"><mml:math id="IM13"><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:mi>q</mml:mi></mml:math></inline-formula><bold>do</bold></p>
            <p>2:   <bold>for</bold> node <italic>u</italic> in preorder traversal of <inline-formula id="IE15"><mml:math id="IM14"><mml:mi mathvariant="script">T</mml:mi></mml:math></inline-formula><bold>do</bold></p>
            <p>3:    Draw <inline-formula id="IE16"><mml:math id="IM15"><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo>∼</mml:mo><mml:mi mathvariant="normal">Beta</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">μ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">ν</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula></p>
            <p>4:   <bold>for</bold><inline-formula id="IE17"><mml:math id="IM16"><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>j</mml:mi><mml:mo>≤</mml:mo><mml:mi>k</mml:mi></mml:math></inline-formula><bold>do</bold></p>
            <p>5:    <inline-formula id="IE18"><mml:math id="IM17"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mi>N</mml:mi></mml:math></inline-formula> % Index 1 refers to the root node</p>
            <p>6:    <bold>for</bold> internal node <italic>u</italic> with children <italic>l</italic> and <italic>r</italic> in preorder traversal <bold>do</bold></p>
            <p>7:     Draw <inline-formula id="IE19"><mml:math id="IM18"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi mathvariant="normal">Binomial</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula></p>
            <p>8:     <inline-formula id="IE20"><mml:math id="IM19"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></p>
            <p>9:    Output <inline-formula id="IE21"><mml:math id="IM20"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> as a new sample and normalize if needed.</p>
          </boxed-text>
        </p>
      </sec>
      <sec>
        <title>2.2.2 Generative model: mixtures</title>
        <p>The model described above is limited in a fundamental way: it assumes all samples are generated from the same underlying distribution. Therefore, it completely ignores the fact that individuals belong to several classes (the identification of which is the goal) and that within each class, confounding factors may create further structure among samples. For example, we may have healthy and diseased samples for our main classes, and for each of those, samples may be further differentiated based on age, gender, weight or other factors (which, may not be known). Thus, the phenotype structure of samples is not modeled.</p>
        <p>To capture the phenotype structure, we use a mixture model. The population is assumed to be divided into clusters, each with its own <inline-formula id="IE22"><mml:math id="IM21"><mml:mi mathvariant="script">M</mml:mi></mml:math></inline-formula> parameters, but all sharing the same phylogeny. Clusters can correspond to class labels, confounding factors or a mixture of the two. In the generative process, each sample is first assigned to a cluster, according to cluster probabilities, and then the procedure described above is followed.</p>
      </sec>
    </sec>
    <sec>
      <title>2.3 Data augmentation procedure</title>
      <p>Assuming the training data come from our generative model, we can design parameter estimators and use the estimated parameters to generate new data. In fact, a model-based approach (coupled with the mixture model), in principle can also infer the class labels. However, on typical microbiome training datasets, the total number of mixture components is likely large, and the model has a large number of parameters. Thus, parameter estimation using these complex models will be underpowered. Moreover, despite a large number of parameters, the model does not come close to capturing all the biological complexity of microbiome. Thus, instead of using the generative model for inference, we use it only as a tool for data augmentation for training ML models.</p>
      <p>Based on the hierarchical model, we design two versions of TADA, which vary in their ambition, ranging from capturing only sampling variation to capturing both sources of variation and confounding factors. The more ambitious versions include more parameters, and this reliance on more parameters makes them vulnerable when applied to limited datasets.</p>
      <p><italic>TADA-SV</italic>. This version only captures sampling variation and has a single user setting: a number <italic>k</italic>. For each training sample <italic>s<sub>i</sub></italic>, we first estimate <inline-formula id="IE23"><mml:math id="IM22"><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> in our training set <italic>independently</italic> from other samples (assuming samples are unlinked). Then, for each sample, <italic>k</italic> new samples are generated and added to the training set using the fixed <inline-formula id="IE24"><mml:math id="IM23"><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> following Algorithm 1 (setting <italic>q </italic>=<italic> </italic>1 and starting in line 5). Thus, this method is only drawing from the Binomial component of our Hierarchical model and ignores the rest. To estimate <inline-formula id="IE25"><mml:math id="IM24"><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> from a single sample <italic>s<sub>i</sub></italic>, we use the total count of sequences that fall to the left of the node <italic>u</italic> in <italic>s<sub>i</sub></italic>, normalized by the total count of sequences below <italic>u</italic>. As proved in the Supplementary Lemma S1, this estimator gives the joint ML estimate for all <inline-formula id="IE26"><mml:math id="IM25"><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> values (treated as parameters of the binomial) over the entire tree.</p>
      <p><italic>TADA-TVSV-C</italic>. This version captures both sampling and true variation and optionally also confounding factors. The method has three user settings: <italic>k</italic>, <italic>q</italic> and <italic>C</italic>. We first cluster samples <inline-formula id="IE27"><mml:math id="IM26"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> into <italic>C</italic> groups <italic>per classification label</italic> based on the training data <bold>X</bold> using any clustering method of choice (see our default choice in Section 3.4). These clusters correspond to components of the mixture model we described before; note that instead of using a complex parameter-rich model-based inference of mixture components, we use a clustering method to approximate the components. The hope is that the clustering based on <bold>X</bold> captures the hidden phenotype structure, at least partially. The choice of <italic>C</italic> controls the level of complexity and therefore the number of parameters. For example, acknowledging the difficulty of finding the phenotype structure, we explore the extreme setting of <italic>C </italic>=<italic> m</italic> where each sample in our training set belongs to its cluster and therefore is unlinked from others, just like SV. We also explore other settings of <italic>C</italic>, including <italic>C </italic>=<italic> </italic>1.</p>
      <p>After clustering, we first estimate <inline-formula id="IE28"><mml:math id="IM27"><mml:mi mathvariant="script">M</mml:mi></mml:math></inline-formula> parameters <italic>per each cluster</italic> using a method of moments. The estimator, as shown in Supplementary Lemma S2, simplifies to computing the sum of counts on the left child of each node <italic>u</italic> across all samples of the cluster, normalized by the sum of the counts under the node <italic>u</italic>. Then, for each cluster, we generate <italic>q</italic> new individuals and <italic>k</italic> new samples per individual (thus, <italic>k </italic>×<italic> q</italic> in total). To do so, we follow the generative procedure given in Algorithm 1.</p>
    </sec>
    <sec>
      <title>2.4 Balancing</title>
      <p>So far, we have generated a fixed number of new samples per input training sample. However, by generating a <italic>different</italic> number of samples per input sample, we can use augmentation for balancing (or otherwise adjusting) our input training set in terms of the distribution of labels. As we will show, the lack of balance between representations from different phenotype classes (e.g. the training labels) can degrade the accuracy of ML methods. TADA, therefore, can also be run with balancing (Supplementary Fig. S2). In this mode, training data are first divided into several groups; these groups can be based on classification labels, the result of clustering training points or a combination. We then choose the number of extra samples generated per sample (e.g. <italic>k</italic> and <italic>q</italic>) such that all groups have the same number of samples after augmentation. We will test two modes.
<list list-type="bullet"><list-item><p>TADA-Balance adds exactly as many new samples as necessary (and not any more) so that all groups have the same total number of samples.</p></list-item><list-item><p>TADA-Balance++ not only makes all groups balanced in size but also increases the total number of samples for all groups, so that the largest group has <italic>q</italic> times more samples than before augmentation.</p></list-item></list></p>
    </sec>
  </sec>
  <sec>
    <title>3 Experimental setup</title>
    <sec>
      <title>3.1 Datasets</title>
      <p>We use two datasets, both based on 16S profiling of gut microbiome.</p>
      <p><italic>Gevers</italic>. As our main dataset, we use a dataset by <xref rid="btz394-B19" ref-type="bibr">Gevers <italic>et al.</italic> (2014)</xref> [publicly available on Qiita (<xref rid="btz394-B21" ref-type="bibr">Gonzalez <italic>et al.</italic>, 2018</xref>); study ID 1939], which the authors put together to study the impact of the microbiome on the IBD. This study has 1359 samples, has been gathered in a clinical setting, is carefully curated, and has reliable class labels. We filtered out samples from people on antibiotics, or with &lt;10<italic> </italic>000 16S sequences. Before running our experiment, we also removed 9 outliers and any OTUs with total counts across all samples below 3. This leaves us with 647 diseased samples and 243 healthy samples, gathered using either biopsy or stool. <xref rid="btz394-B19" ref-type="bibr">Gevers <italic>et al.</italic> (2014)</xref> were able to find a clear indication that IBD changes the microbiome composition, and thus, ML methods should be able to achieve reasonable classification accuracy on this dataset.</p>
      <p><italic>BMI</italic>. In addition, we use the AGP (<xref rid="btz394-B34" ref-type="bibr">McDonald <italic>et al.</italic>, 2018</xref>). This dataset has only self-reported labels and is gathered by crowdsourcing instead of a clinical setting. Thus, it is less curated than the Gevers dataset, though authors have taken several quality control steps. With an understanding of the shortcomings, we use the AGP data to test our method on a phenotype other than IBD. We classified the self-reported body mass index (BMI) phenotype categorized into 1360 lean versus 582 overweight samples (cutoff at BMI: 25). Similar to Gevers dataset, we further filtered this dataset to control for many factors that might affect microbiome composition. These factors include diet (we keep omnivore samples), ethnicity (Caucasian), country (USA), disease (healthy), antibiotics (no antibiotic usage in the past year) and age (between 20 and 70). We also filtered samples with &lt;1000 de-noised reads, one outlier and we removed OTUs with total counts across all samples below 4.</p>
    </sec>
    <sec>
      <title>3.2 Experiments</title>
      <p><italic>E1</italic>. On both Gevers and AGP datasets, we compare TADA, in its two settings, against ADASYN and SMOTE, and the baseline approach where no augmentation is performed. In this experiment, we use all the data for training and testing in a cross-validation setting (see Section 3.3) performed using the held-out original samples.</p>
      <p><italic>E2</italic>. We next test the impact of balancing by generating unbalanced training data. We created datasets such that 1/10, 1/5 or 1/3 of both the training and testing data are from healthy/overweight individuals. We created two versions of this unbalanced dataset. In the first version (E2-fix), we used a fixed number (243 for IBD and 582 for BMI) training samples for all three ratios to test the impact of the ratio without changing the training size. Here, the testing sets are chosen to match the ratio in the training set but have a larger total sum (the maximum possible in each case). We also created a version (E2-max) only for the IBD dataset with the maximum possible training size for each ratio. To do this, we removed the minimum possible number of samples from healthy (for 1/10 and 1/5 ratios) or diseased (for 1/3) so that we obtain the desired ratios; this leaves us with 574 training samples for 1/10, 646 for 1/5 and 587 for 1/3. Once again, the testing sets are chosen to match the training set in ratio. E2-max enables us to make sure results on E2-fix hold if training datasets are as large as possible. Here, in addition to no augmentation, we compare TADA-Balance(++) to ADASYN, SMOTE and a simple balancing strategy that <italic>reduces</italic> the number of diseased samples to match the healthy count by random down-sampling.</p>
      <p><italic>E3</italic>. While E2 is to test lack of balance, E3 is concerned with biases in the composition of classification labels in the training dataset. In E3, we use the 1/5 dataset of E2-fix for training, but for the testing set, we choose samples such that 1/5, 1/2 or 4/5 are healthy (achieved by randomly removing diseased cases until the desired ratio is achieved). Thus, the last two cases have a different composition of labels between training and testing datasets.</p>
    </sec>
    <sec>
      <title>3.3 Evaluation procedure</title>
      <p>For measuring classification accuracy, we rely on the area under curve (AUC) of receiver operating characteristic. The AUC measure is computed by exploring different cutoffs for the threshold used internally in each classification method, hence exploring the tradeoff between precision and recall. AUC is the standard method used for measuring the accuracy of ML classification because it does not depend on arbitrary sensitivity/specificity tradeoffs.</p>
      <p>All of our tests are based on a cross-validation strategy, repeated several times to get a total of 20 evaluations of AUC. We report the mean and standard error of AUC across the 20 replicates. In E1 and E2-max, we use 5-fold cross-validation, repeated four times. In E2-fix and E3, we use 3-fold validation for the 1/3 setting, 5-fold validation for 1/5 and 10-fold validation for 1/10, each repeated enough to get 20 replicates. The augmented samples are only added to the training data, and testing is done using the held-out samples from the original datasets.</p>
    </sec>
    <sec>
      <title>3.4 Method details</title>
      <p><italic>OTU and phylogeny</italic>. We use Deblur (<xref rid="btz394-B5" ref-type="bibr">Amir <italic>et al.</italic>, 2017</xref>) to extract error-corrected (de-noised) sequences from each sample and take each resulting sequence as an OTU. We then use SEPP (<xref rid="btz394-B23" ref-type="bibr">Janssen <italic>et al.</italic>, 2018</xref>; <xref rid="btz394-B36" ref-type="bibr">Mirarab <italic>et al.</italic>, 2012</xref>) to insert OTUs onto a backbone phylogeny of GreenGenes (<xref rid="btz394-B14" ref-type="bibr">DeSantis <italic>et al.</italic>, 2006</xref>); removing the backbone sequences and randomly resolving the remaining polytomies gives us a binary tree on the OTUs observed in the samples. We use this tree as<inline-formula id="IE29"><mml:math id="IM28"><mml:mi mathvariant="script"> </mml:mi><mml:mi mathvariant="script">T</mml:mi></mml:math></inline-formula>.</p>
      <p><italic>TADA</italic>. We implemented TADA in Python using DendroPy (<xref rid="btz394-B47" ref-type="bibr">Sukumaran and Holder, 2010</xref>) for manipulating phylogenies, biom-format (<xref rid="btz394-B33" ref-type="bibr">McDonald <italic>et al.</italic>, 2012</xref>) for processing OTU tables, scikit-learn (<xref rid="btz394-B42" ref-type="bibr">Pedregosa <italic>et al.</italic>, 2011</xref>) for ML methods, and scikit-bio for computing distances between microbiome samples. In all the analyses, we use<inline-formula id="IE30"><mml:math id="IM29"><mml:mo> </mml:mo><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mn>100</mml:mn><mml:msqrt><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:msqrt></mml:math></inline-formula>. The choice of the square root is arbitrary but is motivated by wanting a slower than linear reduction in variance closer to the tips (where <italic>d<sub>u</sub> </italic>&lt;<italic> </italic>1); the constant 100 ensures the variance of Beta is not extremely high and had little impact on results in our initial tests on a different dataset. To cluster samples, we use the <italic>k</italic>-means method (<xref rid="btz394-B6" ref-type="bibr">Arthur and Vassilvitskii, 2007</xref>) applied to the Bray–Curtis (<xref rid="btz394-B35" ref-type="bibr">McMurdie and Holmes, 2014</xref>) distances between samples computed from the normalized matrix <bold>X</bold>. In all analyses, unless specified, we set <italic>k </italic>=<italic> </italic>5 for TADA-SV and <italic>k </italic>=<italic> </italic>1, <italic>q </italic>=<italic> </italic>5 for TADA-TVSV-<italic>C</italic> (our initial experiments showed marginal improvements with increased <italic>k</italic> or <italic>q</italic>; see Supplementary Fig. S3). For TADA-Balance++, we set <italic>k </italic>=<italic> </italic>50 for TADA-SV and <italic>k </italic>=<italic> </italic>1 and <italic>q </italic>=<italic> </italic>50 for TADA-TVSV-<italic>C</italic>. For TVSV, we will explore five settings of <italic>C</italic>, the number of clusters: 1, 4, 8, 40 and <italic>m</italic>. In order to avoid zero counts, we add the pseudocount 5/<italic>n</italic> to the count of all OTUs for all samples (<italic>n </italic>≈<italic> </italic>10<sup>4</sup> for IBD and ≈ 2 × 10<sup>4</sup> for BMI).</p>
      <p><italic>ADASYN/SMOTE</italic>. We use ADASYN and SMOTE implemented in the imbalanced-learn package (ver. 0.4.3) (<xref rid="btz394-B28" ref-type="bibr">Lemaître <italic>et al.</italic>, 2017</xref>). We use the normalized counts of OTUs (so that values in each row of <bold>X</bold> add up to 1) as inputs. We use <italic>k </italic>=<italic> </italic>5 (default value) for the <italic>k</italic>-nearest neighbor clustering step of these methods. Both methods allow us to set the number of samples we want to generate from each class.</p>
      <p><italic>ML</italic>. We use two ML methods: random forests (RF) (<xref rid="btz394-B29" ref-type="bibr">Breiman, 2001</xref>) and neural networks (NN), both as implemented in the scikit-learn package (<xref rid="btz394-B42" ref-type="bibr">Pedregosa <italic>et al.</italic>, 2011</xref>) (ver. 0.20). We use RF because of its superior performance on previous studies of microbiome (e.g. <xref rid="btz394-B46" ref-type="bibr">Statnikov <italic>et al.</italic>, 2013</xref>). We set the number of trees for RF to 2000 and use default options otherwise. For NN, we use Multi-layer Perceptron classifier (MLPC). Our MLPC had two layers with dimensions 2000 and 1000, respectively, with an early stopping rule. For the other parameters of MLPC, we used the default options. We use the normalized counts of OTUs as input features.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Results</title>
    <sec>
      <title>4.1 E1: complete datasets</title>
      <p>We start with the E1 experiment where all the data are used (<xref ref-type="fig" rid="btz394-F2">Fig. 2</xref>).
</p>
      <fig id="btz394-F2" orientation="portrait" position="float">
        <label>Fig. 2.</label>
        <caption>
          <p>Results on E1. Area under curve (AUC) is shown for both neural networks (NN) and random forest (RF) classifiers and on both Gevers IBD dataset and AGP BMI dataset. We compare training on original dataset with no augmentation, SMOTE, ADASYN and using both SV and TVSV versions of TADA. For TVSV-<italic>C</italic>, we set the number of clusters, <italic>C</italic>, to 1, 4, 8, 40 or <italic>m</italic> (number of samples). We used ADASYN and SMOTE with their default settings. We show mean (dots) and standard error over 20 replicates. For TADA-SV, we show both <italic>k </italic>=<italic> </italic>5 and <italic>k </italic>=<italic> </italic>50, and for TADA-TVSV-<italic>m</italic>, we show both <italic>q </italic>=<italic> </italic>5 and <italic>q </italic>=<italic> </italic>50 with <italic>k </italic>=<italic> </italic>1; see <xref ref-type="supplementary-material" rid="sup1">Supplementary Figure S3</xref> for other <italic>q</italic> and <italic>k</italic></p>
        </caption>
        <graphic xlink:href="btz394f2"/>
      </fig>
      <p>On the Gevers IBD dataset, the accuracy of ML methods, as measured by AUC, is reasonably high (mean AUC<italic> </italic>&gt;<italic> </italic>0.8, both for NN and RF) even without augmentation. Nevertheless, TADA is able to increase the mean accuracy for both NN and RF. For example, for RF, the AUC improves from 0.857 to 0.890 with TADA-TVSV-<italic>m</italic> (<italic>q </italic>=<italic> </italic>50) and the difference is statistically significant according to a paired <italic>t</italic>-test (<inline-formula id="IE31"><mml:math id="IM30"><mml:mi mathvariant="italic">P</mml:mi><mml:mo>≪</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>). This improvement, while not large in magnitude, corresponds to a 23% reduction in the gap compared to the ideal AUC<italic> </italic>=<italic> </italic>1 and therefore is substantial. In contrast, ADASYN and SMOTE result in much smaller improvements (mean AUC<italic> </italic>&lt;<italic> </italic>0.87); these improvements are not statistically significant for ADASYN (<italic>P </italic>=<italic> </italic>0.15) but are significant for SMOTE (<italic>P </italic>=<italic> </italic>0.0003).</p>
      <p>For BMI classification using the AGP dataset, the AUC was generally low in the absence of augmentation (mean &lt;0.72 for both methods), perhaps reflecting the heterogeneous nature of the AGP dataset or the difficulty of classifying BMI into two categories based on the microbiome. Data augmentation using TADA-TVSV increases the accuracy for RF; for example, the AUC is increased to 0.73 using TADA-TVSV-<italic>m</italic>, and this improvement is statistically significant (<inline-formula id="IE32"><mml:math id="IM31"><mml:mi mathvariant="italic">P</mml:mi><mml:mo> </mml:mo><mml:mo>≪</mml:mo><mml:mo> </mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>). Here, ADASYN <italic>reduces</italic> accuracy while SMOTE helps accuracy insignificantly (<italic>P </italic>=<italic> </italic>0.18) and not as much as TADA. Unlike RF, NN is not helped by TADA-SV, and TADA-TVSV-<italic>m</italic> gives only a statistically insignificant improvement (<italic>P </italic>=<italic> </italic>0.35). Both ADASYN and SMOTE reduce the accuracy.</p>
      <p>Comparing different numbers of clusters (<italic>C</italic>) for TVSV-<italic>C</italic>, we observe an interesting pattern. Increasing the number of clusters improves AUC consistently, and the trend is especially apparent for RF. The highest accuracy is obtained by either TVSV-40 or TVSV-<italic>m</italic> where a single sample (for <italic>m</italic>) or a handful of samples (for 40) constitute a cluster. Based on these results, we focus only on TVSV-<italic>m</italic> for E2 and E3. Interestingly, the accuracy of the simpler model, SV, is very close to TVSV, except perhaps on the BMI dataset with NN. Finally, increasing <italic>k</italic> or <italic>q</italic> and also using <italic>k </italic>=<italic> q </italic>=<italic> </italic>5 tends to improve accuracy, albeit marginally (Supplementary Fig. S3).</p>
    </sec>
    <sec>
      <title>4.2 E2: unbalanced class labels</title>
      <p>The power of TADA becomes evident when the classes have an unbalanced representation (<xref ref-type="fig" rid="btz394-F3">Fig. 3</xref>). By making the representation of the two labels unbalanced, we observe that the accuracy of ML methods degrades quickly. In E2-fix, we see a sharp drop in AUC of both ML methods as the level of unbalance increases (<xref ref-type="fig" rid="btz394-F3">Fig. 3</xref>). For example, on the IBD dataset, RF with no augmentation goes from AUC<italic> </italic>=<italic> </italic>0.8 with 1/3 healthy samples to AUC<italic> </italic>=<italic> </italic>0.7 when 1/10 are healthy. Similarly, on BMI, AUC goes down from 0.66 in the 1/3 case to AUC<italic> </italic>=<italic> </italic>0.54 when 1/10 are overweight. Simply down-sampling the number of over-represented class to match the other label by random removals <italic>increases</italic> AUC despite training from a smaller dataset. This improved accuracy further underscores the detrimental impact of a lack of balance.
</p>
      <fig id="btz394-F3" orientation="portrait" position="float">
        <label>Fig. 3.</label>
        <caption>
          <p>Results on the E2-fix dataset. Training dataset is randomly subsampled to create unbalance: healthy (for IBD) and overweight (for BMI) samples constitute 1/10 (10-versus-90), 1/5 (20-versus-80) or 1/3 (33-versus-66) of the samples for <italic>both</italic> the training and testing sets. We compare AUC on the original training set (no augmentation); the over-represented class down-sampled to match the number of under-represented class (down-sampling); and, augmentation using SMOTE, ADASYN and TADA. Methods are run in two ways: TADA-Balance just adds samples to the healthy class to balance labels; TADA-Balance++ adds both healthy and unhealthy samples to make them balanced <italic>and</italic> to increase the total number of samples by 50×</p>
        </caption>
        <graphic xlink:href="btz394f3"/>
      </fig>
      <p>Large improvements in AUC are obtained when we use TADA to balance the representation from the two groups. For example, on the IBD dataset, the AUC of RF with TADA-SV-Balance is &gt;0.8 even when the original training data (i.e. before augmentation) has only 1/10 healthy individuals. Similar levels of improvement are observed for BMI. Across both datasets, improvements in accuracy can be as large as 0.11 points for RF and 0.29 points for NN. Thus, TADA-Balance can largely erase the negative impacts of unbalance in the original training dataset. Like E1, here, TADA-SV and TVSV-<italic>m</italic> perform similarly.</p>
      <p>More interestingly, using TADA-Balance++ results in additional improvements beyond TADA-Balance. For example, for IBD, the AUC in the 1/5 healthy case goes from 0.81 with TADA-SV-Balance to 0.83 with TADA-SV-Balance++ with RF (statistically significant: <italic>P </italic>=<italic> </italic>0.00004). The improvements of Balance++ over Balance are consistent with improvements of TADA over no augmentation observed in E1.</p>
      <p>The two standard methods, SMOTE and ADASYN, have mixed performance. We start with RF on the IBD dataset. With the Balance version, both methods improve AUC substantially only for the 1/10 healthy case but they fail to outperform down-sampling. In the 1/5 healthy case, they result in small improvements and in the 1/3 healthy case they <italic>reduce</italic> AUC compared to no augmentation. The Balance++ versions of both methods, however, consistently improve AUC. Nevertheless, with 1/10 or 1/5 healthy, TADA-SV outperforms both methods (<italic>P </italic>&lt;<italic> </italic>0.007 in all four comparisons) whereas with 1/3, TADA-SV and both methods are statistically indistinguishable (<italic>P </italic>&gt;<italic> </italic>0.16 in both comparisons). Similar patterns are observed for BMI with RF. With NN (which has much lower AUC than RF) SMOTE, ADASYN and TADA have similar accuracy in all conditions.</p>
      <p>The positive impact of balancing on E2-fix is not merely due to its small training set. On E2-max, which has roughly double the training set size of E2-fix, TADA continues to improve accuracy over no augmentation and other methods, especially for 1/10 and 1/5 levels of unbalance (<xref ref-type="fig" rid="btz394-F4">Fig. 4</xref>). Compared to E2-fix, AUC is improved for all methods in E2-max, as expected due to the larger training set. Here, down-sampling and SMOTE/ADASYN-Balance stop increasing accuracy for RF.
</p>
      <fig id="btz394-F4" orientation="portrait" position="float">
        <label>Fig. 4.</label>
        <caption>
          <p>Results on the E2-max dataset. Settings similar to <xref ref-type="fig" rid="btz394-F3">Figure 3</xref></p>
        </caption>
        <graphic xlink:href="btz394f4"/>
      </fig>
      <p>Note that before augmentation, the composition of class labels in the testing set matches that of the training set. Thus, the reductions in accuracy for unbalanced data without augmentation are not due to a biased distribution of labels in the training set. In fact, after balancing using TADA, the distribution of labels between training and testing data will not match, making the high accuracy of balanced results even more noteworthy.</p>
    </sec>
    <sec>
      <title>4.3 E3: biased class labels</title>
      <p>Focusing on the IBD data, we next test the impact of not just unbalanced but also biased sampling by fixing training set to have 1/5 healthy (for IBD) but changing the relative representation in testing data. Interestingly, including bias does not further reduce the accuracy in substantial ways (<xref ref-type="fig" rid="btz394-F5">Fig. 5</xref>). However, in the biased scenario, we continue to see dramatic improvements obtained by TADA compared to no augmentation, down-sampling, and to less extent, SMOTE and ADASYN. Thus, for unbalanced training data, augmentation can improve the accuracy regardless of whether the testing data have the same label distribution.
</p>
      <fig id="btz394-F5" orientation="portrait" position="float">
        <label>Fig. 5.</label>
        <caption>
          <p>Results for E3. The training set includes 1/5 healthy out of a total of 243 samples. The testing set has 1/5 (20-versus-80), 1/2 (50-versus-50) or 4/5 (80-versus-20) of samples coming from healthy individuals on the IBD dataset. Methods labeled identically to <xref ref-type="fig" rid="btz394-F3">Figure 3</xref></p>
        </caption>
        <graphic xlink:href="btz394f5"/>
      </fig>
    </sec>
  </sec>
  <sec>
    <title>5 Discussions and conclusions</title>
    <p>We described a new data augmentation method to generate artificial samples for augmenting the training set of ML methods for phenotype classification from microbiome samples. Our method, TADA, combines the power of statistical generative models that incorporate phylogenetic knowledge with the flexibility of black-box ML methods. We tested our method for two phenotypes (IBD and BMI) and using one type of microbiome data, namely 16S. Our results showed that TADA improved the classification accuracy and the improvements were dramatic when the samples were unbalanced in terms of the distribution of class labels.</p>
    <p>We emphasize that the unbalance in training data is not a corner case; in microbiome data, unbalance is the rule, not the exception. Often, microbiome datasets gathered in clinical settings are short on control (i.e. healthy) cases, especially when compared to the larger population. Our results clearly demonstrate that ML methods fail to train well on unbalanced data. While we focused on AUC, it is instructive also to examine the percentage of times a method makes the correct classification call. With 1/10 or 1/5 unbalance levels, the trained ML model is mostly useless because it classifies all testing samples as diseased, achieving artificially high levels of correct classification (<xref ref-type="fig" rid="btz394-F6">Fig. 6</xref>) despite low AUC (<xref ref-type="fig" rid="btz394-F3">Fig. 3</xref>); i.e. here, ML models just match a <italic>no-skill</italic> classifier and are, thus, grossly overfit. Balancing augmentation helps to alleviate this issue, as evident in increased AUC values. Nevertheless, balancing changes the prevalence of labels and needs to be done with care. Overall, our results provide a cautionary note on applying ML methods for unbalanced labels and are a reminder that clinical applications of ML to microbiome are fraught with dangers and can benefit from further improvements in the methodology.
</p>
    <fig id="btz394-F6" orientation="portrait" position="float">
      <label>Fig. 6.</label>
      <caption>
        <p>Percentage of correct classification on the E2-fix IBD dataset. Figure settings are similar to <xref ref-type="fig" rid="btz394-F3">Figure 3</xref> but we show percentage of correct classifications instead of AUC. Red line shows the accuracy achieved by simply guessing the healthy label each time</p>
      </caption>
      <graphic xlink:href="btz394f6"/>
    </fig>
    <p>Our results did not show a consistent difference between SV and TVSV generative models across all dataset. The more complex model, TVSV, was slightly more accurate on the BMI dataset but did not manage to outperform SV on IBD. TVSV seeks to capture variability due to biological sources and adds more variance than SV. The failure of TVSV to provide a substantial improvement over SV only on the IBD dataset may indicate that for some datasets (perhaps more carefully curated) the biological variance is already sufficiently captured. But it could also indicate that the variance generated using our hierarchical Beta model, fails to emulate biological variance in a meaningful way. Beta is a powerful model to capture the distribution of proportions, especially when distributed around a center (or the two extremes) but biological distributions may not fit Beta. Moreover, we make conditional independence assumptions on the phylogenetic tree, which may not match the biology (e.g. due to horizontal gene transfer).</p>
    <p>Our results indicated that clustering samples and using the mixture model could reduce accuracy if the clusters are big and is neutral or only slightly beneficial when clusters are small (<xref ref-type="fig" rid="btz394-F2">Fig. 2</xref>). Thus, sample augmentation was most effective when applied to individual samples or small clusters. It may be that with the small sample sizes that we have and large numbers of confounding factors, samples are so varied that only one or a handful of data points are available per component of the mixture model. Thus, it is possible that as the size of the training datasets increase, the mixture model starts to outperform the TVSV-<italic>m</italic> consistently. Thus, for existing small datasets, using TVSV-<italic>m</italic> is a safe choice, but in the future, as more data become available, this question needs to be revisited.</p>
    <p>The framework we described for combining generative models and ML methods can be extended beyond the exact generative models we used. Our specific generative model combines a Binomial and a Beta distribution, with one learned parameter (μ) and one parameter fixed based on the phylogeny (ν). The method we used to choose the fixed ν (inversely related to the variance) relies on the phylogenetic knowledge, incorporated as the mean divergence below each node (similar to the <italic>F<sub>ST</sub></italic> measure). We selected a particular function <italic>f</italic>, but note that our choice is without strong theoretical underpinnings. Future work should explore more principled choices, deriving the function <italic>f</italic> as a result of a dispersion process running along the branches of the phylogeny (e.g. a Poisson model). These future attempts could also explore the <xref rid="btz394-B7" ref-type="bibr">Balding and Nichols (1995)</xref> model, which also is based on a similar Beta model and the <italic>F<sub>ST</sub></italic> measure.</p>
    <p>A natural extension of our generative model is to let ν be learned from the data instead of using the phylogeny. In fact, we have derived the necessary parameter estimators for such a model using the method of moments (see Appendix A.2). However, using this model will double the number of parameters and will rely less on the known phylogenetic knowledge. Our initial tests (Supplementary Fig. S4) indicate this more parameter-rich model fails to perform well on our two test datasets. However, if substantially larger training sets are available in the future, this method should be revisited. Another natural extension is to use Dirichlet+Multinomial instead of Beta+Binomial to allow multifurcating trees. Finally, instead of assuming all μ<sub><italic>u</italic></sub> and ν<sub><italic>u</italic></sub> parameters are separate parameters, they can be considered random variables drawn from another distribution with appropriate hyperparameters.</p>
    <p>We observed that RF had somewhat higher accuracy than NN in our experiments. This observation is in line with some previous studies (e.g. <xref rid="btz394-B46" ref-type="bibr">Statnikov <italic>et al.</italic>, 2013</xref>), which have demonstrated similar results. However, we note that with augmentation, NN comes much closer to the accuracy of RF. We also note that we have not fine-tuned the NN models. Thus, it is possible that NN, perhaps in the form of smaller networks or conversely deeper networks along with regularization techniques could outperform RF. In particular, deep learning requires large training samples. It is conceivable that deep learning methods paired with augmented data can in the future outperform ensemble methods such as NN in the future.</p>
    <p>Other steps of TADA could also be changed. For example, for the phylogenetic inference, instead of placement on a common backbone tree, a <italic>de novo</italic> inference may be feasible using scalable phylogenetic inference methods. Clustering of samples can also be done using more complex methods designed for microbiome, such as phylogeny-based methods like weighted/unweighted Unifrac distances (<xref rid="btz394-B31" ref-type="bibr">Lozupone <italic>et al.</italic>, 2007</xref>; <xref rid="btz394-B30" ref-type="bibr">Lozupone and Knight, 2005</xref>) and compositional methods like Aitchison’s distance (<xref rid="btz394-B2" ref-type="bibr">Aitchison, 1982</xref>; <xref rid="btz394-B3" ref-type="bibr">Aitchison <italic>et al.</italic>, 2000</xref>). Finally, as features for the ML training, we used OTUs as obtained using the Deblur algorithm (i.e. de-noised sequences). However, extracting features can also follow more complex methods, perhaps using those that include the phylogenetic knowledge (e.g. <xref rid="btz394-B4" ref-type="bibr">Albanese <italic>et al.</italic>, 2015</xref>; <xref rid="btz394-B37" ref-type="bibr">Morton <italic>et al.</italic>, 2017</xref>).</p>
    <p>Our studies show potential for improving the generalization of ML methods. We tested only two datasets, each with only two categories. Future work should explore applications of TADA to more phenotypes, including multi-labeled ones. Also, nothing in the method limits it to gut or human microbiome; the same method should be explored on other types of environments. Future experiments should also explore training models on a dataset and testing on a separate dataset produced by a different lab; perhaps augmentation can also help reduce batch effects, which are notoriously difficult to deal with in microbiome modeling. Finally, we focused on 16S profiling. However, phylogenetic placement methods for shotgun metagenomic samples also exist [e.g. TIPP (<xref rid="btz394-B39" ref-type="bibr">Nguyen <italic>et al.</italic>, 2014</xref>)]; future work should explore the application of TADA to metagenomic data.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>btz394_Supplementary_Data</label>
      <media xlink:href="btz394_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>We thank Austin Swafford for help with gathering datasets and setting up pipeline, Rob Knight for fruitful discussions, Sandrine Javorschi-Miller and Ho-Cheol Kim for consistent support.</p>
    <sec>
      <title>Funding</title>
      <p>This work was supported by IBM Research AI through the AI Horizons Network. S.M. and E.S were supported through the National Science Foundation grants IIS-1565862 and III-1845967.</p>
      <p>Conflict of Interest: none declared.</p>
    </sec>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btz394-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Aagaard</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>A metagenomic approach to characterization of the vaginal microbiome signature in pregnancy</article-title>. <source>PLoS One</source>, <volume>7</volume>, <fpage>e36466.</fpage><pub-id pub-id-type="pmid">22719832</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Aitchison</surname><given-names>J.</given-names></name></person-group> (<year>1982</year>) 
<article-title>The statistical analysis of compositional data</article-title>. <source>J. R. Stat. Soc. Series B (Methodol.)</source>, <volume>44</volume>, <fpage>139</fpage>–<lpage>177</lpage>.</mixed-citation>
    </ref>
    <ref id="btz394-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Aitchison</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2000</year>) 
<article-title>Logratio analysis and compositional distance</article-title>. <source>Math. Geol</source>., <volume>32</volume>, <fpage>271</fpage>–<lpage>275</lpage>.</mixed-citation>
    </ref>
    <ref id="btz394-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Albanese</surname><given-names>D.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Explaining diversity in metagenomic datasets by phylogenetic-based feature weighting</article-title>. <source>PLoS Comput. Biol</source>., <volume>11</volume>, <fpage>e1004186.</fpage><pub-id pub-id-type="pmid">25815895</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Amir</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Deblur rapidly resolves single-nucleotide community sequence patterns</article-title>. <source>mSystems</source>, <volume>2</volume>, e00191–16.</mixed-citation>
    </ref>
    <ref id="btz394-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Arthur</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Vassilvitskii</surname><given-names>S.</given-names></name></person-group> (<year>2007</year>) 
<article-title>K-means++: the advantages of careful seeding</article-title>. In: <source>Proceedings of ACM-SIAM Symposium on Discrete Algorithms</source>, pp. 1027–1035, New Orleans, Louisiana.</mixed-citation>
    </ref>
    <ref id="btz394-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Balding</surname><given-names>D.J.</given-names></name>, <name name-style="western"><surname>Nichols</surname><given-names>R.A.</given-names></name></person-group> (<year>1995</year>) 
<article-title>A method for quantifying differentiation between populations at multi-allelic loci and its implications for investigating identity and paternity</article-title>. <source>Genetica</source>, <volume>96</volume>, <fpage>3</fpage>–<lpage>12</lpage>.<pub-id pub-id-type="pmid">7607457</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Beck</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Foster</surname><given-names>J.A.</given-names></name></person-group> (<year>2014</year>) 
<article-title>Machine learning techniques accurately classify microbial communities by bacterial vaginosis characteristics</article-title>. <source>PLoS One</source>, <volume>9</volume>, <fpage>e87830.</fpage><pub-id pub-id-type="pmid">24498380</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Breiman</surname><given-names>L.</given-names></name></person-group> (<year>2001</year>) 
<article-title>Random Forests</article-title>. <source>Machine Learning</source>, <volume>45</volume>, <fpage>5</fpage>–<lpage>32</lpage>.</mixed-citation>
    </ref>
    <ref id="btz394-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Callahan</surname><given-names>B.J.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>DADA2: high-resolution sample inference from Illumina amplicon data</article-title>. <source>Nat. Methods</source>, <volume>13</volume>, <fpage>581</fpage>–<lpage>583</lpage><fpage>.</fpage><pub-id pub-id-type="pmid">27214047</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Caporaso</surname><given-names>J.G.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) 
<article-title>Moving pictures of the human microbiome</article-title>. <source>Genome Biol</source>., <volume>12</volume>, <fpage>R50.</fpage><pub-id pub-id-type="pmid">21624126</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B11">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Chawla</surname><given-names>N.V.</given-names></name></person-group> (<year>2010</year>) <chapter-title>Data mining for imbalanced datasets: an overview</chapter-title> In: Maimon,O. and Rokach,L. (eds) <source>Data Mining and Knowledge Discovery Handbook</source>. 
<publisher-name>Springer</publisher-name>, 
<publisher-loc>Boston, MA</publisher-loc>, pp. <fpage>875</fpage>–<lpage>886</lpage>.</mixed-citation>
    </ref>
    <ref id="btz394-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chawla</surname><given-names>N.V.</given-names></name></person-group><etal>et al</etal> (<year>2002</year>) 
<article-title>SMOTE: synthetic minority over-sampling technique</article-title>. <source>J. Artif. Intell. Res</source>., <volume>16</volume>, <fpage>321</fpage>–<lpage>357</lpage>.</mixed-citation>
    </ref>
    <ref id="btz394-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dave</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>The human gut microbiome: current knowledge, challenges, and future directions</article-title>. <source>Transl. Res</source>., <volume>160</volume>, <fpage>246</fpage>–<lpage>257</lpage>.<pub-id pub-id-type="pmid">22683238</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>DeSantis</surname><given-names>T.Z.</given-names></name></person-group><etal>et al</etal> (<year>2006</year>) 
<article-title>Greengenes, a chimera-checked 16S rRNA gene database and workbench compatible with ARB</article-title>. <source>Appl. Environ. Microbiol</source>., <volume>72</volume>, <fpage>5069</fpage>–<lpage>5072</lpage>.<pub-id pub-id-type="pmid">16820507</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Edgar</surname><given-names>R.C.</given-names></name></person-group> (<year>2010</year>) 
<article-title>Search and clustering orders of magnitude faster than BLAST</article-title>. <source>Bioinformatics</source>, <volume>26</volume>, <fpage>2460</fpage>–<lpage>2461</lpage>.<pub-id pub-id-type="pmid">20709691</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Edgar</surname><given-names>R.C.</given-names></name></person-group> (<year>2016</year>) 
<article-title>UNOISE2: improved error-correction for Illumina 16S and ITS amplicon sequencing</article-title>. <source>bioRxiv</source>. 081257.</mixed-citation>
    </ref>
    <ref id="btz394-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Feng</surname><given-names>Q.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Gut microbiome development along the colorectal adenoma-carcinoma sequence</article-title>. <source>Nat. Commun</source>., <volume>6</volume>, <fpage>6528.</fpage><pub-id pub-id-type="pmid">25758642</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Flores</surname><given-names>G.E.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Temporal variability is a personalized feature of the human microbiome</article-title>. <source>Genome Biol</source>., <volume>15</volume>, <fpage>531.</fpage><pub-id pub-id-type="pmid">25517225</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gevers</surname><given-names>D.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>The treatment-naive microbiome in new-onset Crohn’s disease</article-title>. <source>Cell Host and Microbe</source>, <volume>15</volume>, <fpage>382</fpage>–<lpage>392</lpage>.<pub-id pub-id-type="pmid">24629344</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gill</surname><given-names>S.R.</given-names></name></person-group><etal>et al</etal> (<year>2006</year>) 
<article-title>Metagenomic analysis of the human distal gut microbiome</article-title>. <source>Science</source>, <volume>312</volume>, <fpage>1355</fpage>–<lpage>1359</lpage>.<pub-id pub-id-type="pmid">16741115</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gonzalez</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Qiita: rapid, web-enabled microbiome meta-analysis</article-title>. <source>Nat. Methods</source>, <volume>15</volume>, <fpage>796</fpage>–<lpage>798</lpage>.<pub-id pub-id-type="pmid">30275573</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B22">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>H.</given-names></name></person-group><etal>et al</etal> (<year>2008</year>) 
<article-title>ADASYN: adaptive synthetic sampling approach for imbalanced learning</article-title>. In: <source>2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)</source>, pp. <fpage>1322</fpage>–<lpage>1328</lpage>, Hong Kong.</mixed-citation>
    </ref>
    <ref id="btz394-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Janssen</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Phylogenetic placement of exact amplicon sequences improves associations with clinical information</article-title>. <source>mSystems</source>, <volume>3</volume>, e<fpage>00021</fpage>-<lpage>18</lpage>.</mixed-citation>
    </ref>
    <ref id="btz394-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Knights</surname><given-names>D.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) 
<article-title>Human-associated microbial signatures: examining their predictive value</article-title>. <source>Cell Host Microbe</source>, <volume>10</volume>, <fpage>292</fpage>–<lpage>296</lpage>.<pub-id pub-id-type="pmid">22018228</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B25">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Kubat</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Matwin</surname><given-names>S.</given-names></name></person-group> (<year>1997</year>) <chapter-title>Addressing the curse of imbalanced training sets: one sided selection</chapter-title> In: <source>Proceedings of the 14th International Conference on Machine Learning</source>, pp. 179–186, Nashville, Tennesse.</mixed-citation>
    </ref>
    <ref id="btz394-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Langille</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Predictive functional profiling of microbial communities using 16S rRNA marker gene sequences</article-title>. <source>Nat. Biotechnol</source>., <volume>31</volume>, <fpage>814</fpage>–<lpage>821</lpage>.<pub-id pub-id-type="pmid">23975157</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Leek</surname><given-names>J.T.</given-names></name></person-group><etal>et al</etal> (<year>2010</year>) 
<article-title>Tackling the widespread and critical impact of batch effects in high-throughput data</article-title>. <source>Nat. Rev. Genet</source>., <volume>11</volume>, <fpage>733</fpage>–<lpage>739</lpage>.<pub-id pub-id-type="pmid">20838408</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lemaître</surname><given-names>G.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Imbalanced-learn: a Python toolbox to tackle the curse of imbalanced datasets in machine learning</article-title>. <source>J. Mach. Learn. Res</source>., <volume>18</volume>, <fpage>1</fpage>–<lpage>5</lpage>.</mixed-citation>
    </ref>
    <ref id="btz394-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lozupone</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Knight</surname><given-names>R.</given-names></name></person-group> (<year>2005</year>) 
<article-title>UniFrac: a new phylogenetic method for comparing microbial communities</article-title>. <source>Appl. Environ. Microbiol</source>., <volume>71</volume>, <fpage>8228</fpage>–<lpage>8235</lpage>.<pub-id pub-id-type="pmid">16332807</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lozupone</surname><given-names>C.A.</given-names></name></person-group><etal>et al</etal> (<year>2007</year>) 
<article-title>Quantitative and qualitative <italic>β</italic> diversity measures lead to different insights into factors that structure microbial communities</article-title>. <source>Appl. Environ. Microbiol</source>., <volume>73</volume>, <fpage>1576</fpage>–<lpage>1585</lpage>.<pub-id pub-id-type="pmid">17220268</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Matsen</surname><given-names>F.A.</given-names></name></person-group> (<year>2015</year>) 
<article-title>Phylogenetics and the human microbiome</article-title>. <source>Syst. Biol</source>., <volume>64</volume>, <fpage>e26</fpage>–<lpage>e41</lpage>.<pub-id pub-id-type="pmid">25102857</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>McDonald</surname><given-names>D.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>The biological observation matrix (BIOM) format or: how I learned to stop worrying and love the ome–ome</article-title>. <source>Gigascience</source>, <volume>1</volume>, <fpage>7</fpage>.<pub-id pub-id-type="pmid">23587224</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>McDonald</surname><given-names>D.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>American gut: an open platform for citizen science microbiome research</article-title>. <source>mSystems</source>, <volume>3</volume>, e00031–18.</mixed-citation>
    </ref>
    <ref id="btz394-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>McMurdie</surname><given-names>P.J.</given-names></name>, <name name-style="western"><surname>Holmes</surname><given-names>S.</given-names></name></person-group> (<year>2014</year>) 
<article-title>Waste not, want not: why rarefying microbiome data is inadmissible</article-title>. <source>PLoS Comput. Biol</source>., <volume>10</volume>, <fpage>e1003531.</fpage><pub-id pub-id-type="pmid">24699258</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B36">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Mirarab</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>). <chapter-title>SEPP: SATté-enabled phylogenetic placement</chapter-title> In: <source>Biocomputing 2012</source>. 
<publisher-name>World Scientific</publisher-name>, Fairmont Orchid, Big Island of Hawaii, pp. <fpage>247</fpage>–<lpage>258</lpage>.</mixed-citation>
    </ref>
    <ref id="btz394-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Morton</surname><given-names>J.T.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Balance trees reveal microbial niche differentiation</article-title>. <source>mSystems</source>, <volume>2</volume>, e00162–16.</mixed-citation>
    </ref>
    <ref id="btz394-B38">
      <mixed-citation publication-type="book">National Research Council (US) Committee on Metagenomics: Challenges and Functional Applications. (<year>2007</year>) <source>The New Science of Metagenomics: Revealing the Secrets of Our Microbial Planet</source>. 
<publisher-name>National Academies Press</publisher-name>, 
<publisher-loc>Washington, DC</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btz394-B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nguyen</surname><given-names>N.P.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>TIPP: taxonomic identification and phylogenetic profiling</article-title>. <source>Bioinformatics</source>, <volume>30</volume>, <fpage>3548</fpage>–<lpage>3555</lpage>.<pub-id pub-id-type="pmid">25359891</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>O’Dwyer</surname><given-names>J.P.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>Phylogenetic diversity theory sheds light on the structure of microbial communities</article-title>. <source>PLoS Comput. Biol</source>., <volume>8</volume>, <fpage>e1002832</fpage>.<pub-id pub-id-type="pmid">23284280</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Paulson</surname><given-names>J.N.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Differential abundance analysis for microbial marker-gene surveys</article-title>. <source>Nat. Methods</source>, <volume>10</volume>, <fpage>1200</fpage>–<lpage>1202</lpage>.<pub-id pub-id-type="pmid">24076764</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pedregosa</surname><given-names>F.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) 
<article-title>Scikit-learn: machine learning in {P}ython</article-title>. <source>J. Mach. Learn. Res</source>., <volume>12</volume>, <fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation>
    </ref>
    <ref id="btz394-B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Russakovsky</surname><given-names>O.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>ImageNet large scale visual recognition challenge</article-title>. <source>Int. J. Comput. Vis</source>., <volume>115</volume>, <fpage>211</fpage>–<lpage>252</lpage>.</mixed-citation>
    </ref>
    <ref id="btz394-B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Saulnier</surname><given-names>D.M.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) 
<article-title>Gastrointestinal microbiome signatures of pediatric patients with irritable bowel syndrome</article-title>. <source>Gastroenterology</source>, <volume>141</volume>, <fpage>1782</fpage>–<lpage>1791</lpage>.<pub-id pub-id-type="pmid">21741921</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Schloss</surname><given-names>P.D.</given-names></name>, <name name-style="western"><surname>Handelsman</surname><given-names>J.</given-names></name></person-group> (<year>2005</year>) 
<article-title>Introducing DOTUR, a computer program for defining operational taxonomic units and estimating species richness</article-title>. <source>Appl. Environ. Microbiol</source>., <volume>71</volume>, <fpage>1501</fpage>–<lpage>1506</lpage>.<pub-id pub-id-type="pmid">15746353</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Statnikov</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>A comprehensive evaluation of multicategory classification methods for microbiomic data</article-title>. <source>Microbiome</source>, <volume>1</volume>, <fpage>11.</fpage><pub-id pub-id-type="pmid">24456583</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sukumaran</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Holder</surname><given-names>M.T.</given-names></name></person-group> (<year>2010</year>) 
<article-title>DendroPy: a Python library for phylogenetic computing</article-title>. <source>Bioinformatics</source>, <volume>26</volume>, <fpage>1569</fpage>–<lpage>1571</lpage>.<pub-id pub-id-type="pmid">20421198</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sze</surname><given-names>M.A.</given-names></name>, <name name-style="western"><surname>Schloss</surname><given-names>P.D.</given-names></name></person-group> (<year>2016</year>) 
<article-title>Looking for a signal in the noise: revisiting obesity and the microbiome</article-title>. <source>mBio</source>, <volume>7</volume>, e01018–16.</mixed-citation>
    </ref>
    <ref id="btz394-B49">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Turnbaugh</surname><given-names>P.J.</given-names></name></person-group><etal>et al</etal> (<year>2007</year>) 
<article-title>The human microbiome project</article-title>. <source>Nature</source>, <volume>449</volume>, <fpage>804</fpage>–<lpage>810</lpage>.<pub-id pub-id-type="pmid">17943116</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B50">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Venter</surname><given-names>J.C.</given-names></name></person-group><etal>et al</etal> (<year>2004</year>) 
<article-title>Environmental genome shotgun sequencing of the Sargasso Sea</article-title>. <source>Science</source>, <volume>304</volume>, <fpage>66</fpage>–<lpage>74</lpage>.<pub-id pub-id-type="pmid">15001713</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B51">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>von Mering</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2007</year>) 
<article-title>Quantitative phylogenetic assessment of microbial communities in diverse environments</article-title>. <source>Science</source>, <volume>315</volume>, <fpage>1126</fpage>–<lpage>1130</lpage>.<pub-id pub-id-type="pmid">17272687</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B52">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Waldor</surname><given-names>M.K.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Where next for microbiome research?</article-title><source>PLoS Biol</source>., <volume>13</volume>, <fpage>e1002050.</fpage><pub-id pub-id-type="pmid">25602283</pub-id></mixed-citation>
    </ref>
    <ref id="btz394-B53">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Weiss</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Tracking down the sources of experimental contamination in microbiome studies</article-title>. <source>Genome Biol</source>., <volume>15</volume>, <fpage>564.</fpage><pub-id pub-id-type="pmid">25608874</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
