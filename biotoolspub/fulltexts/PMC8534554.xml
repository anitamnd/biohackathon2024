<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1-3.dtd?>
<?SourceDTD.Version 1.3?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Entropy (Basel)</journal-id>
    <journal-id journal-id-type="iso-abbrev">Entropy (Basel)</journal-id>
    <journal-id journal-id-type="publisher-id">entropy</journal-id>
    <journal-title-group>
      <journal-title>Entropy</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1099-4300</issn>
    <publisher>
      <publisher-name>MDPI</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8534554</article-id>
    <article-id pub-id-type="doi">10.3390/e23101368</article-id>
    <article-id pub-id-type="publisher-id">entropy-23-01368</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Technical Note</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Scikit-Dimension: A Python Package for Intrinsic Dimension Estimation</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Bac</surname>
          <given-names>Jonathan</given-names>
        </name>
        <xref rid="af1-entropy-23-01368" ref-type="aff">1</xref>
        <xref rid="af2-entropy-23-01368" ref-type="aff">2</xref>
        <xref rid="af3-entropy-23-01368" ref-type="aff">3</xref>
        <xref rid="c1-entropy-23-01368" ref-type="corresp">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1474-1734</contrib-id>
        <name>
          <surname>Mirkes</surname>
          <given-names>Evgeny M.</given-names>
        </name>
        <xref rid="af4-entropy-23-01368" ref-type="aff">4</xref>
        <xref rid="af5-entropy-23-01368" ref-type="aff">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-6224-1430</contrib-id>
        <name>
          <surname>Gorban</surname>
          <given-names>Alexander N.</given-names>
        </name>
        <xref rid="af4-entropy-23-01368" ref-type="aff">4</xref>
        <xref rid="af5-entropy-23-01368" ref-type="aff">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-7359-7966</contrib-id>
        <name>
          <surname>Tyukin</surname>
          <given-names>Ivan</given-names>
        </name>
        <xref rid="af4-entropy-23-01368" ref-type="aff">4</xref>
        <xref rid="af5-entropy-23-01368" ref-type="aff">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-9517-7284</contrib-id>
        <name>
          <surname>Zinovyev</surname>
          <given-names>Andrei</given-names>
        </name>
        <xref rid="af1-entropy-23-01368" ref-type="aff">1</xref>
        <xref rid="af2-entropy-23-01368" ref-type="aff">2</xref>
        <xref rid="af3-entropy-23-01368" ref-type="aff">3</xref>
        <xref rid="af5-entropy-23-01368" ref-type="aff">5</xref>
        <xref rid="c1-entropy-23-01368" ref-type="corresp">*</xref>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Broadbridge</surname>
          <given-names>Philip</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <aff id="af1-entropy-23-01368"><label>1</label>Institut Curie, PSL Research University, 75248 Paris, France</aff>
    <aff id="af2-entropy-23-01368"><label>2</label>INSERM, U900, 75248 Paris, France</aff>
    <aff id="af3-entropy-23-01368"><label>3</label>CBIO-Centre for Computational Biology, Mines ParisTech, PSL Research University, 75272 Paris, France</aff>
    <aff id="af4-entropy-23-01368"><label>4</label>Department of Mathematics, University of Leicester, Leicester LE1 7RH, UK; <email>em322@leicester.ac.uk</email> (E.M.M.); <email>a.n.gorban@leicester.ac.uk</email> (A.N.G.); <email>i.tyukin@leicester.ac.uk</email> (I.T.)</aff>
    <aff id="af5-entropy-23-01368"><label>5</label>Laboratory of Advanced Methods for High-Dimensional Data Analysis, Lobachevsky University, 603105 Nizhniy Novgorod, Russia</aff>
    <author-notes>
      <corresp id="c1-entropy-23-01368"><label>*</label>Correspondence: <email>jonathan.bac@cri-paris.org</email> (J.B.); <email>andrei.zinovyev@curie.fr</email> (A.Z.); Tel.: +33-156-246-989 (A.Z.)</corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>19</day>
      <month>10</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <month>10</month>
      <year>2021</year>
    </pub-date>
    <volume>23</volume>
    <issue>10</issue>
    <elocation-id>1368</elocation-id>
    <history>
      <date date-type="received">
        <day>17</day>
        <month>9</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>16</day>
        <month>10</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2021 by the authors.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Dealing with uncertainty in applications of machine learning to real-life data critically depends on the knowledge of intrinsic dimensionality (ID). A number of methods have been suggested for the purpose of estimating ID, but no standard package to easily apply them one by one or all at once has been implemented in Python. This technical note introduces <monospace>scikit-dimension</monospace>, an open-source Python package for intrinsic dimension estimation. The <monospace>scikit-dimension</monospace> package provides a uniform implementation of most of the known ID estimators based on the scikit-learn application programming interface to evaluate the global and local intrinsic dimension, as well as generators of synthetic toy and benchmark datasets widespread in the literature. The package is developed with tools assessing the code quality, coverage, unit testing and continuous integration. We briefly describe the package and demonstrate its use in a large-scale (more than 500 datasets) benchmarking of methods for ID estimation for real-life and synthetic data.</p>
    </abstract>
    <kwd-group>
      <kwd>intrinsic dimension</kwd>
      <kwd>effective dimension</kwd>
      <kwd>Python package</kwd>
      <kwd>method benchmarking</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="sec1-entropy-23-01368">
    <title>1. Introduction</title>
    <p>We present <monospace>scikit-dimension</monospace>, an open-source Python package for global and local intrinsic dimension (ID) estimation. The package has two main objectives: (i) foster research in ID estimation by providing code to benchmark algorithms and a platform to share algorithms; and (ii) democratize the use of ID estimation by providing user-friendly implementations of algorithms using the scikit-learn application programming interface (API) [<xref rid="B1-entropy-23-01368" ref-type="bibr">1</xref>].</p>
    <p>ID intuitively refers to the minimum number of parameters required to represent a dataset with satisfactory accuracy. The meaning of “accuracy” can be different among various approaches. ID can be more precisely defined to be <italic toggle="yes">n</italic> if the data lie closely to a <italic toggle="yes">n</italic>-dimensional manifold embedded in <inline-formula><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> with little information loss, which corresponds to the so-called “manifold hypothesis” [<xref rid="B2-entropy-23-01368" ref-type="bibr">2</xref>,<xref rid="B3-entropy-23-01368" ref-type="bibr">3</xref>]. ID can be, however, defined without assuming the existence of a data manifold. In this case, data point cloud characteristics (e.g., linear separability or pattern of covariance) are compared to a model <italic toggle="yes">n</italic>-dimensional distribution (e.g., uniformly sampled <italic toggle="yes">n</italic>-sphere or <italic toggle="yes">n</italic>-dimensional isotropic Gaussian distribution), and the term “effective dimensionality” is sometimes used instead of “intrinsic dimensionality” as such <italic toggle="yes">n</italic> giving the most similar characteristics to the one measured in the studied point cloud [<xref rid="B4-entropy-23-01368" ref-type="bibr">4</xref>,<xref rid="B5-entropy-23-01368" ref-type="bibr">5</xref>]. In <monospace>scikit-dimension</monospace>, these two notions are not distinguished.</p>
    <p>The knowledge of ID is important to determine the choice of machine learning algorithm, anticipate the uncertainty of its predictions, and estimate the number of sufficiently distinct clusters of variables [<xref rid="B6-entropy-23-01368" ref-type="bibr">6</xref>,<xref rid="B7-entropy-23-01368" ref-type="bibr">7</xref>]. The well-known <italic toggle="yes">curse of dimensionality</italic>, which states that many problems become exponentially difficult in high dimensions, does not depend on the number of features, but on the dataset’s ID [<xref rid="B8-entropy-23-01368" ref-type="bibr">8</xref>]. More precisely, the effects of the dimensionality curse are expected to be manifested when <inline-formula><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mo>≫</mml:mo><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:mi>M</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">M</italic> is the number of data points [<xref rid="B9-entropy-23-01368" ref-type="bibr">9</xref>,<xref rid="B10-entropy-23-01368" ref-type="bibr">10</xref>].</p>
    <p>Current ID estimators have diverse operating principles (we refer the reader to [<xref rid="B11-entropy-23-01368" ref-type="bibr">11</xref>] for an overview). Each ID estimator is developed based on a selected feature (such as the number of data points in a sphere of fixed radius, linear separability or expected normalized distance to the closest neighbor), which scales with <italic toggle="yes">n</italic>: therefore, various ID estimation methods provide different ID values. Each dataset can be characterized by a unique <italic toggle="yes">dimensionality profile</italic> of ID estimations, according to different existing methods, which can serve as an important signature for choosing the most appropriate data analysis method.</p>
    <p>Dimensionality estimators that provide a single ID value for the whole dataset belong to the category of global estimators. However, datasets can have complex organizations and contain regions with varying dimensionality [<xref rid="B9-entropy-23-01368" ref-type="bibr">9</xref>]. In such a case, they can be explored using local estimators, which estimate ID in local neighborhoods around each point. The neighborhoods are typically defined by considering the <italic toggle="yes">k</italic> closest neighbors. Such approaches also allow repurposing global estimators as local estimators.</p>
    <p>The idea behind local ID estimation is to operate at a scale where the data manifold can be approximated by its tangent space [<xref rid="B12-entropy-23-01368" ref-type="bibr">12</xref>]. In practice, ID is sensitive to scale, and choosing the neighborhood size is a trade-off between opposite requirements [<xref rid="B11-entropy-23-01368" ref-type="bibr">11</xref>,<xref rid="B13-entropy-23-01368" ref-type="bibr">13</xref>]: ideally, the neighborhood should be big relative to the scale of the noise, and contain enough points. At the same time, it should be small enough to be well approximated by a flat and uniform tangent space.</p>
    <p>We perform benchmarking of 19 ID estimators on a large collection of real-life and synthetic datasets. Previously, estimators were benchmarked based mainly on artificial datasets representing uniformly sampled manifolds with known ID [<xref rid="B4-entropy-23-01368" ref-type="bibr">4</xref>,<xref rid="B11-entropy-23-01368" ref-type="bibr">11</xref>,<xref rid="B14-entropy-23-01368" ref-type="bibr">14</xref>], comparing them for the ability to estimate the ID value correctly. Several ID estimators were used on real-life datasets to evaluate the degree of dimensionality curse in a study of various metrics in data space [<xref rid="B15-entropy-23-01368" ref-type="bibr">15</xref>]. Here, we benchmark ID estimation methods, focusing on their applicability to a wide range of datasets of different origin, configuration and size. We also look at how different ID estimations are correlated, and show how <monospace>scikit-dimension</monospace> can be used to derive a consensus measure of data dimensionality by averaging multiple individual measures. The latter can be a robust measure of data dimensionality in various applications.</p>
    <p><monospace>Scikit-dimension</monospace> was applied in several recent studies for estimating the intrinsic dimensionality of real-life datasets [<xref rid="B16-entropy-23-01368" ref-type="bibr">16</xref>,<xref rid="B17-entropy-23-01368" ref-type="bibr">17</xref>].</p>
  </sec>
  <sec id="sec2-entropy-23-01368">
    <title>2. Materials and Methods</title>
    <sec id="sec2dot1-entropy-23-01368">
      <title>2.1. Software Features</title>
      <p><monospace>Scikit-dimension</monospace> is an open-source software available at <uri xlink:href="https://github.com/j-bac/scikit-dimension">https://github.com/j-bac/scikit-dimension</uri> (accessed on 18 October 2021).</p>
      <p><monospace>Scikit-dimension</monospace> consists of two modules. The <italic toggle="yes">id</italic> module provides ID estimators, and the <italic toggle="yes">datasets</italic> module provides synthetic benchmark datasets.</p>
      <sec id="sec2dot1dot1-entropy-23-01368">
        <title>2.1.1. <italic toggle="yes">id</italic> Module</title>
        <p>The <italic toggle="yes">id</italic> module contains estimators based on the following:<list list-type="bullet"><list-item><p>Correlation (fractal) dimension (id.CorrInt) [<xref rid="B18-entropy-23-01368" ref-type="bibr">18</xref>].</p></list-item><list-item><p>Manifold-adaptive fractal dimension (id.MADA) [<xref rid="B19-entropy-23-01368" ref-type="bibr">19</xref>].</p></list-item><list-item><p>Method of moments (id.MOM) [<xref rid="B20-entropy-23-01368" ref-type="bibr">20</xref>].</p></list-item><list-item><p>Principal component analysis (id.lPCA) [<xref rid="B3-entropy-23-01368" ref-type="bibr">3</xref>,<xref rid="B21-entropy-23-01368" ref-type="bibr">21</xref>,<xref rid="B22-entropy-23-01368" ref-type="bibr">22</xref>,<xref rid="B23-entropy-23-01368" ref-type="bibr">23</xref>].</p></list-item><list-item><p>Maximum likelihood (id.MLE) [<xref rid="B24-entropy-23-01368" ref-type="bibr">24</xref>,<xref rid="B25-entropy-23-01368" ref-type="bibr">25</xref>,<xref rid="B26-entropy-23-01368" ref-type="bibr">26</xref>].</p></list-item><list-item><p>Minimum spanning trees (id.KNN) [<xref rid="B27-entropy-23-01368" ref-type="bibr">27</xref>].</p></list-item><list-item><p>Estimators based on concentration of measure (id.MiND_ML, id.DANCo, id.ESS, id.TwoNN, id.FisherS, id.TLE) [<xref rid="B4-entropy-23-01368" ref-type="bibr">4</xref>,<xref rid="B28-entropy-23-01368" ref-type="bibr">28</xref>,<xref rid="B29-entropy-23-01368" ref-type="bibr">29</xref>,<xref rid="B30-entropy-23-01368" ref-type="bibr">30</xref>,<xref rid="B31-entropy-23-01368" ref-type="bibr">31</xref>,<xref rid="B32-entropy-23-01368" ref-type="bibr">32</xref>,<xref rid="B33-entropy-23-01368" ref-type="bibr">33</xref>].</p></list-item></list></p>
        <p>The description of the method principles is provided together with the package documentation at <uri xlink:href="https://scikit-dimension.readthedocs.io/">https://scikit-dimension.readthedocs.io/</uri> (accessed on 18 October 2021) and in reviews [<xref rid="B5-entropy-23-01368" ref-type="bibr">5</xref>,<xref rid="B9-entropy-23-01368" ref-type="bibr">9</xref>,<xref rid="B14-entropy-23-01368" ref-type="bibr">14</xref>].</p>
      </sec>
      <sec id="sec2dot1dot2-entropy-23-01368">
        <title>2.1.2. <italic toggle="yes">Datasets</italic> Module</title>
        <p>The <italic toggle="yes">datasets</italic> module allows user to test estimators on synthetic datasets; <xref rid="entropy-23-01368-f001" ref-type="fig">Figure 1</xref>. It can generate several low-dimensional toy datasets to play with different estimators as well as a set of synthetic manifolds commonly used to benchmark ID estimators, introduced by [<xref rid="B14-entropy-23-01368" ref-type="bibr">14</xref>] and further extended in [<xref rid="B11-entropy-23-01368" ref-type="bibr">11</xref>,<xref rid="B28-entropy-23-01368" ref-type="bibr">28</xref>].</p>
      </sec>
    </sec>
    <sec id="sec2dot2-entropy-23-01368">
      <title>2.2. Development</title>
      <p><monospace>Scikit-dimension</monospace> is built according to the scikit-learn API [<xref rid="B1-entropy-23-01368" ref-type="bibr">1</xref>] with support for Linux, MacOS, Windows and Python <inline-formula><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&gt;</mml:mo><mml:mo>=</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> 3.6. The code style and API design are based on the guidelines of scikit-learn, with the NumPy [<xref rid="B34-entropy-23-01368" ref-type="bibr">34</xref>] documentation format, and continuous integration on all three platforms. The online documentation is built using Sphinx and hosted with ReadTheDocs.</p>
    </sec>
    <sec id="sec2dot3-entropy-23-01368">
      <title>2.3. Dependencies</title>
      <p><monospace>Scikit-dimension</monospace> depends on a limited number of external dependencies on the user side for ease of installation and maintenance:<list list-type="bullet"><list-item><p>Matplotlib [<xref rid="B35-entropy-23-01368" ref-type="bibr">35</xref>]</p></list-item><list-item><p>Pandas [<xref rid="B36-entropy-23-01368" ref-type="bibr">36</xref>].</p></list-item><list-item><p>Scikit-learn [<xref rid="B1-entropy-23-01368" ref-type="bibr">1</xref>].</p></list-item><list-item><p>Numba [<xref rid="B37-entropy-23-01368" ref-type="bibr">37</xref>].</p></list-item><list-item><p>SciPy [<xref rid="B38-entropy-23-01368" ref-type="bibr">38</xref>]</p></list-item><list-item><p>NumPy [<xref rid="B34-entropy-23-01368" ref-type="bibr">34</xref>].</p></list-item></list></p>
    </sec>
    <sec id="sec2dot4-entropy-23-01368">
      <title>2.4. Related Software</title>
      <p>Related open-source software for ID estimation have previously been developed in different languages such as R, MATLAB or C++ and contributed to the development of <monospace>scikit-dimension</monospace>.</p>
      <p>In particular, refs. [<xref rid="B10-entropy-23-01368" ref-type="bibr">10</xref>,<xref rid="B39-entropy-23-01368" ref-type="bibr">39</xref>,<xref rid="B40-entropy-23-01368" ref-type="bibr">40</xref>,<xref rid="B41-entropy-23-01368" ref-type="bibr">41</xref>] provided extensive collections of ID estimators and datasets for R users, with [<xref rid="B40-entropy-23-01368" ref-type="bibr">40</xref>] additionally focusing on dimension reduction algorithms. Similar resources can be found for MATLAB users [<xref rid="B42-entropy-23-01368" ref-type="bibr">42</xref>,<xref rid="B43-entropy-23-01368" ref-type="bibr">43</xref>,<xref rid="B44-entropy-23-01368" ref-type="bibr">44</xref>,<xref rid="B45-entropy-23-01368" ref-type="bibr">45</xref>]. Benchmarking many of the methods for ID estimation included in this package was performed in [<xref rid="B15-entropy-23-01368" ref-type="bibr">15</xref>]. Finally, there exist several packages implementing standalone algorithms; in particular for Python, we refer the reader to complementary implementations of the GeoMLE, full correlation dimension, and GraphDistancesID algorithms [<xref rid="B46-entropy-23-01368" ref-type="bibr">46</xref>,<xref rid="B47-entropy-23-01368" ref-type="bibr">47</xref>,<xref rid="B48-entropy-23-01368" ref-type="bibr">48</xref>,<xref rid="B49-entropy-23-01368" ref-type="bibr">49</xref>].</p>
      <p>To our knowledge, <monospace>scikit-dimension</monospace> is the first Python implementation of an extensive collection of ID methods. Compared to similar efforts in other languages, the package puts emphasis on estimators, quantifying various properties of high-dimensional data geometry, such as the concentration of measure. It is the only package to include ID estimation based on linear separability of data, using Fisher discriminants [<xref rid="B4-entropy-23-01368" ref-type="bibr">4</xref>,<xref rid="B32-entropy-23-01368" ref-type="bibr">32</xref>,<xref rid="B50-entropy-23-01368" ref-type="bibr">50</xref>,<xref rid="B51-entropy-23-01368" ref-type="bibr">51</xref>].</p>
    </sec>
  </sec>
  <sec sec-type="results" id="sec3-entropy-23-01368">
    <title>3. Results</title>
    <sec id="sec3dot1-entropy-23-01368">
      <title>3.1. Benchmarking <monospace>Scikit-Dimension</monospace> on a Large Collection of Datasets</title>
      <p>In order to demonstrate the applicability of <monospace>scikit-dimension</monospace> to a wide range of real-life datasets of various configurations and sizes, we performed a large-scale benchmarking of <monospace>scikit-dimension</monospace>, using the collection of datasets from the <monospace>OpenML</monospace> repository [<xref rid="B52-entropy-23-01368" ref-type="bibr">52</xref>]. We selected those datasets having at least 1000 observations and 10 features, without missing values. We excluded those datasets which were difficult to fetch, either because of their size or an error in the <monospace>OpenML</monospace> API. After filtering out repetitive entries, 499 datasets were collected. Their number of observations varied from 1002 to 9,199,930, and their number of features varied from 10 to 13,196. We focused only on numerical variables, and we subsampled the number of rows in the matrix to a maximum of 100,000. All dataset features were scaled to unit interval using Min/Max scaling. In addition, we filtered out approximate non-unique columns and rows in the data matrices since some of the ID methods could be affected by the presence of identical (or approximately identical) rows or columns.</p>
      <p>We added to the collection 18 datasets, containing single-cell transcriptomic measurements, from the CytoTRACE study [<xref rid="B53-entropy-23-01368" ref-type="bibr">53</xref>] and 4 largest datasets from The Cancer Genome Atlas (TCGA), containing bulk transcriptomic measurements. Therefore, our final collection contained 521 datasets altogether.</p>
      <sec id="sec3dot1dot1-entropy-23-01368">
        <title>3.1.1. <monospace>Scikit-Dimension</monospace> ID Estimator Method Features</title>
        <p>We systematically applied 19 ID estimation methods from <monospace>scikit-dimension</monospace>, with default parameter values, including 7 methods based on application of principal component analysis (“linear” or PCA-based ID methods), and 12 based on application of various other principles, including correlation dimension and concentration of measure-based methods (“nonlinear” ID methods).</p>
        <p>For KNN and MADA methods, we had to further subsample the data matrix to a maximum of 20,000 rows; otherwise they were too greedy in terms of memory consumption. Moreover, DANCo and ESS methods appeared to be too slow, especially in the case of a large number of variables: therefore, we made ID estimations in these cases on small fragments of data matrices. Thus, for DANCo, the maximum matrix size was set to 10,000 × 100, and for ESS to 2000 × 20. The number of features was reduced for these methods when needed, by using PCA-derived coordinates, and the number of observations were reduced by random subsampling.</p>
        <p>In <xref rid="entropy-23-01368-t001" ref-type="table">Table 1</xref>, we provide the summary of characteristics of the tested methods. In more detail, the following method features were evaluated (see <xref rid="entropy-23-01368-f002" ref-type="fig">Figure 2</xref>).</p>
        <p>Firstly, we simply looked at the ranges of ID values produced by the methods across all the datasets. These ranges varied significantly between the methods, especially for the linear ones (<xref rid="entropy-23-01368-f002" ref-type="fig">Figure 2</xref>A).</p>
        <p>Secondly, we tested the methods with respect to their ability to successfully compute the ID as a positive finite value. It appeared that certain methods (such as MADA and TLE), in a certain number of cases produced a significant fraction of uninterpretable estimates (such as “nan” or negative value); <xref rid="entropy-23-01368-f002" ref-type="fig">Figure 2</xref>B. We assume that in most of such cases, the problem with ID estimation is caused by the method implementation, not anticipating certain relatively rare data point configurations, rather than the methodology itself, and that a reasonable ID estimate always exists. Therefore, in the case of an uninterpretable value due to method implementation, for further analysis, we considered it possible to impute the ID value from the results of application of other methods; see below.</p>
        <p>Thirdly, for a small number of datasets, we performed a test of their sensitivity to the presence of strongly redundant features. For this purpose, we duplicated all features in a matrix and recomputed the ID. The resulting sensitivity is the ratio between the ID computed for the larger matrix and the ID computed for the initial matrix, having no duplicated columns. It appears that despite most of the methods being robust with respect to such a matrix duplication, some (such as PCA-based broken stick or the famous Kaiser methods popular in various fields, such as biology [<xref rid="B54-entropy-23-01368" ref-type="bibr">54</xref>,<xref rid="B55-entropy-23-01368" ref-type="bibr">55</xref>]), tend to be very sensitive (<xref rid="entropy-23-01368-f002" ref-type="fig">Figure 2</xref>C), which is compliant with some previous reports [<xref rid="B15-entropy-23-01368" ref-type="bibr">15</xref>].</p>
        <p>Some of the datasets in our collection could be combined in homogeneous groups according to their origin, such as the data coming from quantitative structure–activity relationship (QSAR)–based quantification of a set of chemicals. The size of the QSAR fingerprint for the molecules is the same in all such datasets (1024 features): therefore, we can assume that the estimate of ID will not vary too much across the datasets from the same group. We computed the coefficient of a variation of ID estimates across three such dataset groups, which revealed that certain methods tend to provide less stable estimations than the others; <xref rid="entropy-23-01368-f002" ref-type="fig">Figure 2</xref>D.</p>
        <p>Finally, we recorded the computational time needed for each method. We found that the computational time could be estimated with good precision (<inline-formula><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0.93</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> for all ID estimators), using the multiplicative model: <inline-formula><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mo>×</mml:mo><mml:msubsup><mml:mi>N</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>α</mml:mi></mml:msubsup><mml:mo>×</mml:mo><mml:msubsup><mml:mi>N</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mi>β</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are the number of objects and features in a dataset, correspondingly. Using this model fit for each method, we estimated the time needed to estimate ID for data matrices of four characteristic sizes; <xref rid="entropy-23-01368-f002" ref-type="fig">Figure 2</xref>E.</p>
      </sec>
      <sec id="sec3dot1dot2-entropy-23-01368">
        <title>3.1.2. Metanalysis of <monospace>Scikit-Dimension</monospace> ID Estimates</title>
        <p>After application of <monospace>scikit-dimension</monospace>, each dataset was characterized by a vector of 19 measurements of intrinsic dimensionality. The resulting matrix of ID values contained 2.5% missing values, which were imputed, using the standard IterativeImputer from the <monospace>sklearn</monospace> Python package.</p>
        <p>Using the imputed matrix and scaling it to z-scores, we performed principal component analysis (<xref rid="entropy-23-01368-f003" ref-type="fig">Figure 3</xref>A,B). The first principal component explained 42.6% percent of the total variance in ID estimations, with all of the methods having positive and comparable loadings to the first principal component. This justifies the computation of the “consensus” intrinsic dimension measure, which we define here as the mean value of individual ID estimate z-scores. Therefore, the mean ID can take negative or positive values, roughly dividing the datasets into “lower-dimensional” and “higher-dimensional” (<xref rid="entropy-23-01368-f003" ref-type="fig">Figure 3</xref>A,C). The consensus ID estimate weakly negatively correlated with the number of observations (Pearson <inline-formula><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>0.25</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <italic toggle="yes">p</italic>-value = <inline-formula><mml:math id="mm9" display="block" overflow="scroll"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>) and positively correlated with the number of features in the dataset (r = 0.44, <italic toggle="yes">p</italic>-value = <inline-formula><mml:math id="mm10" display="block" overflow="scroll"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>25</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>). Nevertheless, even for the datasets with similar matrix shapes, the mean ID estimate could be quite different (<xref rid="entropy-23-01368-f003" ref-type="fig">Figure 3</xref>C).</p>
        <p>The second principal component explained 21.3% of the total variance in ID estimates. The loadings of this component roughly differentiated between PCA-based ID estimates and non-linear ID estimation methods, with one exception in the case of the KNN method.</p>
        <p>We computed the correlation matrix between the results of application of different ID methods (<xref rid="entropy-23-01368-f003" ref-type="fig">Figure 3</xref>D), which also distinguished two large groups of PCA-based and “non-linear” methods. Furthermore, non-linear methods were split into the group of methods, producing results similar to the correlation (fractal) dimension (CorrInt, MADA, MOM, TwoNN, MLE, TLE) and methods based on the concentration of measure phenomena (FisherS, ESS, DANCo, MiND_ML).</p>
        <p>In order to illustrate the relation between the dataset geometry and the intrinsic dimension, we produced a gallery of uniform manifold approximation and projection (UMAP) dataset visualizations, with an indication of the ambient dataset dimension (number of features) and the estimated ID, using all methods; <xref rid="entropy-23-01368-f004" ref-type="fig">Figure 4</xref>. One of the conclusions that can be made from this analysis is that the UMAP visualization is not insightful for truly high-dimensional datasets (starting from ID = 10, estimated by the FisherS method). In addition, some datasets, having large ambient dimensions, were characterized with a low ID by most of the methods (e.g., ‘hill-valley’ dataset).</p>
      </sec>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec4-entropy-23-01368">
    <title>4. Conclusions</title>
    <p><monospace>scikit-dimension</monospace> is to our knowledge the first package implemented in Python, containing implementations of the most-used estimators of ID.</p>
    <p>Benchmarking <monospace>scikit-dimension</monospace> on a large collection of real-life and synthetic datasets revealed that different estimators of ID possess internal consistency and that the ensemble of ID estimators allows us to achieve more robust classification of datasets into low- or high-dimensionality.</p>
    <p>The estimation of intrinsic dimensionality of a dataset is essential in various applications of machine learning to real-life data. We can mention here several typical use cases, where the <monospace>scikit-dimension</monospace> package can be used, but this description is by no means comprehensive.</p>
    <p>Firstly, learning low-dimensional data geometry (e.g., learning data manifolds or more complex geometries, such as principal graphs [<xref rid="B60-entropy-23-01368" ref-type="bibr">60</xref>,<xref rid="B61-entropy-23-01368" ref-type="bibr">61</xref>]) frequently requires preliminary data dimensionality reduction for which one has to estimate the ‘true’ global and local data dimensionality. For example, in the analysis of single-cell data in biology, the inference of so-called cellular trajectories can give different results when more or less principal data dimensions are kept. In higher dimensions, more cell fate decisions can be distinguished, but their inference becomes less robust [<xref rid="B62-entropy-23-01368" ref-type="bibr">62</xref>,<xref rid="B63-entropy-23-01368" ref-type="bibr">63</xref>]. Some advanced methods of unsupervised learning, such as quantifying the data manifold curvature, require knowledge of data ID [<xref rid="B64-entropy-23-01368" ref-type="bibr">64</xref>]. In mathematical modeling of biological and other complex systems, it is frequently important to estimate the effective dimensionality of the dynamical process, from the data or from simulations, in order to inform model reduction [<xref rid="B17-entropy-23-01368" ref-type="bibr">17</xref>,<xref rid="B65-entropy-23-01368" ref-type="bibr">65</xref>,<xref rid="B66-entropy-23-01368" ref-type="bibr">66</xref>]. In medical applications and in the analysis of clinical data, knowledge of consensus data dimensionality was shown to be important to distinguish signal from noise and predict patient trajectories [<xref rid="B16-entropy-23-01368" ref-type="bibr">16</xref>].</p>
    <p>Secondly, high-dimensional data geometry is a rapidly evolving field in machine learning [<xref rid="B67-entropy-23-01368" ref-type="bibr">67</xref>,<xref rid="B68-entropy-23-01368" ref-type="bibr">68</xref>,<xref rid="B69-entropy-23-01368" ref-type="bibr">69</xref>]. To know whether the recent theoretical results can be used in practice, one has to estimate the ID of a concrete dataset. More generally, it is important to know if an application of a machine learning method to a dataset will face various types of difficulties, known as the curse of dimensionality. For example, it was shown that, under appropriate assumptions, robustness of general multi-class classifiers to adversarial examples can be achieved only if the intrinsic dimensionality of the AI’s decision variables is sufficiently small [<xref rid="B70-entropy-23-01368" ref-type="bibr">70</xref>]. Knowledge of ID can be important to decide if one can benefit from the blessing of dimensionality in the problem of correcting the AI’s errors when deploying large, pre-trained legacy neural network models [<xref rid="B32-entropy-23-01368" ref-type="bibr">32</xref>,<xref rid="B71-entropy-23-01368" ref-type="bibr">71</xref>]. Estimating data dimensionality can suggest the application of specific data pre-processing methods, such as hubness reduction of point neighborhood graphs, in the tasks of clustering or non-linear dimensionality reduction [<xref rid="B72-entropy-23-01368" ref-type="bibr">72</xref>]. In a recent study, estimating dataset ID was used to show that some old ideas on fighting the curse of dimensionality by modifying global data metrics are not efficient in practice [<xref rid="B15-entropy-23-01368" ref-type="bibr">15</xref>]. In this respect, explicit control of the ID of AI models’ latent spaces appears to be crucial for developing robust and reliable AI. Our work adds to the spectrum of tools to achieve this aim.</p>
    <p>Thirdly, local ID can be used to partition a data point cloud in a way that is complementary to standard clustering [<xref rid="B73-entropy-23-01368" ref-type="bibr">73</xref>]. In 3D, this approach can be used for object detection (see <xref rid="entropy-23-01368-f001" ref-type="fig">Figure 1</xref>), but it can be generalized for higher-dimensional data point clouds. Interestingly, local ID can be related to various object characteristics in various domains: folded versus unfolded configurations in a protein molecular dynamics trajectory, active versus non-active regions in brain imaging data, and firms with different financial risk in company balance sheets [<xref rid="B74-entropy-23-01368" ref-type="bibr">74</xref>].</p>
    <p>Future releases of <monospace>scikit-dimension</monospace> will continuously seek to incorporate new estimators and benchmark datasets introduced in the literature, or new features, such as alternative nearest neighbor search for local ID estimates. The package will also include new ID estimators, which can be derived using the most recent achievements in understanding the properties of high-dimensional data geometry [<xref rid="B71-entropy-23-01368" ref-type="bibr">71</xref>,<xref rid="B75-entropy-23-01368" ref-type="bibr">75</xref>].</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p><bold>Publisher’s Note:</bold> MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <notes>
    <title>Author Contributions</title>
    <p>Conceptualization, J.B., A.Z., I.T., A.N.G.; methodology, J.B., E.M.M., A.N.G.; software, J.B., E.M.M. and A.Z.; formal analysis, J.B., E.M.M., A.Z.; data curation, J.B. and A.Z.; writing—original draft preparation, J.B. and A.Z.; writing—review and editing, all authors; supervision, A.Z. and A.N.G. All authors have read and agreed to the published version of the manuscript.</p>
  </notes>
  <notes>
    <title>Funding</title>
    <p>The work was supported by the Ministry of Science and Higher Education of the Russian Federation (Project No. 075-15-2020-927), by the French government under management of Agence Nationale de la Recherche as part of the “Investissements d’Avenir" program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute), by the Association Science et Technologie, the Institut de Recherches Internationales Servier and the doctoral school Frontières de l’Innovation en Recherche et Education Programme Bettencourt. I.T. was supported by the UKRI Turing AI Acceleration Fellowship (EP/V025295/1).</p>
  </notes>
  <notes>
    <title>Institutional Review Board Statement</title>
    <p>Not applicable.</p>
  </notes>
  <notes>
    <title>Informed Consent Statement</title>
    <p>Not applicable.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data Availability Statement</title>
    <p>The datasets used in this study were retrieved from public sources, namely the <monospace>OpenML</monospace> repository, CytoTRACE website <uri xlink:href="https://cytotrace.stanford.edu/">https://cytotrace.stanford.edu/</uri> (section “Downloads”, accessed on 18 October 2021), from the Data Portal of National Cancer Institute <uri xlink:href="https://portal.gdc.cancer.gov/">https://portal.gdc.cancer.gov/</uri> (accessed on 18 October 2021).</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Conflicts of Interest</title>
    <p>The authors declare no conflict of interest.</p>
  </notes>
  <ref-list>
    <title>References</title>
    <ref id="B1-entropy-23-01368">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pedregosa</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Varoquaux</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Gramfort</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Michel</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Thirion</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Grisel</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Blondel</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Prettenhofer</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Weiss</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Dubourg</surname>
            <given-names>V.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Scikit-learn: Machine learning in Python</article-title>
        <source>J. Mach. Learn. Res.</source>
        <year>2011</year>
        <volume>12</volume>
        <fpage>2825</fpage>
        <lpage>2830</lpage>
      </element-citation>
    </ref>
    <ref id="B2-entropy-23-01368">
      <label>2.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Bishop</surname>
            <given-names>C.M.</given-names>
          </name>
        </person-group>
        <source>Neural Networks for Pattern Recognition</source>
        <publisher-name>Oxford University Press</publisher-name>
        <publisher-loc>Oxford, UK</publisher-loc>
        <year>1995</year>
        <pub-id pub-id-type="doi">10.5555/525960</pub-id>
      </element-citation>
    </ref>
    <ref id="B3-entropy-23-01368">
      <label>3.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Fukunaga</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Intrinsic dimensionality extraction</article-title>
        <source>Pattern Recognition and Reduction of Dimensionality, Handbook of Statistics</source>
        <person-group person-group-type="editor">
          <name>
            <surname>Krishnaiah</surname>
            <given-names>P.R.</given-names>
          </name>
          <name>
            <surname>Kanal</surname>
            <given-names>L.N.</given-names>
          </name>
        </person-group>
        <publisher-name>North-Holland</publisher-name>
        <publisher-loc>Amsterdam, The Netherlands</publisher-loc>
        <year>1982</year>
        <volume>Volume 2</volume>
        <fpage>347</fpage>
        <lpage>362</lpage>
      </element-citation>
    </ref>
    <ref id="B4-entropy-23-01368">
      <label>4.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Albergante</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Bac</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zinovyev</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Estimating the effective dimension of large biological datasets using Fisher separability analysis</article-title>
        <source>Proceedings of the 2019 International Joint Conference on Neural Networks (IJCNN)</source>
        <conf-loc>Budapest, Hungary</conf-loc>
        <conf-date>14–19 July 2019</conf-date>
        <fpage>1</fpage>
        <lpage>8</lpage>
      </element-citation>
    </ref>
    <ref id="B5-entropy-23-01368">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Giudice</surname>
            <given-names>M.D.</given-names>
          </name>
        </person-group>
        <article-title>Effective Dimensionality: A Tutorial</article-title>
        <source>Multivar. Behav. Res.</source>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>16</lpage>
        <pub-id pub-id-type="doi">10.1080/00273171.2020.1743631</pub-id>
      </element-citation>
    </ref>
    <ref id="B6-entropy-23-01368">
      <label>6.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Palla</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Knowles</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Ghahramani</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>A nonparametric variable clustering model</article-title>
        <source>Advances in Neural Information Processing Systems</source>
        <publisher-name>MIT Press</publisher-name>
        <publisher-loc>Cambridge, MA, USA</publisher-loc>
        <year>2012</year>
        <volume>Volume 4</volume>
        <fpage>2987</fpage>
        <lpage>2995</lpage>
      </element-citation>
    </ref>
    <ref id="B7-entropy-23-01368">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Giuliani</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Benigni</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Sirabella</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Zbilut</surname>
            <given-names>J.P.</given-names>
          </name>
          <name>
            <surname>Colosimo</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Nonlinear Methods in the Analysis of Protein Sequences: A Case Study in Rubredoxins</article-title>
        <source>Biophys. J.</source>
        <year>2000</year>
        <volume>78</volume>
        <fpage>136</fpage>
        <lpage>149</lpage>
        <pub-id pub-id-type="doi">10.1016/S0006-3495(00)76580-5</pub-id>
        <pub-id pub-id-type="pmid">10620281</pub-id>
      </element-citation>
    </ref>
    <ref id="B8-entropy-23-01368">
      <label>8.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Jiang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Guan</surname>
            <given-names>M.Y.</given-names>
          </name>
          <name>
            <surname>Gupta</surname>
            <given-names>M.R.</given-names>
          </name>
        </person-group>
        <article-title>To Trust Or Not To Trust A Classifier</article-title>
        <source>NeurIPS</source>
        <publisher-name>Montreal Convention Centre</publisher-name>
        <publisher-loc>Montreal, QC, Canada</publisher-loc>
        <year>2018</year>
        <fpage>5546</fpage>
        <lpage>5557</lpage>
        <pub-id pub-id-type="doi">10.5555/3327345.3327458</pub-id>
      </element-citation>
    </ref>
    <ref id="B9-entropy-23-01368">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bac</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zinovyev</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Lizard Brain: Tackling Locally Low-Dimensional Yet Globally Complex Organization of Multi-Dimensional Datasets</article-title>
        <source>Front. Neurorobotics</source>
        <year>2020</year>
        <volume>13</volume>
        <fpage>110</fpage>
        <pub-id pub-id-type="doi">10.3389/fnbot.2019.00110</pub-id>
      </element-citation>
    </ref>
    <ref id="B10-entropy-23-01368">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hino</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>ider: Intrinsic Dimension Estimation with R</article-title>
        <source>R J.</source>
        <year>2017</year>
        <volume>9</volume>
        <fpage>329</fpage>
        <lpage>341</lpage>
        <pub-id pub-id-type="doi">10.32614/RJ-2017-054</pub-id>
      </element-citation>
    </ref>
    <ref id="B11-entropy-23-01368">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Campadelli</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Casiraghi</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Ceruti</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Rozza</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Intrinsic Dimension Estimation: Relevant Techniques and a Benchmark Framework</article-title>
        <source>Math. Probl. Eng.</source>
        <year>2015</year>
        <volume>2015</volume>
        <fpage>759567</fpage>
        <pub-id pub-id-type="doi">10.1155/2015/759567</pub-id>
      </element-citation>
    </ref>
    <ref id="B12-entropy-23-01368">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Camastra</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Staiano</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Intrinsic dimension estimation: Advances and open problems</article-title>
        <source>Inf. Sci.</source>
        <year>2016</year>
        <volume>328</volume>
        <fpage>26</fpage>
        <lpage>41</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ins.2015.08.029</pub-id>
      </element-citation>
    </ref>
    <ref id="B13-entropy-23-01368">
      <label>13.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Little</surname>
            <given-names>A.V.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Jung</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Maggioni</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Estimation of intrinsic dimensionality of samples from noisy low-dimensional manifolds in high dimensions with multiscale SVD</article-title>
        <source>Proceedings of the 2009 IEEE/SP 15th Workshop on Statistical Signal Processing</source>
        <conf-loc>Cardiff, UK</conf-loc>
        <conf-date>31 August–3 September 2009</conf-date>
        <fpage>85</fpage>
        <lpage>88</lpage>
        <pub-id pub-id-type="doi">10.1109/SSP.2009.5278634</pub-id>
      </element-citation>
    </ref>
    <ref id="B14-entropy-23-01368">
      <label>14.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Hein</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Audibert</surname>
            <given-names>J.Y.</given-names>
          </name>
        </person-group>
        <article-title>Intrinsic dimensionality estimation of submanifolds in <italic toggle="yes">R</italic><sup>d</sup></article-title>
        <source>Proceedings of the 22nd International Conference on Machine Learning</source>
        <conf-loc>Bonn, Germany</conf-loc>
        <conf-date>7–11 August 2005</conf-date>
        <publisher-name>ACM</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2005</year>
        <fpage>289</fpage>
        <lpage>296</lpage>
        <pub-id pub-id-type="doi">10.1145/1102351.1102388</pub-id>
      </element-citation>
    </ref>
    <ref id="B15-entropy-23-01368">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mirkes</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Allohibi</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Gorban</surname>
            <given-names>A.N.</given-names>
          </name>
        </person-group>
        <article-title>Fractional Norms and Quasinorms Do Not Help to Overcome the Curse of Dimensionality</article-title>
        <source>Entropy</source>
        <year>2020</year>
        <volume>22</volume>
        <elocation-id>1105</elocation-id>
        <pub-id pub-id-type="doi">10.3390/e22101105</pub-id>
        <?supplied-pmid 33286874?>
        <pub-id pub-id-type="pmid">33286874</pub-id>
      </element-citation>
    </ref>
    <ref id="B16-entropy-23-01368">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Golovenkin</surname>
            <given-names>S.E.</given-names>
          </name>
          <name>
            <surname>Bac</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Chervov</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Mirkes</surname>
            <given-names>E.M.</given-names>
          </name>
          <name>
            <surname>Orlova</surname>
            <given-names>Y.V.</given-names>
          </name>
          <name>
            <surname>Barillot</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Gorban</surname>
            <given-names>A.N.</given-names>
          </name>
          <name>
            <surname>Zinovyev</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Trajectories, bifurcations, and pseudo-time in large clinical datasets: Applications to myocardial infarction and diabetes data</article-title>
        <source>GigaScience</source>
        <year>2020</year>
        <volume>9</volume>
        <fpage>giaa128</fpage>
        <pub-id pub-id-type="doi">10.1093/gigascience/giaa128</pub-id>
        <?supplied-pmid 33241287?>
        <pub-id pub-id-type="pmid">33241287</pub-id>
      </element-citation>
    </ref>
    <ref id="B17-entropy-23-01368">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zinovyev</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Sadovsky</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Calzone</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Fouché</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Groeneveld</surname>
            <given-names>C.S.</given-names>
          </name>
          <name>
            <surname>Chervov</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Barillot</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Gorban</surname>
            <given-names>A.N.</given-names>
          </name>
        </person-group>
        <article-title>Modeling Progression of Single Cell Populations Through the Cell Cycle as a Sequence of Switches</article-title>
        <source>bioRxiv</source>
        <year>2021</year>
        <pub-id pub-id-type="doi">10.1101/2021.06.14.448414</pub-id>
      </element-citation>
    </ref>
    <ref id="B18-entropy-23-01368">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Grassberger</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Procaccia</surname>
            <given-names>I.</given-names>
          </name>
        </person-group>
        <article-title>Measuring the strangeness of strange attractors</article-title>
        <source>Phys. D Nonlinear Phenom.</source>
        <year>1983</year>
        <volume>9</volume>
        <fpage>189</fpage>
        <lpage>208</lpage>
        <pub-id pub-id-type="doi">10.1016/0167-2789(83)90298-1</pub-id>
      </element-citation>
    </ref>
    <ref id="B19-entropy-23-01368">
      <label>19.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Farahmand</surname>
            <given-names>A.M.</given-names>
          </name>
          <name>
            <surname>Szepesvári</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Audibert</surname>
            <given-names>J.Y.</given-names>
          </name>
        </person-group>
        <article-title>Manifold-adaptive dimension estimation</article-title>
        <source>Proceedings of the 24th International Conference on Machine Learning</source>
        <conf-loc>Corvallis, OR, USA</conf-loc>
        <conf-date>20–24 June 2007</conf-date>
        <fpage>265</fpage>
        <lpage>272</lpage>
        <pub-id pub-id-type="doi">10.1145/1273496.1273530</pub-id>
      </element-citation>
    </ref>
    <ref id="B20-entropy-23-01368">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Amsaleg</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Chelly</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Furon</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Girard</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Houle</surname>
            <given-names>M.E.</given-names>
          </name>
          <name>
            <surname>Kawarabayashi</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Nett</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Extreme-value-theoretic estimation of local intrinsic dimensionality</article-title>
        <source>Data Min. Knowl. Discov.</source>
        <year>2018</year>
        <volume>32</volume>
        <fpage>1768</fpage>
        <lpage>1805</lpage>
        <pub-id pub-id-type="doi">10.1007/s10618-018-0578-6</pub-id>
      </element-citation>
    </ref>
    <ref id="B21-entropy-23-01368">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jackson</surname>
            <given-names>D.A.</given-names>
          </name>
        </person-group>
        <article-title>Stopping rules in principal components analysis: A comparison of heuristical and statistical approaches</article-title>
        <source>Ecology</source>
        <year>1993</year>
        <volume>74</volume>
        <fpage>2204</fpage>
        <lpage>2214</lpage>
        <pub-id pub-id-type="doi">10.2307/1939574</pub-id>
      </element-citation>
    </ref>
    <ref id="B22-entropy-23-01368">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fukunaga</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Olsen</surname>
            <given-names>D.R.</given-names>
          </name>
        </person-group>
        <article-title>An Algorithm for Finding Intrinsic Dimensionality of Data</article-title>
        <source>IEEE Trans. Comput.</source>
        <year>1971</year>
        <volume>C-20</volume>
        <fpage>176</fpage>
        <lpage>183</lpage>
        <pub-id pub-id-type="doi">10.1109/T-C.1971.223208</pub-id>
      </element-citation>
    </ref>
    <ref id="B23-entropy-23-01368">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mingyu</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Gu</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Qiao</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>Intrinsic dimension estimation of data by principal component analysis</article-title>
        <source>arXiv</source>
        <year>2010</year>
        <pub-id pub-id-type="arxiv">1002.2050</pub-id>
      </element-citation>
    </ref>
    <ref id="B24-entropy-23-01368">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hill</surname>
            <given-names>B.M.</given-names>
          </name>
        </person-group>
        <article-title>A simple general approach to inference about the tail of a distribution</article-title>
        <source>Ann. Stat.</source>
        <year>1975</year>
        <fpage>1163</fpage>
        <lpage>1174</lpage>
        <pub-id pub-id-type="doi">10.1214/aos/1176343247</pub-id>
      </element-citation>
    </ref>
    <ref id="B25-entropy-23-01368">
      <label>25.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Levina</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Bickel</surname>
            <given-names>P.J.</given-names>
          </name>
        </person-group>
        <article-title>Maximum Likelihood estimation of intrinsic dimension</article-title>
        <source>Proceedings of the 17th International Conference on Neural Information Processing Systems, Vancouver, Canada, 1 December 2004</source>
        <publisher-name>MIT Press</publisher-name>
        <publisher-loc>Cambridge, MA, USA</publisher-loc>
        <year>2004</year>
        <fpage>777</fpage>
        <lpage>784</lpage>
        <pub-id pub-id-type="doi">10.5555/2976040.2976138</pub-id>
      </element-citation>
    </ref>
    <ref id="B26-entropy-23-01368">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Haro</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Randall</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Sapiro</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Translated poisson mixture model for stratification learning</article-title>
        <source>Int. J. Comput. Vis.</source>
        <year>2008</year>
        <volume>80</volume>
        <fpage>358</fpage>
        <lpage>374</lpage>
        <pub-id pub-id-type="doi">10.1007/s11263-008-0144-6</pub-id>
      </element-citation>
    </ref>
    <ref id="B27-entropy-23-01368">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Carter</surname>
            <given-names>K.M.</given-names>
          </name>
          <name>
            <surname>Raich</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Hero</surname>
            <given-names>A.O.</given-names>
          </name>
        </person-group>
        <article-title>On Local Intrinsic Dimension Estimation and Its Applications</article-title>
        <source>IEEE Trans. Signal Process.</source>
        <year>2010</year>
        <volume>58</volume>
        <fpage>650</fpage>
        <lpage>663</lpage>
        <pub-id pub-id-type="doi">10.1109/TSP.2009.2031722</pub-id>
      </element-citation>
    </ref>
    <ref id="B28-entropy-23-01368">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rozza</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Lombardi</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Ceruti</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Casiraghi</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Campadelli</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Novel high intrinsic dimensionality estimators</article-title>
        <source>Mach. Learn.</source>
        <year>2012</year>
        <volume>89</volume>
        <fpage>37</fpage>
        <lpage>65</lpage>
        <pub-id pub-id-type="doi">10.1007/s10994-012-5294-7</pub-id>
      </element-citation>
    </ref>
    <ref id="B29-entropy-23-01368">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ceruti</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Bassis</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Rozza</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Lombardi</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Casiraghi</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Campadelli</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>DANCo: An intrinsic dimensionality estimator exploiting angle and norm concentration</article-title>
        <source>Pattern Recognit.</source>
        <year>2014</year>
        <volume>47</volume>
        <fpage>2569</fpage>
        <lpage>2581</lpage>
        <pub-id pub-id-type="doi">10.1016/j.patcog.2014.02.013</pub-id>
      </element-citation>
    </ref>
    <ref id="B30-entropy-23-01368">
      <label>30.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Johnsson</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Structures in High-Dimensional Data: Intrinsic Dimension and Cluster Analysis</article-title>
        <source>Ph.D. Thesis</source>
        <publisher-name>Faculty of Engineering, LTH</publisher-name>
        <publisher-loc>Perth, Australia</publisher-loc>
        <year>2016</year>
      </element-citation>
    </ref>
    <ref id="B31-entropy-23-01368">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Facco</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>D’Errico</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Rodriguez</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Laio</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Estimating the intrinsic dimension of datasets by a minimal neighborhood information</article-title>
        <source>Sci. Rep.</source>
        <year>2017</year>
        <volume>7</volume>
        <fpage>12140</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-017-11873-y</pub-id>
        <?supplied-pmid 28939866?>
        <pub-id pub-id-type="pmid">28939866</pub-id>
      </element-citation>
    </ref>
    <ref id="B32-entropy-23-01368">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gorban</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Golubkov</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Grechuk</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Mirkes</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Tyukin</surname>
            <given-names>I.</given-names>
          </name>
        </person-group>
        <article-title>Correction of AI systems by linear discriminants: Probabilistic foundations</article-title>
        <source>Inf. Sci.</source>
        <year>2018</year>
        <volume>466</volume>
        <fpage>303</fpage>
        <lpage>322</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ins.2018.07.040</pub-id>
      </element-citation>
    </ref>
    <ref id="B33-entropy-23-01368">
      <label>33.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Amsaleg</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Chelly</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Houle</surname>
            <given-names>M.E.</given-names>
          </name>
          <name>
            <surname>Kawarabayashi</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Radovanović</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Treeratanajaru</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <article-title>Intrinsic dimensionality estimation within tight localities</article-title>
        <source>Proceedings of the 2019 SIAM International Conference on Data Mining</source>
        <conf-loc>Calgary, AB, Canada</conf-loc>
        <conf-date>2–4 May 2019</conf-date>
        <publisher-name>SIAM</publisher-name>
        <publisher-loc>Philadelphia, PA, USA</publisher-loc>
        <year>2019</year>
        <fpage>181</fpage>
        <lpage>189</lpage>
      </element-citation>
    </ref>
    <ref id="B34-entropy-23-01368">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Harris</surname>
            <given-names>C.R.</given-names>
          </name>
          <name>
            <surname>Millman</surname>
            <given-names>K.J.</given-names>
          </name>
          <name>
            <surname>van der Walt</surname>
            <given-names>S.J.</given-names>
          </name>
          <name>
            <surname>Gommers</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Virtanen</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Cournapeau</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Wieser</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Taylor</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Berg</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>N.J.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Array programming with NumPy</article-title>
        <source>Nature</source>
        <year>2020</year>
        <volume>585</volume>
        <fpage>357</fpage>
        <lpage>362</lpage>
        <pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id>
        <pub-id pub-id-type="pmid">32939066</pub-id>
      </element-citation>
    </ref>
    <ref id="B35-entropy-23-01368">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hunter</surname>
            <given-names>J.D.</given-names>
          </name>
        </person-group>
        <article-title>Matplotlib: A 2D graphics environment</article-title>
        <source>Comput. Sci. Eng.</source>
        <year>2007</year>
        <volume>9</volume>
        <fpage>90</fpage>
        <lpage>95</lpage>
        <pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id>
      </element-citation>
    </ref>
    <ref id="B36-entropy-23-01368">
      <label>36.</label>
      <element-citation publication-type="webpage">
        <article-title>The Pandas Development Team.Pandas-Dev/Pandas: Pandas 1.3.4, Zenodo</article-title>
        <comment>Available online: <ext-link xlink:href="https://zenodo.org/record/5574486#.YW50jhpByUk" ext-link-type="uri">https://zenodo.org/record/5574486#.YW50jhpByUk</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2021-10-18">(accessed on 18 October 2021)</date-in-citation>
        <pub-id pub-id-type="doi">10.5281/zenodo.3509134</pub-id>
      </element-citation>
    </ref>
    <ref id="B37-entropy-23-01368">
      <label>37.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Lam</surname>
            <given-names>S.K.</given-names>
          </name>
          <name>
            <surname>Pitrou</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Seibert</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Numba: A llvm-based python jit compiler</article-title>
        <source>Proceedings of the Second Workshop on the LLVM Compiler Infrastructure in HPC</source>
        <conf-loc>Austin, TX, USA</conf-loc>
        <conf-date>15 November 2015</conf-date>
        <fpage>1</fpage>
        <lpage>6</lpage>
        <pub-id pub-id-type="doi">10.1145/2833157.2833162</pub-id>
      </element-citation>
    </ref>
    <ref id="B38-entropy-23-01368">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Virtanen</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Gommers</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Oliphant</surname>
            <given-names>T.E.</given-names>
          </name>
          <name>
            <surname>Haberland</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Reddy</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Cournapeau</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Burovski</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Peterson</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Weckesser</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Bright</surname>
            <given-names>J.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python</article-title>
        <source>Nat. Methods</source>
        <year>2020</year>
        <volume>17</volume>
        <fpage>261</fpage>
        <lpage>272</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id>
        <?supplied-pmid 32015543?>
        <pub-id pub-id-type="pmid">32015543</pub-id>
      </element-citation>
    </ref>
    <ref id="B39-entropy-23-01368">
      <label>39.</label>
      <element-citation publication-type="webpage">
        <person-group person-group-type="author">
          <name>
            <surname>Johnsson</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>intrinsicDimension: Intrinsic Dimension Estimation (R Package)</article-title>
        <year>2019</year>
        <comment>Available online: <ext-link xlink:href="https://rdrr.io/cran/intrinsicDimension/" ext-link-type="uri">https://rdrr.io/cran/intrinsicDimension/</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2021-09-06">(accessed on 6 September 2021)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B40-entropy-23-01368">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>You</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Rdimtools: An R package for Dimension Reduction and Intrinsic Dimension Estimation</article-title>
        <source>arXiv</source>
        <year>2020</year>
        <pub-id pub-id-type="arxiv">2005.11107</pub-id>
      </element-citation>
    </ref>
    <ref id="B41-entropy-23-01368">
      <label>41.</label>
      <element-citation publication-type="journal">
        <article-title>Denti, Francesco intRinsic: An R package for model-based estimation of the intrinsic dimension of a dataset</article-title>
        <source>arXiv</source>
        <year>2021</year>
        <pub-id pub-id-type="arxiv">2102.11425</pub-id>
      </element-citation>
    </ref>
    <ref id="B42-entropy-23-01368">
      <label>42.</label>
      <element-citation publication-type="webpage">
        <person-group person-group-type="author">
          <name>
            <surname>Hein</surname>
            <given-names>M.J.Y.A.</given-names>
          </name>
        </person-group>
        <article-title>IntDim: Intrindic Dimensionality Estimation</article-title>
        <year>2016</year>
        <comment>Available online: <ext-link xlink:href="https://www.ml.uni-saarland.de/code/IntDim/IntDim.htm" ext-link-type="uri">https://www.ml.uni-saarland.de/code/IntDim/IntDim.htm</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2021-09-06">(accessed on 6 September 2021)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B43-entropy-23-01368">
      <label>43.</label>
      <element-citation publication-type="webpage">
        <person-group person-group-type="author">
          <name>
            <surname>Lombardi</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Intrinsic Dimensionality Estimation Techniques (MATLAB Package)</article-title>
        <year>2013</year>
        <comment>Available online: <ext-link xlink:href="https://fr.mathworks.com/matlabcentral/fileexchange/40112-intrinsic-dimensionality-estimation-techniques" ext-link-type="uri">https://fr.mathworks.com/matlabcentral/fileexchange/40112-intrinsic-dimensionality-estimation-techniques</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2021-09-06">(accessed on 6 September 2021)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B44-entropy-23-01368">
      <label>44.</label>
      <element-citation publication-type="webpage">
        <person-group person-group-type="author">
          <name>
            <surname>Van der Maaten</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Drtoolbox: Matlab Toolbox for Dimensionality Reduction</article-title>
        <year>2020</year>
        <comment>Available online: <ext-link xlink:href="https://lvdmaaten.github.io/drtoolbox/" ext-link-type="uri">https://lvdmaaten.github.io/drtoolbox/</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2021-09-06">(accessed on 6 September 2021)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B45-entropy-23-01368">
      <label>45.</label>
      <element-citation publication-type="webpage">
        <person-group person-group-type="author">
          <name>
            <surname>Radovanović</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Tight Local Intrinsic Dimensionality Estimator (TLE) (MATLAB Package)</article-title>
        <year>2020</year>
        <comment>Available online: <ext-link xlink:href="https://perun.pmf.uns.ac.rs/radovanovic/tle/" ext-link-type="uri">https://perun.pmf.uns.ac.rs/radovanovic/tle/</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2021-09-06">(accessed on 6 September 2021)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B46-entropy-23-01368">
      <label>46.</label>
      <element-citation publication-type="webpage">
        <person-group person-group-type="author">
          <name>
            <surname>Gomtsyan</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Mokrov</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Panov</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Yanovich</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Geometry-Aware Maximum Likelihood Estimation of Intrinsic Dimension (Python Package)</article-title>
        <year>2019</year>
        <comment>Available online: <ext-link xlink:href="https://github.com/stat-ml/GeoMLE" ext-link-type="uri">https://github.com/stat-ml/GeoMLE</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2021-09-06">(accessed on 6 September 2021)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B47-entropy-23-01368">
      <label>47.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Gomtsyan</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Mokrov</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Panov</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Yanovich</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Geometry-Aware Maximum Likelihood Estimation of Intrinsic Dimension</article-title>
        <source>Proceedings of the Eleventh Asian Conference on Machine Learning</source>
        <conf-loc>Nagoya, Japan</conf-loc>
        <conf-date>17–19 November 2019</conf-date>
        <fpage>1126</fpage>
        <lpage>1141</lpage>
      </element-citation>
    </ref>
    <ref id="B48-entropy-23-01368">
      <label>48.</label>
      <element-citation publication-type="webpage">
        <person-group person-group-type="author">
          <name>
            <surname>Erba</surname>
            <given-names>V.</given-names>
          </name>
        </person-group>
        <article-title>pyFCI: A Package for Multiscale-Full-Correlation-Integral Intrinsic Dimension Estimation</article-title>
        <year>2019</year>
        <comment>Available online: <ext-link xlink:href="https://github.com/vittorioerba/pyFCI" ext-link-type="uri">https://github.com/vittorioerba/pyFCI</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2021-09-06">(accessed on 6 September 2021)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B49-entropy-23-01368">
      <label>49.</label>
      <element-citation publication-type="webpage">
        <person-group person-group-type="author">
          <name>
            <surname>Granata</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>Intrinsic-Dimension (Python Package)</article-title>
        <year>2016</year>
        <comment>Available online: <ext-link xlink:href="https://github.com/dgranata/Intrinsic-Dimension" ext-link-type="uri">https://github.com/dgranata/Intrinsic-Dimension</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2021-09-06">(accessed on 6 September 2021)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B50-entropy-23-01368">
      <label>50.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Bac</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zinovyev</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Local intrinsic dimensionality estimators based on concentration of measure</article-title>
        <source>Proceedings of the International Joint Conference on Neural Networks (IJCNN)</source>
        <conf-loc>Glasgow, UK</conf-loc>
        <conf-date>19–24 July 2020</conf-date>
        <fpage>1</fpage>
        <lpage>8</lpage>
      </element-citation>
    </ref>
    <ref id="B51-entropy-23-01368">
      <label>51.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gorban</surname>
            <given-names>A.N.</given-names>
          </name>
          <name>
            <surname>Makarov</surname>
            <given-names>V.A.</given-names>
          </name>
          <name>
            <surname>Tyukin</surname>
            <given-names>I.Y.</given-names>
          </name>
        </person-group>
        <article-title>The unreasonable effectiveness of small neural ensembles in high-dimensional brain</article-title>
        <source>Phys. Life Rev.</source>
        <year>2019</year>
        <volume>29</volume>
        <fpage>55</fpage>
        <lpage>88</lpage>
        <pub-id pub-id-type="doi">10.1016/j.plrev.2018.09.005</pub-id>
        <pub-id pub-id-type="pmid">30366739</pub-id>
      </element-citation>
    </ref>
    <ref id="B52-entropy-23-01368">
      <label>52.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vanschoren</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>van Rijn</surname>
            <given-names>J.N.</given-names>
          </name>
          <name>
            <surname>Bischl</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Torgo</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>OpenML: Networked Science in Machine Learning</article-title>
        <source>SIGKDD Explor.</source>
        <year>2013</year>
        <volume>15</volume>
        <fpage>49</fpage>
        <lpage>60</lpage>
        <pub-id pub-id-type="doi">10.1145/2641190.2641198</pub-id>
      </element-citation>
    </ref>
    <ref id="B53-entropy-23-01368">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gulati</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Sikandar</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Wesche</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Manjunath</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Bharadwaj</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Berger</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Ilagan</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Kuo</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Hsieh</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Cai</surname>
            <given-names>S.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Single-cell transcriptional diversity is a hallmark of developmental potential</article-title>
        <source>Science</source>
        <year>2020</year>
        <volume>24</volume>
        <fpage>405</fpage>
        <lpage>411</lpage>
        <pub-id pub-id-type="doi">10.1126/science.aax0249</pub-id>
      </element-citation>
    </ref>
    <ref id="B54-entropy-23-01368">
      <label>54.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Giuliani</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>The application of principal component analysis to drug discovery and biomedical data</article-title>
        <source>Drug Discov. Today</source>
        <year>2017</year>
        <volume>22</volume>
        <fpage>1069</fpage>
        <lpage>1076</lpage>
        <pub-id pub-id-type="doi">10.1016/j.drudis.2017.01.005</pub-id>
        <?supplied-pmid 28111329?>
        <pub-id pub-id-type="pmid">28111329</pub-id>
      </element-citation>
    </ref>
    <ref id="B55-entropy-23-01368">
      <label>55.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cangelosi</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Goriely</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Component retention in principal component analysis with application to cDNA microarray data</article-title>
        <source>Biol. Direct</source>
        <year>2007</year>
        <volume>2</volume>
        <fpage>2</fpage>
        <pub-id pub-id-type="doi">10.1186/1745-6150-2-2</pub-id>
        <pub-id pub-id-type="pmid">17229320</pub-id>
      </element-citation>
    </ref>
    <ref id="B56-entropy-23-01368">
      <label>56.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Johnsson</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Soneson</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Fontes</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Low Bias Local Intrinsic Dimension Estimation from Expected Simplex Skewness</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <year>2015</year>
        <volume>37</volume>
        <fpage>196</fpage>
        <lpage>202</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2014.2343220</pub-id>
        <?supplied-pmid 26353219?>
        <pub-id pub-id-type="pmid">26353219</pub-id>
      </element-citation>
    </ref>
    <ref id="B57-entropy-23-01368">
      <label>57.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Jolliffe</surname>
            <given-names>I.T.</given-names>
          </name>
        </person-group>
        <source>Principal Component Analysis</source>
        <series>Springer Series in Statistics</series>
        <publisher-name>Springer</publisher-name>
        <publisher-loc>Berlin/Heidelberg, Germany</publisher-loc>
        <year>2002</year>
      </element-citation>
    </ref>
    <ref id="B58-entropy-23-01368">
      <label>58.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kaiser</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>The Application of Electronic Computers to Factor Analysis</article-title>
        <source>Educ. Psychol. Meas.</source>
        <year>1960</year>
        <volume>20</volume>
        <fpage>141</fpage>
        <lpage>151</lpage>
        <pub-id pub-id-type="doi">10.1177/001316446002000116</pub-id>
      </element-citation>
    </ref>
    <ref id="B59-entropy-23-01368">
      <label>59.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Frontier</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Étude de la décroissance des valeurs propres dans une analyse en composantes principales: Comparaison avec le modèle du bâton brisé</article-title>
        <source>J. Exp. Mar. Biol. Ecol.</source>
        <year>1976</year>
        <volume>25</volume>
        <fpage>67</fpage>
        <lpage>75</lpage>
        <pub-id pub-id-type="doi">10.1016/0022-0981(76)90076-9</pub-id>
      </element-citation>
    </ref>
    <ref id="B60-entropy-23-01368">
      <label>60.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gorban</surname>
            <given-names>A.N.</given-names>
          </name>
          <name>
            <surname>Sumner</surname>
            <given-names>N.R.</given-names>
          </name>
          <name>
            <surname>Zinovyev</surname>
            <given-names>A.Y.</given-names>
          </name>
        </person-group>
        <article-title>Topological grammars for data approximation</article-title>
        <source>Appl. Math. Lett.</source>
        <year>2007</year>
        <volume>20</volume>
        <fpage>382</fpage>
        <lpage>386</lpage>
        <pub-id pub-id-type="doi">10.1016/j.aml.2006.04.022</pub-id>
      </element-citation>
    </ref>
    <ref id="B61-entropy-23-01368">
      <label>61.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Albergante</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Mirkes</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Bac</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Faure</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Barillot</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Pinello</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Gorban</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Zinovyev</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Robust and scalable learning of complex intrinsic dataset geometry via ElPiGraph</article-title>
        <source>Entropy</source>
        <year>2020</year>
        <volume>22</volume>
        <elocation-id>296</elocation-id>
        <pub-id pub-id-type="doi">10.3390/e22030296</pub-id>
      </element-citation>
    </ref>
    <ref id="B62-entropy-23-01368">
      <label>62.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lähnemann</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Köster</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Szczurek</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>McCarthy</surname>
            <given-names>D.J.</given-names>
          </name>
          <name>
            <surname>Hicks</surname>
            <given-names>S.C.</given-names>
          </name>
          <name>
            <surname>Robinson</surname>
            <given-names>M.D.</given-names>
          </name>
          <name>
            <surname>Vallejos</surname>
            <given-names>C.A.</given-names>
          </name>
          <name>
            <surname>Campbell</surname>
            <given-names>K.R.</given-names>
          </name>
          <name>
            <surname>Beerenwinkel</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Mahfouz</surname>
            <given-names>A.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Eleven grand challenges in single-cell data science</article-title>
        <source>Genome Biol.</source>
        <year>2020</year>
        <volume>21</volume>
        <fpage>1</fpage>
        <lpage>31</lpage>
        <pub-id pub-id-type="doi">10.1186/s13059-020-1926-6</pub-id>
      </element-citation>
    </ref>
    <ref id="B63-entropy-23-01368">
      <label>63.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Albergante</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Hsu</surname>
            <given-names>J.Y.</given-names>
          </name>
          <name>
            <surname>Lareau</surname>
            <given-names>C.A.</given-names>
          </name>
          <name>
            <surname>Lo Bosco</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Guan</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Gorban</surname>
            <given-names>A.N.</given-names>
          </name>
          <name>
            <surname>Bauer</surname>
            <given-names>D.E.</given-names>
          </name>
          <name>
            <surname>Aryee</surname>
            <given-names>M.J.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Single-cell trajectories reconstruction, exploration and mapping of omics data with STREAM</article-title>
        <source>Nat. Commun.</source>
        <year>2019</year>
        <volume>10</volume>
        <fpage>1</fpage>
        <lpage>14</lpage>
        <pub-id pub-id-type="doi">10.1038/s41467-019-09670-4</pub-id>
        <pub-id pub-id-type="pmid">30602773</pub-id>
      </element-citation>
    </ref>
    <ref id="B64-entropy-23-01368">
      <label>64.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sritharan</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Hormoz</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Computing the Riemannian curvature of image patch and single-cell RNA sequencing data manifolds using extrinsic differential geometry</article-title>
        <source>Proc. Natl. Acad. Sci. USA</source>
        <year>2021</year>
        <volume>118</volume>
        <fpage>e2100473118</fpage>
        <pub-id pub-id-type="doi">10.1073/pnas.2100473118</pub-id>
        <pub-id pub-id-type="pmid">34272279</pub-id>
      </element-citation>
    </ref>
    <ref id="B65-entropy-23-01368">
      <label>65.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Radulescu</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Gorban</surname>
            <given-names>A.N.</given-names>
          </name>
          <name>
            <surname>Zinovyev</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Lilienbaum</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Robust simplifications of multiscale biochemical networks</article-title>
        <source>BMC Syst. Biol.</source>
        <year>2008</year>
        <volume>2</volume>
        <elocation-id>86</elocation-id>
        <pub-id pub-id-type="doi">10.1186/1752-0509-2-86</pub-id>
        <pub-id pub-id-type="pmid">18854041</pub-id>
      </element-citation>
    </ref>
    <ref id="B66-entropy-23-01368">
      <label>66.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gorban</surname>
            <given-names>A.N.</given-names>
          </name>
          <name>
            <surname>Zinovyev</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Principal manifolds and graphs in practice: From molecular biology to dynamical systems</article-title>
        <source>Int. J. Neural Syst.</source>
        <year>2010</year>
        <volume>20</volume>
        <fpage>219</fpage>
        <lpage>232</lpage>
        <pub-id pub-id-type="doi">10.1142/S0129065710002383</pub-id>
        <pub-id pub-id-type="pmid">20556849</pub-id>
      </element-citation>
    </ref>
    <ref id="B67-entropy-23-01368">
      <label>67.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Donoho</surname>
            <given-names>D.L.</given-names>
          </name>
        </person-group>
        <article-title>High-dimensional data analysis: The curses and blessings of dimensionality</article-title>
        <source>AMS Math Challenges Lect.</source>
        <year>2000</year>
        <volume>1</volume>
        <fpage>1</fpage>
        <lpage>32</lpage>
      </element-citation>
    </ref>
    <ref id="B68-entropy-23-01368">
      <label>68.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gorban</surname>
            <given-names>A.N.</given-names>
          </name>
          <name>
            <surname>Tyukin</surname>
            <given-names>I.Y.</given-names>
          </name>
        </person-group>
        <article-title>Blessing of dimensionality: Mathematical foundations of the statistical physics of data</article-title>
        <source>Philos. Trans. R. Soc. A Math. Phys. Eng. Sci.</source>
        <year>2018</year>
        <volume>376</volume>
        <fpage>20170237</fpage>
        <pub-id pub-id-type="doi">10.1098/rsta.2017.0237</pub-id>
      </element-citation>
    </ref>
    <ref id="B69-entropy-23-01368">
      <label>69.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kainen</surname>
            <given-names>P.C.</given-names>
          </name>
          <name>
            <surname>Kůrková</surname>
            <given-names>V.</given-names>
          </name>
        </person-group>
        <article-title>Quasiorthogonal dimension of euclidean spaces</article-title>
        <source>Appl. Math. Lett.</source>
        <year>1993</year>
        <volume>6</volume>
        <fpage>7</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="doi">10.1016/0893-9659(93)90023-G</pub-id>
      </element-citation>
    </ref>
    <ref id="B70-entropy-23-01368">
      <label>70.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Tyukin</surname>
            <given-names>I.Y.</given-names>
          </name>
          <name>
            <surname>Higham</surname>
            <given-names>D.J.</given-names>
          </name>
          <name>
            <surname>Gorban</surname>
            <given-names>A.N.</given-names>
          </name>
        </person-group>
        <article-title>On Adversarial Examples and Stealth Attacks in Artificial Intelligence Systems</article-title>
        <source>Proceedings of the International Joint Conference on Neural Networks (IJCNN)</source>
        <conf-loc>Glasgow, UK</conf-loc>
        <conf-date>19–24 July 2020</conf-date>
        <fpage>1</fpage>
        <lpage>6</lpage>
      </element-citation>
    </ref>
    <ref id="B71-entropy-23-01368">
      <label>71.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gorban</surname>
            <given-names>A.N.</given-names>
          </name>
          <name>
            <surname>Grechuk</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Mirkes</surname>
            <given-names>E.M.</given-names>
          </name>
          <name>
            <surname>Stasenko</surname>
            <given-names>S.V.</given-names>
          </name>
          <name>
            <surname>Tyukin</surname>
            <given-names>I.Y.</given-names>
          </name>
        </person-group>
        <article-title>High-Dimensional Separability for One- and Few-Shot Learning</article-title>
        <source>Entropy</source>
        <year>2021</year>
        <volume>23</volume>
        <elocation-id>1090</elocation-id>
        <pub-id pub-id-type="doi">10.3390/e23081090</pub-id>
        <?supplied-pmid 34441230?>
        <pub-id pub-id-type="pmid">34441230</pub-id>
      </element-citation>
    </ref>
    <ref id="B72-entropy-23-01368">
      <label>72.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Amblard</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Bac</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Chervov</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Soumelis</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Zinovyev</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Hubness reduction improves clustering and trajectory inference in single-cell transcriptomic data</article-title>
        <source>bioRxiv</source>
        <year>2021</year>
        <pub-id pub-id-type="doi">10.1101/2021.03.18.435808</pub-id>
      </element-citation>
    </ref>
    <ref id="B73-entropy-23-01368">
      <label>73.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Gionis</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Hinneburg</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Papadimitriou</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Tsaparas</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Dimension Induced Clustering</article-title>
        <source>KDD ’05: Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining</source>
        <publisher-name>Association for Computing Machinery</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2005</year>
        <fpage>51</fpage>
        <lpage>60</lpage>
        <pub-id pub-id-type="doi">10.1145/1081870.1081880</pub-id>
      </element-citation>
    </ref>
    <ref id="B74-entropy-23-01368">
      <label>74.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Allegra</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Facco</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Denti</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Laio</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Mira</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Data segmentation based on the local intrinsic dimension</article-title>
        <source>Sci. Rep.</source>
        <year>2020</year>
        <volume>10</volume>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="doi">10.1038/s41598-020-72222-0</pub-id>
        <?supplied-pmid 33020515?>
        <pub-id pub-id-type="pmid">31913322</pub-id>
      </element-citation>
    </ref>
    <ref id="B75-entropy-23-01368">
      <label>75.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Grechuk</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Gorban</surname>
            <given-names>A.N.</given-names>
          </name>
          <name>
            <surname>Tyukin</surname>
            <given-names>I.Y.</given-names>
          </name>
        </person-group>
        <article-title>General stochastic separation theorems with optimal bounds</article-title>
        <source>Neural Netw.</source>
        <year>2021</year>
        <volume>138</volume>
        <fpage>33</fpage>
        <lpage>56</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neunet.2021.01.034</pub-id>
        <pub-id pub-id-type="pmid">33621897</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig position="float" id="entropy-23-01368-f001">
    <label>Figure 1</label>
    <caption>
      <p>Example usage: generating the Line–Disk–Ball dataset [<xref rid="B10-entropy-23-01368" ref-type="bibr">10</xref>]), which has clusters of varying local ID, and coloring points by estimates of local ID obtained by id.lPCA.</p>
    </caption>
    <graphic xlink:href="entropy-23-01368-g001" position="float"/>
  </fig>
  <fig position="float" id="entropy-23-01368-f002">
    <label>Figure 2</label>
    <caption>
      <p>Illustrating different ID method general characteristics: (<bold>A</bold>) range of estimated ID values; (<bold>B</bold>) ability to produce interpretable (positive finite value) result; (<bold>C</bold>) sensitivity to feature redundancy (after duplicating matrix columns); (<bold>D</bold>) uniform ID estimation across datasets of similar nature; (<bold>E</bold>) computational time needed to compute ID for matrices of four characteristic sizes.</p>
    </caption>
    <graphic xlink:href="entropy-23-01368-g002" position="float"/>
  </fig>
  <fig position="float" id="entropy-23-01368-f003">
    <label>Figure 3</label>
    <caption>
      <p>Characterizing <monospace>OpenML</monospace> dataset collection in terms of ID estimates. (<bold>A</bold>) PCA visualizations of datasets characterized by vectors of 19 ID measures. Size of the point corresponds to the logarithm of the number of matrix entries (<inline-formula><mml:math id="mm11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>). The color corresponds to the mean ID estimate taken as the mean of all ID measure z-scores. (<bold>B</bold>) Loadings of various methods into the first and the second principal component from (<bold>A</bold>). (<bold>C</bold>) Visualization of the mean ID score as a function of data matrix shape. The color is the same as in (<bold>A</bold>). (<bold>D</bold>) Correlation matrix between different ID estimates computed over all analyzed datasets.</p>
    </caption>
    <graphic xlink:href="entropy-23-01368-g003" position="float"/>
  </fig>
  <fig position="float" id="entropy-23-01368-f004">
    <label>Figure 4</label>
    <caption>
      <p>A gallery of UMAP plots computed for a selection of datasets from <monospace>OpenML</monospace> collection, with indication of ID estimates, ranked by the ID value estimated using Fisher separability-based method (indicated in the left top corner). The ambient dimension of the data (number of features <inline-formula><mml:math id="mm12" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) is indicated in the bottom left corner, and the color reflects the <inline-formula><mml:math id="mm13" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> ratio, from red (close to 0.0 value) to green (close to 1.0). On the right from the UMAP plot, all 19 ID measures are indicated, with color mapped to the value range, from green (small dimension) to red (high dimension).</p>
    </caption>
    <graphic xlink:href="entropy-23-01368-g004" position="float"/>
  </fig>
  <table-wrap position="float" id="entropy-23-01368-t001">
    <object-id pub-id-type="pii">entropy-23-01368-t001_Table 1</object-id>
    <label>Table 1</label>
    <caption>
      <p>Summary table of ID methods characteristics. The qualitative score changes from “<inline-formula><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mo>−</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>” (worst) to “+++” (best).</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method Name</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Short Name(s)</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Ref(s)</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Valid Result</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Insensitivity to Redundancy</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Uniform ID Estimate in Similar Datasets</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Performance with Many Observations</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Performance with Many Features</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">PCA Fukunaga-Olsen</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">PCA FO, PFO</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B15-entropy-23-01368" ref-type="bibr">15</xref>,<xref rid="B22-entropy-23-01368" ref-type="bibr">22</xref>]</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">PCA Fan</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">PFN</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B23-entropy-23-01368" ref-type="bibr">23</xref>]</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">PCA maxgap</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">PMG</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B56-entropy-23-01368" ref-type="bibr">56</xref>]</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm15" display="block" overflow="scroll">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mo>−</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mo>−</mml:mo>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">PCA ratio</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">PRT</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B57-entropy-23-01368" ref-type="bibr">57</xref>]</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">PCA participation ratio</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">PPR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B57-entropy-23-01368" ref-type="bibr">57</xref>]</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">PCA Kaiser</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">PKS</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B54-entropy-23-01368" ref-type="bibr">54</xref>,<xref rid="B58-entropy-23-01368" ref-type="bibr">58</xref>]</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">PCA broken stick</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">PBS</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B55-entropy-23-01368" ref-type="bibr">55</xref>,<xref rid="B59-entropy-23-01368" ref-type="bibr">59</xref>]</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm16" display="block" overflow="scroll">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mo>−</mml:mo>
                    <mml:mo>−</mml:mo>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Correlation (fractal) dimensionality</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">CorrInt, CID</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B18-entropy-23-01368" ref-type="bibr">18</xref>]</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Fisher separability</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">FisherS, FSH</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B4-entropy-23-01368" ref-type="bibr">4</xref>,<xref rid="B32-entropy-23-01368" ref-type="bibr">32</xref>]</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">K-nearest neighbours</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">KNN</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B27-entropy-23-01368" ref-type="bibr">27</xref>]</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm17" display="block" overflow="scroll">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mo>−</mml:mo>
                    <mml:mo>−</mml:mo>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm18" display="block" overflow="scroll">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mo>−</mml:mo>
                    <mml:mo>−</mml:mo>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">++</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Manifold-adaptive fractal dimension</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">MADA, MDA</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B19-entropy-23-01368" ref-type="bibr">19</xref>]</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Minimum neighbor distance—ML</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">MIND_ML,MMk, MMi</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B28-entropy-23-01368" ref-type="bibr">28</xref>]</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Maximum likelihood</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">MLE</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B25-entropy-23-01368" ref-type="bibr">25</xref>]</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Methods of moments</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">MOM</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B20-entropy-23-01368" ref-type="bibr">20</xref>]</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Estimation within tight localities</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">TLE</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B33-entropy-23-01368" ref-type="bibr">33</xref>]</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm19" display="block" overflow="scroll">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mo>−</mml:mo>
                    <mml:mo>−</mml:mo>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Minimal neighborhood information</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">TwoNN,TNN</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B31-entropy-23-01368" ref-type="bibr">31</xref>]</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Angle and norm concentration</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">DANCo,DNC</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B29-entropy-23-01368" ref-type="bibr">29</xref>]</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm20" display="block" overflow="scroll">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mo>−</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mo>−</mml:mo>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm21" display="block" overflow="scroll">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mo>−</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mo>−</mml:mo>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Expected simplex skewness</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ESS</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B56-entropy-23-01368" ref-type="bibr">56</xref>]</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">+++</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm22" display="block" overflow="scroll">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mo>−</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mo>−</mml:mo>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm23" display="block" overflow="scroll">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mo>−</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mo>−</mml:mo>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
