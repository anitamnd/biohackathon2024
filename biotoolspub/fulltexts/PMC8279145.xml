<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PeerJ Comput Sci</journal-id>
    <journal-id journal-id-type="iso-abbrev">PeerJ Comput Sci</journal-id>
    <journal-id journal-id-type="pmc">peerj-cs</journal-id>
    <journal-id journal-id-type="publisher-id">peerj-cs</journal-id>
    <journal-title-group>
      <journal-title>PeerJ Computer Science</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2376-5992</issn>
    <publisher>
      <publisher-name>PeerJ Inc.</publisher-name>
      <publisher-loc>San Diego, USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8279145</article-id>
    <article-id pub-id-type="publisher-id">cs-601</article-id>
    <article-id pub-id-type="doi">10.7717/peerj-cs.601</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Data Science</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Software Engineering</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>GrimoireLab: A toolset for software development analytics</article-title>
    </title-group>
    <contrib-group>
      <contrib id="author-1" contrib-type="author">
        <name>
          <surname>Dueñas</surname>
          <given-names>Santiago</given-names>
        </name>
        <xref ref-type="aff" rid="aff-1">1</xref>
      </contrib>
      <contrib id="author-2" contrib-type="author">
        <name>
          <surname>Cosentino</surname>
          <given-names>Valerio</given-names>
        </name>
        <xref ref-type="aff" rid="aff-1">1</xref>
      </contrib>
      <contrib id="author-3" contrib-type="author" corresp="yes">
        <name>
          <surname>Gonzalez-Barahona</surname>
          <given-names>Jesus M.</given-names>
        </name>
        <xref ref-type="aff" rid="aff-2">2</xref>
        <email>jesus.gonzalez.barahona@urjc.es</email>
      </contrib>
      <contrib id="author-4" contrib-type="author">
        <name>
          <surname>del Castillo San Felix</surname>
          <given-names>Alvaro</given-names>
        </name>
        <xref ref-type="aff" rid="aff-1">1</xref>
      </contrib>
      <contrib id="author-5" contrib-type="author">
        <name>
          <surname>Izquierdo-Cortazar</surname>
          <given-names>Daniel</given-names>
        </name>
        <xref ref-type="aff" rid="aff-1">1</xref>
      </contrib>
      <contrib id="author-6" contrib-type="author">
        <name>
          <surname>Cañas-Díaz</surname>
          <given-names>Luis</given-names>
        </name>
        <xref ref-type="aff" rid="aff-1">1</xref>
      </contrib>
      <contrib id="author-7" contrib-type="author">
        <name>
          <surname>Pérez García-Plaza</surname>
          <given-names>Alberto</given-names>
        </name>
        <xref ref-type="aff" rid="aff-1">1</xref>
      </contrib>
      <aff id="aff-1"><label>1</label><institution>Bitergia</institution>, <addr-line>Leganes, Madrid</addr-line>, <country>Spain</country></aff>
      <aff id="aff-2"><label>2</label><institution>Escuela Superior de Ingeniería de Telecomunicación, Universidad Rey Juan Carlos</institution>, <addr-line>Fuenlabrada, Madrid</addr-line>, <country>Spain</country></aff>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Hassan</surname>
          <given-names>Ahmed</given-names>
        </name>
      </contrib>
    </contrib-group>
    <pub-date pub-type="epub" date-type="pub" iso-8601-date="2021-07-09">
      <day>9</day>
      <month>7</month>
      <year iso-8601-date="2021">2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>7</volume>
    <elocation-id>e601</elocation-id>
    <history>
      <date date-type="received" iso-8601-date="2020-11-11">
        <day>11</day>
        <month>11</month>
        <year iso-8601-date="2020">2020</year>
      </date>
      <date date-type="accepted" iso-8601-date="2021-05-28">
        <day>28</day>
        <month>5</month>
        <year iso-8601-date="2021">2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2021 Dueñas et al.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Dueñas et al.</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, reproduction and adaptation in any medium and for any purpose provided that it is properly attributed. For attribution, the original author(s), title, publication source (PeerJ Computer Science) and either DOI or URL of the article must be cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="https://peerj.com/articles/cs-601"/>
    <abstract>
      <sec>
        <title>Background</title>
        <p>After many years of research on software repositories, the knowledge for building mature, reusable tools that perform data retrieval, storage and basic analytics is readily available. However, there is still room to improvement in the area of reusable tools implementing this knowledge.</p>
      </sec>
      <sec>
        <title>Goal</title>
        <p>To produce a reusable toolset supporting the most common tasks when retrieving, curating and visualizing data from software repositories, allowing for the easy reproduction of data sets ready for more complex analytics, and sparing the researcher or the analyst of most of the tasks that can be automated.</p>
      </sec>
      <sec>
        <title>Method</title>
        <p>Use our experience in building tools in this domain to identify a collection of scenarios where a reusable toolset would be convenient, and the main components of such a toolset. Then build those components, and refine them incrementally using the feedback from their use in both commercial, community-based, and academic environments.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>GrimoireLab, an efficient toolset composed of five main components, supporting about 30 different kinds of data sources related to software development. It has been tested in many environments, for performing different kinds of studies, and providing different kinds of services. It features a common API for accessing the retrieved data, facilities for relating items from different data sources, semi-structured storage for easing later analysis and reproduction, and basic facilities for visualization, preliminary analysis and drill-down in the data. It is also modular, making it easy to support new kinds of data sources and analysis.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p>We present a mature toolset, widely tested in the field, that can help to improve the situation in the area of reusable tools for mining software repositories. We show some scenarios where it has already been used. We expect it will help to reduce the effort for doing studies or providing services in this area, leading to advances in reproducibility and comparison of results.</p>
      </sec>
    </abstract>
    <kwd-group kwd-group-type="author">
      <kwd>Mining software repositories</kwd>
      <kwd>Empirical software engineering</kwd>
      <kwd>Software development</kwd>
      <kwd>Software analytics</kwd>
      <kwd>Datasets</kwd>
      <kwd>Toolset</kwd>
      <kwd>Software development visualization</kwd>
    </kwd-group>
    <funding-group>
      <award-group id="fund-1">
        <funding-source>Ministerio de Ciencia y Tecnología of Spain</funding-source>
        <award-id>RTI2018-101963-B-I00, RTC-2017-6554-7</award-id>
      </award-group>
      <award-group id="fund-2">
        <funding-source>Ministerio de Economia y Competitividad of Spain</funding-source>
        <award-id>PTQ-15-07709</award-id>
      </award-group>
      <funding-statement>This work is supported by Ministerio de Ciencia y Tecnología of Spain under Project BugBirth, RTI2018-101963-B-I00 (Retos) and Grimoire as a Service, RTC-2017-6554-7 (Retos Colaboracion), and by Ministerio de Economia y Competitividad of Spain under Grant PTQ-15-07709 (Torres Quevedo). There was no additional external funding received for this study. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
  </article-meta>
</front>
<body>
  <sec sec-type="intro">
    <title>Introduction</title>
    <p>Software development, and in particular open source software development, relies on an increasing number of support tools (<xref rid="ref-18" ref-type="bibr">Dabbish et al., 2012</xref>; <xref rid="ref-69" ref-type="bibr">Storey et al., 2010</xref>; <xref rid="ref-47" ref-type="bibr">Lanubile et al., 2010</xref>). Each of them maintain data about the software development process, the developed artifacts, and how developers are working. The analysis of these data sources (usually referred as software repositories) has favored the creation of an active community of miners, both from academia and industry, interested in the empirical study of how software artifacts are created and maintained, and the related processes, activities and persons (<xref rid="ref-15" ref-type="bibr">Cosentino, Izquierdo &amp; Cabot, 2017</xref>).</p>
    <sec>
      <title>Motivation</title>
      <p>As the mining software repositories community has matured (<xref rid="ref-37" ref-type="bibr">Hemmati et al., 2013</xref>), tools have been built to retrieve and curate large datasets. Already before 2010, many tools had been built to deal with a wide spectrum of repositories: <italic>CVSAnalY</italic> (<xref rid="ref-62" ref-type="bibr">Robles, Koch &amp; Gonzalez-Barahona, 2004</xref>), <italic>FLOSSMole</italic> (<xref rid="ref-39" ref-type="bibr">Howison, Conklin &amp; Crowston, 2006</xref>), <italic>FOSSology</italic> (<xref rid="ref-29" ref-type="bibr">Gobeille, 2008</xref>), <italic>SQO-OSS</italic> (<xref rid="ref-32" ref-type="bibr">Gousios, Kalliamvakou &amp; Spinellis, 2008</xref>), to name just a few of them. These tools showed how data retrieval, storage, and at least a part of the analysis could be automated and made generic enough to support different kinds of studies; were used to explore the limits to scalability, and the benefits of developing reusable tools; and served to demonstrate different approaches to avoid harming the project hosting systems, while at the same time being efficient in retrieving data (for example, by retrieving data once, storing it in a database, and later analyzing that data as many times as needed). After this “first wave” many other tools, developed during the last decade, were built on these lessons, offering more sophisticated functionality, better performance and scalability, and in some cases, more variety of data sources. Examples of this second-generation tools are <italic>MetricsGrimoire</italic> (<xref rid="ref-31" ref-type="bibr">Gonzalez-Barahona, Robles &amp; Izquierdo-Cortazar, 2015</xref>), <italic>Kibble</italic> (<xref rid="ref-1" ref-type="bibr">Apache, 2022</xref>), or <italic>Gitana</italic> (<xref rid="ref-16" ref-type="bibr">Cosentino, Izquierdo &amp; Cabot, 2018</xref>).</p>
      <p>In the specific case of GitHub, which currently hosts a vast majority of FOSS software projects, and most of the public code available today, several tools are retrieving data and source code from it. Some of them provide means to query that data, or to produce some analysis and visualizations of it: <italic>GHTorrent</italic> (<xref rid="ref-35" ref-type="bibr">Gousios &amp; Spinellis, 2012</xref>) and <italic>BOA</italic> (<xref rid="ref-23" ref-type="bibr">Dyer et al., 2013a</xref>), <italic>GHArchive</italic> (<xref rid="ref-36" ref-type="bibr">Grigorik, 2022</xref>), and <italic>OpenHub</italic> (<xref rid="ref-27" ref-type="bibr">Farah, Tejada &amp; Correal, 2014</xref>), to mention just some of the better known. Some tools have also been deployed specifically to retrieve source code or data related to software development, and store it for preservation, such as <italic>Software Heritage</italic> (<xref rid="ref-21" ref-type="bibr">Di Cosmo &amp; Zacchiroli, 2017</xref>) and <italic>SARA</italic> (<xref rid="ref-65" ref-type="bibr">SARA, 2022</xref>).</p>
      <p>Despite the many benefits that all of these tools provide when a researcher or a practitioner needs to deal with software development data retrieval and analysis, there is still room for improvement in many areas. Most of the tools in these two generations are focused on one, or in some cases, a small subset of kinds of data sources; use disparate data formats and integration APIs, making it difficult to combine results for different kinds of repositories; and in many cases are not easy to deploy and operate, or difficult to use for large-scale, continuous data retrieval. Not all of these tools provide support for retrieval, storage and analysis of the data, and when they do, usually the opportunities for analysis are very limited. Only a few of them are extensible, and just a few are mature enough for large-scale, industrial endeavors. During the last years, some new tools or toolsets are emerging that try to address some of these issues, such as <italic>PyDriller</italic> (<xref rid="ref-67" ref-type="bibr">Spadini, Aniche &amp; Bacchelli, 2018</xref>) and <italic>SmartSHARK</italic> (<xref rid="ref-73" ref-type="bibr">Trautsch et al., 2017</xref>). <italic>GrimoireLab</italic>, which we started to design and implement in 2016, is one of them.</p>
    </sec>
    <sec>
      <title>Overview</title>
      <p><italic>GrimoireLab</italic> is a free, open source set of Python tools to retrieve, organize, analyze and visualize software development data. It automatically collects, processes and stores data from more than 30 kinds of repositories used in software development (source code management, issue tracking, code review, messaging, continuous integration, etc). <italic>GrimoireLab</italic> builds on previous experiences, paying special attention to recurrent issues that miners face in their activities such as data loss or corruption due to connection problems, data freshness and incremental retrieval, identities management, and heterogeneous formats that come from different data sources. It has been designed and built as a modular toolset suitable for its use by third parties, with the aim of satisfying the needs of researchers, but also of commercial exploitation.</p>
      <p>Miners can use the functionality provided by <italic>GrimoireLab</italic> as a black box, to efficiently retrieve, analyze, store, and visualize data for a collection of projects. Or they can use specific tools, maybe integrating them with their own mining applications. For example, it provides modules for retrieving data from many kinds of data sources, with a common API, and for integrating third party tools for code analysis that can be used standalone from Python scripts. <italic>GrimoireLab</italic> also includes a module for identity management that can be used in combination with custom code to merge or tag identities, something that is fundamental to analyze activity of persons using several identities, to merge activity from different data sources, and to annotate identities with affiliation information, for example. There are also scheduling and orchestration modules that can be used or not, depending on the complexity of the scenario. <italic>GrimoireLab</italic> also defines some data formats for several steps in the usual analysis pipelines (raw retrieval formats, enriched formats) that can be used for integration with other tools or for replication.</p>
    </sec>
    <sec>
      <title>Contributions</title>
      <p>The main contributions of the toolset presented in this paper are:<list list-type="bullet"><list-item><p>Breadth. Support for activities in many areas related to the mining of software repositories: data retrieval, storage, analysis, identity management, scheduling, visualization, reporting, etc.</p></list-item><list-item><p>Modularity. A modular and extensible design, including the identification of the modules useful in common tasks in this domain, and common APIs for similar functions.</p></list-item><list-item><p>Data formats. Definition of data formats for the main stages of software development analytics.</p></list-item><list-item><p>Readiness. Implementation as a collection of easy-to-install, easy-to-use Python packages, also available as Docker images.</p></list-item><list-item><p>Maturity. Extensive testing and regular usage, both in academic and industrial environments.</p></list-item><list-item><p>Extra functionality. Built-in functionality for addressing common problems in real-world data retrieval, storage and analysis: fault-tolerance, incremental retrieval, extensibility, facilities for data curation, identity management (including tracking of identities in different data sources), data persistence, traceability and uniform access to the data.</p></list-item></list></p>
    </sec>
    <sec>
      <title>Structure of the paper and definitions</title>
      <p>This paper presents <italic>GrimoireLab</italic>, the main result of a “solution-seeking” research line (<xref rid="ref-68" ref-type="bibr">Stol &amp; Fitzgerald, 2018</xref>), aiming to improve the situation in solving the practical problem of retrieving data from software development repositories, preparing it for further analysis, and providing basic analysis and visualization tools that help in exploratory studies. The approach used has been holistic, trying to first understand (by experience and by study) the problems, and then providing a toolset that addresses many of them in combination. We also show how <italic>GrimoireLab</italic> can be used in some research scenarios, and how it was used in some real use cases, and discuss its main characteristics both in research and industrial environments.</p>
      <p>The rest of this paper is organized as follows: “The components” section describes the different components of <italic>GrimoireLab</italic>; the “Combining the modules” section illustrates how those components can be combined in several exemplary research scenarios, and in some real use cases (presented with their main magnitudes and performance metrics); the “Discussion” section summarizes and discusses the main features of the toolset, how its use in research studies may affect researchers, some lessons learned from its use in industry, and presents <italic>GrimoireLab</italic> in the context of other related work. Finally, “Availability and usage” summarizes availability and usage of the toolset, and “Conclusion” highlights some conclusions and future work.</p>
      <p>Some definitions of terms that we will use through this paper are:<list list-type="bullet"><list-item><p><italic>data source</italic>: any system providing retrieval mechanisms (usually, an API) to access data related to software development: source code management, issue tracking, code review, synchronous or asynchronous communication, testing, collaborative documentation writing, Q/A forums, etc. Examples of a data source are a Git server, a Bugzilla instance, a Mailman archive, or some Slack instance.</p></list-item><list-item><p><italic>kind of data source</italic>: all data sources with the same retrieval API. Examples of kinds of data sources are “Git”, “Bugzilla”, “Mailman”, or “Slack”.</p></list-item><list-item><p><italic>repository</italic>: a part of a data source, usually corresponding to the data managed for a certain project. Examples of repositories are a Git repository, a Bugzilla issue tracker, a mailing list archive in a Mailman instance, or a Slack channel.</p></list-item><list-item><p><italic>index</italic>: all data corresponding to a certain kind of data source, as it is stored in the <italic>GrimoireLab</italic> database. Indexes may be raw, with data as similar as possible to the one provided by the data sources, or enriched, which are tailored to easy visualization and reporting.</p></list-item><list-item><p><italic>item</italic>: unity of data stored in an index, usually corresponding to what developers consider as a unity of action a kind of data source. Examples of items are “commit” for Git, “issue” for Bugzilla, “message” for Mailman, or “message” for Slack.</p></list-item><list-item><p><italic>GrimoireLab component</italic>: software module, maintained in a separate repository, and as a separate Python package, which is a part of <italic>GrimoireLab</italic>.</p></list-item></list></p>
    </sec>
  </sec>
  <sec>
    <title>The Components</title>
    <p><italic>GrimoireLab</italic> is structured in several components, which are outlined in this section. Components can be composed in different ways, to support different use cases. Each component can be installed as a Python package, which may need some other components to work: in that case they are installed as dependencies. Most components are Python modules that can be imported as libraries, but many of them also provide driver scripts to provide a certain CLI (command line interface).</p>
    <p>The overall structure of <italic>GrimoireLab</italic> is sketched in <xref ref-type="fig" rid="fig-1">Fig. 1</xref>. In it, components are grouped in four areas: Data Retrieval, Analytics (including permanent storage), Identities Management, and Visualization and Reporting. Additionally, there is also a module for orchestration. This separation in areas is introduced to help in the process to understand <italic>GrimoireLab</italic> components, and their role in the functionalities provided. At the same time, it allows for the introduction of the main interfaces that allow for the relatively independent development of the components presented in the rest of this section. These interfaces are:</p>
    <p>
      <list list-type="bullet">
        <list-item>
          <p>Retrieval components always provide JSON documents: data retrieved for each item is encoded as a JSON document resembling as much as possible the data structure provided by the corresponding data source. However, it also includes some metadata common for all kinds of data sources, which allows for a uniform data processing when peculiarities of a data source are not relevant (for example, when storing the data in permanent storage, or for temporal ordering of the items). When convenient, these JSON documents are mapped to Python dictionaries to provide a Python API based on Python generators.</p>
        </list-item>
        <list-item>
          <p>Identities Management components are accessed through a Python API, mapped to a CLI (command line interface) when convenient. This API allows for the registration of new identities found in data sources, for mapping them to unique identities identifying persons, and for the retrieval of tags (such as affiliation) associated to identities.</p>
        </list-item>
        <list-item>
          <p>Analytics modules produce results that are stored in permanent storage (enriched indexes in a database). Other components (Visualization and Reporting) using these results access them via the database interface to these indexes. Enriched indexes are composed by a flat JSON document per item. These documents are suitable to plug into visualization tools, or to perform further processing (for example, mapping collections of JSON documents to Python Pandas dataframes) towards specific reports. The document for each item includes the most usual fields for the analysis of the corresponding data source, and some others common to all data sources, thus enabling cross-data source analysis (for example, “creation date” or “author” of the item).</p>
        </list-item>
      </list>
    </p>
    <fig id="fig-1" orientation="portrait" position="float">
      <object-id pub-id-type="doi">10.7717/peerj-cs.601/fig-1</object-id>
      <label>Figure 1</label>
      <caption>
        <title>Main structure of GrimoireLab.</title>
        <p>The four main areas that structure the toolset are presented,showing the data flow between them and with data sources. Orchestration is also represented as managingall components. •Twitter: <uri xlink:href="https://www.iconfinder.com/icons/211920/twitter_83905social_icon">https://www.iconfinder.com/icons/211920/twitter_83905social_icon</uri> -&gt; MIT; •GitHub: <uri xlink:href="https://www.iconfinder.com/icons/298822/github_mark_icon">https://www.iconfinder.com/icons/298822/github_mark_icon</uri> -&gt; MIT; •bugzilla: recreated from <uri xlink:href="https://commons.wikimedia.org/wiki/File:Buggie.svg">https://commons.wikimedia.org/wiki/File:Buggie.svg</uri> -&gt; MPL 1.1; Reddit: <uri xlink:href="https://www.iconfinder.com/icons/211911/reddit_social_icon">https://www.iconfinder.com/icons/211911/reddit_social_icon</uri> -&gt; MIT; •Slack: <uri xlink:href="https://www.iconfinder.com/icons/710268/slack_social_icon">https://www.iconfinder.com/icons/710268/slack_social_icon</uri> -&gt; CC by 2.5; •Meetup: <uri xlink:href="https://www.iconfinder.com/icons/306191/meetup_icon">https://www.iconfinder.com/icons/306191/meetup_icon</uri> -&gt; CC by 2.5; •Telegram: <uri xlink:href="https://www.iconfinder.com/icons/2644993/media_messenger_social_telegram_icon">https://www.iconfinder.com/icons/2644993/media_messenger_social_telegram_icon</uri> -&gt; -&gt; CC by 3.0; •Stackoverflow: <uri xlink:href="https://www.iconfinder.com/icons/394194/overflow_stack_stackoverflow_icon">https://www.iconfinder.com/icons/394194/overflow_stack_stackoverflow_icon</uri> -&gt; Free commercial use. Use icon for commercial purpose, edit, share, etc.; •Jira: from <uri xlink:href="https://iconscout.com/icon/jira-1">https://iconscout.com/icon/jira-1</uri>-&gt; MIT.</p>
      </caption>
      <graphic xlink:href="peerj-cs-07-601-g001"/>
    </fig>
    <p>The details of each of these areas, and their underlying components, are described in the next sections. For help going through them, check <xref ref-type="fig" rid="fig-2">Fig. 2</xref>.</p>
    <fig id="fig-2" orientation="portrait" position="float">
      <object-id pub-id-type="doi">10.7717/peerj-cs.601/fig-2</object-id>
      <label>Figure 2</label>
      <caption>
        <title>Components of the GrimoireLab toolset, grouped in the same areas shown in <xref ref-type="fig" rid="fig-1">Fig. 1</xref>.</title>
        <p>Permanent storage used (an SQL database for identity data, on top, and Elasticsearch for raw andenriched indexes) is also shown. Arrows represent the flow of data from data sources to the componentsin the different areas, between them, and with the databases. •Kidash: <uri xlink:href="https://es.wikipedia.org/wiki/Archivo:KI-hiragana.gif">https://es.wikipedia.org/wiki/Archivo:KI-hiragana.gif</uri> -&gt; CC by 3.0; •Graal: <uri xlink:href="https://icon-icons.com/es/icono/Santo-Grial-holy/39098">https://icon-icons.com/es/icono/Santo-Grial-holy/39098</uri>-&gt; CC Attribution.</p>
      </caption>
      <graphic xlink:href="peerj-cs-07-601-g002"/>
    </fig>
    <sec>
      <title>Retrieval</title>
      <p><italic>GrimoireLab</italic> pipelines usually start by retrieving data from some software development repository. This involves accessing the APIs of the services managing those repositories (such as GitHub, Stack Overflow or Slack, San Francisco, California, USA), or using external tools or libraries to directly access the artifacts (such as Git repositories or mailing list archives). In the specific case of source code management repositories, some tools may also be run to obtain metrics about the source code. For large-scale retrieval, work is organized in jobs that have to be scheduled to minimize impact on the target platform, and to maximize performance. <italic>GrimoireLab</italic> provides three components for dealing with these issues:</p>
      <list list-type="bullet">
        <list-item>
          <p><italic>Perceval</italic> fetches data from the original data sources. Usually, it works as a library, providing a uniform Python API to access software development repositories. Relevant data in these repositories are produced as “items”, that can be managed as Python dictionaries or JSON documents. <italic>Perceval</italic> provides access to the following data sources (although for some of them, not all APIs are always supported):
<list list-type="simple"><list-item><p>-Version control systems: Git.</p></list-item><list-item><p>-Source code review systems: Gerrit, GitHub, GitLab.</p></list-item><list-item><p>-Bugs/Ticketing tools: Bugzilla, GitHub, JIRA, Launchpad, Phabricator, Redmine, GitLab.</p></list-item><list-item><p>-Asynchronous. communication: Hyperkitty, MBox archives, NNTP, Pipermail, Groups.io</p></list-item><list-item><p>-Forums: RSS, NNTP</p></list-item><list-item><p>-Continuous integration: Jenkins</p></list-item><list-item><p>-Instant messaging: Slack, Mattermost, Gitter, RocketChat, Supybot archives (IRC), Telegram</p></list-item><list-item><p>-Q/A: Askbot, Discourse, Stack Exchange</p></list-item><list-item><p>-Documentation: Confluence, Mediawiki</p></list-item><list-item><p>-Other: DockerHub, Meetup, Twitter</p></list-item></list></p>
        </list-item>
        <list-item>
          <p><italic>Graal</italic> runs third party tools on Git repositories, to obtain source code analysis data, at the file level, for each commit found. It uses <italic>Perceval</italic> to get the list of commits, and then runs the tools selected on checkouts of those commits. <italic>Graal</italic> can run tools for computing metrics in the areas of code complexity, code size, code quality, potential vulnerabilities, and licensing. <italic>Graal</italic> captures the output of these tools, encoding the data they produce in JSON items similar to those produced by <italic>Perceval</italic>.</p>
        </list-item>
        <list-item>
          <p><italic>Arthur</italic> schedules and run <italic>Perceval</italic> and <italic>Graal</italic> jobs at scale through distributed queues<xref ref-type="fn" rid="fn-1"><sup>1</sup></xref><fn id="fn-1"><label>1</label><p>At the moment of writing this paper, support for <italic>Graal</italic> in <italic>Arthur</italic> is still not completely integrated.</p></fn>.</p>
        </list-item>
      </list>
      <p>Therefore, <italic>Perceval</italic> and <italic>Graal</italic> are the two only components in <italic>GrimoireLab</italic> directly performing data retrieval. <italic>Perceval</italic> has backends for dealing with the peculiarities of data sources APIs, and <italic>Graal</italic> is specialized in the analysis of snapshots of code retrieved from a source code management system (using <italic>Perceval</italic> just for getting the list of commit hashes, in the case of Git). <italic>Arthur</italic>’s concern is to organize the work of <italic>Perceval</italic> and <italic>Graal</italic> when retrieving large quantities of data, by providing a system supporting the scheduling of parallel asynchronous jobs, in several nodes, and making all the details transparent to the next component in the pipeline (usually, <italic>GrimoireELK</italic>, see below).</p>
      <p>A common <italic>Perceval</italic> job consists of fetching a collection of homogeneous items from a given data source: tickets extracted from Bugzilla or GitHub issue trackers, commits from a Git repository, or code reviews from a Gerrit instance. Each item is extended with related information (e.g., comments and authors of a GitHub issue) obtained from the data source, and metadata useful for debugging and tracing (e.g., backend version and timestamp of the execution). When a data source provides several types of items, <italic>Perceval</italic> usually labels the resulting items in a way that can be identified by other components processing them later. For example, the GitHub Issues API provides both issues and pull requests for a repository: <italic>Perceval</italic> uses the field <monospace>pull_request</monospace> to let other components know if the item is an issue or a pull request.</p>
      <p>The output of the execution of <italic>Perceval</italic> is a list of Python dictionaries (or JSON documents), one per item. All these dictionaries, for all data sources, follow the same top-level schema: some fields with metainformation that can be used for traceability, for incremental retrieval, and to simplify tasks by other components. <xref ref-type="fig" rid="fig-3">Figure 3</xref> shows an example of the top level fields for an item corresponding to a GitHub pull request. The field <monospace>data</monospace> is a dictionary with all the data produced by the data source API, with a structure as similar as possible to the one produced by that API.</p>
      <fig id="fig-3" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.601/fig-3</object-id>
        <label>Figure 3</label>
        <caption>
          <title>Top-level fields for a certain item produced by Perceval (the example is for a GitHub pullrequest).</title>
          <p>Origin refers to the repository of origin for the item. Timestamp refers to the moment theitem was retrieved, updated on to the moment the item was last updated in the data source. uuid is a unique identifier for the item.</p>
        </caption>
        <graphic xlink:href="peerj-cs-07-601-g003"/>
      </fig>
      <p><italic>Perceval</italic> ’s design is shown in <xref ref-type="fig" rid="fig-4">Figs. 4</xref> and <xref ref-type="fig" rid="fig-5">5</xref>. For each data source, it includes a <italic>Client</italic>, a <italic>Backend</italic>, and a <italic>CommandLine</italic> class. <italic>Backend</italic> organizes the gathering process for a specific data source sharing common features, such as incrementally and caching, and defines those specific to the data source. For instance, the GitHub backend requires an API token and the names of the repository and owner, while the Stack Exchange backend needs an API token plus the tag to filter questions. The complexities for querying the data source are encapsulated in <italic>Client</italic>. Most of the code for each <italic>Client</italic> is specific for the kind of data source it is dealing with. However, some code is shared, such as token management (for those HTTP APIs that implement it), handling of interrupted connections (for APIs accessed via a TCP connection), and management of the retrieval cycle (provision of a Python generator to consumers of data retrieved by <italic>Perceval</italic>). <italic>CommandLine</italic> is provided to make parameters for each data source available via the command line. More details about <italic>Perceval</italic> are described in (<xref rid="ref-22" ref-type="bibr">Dueñas et al., 2018</xref>).</p>
      <fig id="fig-4" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.601/fig-4</object-id>
        <label>Figure 4</label>
        <caption>
          <title>Overview of the structure of Perceval.</title>
          <p>Solid arrows show the flow of data from data sources tothe JSON documents produced (one for each item in the data source). Dashed lines show the flow ofinvocations, from the command line module to the data source API.</p>
        </caption>
        <graphic xlink:href="peerj-cs-07-601-g004"/>
      </fig>
      <fig id="fig-5" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.601/fig-5</object-id>
        <label>Figure 5</label>
        <caption>
          <title>Simplified structure of Perceval, in UML.</title>
          <p>Only the main hierarchies of classes (Backend, BackendCommand, HttpClient and RateLimitHandler are shown, and only for two backends (GitHub and GitLab)).</p>
        </caption>
        <graphic xlink:href="peerj-cs-07-601-g005"/>
      </fig>
      <p><italic>Graal</italic> provides a mechanism to plug third party tools and libraries performing source code analysis. It produces analysis in the areas of code complexity, quality, dependencies, vulnerability and licensing. See an overview of the structure of <italic>Graal</italic> in <xref ref-type="fig" rid="fig-6">Fig. 6</xref>. <italic>Graal</italic> uses <italic>Perceval</italic> to clone the Git repository to analyze, and to get its list of commit hashes, via the <italic>Graal</italic> Client module. Then, the <italic>Graal</italic> Analyzer module runs some of the third party tools on each specified snapshots (by checking out the corresponding commits), and transforms the data produced by the tools in a Python dictionary. This dictionary is fed to the Backend component, which complements it with some data (such as <italic>Graal</italic> version, date of the analysis, etc.), and produces the resulting JSON document. The structure and functioning of <italic>Graal</italic> is described in more detail in (<xref rid="ref-14" ref-type="bibr">Cosentino et al., 2018</xref>).</p>
      <fig id="fig-6" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.601/fig-6</object-id>
        <label>Figure 6</label>
        <caption>
          <title>Overview of the structure of Graal. Solid arrows show the data flow from the Git repository tothe final JSON documents produced by Graal, dashed arrows show the flow of invocation data.</title>
          <p>The Analyzer module runs third party tools on the checkouts of the repository, produced by the Client module, and passes the results, properly formatted, to the Backend module. The “tools” box shows third party tools.</p>
        </caption>
        <graphic xlink:href="peerj-cs-07-601-g006"/>
      </fig>
      <p><italic>Arthur</italic> provides an HTTP API (via its <italic>Server</italic> class), which allows for the management (submit, delete, list) of <italic>Perceval</italic> jobs, defined as JSON documents specifying the details of the job. These details include the category of the job, parameters to run <italic>Perceval</italic>, or parameters to the scheduler, such as the maximum number of retries upon failures. Jobs are sent to the <italic>Scheduler</italic> class, which maintains queues for first-time and incremental retrievals, rescheduling in case of failures. These queues submit jobs to <italic>Workers</italic> (which can run in different machines), which are the key scalability element of <italic>Arthur</italic>. When jobs are done, workers notify the scheduler, and in case of success, they send the JSON documents, resulting from <italic>Perceval</italic> data retrieval, to a storage queue, where they are consumed by writers, making it possible to live-stream data or serialize it to database management systems. See an overview of the structure of <italic>Arthur</italic> in <xref ref-type="fig" rid="fig-7">Fig. 7</xref>.</p>
      <fig id="fig-7" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.601/fig-7</object-id>
        <label>Figure 7</label>
        <caption>
          <title>Overview of the structure of Arthur.</title>
          <p>Solid arrows show the jobs flow, since they arrive as jobrequests (usually produced by GrimoireELK), to the moment they run as Perceval or Graal invocations inworkers. Dashed arrows show the data flow from Graal and Perceval (which access the data sources) until it produces items ready to be uploaded to the database. •Redis: <uri xlink:href="https://www.iconfinder.com/icons/4691219/redis_icon">https://www.iconfinder.com/icons/4691219/redis_icon</uri> -&gt; CC by 3.0; •workers: <uri xlink:href="https://icon-icons.com/icon/gear-hammer/38299">https://icon-icons.com/icon/gear-hammer/38299</uri> -&gt; CC Attribution.</p>
        </caption>
        <graphic xlink:href="peerj-cs-07-601-g007"/>
      </fig>
      <p><italic>Perceval</italic> and <italic>Graal</italic> can be used on their own, usually from a Python script. <italic>Arthur</italic> provides a HTTP API, to control its operation.</p>
    </sec>
    <sec>
      <title>Analytics (and permanent storage)</title>
      <p>The main aim of Analytics components is to process retrieved data to produce items more suitable for visualization and reporting, in a process called enrichment. This allows for separation of retrieval; preparation for the final visualization and reporting; and the actual visualization and reporting. Components in this area also store both the retrieved and the enriched data. This strategy of storing the data at two points is convenient for two reasons: allowing the easy reproduction of the pipelines if needed, without the need to retrieve the data once again from the original data sources, and the production of visualization and analysis at any time, sparing the need to re-enrich raw data. Of course, this is possible thanks to the identification of several actions needed for most visualizations and reporting: flattening of the data, normalization of dates, identities management, etc.</p>
      <p>The Analytics area is covered by two components, <italic>GrimoireELK</italic> and <italic>Cereslib</italic>. The first one implements the core <italic>GrimoireLab</italic> pipeline: obtaining JSON items from the Data Retrieval components, storing them with persistence in “raw indexes”, enriching those indexes by producing items more suitable for visualization and reporting, and storing them in persistent “enriched indexes”. In the process, <italic>GrimoireELK</italic> also uses <italic>SortingHat</italic>, in the Identities Management area, for identifying new identities, and finding the corresponding unique (merged) identities. Since both raw and enriched indexes are Elasticsearch indexes, they are basically collections of JSON items (named “documents” in Elasticsearch). All usual operations on noSQL databases are possible on those indexes: retrieving one or more items given some constraints, aggregating values for certain fields for a certain selection of items, updating items matching certain values, etc.</p>
      <p>The other component, <italic>Cereslib</italic>, is a library providing an API with useful functionality for certain kinds of specialized functionality. The <italic>Cereslib</italic> API is invoked by <italic>GrimoireELK</italic> to run “studies”, which produces some specific enriched indexes. Studies are specialized preanalysis, producing items with a specific aim in mind. For example, one of them, “Areas of code”, produces commit data at the file level (each item consists of commit metadata for each revision of each file), which is useful to analyze how different areas of code evolve.</p>
      <p><italic>GrimoireELK</italic> is the main actor of this area, interacting with the database. Its design is shown in <xref ref-type="fig" rid="fig-8">Fig. 8</xref>. A feeder collects JSON documents produced by the data retrieval, storing them as the raw database (in an Elasticsearch index). Dumps of this raw data can be easily created to make any analysis reproducible, or to analyze directly with third party tools.</p>
      <fig id="fig-8" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.601/fig-8</object-id>
        <label>Figure 8</label>
        <caption>
          <title>Overview of the structure of GrimoireELK. Arrows show the data flow from JSON documents fed from Retrieval components to the enriched indexes.</title>
          <p>Retrieved data is received by the Feeder, whichstores them in raw indexes. Then, the Enricher module produces enriched items, submits new identities toSortingHat, and adds unique (merged) identities and related data to those items before storing them. •feeder: <uri xlink:href="https://thenounproject.com/term/pac-man/461024/">https://thenounproject.com/term/pac-man/461024/</uri> -&gt; CC Attribution; •enricher:<uri xlink:href="https://icon-icons.com/icon/mine-wagon/39492"> https://icon-icons.com/icon/mine-wagon/39492</uri> -&gt; CC Attribution; •redis: <uri xlink:href="https://www.iconfinder.com/icons/4691219/redis_icon">https://www.iconfinder.com/icons/4691219/redis_icon</uri> -&gt; CC by 3.0.</p>
        </caption>
        <graphic xlink:href="peerj-cs-07-601-g008"/>
      </fig>
      <p>Raw data is then enriched, summarizing the information usually needed for analysis and visualization, in some cases computing new fields. For example, pair programming information is added to Git data, when it can be extracted from commit messages; or time to solve an issue is added to GitHub data. The enriched data is stored in Elasticsearch as an index with flat JSON documents, embedding references to the raw documents for traceability.</p>
      <p>Each of the items in enriched indexes stores data about a single commit, issue report, code review, message, etc. For example, for a commit, 54 different fields are stored (see <xref ref-type="fig" rid="fig-9">Fig. 9</xref> for a more complete description of some of them<xref ref-type="fn" rid="fn-2"><sup>2</sup></xref>
<fn id="fn-2"><label>2</label><p>Full list of fields per item in the enriched Git index: <uri xlink:href="https://github.com/chaoss/grimoirelab-elk/blob/master/schema/git.csv">https://github.com/chaoss/grimoirelab-elk/blob/master/schema/git.csv</uri></p></fn>), including, among others: <monospace>author_uuid</monospace> (unique author identified, provided by SortingHat), <monospace>author_date</monospace> (author date in the commit record), <monospace>files</monospace> (number of files touched by this commit), <monospace>lines_added</monospace> (number of lines added), <monospace>lines_removed</monospace> (number of lines removed), <monospace>message</monospace> (commit message), <monospace>project</monospace> (project to which the repository is assigned), <monospace>branches</monospace> (list of branches in which the commit appears). Enriched items are not normalized due to limitations of Elasticsearch, which does not support table (index) join. This has some impact on the size of the indexes (some fields are repeated once and again, when they could be in a separate table, with cross-references). However, the impact is not large, since those fields tend to be relatively small compared with the whole size of the item. The main impact of this lack of normalization is observed when one of those fields changes, and all items with the old value have to be modified. For example, if the name of an author was wrong, and is fixed, all the items authored for that person need to be fixed.</p>
      <fig id="fig-9" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.601/fig-9</object-id>
        <label>Figure 9</label>
        <caption>
          <title>Description of some fields of the Git enriched indexes.</title>
        </caption>
        <graphic xlink:href="peerj-cs-07-601-g009"/>
      </fig>
      <p>For each of the data sources supported, one or more enriched indexes are produced, aimed to have useful data to produce the metrics that are finally visualized, or used to produce reports. Therefore, aggregated metrics are not a part of the indexes stored in Elasticsearch: they are computed either by the visualizations, or by the tools producing the reports, by aggregating and filtering data present as fields in each of the items. A list of all the fields of all the indexes is also available<xref ref-type="fn" rid="fn-3"><sup>3</sup></xref>
<fn id="fn-3"><label>3</label><p>Full list of fields for all enriched indexes produced by <italic>GrimoireLab</italic>: <uri xlink:href="https://github.com/chaoss/grimoirelab-elk/tree/master/schema">https://github.com/chaoss/grimoirelab-elk/tree/master/schema</uri></p></fn>.</p>
    </sec>
    <sec>
      <title>Identities Management</title>
      <p>Modules in the Identities Management area manage data about personal identities. This allows analysis in which contributor identities and related information (tags), such as team/organization affiliations, are needed. <italic>SortingHat</italic> and <italic>HatStall</italic> are the components in this area. The first one deals with identities management itself, receiving new identities found, grouping them in unique (merged) identities, etc. <italic>HatStall</italic> provides a web-based interface so that users can manually mage identities when needed, thus complementing the algorithmic procedures that <italic>SortingHat</italic> follows. <italic>HatStall</italic> does no management on its own: for any operation on identities, it uses <italic>SortingHat</italic> services.</p>
      <p>For understanding why identities management is convenient in <italic>GrimoireLab</italic>, it is important to notice how personal identities are found in data sources. Depending on the data source, identities come in different formats: commit signatures (e.g., full names and email addresses) in Git repositories, email addresses, GitHub or Slack usernames, etc. Any person may use several identities even in the same repository, and certainly in different data sources. In some cases, an identity can be shared by several contributors (e.g., support email addresses in forums). Finally, identities may need to be linked to other information, in a process we call “tagging”, for certain analysis. For example, affiliation data can be extracted from domains in email addresses, or from other sources, and used to tag unique (merged) identities, so that affiliation information becomes available for actions for the corresponding person even in data sources where the data was not originally available.</p>
      <p>In the usual pipeline, <italic>GrimoireELK</italic> feeds <italic>SortingHat</italic> with identities found in raw data, which deals with merging and tagging according to its configuration, and sends them back to be added to the enriched data. For doing its job, <italic>SortingHat</italic> maintains a relational database with identities and related data, including the origin of each identity, for traceability. <italic>SortingHat</italic> may also automatically read identities-related data in some formats: <italic>Gitdm</italic>, <italic>MailMap</italic>, <italic>Stackalytics</italic>, and the formats used by <italic>Eclipse</italic> and <italic>Mozilla</italic> projects. The overall design of <italic>SortingHat</italic> is summarized in <xref ref-type="fig" rid="fig-10">Fig. 10</xref>. The conceptual schema of the <italic>SortingHat</italic> database is shown in <xref ref-type="fig" rid="fig-11">Fig. 11</xref>. More details are described in (<xref rid="ref-54" ref-type="bibr">Moreno et al., 2019</xref>).</p>
      <fig id="fig-10" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.601/fig-10</object-id>
        <label>Figure 10</label>
        <caption>
          <title>Overview of the structure of SortingHat. Dashed arrows show the flow of new identities, andsolid arrows, the flow of unique (merged) identities.</title>
          <p>The API can be accessed via a CLI (command lineinterface), or HatStall. Usually, GrimoireLab plugs to it via the CLI. The API invokes commands thatquery the database with identities, affiliation and other tags.</p>
        </caption>
        <graphic xlink:href="peerj-cs-07-601-g010"/>
      </fig>
      <fig id="fig-11" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.601/fig-11</object-id>
        <label>Figure 11</label>
        <caption>
          <title>Overview of the SortingHat schema, modeling identities and related information.</title>
        </caption>
        <graphic xlink:href="peerj-cs-07-601-g011"/>
      </fig>
      <p><italic>SortingHat</italic> uses a very conservative approach to merging identities: it uses algorithms that are quite likely to only merge identities that really correspond with the same person. This approach is used because in production environments, experience has shown how erroneously merging identities causes much more problems than failing to merge some identities, and because it can more easily be complemented with manual curation of the data. For example, the naive algorithm of “merge two identities if the email address is present in both, and it is exactly equal”, fails in large datasets for common cases such as “<monospace>root@localhost</monospace>”, merging for example “<monospace>John Smith &lt;root@localhost&gt;</monospace>” with “<monospace>Mary Williams &lt;root@localhost&gt;</monospace>”. <italic>SortingHat</italic> provides this algorithm, which can be activated, but we had to include a deny list with common addresses such as this “<monospace>root@localhost</monospace>” to make it useful.</p>
      <p><italic>SortingHat</italic> periodically merges identities using these conservative algorithms, that can also be activated (or not) in its configuration. If more detail is needed, ingestion of identities data from reliable sources (such as company records, or FOSS Foundation data about its developers), or manual curation (usually via <italic>HatStall</italic>) can be used. However, since <italic>SortingHat</italic> offers an API to manage the identities it stores, more aggressive automatic algorithms for merging them could be easily implemented.</p>
      <p><italic>HatStall</italic> complements the automatic processes followed by <italic>SortingHat</italic>, by providing a web interface that can be used to manually manage identities. That interface permits, for example, manually merging, or adding affiliation data to identities. <italic>HatStall</italic> has proven to be very useful to fix by hand some errors that automatic procedures may cause in complex situations, or to manually complement data related to identities when there are informal sources of knowledge.</p>
      <p>Most identities found in software repositories can be considered as personal information, therefore subject to laws protecting privacy, and to ethical guidelines on the matter. Due to this circumstance, in some cases identity management can deemed unethical, or unlawful (for example, under GDPR (<xref rid="ref-26" ref-type="bibr">European Parliament &amp; Council of the European Union, 2016</xref>), if there is no clear legitimate interest for the processing of personal information, and it is considered that there is no informed consent from identity holders). To have this situation into account, <italic>GrimoireLab</italic> allows for the pseudoanonymization of identities as they are retrieved, via configuration switches in <italic>Perceval</italic> and <italic>GrimoireELK</italic>. If those switches are activated, <italic>Perceval</italic> hashes identities found in retrieved data, and <italic>GrimoireELK</italic> does not use <italic>SortingHat</italic>, producing raw and enriched indexes with pseudoanonymized identities. When orchestration is used, switches are activated with an option in the <italic>Mordred</italic> configuration file.</p>
    </sec>
    <sec>
      <title>Visualization and reporting</title>
      <p>Visualization and reporting are usually the latest stages of any study performed with <italic>GrimoireLab</italic>. They are usually performed by querying data in enriched indexes, and then further processing it until the expected results are produced, or visualizing it. Although any custom program can do this, <italic>GrimoireLab</italic> provides some components that may help in this area:<list list-type="bullet"><list-item><p><italic>Manuscripts</italic> is a tool that queries enriched indexes, providing analytics results such as summary tables, built from templates. Tables are produced in CSV format, thus they can be imported into spreadsheets or other programs. It can also produce reports as PDF documents, including a part of the information in those tables, with some textual explanations. <italic>Manuscripts</italic> therefore produces a certain kind of report for a set of repositories, but can also be used as a template to produce customized reports.</p></list-item><list-item><p>For assisting in the creation and presentation of interactive visualizations, <italic>GrimoireLab</italic> provides three components: <italic>Sigils</italic>, a set of predefined widgets (visualizations and charts); <italic>Kidash</italic>, which loads <italic>Sigils</italic> widgets to <italic>Kibiter</italic>, and <italic>Kibiter</italic>, a soft-fork of Kibana<xref ref-type="fn" rid="fn-4"><sup>4</sup></xref>
<fn id="fn-4"><label>4</label><p>Kibana: <uri xlink:href="https://www.elastic.co/products/kibana">https://www.elastic.co/products/kibana</uri></p></fn>) which provides web-based actionable dashboards (users can interact with the data shown, by filtering, bucketing, drilling down, etc.).</p></list-item></list></p>
      <p>The predefined widgets provided by <italic>Sigils</italic> are organized as a collection of Kibana panels<xref ref-type="fn" rid="fn-5"><sup>5</sup></xref>
<fn id="fn-5"><label>5</label><p>Full list of the descriptions of panels provided by <italic>Sigils</italic> in <uri xlink:href="https://chaoss.github.io/grimoirelab-sigils/">https://chaoss.github.io/grimoirelab-sigils/</uri></p></fn>, usually grouping several metrics in an interactive dashboard that can be used not only to track their evolution over time, but also to drill down in case more details are needed. Some example of those panels and the metrics that they provide are:<list list-type="bullet"><list-item><p>Contributors growth (shown as illustration in <xref ref-type="fig" rid="fig-12">Fig. 12</xref><xref ref-type="fn" rid="fn-6"><sup>6</sup></xref>
<fn id="fn-6"><label>6</label><p>Full description of the Contributors growth panel: <uri xlink:href="https://chaoss.github.io/grimoirelab-sigils/panels/contributors-growth/">https://chaoss.github.io/grimoirelab-sigils/panels/contributors-growth/</uri></p></fn>): total number of contributors, active contributors over time, contributors growth by repository, difference with average of active contributors over time. This panel is offered for most of the data sources (Git, GitHub issues, GitHub pull requests, Gerrit, Bugzilla, Jira, mailing lists, etc), and in each case “contributor” is defined accordingly to the actions in that kind of repository (for Git, it is commit authors, for Bugzilla it is issue reporters, for GitHub pull requests is the pull requester, etc.).</p></list-item><list-item><p>Bugzilla timing<xref ref-type="fn" rid="fn-7"><sup>7</sup></xref>
<fn id="fn-7"><label>7</label><p>Full description of the Bugzilla timing panel: <uri xlink:href="https://chaoss.github.io/grimoirelab-sigils/panels/bugzilla-timing/">https://chaoss.github.io/grimoirelab-sigils/panels/bugzilla-timing/</uri></p></fn>: median and 80% percentile of open time, evolution of the status of issues over time, issues by resolution and issues by severity, evolution of the number of issue submitters over time, table with main submitters, table with latest issues, etc. Similar panels are provided for other issue tracking systems and code review systems.</p></list-item><list-item><p>Gerrit efficiency<xref ref-type="fn" rid="fn-8"><sup>8</sup></xref>
<fn id="fn-8"><label>8</label><p>Full description of the Gerrit efficiency panel: <uri xlink:href="https://chaoss.github.io/grimoirelab-sigils/panels/gerrit-efficiency/">https://chaoss.github.io/grimoirelab-sigils/panels/gerrit-efficiency/</uri></p></fn>: review efficiency index (number of closed divided by open changesets), average and median time to merge, over time. Similar panels are provided for other code review systems.</p></list-item><list-item><p>Jenkins jobs<xref ref-type="fn" rid="fn-9"><sup>9</sup></xref>
<fn id="fn-9"><label>9</label><p>Full description of the Jenkins jobs panel: <uri xlink:href="https://chaoss.github.io/grimoirelab-sigils/panels/jenkins-jobs/">https://chaoss.github.io/grimoirelab-sigils/panels/jenkins-jobs/</uri></p></fn>: total number of builds, jobs, active nodes; proportion of build results; evolution of jobs over time: table with builds, durations, success status per job.</p></list-item></list></p>
      <fig id="fig-12" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.601/fig-12</object-id>
        <label>Figure 12</label>
        <caption>
          <title>Example of information about a Sigils panel: contributors growth.</title>
        </caption>
        <graphic xlink:href="peerj-cs-07-601-g012"/>
      </fig>
      <p>Almost all of the panels are actionable, in the sense that can be filtered by arbitrary periods of time (including selecting time periods in the charts), by specific repositories, by organizations (when this makes sense, such as commits performed by authors of a given organization), etc. For these features, <italic>Sigils</italic> panels use the facilities provided by Kibana, which is querying in real time the enriched indexes stored in Elasticsearch. This setup allows for a lot of flexibility. In addition, users can produce their own visualizations in Kibana, if they have the right permissions.</p>
    </sec>
    <sec>
      <title>Orchestration</title>
      <p><italic>Mordred</italic> can be used to orchestrate all the other components to retrieve data from a set of repositories, produce raw and enriched data, load predefined widgets and generate documents and web-based dashboards. It uses some configuration files, designed to keep sensitive data separated from the one that can be publicly shared. These files include the details for accessing all repositories, including addresses and credentials, and all the servers (e.g., the <italic>SortingHat</italic> database manager). Repositories can be arranged hierarchically in several levels (projects, sub-projects).</p>
      <p><italic>Mordred</italic> also takes care of continuous incremental retrieval. In general, <italic>GrimoireLab</italic> does not use event streams and similar synchronous APIs, because they usually do not allow for the retrieval of past items, which are already not available from them. Instead, it uses timestamps and batch retrieval from APIs that provide all the items in the history of a repository. For allowing this incremental retrieval, <italic>GrimoireELK</italic> includes some metadata in raw and enriched indexes, based on the date when retrieved items were last updated. This metadata can be used to query data sources for all items since last update, and when processing the raw index, all items since the last processed. Even when these techniques in some cases are more complex than those based on event streams, they ensure complete retrieval of all items in the data source at the price of polling it frequently to check if new items are available. Fortunately, most of the use cases allow for some minutes of delay in data processing, which means data sources are not polled too much. <italic>Mordred</italic> instructs <italic>GrimoireELK</italic> about when to poll based on its configuration, and <italic>GrimoireELK</italic> constructs the corresponding queries (to data sources, via <italic>Perceval</italic>, or to raw indexes) using metadata (in raw indexes or in enriched indexes, respectively). When <italic>Arthur</italic> is used, <italic>Mordred</italic> instructs it directly about polling frequencies.</p>
    </sec>
  </sec>
  <sec>
    <title>Combining the Modules</title>
    <p>Due to its structure as a toolset, rather than a monolithic application, <italic>GrimoireLab</italic> modules can be used in many different combinations. In this section we describe some of them, focusing on those that may be more relevant for researchers. First, we illustrate how <italic>GrimoireLab</italic> can be used in some scenarios common in research: data retrieval for a custom analysis; retrieval and storage of data for an exploratory study; and large-scale continuously updated dataset suitable for different studies. Then, we describe in detail three systems that were deployed to fulfill the requirements of specific use cases: a one-time analysis of a large set of repositories, a deployment for continuously analyzing a large software project, and a system providing software development metrics a service. These cases do not intend to show insights on the analyzed data sources, but to show how <italic>GrimoireLab</italic> can be used to collect and process data which could later be used for different purposes, sparing the researcher or the practitioner of the burden and complexities involved.</p>
    <sec>
      <title>Research scenario: data retrieval for custom analysis</title>
      <p><bold>Description:</bold> Analysis of a relatively small number of repositories, retrieving all the data available from the API they provide, using some scripts to answer the research questions.</p>
      <p>
        <bold>Examples:</bold>
        <list list-type="bullet">
          <list-item>
            <p>Changes to the source code: activity and length of comments. <bold>Research objective:</bold> to explore the relationship between activity of developers in modifying the source code, and the details of their comments in those modifications. <bold>Example of RQ:</bold> Are more active developers writing less detailed commits? <bold>Method:</bold> Extraction of all commit records from a small set of Git repositories. For each of them, identification of author, computation of some metrics which could be a proxy for detail (length, number of distinct words, etc.) and estimation of correlations between aggregations of them (mean, median) for each author, and their number of commits.</p>
          </list-item>
          <list-item>
            <p>Complexity of code and change requests. <bold>Research objective:</bold> to explore the relationship between characteristics of code review and the complexity of the code change being reviewed. <bold>Example of RQ:</bold> Are those changes with complexity to the code ore prone to have longer code review processes? <bold>Method:</bold> Retrieval of accepted change requests from a code review system (GitHub pull requests, Gerrit, etc.), and of complexity metrics for the corresponding snapshots in the source code. For each change request, identify the starting and end time of the code review process, and its duration. For each snapshot, identify the added or removed complexity. Then, compute the correlation between duration and added complexity to answer the RQ.</p>
          </list-item>
        </list>
      </p>
      <p><bold><italic>GrimoireLab</italic> components and procedure:</bold> The main component in this scenario is <italic>Perceval</italic>, which will retrieve data from the data sources API. If source code metrics are to be obtained, <italic>Graal</italic> will also be involved. All of them will produce collections of JSON documents that will be stored for further processing. Those JSON documents will be stored and published for reproducibility of the study, and used as the data set for the analysis.</p>
      <p>To illustrate how <italic>GrimoireLab</italic> can be used in this kind of scenario, let’s start with the most most simple case: the retrieval of data from a single repository (in this example, a GitHub repository), as of a file with one JSON document per line, that will be processed later. Each document, in this case, will correspond to an issue or a pull request.</p>
      <p>
        <monospace>$ perceval github [owner] [repo] --json-line &gt; file.json</monospace>
      </p>
      <p>Since the generated JSON documents include fields to identify the mined repository, this operation can be repeated for as many repositories as needed, just adding items to the file, so that all data for a multi-repository analysis can be contained in that file.</p>
      <p>
        <monospace>$ perceval github [owner2] [repo2] --json-line &gt;&gt; file.json</monospace>
      </p>
      <p>
        <monospace>$ perceval github [owner3] [repo3] --json-line &gt;&gt; file.json</monospace>
      </p>
      <p>A similar approach can be used to obtain metrics about files in any checkout of a git repository. In this case, we will use a single command to run <italic>Graal</italic>, which will use <italic>Perceval</italic> (as library) in the background to clone the repository and get the list of commits. Then, <italic>Graal</italic> will run third party tools to obtain complexity metrics for each source code file in each commit, producing a single JSON document:</p>
      <p>
        <monospace>$ graal cocom [repo_url] --git-path [dir_for_clone] &gt; file.json</monospace>
      </p>
      <p><xref ref-type="fig" rid="fig-13">Fig. 13</xref> shows schemes for these three cases (retrieving metadata from a single git repository, from several git repositories, and analyzing some source code metrics for all files in a git repository).</p>
      <fig id="fig-13" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.601/fig-13</object-id>
        <label>Figure 13</label>
        <caption>
          <title>Using Perceval to retrieve data from a single repository (left), or from several (center), andGraal to analyze a git repository (right).</title>
          <p>Arrows represent the data flow from data sources to the JSON documents produced.</p>
        </caption>
        <graphic xlink:href="peerj-cs-07-601-g013"/>
      </fig>
      <p>Instead of the command line version of <italic>Perceval</italic> and <italic>Graal</italic> we can also use them as modules, from a Python script. The script can perform any analysis needed, benefiting from the uniform structure of the dictionaries returned by <italic>Perceval</italic> and <italic>Graal</italic> generators, that can be consumed in loops. Of course, these scripts can be written as Python notebooks, and integrated with the usual Python data analytics tools. The general code structure in this case is as follows (DataSource is a class provided by <italic>Perceval</italic> or <italic>Graal</italic>, which implements fetch as a Python generator). For each item, origin allows to identify the origin repository, and data is a dictionary with the retrieved data.</p>
      <p>
        <monospace>repos = [DataSource([repo1 args]),</monospace>
      </p>
      <p><monospace>DataSource([repo2 args]),</monospace> …<monospace>]</monospace></p>
      <p>
        <monospace>for repo in repos:</monospace>
      </p>
      <p>
        <monospace>for item in repo.fetch():</monospace>
      </p>
      <p><monospace>process(item[</monospace>'<monospace>origin</monospace>'<monospace>],</monospace></p>
      <p><monospace>item[</monospace>'<monospace>data</monospace>'<monospace>])</monospace></p>
      <p>Following this code structure, see below an example program to obtain the number of spelling errors in git comments per year for a collection of GitLab Git repositories. This example assumes spell_errors returns the number of spelling errors for a certain string, and get_year gets the year from a Git date.</p>
      <p><monospace>repos = [Git(uri=</monospace>'<monospace>https://gitlab.com/owner/repo1</monospace>'<monospace>, </monospace>…<monospace>),</monospace></p>
      <p><monospace>Git(uri=</monospace>'<monospace>https://gitlab.com/owner/repo2</monospace>’<monospace>, </monospace>…<monospace>),</monospace></p>
      <p>…</p>
      <p><monospace>Git(uri=</monospace>'<monospace>https://gitlab.com/owner/repon</monospace>'<monospace>, </monospace>…<monospace>)]</monospace></p>
      <p>
        <monospace>terrors = { }</monospace>
      </p>
      <p>
        <monospace>cerrors = { }</monospace>
      </p>
      <p>
        <monospace>for repo in repos:</monospace>
      </p>
      <p>
        <monospace>for item in repo.fetch( ):</monospace>
      </p>
      <p><monospace>print(f</monospace>''<monospace>Processing {item[</monospace>'<monospace>data</monospace>'<monospace>][</monospace>'<monospace>commit</monospace>'<monospace>]} from {item[</monospace>'<monospace>origin</monospace>'<monospace>]}</monospace>''<monospace>)</monospace></p>
      <p><monospace>errors = spell_errors(item[</monospace>'<monospace>data</monospace>'<monospace>][</monospace>'<monospace>message</monospace>'<monospace>])</monospace></p>
      <p>
        <monospace>if errors &gt; 0:</monospace>
      </p>
      <p><monospace>year = get_year(item[</monospace>'<monospace>data</monospace>'<monospace>][</monospace>'<monospace>AuthorDate</monospace>'<monospace>])</monospace></p>
      <p>
        <monospace>terrors[year] = terrors.get(year, 0) + errors</monospace>
      </p>
      <p>
        <monospace>cerrors[year] = cerrors.get(year, 0) + 1</monospace>
      </p>
      <p>
        <monospace>for year in sorted(terrors):</monospace>
      </p>
      <p>
        <monospace>c = cerrors[year]</monospace>
      </p>
      <p>
        <monospace>t = terrors[year]</monospace>
      </p>
      <p><monospace>print(f</monospace>''<monospace>{year}: {c} commits with errors, {t} total errors</monospace>''<monospace>)</monospace></p>
    </sec>
    <sec>
      <title>Research scenario: retrieval and storage for exploratory study</title>
      <p><bold>Description:</bold> Retrieval of data from a large collection of repositories to store it in a database, so that it can be later analyzed as a part of an exploratory study.</p>
      <p>
        <bold>Examples:</bold>
        <list list-type="bullet">
          <list-item>
            <p>Relationship between how bug reports are closed and developer retention. <bold>Research objective:</bold> To explore how the timing, or other features, related to how bug reports are closed, could influence core developer retention in a FOSS (free, open source software) project. <bold>Example of RQ:</bold> Does a longer time-to-close for bug reports cause developers to stop earlier contributing to the source code of a project? <bold>Method:</bold> Retrieve data about the issues (including bug reports) for a large and diverse set of FOSS projects, if possible with different issue tracking systems, so that specific features of it don’t affect the results. Retrieve data from the source code management system of the same projects. Once all the data retrieved is stored in a database, use it to explore different proxies for time-to-close bug reports and for estimating periods of continuous contribution. For estimating time-to-close, explore different strategies for telling bug reports apart from other issues (machine learning on title and description, tags, etc). For estimating periods of contribution explore different approaches (maximum period without contributions, number of contributions over a certain period, etc.) to tell apart frequent (likely core) contributors from casual contributors. Then, explore how to estimate the period until stopping contributions (considering extending temporary periods, such as vacation). Once the most reliable method is exactly defined, conduct the study in as many repositories as possible.</p>
          </list-item>
          <list-item>
            <p>Personal trajectories in software development. <bold>Research objective:</bold> Explore ways to track trajectories of developers, by analyzing their footprints in different kinds of software development repositories. <bold>Example of RQ:</bold> Do core contributors usually follow a path from messages in communication channels to issue submitters, to code review submitters? <bold>Method:</bold> Retrieve data from mailing lists, GitHub issues and pull requests, and GitHub Git repositories, for a large collection of projects. Merge identities using email addresses for linking identities in email messages to identities in Git commits, and the GitHub commit API to link email addresses to GitHub user IDs. If possible, improve identities data by manually merging and de-merging identities using other data sources (for example, public Internet profiles). Once the identities data is curated, use it to identify contributions by persons in all data sources, and explore the different tracks followed.</p>
          </list-item>
        </list>
      </p>
      <p><bold><italic>GrimoireLab</italic> components and procedure:</bold> For these kind of studies, <italic>GrimoireLab</italic> enriched indexes would be convenient, and could be complemented, if needed, with <italic>GrimoireLab</italic> raw indexes that will be produced anyway. Using <italic>GrimoireELK</italic> for the data collection and enrichment ensures that the indexes will be properly stored in Elasticsearch databases. <italic>SortingHat</italic> will be used when identity merging is important for the study (as in the second example above). Kibana can be used to visualize the indexes in the enriched database, which can be useful for the exploratory study. For example, Kibana can easily show the activity of a single person in all data sources over time.</p>
      <p>In this case (see <xref ref-type="fig" rid="fig-14">Fig. 14</xref>), <italic>GrimoireELK</italic> will run <italic>Perceval</italic> to retrieve data from repositories, and then store it in Elasticsearch raw indexes. Then, <italic>GrimoireELK</italic> processes them, creating enriched indexes. Usually, researchers will write scripts to query enriched indexes, since they are easier to query and process. But they can also query raw indexes if they need some detail that is only available in them. Since there are Elasticsearch modules for many programming languages, scripts can be written in any of them. Indexes can also be dumped as JSON files, that can be consumed directly by scripts, or uploaded to another Elasticsearch instance, where fellow researchers can work with exactly the same data.</p>
      <fig id="fig-14" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.601/fig-14</object-id>
        <label>Figure 14</label>
        <caption>
          <title>Using GrimoireELK to produce Elasticsearch raw and enriched indexes.</title>
          <p>Arrows show thedata flow from data sources to database indexes and finally to scripts that query them.</p>
        </caption>
        <graphic xlink:href="peerj-cs-07-601-g014"/>
      </fig>
      <p>If identity management is needed (as in the second example), <italic>SortingHat</italic> will be used (see <xref ref-type="fig" rid="fig-15">Fig. 15</xref>). When processing raw indexes, <italic>GrimoireELK</italic> will extract identities found in raw items, providing them to <italic>SortingHat</italic>. For each identity in a raw item, <italic>SortingHat</italic> will return its corresponding unique (merged) identity, and tagging information for it, that <italic>GrimoireELK</italic> will use when producing the enriched database. <italic>GrimoireELK</italic> can also access the GitHub commit API to obtain relationships between email addresses and GitHub user ids, and inject that data to <italic>SortingHat</italic>. <italic>SortingHat</italic> can run simple exact email-address matching algorithms to merge identities. Via <italic>HatStall</italic>, researchers can curate the resulting merged identities manually.</p>
      <fig id="fig-15" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.601/fig-15</object-id>
        <label>Figure 15</label>
        <caption>
          <title>Using GrimoireELK and SortingHat. Data is consumed by Kibana.</title>
        </caption>
        <graphic xlink:href="peerj-cs-07-601-g015"/>
      </fig>
      <p>The introduction of the database allows for the massive collection of data, just by running (sequentially or in parallel) <italic>GrimoireELK</italic> for the different repositories to be mined. The database can easily include data for hundreds or even thousands of repositories of different kinds of data sources. The availability of enriched indexes, which are summarized, flat versions of the data obtained from the data source APIs, also allows for easy import in data structures such as Python/Pandas or R data frames, and visualizations using tools, such as Kibana or Graphana that can connect to Elasticsearch. Files produced when dumping the data in the database are also the core of good reproduction packages, and a simple way to exchange and archive data for other researchers.</p>
    </sec>
    <sec>
      <title>Research scenario: large-scale, continuously updated dataset</title>
      <p><bold>Description:</bold> Production of a large-scale, continuously updated dataset, with data for projects of interest using different kinds of data sources</p>
      <p>
        <bold>Example:</bold>
        <list list-type="bullet">
          <list-item>
            <p>Dataset about all the projects hosted by the Apache Foundation <bold>Research objective:</bold> To produce a dataset that may help to better understand software development processes used in Apache projects. <bold>Example of RQ:</bold> Which ones are the different patterns of joining and leaving Apache projects? <bold>Method:</bold> Obtain the description of all the Apache projects, maintained by the Apache Foundation. Since this description includes links to all data sources (and repositories) used by those projects, produce a comprehensive list of all repositories that should be visited to maintain the dataset. Then, do a first retrieval of data from all of them, update it by frequent periodic visits, and dump it in a file that can be easily shared with researchers. Apache projects use, in different projects, Git repositories, GitHub projects for issues and pull requests, Bugzilla for issues, and change requests, mailing lists, and some other kinds of data sources, thus all of them need to be mined.</p>
          </list-item>
        </list>
      </p>
      <p><bold><italic>GrimoireLab</italic> components and procedure:</bold> In this scenario, involving thousands, maybe tens of thousands of repositories, from several different data sources, new problems arise. It is no longer possible to just use a single script to call <italic>GrimoireELK</italic>. Configuration and organization of the retrieval process becomes an issue, and for the continuous update it is important to keep raw and enriched indexes in sync with updates in the repositories, in presence of network or other infrastructure temporary failures. In these cases, <italic>Mordred</italic> can be used to orchestrate the setting (see <xref ref-type="fig" rid="fig-16">Fig. 16</xref>).</p>
      <fig id="fig-16" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.601/fig-16</object-id>
        <label>Figure 16</label>
        <caption>
          <title>Mordred driving GrimoireELK, SortingHat, and other components.</title>
        </caption>
        <graphic xlink:href="peerj-cs-07-601-g016"/>
      </fig>
      <p>To configure this setting, once this system is deployed, the list of repositories to analyze (their URLs) is written in a JSON file. Then, another file is used to configure <italic>Mordred</italic> specifying the details of the deployment (such as polling periods, kind of identity unification to perform, or specific processing to the data). For example in this file it is specified how often data sources are visited for incremental retrieval.</p>
      <p>In some cases it is convenient to schedule the retrieval as a collection of tasks that can run in parallel. This happens for example when we can benefit from several nodes analyzing different Git repositories in parallel, or when several nodes can consume a certain API quicker than a single one. In these cases we can add <italic>Arthur</italic>, which will schedule <italic>Perceval</italic> and <italic>Graal</italic> jobs taking into account aspects such as availability of tokens to access data sources, or refresh periods (how often data will be retrieved incrementally from repositories). <italic>Arthur</italic> uses a Redis database to manage jobs and batches of retrieved items (see <xref ref-type="fig" rid="fig-17">Fig. 17</xref>).</p>
      <fig id="fig-17" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.601/fig-17</object-id>
        <label>Figure 17</label>
        <caption>
          <title>A GrimoireLab system including Mordred and Arthur.</title>
        </caption>
        <graphic xlink:href="peerj-cs-07-601-g017"/>
      </fig>
      <p>In this scenario, we can review the main interactions between <italic>GrimoireLab</italic> components:<list list-type="bullet"><list-item><p><italic>Perceval</italic> retrieves data from repositories.</p></list-item><list-item><p>For Git repositories, <italic>Graal</italic> analyzes source code, by running third party tools with the help of <italic>Perceval</italic>.</p></list-item><list-item><p><italic>Arthur</italic> schedules <italic>Perceval</italic> and <italic>Graal</italic> jobs in workers, to organize the retrieval.</p></list-item><list-item><p><italic>GrimoireELK</italic> receives retrieved items to produce raw indexes in Elasticsearch, to some extent replicating data sources.</p></list-item><list-item><p><italic>GrimoireELK</italic> interacts with <italic>SortingHat</italic> to store new identities in its database and be informed about merged identities and their tags.</p></list-item><list-item><p>Using data from raw indexes and <italic>SortingHat</italic>, <italic>GrimoireELK</italic> produces enriched indexes in Elasticsearch. These indexes (and raw indexes, when convenient) can be analyzed with scripts.</p></list-item><list-item><p><italic>Mordred</italic> orchestrates all the process, according to the information in its configuration files, deciding which repositories to retrieve, how enriched indexes are produced, when data should be updated, etc.</p></list-item></list></p>
      <p>In all the cases when Kibana is used for interactively visualizing data (see <xref ref-type="fig" rid="fig-15">Fig. 15</xref>), <italic>Sigils</italic> provides a set of ready-to-use visualizations and dashboards. See examples of a summary dashboard provided by <italic>Sigils</italic> in <xref ref-type="fig" rid="fig-18">Figs. 18</xref> and <xref ref-type="fig" rid="fig-19">19</xref>. The use of <italic>Arthur</italic> is optional: users can write their own schedulers, if they prefer.</p>
      <fig id="fig-18" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.601/fig-18</object-id>
        <label>Figure 18</label>
        <caption>
          <title>Metrics summary dashboard, produced with GrimoireLab for the GrimoireLab project.</title>
        </caption>
        <graphic xlink:href="peerj-cs-07-601-g018"/>
      </fig>
      <fig id="fig-19" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.601/fig-19</object-id>
        <label>Figure 19</label>
        <caption>
          <title>Example of summary of metrics over time, produced with GrimoireLab, in this case byvisualizing enriched indexes in Kibana (data for OPNFV project as of January 2020).</title>
          <p>Information isretrieved from Git, Jira, Gerrit, mailing lists, IRC, Confluence and Jenkins repositories.</p>
        </caption>
        <graphic xlink:href="peerj-cs-07-601-g019"/>
      </fig>
      <p>The kind of studies that can be done in this setting is similar to those done on subsets of repositories in GHTorrent, for example, but letting researchers decide both the kinds of data sources they want, and the specific projects they target (be them in GitHub or not). The drawback, of course, is that once the list of repositories is defined, researchers need to deploy the system, configure it, and wait until the data is obtained from the different data sources.</p>
    </sec>
    <sec>
      <title>Use case: one-time analysis of a collection of repositories</title>
      <p><bold>Requirements:</bold> One-time retrieval of all the data from two kinds of data sources (Git and GitHub) for a medium sized list of repositories, all of them related to IoT (Internet of Things).</p>
      <p><bold>Magnitudes:</bold> See <xref rid="table-1" ref-type="table">Table 1</xref>.</p>
      <table-wrap id="table-1" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.601/table-1</object-id>
        <label>Table 1</label>
        <caption>
          <title>Magnitudes of the IoT repositories case.</title>
        </caption>
        <alternatives>
          <graphic xlink:href="peerj-cs-07-601-g022"/>
          <table frame="hsides" rules="groups" content-type="text">
            <colgroup span="1">
              <col span="1"/>
              <col span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Git repos:</th>
                <th rowspan="1" colspan="1">54</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">GitHub repos:</td>
                <td rowspan="1" colspan="1">48</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Commits (items in raw index)</td>
                <td rowspan="1" colspan="1">276,860</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Issues &amp; pull requests (items in raw index)</td>
                <td rowspan="1" colspan="1">95,370</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p><bold><italic>GrimoireLab</italic> setup:</bold> The setup corresponds to the description of the research scenario “Large-scale, continuously updated dataset”, described in Subsection “Research scenario: Large-scale, continuously updated dataset” (<xref ref-type="fig" rid="fig-16">Fig. 16</xref>), although in this case the data retrieval was performed once, and not updated later. All data retrieval and analysis was done in a single thread.</p>
      <p>Everything was run by <italic>Mordred</italic>, which started with data retrieval in two threads: one cloning and then extracting metadata from Git repositories (for all commits in all of them), the other one accessing the GitHub API to retrieve issues and pull requests for all repositories, using three API tokens. In each of the threads, once the retrieval for all repositories is complete, with the production of the corresponding raw index, the analysis of the retrieved data starts, until all the items (commits in the case of Git repositories, issues and pull requests in the case of the GitHub API) are analyzed.</p>
      <p><xref rid="table-2" ref-type="table">Table 2</xref> shows when the most relevant stages of this case started and finished, and their duration. The deployment was in a 2.5 GHz CPU with 4 cores, 8 GB of RAM, SSD storage. In both tables, “git” refers to the analysis of Git repositories, “github” to the analysis of GitHub issues and pull requests retrieved from the GitHub API. Data collection for “git” includes cloning of Git repositories for GitHub, and production of the raw index by analyzing those clones. Data collection for “github” includes waiting periods while the API tokens are exhausted, and calls to the API to resolve identities, not only retrieval of issues and pull requests. Two API tokens were used in this case.</p>
      <table-wrap id="table-2" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.601/table-2</object-id>
        <label>Table 2</label>
        <caption>
          <title>Main stages of data collection and enrichment for the IoT repositories case.</title>
        </caption>
        <alternatives>
          <graphic xlink:href="peerj-cs-07-601-g023"/>
          <table frame="hsides" rules="groups" content-type="text">
            <colgroup span="1">
              <col span="1"/>
              <col span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Elapsed time</th>
                <th rowspan="1" colspan="1">Event</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">00:00:00</td>
                <td rowspan="1" colspan="1">Starting</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">00:00:14</td>
                <td rowspan="1" colspan="1">Retrieval starts (git, github)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">00:43:00</td>
                <td rowspan="1" colspan="1">Retrieval finished (git)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">00:43:10</td>
                <td rowspan="1" colspan="1">Enrichment starts (git)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">01:05:19</td>
                <td rowspan="1" colspan="1">Enrichment finished (git)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">10:09:18</td>
                <td rowspan="1" colspan="1">Retrieval finished (github)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">10:16:29</td>
                <td rowspan="1" colspan="1">Enrichment starts (github)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">10:51:37</td>
                <td rowspan="1" colspan="1">Enrichment finished (github)</td>
              </tr>
            </tbody>
          </table>
          <table frame="hsides" rules="groups" content-type="text">
            <colgroup span="1">
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Action</th>
                <th rowspan="1" colspan="1">Duration</th>
                <th rowspan="1" colspan="1">Performance</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">Retrieval (git)</td>
                <td rowspan="1" colspan="1">00:42:46</td>
                <td rowspan="1" colspan="1">108 commits/s</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Retrieval (github)</td>
                <td rowspan="1" colspan="1">10:09:04</td>
                <td rowspan="1" colspan="1">2.6 items/s</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Enrichment (git)</td>
                <td rowspan="1" colspan="1">00:22:09</td>
                <td rowspan="1" colspan="1">208 commits/s</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Enrichment (github)</td>
                <td rowspan="1" colspan="1">00:35:08</td>
                <td rowspan="1" colspan="1">45 items/s</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
        <table-wrap-foot>
          <fn id="table-2fn">
            <p>
              <bold>Note:</bold>
            </p>
          </fn>
          <fn id="table-2fn1" fn-type="other">
            <p>Top: elapsed time for the main recorded events. Bottom: duration and performance. Elapsed time and duration are in hours. Items are GitHub issues and pull requests. “Retrieval finished (git)” means all git repositories were cloned and their metadata was stored in the raw index. “Retrieval finished (github)”, means all data was retrieved from the GitHub API (issues and pull requests), and it was stored in the raw index.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>Also in <xref rid="table-2" ref-type="table">Table 2</xref>, it can be seen how the performance is close to the maximum allowed by the GitHub API token rate: 5,000 calls per hour, or about 83 calls per minute. Using two tokens, that maximum would amount to 163 calls per minute. Processing of Git commits is much faster, since it does not involve API rates, and is limited only by the Git server response time, download time, and git processing. Enrichment processes for git are much faster than for GitHub because they are also lighter: in the GitHub case they include the computing of some duration metrics, based on the data in the raw index, which is a bit longer to retrieve.</p>
    </sec>
    <sec>
      <title>Use case: continuous analysis of a large project</title>
      <p><bold>Requirements:</bold> Continuous analysis of all relevant repositories, from several data sources, of a large project (all software promoted by Wikimedia Foundation).</p>
      <p><bold>Magnitudes:</bold> See <xref rid="table-3" ref-type="table">Table 3</xref>.</p>
      <table-wrap id="table-3" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.601/table-3</object-id>
        <label>Table 3</label>
        <caption>
          <title>Magnitudes of the GrimoireLab deployment for analyzing Wikimedia Foundation projects (enriched data), as of January 10, 2020.</title>
        </caption>
        <alternatives>
          <graphic xlink:href="peerj-cs-07-601-g024"/>
          <table frame="hsides" rules="groups" content-type="text">
            <colgroup span="1">
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Data source</th>
                <th rowspan="1" colspan="1">Repos</th>
                <th rowspan="1" colspan="1">Items</th>
                <th rowspan="1" colspan="1">Items no.</th>
                <th rowspan="1" colspan="1">Size (GB)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">Git</td>
                <td rowspan="1" colspan="1">2,675</td>
                <td rowspan="1" colspan="1">Commits</td>
                <td rowspan="1" colspan="1">1,647,481</td>
                <td rowspan="1" colspan="1">4.9</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Git (AOC)</td>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">File revisions</td>
                <td rowspan="1" colspan="1">12,309,316</td>
                <td rowspan="1" colspan="1">7.6</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Gerrit</td>
                <td rowspan="1" colspan="1">2,098</td>
                <td rowspan="1" colspan="1">Patchsets</td>
                <td rowspan="1" colspan="1">7,461,755</td>
                <td rowspan="1" colspan="1">27.8</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Maniphest</td>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Issues</td>
                <td rowspan="1" colspan="1">231,833</td>
                <td rowspan="1" colspan="1">0.44</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Mailing lists</td>
                <td rowspan="1" colspan="1">46</td>
                <td rowspan="1" colspan="1">Messages</td>
                <td rowspan="1" colspan="1">263,419</td>
                <td rowspan="1" colspan="1">0.59</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Mediawiki</td>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Pages</td>
                <td rowspan="1" colspan="1">1,116,469</td>
                <td rowspan="1" colspan="1">0.83</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p><bold><italic>GrimoireLab</italic> setup:</bold> The setup corresponds to the description of the research scenario “Large-scale, continuously updated dataset”, described in Subsection “Research scenario: Large-scale, continuously updated dataset” (<xref ref-type="fig" rid="fig-16">Fig. 16</xref>), including continuous update and identity management.</p>
      <p>Since identity management is included in this use case, identities found during the production of the enriched indexes are by <italic>GrimoireELK</italic> to <italic>SortingHat</italic>, to get the corresponding merged identity. Therefore, enriched indexes include the identifier of the merged identity, which permits that persons with several identities are considered as a single person (merged identity). Since continuous update is configured, after enrichment threads sleep for a configurable amount of time (300 s by default), and then restart the process, retrieving incrementally new data. In a separate process, run periodically (usually between incremental retrieval phases), <italic>SortingHat</italic> processes its data finding new cases of identities to merge, and enriched indexes are modified adding these new merged identities to their items.</p>
      <p>In the case of the Wikimedia Foundation Git is used for code management, Gerrit for code review, Maniphest (issue tracker in the Phabricator forge) for issue tracking, mailing lists for asynchronous communication, and Mediawiki for documentation. In <xref rid="table-3" ref-type="table">Table 3</xref> there is no repository count for Maniphest because issues in Phabricator are not organized in repositories, and in Mediawiki because in that case data is organized in pages (61,301). “Git AOC” is the enriched index for areas of code analysis, with Git data for each version of each file. The deployment has been running continuously for more than 4 years, retrieving data incrementally from repositories, except for downtime due to stopping and restarting the system due to the deployment of a version. The dashboard showing visualizations for main metrics, using standard <italic>Sigils</italic> visualizations, is available publicly online<xref ref-type="fn" rid="fn-10"><sup>10</sup></xref>
<fn id="fn-10"><label>10</label><p>Dashboard for all Wikimedia projects: <uri xlink:href="https://wikimedia.biterg.io">https://wikimedia.biterg.io</uri> (last visited on March 10 2021)</p></fn> (see screenshot of its entry page in <xref ref-type="fig" rid="fig-20">Fig. 20</xref>).</p>
      <fig id="fig-20" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.601/fig-20</object-id>
        <label>Figure 20</label>
        <caption>
          <title>Entry page to the Wikimedia dashboard, produced with GrimoireLab.</title>
        </caption>
        <graphic xlink:href="peerj-cs-07-601-g020"/>
      </fig>
      <p>This use case shows how <italic>GrimoireLab</italic> can be used in production, for continuously analyzing large-scale projects, during long periods of time. We consider this use case, which is representative of a number of others similar, as an illustration of the maturity, and adaptation to real-world constraints, of the toolset.</p>
    </sec>
    <sec>
      <title>Use case: metrics as a service</title>
      <p><bold>Requirements:</bold> Industrial-grade deployment, for retrieval of an arbitrary, potentially very large (tens of thousands of repositories), and visualization of some of the main metrics of arbitrary groups of repositories in it.</p>
      <p><bold>Magnitudes:</bold> See <xref rid="table-4" ref-type="table">Table 4</xref>.</p>
      <table-wrap id="table-4" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.601/table-4</object-id>
        <label>Table 4</label>
        <caption>
          <title>Some magnitudes for Cauldron, as of March 10, 202.</title>
        </caption>
        <alternatives>
          <graphic xlink:href="peerj-cs-07-601-g025"/>
          <table frame="hsides" rules="groups" content-type="text">
            <colgroup span="1">
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Data source</th>
                <th rowspan="1" colspan="1">Index</th>
                <th rowspan="1" colspan="1">Repos</th>
                <th rowspan="1" colspan="1">Items</th>
                <th rowspan="1" colspan="1">Size (GB)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">Git</td>
                <td rowspan="1" colspan="1">Raw</td>
                <td rowspan="1" colspan="1">50,280</td>
                <td rowspan="1" colspan="1">76,269,234</td>
                <td rowspan="1" colspan="1">136.1</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Git</td>
                <td rowspan="1" colspan="1">Enriched</td>
                <td rowspan="1" colspan="1">50,280</td>
                <td rowspan="1" colspan="1">76,134,443</td>
                <td rowspan="1" colspan="1">135</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">GitHub</td>
                <td rowspan="1" colspan="1">Raw</td>
                <td rowspan="1" colspan="1">40,907</td>
                <td rowspan="1" colspan="1">9,089,137</td>
                <td rowspan="1" colspan="1">38.1</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">GitHub</td>
                <td rowspan="1" colspan="1">Enriched</td>
                <td rowspan="1" colspan="1">40,907</td>
                <td rowspan="1" colspan="1">9,016,199</td>
                <td rowspan="1" colspan="1">9.6</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">GitLab</td>
                <td rowspan="1" colspan="1">Raw</td>
                <td rowspan="1" colspan="1">3,073</td>
                <td rowspan="1" colspan="1">282,739</td>
                <td rowspan="1" colspan="1">2.2</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">GitLab</td>
                <td rowspan="1" colspan="1">Enriched</td>
                <td rowspan="1" colspan="1">3,073</td>
                <td rowspan="1" colspan="1">282,598</td>
                <td rowspan="1" colspan="1">0.3</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p><bold><italic>GrimoireLab</italic> setup:</bold> The setup corresponds to the description of the research scenario “Large-scale, continuously updated dataset”, described in Subsection “Research scenario: Large-scale, continuously updated dataset” (<xref ref-type="fig" rid="fig-16">Fig. 16</xref>), not including identity management (therefore <italic>SortingHat</italic> and <italic>HatStall</italic> are not used), and with a specialized scheduling system for <italic>Mordred</italic> jobs, that are serviced by an arbitrary number of parallel workers.</p>
      <p>The system described in this use case is <italic>Cauldron</italic><xref ref-type="fn" rid="fn-11"><sup>11</sup></xref>
<fn id="fn-11"><label>11</label><p><italic>Cauldron</italic>: <uri xlink:href="https://cauldron.io">https://cauldron.io</uri></p></fn>, which provides customized dashboards with data obtained from FOSS software development repositories. Users can order an analysis of as many repositories as they want, organized in projects (collections of repositories). <italic>Cauldron</italic> uses <italic>GrimoireLab</italic> to retrieve data, analyze it, and provide visualizations. The system has been running for more than 14 months, allowing users to select GitHub, GitLab (issues and change requests in both cases), and Git repositories to analyze.</p>
      <p>Data is retrieved by <italic>Perceval</italic>, by cloning repositories, and the analysis is performed by <italic>GrimoireELK</italic>. For GitHub and GitLab, issues and pull requests (or merge requests) are retrieved by <italic>Perceval</italic> from the GitHub API, using access tokens provided by users, and then enriched indexes are produced by <italic>GrimoireELK</italic>. Several instances of <italic>Mordred</italic> run in parallel (in different workers) driving the retrieval and analysis of some repositories each, according to users’ demand. The system is designed to grow at least one order of magnitude larger with no change. As of January 10, 2021, it served 732 users.</p>
      <p>In addition to some of the visualization panels provided by <italic>Sigils</italic>, Cauldron offers also more than 50 different Kibana visualizations, and a summary of more than 40 metrics as charts produced with JavaScript, using data provided by a Django API that queries directly the Elasticsearch enriched indexes produced by <italic>GrimoireLab</italic>. The main view for a project in Cauldron (see <xref ref-type="fig" rid="fig-21">Fig. 21</xref>) includes four of these visualizations, showing the extensibility of <italic>GrimoireLab</italic>, in this case to interface to external visualization services.</p>
      <fig id="fig-21" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.601/fig-21</object-id>
        <label>Figure 21</label>
        <caption>
          <title>Main view provided by Cauldron for an analyzed project (January 2021).</title>
        </caption>
        <graphic xlink:href="peerj-cs-07-601-g021"/>
      </fig>
      <p>Cauldron is one of the largest <italic>GrimoireLab</italic> deployments, by size of analyzed data. Therefore, the numbers shown in <xref rid="table-4" ref-type="table">Table 4</xref> can be used to estimate a lower limit of the scale (by number of repositories, by number of items in those repositories) that can be analyzed with <italic>GrimoireLab</italic>.</p>
      <p>For benchmarking <italic>Cauldron</italic> more precisely, and with it <italic>GrimoireLab</italic>, we set up a specific instance of <italic>Cauldron</italic> running in a single machine (2.5 GHz CPU with 4 cores, 16 GB of RAM, SSD storage). We configured it to analyze the complete GNOME organization in the GitLab instance maintained by the GNOME project<xref ref-type="fn" rid="fn-12"><sup>12</sup></xref>
<fn id="fn-12"><label>12</label><p>GNOME GitLab instance: <uri xlink:href="https://gitlab.gnome.org/GNOME">https://gitlab.gnome.org/GNOME</uri></p></fn>: a total of 564 GitLab repositories, each of them with the corresponding Git repository. We started with an empty database, and we analyzed nothing else in parallel. We had 15 workers, deployed as Docker containers with <italic>GrimoireLab</italic> installed in them. At any given time, each worker runs at most a single job, corresponding to a certain repository, for which it was producing either raw or the enriched data. A single GitLab token was used for the experiment.</p>
      <p>The main performance metrics of this experiment are detailed in <xref rid="table-5" ref-type="table">Table 5</xref>. Several different processes were measured: production of raw or enriched indexes for each type of item (commits, issues, merge requests), production of both indexes (“complete”) for each type of item, and production of all the indexes (“all”). Timing for “all (complete)” shows how long did the experiment need to complete (almost 19 h of clock time), and how much processing time it needed from workers (about 58.5 h). With 19 h of clock time and 15 workers, it is clear how most of the time workers were idle, usually waiting for the token rate to be reset. In general, throughput (in items per time) numbers are similar to those presented in Section “Use case: One-time analysis of a collection of repositories”. As we already observed in that case, processing for Git is much faster than for GitLab (or GitHub), because for them the API token rate is a strong limitation. The table also shows how processing time for enriching processes is one or two orders of magnitude shorter than clock time. The main cause of this loss of performance is the access to the database, which is done frequently to ensure enriched data is soon in stable storage. In the context of the total time elapsed for the retrieval and analysis, this decision does not cause long delays. The access to the database could be improved by using caching, Elasticsearch shards (which allow for parallel access), and by improving the hardware setup, which was not designed in this case to optimize database access.</p>
      <table-wrap id="table-5" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.601/table-5</object-id>
        <label>Table 5</label>
        <caption>
          <title>Some metrics for the GNOME GitLab experiment.</title>
        </caption>
        <alternatives>
          <graphic xlink:href="peerj-cs-07-601-g026"/>
          <table frame="hsides" rules="groups" content-type="text">
            <colgroup span="1">
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Process</th>
                <th rowspan="1" colspan="1">Items</th>
                <th rowspan="1" colspan="1">Clock time</th>
                <th rowspan="1" colspan="1">Processing time</th>
                <th rowspan="1" colspan="1">Items/s (clock)</th>
                <th rowspan="1" colspan="1">Items/s (proc)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">All (complete)</td>
                <td rowspan="1" colspan="1">1,460,054</td>
                <td rowspan="1" colspan="1">18:52:37.64</td>
                <td rowspan="1" colspan="1">58:33:36.27</td>
                <td rowspan="1" colspan="1">21.48</td>
                <td rowspan="1" colspan="1">6.93</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Commits (complete)</td>
                <td rowspan="1" colspan="1">2,676,345</td>
                <td rowspan="1" colspan="1">00:41:48.67</td>
                <td rowspan="1" colspan="1">04:37:40.86</td>
                <td rowspan="1" colspan="1">1066.84</td>
                <td rowspan="1" colspan="1">160.64</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Commits (raw)</td>
                <td rowspan="1" colspan="1">1,369,386</td>
                <td rowspan="1" colspan="1">00:38:13.50</td>
                <td rowspan="1" colspan="1">02:46:13.62</td>
                <td rowspan="1" colspan="1">597.07</td>
                <td rowspan="1" colspan="1">137.30</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Commits (enriched)</td>
                <td rowspan="1" colspan="1">1,306,959</td>
                <td rowspan="1" colspan="1">00:41:31.60</td>
                <td rowspan="1" colspan="1">01:51:27.24</td>
                <td rowspan="1" colspan="1">524.55</td>
                <td rowspan="1" colspan="1">195.44</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Issues (complete)</td>
                <td rowspan="1" colspan="1">122,294</td>
                <td rowspan="1" colspan="1">18:52:25.05</td>
                <td rowspan="1" colspan="1">25:31:54.53</td>
                <td rowspan="1" colspan="1">1.80</td>
                <td rowspan="1" colspan="1">1.33</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Issues (raw)</td>
                <td rowspan="1" colspan="1">61,432</td>
                <td rowspan="1" colspan="1">18:03:12.00</td>
                <td rowspan="1" colspan="1">25:28:12.98</td>
                <td rowspan="1" colspan="1">0.95</td>
                <td rowspan="1" colspan="1">0.67</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Issues (enriched)</td>
                <td rowspan="1" colspan="1">60,862</td>
                <td rowspan="1" colspan="1">18:52:11.13</td>
                <td rowspan="1" colspan="1">00:03:41.56</td>
                <td rowspan="1" colspan="1">0.90</td>
                <td rowspan="1" colspan="1">274.70</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Merges (complete)</td>
                <td rowspan="1" colspan="1">58,311</td>
                <td rowspan="1" colspan="1">18:52:27.07</td>
                <td rowspan="1" colspan="1">28:24:00.87</td>
                <td rowspan="1" colspan="1">0.86</td>
                <td rowspan="1" colspan="1">0.57</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Merges (raw)</td>
                <td rowspan="1" colspan="1">29,236</td>
                <td rowspan="1" colspan="1">18:52:00.07</td>
                <td rowspan="1" colspan="1">28:20:52.28</td>
                <td rowspan="1" colspan="1">0.43</td>
                <td rowspan="1" colspan="1">0.29</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Merges (enriched)</td>
                <td rowspan="1" colspan="1">29,075</td>
                <td rowspan="1" colspan="1">18:52:12.41</td>
                <td rowspan="1" colspan="1">00:03:08.59</td>
                <td rowspan="1" colspan="1">0.43</td>
                <td rowspan="1" colspan="1">154.17</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
        <table-wrap-foot>
          <fn id="table-5fn">
            <p>
              <bold>Note:</bold>
            </p>
          </fn>
          <fn id="table-5fn1" fn-type="other">
            <p>Processing time is the accumulated processing time of all the jobs in all the workers, for the specified process. Items/sec is the number of items processed per second, either for clock time, or for processing time.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
  </sec>
  <sec sec-type="discussion">
    <title>Discussion</title>
    <p><italic>GrimoireLab</italic> is the result of many years of research and development in the area of tools for software development analytics. A part of the people involved in building it had the experience of building another, less ambitious toolset, <italic>MetricsGrimoire</italic>. This was fundamental to identify problems and features for both the research and industrial scenarios. In this section we discuss the main features of <italic>GrimoireLab</italic> along with their rationale, provide some notes on its usage for research, and present some lessons learned from its usage in industrial settings.</p>
    <sec>
      <title>Main features and their rationale</title>
      <p>Some of the most relevant features of <italic>GrimoireLab</italic>, and their rationale, are:<list list-type="bullet"><list-item><p>Minimizing interactions with data sources. Accessing data sources causes stress to them, which may lead to being banned. In addition, retrieving data from data sources is usually slow, compared to just accessing it locally. <italic>GrimoireLab</italic> follows the strategy of getting as much data as reasonably possible from the data source, and storing it to avoid retrieving it again in the future. Raw indexes are used with this objective, and to some extent, enriched indexes too: once an item is in the index, there is no need to retrieve it again, with no loss of functionality. A consequence is that retrieval is incremental whenever possible: when synchronizing with a data source, only changes since the last retrieval will be requested.</p></list-item><list-item><p>Support of non-uniform data sources. A data source may be implemented by different versions of a software system. The code needs to detect the version of the data source API, and access the data in slightly different ways (for example, using different parameters in a call to the API) depending on the version detected<xref ref-type="fn" rid="fn-13"><sup>13</sup></xref>
<fn id="fn-13"><label>13</label><p>For example, there are many versions of Bugzilla, providing similar but not exactly equal APIs. One of these differences is the name of the parameter to order a list of issues by date: Last + Changed (prior to the 3.4 branch) or changeddate (starting with 3.4 branch). The difference is minimum, but enough to break the retrieval process if it uses the wrong name.</p></fn>. <italic>GrimoireLab</italic> deals with these problems by hiding all data source API details, providing a unified API to data consumers, which can thus be written as modules independent from these details.</p></list-item><list-item><p>Support of non-uniform data provided by data sources. Different versions of data sources, or even deployments of the same version, may provide different data details. For example, different instances of the same issue tracking system may configure states or additional fields to tickets or comments. In some cases those changes are significant for an analysis, but in some others they are not. <italic>GrimoireLab</italic> deals with this diversity by being agnostic with respect to the data provided by data sources: each item will be a Python dictionary with any needed structure, that can be flattened as a JSON document. If different versions of a data source produce different fields, they will just be mapped to the corresponding parts of the dictionary. This means that the code may be more generic, and more resilient to changes in data structure.</p></list-item><list-item><p>Unified APIs and data formats. For building a toolbox, reusing code, and enabling users to reuse code, is fundamental. It is key to identify points in the usual pipelines where a single API can be provided, with independence of the data sources considered, and data formats that are common, specially when the code dealing with them can be reused. <italic>GrimoireLab</italic> does this in several areas. <italic>Perceval</italic> provides a common API to all data sources. The data produced by <italic>Perceval</italic> is always a dictionary, with some common fields useful for traceability (for example the data source, or when the item was retrieved), and can be dumped in a JSON document, so that it can be easily consumed. Elasticsearch provides a well known and documented API for storing and retrieving both raw and enriched indexes, which consumers can query for the JSON documents they need. <italic>SortingHat</italic> provides a common API for managing identities that can be used from any component. Data formats for raw and enriched indexes are defined with the same general structure, so that code for, for example, dealing with date formats or identities, can be made generic. Finally, enriched indexes are designed so they can be consumed directly by most visualization systems: we have tested them with Kibana, Grafana, and several Python-based visualization libraries.</p></list-item><list-item><p>Separation of raw indexes and enriched indexes. This separation has proven very useful to isolate problems. Data retrieval problems are “encapsulated” in raw indexes. Once they have been solved, and a correct raw index has been produced, there is no need to come back to the original data source to compute anything: querying the raw index is enough. Querying the raw index instead of retrieving from the data source is also much more efficient, and does not cause stress in the infrastructure supporting the data source. Then, enriched indexes are simpler, and more convenient for visualization tools. Since they are flat, they can be queried more easily and efficiently than raw indexes, which need to maintain a nested structure close to the data in the data source. Enriched indexes are also enriched with summary metrics computed at the item level, but not present directly in the original data source. For example, from the number of lines added and removed by a commit to a file, the total number of added and removed lines can be pre-computed, or from the submission and closing time of an issue report, the time-open can be computed. Since these metrics are computed from data in the item, they are invariant as long as the raw item does not change.</p></list-item><list-item><p>Aggregated metrics are, in general, not stored in the indexes, but computed by visualization or reporting tools when needed. This approach is the result of a tradeoff between the convenience of having all data (including aggregated data) readily available in the database, and the flexibility of allowing many different aggregations to be performed, and the easy addition of new data, at any point, with little overhead. On the one hand, having aggregated data readily available is convenient, because it can be accessed and shown right away. However, we wanted a more flexible setup, where the consumer of the data may decide how to aggregate it. This is common with actionable visualization tools, where every time the user defines a different time period, or filters data in a different way, aggregations change. In addition, we wanted to add data to the database incrementally and frequently, as new data is available in the original data sources. Having pre-computed aggregations in the database forces to re-compute them every time new data is added, which means an overhead, especially large for large databases with many different aggregations. Fortunately, when the consumer wants some aggregation, it can usually be computed by adding it to the database query, which makes the operation efficient and relatively simple.</p></list-item><list-item><p>Data merging. Even when data is retrieved from different data sources, an analysis may want to consider common aspects of all retrieved data. For example, a study on onboarding in a project may be interested in knowing about contributors to all repositories in all its data sources. Thus, including facilities for merging data is fundamental, and <italic>GrimoireLab</italic> recognizes it. Date fields are all converted to the same format, and have uniform names (eg, creation_date), so they are easily merged in, for example, a Pandas data frame, and in “person fields” such as the author as well. All items include fields to track the repository of origin, and fields for hierarchical organization of repositories. <italic>SortingHat</italic> provides a unified view of all contributors, by merging identities in different data sources, mapping all of them to the same uuid which is used in enriched indexes. When data sources are similar, common fields are identified, so that data can be merged and to some extent compared. For example, for issue trackers there are fields with opening and closing dates. Finally, the fact that data is stored in Elasticsearch may allow for merging directly when querying, such as “all data for all projects from date A to date B, authored by person P”.</p></list-item><list-item><p>Consumer-agnostic. It is very difficult to know how some person will exploit the data in the future. Since our approach is “retrieve and store”, stored data may be useful even long after it was retrieved, or by a different team. <italic>GrimoireLab</italic> makes as few assumptions about analysis or visualization tools as possible. Enriched indexes are usually flat because then they are easier to import in visualization and analysis tools (for example, import into a Pandas data frame), but this is most of it. Thanks to this approach, we have tested the exploitation of data with several visualization tools, and the most common toolsets used for analysis. Anything that can consume JSON documents from Elasticsearch can be easily used to visualize or further analyze data produced by <italic>GrimoireLab</italic>.</p></list-item><list-item><p>Performance and efficiency. When retrieving data from many repositories, performance is an issue. From one point of view, data sources should be used efficiently to avoid unnecessary stress. From another, we want the data as soon as possible. To address the first approach, <italic>GrimoireLab</italic> builds on an extensive research of data source APIs and retrieval options, to avoid those that could cause trouble. This is one of the reasons for using a toolset like this if you are not familiar enough with the details of data retrieval: avoiding causing unnecessary pain to the data source. Incremental access, and storage of data in raw indexes to avoid recurrent retrievals are a consequence of this goal. To address the second aspect, <italic>GrimoireLab</italic> can do data retrieval and enrichment in parallel, with components such as <italic>Arthur</italic> and <italic>Mordred</italic> in charge of scheduling and coordinating concurrent activities. For several data sources, <italic>GrimoireLab</italic> allows also for the use of a pool of API rate tokens, when available and allowed, which may increase data retrieval speed.</p></list-item><list-item><p>Identity management. Dealing with personal identities is a key aspect of many studies about software development. Even for studies as simple as counting contributors, you need to find out the several identities that the same person may be using. <italic>SortingHat</italic> deals with these issues, plus the problem of tagging people (for example, mapping them to the company they work for). <italic>SortingHat</italic> is no silver bullet: it just uses some heuristics, allows for the importing of identity data when it is available, and provides means (via command line, or via the <italic>HatStall</italic> web application) to manually merge and tag identities. Since <italic>SortingHat</italic> is integrated in the toolchain, changes reflected in its identities database are later reflected in any enriched index. A related problem to identities is privacy: in many cases, identities should not be provided to consumers of the data, to respect privacy of the persons participating in projects. Currently, there is on-going work in <italic>GrimoireLab</italic> to improve the situation in this area.</p></list-item><list-item><p>Long-term performance. For dealing with the retrieval and analysis of large projects, the system must run for extended periods of time. During that time, failures occur: token rate exhaustion, network glitches, server reboots, even host shutdowns. <italic>GrimoireLab</italic> recovers as nicely as possible by retrying, and by continuing after failure. <italic>Perceval</italic>, <italic>Arthur</italic> and <italic>Mordred</italic> are designed to retry when the API rate provided by a token is exhausted, or when some network failures happen. For dealing with continuation, new runs of <italic>GrimoireELK</italic> and <italic>Mordred</italic> can be configured to be incremental, checking in the database the last items retrieved and enriched, and following from there.</p></list-item><list-item><p>Data maintainability. Data retrieved and enriched should be easily inspected, so that people using it can detect errors. Errors can happen, for example, when the wrong repository is configured, or when timestamps are shifted due to misconfigured default timezones, or when bugs in the code produce some field with errors. If detected, errors should be fixed with minimum impact on the original data sources. <italic>GrimoireLab</italic> components store data in databases, so that errors can be fixed directly on them. For example, a bug due to incorrect parsing of a date format can be fixed by substituting wrong dates in enriched indexes from data in raw indexes. Some fields are also included in all items to assist traceability.</p></list-item><list-item><p>Modularity. Since the kinds of consumption of data will be diverse, a toolset provides more flexibility, with many of its components being able to work on their own, but also in different combinations. Raw and enriched indexes provide good synchronization points for components in pipelines, and information hiding (such as configuration data in <italic>Mordred</italic> or identities data in <italic>SortingHat</italic>) helps to keep each problem domain within its own component, exposing only an API hiding unneeded details.</p></list-item><list-item><p>Extensibility. In several aspects, <italic>GrimoireLab</italic> provides a vanilla system that can be easily extended to fit specific needs. The most clear cases of extensibility are:
<list list-type="simple"><list-item><p>-New visualizations. <italic>Kibiter</italic> (or Kibana) allows for the creation of new visualizations, and arrangement of them in dashboards. All the process can be done via the graphical user interface, and only requires some knowledge about the data in Elasticsearch indexes, and some training on the Kibana user interface. New visualizations can be created from scratch, or by modifying those provided by <italic>Sigils</italic>. Both new visualization and <italic>Sigils</italic> visualizations can be mixed in dashboards. Once these visualizations and dashboards are created, they benefit from the data produced by the rest of <italic>GrimoireLab</italic>.</p></list-item><list-item><p>-New indexes. <italic>GrimoireELK</italic> provides a simple mechanism for creating new enriched indexes: studies. A <italic>GrimoireELK</italic> study is a Python script, with a certain structure, that basically is fed with raw or enriched indexes, and produces a new index tailored for some specific analysis. <italic>GrimoireLab</italic> provides some of these studies, for example for analyzing the joining and leaving processes of a project (enrollment, abandonment, experience, etc.). These studies can be used as templates for producing new ones. Studies are run by <italic>Mordred</italic>, so that they are easily integrated in the data retrieval and analysis pipelines, and new visualizations can be produced for the indexes they produce.</p></list-item><list-item><p>-New data sources. Supporting a new data source with <italic>GrimoireLab</italic> amounts to building some modules which integrate with the rest of the system. The process starts by building a new <italic>Perceval</italic> client, which will implement a Python generator that will retrieve data from the intended source, and produce dictionaries with a common structure. This client usually will automatically plug into the <italic>Perceval</italic> backend, producing JSON documents that will be stored in Elasticsearch by <italic>Arthur</italic> and <italic>GrimoireELK</italic>. Then, enrichment code has to be inserted in <italic>GrimoireELK</italic> so that enriched indexes can be produced, usually by selecting which fields from raw items should be copied, or transformed, into fields in the enriched index. If identities are to be managed, the appropriate calls to <italic>SortingHat</italic> will be included in this code too. Finally, new visualizations have to be produced in <italic>Kibiter</italic> to show the data in these new enriched indexes.</p></list-item></list></p></list-item></list></p>
      <p><italic>GrimoireLab</italic> has been evolving for several years, improving incrementally in dealing with all these issues. Unfortunately, many of them appeared while the system was already in use and evolving, which means that they needed to be addressed on the go, leading in some cases to not so clean solutions.</p>
    </sec>
    <sec>
      <title><italic>GrimoireLab</italic> for researchers</title>
      <p>In the specific case of researchers, <italic>GrimoireLab</italic> can contribute to solve some usual problems:<list list-type="bullet"><list-item><p>Data retrieval software. For some kinds of data sources, writing some script to retrieve data is not difficult, if only some casual data is wanted. But retrieving large datasets in an efficient way, with minimal stress on the mined infrastructure, it is not that easy. <italic>GrimoireLab</italic> provides a simple way of getting the data needed for a study in reasonable time, just by deploying it with the right configuration.</p></list-item><list-item><p>Reproducibility. <italic>GrimoireLab</italic> helps reproducibility in two different ways, depending on the starting point of the reproduction study. To assist in full reproduction, starting by retrieving the data from data sources (maybe with new data in them), researchers can declare the version of <italic>GrimoireLab</italic> used, and its configuration files, in addition to the processing scripts. That would be enough for anyone to get the same data again, provided API of the data sources didn’t change. If they did, it is likely that new versions of <italic>GrimoireLab</italic> adapted, so the retrieval can still be tried with those newer versions. For improved reproducibility, the complete list of <italic>GrimoireLab</italic> versions and dependencies can be provided, using common Python tools (for example, versions freeze of a virtual environment), or versions of Docker images. To assist on reproducibility from the retrieved dataset, raw and enriched indexes can be dumped, so that other researchers use them as their starting point, or compare them with those they get. This can be done at the <italic>Perceval</italic> level, producing mirrors of the relevant data sources, or by dumping data from Elasticsearch with customary tools like <monospace>elasticdump</monospace>.</p></list-item><list-item><p>Collaboration in producing datasets. The use of <italic>GrimoireLab</italic> by different research groups allows the production of collaborative collections of data, resulting from merging the datasets produced by each group. The groups producing the collection should just use a common codification for repository identifiers, and <italic>GrimoireLab</italic> raw indexes can just be put together to produce a raw index for the collection. A “common codification” means using the same identifier for the same repository: for example, if they are accessible via different URLs, ensuring a common one is decided. Since all items in raw indexes have origin fields, with the identifier for the repository, if the same repository is more than once in the datasets, its items will clash. A simple policy, such as “if two items clash, use the most recent one” will allow for the production of a collection with no duplicated items. Metastudies can then easily run on the aggregated dataset.</p></list-item></list></p>
      <p>However, at least in some cases, new problems may appear:<list list-type="bullet"><list-item><p>Bugs. Researchers using <italic>GrimoireLab</italic> become dependent on it producing data correctly. This makes their studies subject to bugs or errors in <italic>GrimoireLab</italic> or its configuration. Experience shows that it is more likely that a single researcher writing code makes mistakes causing bugs, than a software used in many different scenarios by many different people. <italic>GrimoireLab</italic> uses unit testing to prevent new bugs and regressions, with relatively high test coverage (<italic>Graal</italic>: 99%, <italic>Perceval</italic>: 98%, <italic>SortingHat</italic>: 93%, <italic>GrimoireELK</italic>: 82%, <italic>Mordred</italic>: 63%). But still, if there are bugs in some component, those could cause wrong data to entire datasets. Therefore, data checking should still be an important part of any research using <italic>GrimoireLab</italic>.</p></list-item><list-item><p>Adaptability. If a study is designed as a function of what can be done with <italic>GrimoireLab</italic>, or what can be done easily with it, there is a risk of focusing on what the toolset can do, and being limited by it. Researchers should confront the <italic>GrimoireLab</italic> model, designing new components when needed, or realizing when it is not well suited for a certain kind of study.</p></list-item><list-item><p>Evolution and poor documentation. Even when <italic>GrimoireLab</italic> features extensive documentation, including a tutorial, and specific documentation for modules, it is not always easy to know what it can do, or how to tailor it to specific needs. This, together with the fact that <italic>GrimoireLab</italic> is for now a moving target, continuously evolving, may make it complex to use in some cases. Its community is already working on improving documentation, and keeping it updated, but still this may be an issue.</p></list-item></list></p>
    </sec>
    <sec>
      <title>Industrial use</title>
      <p><italic>GrimoireLab</italic> has been used for more than 5 years by a company to provide metrics as a service (with deployments tailored to the needs of customers) and consulting services based on software development metrics. To know about the main lessons learned after this experience, we conducted some conversations with key persons in the company (some of them co-authors of this paper). These are the main takeaways obtained from those conversations (some are specifically related to the tools, some other are more general of using metrics to track software development):<list list-type="bullet"><list-item><p>Importance of automation and configuration. Since the deployment of <italic>GrimoireLab</italic> requires a relatively small effort, and is fully automated once configuration files are ready, most of the effort for running systems is in maintenance. And most of the maintenance tasks can be traced to external events (such as recovery from failures) or configuration changes (due to changes in the data sources to track, for example). When both cases are covered, maintenance of continuously working instances of GrimoireLab are negligible. To cover those cases, the company built scripts to react to common events and changes in configuration (most of them already a part of <italic>Mordred</italic> and other <italic>GrimoireLab</italic> components).</p></list-item><list-item><p>Traceability of data items. In some cases, when specific items are checked, some error in them can be found. This could be tracked to errors in data collection, or more usually, in data enrichment. In any case, in those cases it is fundamental to be able to trace as much as possible how the item was produced, so that it can be linked to specific versions of the software, and to the corresponding log files. This is one of the reasons to keep detailed information in every item about the version of <italic>GrimoireLab</italic> used to produce it, and about the exact date when it was produced.</p></list-item><list-item><p>Importance of details. Some details that in other environments could be considered as minor are not minor when people invested in the analyzed projects have access to the data. For example, the name of a person should be correct, even if during some time it was wrong in some repository. For example, all contributions of a person, or of all persons working of a company, in different data sources, have to be found in the final reports. Even if the impact in the final numbers is minor, errors in these cases diminishes the trust on the number provided in the reports, in general.</p></list-item><list-item><p>Capturing complexity with metrics. Real software development is complex, and that complexity has to be captured when producing useful metrics about it. For example, even apparently simple metrics, such as number of bug reports still open are difficult to present at a given time spot, as a metric suitable for comparison with previous time spots, to learn if the situation is improving or not. First of all, bug reports have to be identified among all issues, which usually requires careful work with developers who label them. Then, important details need to be taken into account: impact of duplicates, impact of bots closing old bug reports, policies of closing bug reports without actually fixing them (for example, for difficult to reproduce cases), etc. If a certain project has less bug reports today than 1 month ago, it is important to know if the reason is a bot, running yesterday, and closing all issues older than six months. This kind of complexity happens everywhere: pair-programming makes many metrics based on code changes and code review difficult to apply; which Git branches to take into account may cause big differences, etc. Many of these cases can be dealt with options on how retrieval or enrichment works (for example, selection of branches, detection of bots closing issues, different metrics if pair-programming is used). Therefore, letting users filter the data the way they need, or configure the retrieval and analysis processes, has to be supported by the tools.</p></list-item><list-item><p>No single metric characterizes software development. There is not a single metric, not even a small set of metrics, that can characterize software development in a given project. Depending on the goals, relevant metrics are different, depending on the project, how to compute those metrics is different. Therefore, every stakeholder in every project needs the flexibility to select what metrics to track, and to tune them to their needs. Tools, again, need to provide as much flexibility as possible for this.</p></list-item><list-item><p>Explanation of metrics matter. In some cases, a specific set of metrics can capture the fundamental aspects of some process. But still, it is important to explain well why and how those metrics are a good characterization, and what happens when they change. For example, time-to-close, measured over the issues closed during the last month tells a different story than time-to-close, measured for the issues opened during the last month. Both are important, but both are radically different. Quite usually, when one of them goes up, the other may go down, and both things could be positive or negative depending on the goals of the project. If people are to engage with the metrics they need some training, or at least some explanation, of how they can use and understand the metrics in a way that helps them to understand the underlying trends and processes.</p></list-item><list-item><p>The problem of gaming. If some metrics are used as a proxy for individual or collective performance, it is important to find incentives to avoid that gaming. And one of the best incentives comes from transparency: when anyone can see how your metrics are composed, and how are your individual (or corporate) contributions, it is much easier to have collective control over them. For this, the tool needs to be able to show not only aggregated metrics, but also to allow for drilling down, showing how aggregated metrics are composed.</p></list-item></list></p>
    </sec>
  </sec>
  <sec>
    <title>Related Work</title>
    <p>The idea of providing analytics for software development, which would help to track performance, and improve processes, is not new (see summaries of past experiences in (<xref rid="ref-5" ref-type="bibr">Buse &amp; Zimmermann, 2010</xref>; <xref rid="ref-53" ref-type="bibr">Menzies &amp; Zimmermann, 2013</xref>; <xref rid="ref-75" ref-type="bibr">Zhang et al., 2013</xref>)). Many companies also realized early the benefits of building their internal systems for performing software development analytics on the software they produced, as is shown for example in (<xref rid="ref-17" ref-type="bibr">Czerwonka et al., 2013</xref>). During the last decade, these ideas have been explored mainly in two areas: the creation (and in some cases, maintenance) of large datasets for researchers and practitioners, and the tools designed to assist in the creation and analysis of datasets about specific software repositories. In this section we discuss related work in both areas. As a brief summary of this discussion <xref rid="table-6" ref-type="table">Table 6</xref> provides a briefing about some features of some of the systems most comparable to <italic>GrimoireLab</italic>.</p>
    <table-wrap id="table-6" orientation="portrait" position="float">
      <object-id pub-id-type="doi">10.7717/peerj-cs.601/table-6</object-id>
      <label>Table 6</label>
      <caption>
        <title>Feature analysis of some related work.</title>
      </caption>
      <alternatives>
        <graphic xlink:href="peerj-cs-07-601-g027"/>
        <table frame="hsides" rules="groups" content-type="text">
          <colgroup span="1">
            <col span="1"/>
            <col span="1"/>
            <col span="1"/>
            <col span="1"/>
            <col span="1"/>
            <col span="1"/>
            <col span="1"/>
            <col span="1"/>
            <col span="1"/>
            <col span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1">GrimoireLab</th>
              <th rowspan="1" colspan="1">GHTorrent</th>
              <th rowspan="1" colspan="1">GitHub Archive</th>
              <th rowspan="1" colspan="1">Boa</th>
              <th rowspan="1" colspan="1">Gitana</th>
              <th rowspan="1" colspan="1">PyDriller</th>
              <th rowspan="1" colspan="1">Metrics Grimoire</th>
              <th rowspan="1" colspan="1">Kibble</th>
              <th rowspan="1" colspan="1">SmartSHARK</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Kind</td>
              <td rowspan="1" colspan="1">Toolset</td>
              <td rowspan="1" colspan="1">Dataset</td>
              <td rowspan="1" colspan="1">Dataset</td>
              <td rowspan="1" colspan="1">Dataset</td>
              <td rowspan="1" colspan="1">Toolset</td>
              <td rowspan="1" colspan="1">Toolset</td>
              <td rowspan="1" colspan="1">Toolset</td>
              <td rowspan="1" colspan="1">Toolset</td>
              <td rowspan="1" colspan="1">Toolset</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Data sources</td>
              <td rowspan="1" colspan="1">34</td>
              <td rowspan="1" colspan="1">2</td>
              <td rowspan="1" colspan="1">1</td>
              <td rowspan="1" colspan="1">1</td>
              <td rowspan="1" colspan="1">8</td>
              <td rowspan="1" colspan="1">1</td>
              <td rowspan="1" colspan="1">15</td>
              <td rowspan="1" colspan="1">13</td>
              <td rowspan="1" colspan="1">8</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Storage</td>
              <td rowspan="1" colspan="1">Elastic, MySQL</td>
              <td rowspan="1" colspan="1">MongoDB, MySQL</td>
              <td rowspan="1" colspan="1">BigQuery</td>
              <td rowspan="1" colspan="1">Hadoop</td>
              <td rowspan="1" colspan="1">MySQL</td>
              <td rowspan="1" colspan="1">None</td>
              <td rowspan="1" colspan="1">MySQL</td>
              <td rowspan="1" colspan="1">Elastic</td>
              <td rowspan="1" colspan="1">MongoDB</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Scope</td>
              <td rowspan="1" colspan="1">Selected repos</td>
              <td rowspan="1" colspan="1">GitHub complete</td>
              <td rowspan="1" colspan="1">Complete gitHub</td>
              <td rowspan="1" colspan="1">GitHub subset</td>
              <td rowspan="1" colspan="1">Selected repos</td>
              <td rowspan="1" colspan="1">Selected repos</td>
              <td rowspan="1" colspan="1">Selected repos</td>
              <td rowspan="1" colspan="1">Selected repos</td>
              <td rowspan="1" colspan="1">Selected repos</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Visualization</td>
              <td rowspan="1" colspan="1">Yes</td>
              <td rowspan="1" colspan="1">No</td>
              <td rowspan="1" colspan="1">No</td>
              <td rowspan="1" colspan="1">No</td>
              <td rowspan="1" colspan="1">No</td>
              <td rowspan="1" colspan="1">No</td>
              <td rowspan="1" colspan="1">Yes</td>
              <td rowspan="1" colspan="1">Yes</td>
              <td rowspan="1" colspan="1">Yes</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Reporting</td>
              <td rowspan="1" colspan="1">Yes</td>
              <td rowspan="1" colspan="1">No</td>
              <td rowspan="1" colspan="1">No</td>
              <td rowspan="1" colspan="1">No</td>
              <td rowspan="1" colspan="1">No</td>
              <td rowspan="1" colspan="1">No</td>
              <td rowspan="1" colspan="1">Yes</td>
              <td rowspan="1" colspan="1">Yes</td>
              <td rowspan="1" colspan="1">No</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Continuous</td>
              <td rowspan="1" colspan="1">Yes</td>
              <td rowspan="1" colspan="1">Yes</td>
              <td rowspan="1" colspan="1">Yes</td>
              <td rowspan="1" colspan="1">No</td>
              <td rowspan="1" colspan="1">No</td>
              <td rowspan="1" colspan="1">No</td>
              <td rowspan="1" colspan="1">Yes</td>
              <td rowspan="1" colspan="1">Yes</td>
              <td rowspan="1" colspan="1">Yes</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Raw</td>
              <td rowspan="1" colspan="1">Yes</td>
              <td rowspan="1" colspan="1">Yes</td>
              <td rowspan="1" colspan="1">Yes</td>
              <td rowspan="1" colspan="1">No</td>
              <td rowspan="1" colspan="1">No</td>
              <td rowspan="1" colspan="1">Yes</td>
              <td rowspan="1" colspan="1">No</td>
              <td rowspan="1" colspan="1">No</td>
              <td rowspan="1" colspan="1">No</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Processed</td>
              <td rowspan="1" colspan="1">Yes</td>
              <td rowspan="1" colspan="1">Yes</td>
              <td rowspan="1" colspan="1">No</td>
              <td rowspan="1" colspan="1">Yes</td>
              <td rowspan="1" colspan="1">Yes</td>
              <td rowspan="1" colspan="1">No</td>
              <td rowspan="1" colspan="1">Yes</td>
              <td rowspan="1" colspan="1">Yes</td>
              <td rowspan="1" colspan="1">Yes</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Identities</td>
              <td rowspan="1" colspan="1">Manual heurist</td>
              <td rowspan="1" colspan="1">GitHub</td>
              <td rowspan="1" colspan="1">GitHub</td>
              <td rowspan="1" colspan="1">No</td>
              <td rowspan="1" colspan="1">Manual</td>
              <td rowspan="1" colspan="1">No</td>
              <td rowspan="1" colspan="1">Manual heurist,</td>
              <td rowspan="1" colspan="1">No</td>
              <td rowspan="1" colspan="1">Heurist</td>
            </tr>
          </tbody>
        </table>
      </alternatives>
      <table-wrap-foot>
        <fn id="table-6fn">
          <p>
            <bold>Note:</bold>
          </p>
        </fn>
        <fn id="table-6fn1" fn-type="other">
          <p>The analysis is based on published literature, and in some cases, direct experience with the system. <italic>Kind</italic> shows if the system is specifically targeted at producing a dataset (“Dataset”), or intended as a generic toolset (“Toolset”). In <italic>Data sources</italic> we have considered different GitHub or GitLab APIs as different data sources, and code analysis as a single data source. In <italic>Storage</italic> we have considered the main storage systems, “Elastic” stands for Elasticsearch. In <italic>Scope</italic> “Selected repos” means that the user can select any collection of repositories for retrieval and storage (if applicable) from the set of supported data sources, “GitHub Complete” means that all of GitHub is retrieved and stored, “GitHub Subset” means that a pre-defined subset of GitHub repositories is retrieved and stored. <italic>Visualization</italic> and <italic>Reporting</italic> show if there are visualization and reporting components in the system. “Continuous” shows if the system can perform continuous retrieval. <italic>Raw</italic> and <italic>Processed</italic> shows if raw data (as obtained from the data source) and processed data (data with some processing more suitable for analysis) are available, stored, for further analysis. <italic>Identities</italic> show if identity management is included: “Manual” stands for “support for manual management”, “Heurist.” stands for “components performing heuristics for identity management”, “GitHub” stands for “uses GitHub users API only”.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <sec>
      <title>Large datasets about software development</title>
      <p>Among the large datasets about software development with their own software used to create and maintain them, we can differentiate two kinds: those specialized in source code, and those focused on other data related to software development (including metadata about source code changes, but usually not source code itself). The keystone systems of the second kind are:<list list-type="bullet"><list-item><p>The <italic>SourceForge Research Data Archive (SRDA)</italic> (<xref rid="ref-74" ref-type="bibr">Van Antwerp &amp; Madey, 2008</xref>) was the first dataset organized and maintained to allow researchers to have metadata about a large number of software development repositories. It provided reduced dumps of SourceForge, which was during most of the 2000s decade the preferred hosting site (software forge) for FOSS projects. At about the same time, FLOSSmole (<xref rid="ref-39" ref-type="bibr">Howison, Conklin &amp; Crowston, 2006</xref>) retrieved data from SourceForge, and later other software development hosting sites, via their APIs, offering a wealth of metadata for researchers. FLOSSmole was to our knowledge the first system to perform a method, large-scale data retrieval via their API from large forges, making its data available to third parties. The kind of metadata they provide for each project include project description, project status, programming language, developers, license, programming language, and some general statistics about it.</p></list-item><list-item><p><italic>FLOSSMetrics</italic> (<xref rid="ref-38" ref-type="bibr">Herraiz et al., 2009</xref>) and <italic>SQO-OSS</italic> (“Software quality observatory for open source software” (<xref rid="ref-33" ref-type="bibr">Gousios et al., 2007</xref>)) had similar aims: to collect not only metadata about the projects, but as much data as possible about software development (the complete list of commits or issues, for example), via the APIs provided by software forges. Both produced their own software to that aim (<italic>MetricsGrimoire</italic> and Alitheia Core, mentioned below), which they used to collect data from hundreds of software repositories (commit records, issue reports, code review discussions, asynchronous communication via mailing lists, etc.), being some of the first demonstrators of how massive retrieval of data from software forges could be performed. Both worked with a diverse collection of data sources. FLOSSMetrics and SQO-OSS started the path towards automated collection of many different data kinds about software development, which is also the goal of <italic>GrimoireLab</italic>. Many of the features provided by <italic>GrimoireLab</italic> were first demonstrated, or at least set as a long-term goal, by those systems: retrieval of data from many different data sources; automated retrieval, storage, and analysis; fault-tolerance; massive collection and analysis; etc. The tools provided by <italic>GrimoireLab</italic> could be composed to produce systems similar to FLOSSMetrics and SQO-OSS, and <italic>GrimoireLab</italic> owes these systems the architecture based on storing data for further analysis (instead of having to retrieve it once and again from the original data sources), and the idea of incremental retrieval, fundamental for efficient retrieval of data from repositories already visited. In all of them, the basic idea was to interfere as little as possible with the infrastructure provided by the original data sources.</p></list-item><list-item><p><italic>GHTorrent</italic> (<xref rid="ref-35" ref-type="bibr">Gousios &amp; Spinellis, 2012</xref>) and <italic>GHArchive</italic> (<xref rid="ref-36" ref-type="bibr">Grigorik, 2022</xref>) were developed later, focusing on the GitHub platform. Both work by querying the GitHub API to produce a complete dataset including most of the events noticeable in it (code commit records, issue reports, pull requests, changes in repository metadata). Both developed their own software for the retrieval and curation of the data. In the case of GHTorrent, curation includes linking actors of events to GitHub users, and adding metainformation provided by the GitHub repositories API (such as number of stars or programming languages used), that make the dataset more valuable. Given that both are focused on GitHub, both have specialized components tailored to optimize the retrieval of data from the GitHub events channel.</p></list-item></list></p>
      <p>In this respect, the GitHub backend of <italic>Perceval</italic> in <italic>GrimoireLab</italic> offers a similar functionality, but using the API for projects. In the end, both APIs (the projects API and the events channel) offer similar data, but with different perspectives. The former provides a log with all the data “as it is today”: all issues and pull requests, with details about all comments, timing, etc, for example. The latter provides similar data “as it happens”: as an issue or pull request is modified, a new event is received through the channel. The events API is therefore designed to be consumed as time passes (although it has some memory, of 300 events per end-point and not more than 3 months in the past). This means that the events channel needs to be parsed for all the projects of interest, and the information you cannot consume at a certain point, cannot be retrieved again. It also does not permit to retrieve data since the beginning of the activity for each project. The projects API, on the contrary, can be queried at any time, allowing to get most of the history pertaining to that repository at any point in time. <italic>GrimoireLab</italic> uses the projects API approach to be able to selectively retrieve data from any repository, at any point in time, since the project started. In addition, the <italic>Perceval</italic> backend for Git directly clones Git repositories, instead of relying on commit events produced by GitHub. Again, this allows it to get at any point in time most of the history of any given repository.</p>
      <p>With respect to systems massively retrieving and archiving source code, and in some cases doing some kind of analysis on it, we can mention:<list list-type="bullet"><list-item><p><italic>Sourcerer</italic> (<xref rid="ref-4" ref-type="bibr">Bajracharya et al., 2006</xref>) was designed to be a search engine for source code. It retrieved code from source code repositories, analyzed it, and produced a database designed to be queried. In some sense, it can be considered the ancestor of systems massively archiving source code and allowing queries on it.</p></list-item><list-item><p><italic>Boa</italic> is a system to massively collect source code, analyze it, and store some of its characteristics in a database, allowing researchers to query on them. Boa also provides a programming language (also named Boa), supported by an infrastructure, to ease queries and studies about large quantities of source code repositories. In its first incarnation (<xref rid="ref-24" ref-type="bibr">Dyer et al., 2013b</xref>, <xref rid="ref-25" ref-type="bibr">Dyer et al., 2015</xref>) it extracted metadata and source code from SourceForge (a popular software development forge at that time), storing it in mirrored Subversion repositories and a Hadoop cluster, which is where the queries were actually run. It was later complemented with a massive number of repositories from GitHub and other sources. Boa was designed to let users create simple programs that allowed for a quick and comprehensive exploration of all the data.</p></list-item><list-item><p><italic>Debsources</italic> (<xref rid="ref-7" ref-type="bibr">Caneill, Germán &amp; Zacchiroli, 2017</xref>), <italic>Software Heritage</italic> (<xref rid="ref-57" ref-type="bibr">Pietri, Spinellis &amp; Zacchiroli, 2019</xref>) and <italic>World of Code</italic> (<xref rid="ref-49" ref-type="bibr">Ma et al., 2019</xref>) are aimed to retrieve and archive source code, for different purposes. Debsources maintains a dataset with all the source code from Debian packages, including some metadata about it. Software Heritage retrieves data from source code management systems, with the goal of preserving it and making it available in the long term. Their goal is to store all publicly available source code. World of code is specifically targeted at maintaining a database with all FOSS code, which researchers can use to investigate the global properties of FOSS. All of them are designed to be updated with new data, incrementally and automatically.</p></list-item></list></p>
      <p><italic>GrimoireLab</italic> allows source code retrieval and analysis, via <italic>Graal</italic>. But instead of being designed for archiving all the source code, its approach is to analyze it, computing the desired metrics of each version of each file found, and storing them in a database, which can later be queried. In this respect, this is in part similar to what Sourcerer or Boa do, but the focus of <italic>Graal</italic> is not to produce a specific kind of analysis, but to allow any third party tool, run via a simple plugin module, to produce data that will be stored in the database. Both Software Heritage and World of Code can also be queried via a database, but in the case of Software Heritage this is only for finding specific source code, while in the case of World of Code, the approach is to provide data about the interdependence of projects. Both are very different from the modular approach used in <italic>GrimoireLab</italic>, which allows for studies at many scales (from the single repository to the many tens of thousands of them), and for many different purposes.</p>
      <p><italic>MetricMiner</italic> (<xref rid="ref-66" ref-type="bibr">Sokol, Aniche &amp; Gerosa, 2013</xref>), <italic>SeCold</italic> (<xref rid="ref-44" ref-type="bibr">Keivanloo et al., 2012</xref>), and <italic>OpenHub</italic> (<xref rid="ref-27" ref-type="bibr">Farah, Tejada &amp; Correal, 2014</xref>) are systems that offered a mix of source code analysis and data about software development. In that respect, their goals are more similar to those of <italic>GrimoireLab</italic>, although their structure and implementation are aimed to produce specific outcomes, very different from the flexible toolbox approach of <italic>GrimoireLab</italic>.</p>
    </sec>
    <sec>
      <title>Tools for analyzing software repositories</title>
      <p>With respect to tools capable of retrieving and analyzing software repositories, there are many of them:<list list-type="bullet"><list-item><p><italic>SoftChange</italic> (<xref rid="ref-28" ref-type="bibr">German &amp; Mockus, 2003</xref>) and <italic>GlueTheos</italic> (<xref rid="ref-60" ref-type="bibr">Robles, González-Barahona &amp; Ghosh, 2004</xref>) were some of the first tools that were designed to retrieve data from source code repositories (CVS, and later Subversion), with the aim of being generic enough to be useful in more than a single study.</p></list-item><list-item><p><italic>gitdm</italic> (<xref rid="ref-13" ref-type="bibr">Corbet, 2008</xref>) and <italic>PyDriller</italic> (<xref rid="ref-67" ref-type="bibr">Spadini, Aniche &amp; Bacchelli, 2018</xref>) are good examples of more modern reimplementations of these tools. Both are focused on the analysis of Git repositories. gitdm was developed to analyze the Linux kernel Git repository, and produce some simple stats for it. It has been later adapted to produce other metrics of interest to other projects. PyDriller is a recent framework for retrieving and analyzing Git repositories. It allows for sophisticated options to decide which parts of a repository to analyze, and is easily extensible to produce different kinds of metrics.</p></list-item><list-item><p><italic>MetricsGrimoire</italic> (<xref rid="ref-31" ref-type="bibr">Gonzalez-Barahona, Robles &amp; Izquierdo-Cortazar, 2015</xref>) was to our knowledge the first toolset for retrieval of data from several kinds of software development repositories (source code management, issue tracking, code review, mailing lists discussions, etc.) conceived to be reusable outside the team producing them. <italic>MetricsGrimoire</italic> in fact included in its maintenance team some volunteers external to the original research team that produced them, and were maintained over a period of about a decade, being used by many different research teams. <italic>GrimoireLab</italic> owes to it the design as a toolset capable of retrieving data from several kinds of repositories, with tools that can work alone or in combination. However, the coordination of tools in <italic>MetricsGrimoire</italic> was very shallow, not even using compatible formats for the same information. <italic>GrimoireLab</italic> went further in this approach, allowing for a much complete integration of data coming from different data sources, with for example same fields for actors in the database, and a single system (<italic>SortingHat</italic>, which was originally built as a part of <italic>MetricsGrimoire</italic>) for managing identities in exactly the same way. <italic>GrimoireLab</italic> also learned from these systems the problems of trying to define a specific schema for all incoming data, avoiding these problems by using flexible, nested JSON-based formats in the data-retrieval stage, while enforcing flat, uniform formats for enriched data.</p></list-item><list-item><p><italic>Alitheia Core</italic> (<xref rid="ref-34" ref-type="bibr">Gousios &amp; Spinellis, 2009</xref>) was built to support SQO-OSS, and was one of the first systems designed to continuously retrieve data from different kinds of software repositories, via their APIs. It was also a system composed as a toolset, although tools were specifically designed to work together, not in isolation. One of its features was its continuous operation, well suited for automatic retrieval and update, which was also one of the design goals of <italic>GrimoireLab</italic>, although in this case via a specific component, <italic>Mordred</italic>, that orchestrates the rest of the tools.</p></list-item><list-item><p>Some other tools that allowed for the retrieval and relatively simple analysis of software development repositories are: <italic>CODEMINE</italic> (<xref rid="ref-17" ref-type="bibr">Czerwonka et al., 2013</xref>), <italic>BuCo Reporter</italic> (<xref rid="ref-48" ref-type="bibr">Ligu, Chaikalis &amp; Chatzigeorgiou, 2013</xref>), Gitana (<xref rid="ref-16" ref-type="bibr">Cosentino, Izquierdo &amp; Cabot, 2018</xref>), and <italic>Candoia</italic> (<xref rid="ref-72" ref-type="bibr">Tiwari, Upadhyaya &amp; Rajan, 2016</xref>).</p></list-item><list-item><p><italic>PROM</italic> (<xref rid="ref-64" ref-type="bibr">Rubin et al., 2007</xref>), <italic>FOSSology</italic> (<xref rid="ref-29" ref-type="bibr">Gobeille, 2008</xref>), <italic>Qualipso</italic> (<xref rid="ref-19" ref-type="bibr">Del Bianco et al., 2009</xref>), <italic>FRASR</italic> (<xref rid="ref-58" ref-type="bibr">Poncin, Serebrenik &amp; Van Den Brand, 2011</xref>), and <italic>Q-Rapids</italic> (<xref rid="ref-50" ref-type="bibr">Martinez-Fernández et al., 2018</xref>) are systems or tools designed to produce some specific higher level metrics (e.g., processes and quality metrics), which require more data processing. For this, they had their own components that allowed for the retrieval and analysis of data (<italic>MetricsGrimoire</italic> in the case of Qualipso), and in general, stored the resulting metrics in a queryable database.</p></list-item><list-item><p>There are many other tools providing different kinds of comprehensive views of software development. Some of them are: <italic>Complicity</italic> (<xref rid="ref-55" ref-type="bibr">Neu et al., 2011</xref>), <italic>RepoGrams</italic> (<xref rid="ref-63" ref-type="bibr">Rozenberg et al., 2016</xref>), <italic>CROSSMINER</italic> (<xref rid="ref-3" ref-type="bibr">Bagnato et al., 2017</xref>), <italic>Kibble</italic> (<xref rid="ref-1" ref-type="bibr">Apache, 2022</xref>), <italic>SmartSHARK</italic> (<xref rid="ref-73" ref-type="bibr">Trautsch et al., 2017</xref>), <italic>Augur</italic> (<xref rid="ref-30" ref-type="bibr">Goggins, 2022</xref>), etc.</p></list-item></list></p>
      <p>When compared to these tools, <italic>GrimoireLab</italic> is in general more diverse in terms of data sources supported with a common interface: all of them can be retrieved using the <italic>Perceval</italic> API. For all of them raw and enriched indexes are produced with a similar structure, all of them can be retrieved and analyzed automatically with the same <italic>Mordred</italic> configuration. Most of the tools mentioned above support one, or a small number, of different data sources, and in general are not designed to produce uniform data that can be later queried for further analysis in a uniform way. <italic>GrimoireLab</italic> also provides identity management for all of these data sources, and a common way of visualizing and reporting data. However, the main difference is probably the fact that <italic>GrimoireLab</italic> tools can be used, if needed, in isolation, and that the data is stored in a way that allows for many different kinds of further analysis.</p>
    </sec>
  </sec>
  <sec>
    <title>Availability and Usage</title>
    <p><italic>GrimoireLab</italic> is a free, open source software toolset, hosted in GitHub, as a part of the <italic>CHAOSS</italic> project. Each tool is maintained in a separate repository, using Git for source code management. This guarantees future availability, via the Software Heritage repository<xref ref-type="fn" rid="fn-14"><sup>14</sup></xref>
<fn id="fn-14"><label>14</label><p>Software Heritage: <uri xlink:href="https://softwareheritage.org">https://softwareheritage.org</uri></p></fn>, or GitHub itself. The version described in this paper corresponds to the current latest commits for each repository, as of March 1st 2021 (see a detailed list in the companion package).</p>
    <p>The system can be installed as a collection of coordinated Python packages, from Pypi, by running a single command which pulls as dependencies all <italic>GrimoireLab</italic> modules:</p>
    <p>
      <monospace>pip install grimoirelab</monospace>
    </p>
    <p>Modules which can be used on their own (<italic>Perceval</italic>, <italic>Graal</italic>, <italic>SortingHat, GrimoireELK</italic>, <italic>Mordred</italic>, <italic>Kidash</italic>) provide a driver program that can be run directly. Some of them may need some services to work (Elasticsearch, Kibana, MariaDB, Redis), which can be deployed locally or somewhere on the Internet.</p>
    <p>Several Docker container images are also provided to run pre-configured versions of <italic>GrimoireLab</italic>, with all services already pre-installed. They can produce complete dashboards, with raw and enriched data for all repositories, just by running the container with the appropriate configuration data. They can also be used with official container images for services, via <monospace>docker-compose</monospace> (see the companion dataset, described in Section “A companion package and other information”, for an example of running the toolset this way, including configuration files). Docker images for <italic>GrimoireLab</italic> are stored in DockerHub, so that they can be recovered later (for any <italic>GrimoireLab</italic> release). They are also produced from Dockerfile configuration files, publicly available from <italic>GrimoireLab</italic> repositories.</p>
    <p>Extensive documentation is provided with each of the components, a tutorial is also available with step by step details for running the system as a whole, and with many specific scenarios.</p>
    <p>An active community of developers is collaborating in the maintenance and extension of the system, led by a company which is using it for its core services. The toolset is being used by several groups, FOSS foundations, companies and individuals not related to their authors. Some of these uses have lead to research publications, most of them by other groups (<xref rid="ref-76" ref-type="bibr">Zhao et al., 2017</xref>; <xref rid="ref-12" ref-type="bibr">Claes et al., 2017</xref>; <xref rid="ref-52" ref-type="bibr">Mens, Adams &amp; Marsan, 2017</xref>; <xref rid="ref-61" ref-type="bibr">Robles et al., 2017</xref>; <xref rid="ref-20" ref-type="bibr">Devanbu et al., 2017</xref>; <xref rid="ref-9" ref-type="bibr">Claes et al., 2018a</xref>; <xref rid="ref-11" ref-type="bibr">Claes et al., 2018b</xref>; <xref rid="ref-46" ref-type="bibr">Kuutila et al., 2018</xref>; <xref rid="ref-8" ref-type="bibr">Claes, Mantyla &amp; Farooq, 2018</xref>; <xref rid="ref-41" ref-type="bibr">Izquierdo et al., 2019a</xref>; <xref rid="ref-42" ref-type="bibr">Izquierdo et al., 2019b</xref>; <xref rid="ref-59" ref-type="bibr">Robles, Gamalielsson &amp; Lundell, 2019</xref>; <xref rid="ref-70" ref-type="bibr">Sulun, Tuzun &amp; Dogrusoz, 2019</xref>; <xref rid="ref-40" ref-type="bibr">Itkin, Novikov &amp; Yavorskiy, 2019</xref>; <xref rid="ref-56" ref-type="bibr">Orviz Fernandez et al., 2020</xref>; <xref rid="ref-6" ref-type="bibr">Butler et al., 2020</xref>; <xref rid="ref-2" ref-type="bibr">Ashraf et al., 2020</xref>; <xref rid="ref-10" ref-type="bibr">Claes &amp; Mantyla, 2020</xref>; <xref rid="ref-45" ref-type="bibr">Kuutila, Mantyla &amp; Claes, 2020</xref>; <xref rid="ref-71" ref-type="bibr">Sulun, Tuzun &amp; Dogrusoz, 2021</xref>). <italic>GrimoireLab</italic> deployments include <italic>Cauldron</italic>, a SaaS platform which allows users to select repositories to be analyzed via a web interface, producing data in Elasticsearch which is shown in custom Kibana dashboards.</p>
  </sec>
  <sec sec-type="conclusions">
    <title>Conclusions</title>
    <p>In this paper we have presented <italic>GrimoireLab</italic>, an extensible and modular open source toolset which offers (i) automatic and incremental data gathering from a large set of tools used in software development, (ii) storage and enrichment of the retrieved data, (iii) identities management, and (iv) data visualization and reporting to allow inspecting specific aspects of software development. <italic>GrimoireLab</italic> relies on different components that can be used together or standalone. It also may help researchers to enhance reproducibility of their studies, and traceability of their data. <italic>GrimoireLab</italic> reimplements and extends previous approaches to create a mature platform, currently used in commercial and academic settings.</p>
    <p>The main characteristics of <italic>GrimoireLab</italic> which make it unique when compared to other tools to analyze software development repositories are:<list list-type="bullet"><list-item><p>Support of many different data sources (close to 30).</p></list-item><list-item><p>Flexibility and configurability of the tool, even for large-scale analysis (10,000 s of repositories).</p></list-item><list-item><p>The storage model, with raw data mimicking the original API, kept for further analysis, and enriched, identity-merged data for visualization.</p></list-item><list-item><p>Identity merging is unique (or at least at the level of the best of other tools).</p></list-item><list-item><p>Easy deployment as a complete system using Docker or docker-compose, with a single command, but at the same time can be custom-installed using pypi packages.</p></list-item><list-item><p>It can be used as a complete toolset, but most of its tools can be also used by themselves, as modules, integrated with user-implemented software.</p></list-item><list-item><p>Tested in real-world (both industrial and research) cases, including systems running continuously for months, retrieving data from thousands of repositories.</p></list-item></list></p>
    <p><italic>GrimoireLab</italic> is managed as an open free, open source software project, with a public roadmap, and all contributions managed through pull requests in GitHub. The project documents how to contribute to it, and in fact some important contributions (such as partial support for some data sources) have been received by the core team of developers. Researchers and developers of any kind are welcome to propose their patches fixing errors or providing new features.</p>
    <p>To improve the usability in different scenarios, we are working in community repositories to share configuration files and widgets tailored to specific analysis. We also intend to extend <italic>GrimoireLab</italic> in different directions. First, we plan to support graph data storage to allow the user to answer questions that cannot be easily addressed with a document-based database (<xref rid="ref-43" ref-type="bibr">Kaur &amp; Rani, 2013</xref>). Second, we will speed up the data enrichment using in-memory data processing libraries (<xref rid="ref-51" ref-type="bibr">McKinney, 2011</xref>). Finally, we would like to improve the retrieval of data buried in source code (<xref rid="ref-14" ref-type="bibr">Cosentino et al., 2018</xref>), helping researchers to perform cross-cutting analysis over a wider spectrum of software development data.</p>
  </sec>
  <sec>
    <title>Companion Package and Other Information</title>
    <p>A companion package for this paper is available<xref ref-type="fn" rid="fn-15"><sup>15</sup></xref>
<fn id="fn-15"><label>15</label><p>Companion package for this paper: <uri xlink:href="https://doi.org/10.5281/zenodo.4656469">https://doi.org/10.5281/zenodo.4656469</uri></p></fn>. It includes data, logs, and configuration files for the IoT and the GNOME GitLab cases, and a list of Software Heritage identifiers for the software components presented in this paper. The full source code of <italic>GrimoireLab</italic>, tutorials about it, and other information is also available<xref ref-type="fn" rid="fn-16"><sup>16</sup></xref>
<fn id="fn-16"><label>16</label><p><italic>GrimoireLab</italic> main website: <uri xlink:href="https://grimoirelab.github.io">https://grimoirelab.github.io</uri></p></fn>.</p>
  </sec>
</body>
<back>
  <sec sec-type="additional-information">
    <title>Additional Information and Declarations</title>
    <fn-group content-type="competing-interests">
      <title>Competing Interests</title>
      <fn fn-type="COI-statement" id="conflict-1">
        <p>Santiago Dueñas, Daniel Izquierdo-Cortazar, Luis Cañas and Alberto Pérez García-Plaza are employees of Bitergia, the main contributor to GrimoireLab.</p>
      </fn>
    </fn-group>
    <fn-group content-type="author-contributions">
      <title>Author Contributions</title>
      <fn fn-type="con" id="contribution-1">
        <p><xref ref-type="contrib" rid="author-1">Santiago Dueñas</xref> performed the computation work, authored or reviewed drafts of the paper, and approved the final draft.</p>
      </fn>
      <fn fn-type="con" id="contribution-2">
        <p><xref ref-type="contrib" rid="author-2">Valerio Cosentino</xref> conceived and designed the experiments, performed the experiments, analyzed the data, performed the computation work, prepared figures and/or tables, authored or reviewed drafts of the paper, and approved the final draft.</p>
      </fn>
      <fn fn-type="con" id="contribution-3">
        <p><xref ref-type="contrib" rid="author-3">Jesus M. Gonzalez-Barahona</xref> conceived and designed the experiments, performed the experiments, analyzed the data, performed the computation work, prepared figures and/or tables, authored or reviewed drafts of the paper, and approved the final draft.</p>
      </fn>
      <fn fn-type="con" id="contribution-4">
        <p><xref ref-type="contrib" rid="author-4">Alvaro del Castillo San Felix</xref> conceived and designed the experiments, performed the experiments, analyzed the data, performed the computation work, authored or reviewed drafts of the paper, and approved the final draft.</p>
      </fn>
      <fn fn-type="con" id="contribution-5">
        <p><xref ref-type="contrib" rid="author-5">Daniel Izquierdo-Cortazar</xref> performed the computation work, authored or reviewed drafts of the paper, and approved the final draft.</p>
      </fn>
      <fn fn-type="con" id="contribution-6">
        <p><xref ref-type="contrib" rid="author-6">Luis Cañas-Díaz</xref> performed the computation work, authored or reviewed drafts of the paper, and approved the final draft.</p>
      </fn>
      <fn fn-type="con" id="contribution-7">
        <p><xref ref-type="contrib" rid="author-7">Alberto Pérez García-Plaza</xref> performed the computation work, authored or reviewed drafts of the paper, and approved the final draft.</p>
      </fn>
    </fn-group>
    <fn-group content-type="other">
      <title>Data Availability</title>
      <fn id="addinfo-1">
        <p>The following information was supplied regarding data availability:</p>
        <p>The companion package is available at Zenodo: “Companion package for GrimorieLab: A toolset for software development analytics” <uri xlink:href="https://doi.org/10.5281/zenodo.4656469">https://doi.org/10.5281/zenodo.4656469</uri></p>
        <p>The source code for GrimoireLab is available at GitHub, in several repositories, linked from <uri xlink:href="https://chaoss.github.io/grimoirelab/#components">https://chaoss.github.io/grimoirelab/#components</uri>.</p>
      </fn>
    </fn-group>
  </sec>
  <ref-list content-type="authoryear">
    <title>References</title>
    <ref id="ref-1">
      <label>Apache (2022)</label>
      <element-citation publication-type="other">
        <person-group person-group-type="author">
          <collab>
            <institution>Apache</institution>
          </collab>
        </person-group>
        <article-title>Kibble</article-title>
        <year>2022</year>
        <uri xlink:href="https://kibble.apache.org/">https://kibble.apache.org/</uri>
        <date-in-citation content-type="access-date" iso-8601-date="2020-10-18">18 October 2020</date-in-citation>
      </element-citation>
    </ref>
    <ref id="ref-2">
      <label>Ashraf et al., 2020</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Ashraf</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Mayr-Dorn</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Egyed</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Panichella</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>A mixed graph-relational dataset of socio-technical interactions in open source systems</article-title>
        <year>2020</year>
        <conf-name>Proceedings of the 17th International Conference on Mining Software Repositories</conf-name>
        <fpage>538</fpage>
        <lpage>542</lpage>
      </element-citation>
    </ref>
    <ref id="ref-3">
      <label>Bagnato et al. (2017)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Bagnato</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Barmpis</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Bessis</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Cabrera-Diego</surname>
            <given-names>LA</given-names>
          </name>
          <name>
            <surname>Di Rocco</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Di Roscio</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Gergely</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Hansen</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kolovos</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Krief</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Korkontzelos</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Laurie`re</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lopez de la Fuente</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Malo</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Paige</surname>
            <given-names>RF</given-names>
          </name>
          <name>
            <surname>Spinellis</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Thomas</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Vinju</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Developer-centric knowledge mining from large open-source software repositories (CROSSMINER)</article-title>
        <year>2017</year>
        <conf-name>Federation of International Conferences on Software Technologies: Applications and Foundations</conf-name>
        <publisher-name>Springer</publisher-name>
        <fpage>375</fpage>
        <lpage>384</lpage>
      </element-citation>
    </ref>
    <ref id="ref-4">
      <label>Bajracharya et al. (2006)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Bajracharya</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ngo</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Linstead</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Dou</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Rigor</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Baldi</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Lopes</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Sourcerer: a search engine for open source code supporting structure-based search</article-title>
        <year>2006</year>
        <conf-name>Companion to the 21st ACM SIGPLAN Symposium on Object-Oriented Programming Systems, Languages, and Applications</conf-name>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>ACM</publisher-name>
        <fpage>681</fpage>
        <lpage>682</lpage>
      </element-citation>
    </ref>
    <ref id="ref-5">
      <label>Buse &amp; Zimmermann (2010)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Buse</surname>
            <given-names>RP</given-names>
          </name>
          <name>
            <surname>Zimmermann</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Analytics for software development</article-title>
        <year>2010</year>
        <conf-name>Proceedings of the FSE/SDP Workshop on Future of Software Engineering Research</conf-name>
        <fpage>77</fpage>
        <lpage>80</lpage>
      </element-citation>
    </ref>
    <ref id="ref-6">
      <label>Butler et al. (2020)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Butler</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gamalielsson</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lundell</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Brax</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Mattsson</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gustavsson</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Feist</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lonroth</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Maintaining interoperability in open source software: a case study of the Apache PDFBox project</article-title>
        <source>Journal of Systems and Software</source>
        <year>2020</year>
        <volume>159</volume>
        <fpage>110452</fpage>
      </element-citation>
    </ref>
    <ref id="ref-7">
      <label>Caneill, Germán &amp; Zacchiroli (2017)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Caneill</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Germán</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>Zacchiroli</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>The Debsources dataset: two decades of free and open source software</article-title>
        <source>Empirical Software Engineering</source>
        <year>2017</year>
        <volume>22</volume>
        <issue>3</issue>
        <fpage>1405</fpage>
        <lpage>1437</lpage>
        <pub-id pub-id-type="doi">10.1007/s10664-016-9461-5</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-8">
      <label>Claes, Mantyla &amp; Farooq (2018)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Claes</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Mantyla</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Farooq</surname>
            <given-names>U</given-names>
          </name>
        </person-group>
        <article-title>On the use of emoticons in open source software development</article-title>
        <year>2018</year>
        <conf-name>Proceedings of the 12th ACM/IEEE International Symposium on Empirical Soft-ware Engineering and Measurement, ESEM’18</conf-name>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>Association for Computing Machinery</publisher-name>
      </element-citation>
    </ref>
    <ref id="ref-9">
      <label>Claes et al., 2018a</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Claes</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Mantyla</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kuutila</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Farooq</surname>
            <given-names>U</given-names>
          </name>
        </person-group>
        <article-title>Towards automatically identifying paid open source developers</article-title>
        <year>2018a</year>
        <conf-name>Proceedings of the 15th International Conference on Mining Software Repositories</conf-name>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>ACM</publisher-name>
        <fpage>437</fpage>
        <lpage>441</lpage>
      </element-citation>
    </ref>
    <ref id="ref-10">
      <label>Claes &amp; Mantyla (2020)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Claes</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Mantyla</surname>
            <given-names>MV</given-names>
          </name>
        </person-group>
        <article-title>20-MAD: 20 years of issues and commits of Mozilla and Apache development</article-title>
        <year>2020</year>
        <conf-name>Proceedings of the 17th International Conference on Mining Software Repositories</conf-name>
        <fpage>503</fpage>
        <lpage>507</lpage>
      </element-citation>
    </ref>
    <ref id="ref-11">
      <label>Claes et al., 2018b</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Claes</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Mantyla</surname>
            <given-names>MV</given-names>
          </name>
          <name>
            <surname>Kuutila</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Adams</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Do programmers work at night or during the weekend?</article-title>
        <year>2018b</year>
        <conf-name>Proceedings of the 40th International Conference on Software Engineering</conf-name>
        <fpage>705</fpage>
        <lpage>715</lpage>
      </element-citation>
    </ref>
    <ref id="ref-12">
      <label>Claes et al. (2017)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Claes</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Mantyla</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kuutila</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Adams</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Abnormal working hours: effect of rapid releases and implications to work content</article-title>
        <year>2017</year>
        <conf-name>2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR)</conf-name>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>243</fpage>
        <lpage>247</lpage>
      </element-citation>
    </ref>
    <ref id="ref-13">
      <label>Corbet (2008)</label>
      <element-citation publication-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Corbet</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>gitdm v0.10 available</article-title>
        <year>2008</year>
        <uri xlink:href="https://lwn.net/Articles/290957/">https://lwn.net/Articles/290957/</uri>
        <date-in-citation content-type="access-date" iso-8601-date="2020-10-27">27 October 2020</date-in-citation>
      </element-citation>
    </ref>
    <ref id="ref-14">
      <label>Cosentino et al. (2018)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Cosentino</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Dueñas</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Zerouali</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Robles</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Gonzalez-Barahona</surname>
            <given-names>JM</given-names>
          </name>
        </person-group>
        <article-title>Graal: The quest for source code knowledge</article-title>
        <year>2018</year>
        <conf-name>SCAM</conf-name>
        <fpage>123</fpage>
        <lpage>128</lpage>
      </element-citation>
    </ref>
    <ref id="ref-15">
      <label>Cosentino, Izquierdo &amp; Cabot (2017)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cosentino</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Izquierdo</surname>
            <given-names>JLC</given-names>
          </name>
          <name>
            <surname>Cabot</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>A systematic mapping study of software development with GitHub</article-title>
        <source>IEEE Access</source>
        <year>2017</year>
        <volume>5</volume>
        <fpage>7173</fpage>
        <lpage>7192</lpage>
        <pub-id pub-id-type="doi">10.1109/ACCESS.2017.2682323</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-16">
      <label>Cosentino, Izquierdo &amp; Cabot (2018)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cosentino</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Izquierdo</surname>
            <given-names>JLC</given-names>
          </name>
          <name>
            <surname>Cabot</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Gitana: a software project inspector</article-title>
        <source>Science of Computer Programming</source>
        <year>2018</year>
        <volume>153</volume>
        <fpage>30</fpage>
        <lpage>33</lpage>
        <pub-id pub-id-type="doi">10.1016/j.scico.2017.12.002</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-17">
      <label>Czerwonka et al. (2013)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Czerwonka</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Nagappan</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Schulte</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Murphy</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>CODEMINE: building a software development data analytics platform at Microsoft</article-title>
        <source>IEEE Software</source>
        <year>2013</year>
        <volume>30</volume>
        <issue>4</issue>
        <fpage>64</fpage>
        <lpage>71</lpage>
        <pub-id pub-id-type="doi">10.1109/MS.2013.68</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-18">
      <label>Dabbish et al. (2012)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Dabbish</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Stuart</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Tsay</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Herbsleb</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Social coding in GitHub: transparency and collaboration in an open software repository</article-title>
        <year>2012</year>
        <conf-name>CSCW</conf-name>
        <fpage>1277</fpage>
        <lpage>1286</lpage>
      </element-citation>
    </ref>
    <ref id="ref-19">
      <label>Del Bianco et al. (2009)</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Del Bianco</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Lavazza</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Morasca</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Taibi</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Boldyreff</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Crowston</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Lundell</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Wasserman</surname>
            <given-names>AI</given-names>
          </name>
        </person-group>
        <article-title>Quality of open source software: the QualiPSo trustworthiness model</article-title>
        <year>2009</year>
        <source>Open Source Ecosystems: Diverse Communities Interacting. OSS 2009. IFIP Advances in Information and Communication Technology</source>
        <volume>299</volume>
        <publisher-loc>Berlin</publisher-loc>
        <publisher-name>Springer</publisher-name>
      </element-citation>
    </ref>
    <ref id="ref-20">
      <label>Devanbu et al. (2017)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Devanbu</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Kudigrama</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Rubio-Gonzalez</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Vasilescu</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Timezone and time-of day variance in GitHub teams: an empirical method and study</article-title>
        <year>2017</year>
        <conf-name>Proceedings of the 3rd ACM SIGSOFT International Workshop on Software Analytics, SWAN 2017</conf-name>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>Association for Computing Machinery</publisher-name>
        <fpage>19</fpage>
        <lpage>22</lpage>
      </element-citation>
    </ref>
    <ref id="ref-21">
      <label>Di Cosmo &amp; Zacchiroli (2017)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Di Cosmo</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Zacchiroli</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Software heritage: why and how to preserve software source code</article-title>
        <year>2017</year>
        <conf-name>iPRES 2017: 14th International Conference on Digital Preservation</conf-name>
      </element-citation>
    </ref>
    <ref id="ref-22">
      <label>Dueñas et al. (2018)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Dueñas</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Cosentino</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Robles</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Gonzalez-Barahona</surname>
            <given-names>JM</given-names>
          </name>
        </person-group>
        <article-title>Perceval: software project data at your will</article-title>
        <year>2018</year>
        <conf-name>ICSE</conf-name>
        <fpage>1</fpage>
        <lpage>4</lpage>
      </element-citation>
    </ref>
    <ref id="ref-23">
      <label>Dyer et al. (2013a)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Dyer</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Nguyen</surname>
            <given-names>HA</given-names>
          </name>
          <name>
            <surname>Rajan</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Nguyen</surname>
            <given-names>TN</given-names>
          </name>
        </person-group>
        <article-title>Boa: a language and infrastructure for analyzing ultra-large-scale software repositories</article-title>
        <year>2013a</year>
        <conf-name>MSR</conf-name>
        <fpage>422</fpage>
        <lpage>431</lpage>
      </element-citation>
    </ref>
    <ref id="ref-24">
      <label>Dyer et al. (2013b)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Dyer</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Nguyen</surname>
            <given-names>HA</given-names>
          </name>
          <name>
            <surname>Rajan</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Nguyen</surname>
            <given-names>TN</given-names>
          </name>
        </person-group>
        <article-title>Boa: a language and infrastructure for analyzing ultra-large-scale software repositories</article-title>
        <year>2013b</year>
        <conf-name>Proceedings of the 2013 International Conference on Software Engineering, ICSE-13</conf-name>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>422</fpage>
        <lpage>431</lpage>
      </element-citation>
    </ref>
    <ref id="ref-25">
      <label>Dyer et al. (2015)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dyer</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Nguyen</surname>
            <given-names>HA</given-names>
          </name>
          <name>
            <surname>Rajan</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Nguyen</surname>
            <given-names>TN</given-names>
          </name>
        </person-group>
        <article-title>Boa: ultra-large-scale software repository and source-code mining</article-title>
        <source>ACM Transactions on Software Engineering and Methodology</source>
        <year>2015</year>
        <volume>25</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>34</lpage>
        <pub-id pub-id-type="doi">10.1145/2803171</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-26">
      <label>European Parliament &amp; Council of the European Union (2016)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <collab>
            <institution>European Parliament and Council of the European Union</institution>
          </collab>
        </person-group>
        <article-title>Regulation on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (Data Protection Directive)</article-title>
        <source>Official Journal of the European Union, EUR-Lex Document 32016R0679, May 2016</source>
        <year>2016</year>
        <fpage>1</fpage>
        <lpage>88</lpage>
      </element-citation>
    </ref>
    <ref id="ref-27">
      <label>Farah, Tejada &amp; Correal (2014)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Farah</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Tejada</surname>
            <given-names>JS</given-names>
          </name>
          <name>
            <surname>Correal</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>OpenHub: a scalable architecture for the analysis of software quality attributes</article-title>
        <year>2014</year>
        <conf-name>Proceedings of the 11th Working Conference on Mining Software Repositories</conf-name>
        <fpage>420</fpage>
        <lpage>423</lpage>
      </element-citation>
    </ref>
    <ref id="ref-28">
      <label>German &amp; Mockus (2003)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>German</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Mockus</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Automating the measurement of open source projects</article-title>
        <year>2003</year>
        <conf-name>Proceedings of the 3rd Workshop on Open Source Software Engineering</conf-name>
        <publisher-loc>Cork Ireland</publisher-loc>
        <publisher-name>University College Cork</publisher-name>
        <fpage>63</fpage>
        <lpage>67</lpage>
      </element-citation>
    </ref>
    <ref id="ref-29">
      <label>Gobeille (2008)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Gobeille</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>The FOSSology project</article-title>
        <year>2008</year>
        <conf-name>MSR 2008</conf-name>
        <fpage>47</fpage>
        <lpage>50</lpage>
      </element-citation>
    </ref>
    <ref id="ref-30">
      <label>Goggins (2022)</label>
      <element-citation publication-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Goggins</surname>
            <given-names>SP</given-names>
          </name>
        </person-group>
        <article-title>AugurLabs</article-title>
        <year>2022</year>
        <uri xlink:href="http://www.augurlabs.io/">http://www.augurlabs.io/</uri>
        <date-in-citation content-type="access-date" iso-8601-date="2020-10-18">18 October 2020</date-in-citation>
      </element-citation>
    </ref>
    <ref id="ref-31">
      <label>Gonzalez-Barahona, Robles &amp; Izquierdo-Cortazar (2015)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Gonzalez-Barahona</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Robles</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Izquierdo-Cortazar</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>The MetricsGrimoire database collection</article-title>
        <year>2015</year>
        <conf-name>MSR 2015</conf-name>
        <fpage>478</fpage>
        <lpage>481</lpage>
      </element-citation>
    </ref>
    <ref id="ref-32">
      <label>Gousios, Kalliamvakou &amp; Spinellis (2008)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Gousios</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Kalliamvakou</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Spinellis</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Measuring developer contribution from software repository data</article-title>
        <year>2008</year>
        <conf-name>MSR</conf-name>
        <fpage>129</fpage>
        <lpage>132</lpage>
      </element-citation>
    </ref>
    <ref id="ref-33">
      <label>Gousios et al. (2007)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Gousios</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Karakoidas</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Stroggylos</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Louridas</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Vlachos</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Spinellis</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Software quality assessment of open source software</article-title>
        <year>2007</year>
        <conf-name>Proceedings of the 11th Panhellenic Conference on Informatics</conf-name>
      </element-citation>
    </ref>
    <ref id="ref-34">
      <label>Gousios &amp; Spinellis (2009)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Gousios</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Spinellis</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Alitheia core: an extensible software quality monitoring platform</article-title>
        <year>2009</year>
        <conf-name>2009 IEEE 31st International Conference on Software Engineering</conf-name>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>579</fpage>
        <lpage>582</lpage>
      </element-citation>
    </ref>
    <ref id="ref-35">
      <label>Gousios &amp; Spinellis (2012)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Gousios</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Spinellis</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>GHTorrent: GitHub’s data from a firehose</article-title>
        <year>2012</year>
        <conf-name>MSR 2012</conf-name>
        <fpage>12</fpage>
        <lpage>21</lpage>
      </element-citation>
    </ref>
    <ref id="ref-36">
      <label>Grigorik (2022)</label>
      <element-citation publication-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Grigorik</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>GHArchive</article-title>
        <year>2022</year>
        <uri xlink:href="https://www.gharchive.org/">https://www.gharchive.org/</uri>
        <date-in-citation content-type="access-date" iso-8601-date="2020-10-18">18 October 2020</date-in-citation>
      </element-citation>
    </ref>
    <ref id="ref-37">
      <label>Hemmati et al. (2013)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Hemmati</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Nadi</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Baysal</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Kononenko</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Holmes</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Godfrey</surname>
            <given-names>MW</given-names>
          </name>
        </person-group>
        <article-title>The MSR cookbook: mining a decade of research</article-title>
        <year>2013</year>
        <conf-name>MSR 2013</conf-name>
        <fpage>343</fpage>
        <lpage>352</lpage>
      </element-citation>
    </ref>
    <ref id="ref-38">
      <label>Herraiz et al. (2009)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Herraiz</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Izquierdo-Cortazar</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Rivas-Hernández</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Gonzalez-Barahona</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Robles</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Duenas-Dominguez</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Garcia-Campos</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Gato</surname>
            <given-names>JF</given-names>
          </name>
          <name>
            <surname>Tovar</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>FlossMetrics: free/libre/open source software metrics</article-title>
        <year>2009</year>
        <conf-name>13th European Conference on Software Maintenance and Reengineering (CSMR 2009)</conf-name>
        <fpage>281</fpage>
        <lpage>284</lpage>
      </element-citation>
    </ref>
    <ref id="ref-39">
      <label>Howison, Conklin &amp; Crowston (2006)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Howison</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Conklin</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Crowston</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>FLOSSmole: a collaborative repository for FLOSS research data and analyses</article-title>
        <source>International Journal of Information Technology and Web Engineering (IJITWE)</source>
        <year>2006</year>
        <volume>1</volume>
        <issue>3</issue>
        <fpage>17</fpage>
        <lpage>26</lpage>
        <pub-id pub-id-type="doi">10.4018/IJITWE</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-40">
      <label>Itkin, Novikov &amp; Yavorskiy (2019)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Itkin</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Novikov</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Yavorskiy</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Development of intelligent virtual assistant for software testing team</article-title>
        <year>2019</year>
        <conf-name>2019 IEEE 19th International Conference on Software Quality, Reliability and Security Companion (QRS-C)</conf-name>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>126</fpage>
        <lpage>129</lpage>
      </element-citation>
    </ref>
    <ref id="ref-41">
      <label>Izquierdo et al. (2019a)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Izquierdo</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Gonzalez-Barahona</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Kurth</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Robles</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Software development analytics for Xen: why and how</article-title>
        <source>IEEE Software</source>
        <year>2019a</year>
        <volume>36</volume>
        <issue>3</issue>
        <fpage>28</fpage>
        <lpage>32</lpage>
      </element-citation>
    </ref>
    <ref id="ref-42">
      <label>Izquierdo et al., 2019b</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Izquierdo</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Huesman</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Serebrenik</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Robles</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>OpenStack gender diversity report</article-title>
        <source>IEEE Software</source>
        <year>2019b</year>
        <volume>36</volume>
        <issue>1</issue>
        <fpage>28</fpage>
        <lpage>33</lpage>
      </element-citation>
    </ref>
    <ref id="ref-43">
      <label>Kaur &amp; Rani (2013)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Kaur</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Rani</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Modeling and querying data in NoSQL databases</article-title>
        <year>2013</year>
        <conf-name>IEEE BigData</conf-name>
        <fpage>1</fpage>
        <lpage>7</lpage>
      </element-citation>
    </ref>
    <ref id="ref-44">
      <label>Keivanloo et al. (2012)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Keivanloo</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Forbes</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Hmood</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Erfani</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Neal</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Peristerakis</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Rilling</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>A linked data platform for mining software repositories</article-title>
        <year>2012</year>
        <conf-name>2012 9th IEEE Working Conference on Mining Software Repositories (MSR)</conf-name>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>32</fpage>
        <lpage>35</lpage>
      </element-citation>
    </ref>
    <ref id="ref-45">
      <label>Kuutila, Mantyla &amp; Claes (2020)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Kuutila</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Mantyla</surname>
            <given-names>MV</given-names>
          </name>
          <name>
            <surname>Claes</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Chat activity is a better predictor than chat sentiment on software developers productivity</article-title>
        <year>2020</year>
        <conf-name>Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops</conf-name>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>553</fpage>
        <lpage>556</lpage>
      </element-citation>
    </ref>
    <ref id="ref-46">
      <label>Kuutila et al. (2018)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Kuutila</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Mantyla</surname>
            <given-names>MV</given-names>
          </name>
          <name>
            <surname>Claes</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Elovainio</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Adams</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Using experience sampling to link software repositories with emotions and work well-being</article-title>
        <year>2018</year>
        <conf-name>Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement, ESEM ’18</conf-name>
        <publisher-loc>Oulu, Finland</publisher-loc>
        <publisher-name>Association for Computing Machinery</publisher-name>
      </element-citation>
    </ref>
    <ref id="ref-47">
      <label>Lanubile et al. (2010)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lanubile</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Ebert</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Prikladnicki</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Vizcano</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Collaboration tools for global software engineering</article-title>
        <source>IEEE Software</source>
        <year>2010</year>
        <volume>27</volume>
        <issue>2</issue>
        <fpage>52</fpage>
        <lpage>55</lpage>
        <pub-id pub-id-type="doi">10.1109/MS.2010.39</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-48">
      <label>Ligu, Chaikalis &amp; Chatzigeorgiou (2013)</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Ligu</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Chaikalis</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Chatzigeorgiou</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Georgiadis</surname>
            <given-names>CK</given-names>
          </name>
          <name>
            <surname>Kefalas</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Stamatis</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>BuCo reporter: mining software and bug repositories</article-title>
        <source>Local Proceedings of the Sixth Balkan Conference in Informatics, CEUR Workshop Proceedings</source>
        <year>2013</year>
        <volume>1036</volume>
        <publisher-loc>Thessaloniki, Greece</publisher-loc>
        <fpage>121</fpage>
      </element-citation>
    </ref>
    <ref id="ref-49">
      <label>Ma et al. (2019)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Ma</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Bogart</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Amreen</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Zaretzki</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Mockus</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>World of Code: An infrastructure for mining the universe of open source VCS data</article-title>
        <year>2019</year>
        <conf-name>Proceedings of the 16th International Conference on Mining Software Repositories, MSR’19</conf-name>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>143</fpage>
        <lpage>154</lpage>
      </element-citation>
    </ref>
    <ref id="ref-50">
      <label>Martinez-Fernández et al. (2018)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Martinez-Fernández</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Jedlitschka</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Guzmán</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Vollmer</surname>
            <given-names>AM</given-names>
          </name>
        </person-group>
        <article-title>A quality model for actionable analytics in rapid software development</article-title>
        <year>2018</year>
        <conf-name>2018 44th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)</conf-name>
        <publisher-name>IEEE</publisher-name>
        <fpage>370</fpage>
        <lpage>377</lpage>
      </element-citation>
    </ref>
    <ref id="ref-51">
      <label>McKinney (2011)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>McKinney</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Pandas: a foundational Python library for data analysis and statistics</article-title>
        <year>2011</year>
        <conf-name>PyHPC</conf-name>
        <fpage>1</fpage>
        <lpage>9</lpage>
      </element-citation>
    </ref>
    <ref id="ref-52">
      <label>Mens, Adams &amp; Marsan (2017)</label>
      <element-citation publication-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Mens</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Adams</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Marsan</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Towards an interdisciplinary, socio-technical analysis of software ecosystem health</article-title>
        <year>2017</year>
        <comment>arXiv preprint</comment>
        <uri xlink:href="https://arxiv.org/abs/1711.04532">https://arxiv.org/abs/1711.04532</uri>
      </element-citation>
    </ref>
    <ref id="ref-53">
      <label>Menzies &amp; Zimmermann (2013)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Menzies</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Zimmermann</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Software analytics: so what?</article-title>
        <source>IEEE Software</source>
        <year>2013</year>
        <volume>30</volume>
        <issue>4</issue>
        <fpage>31</fpage>
        <lpage>37</lpage>
        <pub-id pub-id-type="doi">10.1109/MS.2013.86</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-54">
      <label>Moreno et al. (2019)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Moreno</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Dueñas</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Cosentino</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Fernandez</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Zerouali</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Robles</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Gonzalez-Barahona</surname>
            <given-names>JM</given-names>
          </name>
        </person-group>
        <article-title>Sortinghat: wizardry on software project members</article-title>
        <year>2019</year>
        <conf-name>Proceedings of the 41st International Conference on Software Engineering: Companion Proceedings (ICSE’19)</conf-name>
        <fpage>51</fpage>
        <lpage>54</lpage>
      </element-citation>
    </ref>
    <ref id="ref-55">
      <label>Neu et al. (2011)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Neu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lanza</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Hattori</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>D’Ambros</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Telling stories about GNOME with complicity</article-title>
        <year>2011</year>
        <conf-name> 2011 6th International Workshop on Visualizing Software for Understanding and Analysis (VISSOFT)</conf-name>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>1</fpage>
        <lpage>8</lpage>
      </element-citation>
    </ref>
    <ref id="ref-56">
      <label>Orviz Fernandez et al. (2020)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Orviz Fernandez</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>David</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Duma</surname>
            <given-names>DC</given-names>
          </name>
          <name>
            <surname>Ronchieri</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Gomes</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Salomoni</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Software quality assurance in INDIGO-DataCloud project: a converging evolution of software engineering practices to support European research e-infrastructures</article-title>
        <source>Journal of Grid Computing</source>
        <year>2020</year>
        <volume>18</volume>
        <fpage>81</fpage>
        <lpage>98</lpage>
      </element-citation>
    </ref>
    <ref id="ref-57">
      <label>Pietri, Spinellis &amp; Zacchiroli (2019)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Pietri</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Spinellis</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Zacchiroli</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>The Software heritage graph dataset: public software development under one roof</article-title>
        <year>2019</year>
        <conf-name>2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)</conf-name>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>138</fpage>
        <lpage>142</lpage>
      </element-citation>
    </ref>
    <ref id="ref-58">
      <label>Poncin, Serebrenik &amp; Van Den Brand (2011)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Poncin</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Serebrenik</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Van Den Brand</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Process mining software repositories</article-title>
        <year>2011</year>
        <conf-name>2011 15th European Conference on Software Maintenance and Reengineering</conf-name>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>5</fpage>
        <lpage>14</lpage>
      </element-citation>
    </ref>
    <ref id="ref-59">
      <label>Robles, Gamalielsson &amp; Lundell (2019)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Robles</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Gamalielsson</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lundell</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Setting up government 3.0 solutions based on open source software: the case of X-road</article-title>
        <year>2019</year>
        <conf-name>Proceedings of the International Conference on Electronic Government</conf-name>
        <publisher-name>Springer</publisher-name>
        <fpage>69</fpage>
        <lpage>81</lpage>
      </element-citation>
    </ref>
    <ref id="ref-60">
      <label>Robles, González-Barahona &amp; Ghosh (2004)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Robles</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>González-Barahona</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Ghosh</surname>
            <given-names>RA</given-names>
          </name>
        </person-group>
        <article-title>Glutheos: Automating the retrieval and analysis of data from publicly available software repositories</article-title>
        <year>2004</year>
        <conf-name>MSR</conf-name>
        <volume>4</volume>
        <publisher-name>IET</publisher-name>
        <fpage>28</fpage>
        <lpage>31</lpage>
      </element-citation>
    </ref>
    <ref id="ref-61">
      <label>Robles et al. (2017)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Robles</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Ho-Quang</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Hebig</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Chaudron</surname>
            <given-names>MR</given-names>
          </name>
          <name>
            <surname>Fernandez</surname>
            <given-names>MA</given-names>
          </name>
        </person-group>
        <article-title>An extensive dataset of UML models in GitHub</article-title>
        <year>2017</year>
        <conf-name>2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR)</conf-name>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>519</fpage>
        <lpage>522</lpage>
      </element-citation>
    </ref>
    <ref id="ref-62">
      <label>Robles, Koch &amp; Gonzalez-Barahona (2004)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Robles</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Koch</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gonzalez-Barahona</surname>
            <given-names>JM</given-names>
          </name>
        </person-group>
        <article-title>Remote analysis and measurement of libre software systems by means of the CVSAnalY tool</article-title>
        <year>2004</year>
        <conf-name>2nd Workshop on Remote Analysis and Measurement of Software Systems</conf-name>
        <fpage>51</fpage>
        <lpage>56</lpage>
      </element-citation>
    </ref>
    <ref id="ref-63">
      <label>Rozenberg et al. (2016)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Rozenberg</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Beschastnikh</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Kosmale</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Poser</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Becker</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Palyart</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Murphy</surname>
            <given-names>GC</given-names>
          </name>
        </person-group>
        <article-title>Comparing repositories visually with repograms</article-title>
        <year>2016</year>
        <conf-name>Proceedings of the 13th International Conference on Mining Software Repositories</conf-name>
        <fpage>109</fpage>
        <lpage>120</lpage>
      </element-citation>
    </ref>
    <ref id="ref-64">
      <label>Rubin et al. (2007)</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Rubin</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Günther</surname>
            <given-names>CW</given-names>
          </name>
          <name>
            <surname>Van Ger Aalst</surname>
            <given-names>WM</given-names>
          </name>
          <name>
            <surname>Kindler</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Van Dongen</surname>
            <given-names>BF</given-names>
          </name>
          <name>
            <surname>Schäfer</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Wang</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Pfahl</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Raffo</surname>
            <given-names>DM</given-names>
          </name>
        </person-group>
        <article-title>Process mining framework for software processes</article-title>
        <year>2007</year>
        <source>Software Process Dynamics and Agility. ICSP 2007. Lecture Notes in Computer Science</source>
        <volume>4470</volume>
        <publisher-loc>Berlin</publisher-loc>
        <publisher-name>Springer</publisher-name>
      </element-citation>
    </ref>
    <ref id="ref-65">
      <label>SARA (2022)</label>
      <element-citation publication-type="other">
        <person-group person-group-type="author">
          <collab>
            <institution>SARA</institution>
          </collab>
        </person-group>
        <article-title>SARA: software archiving of research artefacts</article-title>
        <year>2022</year>
        <uri xlink:href="https://www.sara-service.org/">https://www.sara-service.org/</uri>
        <date-in-citation content-type="access-date" iso-8601-date="2020-01-16">16 January 2020</date-in-citation>
      </element-citation>
    </ref>
    <ref id="ref-66">
      <label>Sokol, Aniche &amp; Gerosa (2013)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Sokol</surname>
            <given-names>FZ</given-names>
          </name>
          <name>
            <surname>Aniche</surname>
            <given-names>MF</given-names>
          </name>
          <name>
            <surname>Gerosa</surname>
            <given-names>MA</given-names>
          </name>
        </person-group>
        <article-title>MetricMiner: Supporting researchers in mining software repositories</article-title>
        <year>2013</year>
        <conf-name>2013 IEEE 13th International Working Conference on Source Code Analysis and Manipulation (SCAM)</conf-name>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>142</fpage>
        <lpage>146</lpage>
      </element-citation>
    </ref>
    <ref id="ref-67">
      <label>Spadini, Aniche &amp; Bacchelli (2018)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Spadini</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Aniche</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Bacchelli</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>PyDriller: python framework for mining software repositories</article-title>
        <year>2018</year>
        <conf-name>The 26th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE)</conf-name>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>ACM</publisher-name>
        <fpage>908</fpage>
        <lpage>911</lpage>
      </element-citation>
    </ref>
    <ref id="ref-68">
      <label>Stol &amp; Fitzgerald (2018)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stol</surname>
            <given-names>K-J</given-names>
          </name>
          <name>
            <surname>Fitzgerald</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>The ABC of software engineering research</article-title>
        <source>ACM Transactions on Software Engineering and Methodology</source>
        <year>2018</year>
        <volume>27</volume>
        <issue>3</issue>
        <fpage>1</fpage>
        <lpage>51</lpage>
        <pub-id pub-id-type="doi">10.1145/3241743</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-69">
      <label>Storey et al. (2010)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Storey</surname>
            <given-names>M-A</given-names>
          </name>
          <name>
            <surname>Treude</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>van Deursen</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>L-T</given-names>
          </name>
        </person-group>
        <article-title>The impact of social media on software engineering practices and tools</article-title>
        <year>2010</year>
        <conf-name>FSE</conf-name>
        <publisher-name>ACM</publisher-name>
        <fpage>359</fpage>
        <lpage>364</lpage>
      </element-citation>
    </ref>
    <ref id="ref-70">
      <label>Sulun, Tuzun &amp; Dogrusoz (2019)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Sulun</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Tuzun</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Dogrusoz</surname>
            <given-names>U</given-names>
          </name>
        </person-group>
        <article-title>Reviewer recommendation using software artifact traceability graphs</article-title>
        <year>2019</year>
        <conf-name>Proceedings of the Fifteenth International Conference on Predictive Models and Data Analytics in Software Engineering</conf-name>
        <fpage>66</fpage>
        <lpage>75</lpage>
      </element-citation>
    </ref>
    <ref id="ref-71">
      <label>Sulun, Tuzun &amp; Dogrusoz (2021)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sulun</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Tuzun</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Dogrusoz</surname>
            <given-names>U</given-names>
          </name>
        </person-group>
        <article-title>RSTrace+: reviewer suggestion using software artifacttraceability graphs</article-title>
        <source>Information and Software Technology</source>
        <year>2021</year>
        <volume>130</volume>
        <fpage>106455</fpage>
      </element-citation>
    </ref>
    <ref id="ref-72">
      <label>Tiwari, Upadhyaya &amp; Rajan (2016)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Tiwari</surname>
            <given-names>NM</given-names>
          </name>
          <name>
            <surname>Upadhyaya</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Rajan</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Candoia: a platform and ecosystem for mining software repositories tools</article-title>
        <year>2016</year>
        <conf-name>2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C)</conf-name>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>759</fpage>
        <lpage>761</lpage>
      </element-citation>
    </ref>
    <ref id="ref-73">
      <label>Trautsch et al. (2017)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Trautsch</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Herbold</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Makedonski</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Grabowski</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Addressing problems with replicability and validity of repository mining studies through a smart data platform</article-title>
        <source>Empirical Software Engineering</source>
        <year>2017</year>
        <volume>23</volume>
        <issue>2</issue>
        <fpage>1036</fpage>
        <lpage>1083</lpage>
        <pub-id pub-id-type="doi">10.1007/s10664-017-9537-x</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-74">
      <label>Van Antwerp &amp; Madey (2008)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Van Antwerp</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Madey</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Advances in the sourceforge research data archive</article-title>
        <year>2008</year>
        <conf-name>Workshop on Public Data about Software Development (WoPDaSD) at the 4th International Conference on Open Source Systems</conf-name>
        <publisher-loc>Milan, Italy</publisher-loc>
        <fpage>1</fpage>
        <lpage>6</lpage>
      </element-citation>
    </ref>
    <ref id="ref-75">
      <label>Zhang et al. (2013)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Dang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Lou</surname>
            <given-names>J-G</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Software analytics in practice</article-title>
        <source>IEEE Software</source>
        <year>2013</year>
        <volume>30</volume>
        <issue>5</issue>
        <fpage>30</fpage>
        <lpage>37</lpage>
        <pub-id pub-id-type="doi">10.1109/MS.2013.94</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-76">
      <label>Zhao et al. (2017)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Serebrenik</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Filkov</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Vasilescu</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>The impact of continuous integration on other software development practices: a large-scale empirical study</article-title>
        <year>2017</year>
        <conf-name>Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering, ASE 2017</conf-name>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>60</fpage>
        <lpage>71</lpage>
      </element-citation>
    </ref>
  </ref-list>
</back>
