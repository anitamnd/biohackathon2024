<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD with MathML3 v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1-mathml3.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?properties manuscript?>
<?origin nihpa?>
<?iso-abbr Pac Symp Biocomput?>
<?submitter-system nihms?>
<?submitter-canonical-name World Scientific Publishing?>
<?submitter-canonical-id WORLDSCIENCE?>
<?submitter-userid 8068881?>
<?submitter-authority myNCBI?>
<?submitter-login worldscience?>
<?submitter-name World Scientific Publishing?>
<?domain nihpa?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">9711271</journal-id>
    <journal-id journal-id-type="pubmed-jr-id">20660</journal-id>
    <journal-id journal-id-type="nlm-ta">Pac Symp Biocomput</journal-id>
    <journal-id journal-id-type="iso-abbrev">Pac Symp Biocomput</journal-id>
    <journal-title-group>
      <journal-title>Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">2335-6928</issn>
    <issn pub-type="epub">2335-6936</issn>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10764073</article-id>
    <article-id pub-id-type="pmid">38160271</article-id>
    <article-id pub-id-type="manuscript">nihpa1952171</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Zoish: A Novel Feature Selection Approach Leveraging Shapley Additive Values for Machine Learning Applications in Healthcare</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Sadaei</surname>
          <given-names>Hossein Javedani</given-names>
        </name>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Loguercio</surname>
          <given-names>Salvatore</given-names>
        </name>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Neyestanak</surname>
          <given-names>Mahdi Shafiei</given-names>
        </name>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Torkamani</surname>
          <given-names>Ali</given-names>
        </name>
        <xref rid="CR1" ref-type="corresp">†</xref>
        <aff id="A1">Scripps Research Translational Institute, and Department of Integrative Structural and Computational Biology Scripps Research, La Jolla, CA 92037, USA</aff>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Prilutsky</surname>
          <given-names>Daria</given-names>
        </name>
        <aff id="A2">Takeda Development Center Americas, Inc., Cambridge, MA, 02139, USA</aff>
      </contrib>
    </contrib-group>
    <author-notes>
      <fn fn-type="con" id="FN1">
        <p id="P1">Author contributions statement</p>
        <p id="P2">Author contributions include; conceptualization (HJS, AT), methodology and software (HJS), validation and data curation (HJS, SL, MSN, AT, and DP), writing (HJS, SL, MSN, and AT), and funding (AT).</p>
      </fn>
      <corresp id="CR1">
        <label>†</label>
        <email>atorkama@scripps.edu</email>
      </corresp>
    </author-notes>
    <pub-date pub-type="nihms-submitted">
      <day>15</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>03</day>
      <month>1</month>
      <year>2024</year>
    </pub-date>
    <volume>29</volume>
    <fpage>81</fpage>
    <lpage>95</lpage>
    <permissions>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>Open Access chapter published by World Scientific Publishing Company and distributed under the terms of the Creative Commons Attribution Non-Commercial (CC BY-NC) 4.0 License.</license-p>
      </license>
    </permissions>
    <abstract id="ABS1">
      <p id="P3">In the intricate landscape of healthcare analytics, effective feature selection is a prerequisite for generating robust predictive models, especially given the common challenges of sample sizes and potential biases. Zoish uniquely addresses these issues by employing Shapley additive values—an idea rooted in cooperative game theory—to enable both transparent and automated feature selection. Unlike existing tools, Zoish is versatile, designed to seamlessly integrate with an array of machine learning libraries including scikit-learn, XGBoost, CatBoost, and imbalanced-learn.</p>
      <p id="P4">The distinct advantage of Zoish lies in its dual algorithmic approach for calculating Shapley values, allowing it to efficiently manage both large and small datasets. This adaptability renders it exceptionally suitable for a wide spectrum of healthcare-related tasks. The tool also places a strong emphasis on interpretability, providing comprehensive visualizations for analyzed features. Its customizable settings offer users fine-grained control over feature selection, thus optimizing for specific predictive objectives.</p>
      <p id="P5">This manuscript elucidates the mathematical framework underpinning Zoish and how it uniquely combines local and global feature selection into a single, streamlined process. To validate Zoish’s efficiency and adaptability, we present case studies in breast cancer prediction and Montreal Cognitive Assessment (MoCA) prediction in Parkinson’s disease, along with evaluations on 300 synthetic datasets. These applications underscore Zoish’s unparalleled performance in diverse healthcare contexts and against its counterparts.</p>
    </abstract>
    <kwd-group>
      <kwd>Feature Selectors</kwd>
      <kwd>Zoish</kwd>
      <kwd>SHapley Additive exPlanations</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <label>1.</label>
    <title>Introduction</title>
    <p id="P6">Healthcare datasets, despite being typically sparse and heterogeneous, are a treasure trove of rich information. However, their high-dimensionality, often paired with smaller sizes, presents obstacles to building predictive models, with overfitting and extensive training time being common concerns.<sup><xref rid="R1" ref-type="bibr">1</xref>–<xref rid="R3" ref-type="bibr">3</xref></sup> Feature selection becomes an essential strategy in this context, aimed at pruning redundant or less important features. This helps to minimize information loss, enhance model interpretability, and curtail computational demands.</p>
    <p id="P7">Although traditional feature selection methods, grounded in statistical concepts like correlation analysis or chi-square tests, are widely used, they tend to fall short in offering detailed insights into feature importance. This shortcoming, along with the manual effort required, can lead to a time-intensive cycle of feature selection and performance evaluation, thus requiring expert intervention.<sup><xref rid="R4" ref-type="bibr">4</xref>,<xref rid="R5" ref-type="bibr">5</xref></sup></p>
    <p id="P8">Our proposed feature selection tool, Zoish, aims to overcome these limitations by utilizing the mathematical framework of additive Shapley values. Originating from cooperative game theory, Shapley values offer detailed understanding of feature importance,<sup><xref rid="R6" ref-type="bibr">6</xref></sup> thereby enhancing both local (instance-level) and global interpretability. Moreover, the integration of Zoish with our scalable hyperparameter optimization package, Lohrasb,<sup><xref rid="R7" ref-type="bibr">7</xref></sup> facilitates building models with optimal feature sets, all the while maintaining an industry-ready, user-friendly design.</p>
    <p id="P9">The structure of the paper is as follows. The first section sheds light on the core concepts of additive Shapley values and delves into the mathematical principles vital to Zoish. Subsequent to this, a section introducing a user guide for Zoish is presented. Lastly, we demonstrate the adaptability of Zoish through a variety of use-cases, experiments on large synthetic datasets, and a closing discussion.</p>
  </sec>
  <sec id="S2">
    <label>2.</label>
    <title>Theoretical Foundations of Zoish</title>
    <p id="P10">Our exploration into Zoish begins with the foundation of its theoretical structure, built upon Shapley additive values. First proposed in the field of cooperative game theory, Shapley additive values have proven to be a potent tool for understanding the contribution of each feature to a prediction made by a machine learning model. Simply put, the Shapley value of a feature represents the average marginal contribution of that feature, factored across all possible combinations of features.</p>
    <sec id="S3">
      <label>2.1.</label>
      <title>Shapley Additive Values and Feature Selection: A Game Theoretical Approach</title>
      <p id="P11">In the realm of cooperative game theory, the Shapley value denotes each player’s payoff based on their marginal contribution across all possible coalitions. In machine learning, ‘players’ correspond to the features and ‘game’ to the prediction task.<sup><xref rid="R6" ref-type="bibr">6</xref></sup></p>
      <p id="P12">An additive cooperative game assumes that the value of any coalition equals the sum of its members’ independent values. This idea leads us to Shapley additive values, where the Shapley value of a feature equals its average marginal contribution across all feature subsets.</p>
      <p id="P13">Mathematically, the Shapley value for a feature <italic toggle="yes">i</italic> in an additive game is:
<disp-formula id="FD1"><mml:math id="M1" display="block"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>S</mml:mi><mml:mo>⊆</mml:mo><mml:mi>N</mml:mi><mml:mo>∖</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>i</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>!</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>N</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>!</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>N</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>!</mml:mo></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo>∪</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>i</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>i</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p id="P14">In this formula, <italic toggle="yes">|N|</italic> denotes the total number of features, <italic toggle="yes">S</italic> a subset of features excluding feature <italic toggle="yes">i</italic>, and <italic toggle="yes">|S|</italic> the number of features in subset <italic toggle="yes">S</italic>. The terms <inline-formula><mml:math id="M2" display="inline"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>!</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>N</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>!</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M30" display="inline"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>N</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>!</mml:mo></mml:mrow></mml:math></inline-formula> calculate the number of possible permutations of features. The expression <inline-formula><mml:math id="M3" display="inline"><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo>∪</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>i</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> computes the marginal contribution of feature i when added to subset <italic toggle="yes">S</italic>.</p>
      <p id="P15">In feature selection, the Shapley value presents a way to distribute the model’s prediction among the features, based on their marginal contribution.<sup><xref rid="R8" ref-type="bibr">8</xref>,<xref rid="R9" ref-type="bibr">9</xref></sup> Shapley Additive exPlanations (SHAP) values, maintaining additivity, provide a unified measure of feature importance, attributing the difference between the model’s actual and expected output to each influencing feature.<sup><xref rid="R10" ref-type="bibr">10</xref></sup> High Shapley or SHAP values indicate significant feature importance, while values near zero suggest negligible predictive power.<sup><xref rid="R11" ref-type="bibr">11</xref></sup> This correlation aids in reducing data dimensionality and enhances model interpretability, marking a significant stride in feature selection.</p>
    </sec>
    <sec id="S4">
      <label>2.2.</label>
      <title>Properties of Shapley Additive Values</title>
      <p id="P16">The Shapley additive values satisfy a number of properties that make them particularly useful for interpreting machine learning models:
<list list-type="bullet" id="L2"><list-item><p id="P17"><bold>Efficiency</bold>: The sum of the Shapley values of all features is equal to the difference between the prediction for an instance and the average prediction over all instances.</p></list-item><list-item><p id="P18"><bold>Symmetry</bold>: If two features contribute equally to all possible combinations of features, they have the same Shapley value.</p></list-item><list-item><p id="P19"><bold>Additivity</bold>: Given two games (or in our context, two models), the Shapley value of the combined game is the sum of the Shapley values of the individual games.</p></list-item><list-item><p id="P20"><bold>Nullity (Dummy):</bold> If a feature does not improve the prediction for any combination of features, its Shapley value is zero.</p></list-item></list></p>
      <sec id="S5">
        <label>2.2.1.</label>
        <title>Proof of Nullity (Dummy)</title>
        <p id="P21">The Nullity (Dummy) property states that if a feature does not change the prediction model, i.e., its contribution is always zero, then its Shapley value is also zero. Let <italic toggle="yes">f</italic> be the prediction model and <italic toggle="yes">d</italic> be such a dummy feature.</p>
        <p id="P22">According to the definition of Shapley values, the Shapley value of a feature is the average of its marginal contributions across all possible subsets of features. Therefore, the Shapley value <italic toggle="yes">s</italic><sub><italic toggle="yes">f</italic></sub> (<italic toggle="yes">d</italic>) for the dummy feature <italic toggle="yes">d</italic> is:
<disp-formula id="FD2"><mml:math id="M4" display="block"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:munder><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>S</mml:mi><mml:mo>⊆</mml:mo><mml:mi>N</mml:mi><mml:mo>∖</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>d</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>!</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>N</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>!</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo>∪</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>d</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="P23">In the above formula, <italic toggle="yes">M</italic> is the total number of possible subsets of <italic toggle="yes">N</italic> that can be formed when the dummy feature <italic toggle="yes">d</italic> is excluded. The term <inline-formula><mml:math id="M5" display="inline"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>!</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>N</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>!</mml:mo></mml:mrow></mml:math></inline-formula> is the number of permutations of <italic toggle="yes">N</italic> in which the dummy feature <italic toggle="yes">d</italic> and the features in subset <italic toggle="yes">S</italic> appear together, and <inline-formula><mml:math id="M6" display="inline"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo>∪</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>d</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <italic toggle="yes">f</italic>(<italic toggle="yes">S</italic>) are the values of the game <italic toggle="yes">f</italic> when the dummy feature <italic toggle="yes">d</italic> is added to subset <italic toggle="yes">S</italic> and when it is not, respectively.</p>
        <p id="P24">Since <italic toggle="yes">d</italic> is a dummy feature, adding it to any subset <italic toggle="yes">S</italic> does not change the value of the game <italic toggle="yes">f</italic>. Thus, we have <inline-formula><mml:math id="M7" display="inline"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo>∪</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>d</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> for all <inline-formula><mml:math id="M8" display="inline"><mml:mrow><mml:mi>S</mml:mi><mml:mo>⊆</mml:mo><mml:mi>N</mml:mi><mml:mo>∖</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>d</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> which simplifies the above formula to:
<disp-formula id="FD3"><mml:math id="M9" display="block"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:munder><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>S</mml:mi><mml:mo>⊆</mml:mo><mml:mi>N</mml:mi><mml:mo>∖</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>d</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>!</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>N</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>!</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD4"><mml:math id="M10" display="block"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></disp-formula></p>
        <p id="P25">This confirms that the Shapley value of a feature that does not contribute to the prediction model is indeed zero, thus proving the Nullity (Dummy) property.<sup><xref rid="R9" ref-type="bibr">9</xref></sup></p>
      </sec>
    </sec>
    <sec id="S6">
      <label>2.3.</label>
      <title>Leveraging Shapley Additive Values for Efficient Feature Selection</title>
      <p id="P26">Shapley additive values have become increasingly prominent in feature selection for machine learning due to their robustness, efficiency, and power. Verdinelli et al. examined the explainability of machine learning models, focusing on methods such as LOCO and Shapley Values for assessing feature importance. Although their research indicated that Shapley Values do not eliminate feature correlation, they proposed new, statistically sound axioms for measuring feature importance.<sup><xref rid="R12" ref-type="bibr">12</xref></sup> In a separate study, Karczmarz et al. compared Shapley and Banzhaf values in the context of explaining tree ensemble models. They found Banzhaf values to be more intuitive, efficient, and numerically robust, and introduced faster algorithms for both methods to improve computational efficiency.<sup><xref rid="R13" ref-type="bibr">13</xref></sup> The SHAP (SHapley Additive exPlanation) library serves as a prime example of effectively leveraging the additivity and efficiency inherent in Shapley values.<sup><xref rid="R14" ref-type="bibr">14</xref></sup> A fundamental advantage of Shapley values lies in their additivity, which enables fast and efficient computation, especially in the context of tree-based models. The SHAP and FastTreeShap libraries employ Tree SHAP, a highly efficient and accurate algorithm designed for tree ensembles.<sup><xref rid="R14" ref-type="bibr">14</xref>,<xref rid="R15" ref-type="bibr">15</xref></sup> Given the innate additivity of ensemble tree models, which amalgamate multiple decision trees, this characteristic ensures swift and precise computation of Shapley values.<sup><xref rid="R6" ref-type="bibr">6</xref></sup> The efficacy of Shapley values is further underscored by their intrinsic efficiency. This is manifested in the fact that the sum of the Shapley values for all features equals the difference between the prediction for a specific instance and the average prediction across all instances. This aspect permits a meaningful distribution of the ”credit” for a prediction across features, hence illuminating their relative importance. The Zoish package,<sup><xref rid="R16" ref-type="bibr">16</xref></sup> designed to optimize feature selection, taps into the beneficial properties of Shapley values. It employs the Nullity (or Dummy) property to eliminate features with Shapley values close to or exactly zero, indicating their minimal predictive relevance. To assist users in setting the cut-off level, two methods are offered: one involves setting an internal parameter called <italic toggle="yes">threshold</italic>, while the other entails defining the number of desired features to retain in the model. By removing these non-influential features, the package enables dimensionality reduction of the model without sacrificing prediction quality. The symmetry property of Shapley values is also exploited by Zoish. This property mandates that if two features contribute equally to all possible subsets of other features, they must have identical Shapley values. By identifying and discarding these redundant features, Zoish facilitates the construction of models that are simpler and more interpretable, with no compromise on predictive power. By utilizing the SHAP and FastTreeShap libraries to incorporate Shapley additive values, Zoish implicitly benefits from its advantages, including the mathematical robustness and beneficial properties of Shapley values. Therefore, these libraries and the Zoish package present themselves as potent instruments for feature selection, spanning a wide range of machine learning tasks.</p>
    </sec>
  </sec>
  <sec id="S7">
    <label>3.</label>
    <title>Feature Selection Approaches</title>
    <p id="P27">Zoish is a versatile package designed to enhance the evaluation of feature importance and improve the overall performance of machine learning models.<sup><xref rid="R16" ref-type="bibr">16</xref></sup> While Zoish can function effectively as a standalone tool for feature selection, it is engineered to be highly extensible and can seamlessly integrate with hyperparameter optimization packages to further refine its capabilities. One such potent integration is with the Lohrasb package,<sup><xref rid="R7" ref-type="bibr">7</xref></sup> which provides advanced tuning methods to optimize the feature selection process. However, it’s worth noting that users are not confined to using Lohrasb; Zoish’s flexible architecture allows for easy integration with other hyperparameter optimization tools as well.</p>
    <sec id="S8">
      <label>3.1.</label>
      <title>Optimization and Flexibility in Zoish</title>
      <p id="P28">Zoish’s integration with Lohrasb serves a dual purpose: it not only optimizes the tree-based estimator used for feature selection but also offers a choice of hyperparameter tuning methods, including Optuna, GridSearchCV,<sup><xref rid="R17" ref-type="bibr">17</xref></sup> RandomizedSearchCV,<sup><xref rid="R18" ref-type="bibr">18</xref></sup> OptunaSearchCV, tune-sklearn,<sup><xref rid="R19" ref-type="bibr">19</xref></sup> and Ray’s Tune.<sup><xref rid="R20" ref-type="bibr">20</xref></sup> This optimization is crucial for enhancing Zoish’s feature selection capabilities, as represented in <xref rid="F1" ref-type="fig">Fig 1</xref>. However, the use of Lohrasb is optional, giving users the freedom to employ other tree-based estimators or hyperparameter tuning engines. Even without hyperparameter optimization, Zoish maintains its core functionality, allowing for a balance between efficiency and interpretability. The importance of hyperparameter optimization for feature selection is further elaborated in <xref rid="S14" ref-type="sec">Section 6</xref>. Therefore, while Lohrasb’s role is significant for optimal performance, users have the flexibility to choose an approach that best suits their specific needs.</p>
    </sec>
    <sec id="S9">
      <label>3.2.</label>
      <title>Workflow explanation</title>
      <p id="P29">Within a machine learning pipeline, Zoish functions as a feature selection component. The pipeline commences by cleaning and splitting the original dataset into training and validation subsets. A tree-based estimator, which is compatible with Zoish, is trained on the training subset. If hyperparameter tuning is applied, tools such as Lohrasb optimize the estimator against a specific metric, as shown in <xref rid="F1" ref-type="fig">Fig 1</xref>.</p>
      <p id="P30">Once the estimator is optimized, it becomes an input to Zoish along with a set of parameters, such as cross-validation settings, Shapley value calculation algorithms, and feature importance thresholds. Zoish computes Shapley values via either the SHAP library for smaller datasets, due to its exhaustive computational approach, or FastTreeShap for larger datasets, owing to its computational efficiency.</p>
      <p id="P31">Based on the calculated Shapley values, Zoish automatically selects the highest-ranking features. The training set is then narrowed down to these selected features. Subsequently, these refined training and validation sets are channeled to the next steps in the pipeline, which usually involve fitting another predictive model.</p>
      <p id="P32">To ensure robustness in feature selection, Zoish employs multiple rounds of cross-validation on the same training set, regulated by a parameter named <italic toggle="yes">n</italic><sub>-</sub><italic toggle="yes">iter</italic>.</p>
      <p id="P33">Documentation and code examples elucidating these operational details can be found in the Zoish repository.</p>
    </sec>
  </sec>
  <sec id="S10">
    <label>4.</label>
    <title>Source Code, installation and usage example</title>
    <p id="P34">The public repository of Zoish is available on GitHub alongside examples for end users is <ext-link xlink:href="https://github.com/TorkamaniLab/zoish" ext-link-type="uri">https://github.com/TorkamaniLab/zoish</ext-link>. Zoish package is available on PyPI and can be installed with pip:
<preformat position="float" xml:space="preserve">
pip install zoish
</preformat></p>
    <p id="P401">A straightforward example demonstrates how Zoish can be effectively combined with hyperparameter optimizers. Both this example and the comprehensive documentation in the repository highlight the package’s flexibility and adaptability across various scenarios.</p>
    <preformat position="float" xml:space="preserve">
import xgboost as xgb
from sklearn.model_selection import KFold, GridSearchCV
from zoish.feature_selectors.shap_selectors import ShapFeatureSelector
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

X_train, X_test, y_train, y_test = … # Your dataset here

grid = GridSearchCV(xgb.XGBClassifier(), {‘n_estimators’: [100, 150], ‘max_depth’:\
[6, 10], ‘gamma’: [0.5, 1.0]}, cv=5, n_jobs=−1, scoring=‘accuracy’).\
fit(X_train, y_train)
shap_selector = ShapFeatureSelector(grid.best_estimator_, \
num_features=15, cv=KFold(10), n_iter=5, direction=“maximum”, \
scoring=“accuracy”, algorithm=‘auto’, use_faster_algorithm=True)
pipeline = Pipeline(steps=[(“s”, shap_selector), \
(“m”, LogisticRegression())]).fit(X_train, y_train)
</preformat>
  </sec>
  <sec id="S11">
    <label>5.</label>
    <title>Use cases and applications</title>
    <sec id="S12">
      <label>5.1.</label>
      <title>Use case 1: Application to UCI breast cancer dataset - comparison with related XAI work</title>
      <p id="P51">To demonstrate the value of the Zoish feature selector in a real use case from the biomedical domain, we applied it to the openly available breast cancer dataset from the UCI Archive,<sup><xref rid="R21" ref-type="bibr">21</xref></sup> and compared the results with a recent study evaluating different feature importance measures for the same dataset.<sup><xref rid="R22" ref-type="bibr">22</xref></sup></p>
      <p id="P52">The UCI dataset includes benign and malignant samples from 569 patients, 212 with cancer and 157 with fibrocystic breast masses. Each sample includes thirty features - ten real valued features for each cell nucleus (<italic toggle="yes">radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, fractal dimension</italic>) each reported as Mean, Standard Error (SE) and Worst.<sup><xref rid="R23" ref-type="bibr">23</xref></sup> As the classes in this dataset are almost linearly separable, classification per se is not a difficult task; however, the most important features generally differ depending on the technique used.<sup><xref rid="R24" ref-type="bibr">24</xref></sup> To further investigate this aspect, Saarela et al.<sup><xref rid="R22" ref-type="bibr">22</xref></sup> compared different feature importance measures using both linear (logistic regression) and non-linear (random forest) models and local interpretable model-agnostic explanations for the same dataset. In <xref rid="F2" ref-type="fig">Fig. 2</xref> we show the top 20 important features for the UCI Breast Cancer dataset computed with Zoish by training a XGboost classifier over ten folds of cross validation. The AUC for the trained classifier was 0.96, similar to the mean AUC reported in<sup><xref rid="R22" ref-type="bibr">22</xref></sup> (0.99+−002). Overall, the most important features in Zoish/XGboost agree well with the set of nine statistically significant features for both RF and LR reported in<sup><xref rid="R22" ref-type="bibr">22</xref></sup> – where, for each method, significance was computed through a procedure based on permutation tests – i.e. by shuffling class labels in the training data over hundreds of runs. Seven out of nine features deemed statistically significant in<sup><xref rid="R22" ref-type="bibr">22</xref></sup> were found in <xref rid="F2" ref-type="fig">Fig. 2</xref> (<italic toggle="yes">Mean concave points, Worst concave points, Worst Area, Worst Radius, Worst Perimeter, Mean Concavity, Mean Area</italic>), with five of the most significant features near the top of the list (<xref rid="T1" ref-type="table">Table 1</xref>). Only one feature was labeled as not significant by both RF and LR (<italic toggle="yes">Worst Compactness</italic>) and such feature has consistently a zero Shap value in Zoish/XGboost (not shown).</p>
      <p id="P53">It is worth noting that certain features which rank very high in Zoish/XGboost (<italic toggle="yes">Worst concavity, Worst texture, Compactness SE</italic>) appeared not to be significant in RF classification, hence not reported in the set of nine common, statistically significant features for breast cancer classification in the UCI dataset. Why did Zoish / XGboost select them up then? A very interesting hint comes from the analysis of local importance measures for a specific set of observations in the UCI dataset. Again in,<sup><xref rid="R22" ref-type="bibr">22</xref></sup> LIME (local interpretable model-agnostic explanations,<sup><xref rid="R25" ref-type="bibr">25</xref></sup>) was used to estimate local importances for the four most interesting observations, (i.e. correctly classified as benign with highest probability, correctly classified as malignant with highest probability, misclassified as benign with highest probability, and misclassified as malignant with highest probability). Strikingly, the features ranking high in Zoish/XGboost but absent in RF were also important features in LIME, especially for the observations misclassified as benign (false negatives), which are critical for medical purposes (<xref rid="T1" ref-type="table">Table 1</xref>). The final recommendation in<sup><xref rid="R22" ref-type="bibr">22</xref></sup> was to combine several explanation techniques in order to provide more reliable and trustworthy results, but this advice can often be impractical. Conversely, The Zoish/XGboost feature selector appears to select relevant features both at the global and local level, adding more detailed explanations of feature importance (i.e., not just the magnitude but also the direction of change), while being fast and straightforward to use.</p>
    </sec>
    <sec id="S13">
      <label>5.2.</label>
      <title>Use case 2: Predict short-term PD progression status using the Montreal Cognitive Assessment (MoCA)</title>
      <p id="P54">Our model, Zoish, was put to another practical test where we aimed to predict short-term PD progression status using the Montreal Cognitive Assessment (MoCA) total scores for patients in baseline. MoCA was developed as a tool to screen patients who present with mild cognitive complaints and usually perform in the normal range on the MMSE (Mini-Mental State Examination).<sup><xref rid="R26" ref-type="bibr">26</xref></sup> For this prediction task, we utilized the AMP-PD dataset, which is a comprehensive collection of data from various sources, including clinical information, genetic data, imaging data, and other biomarkers from individuals with Parkinson’s disease. The dataset consists of eight cohorts, making it a large and harmonized resource. Access to the data was obtained under the AMP-PD Data Use Agreement, and the information was retrieved from the website: <ext-link xlink:href="https://amp-pd.org/" ext-link-type="uri">https://amp-pd.org/</ext-link>. Our prediction model incorporated several essential features from the datasets, such as ”family history,” genetic information (PRS), ”medical history,” ”smoking and alcohol history,” and demographic information of the participants from the eight cohorts. After fitting the model, we evaluated its performance using the coefficient of determination, commonly known as R-squared, and achieved an R-squared value of 23.6 percent on the test dataset. Additionally, we calculated the Mean Squared Error (MSE) of the model to be 2.49 for the test dataset. Furthermore, the Mean Absolute Error (MAE) was found to be 13.04. The MAE represents the average absolute difference between the predicted values and the actual total score values in the test dataset. List of selected features by Zoish can be seen in <xref rid="F3" ref-type="fig">Fig. 3</xref></p>
      <p id="P55">In order to draw a comparison with another prevalent feature selector, specifically, <bold>SelectFromModel</bold> from the sklearn library, we applied it to the same dataset under identical conditions. This application yielded a Mean Absolute Error (MAE) of 15.91, a Mean Squared Error (MSE) of 2.69, and an R-squared value of 0.12. The features selected by this approach are depicted in <xref rid="F4" ref-type="fig">Fig. 4</xref>.</p>
      <p id="P56">As observed, Zoish not only outperforms its counterparts in terms of prediction accuracy, but it also excels in the selection of meaningful features. Notably, the Polygenic Risk Scores (PRSs) selected by Zoish have demonstrated substantial relevance to the Montreal Cognitive Assessment (MoCA). A prime example among these is <italic toggle="yes">PGS001641</italic>, which is renowned for its strong correlation with the volume of white matter, normalised for head size. This particular PRS underscores the genetic predisposition towards the volume of white matter, a crucial neuroimaging measurement that relates directly to cognitive functions evaluated in the MoCA test. Therefore, the selection of this PRS by Zoish validates its capability in discerning features with profound implications for cognitive assessment.</p>
    </sec>
  </sec>
  <sec id="S14">
    <label>6.</label>
    <title>Evaluations and Performance Analysis</title>
    <p id="P57">To offer a comprehensive evaluation of Zoish, we performed an array of tests ranging from comparative analyses to hyperparameter optimization and scalability assessments.</p>
    <sec id="S15">
      <title>Comparative Analysis:</title>
      <p id="P58">We initiated our evaluation with a rigorous comparison involving 300 synthetic datasets tailored to mirror the complexities of healthcare data. These datasets span regression, binary classification, and multi-label classification tasks. Zoish was compared against six established feature selection techniques from Scikit-learn under identical conditions.Our findings suggested that Zoish surpassed other selectors in 77% of regression problems, while in multi-label classification and binary classification tasks, Zoish outperformed in 53% and 57% of the cases, respectively (refer to <xref rid="T2" ref-type="table">Table 2</xref>).</p>
    </sec>
    <sec id="S16">
      <title>Hyperparameter Optimization:</title>
      <p id="P59">While Zoish itself is powerful, coupling it with a hyperparameter optimization tool like Lohrasb significantly improves performance. We performed 100 runs comparing Zoish’s efficacy with and without Lohrasb, and found marked improvements when paired with Lohrasb (see <xref rid="F5" ref-type="fig">Fig. 5</xref>).</p>
    </sec>
    <sec id="S17">
      <title>Scalability:</title>
      <p id="P60">Our most recent update introduces a faster algorithm for Shapley value computation, making Zoish efficient on large datasets. In our trials, Zoish selected 500 features from a dataset with 10,000 samples in under 2 minutes on a machine with a 2.3 GHz Quad-Core Intel Core i7 processor and 32 GB RAM.</p>
      <p id="P61">All the code for our tests is available in the public repository, allowing for independent verification and further exploration of Zoish’s capabilities.</p>
    </sec>
  </sec>
  <sec id="S18">
    <label>7.</label>
    <title>Discussion and Limitations</title>
    <p id="P62">This paper introduces Zoish, a feature selection tool built on cooperative game theory principles.<sup><xref rid="R16" ref-type="bibr">16</xref></sup> Zoish has gained traction in the community, as evidenced by a significant number of downloads from pip-trends (<ext-link xlink:href="https://piptrends.com/package/zoish" ext-link-type="uri">https://piptrends.com/package/zoish</ext-link>). The tool specializes in optimizing predictive models, particularly in the healthcare sector, and leverages Shapley additive values for a comprehensive view of feature importance at both local and global scales.<sup><xref rid="R10" ref-type="bibr">10</xref></sup> Through its Nullity property, Zoish effectively minimizes model complexity by omitting features with negligible Shapley values, thereby retaining model performance.<sup><xref rid="R3" ref-type="bibr">3</xref></sup> The tool is further enriched by integration with the Lohrasb package, which aids in achieving optimal estimators and hyperparameter settings.<sup><xref rid="R7" ref-type="bibr">7</xref></sup></p>
    <p id="P63">While Zoish’s capabilities are robust, some limitations are noteworthy. Firstly, its computational efficiency may be compromised when dealing with exceptionally large datasets. Secondly, the Shapley values employed assume feature independence and local linearity—assumptions that may not be fully met in complex applications like healthcare. These limitations are partially mitigated by Zoish’s tree-based modeling approach, which is robust to feature correlation and can capture non-linear relationships.<sup><xref rid="R6" ref-type="bibr">6</xref></sup></p>
    <p id="P64">The flexibility and interpretability of Zoish make it a promising tool for future applications in other high-dimensional data fields, including finance and e-commerce. Additional functionalities could be incorporated to broaden its applicability further.</p>
    <p id="P65">Future work will focus on extending Zoish’s utility to various high-dimensional domains and incorporating more algorithms and tools for an even more robust feature selection process. We have conducted extensive tests on synthetic datasets mimicking real-world complexities in healthcare, which are detailed in <xref rid="S14" ref-type="sec">Section 6</xref>. These tests demonstrate Zoish’s reliability and adaptability, even under challenging conditions.</p>
  </sec>
</body>
<back>
  <ack id="S19">
    <title>Acknowledgments</title>
    <p id="P66">This work is supported by UL1RR025774 and R01HG010881. Also We would like to express our gratitude to Dua, D. and Graff, C.<sup><xref rid="R21" ref-type="bibr">21</xref></sup> for generously providing us with the Breast Cancer Data used in this study. Without their contribution, this research would not have been possible.</p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="R1">
      <label>1.</label>
      <mixed-citation publication-type="other"><name><surname>Johnstone</surname><given-names>IM</given-names></name> and <name><surname>Titterington</surname><given-names>DM</given-names></name>, <source>Statistical challenges of high-dimensional data</source> (<year>2009</year>).</mixed-citation>
    </ref>
    <ref id="R2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><name><surname>Muja</surname><given-names>M</given-names></name> and <name><surname>Lowe</surname><given-names>DG</given-names></name>, <article-title>Scalable nearest neighbor algorithms for high dimensional data</article-title>, <source>IEEE transactions on pattern analysis and machine intelligence</source>
<volume>36</volume>, <fpage>2227</fpage> (<year>2014</year>).<pub-id pub-id-type="pmid">26353063</pub-id>
</mixed-citation>
    </ref>
    <ref id="R3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><name><surname>Sadaei</surname><given-names>HJ</given-names></name>, <name><surname>Cordova-Palomera</surname><given-names>A</given-names></name>, <name><surname>Lee</surname><given-names>J</given-names></name>, <name><surname>Padmanabhan</surname><given-names>J</given-names></name>, <name><surname>Chen</surname><given-names>S-F</given-names></name>, <name><surname>Wineinger</surname><given-names>NE</given-names></name>, <name><surname>Dias</surname><given-names>R</given-names></name>, <name><surname>Prilutsky</surname><given-names>D</given-names></name>, <name><surname>Szalma</surname><given-names>S</given-names></name> and <name><surname>Torkamani</surname><given-names>A</given-names></name>, <article-title>Genetically-informed prediction of short-term parkinson’s disease progression</article-title>, <source>npj Parkinson’s Disease</source>
<volume>8</volume>, p. <fpage>143</fpage> (<year>2022</year>).</mixed-citation>
    </ref>
    <ref id="R4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><name><surname>Tang</surname><given-names>J</given-names></name>, <name><surname>Alelyani</surname><given-names>S</given-names></name> and <name><surname>Liu</surname><given-names>H</given-names></name>, <article-title>Feature selection for classification: A review</article-title>, <source>Data classification: Algorithms and applications</source>, p. <fpage>37</fpage> (<year>2014</year>).</mixed-citation>
    </ref>
    <ref id="R5">
      <label>5.</label>
      <mixed-citation publication-type="other"><name><surname>Hall</surname><given-names>MA</given-names></name>, <source>Correlation-based feature selection for machine learning</source> (<year>1999</year>).</mixed-citation>
    </ref>
    <ref id="R6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><name><surname>Fryer</surname><given-names>D</given-names></name>, <name><surname>Strümke</surname><given-names>I</given-names></name> and <name><surname>Nguyen</surname><given-names>H</given-names></name>, <article-title>Shapley values for feature selection: The good, the bad, and the axioms</article-title>, <source>Ieee Access</source>
<volume>9</volume>, <fpage>144352</fpage> (<year>2021</year>).</mixed-citation>
    </ref>
    <ref id="R7">
      <label>7.</label>
      <mixed-citation publication-type="other"><name><surname>Javedani Sadaei</surname><given-names>H</given-names></name> and <name><surname>Torkamani</surname><given-names>A</given-names></name>, <source>Lohrasb: A scalable estimator optimization tool compatible with scikit-learn APIs</source> (<month>April</month>
<year>2023</year>).</mixed-citation>
    </ref>
    <ref id="R8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><name><surname>Michalak</surname><given-names>TP</given-names></name>, <name><surname>Aadithya</surname><given-names>KV</given-names></name>, <name><surname>Szczepański</surname><given-names>PL</given-names></name>, <name><surname>Jennings</surname><given-names>NR</given-names></name> and <name><surname>Narayanam</surname><given-names>R</given-names></name>, <article-title>Efficient computation of the shapley value for game-theoretic network centrality</article-title>, <source>Journal of Artificial Intelligence Research</source>
<volume>46</volume>, <fpage>607</fpage> (<year>2013</year>).</mixed-citation>
    </ref>
    <ref id="R9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><name><surname>Shapley</surname><given-names>LS</given-names></name>, <article-title>A value for n-person games</article-title>, <source>Contributions to the Theory of Games</source><volume>2</volume>, <fpage>307</fpage> (<year>1953</year>).</mixed-citation>
    </ref>
    <ref id="R10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><name><surname>Lundberg</surname><given-names>SM</given-names></name> and <name><surname>Lee</surname><given-names>S-I</given-names></name>, <article-title>A unified approach to interpreting model predictions</article-title>, <source>Advances in neural information processing systems</source>
<volume>30</volume> (<year>2017</year>).</mixed-citation>
    </ref>
    <ref id="R11">
      <label>11.</label>
      <mixed-citation publication-type="book"><name><surname>Molnar</surname><given-names>C</given-names></name>, <source>Interpretable machine learning</source> (<publisher-name>Lulu.com</publisher-name>, <year>2020</year>).</mixed-citation>
    </ref>
    <ref id="R12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><name><surname>Verdinelli</surname><given-names>I</given-names></name> and <name><surname>Wasserman</surname><given-names>L</given-names></name>, <article-title>Feature importance: A closer look at shapley values and loco</article-title>, <source>arXiv preprint arXiv:2303.05981</source> (<year>2023</year>).</mixed-citation>
    </ref>
    <ref id="R13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><name><surname>Karczmarz</surname><given-names>A</given-names></name>, <name><surname>Mukherjee</surname><given-names>A</given-names></name>, <name><surname>Sankowski</surname><given-names>P</given-names></name> and <name><surname>Wygocki</surname><given-names>P</given-names></name>, <article-title>Improved feature importance computations for tree models: Shapley vs. banzhaf</article-title>, <source>arXiv preprint arXiv:2108.04126</source> (<year>2021</year>).</mixed-citation>
    </ref>
    <ref id="R14">
      <label>14.</label>
      <mixed-citation publication-type="webpage"><name><surname>Lundberg</surname><given-names>S</given-names></name>, <name><surname>Lee</surname><given-names>S-I</given-names></name><etal/>, <source>SHAP (SHapley Additive exPlanations)</source><comment><ext-link xlink:href="https://github.com/slundberg/shap" ext-link-type="uri">https://github.com/slundberg/shap</ext-link></comment>, (<year>2023</year>), <comment>Accessed</comment>: <date-in-citation>2023-09-25</date-in-citation>.</mixed-citation>
    </ref>
    <ref id="R15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><name><surname>Yang</surname><given-names>J</given-names></name>, <article-title>Fast treeshap: Accelerating shap value computation for trees</article-title>, <source>arXiv preprint arXiv:2109.09847</source> (<year>2021</year>).</mixed-citation>
    </ref>
    <ref id="R16">
      <label>16.</label>
      <mixed-citation publication-type="other"><name><surname>Javedani Sadaei</surname><given-names>H</given-names></name> and <name><surname>Torkamani</surname><given-names>A</given-names></name>, <source>Zoish: Automated feature selectoion tools (July 2023), If you use this software, please cite it as below</source>.</mixed-citation>
    </ref>
    <ref id="R17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><name><surname>Varma</surname><given-names>S</given-names></name> and <name><surname>Simon</surname><given-names>R</given-names></name>, <article-title>Parameter estimation using grid search with cross-validation</article-title>, <source>XGBoost: A Scalable Tree Boosting System</source>
<volume>12</volume>, p. <fpage>48</fpage> (<year>2012</year>).</mixed-citation>
    </ref>
    <ref id="R18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><name><surname>Bergstra</surname><given-names>J</given-names></name> and <name><surname>Bengio</surname><given-names>Y</given-names></name>, <article-title>Random search for hyper-parameter optimization</article-title>, <source>Journal of Machine Learning Research</source>
<volume>13</volume>, <fpage>281</fpage> (<year>2012</year>).</mixed-citation>
    </ref>
    <ref id="R19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><name><surname>Liaw</surname><given-names>R</given-names></name>, <name><surname>Liang</surname><given-names>E</given-names></name>, <name><surname>Nishihara</surname><given-names>R</given-names></name>, <name><surname>Moritz</surname><given-names>P</given-names></name>, <name><surname>Gonzalez</surname><given-names>JE</given-names></name> and <name><surname>Stoica</surname><given-names>I</given-names></name>, <article-title>Tune: A research platform for distributed model selection and training in pytorch</article-title>, <source>arXiv preprint arXiv:1807.05118</source> (<year>2018</year>).</mixed-citation>
    </ref>
    <ref id="R20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><name><surname>Liaw</surname><given-names>R</given-names></name>, <name><surname>Liang</surname><given-names>E</given-names></name>, <name><surname>Nishihara</surname><given-names>R</given-names></name>, <name><surname>Moritz</surname><given-names>P</given-names></name>, <name><surname>Gonzalez</surname><given-names>JE</given-names></name> and <name><surname>Stoica</surname><given-names>I</given-names></name>, <article-title>Tune: A research platform for distributed model selection and training in pytorch</article-title>, <source>arXiv preprint arXiv:2002.02613</source> (<year>2020</year>).</mixed-citation>
    </ref>
    <ref id="R21">
      <label>21.</label>
      <mixed-citation publication-type="other"><name><surname>Dua</surname><given-names>D</given-names></name> and <name><surname>Graff</surname><given-names>C</given-names></name>, <source>UCI machine learning repository</source> (<year>2017</year>).</mixed-citation>
    </ref>
    <ref id="R22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><name><surname>Mirka Saarela</surname><given-names>SJ</given-names></name>, <article-title>Comparison of feature importance measures as explanations for classification models</article-title>, <source>SN Applied Sciences</source><volume>3</volume> (<year>2021</year>).</mixed-citation>
    </ref>
    <ref id="R23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><name><surname>WH Wolberg</surname><given-names>OM</given-names></name>, <article-title>WN Street, Machine learning techniques to diagnose breast cancer from image-processed nuclear features of fine needle aspirates</article-title>, <source>Cancer Letters</source><volume>77</volume>, p. <fpage>163</fpage>–<lpage>171</lpage> (<year>1994</year>).<pub-id pub-id-type="pmid">8168063</pub-id>
</mixed-citation>
    </ref>
    <ref id="R24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><name><surname>E Aličković</surname><given-names>AS</given-names></name>, <article-title>Breast cancer diagnosis using ga feature selection and rotation forest</article-title>, <source>Neural Computation Applications</source><volume>28</volume>, p. <fpage>753</fpage>–<lpage>763</lpage> (<year>2017</year>).</mixed-citation>
    </ref>
    <ref id="R25">
      <label>25.</label>
      <mixed-citation publication-type="other"><name><surname>Ribeiro</surname><given-names>MT</given-names></name>, <name><surname>Singh</surname><given-names>S</given-names></name> and <name><surname>Guestrin</surname><given-names>C</given-names></name>, <source>”why should i trust you?”: Explaining the predictions of any classifier</source> (<year>2016</year>).</mixed-citation>
    </ref>
    <ref id="R26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><name><surname>Nasreddine</surname><given-names>ZS</given-names></name>, <name><surname>Phillips</surname><given-names>NA</given-names></name>, <name><surname>Bédirian</surname><given-names>V</given-names></name>, <name><surname>Charbonneau</surname><given-names>S</given-names></name>, <name><surname>Whitehead</surname><given-names>V</given-names></name>, <name><surname>Collin</surname><given-names>I</given-names></name>, <name><surname>Cummings</surname><given-names>JL</given-names></name> and <name><surname>Chertkow</surname><given-names>H</given-names></name>, <article-title>The montreal cognitive assessment, moca: a brief screening tool for mild cognitive impairment</article-title>, <source>Journal of the American Geriatrics Society</source>
<volume>53</volume>, <fpage>695</fpage> (<year>2005</year>).<pub-id pub-id-type="pmid">15817019</pub-id>
</mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig position="float" id="F1">
    <label>Fig. 1.</label>
    <caption>
      <p id="P67">Zoish workflow</p>
    </caption>
    <graphic xlink:href="nihms-1952171-f0001" position="float"/>
  </fig>
  <fig position="float" id="F2">
    <label>Fig. 2.</label>
    <caption>
      <p id="P68">Shap summary plot of the top 20 important features for the UCI Breast Cancer dataset - computed with Zoish by training a XGboost classifier.</p>
    </caption>
    <graphic xlink:href="nihms-1952171-f0002" position="float"/>
  </fig>
  <fig position="float" id="F3">
    <label>Fig. 3.</label>
    <caption>
      <p id="P69">List of selected features and their importance by Zoish</p>
    </caption>
    <graphic xlink:href="nihms-1952171-f0003" position="float"/>
  </fig>
  <fig position="float" id="F4">
    <label>Fig. 4.</label>
    <caption>
      <p id="P70">List of selected features and their importance by SelectFromModel</p>
    </caption>
    <graphic xlink:href="nihms-1952171-f0004" position="float"/>
  </fig>
  <fig position="float" id="F5">
    <label>Fig. 5.</label>
    <caption>
      <p id="P71">The importance of Hyperparameter Optimization for Better Feature Selection</p>
    </caption>
    <graphic xlink:href="nihms-1952171-f0005" position="float"/>
  </fig>
  <table-wrap position="float" id="T1">
    <label>Table 1.</label>
    <caption>
      <p id="P72">Comparison of top features in the UCI Breast Cancer dataset</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Zoish features</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Significant features in LR and RF</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">LIME - Correctly classified benign (RF)</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">LIME - Misclassified benign (RF)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">concave points 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">concave points 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">perimeter 3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">concavity 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">perimeter 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">concave points 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">concave points 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">radius 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">radius 3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">concave points 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">texture 3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">texture 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">perimeter 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">texture 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">concavity 3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">compactness 2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">radius 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">concave points 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 2</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">radius 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">concavity 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">concavity 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">smoothness 3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">perimeter 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 1</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">perimeter 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">radius 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">texture 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">concave points 1</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">symmetry 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">concave points 2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">concavity 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">texture 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">texture 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">symmetry 2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">texture 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">smoothness 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">compactness 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">a</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">smoothness 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">radius 2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">concavity 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">perimeter 2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">radius 2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">compactness 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">fractal dimension 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">smoothness 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">compactness 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">symmetry 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">fractal dimension 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
      </tbody>
    </table>
    <table-wrap-foot>
      <fn id="TFN1">
        <p id="P73">This Table is about comparison of top features in the UCI Breast Cancer dataset computed by Zoish/XGboost vs. Random Forest / Logistic Regression / LIME. For all features, 1=Mean, 2=Standard Error, 3=Worst. Top 20 features in Column 2 are ranked based on permutation p-value for RF. In red are features found in LIME but not considered significant in RF/LR.</p>
      </fn>
    </table-wrap-foot>
  </table-wrap>
  <table-wrap position="float" id="T2">
    <label>Table 2.</label>
    <caption>
      <p id="P74">Performance comparison of Zoish with other feature selectors</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Selector</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Regression</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Binary Classification</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Multi-label Classification</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Zoish</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>77%</bold>
          </td>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>57%</bold>
          </td>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>53%</bold>
          </td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">VarianceThreshold</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">6%</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SelectKBest</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">6%</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SelectPercentile</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2%</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">RFE</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">10%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">6%</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">RFECV</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">7%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">10%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">11%</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SelectFromModel</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">15%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">16%</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD with MathML3 v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1-mathml3.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?properties manuscript?>
<?origin nihpa?>
<?iso-abbr Pac Symp Biocomput?>
<?submitter-system nihms?>
<?submitter-canonical-name World Scientific Publishing?>
<?submitter-canonical-id WORLDSCIENCE?>
<?submitter-userid 8068881?>
<?submitter-authority myNCBI?>
<?submitter-login worldscience?>
<?submitter-name World Scientific Publishing?>
<?domain nihpa?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">9711271</journal-id>
    <journal-id journal-id-type="pubmed-jr-id">20660</journal-id>
    <journal-id journal-id-type="nlm-ta">Pac Symp Biocomput</journal-id>
    <journal-id journal-id-type="iso-abbrev">Pac Symp Biocomput</journal-id>
    <journal-title-group>
      <journal-title>Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">2335-6928</issn>
    <issn pub-type="epub">2335-6936</issn>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10764073</article-id>
    <article-id pub-id-type="pmid">38160271</article-id>
    <article-id pub-id-type="manuscript">nihpa1952171</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Zoish: A Novel Feature Selection Approach Leveraging Shapley Additive Values for Machine Learning Applications in Healthcare</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Sadaei</surname>
          <given-names>Hossein Javedani</given-names>
        </name>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Loguercio</surname>
          <given-names>Salvatore</given-names>
        </name>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Neyestanak</surname>
          <given-names>Mahdi Shafiei</given-names>
        </name>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Torkamani</surname>
          <given-names>Ali</given-names>
        </name>
        <xref rid="CR1" ref-type="corresp">†</xref>
        <aff id="A1">Scripps Research Translational Institute, and Department of Integrative Structural and Computational Biology Scripps Research, La Jolla, CA 92037, USA</aff>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Prilutsky</surname>
          <given-names>Daria</given-names>
        </name>
        <aff id="A2">Takeda Development Center Americas, Inc., Cambridge, MA, 02139, USA</aff>
      </contrib>
    </contrib-group>
    <author-notes>
      <fn fn-type="con" id="FN1">
        <p id="P1">Author contributions statement</p>
        <p id="P2">Author contributions include; conceptualization (HJS, AT), methodology and software (HJS), validation and data curation (HJS, SL, MSN, AT, and DP), writing (HJS, SL, MSN, and AT), and funding (AT).</p>
      </fn>
      <corresp id="CR1">
        <label>†</label>
        <email>atorkama@scripps.edu</email>
      </corresp>
    </author-notes>
    <pub-date pub-type="nihms-submitted">
      <day>15</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>03</day>
      <month>1</month>
      <year>2024</year>
    </pub-date>
    <volume>29</volume>
    <fpage>81</fpage>
    <lpage>95</lpage>
    <permissions>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>Open Access chapter published by World Scientific Publishing Company and distributed under the terms of the Creative Commons Attribution Non-Commercial (CC BY-NC) 4.0 License.</license-p>
      </license>
    </permissions>
    <abstract id="ABS1">
      <p id="P3">In the intricate landscape of healthcare analytics, effective feature selection is a prerequisite for generating robust predictive models, especially given the common challenges of sample sizes and potential biases. Zoish uniquely addresses these issues by employing Shapley additive values—an idea rooted in cooperative game theory—to enable both transparent and automated feature selection. Unlike existing tools, Zoish is versatile, designed to seamlessly integrate with an array of machine learning libraries including scikit-learn, XGBoost, CatBoost, and imbalanced-learn.</p>
      <p id="P4">The distinct advantage of Zoish lies in its dual algorithmic approach for calculating Shapley values, allowing it to efficiently manage both large and small datasets. This adaptability renders it exceptionally suitable for a wide spectrum of healthcare-related tasks. The tool also places a strong emphasis on interpretability, providing comprehensive visualizations for analyzed features. Its customizable settings offer users fine-grained control over feature selection, thus optimizing for specific predictive objectives.</p>
      <p id="P5">This manuscript elucidates the mathematical framework underpinning Zoish and how it uniquely combines local and global feature selection into a single, streamlined process. To validate Zoish’s efficiency and adaptability, we present case studies in breast cancer prediction and Montreal Cognitive Assessment (MoCA) prediction in Parkinson’s disease, along with evaluations on 300 synthetic datasets. These applications underscore Zoish’s unparalleled performance in diverse healthcare contexts and against its counterparts.</p>
    </abstract>
    <kwd-group>
      <kwd>Feature Selectors</kwd>
      <kwd>Zoish</kwd>
      <kwd>SHapley Additive exPlanations</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <label>1.</label>
    <title>Introduction</title>
    <p id="P6">Healthcare datasets, despite being typically sparse and heterogeneous, are a treasure trove of rich information. However, their high-dimensionality, often paired with smaller sizes, presents obstacles to building predictive models, with overfitting and extensive training time being common concerns.<sup><xref rid="R1" ref-type="bibr">1</xref>–<xref rid="R3" ref-type="bibr">3</xref></sup> Feature selection becomes an essential strategy in this context, aimed at pruning redundant or less important features. This helps to minimize information loss, enhance model interpretability, and curtail computational demands.</p>
    <p id="P7">Although traditional feature selection methods, grounded in statistical concepts like correlation analysis or chi-square tests, are widely used, they tend to fall short in offering detailed insights into feature importance. This shortcoming, along with the manual effort required, can lead to a time-intensive cycle of feature selection and performance evaluation, thus requiring expert intervention.<sup><xref rid="R4" ref-type="bibr">4</xref>,<xref rid="R5" ref-type="bibr">5</xref></sup></p>
    <p id="P8">Our proposed feature selection tool, Zoish, aims to overcome these limitations by utilizing the mathematical framework of additive Shapley values. Originating from cooperative game theory, Shapley values offer detailed understanding of feature importance,<sup><xref rid="R6" ref-type="bibr">6</xref></sup> thereby enhancing both local (instance-level) and global interpretability. Moreover, the integration of Zoish with our scalable hyperparameter optimization package, Lohrasb,<sup><xref rid="R7" ref-type="bibr">7</xref></sup> facilitates building models with optimal feature sets, all the while maintaining an industry-ready, user-friendly design.</p>
    <p id="P9">The structure of the paper is as follows. The first section sheds light on the core concepts of additive Shapley values and delves into the mathematical principles vital to Zoish. Subsequent to this, a section introducing a user guide for Zoish is presented. Lastly, we demonstrate the adaptability of Zoish through a variety of use-cases, experiments on large synthetic datasets, and a closing discussion.</p>
  </sec>
  <sec id="S2">
    <label>2.</label>
    <title>Theoretical Foundations of Zoish</title>
    <p id="P10">Our exploration into Zoish begins with the foundation of its theoretical structure, built upon Shapley additive values. First proposed in the field of cooperative game theory, Shapley additive values have proven to be a potent tool for understanding the contribution of each feature to a prediction made by a machine learning model. Simply put, the Shapley value of a feature represents the average marginal contribution of that feature, factored across all possible combinations of features.</p>
    <sec id="S3">
      <label>2.1.</label>
      <title>Shapley Additive Values and Feature Selection: A Game Theoretical Approach</title>
      <p id="P11">In the realm of cooperative game theory, the Shapley value denotes each player’s payoff based on their marginal contribution across all possible coalitions. In machine learning, ‘players’ correspond to the features and ‘game’ to the prediction task.<sup><xref rid="R6" ref-type="bibr">6</xref></sup></p>
      <p id="P12">An additive cooperative game assumes that the value of any coalition equals the sum of its members’ independent values. This idea leads us to Shapley additive values, where the Shapley value of a feature equals its average marginal contribution across all feature subsets.</p>
      <p id="P13">Mathematically, the Shapley value for a feature <italic toggle="yes">i</italic> in an additive game is:
<disp-formula id="FD1"><mml:math id="M1" display="block"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>S</mml:mi><mml:mo>⊆</mml:mo><mml:mi>N</mml:mi><mml:mo>∖</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>i</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>!</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>N</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>!</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>N</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>!</mml:mo></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo>∪</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>i</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>i</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p id="P14">In this formula, <italic toggle="yes">|N|</italic> denotes the total number of features, <italic toggle="yes">S</italic> a subset of features excluding feature <italic toggle="yes">i</italic>, and <italic toggle="yes">|S|</italic> the number of features in subset <italic toggle="yes">S</italic>. The terms <inline-formula><mml:math id="M2" display="inline"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>!</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>N</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>!</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M30" display="inline"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>N</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>!</mml:mo></mml:mrow></mml:math></inline-formula> calculate the number of possible permutations of features. The expression <inline-formula><mml:math id="M3" display="inline"><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo>∪</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>i</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> computes the marginal contribution of feature i when added to subset <italic toggle="yes">S</italic>.</p>
      <p id="P15">In feature selection, the Shapley value presents a way to distribute the model’s prediction among the features, based on their marginal contribution.<sup><xref rid="R8" ref-type="bibr">8</xref>,<xref rid="R9" ref-type="bibr">9</xref></sup> Shapley Additive exPlanations (SHAP) values, maintaining additivity, provide a unified measure of feature importance, attributing the difference between the model’s actual and expected output to each influencing feature.<sup><xref rid="R10" ref-type="bibr">10</xref></sup> High Shapley or SHAP values indicate significant feature importance, while values near zero suggest negligible predictive power.<sup><xref rid="R11" ref-type="bibr">11</xref></sup> This correlation aids in reducing data dimensionality and enhances model interpretability, marking a significant stride in feature selection.</p>
    </sec>
    <sec id="S4">
      <label>2.2.</label>
      <title>Properties of Shapley Additive Values</title>
      <p id="P16">The Shapley additive values satisfy a number of properties that make them particularly useful for interpreting machine learning models:
<list list-type="bullet" id="L2"><list-item><p id="P17"><bold>Efficiency</bold>: The sum of the Shapley values of all features is equal to the difference between the prediction for an instance and the average prediction over all instances.</p></list-item><list-item><p id="P18"><bold>Symmetry</bold>: If two features contribute equally to all possible combinations of features, they have the same Shapley value.</p></list-item><list-item><p id="P19"><bold>Additivity</bold>: Given two games (or in our context, two models), the Shapley value of the combined game is the sum of the Shapley values of the individual games.</p></list-item><list-item><p id="P20"><bold>Nullity (Dummy):</bold> If a feature does not improve the prediction for any combination of features, its Shapley value is zero.</p></list-item></list></p>
      <sec id="S5">
        <label>2.2.1.</label>
        <title>Proof of Nullity (Dummy)</title>
        <p id="P21">The Nullity (Dummy) property states that if a feature does not change the prediction model, i.e., its contribution is always zero, then its Shapley value is also zero. Let <italic toggle="yes">f</italic> be the prediction model and <italic toggle="yes">d</italic> be such a dummy feature.</p>
        <p id="P22">According to the definition of Shapley values, the Shapley value of a feature is the average of its marginal contributions across all possible subsets of features. Therefore, the Shapley value <italic toggle="yes">s</italic><sub><italic toggle="yes">f</italic></sub> (<italic toggle="yes">d</italic>) for the dummy feature <italic toggle="yes">d</italic> is:
<disp-formula id="FD2"><mml:math id="M4" display="block"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:munder><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>S</mml:mi><mml:mo>⊆</mml:mo><mml:mi>N</mml:mi><mml:mo>∖</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>d</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>!</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>N</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>!</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo>∪</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>d</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="P23">In the above formula, <italic toggle="yes">M</italic> is the total number of possible subsets of <italic toggle="yes">N</italic> that can be formed when the dummy feature <italic toggle="yes">d</italic> is excluded. The term <inline-formula><mml:math id="M5" display="inline"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>!</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>N</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>!</mml:mo></mml:mrow></mml:math></inline-formula> is the number of permutations of <italic toggle="yes">N</italic> in which the dummy feature <italic toggle="yes">d</italic> and the features in subset <italic toggle="yes">S</italic> appear together, and <inline-formula><mml:math id="M6" display="inline"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo>∪</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>d</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <italic toggle="yes">f</italic>(<italic toggle="yes">S</italic>) are the values of the game <italic toggle="yes">f</italic> when the dummy feature <italic toggle="yes">d</italic> is added to subset <italic toggle="yes">S</italic> and when it is not, respectively.</p>
        <p id="P24">Since <italic toggle="yes">d</italic> is a dummy feature, adding it to any subset <italic toggle="yes">S</italic> does not change the value of the game <italic toggle="yes">f</italic>. Thus, we have <inline-formula><mml:math id="M7" display="inline"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo>∪</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>d</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> for all <inline-formula><mml:math id="M8" display="inline"><mml:mrow><mml:mi>S</mml:mi><mml:mo>⊆</mml:mo><mml:mi>N</mml:mi><mml:mo>∖</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>d</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> which simplifies the above formula to:
<disp-formula id="FD3"><mml:math id="M9" display="block"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:munder><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>S</mml:mi><mml:mo>⊆</mml:mo><mml:mi>N</mml:mi><mml:mo>∖</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>d</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>!</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>N</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>!</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD4"><mml:math id="M10" display="block"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></disp-formula></p>
        <p id="P25">This confirms that the Shapley value of a feature that does not contribute to the prediction model is indeed zero, thus proving the Nullity (Dummy) property.<sup><xref rid="R9" ref-type="bibr">9</xref></sup></p>
      </sec>
    </sec>
    <sec id="S6">
      <label>2.3.</label>
      <title>Leveraging Shapley Additive Values for Efficient Feature Selection</title>
      <p id="P26">Shapley additive values have become increasingly prominent in feature selection for machine learning due to their robustness, efficiency, and power. Verdinelli et al. examined the explainability of machine learning models, focusing on methods such as LOCO and Shapley Values for assessing feature importance. Although their research indicated that Shapley Values do not eliminate feature correlation, they proposed new, statistically sound axioms for measuring feature importance.<sup><xref rid="R12" ref-type="bibr">12</xref></sup> In a separate study, Karczmarz et al. compared Shapley and Banzhaf values in the context of explaining tree ensemble models. They found Banzhaf values to be more intuitive, efficient, and numerically robust, and introduced faster algorithms for both methods to improve computational efficiency.<sup><xref rid="R13" ref-type="bibr">13</xref></sup> The SHAP (SHapley Additive exPlanation) library serves as a prime example of effectively leveraging the additivity and efficiency inherent in Shapley values.<sup><xref rid="R14" ref-type="bibr">14</xref></sup> A fundamental advantage of Shapley values lies in their additivity, which enables fast and efficient computation, especially in the context of tree-based models. The SHAP and FastTreeShap libraries employ Tree SHAP, a highly efficient and accurate algorithm designed for tree ensembles.<sup><xref rid="R14" ref-type="bibr">14</xref>,<xref rid="R15" ref-type="bibr">15</xref></sup> Given the innate additivity of ensemble tree models, which amalgamate multiple decision trees, this characteristic ensures swift and precise computation of Shapley values.<sup><xref rid="R6" ref-type="bibr">6</xref></sup> The efficacy of Shapley values is further underscored by their intrinsic efficiency. This is manifested in the fact that the sum of the Shapley values for all features equals the difference between the prediction for a specific instance and the average prediction across all instances. This aspect permits a meaningful distribution of the ”credit” for a prediction across features, hence illuminating their relative importance. The Zoish package,<sup><xref rid="R16" ref-type="bibr">16</xref></sup> designed to optimize feature selection, taps into the beneficial properties of Shapley values. It employs the Nullity (or Dummy) property to eliminate features with Shapley values close to or exactly zero, indicating their minimal predictive relevance. To assist users in setting the cut-off level, two methods are offered: one involves setting an internal parameter called <italic toggle="yes">threshold</italic>, while the other entails defining the number of desired features to retain in the model. By removing these non-influential features, the package enables dimensionality reduction of the model without sacrificing prediction quality. The symmetry property of Shapley values is also exploited by Zoish. This property mandates that if two features contribute equally to all possible subsets of other features, they must have identical Shapley values. By identifying and discarding these redundant features, Zoish facilitates the construction of models that are simpler and more interpretable, with no compromise on predictive power. By utilizing the SHAP and FastTreeShap libraries to incorporate Shapley additive values, Zoish implicitly benefits from its advantages, including the mathematical robustness and beneficial properties of Shapley values. Therefore, these libraries and the Zoish package present themselves as potent instruments for feature selection, spanning a wide range of machine learning tasks.</p>
    </sec>
  </sec>
  <sec id="S7">
    <label>3.</label>
    <title>Feature Selection Approaches</title>
    <p id="P27">Zoish is a versatile package designed to enhance the evaluation of feature importance and improve the overall performance of machine learning models.<sup><xref rid="R16" ref-type="bibr">16</xref></sup> While Zoish can function effectively as a standalone tool for feature selection, it is engineered to be highly extensible and can seamlessly integrate with hyperparameter optimization packages to further refine its capabilities. One such potent integration is with the Lohrasb package,<sup><xref rid="R7" ref-type="bibr">7</xref></sup> which provides advanced tuning methods to optimize the feature selection process. However, it’s worth noting that users are not confined to using Lohrasb; Zoish’s flexible architecture allows for easy integration with other hyperparameter optimization tools as well.</p>
    <sec id="S8">
      <label>3.1.</label>
      <title>Optimization and Flexibility in Zoish</title>
      <p id="P28">Zoish’s integration with Lohrasb serves a dual purpose: it not only optimizes the tree-based estimator used for feature selection but also offers a choice of hyperparameter tuning methods, including Optuna, GridSearchCV,<sup><xref rid="R17" ref-type="bibr">17</xref></sup> RandomizedSearchCV,<sup><xref rid="R18" ref-type="bibr">18</xref></sup> OptunaSearchCV, tune-sklearn,<sup><xref rid="R19" ref-type="bibr">19</xref></sup> and Ray’s Tune.<sup><xref rid="R20" ref-type="bibr">20</xref></sup> This optimization is crucial for enhancing Zoish’s feature selection capabilities, as represented in <xref rid="F1" ref-type="fig">Fig 1</xref>. However, the use of Lohrasb is optional, giving users the freedom to employ other tree-based estimators or hyperparameter tuning engines. Even without hyperparameter optimization, Zoish maintains its core functionality, allowing for a balance between efficiency and interpretability. The importance of hyperparameter optimization for feature selection is further elaborated in <xref rid="S14" ref-type="sec">Section 6</xref>. Therefore, while Lohrasb’s role is significant for optimal performance, users have the flexibility to choose an approach that best suits their specific needs.</p>
    </sec>
    <sec id="S9">
      <label>3.2.</label>
      <title>Workflow explanation</title>
      <p id="P29">Within a machine learning pipeline, Zoish functions as a feature selection component. The pipeline commences by cleaning and splitting the original dataset into training and validation subsets. A tree-based estimator, which is compatible with Zoish, is trained on the training subset. If hyperparameter tuning is applied, tools such as Lohrasb optimize the estimator against a specific metric, as shown in <xref rid="F1" ref-type="fig">Fig 1</xref>.</p>
      <p id="P30">Once the estimator is optimized, it becomes an input to Zoish along with a set of parameters, such as cross-validation settings, Shapley value calculation algorithms, and feature importance thresholds. Zoish computes Shapley values via either the SHAP library for smaller datasets, due to its exhaustive computational approach, or FastTreeShap for larger datasets, owing to its computational efficiency.</p>
      <p id="P31">Based on the calculated Shapley values, Zoish automatically selects the highest-ranking features. The training set is then narrowed down to these selected features. Subsequently, these refined training and validation sets are channeled to the next steps in the pipeline, which usually involve fitting another predictive model.</p>
      <p id="P32">To ensure robustness in feature selection, Zoish employs multiple rounds of cross-validation on the same training set, regulated by a parameter named <italic toggle="yes">n</italic><sub>-</sub><italic toggle="yes">iter</italic>.</p>
      <p id="P33">Documentation and code examples elucidating these operational details can be found in the Zoish repository.</p>
    </sec>
  </sec>
  <sec id="S10">
    <label>4.</label>
    <title>Source Code, installation and usage example</title>
    <p id="P34">The public repository of Zoish is available on GitHub alongside examples for end users is <ext-link xlink:href="https://github.com/TorkamaniLab/zoish" ext-link-type="uri">https://github.com/TorkamaniLab/zoish</ext-link>. Zoish package is available on PyPI and can be installed with pip:
<preformat position="float" xml:space="preserve">
pip install zoish
</preformat></p>
    <p id="P401">A straightforward example demonstrates how Zoish can be effectively combined with hyperparameter optimizers. Both this example and the comprehensive documentation in the repository highlight the package’s flexibility and adaptability across various scenarios.</p>
    <preformat position="float" xml:space="preserve">
import xgboost as xgb
from sklearn.model_selection import KFold, GridSearchCV
from zoish.feature_selectors.shap_selectors import ShapFeatureSelector
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

X_train, X_test, y_train, y_test = … # Your dataset here

grid = GridSearchCV(xgb.XGBClassifier(), {‘n_estimators’: [100, 150], ‘max_depth’:\
[6, 10], ‘gamma’: [0.5, 1.0]}, cv=5, n_jobs=−1, scoring=‘accuracy’).\
fit(X_train, y_train)
shap_selector = ShapFeatureSelector(grid.best_estimator_, \
num_features=15, cv=KFold(10), n_iter=5, direction=“maximum”, \
scoring=“accuracy”, algorithm=‘auto’, use_faster_algorithm=True)
pipeline = Pipeline(steps=[(“s”, shap_selector), \
(“m”, LogisticRegression())]).fit(X_train, y_train)
</preformat>
  </sec>
  <sec id="S11">
    <label>5.</label>
    <title>Use cases and applications</title>
    <sec id="S12">
      <label>5.1.</label>
      <title>Use case 1: Application to UCI breast cancer dataset - comparison with related XAI work</title>
      <p id="P51">To demonstrate the value of the Zoish feature selector in a real use case from the biomedical domain, we applied it to the openly available breast cancer dataset from the UCI Archive,<sup><xref rid="R21" ref-type="bibr">21</xref></sup> and compared the results with a recent study evaluating different feature importance measures for the same dataset.<sup><xref rid="R22" ref-type="bibr">22</xref></sup></p>
      <p id="P52">The UCI dataset includes benign and malignant samples from 569 patients, 212 with cancer and 157 with fibrocystic breast masses. Each sample includes thirty features - ten real valued features for each cell nucleus (<italic toggle="yes">radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, fractal dimension</italic>) each reported as Mean, Standard Error (SE) and Worst.<sup><xref rid="R23" ref-type="bibr">23</xref></sup> As the classes in this dataset are almost linearly separable, classification per se is not a difficult task; however, the most important features generally differ depending on the technique used.<sup><xref rid="R24" ref-type="bibr">24</xref></sup> To further investigate this aspect, Saarela et al.<sup><xref rid="R22" ref-type="bibr">22</xref></sup> compared different feature importance measures using both linear (logistic regression) and non-linear (random forest) models and local interpretable model-agnostic explanations for the same dataset. In <xref rid="F2" ref-type="fig">Fig. 2</xref> we show the top 20 important features for the UCI Breast Cancer dataset computed with Zoish by training a XGboost classifier over ten folds of cross validation. The AUC for the trained classifier was 0.96, similar to the mean AUC reported in<sup><xref rid="R22" ref-type="bibr">22</xref></sup> (0.99+−002). Overall, the most important features in Zoish/XGboost agree well with the set of nine statistically significant features for both RF and LR reported in<sup><xref rid="R22" ref-type="bibr">22</xref></sup> – where, for each method, significance was computed through a procedure based on permutation tests – i.e. by shuffling class labels in the training data over hundreds of runs. Seven out of nine features deemed statistically significant in<sup><xref rid="R22" ref-type="bibr">22</xref></sup> were found in <xref rid="F2" ref-type="fig">Fig. 2</xref> (<italic toggle="yes">Mean concave points, Worst concave points, Worst Area, Worst Radius, Worst Perimeter, Mean Concavity, Mean Area</italic>), with five of the most significant features near the top of the list (<xref rid="T1" ref-type="table">Table 1</xref>). Only one feature was labeled as not significant by both RF and LR (<italic toggle="yes">Worst Compactness</italic>) and such feature has consistently a zero Shap value in Zoish/XGboost (not shown).</p>
      <p id="P53">It is worth noting that certain features which rank very high in Zoish/XGboost (<italic toggle="yes">Worst concavity, Worst texture, Compactness SE</italic>) appeared not to be significant in RF classification, hence not reported in the set of nine common, statistically significant features for breast cancer classification in the UCI dataset. Why did Zoish / XGboost select them up then? A very interesting hint comes from the analysis of local importance measures for a specific set of observations in the UCI dataset. Again in,<sup><xref rid="R22" ref-type="bibr">22</xref></sup> LIME (local interpretable model-agnostic explanations,<sup><xref rid="R25" ref-type="bibr">25</xref></sup>) was used to estimate local importances for the four most interesting observations, (i.e. correctly classified as benign with highest probability, correctly classified as malignant with highest probability, misclassified as benign with highest probability, and misclassified as malignant with highest probability). Strikingly, the features ranking high in Zoish/XGboost but absent in RF were also important features in LIME, especially for the observations misclassified as benign (false negatives), which are critical for medical purposes (<xref rid="T1" ref-type="table">Table 1</xref>). The final recommendation in<sup><xref rid="R22" ref-type="bibr">22</xref></sup> was to combine several explanation techniques in order to provide more reliable and trustworthy results, but this advice can often be impractical. Conversely, The Zoish/XGboost feature selector appears to select relevant features both at the global and local level, adding more detailed explanations of feature importance (i.e., not just the magnitude but also the direction of change), while being fast and straightforward to use.</p>
    </sec>
    <sec id="S13">
      <label>5.2.</label>
      <title>Use case 2: Predict short-term PD progression status using the Montreal Cognitive Assessment (MoCA)</title>
      <p id="P54">Our model, Zoish, was put to another practical test where we aimed to predict short-term PD progression status using the Montreal Cognitive Assessment (MoCA) total scores for patients in baseline. MoCA was developed as a tool to screen patients who present with mild cognitive complaints and usually perform in the normal range on the MMSE (Mini-Mental State Examination).<sup><xref rid="R26" ref-type="bibr">26</xref></sup> For this prediction task, we utilized the AMP-PD dataset, which is a comprehensive collection of data from various sources, including clinical information, genetic data, imaging data, and other biomarkers from individuals with Parkinson’s disease. The dataset consists of eight cohorts, making it a large and harmonized resource. Access to the data was obtained under the AMP-PD Data Use Agreement, and the information was retrieved from the website: <ext-link xlink:href="https://amp-pd.org/" ext-link-type="uri">https://amp-pd.org/</ext-link>. Our prediction model incorporated several essential features from the datasets, such as ”family history,” genetic information (PRS), ”medical history,” ”smoking and alcohol history,” and demographic information of the participants from the eight cohorts. After fitting the model, we evaluated its performance using the coefficient of determination, commonly known as R-squared, and achieved an R-squared value of 23.6 percent on the test dataset. Additionally, we calculated the Mean Squared Error (MSE) of the model to be 2.49 for the test dataset. Furthermore, the Mean Absolute Error (MAE) was found to be 13.04. The MAE represents the average absolute difference between the predicted values and the actual total score values in the test dataset. List of selected features by Zoish can be seen in <xref rid="F3" ref-type="fig">Fig. 3</xref></p>
      <p id="P55">In order to draw a comparison with another prevalent feature selector, specifically, <bold>SelectFromModel</bold> from the sklearn library, we applied it to the same dataset under identical conditions. This application yielded a Mean Absolute Error (MAE) of 15.91, a Mean Squared Error (MSE) of 2.69, and an R-squared value of 0.12. The features selected by this approach are depicted in <xref rid="F4" ref-type="fig">Fig. 4</xref>.</p>
      <p id="P56">As observed, Zoish not only outperforms its counterparts in terms of prediction accuracy, but it also excels in the selection of meaningful features. Notably, the Polygenic Risk Scores (PRSs) selected by Zoish have demonstrated substantial relevance to the Montreal Cognitive Assessment (MoCA). A prime example among these is <italic toggle="yes">PGS001641</italic>, which is renowned for its strong correlation with the volume of white matter, normalised for head size. This particular PRS underscores the genetic predisposition towards the volume of white matter, a crucial neuroimaging measurement that relates directly to cognitive functions evaluated in the MoCA test. Therefore, the selection of this PRS by Zoish validates its capability in discerning features with profound implications for cognitive assessment.</p>
    </sec>
  </sec>
  <sec id="S14">
    <label>6.</label>
    <title>Evaluations and Performance Analysis</title>
    <p id="P57">To offer a comprehensive evaluation of Zoish, we performed an array of tests ranging from comparative analyses to hyperparameter optimization and scalability assessments.</p>
    <sec id="S15">
      <title>Comparative Analysis:</title>
      <p id="P58">We initiated our evaluation with a rigorous comparison involving 300 synthetic datasets tailored to mirror the complexities of healthcare data. These datasets span regression, binary classification, and multi-label classification tasks. Zoish was compared against six established feature selection techniques from Scikit-learn under identical conditions.Our findings suggested that Zoish surpassed other selectors in 77% of regression problems, while in multi-label classification and binary classification tasks, Zoish outperformed in 53% and 57% of the cases, respectively (refer to <xref rid="T2" ref-type="table">Table 2</xref>).</p>
    </sec>
    <sec id="S16">
      <title>Hyperparameter Optimization:</title>
      <p id="P59">While Zoish itself is powerful, coupling it with a hyperparameter optimization tool like Lohrasb significantly improves performance. We performed 100 runs comparing Zoish’s efficacy with and without Lohrasb, and found marked improvements when paired with Lohrasb (see <xref rid="F5" ref-type="fig">Fig. 5</xref>).</p>
    </sec>
    <sec id="S17">
      <title>Scalability:</title>
      <p id="P60">Our most recent update introduces a faster algorithm for Shapley value computation, making Zoish efficient on large datasets. In our trials, Zoish selected 500 features from a dataset with 10,000 samples in under 2 minutes on a machine with a 2.3 GHz Quad-Core Intel Core i7 processor and 32 GB RAM.</p>
      <p id="P61">All the code for our tests is available in the public repository, allowing for independent verification and further exploration of Zoish’s capabilities.</p>
    </sec>
  </sec>
  <sec id="S18">
    <label>7.</label>
    <title>Discussion and Limitations</title>
    <p id="P62">This paper introduces Zoish, a feature selection tool built on cooperative game theory principles.<sup><xref rid="R16" ref-type="bibr">16</xref></sup> Zoish has gained traction in the community, as evidenced by a significant number of downloads from pip-trends (<ext-link xlink:href="https://piptrends.com/package/zoish" ext-link-type="uri">https://piptrends.com/package/zoish</ext-link>). The tool specializes in optimizing predictive models, particularly in the healthcare sector, and leverages Shapley additive values for a comprehensive view of feature importance at both local and global scales.<sup><xref rid="R10" ref-type="bibr">10</xref></sup> Through its Nullity property, Zoish effectively minimizes model complexity by omitting features with negligible Shapley values, thereby retaining model performance.<sup><xref rid="R3" ref-type="bibr">3</xref></sup> The tool is further enriched by integration with the Lohrasb package, which aids in achieving optimal estimators and hyperparameter settings.<sup><xref rid="R7" ref-type="bibr">7</xref></sup></p>
    <p id="P63">While Zoish’s capabilities are robust, some limitations are noteworthy. Firstly, its computational efficiency may be compromised when dealing with exceptionally large datasets. Secondly, the Shapley values employed assume feature independence and local linearity—assumptions that may not be fully met in complex applications like healthcare. These limitations are partially mitigated by Zoish’s tree-based modeling approach, which is robust to feature correlation and can capture non-linear relationships.<sup><xref rid="R6" ref-type="bibr">6</xref></sup></p>
    <p id="P64">The flexibility and interpretability of Zoish make it a promising tool for future applications in other high-dimensional data fields, including finance and e-commerce. Additional functionalities could be incorporated to broaden its applicability further.</p>
    <p id="P65">Future work will focus on extending Zoish’s utility to various high-dimensional domains and incorporating more algorithms and tools for an even more robust feature selection process. We have conducted extensive tests on synthetic datasets mimicking real-world complexities in healthcare, which are detailed in <xref rid="S14" ref-type="sec">Section 6</xref>. These tests demonstrate Zoish’s reliability and adaptability, even under challenging conditions.</p>
  </sec>
</body>
<back>
  <ack id="S19">
    <title>Acknowledgments</title>
    <p id="P66">This work is supported by UL1RR025774 and R01HG010881. Also We would like to express our gratitude to Dua, D. and Graff, C.<sup><xref rid="R21" ref-type="bibr">21</xref></sup> for generously providing us with the Breast Cancer Data used in this study. Without their contribution, this research would not have been possible.</p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="R1">
      <label>1.</label>
      <mixed-citation publication-type="other"><name><surname>Johnstone</surname><given-names>IM</given-names></name> and <name><surname>Titterington</surname><given-names>DM</given-names></name>, <source>Statistical challenges of high-dimensional data</source> (<year>2009</year>).</mixed-citation>
    </ref>
    <ref id="R2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><name><surname>Muja</surname><given-names>M</given-names></name> and <name><surname>Lowe</surname><given-names>DG</given-names></name>, <article-title>Scalable nearest neighbor algorithms for high dimensional data</article-title>, <source>IEEE transactions on pattern analysis and machine intelligence</source>
<volume>36</volume>, <fpage>2227</fpage> (<year>2014</year>).<pub-id pub-id-type="pmid">26353063</pub-id>
</mixed-citation>
    </ref>
    <ref id="R3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><name><surname>Sadaei</surname><given-names>HJ</given-names></name>, <name><surname>Cordova-Palomera</surname><given-names>A</given-names></name>, <name><surname>Lee</surname><given-names>J</given-names></name>, <name><surname>Padmanabhan</surname><given-names>J</given-names></name>, <name><surname>Chen</surname><given-names>S-F</given-names></name>, <name><surname>Wineinger</surname><given-names>NE</given-names></name>, <name><surname>Dias</surname><given-names>R</given-names></name>, <name><surname>Prilutsky</surname><given-names>D</given-names></name>, <name><surname>Szalma</surname><given-names>S</given-names></name> and <name><surname>Torkamani</surname><given-names>A</given-names></name>, <article-title>Genetically-informed prediction of short-term parkinson’s disease progression</article-title>, <source>npj Parkinson’s Disease</source>
<volume>8</volume>, p. <fpage>143</fpage> (<year>2022</year>).</mixed-citation>
    </ref>
    <ref id="R4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><name><surname>Tang</surname><given-names>J</given-names></name>, <name><surname>Alelyani</surname><given-names>S</given-names></name> and <name><surname>Liu</surname><given-names>H</given-names></name>, <article-title>Feature selection for classification: A review</article-title>, <source>Data classification: Algorithms and applications</source>, p. <fpage>37</fpage> (<year>2014</year>).</mixed-citation>
    </ref>
    <ref id="R5">
      <label>5.</label>
      <mixed-citation publication-type="other"><name><surname>Hall</surname><given-names>MA</given-names></name>, <source>Correlation-based feature selection for machine learning</source> (<year>1999</year>).</mixed-citation>
    </ref>
    <ref id="R6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><name><surname>Fryer</surname><given-names>D</given-names></name>, <name><surname>Strümke</surname><given-names>I</given-names></name> and <name><surname>Nguyen</surname><given-names>H</given-names></name>, <article-title>Shapley values for feature selection: The good, the bad, and the axioms</article-title>, <source>Ieee Access</source>
<volume>9</volume>, <fpage>144352</fpage> (<year>2021</year>).</mixed-citation>
    </ref>
    <ref id="R7">
      <label>7.</label>
      <mixed-citation publication-type="other"><name><surname>Javedani Sadaei</surname><given-names>H</given-names></name> and <name><surname>Torkamani</surname><given-names>A</given-names></name>, <source>Lohrasb: A scalable estimator optimization tool compatible with scikit-learn APIs</source> (<month>April</month>
<year>2023</year>).</mixed-citation>
    </ref>
    <ref id="R8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><name><surname>Michalak</surname><given-names>TP</given-names></name>, <name><surname>Aadithya</surname><given-names>KV</given-names></name>, <name><surname>Szczepański</surname><given-names>PL</given-names></name>, <name><surname>Jennings</surname><given-names>NR</given-names></name> and <name><surname>Narayanam</surname><given-names>R</given-names></name>, <article-title>Efficient computation of the shapley value for game-theoretic network centrality</article-title>, <source>Journal of Artificial Intelligence Research</source>
<volume>46</volume>, <fpage>607</fpage> (<year>2013</year>).</mixed-citation>
    </ref>
    <ref id="R9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><name><surname>Shapley</surname><given-names>LS</given-names></name>, <article-title>A value for n-person games</article-title>, <source>Contributions to the Theory of Games</source><volume>2</volume>, <fpage>307</fpage> (<year>1953</year>).</mixed-citation>
    </ref>
    <ref id="R10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><name><surname>Lundberg</surname><given-names>SM</given-names></name> and <name><surname>Lee</surname><given-names>S-I</given-names></name>, <article-title>A unified approach to interpreting model predictions</article-title>, <source>Advances in neural information processing systems</source>
<volume>30</volume> (<year>2017</year>).</mixed-citation>
    </ref>
    <ref id="R11">
      <label>11.</label>
      <mixed-citation publication-type="book"><name><surname>Molnar</surname><given-names>C</given-names></name>, <source>Interpretable machine learning</source> (<publisher-name>Lulu.com</publisher-name>, <year>2020</year>).</mixed-citation>
    </ref>
    <ref id="R12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><name><surname>Verdinelli</surname><given-names>I</given-names></name> and <name><surname>Wasserman</surname><given-names>L</given-names></name>, <article-title>Feature importance: A closer look at shapley values and loco</article-title>, <source>arXiv preprint arXiv:2303.05981</source> (<year>2023</year>).</mixed-citation>
    </ref>
    <ref id="R13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><name><surname>Karczmarz</surname><given-names>A</given-names></name>, <name><surname>Mukherjee</surname><given-names>A</given-names></name>, <name><surname>Sankowski</surname><given-names>P</given-names></name> and <name><surname>Wygocki</surname><given-names>P</given-names></name>, <article-title>Improved feature importance computations for tree models: Shapley vs. banzhaf</article-title>, <source>arXiv preprint arXiv:2108.04126</source> (<year>2021</year>).</mixed-citation>
    </ref>
    <ref id="R14">
      <label>14.</label>
      <mixed-citation publication-type="webpage"><name><surname>Lundberg</surname><given-names>S</given-names></name>, <name><surname>Lee</surname><given-names>S-I</given-names></name><etal/>, <source>SHAP (SHapley Additive exPlanations)</source><comment><ext-link xlink:href="https://github.com/slundberg/shap" ext-link-type="uri">https://github.com/slundberg/shap</ext-link></comment>, (<year>2023</year>), <comment>Accessed</comment>: <date-in-citation>2023-09-25</date-in-citation>.</mixed-citation>
    </ref>
    <ref id="R15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><name><surname>Yang</surname><given-names>J</given-names></name>, <article-title>Fast treeshap: Accelerating shap value computation for trees</article-title>, <source>arXiv preprint arXiv:2109.09847</source> (<year>2021</year>).</mixed-citation>
    </ref>
    <ref id="R16">
      <label>16.</label>
      <mixed-citation publication-type="other"><name><surname>Javedani Sadaei</surname><given-names>H</given-names></name> and <name><surname>Torkamani</surname><given-names>A</given-names></name>, <source>Zoish: Automated feature selectoion tools (July 2023), If you use this software, please cite it as below</source>.</mixed-citation>
    </ref>
    <ref id="R17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><name><surname>Varma</surname><given-names>S</given-names></name> and <name><surname>Simon</surname><given-names>R</given-names></name>, <article-title>Parameter estimation using grid search with cross-validation</article-title>, <source>XGBoost: A Scalable Tree Boosting System</source>
<volume>12</volume>, p. <fpage>48</fpage> (<year>2012</year>).</mixed-citation>
    </ref>
    <ref id="R18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><name><surname>Bergstra</surname><given-names>J</given-names></name> and <name><surname>Bengio</surname><given-names>Y</given-names></name>, <article-title>Random search for hyper-parameter optimization</article-title>, <source>Journal of Machine Learning Research</source>
<volume>13</volume>, <fpage>281</fpage> (<year>2012</year>).</mixed-citation>
    </ref>
    <ref id="R19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><name><surname>Liaw</surname><given-names>R</given-names></name>, <name><surname>Liang</surname><given-names>E</given-names></name>, <name><surname>Nishihara</surname><given-names>R</given-names></name>, <name><surname>Moritz</surname><given-names>P</given-names></name>, <name><surname>Gonzalez</surname><given-names>JE</given-names></name> and <name><surname>Stoica</surname><given-names>I</given-names></name>, <article-title>Tune: A research platform for distributed model selection and training in pytorch</article-title>, <source>arXiv preprint arXiv:1807.05118</source> (<year>2018</year>).</mixed-citation>
    </ref>
    <ref id="R20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><name><surname>Liaw</surname><given-names>R</given-names></name>, <name><surname>Liang</surname><given-names>E</given-names></name>, <name><surname>Nishihara</surname><given-names>R</given-names></name>, <name><surname>Moritz</surname><given-names>P</given-names></name>, <name><surname>Gonzalez</surname><given-names>JE</given-names></name> and <name><surname>Stoica</surname><given-names>I</given-names></name>, <article-title>Tune: A research platform for distributed model selection and training in pytorch</article-title>, <source>arXiv preprint arXiv:2002.02613</source> (<year>2020</year>).</mixed-citation>
    </ref>
    <ref id="R21">
      <label>21.</label>
      <mixed-citation publication-type="other"><name><surname>Dua</surname><given-names>D</given-names></name> and <name><surname>Graff</surname><given-names>C</given-names></name>, <source>UCI machine learning repository</source> (<year>2017</year>).</mixed-citation>
    </ref>
    <ref id="R22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><name><surname>Mirka Saarela</surname><given-names>SJ</given-names></name>, <article-title>Comparison of feature importance measures as explanations for classification models</article-title>, <source>SN Applied Sciences</source><volume>3</volume> (<year>2021</year>).</mixed-citation>
    </ref>
    <ref id="R23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><name><surname>WH Wolberg</surname><given-names>OM</given-names></name>, <article-title>WN Street, Machine learning techniques to diagnose breast cancer from image-processed nuclear features of fine needle aspirates</article-title>, <source>Cancer Letters</source><volume>77</volume>, p. <fpage>163</fpage>–<lpage>171</lpage> (<year>1994</year>).<pub-id pub-id-type="pmid">8168063</pub-id>
</mixed-citation>
    </ref>
    <ref id="R24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><name><surname>E Aličković</surname><given-names>AS</given-names></name>, <article-title>Breast cancer diagnosis using ga feature selection and rotation forest</article-title>, <source>Neural Computation Applications</source><volume>28</volume>, p. <fpage>753</fpage>–<lpage>763</lpage> (<year>2017</year>).</mixed-citation>
    </ref>
    <ref id="R25">
      <label>25.</label>
      <mixed-citation publication-type="other"><name><surname>Ribeiro</surname><given-names>MT</given-names></name>, <name><surname>Singh</surname><given-names>S</given-names></name> and <name><surname>Guestrin</surname><given-names>C</given-names></name>, <source>”why should i trust you?”: Explaining the predictions of any classifier</source> (<year>2016</year>).</mixed-citation>
    </ref>
    <ref id="R26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><name><surname>Nasreddine</surname><given-names>ZS</given-names></name>, <name><surname>Phillips</surname><given-names>NA</given-names></name>, <name><surname>Bédirian</surname><given-names>V</given-names></name>, <name><surname>Charbonneau</surname><given-names>S</given-names></name>, <name><surname>Whitehead</surname><given-names>V</given-names></name>, <name><surname>Collin</surname><given-names>I</given-names></name>, <name><surname>Cummings</surname><given-names>JL</given-names></name> and <name><surname>Chertkow</surname><given-names>H</given-names></name>, <article-title>The montreal cognitive assessment, moca: a brief screening tool for mild cognitive impairment</article-title>, <source>Journal of the American Geriatrics Society</source>
<volume>53</volume>, <fpage>695</fpage> (<year>2005</year>).<pub-id pub-id-type="pmid">15817019</pub-id>
</mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig position="float" id="F1">
    <label>Fig. 1.</label>
    <caption>
      <p id="P67">Zoish workflow</p>
    </caption>
    <graphic xlink:href="nihms-1952171-f0001" position="float"/>
  </fig>
  <fig position="float" id="F2">
    <label>Fig. 2.</label>
    <caption>
      <p id="P68">Shap summary plot of the top 20 important features for the UCI Breast Cancer dataset - computed with Zoish by training a XGboost classifier.</p>
    </caption>
    <graphic xlink:href="nihms-1952171-f0002" position="float"/>
  </fig>
  <fig position="float" id="F3">
    <label>Fig. 3.</label>
    <caption>
      <p id="P69">List of selected features and their importance by Zoish</p>
    </caption>
    <graphic xlink:href="nihms-1952171-f0003" position="float"/>
  </fig>
  <fig position="float" id="F4">
    <label>Fig. 4.</label>
    <caption>
      <p id="P70">List of selected features and their importance by SelectFromModel</p>
    </caption>
    <graphic xlink:href="nihms-1952171-f0004" position="float"/>
  </fig>
  <fig position="float" id="F5">
    <label>Fig. 5.</label>
    <caption>
      <p id="P71">The importance of Hyperparameter Optimization for Better Feature Selection</p>
    </caption>
    <graphic xlink:href="nihms-1952171-f0005" position="float"/>
  </fig>
  <table-wrap position="float" id="T1">
    <label>Table 1.</label>
    <caption>
      <p id="P72">Comparison of top features in the UCI Breast Cancer dataset</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Zoish features</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Significant features in LR and RF</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">LIME - Correctly classified benign (RF)</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">LIME - Misclassified benign (RF)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">concave points 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">concave points 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">perimeter 3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">concavity 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">perimeter 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">concave points 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">concave points 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">radius 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">radius 3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">concave points 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">texture 3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">texture 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">perimeter 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">texture 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">concavity 3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">compactness 2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">radius 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">concave points 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 2</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">radius 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">concavity 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">concavity 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">smoothness 3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">perimeter 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 1</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">perimeter 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">radius 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">texture 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">concave points 1</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">symmetry 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">concave points 2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">concavity 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">texture 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">texture 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">symmetry 2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">texture 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">smoothness 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">compactness 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">a</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">smoothness 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">radius 2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">concavity 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">perimeter 2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">radius 2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">compactness 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">fractal dimension 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">smoothness 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">compactness 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">symmetry 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">fractal dimension 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
      </tbody>
    </table>
    <table-wrap-foot>
      <fn id="TFN1">
        <p id="P73">This Table is about comparison of top features in the UCI Breast Cancer dataset computed by Zoish/XGboost vs. Random Forest / Logistic Regression / LIME. For all features, 1=Mean, 2=Standard Error, 3=Worst. Top 20 features in Column 2 are ranked based on permutation p-value for RF. In red are features found in LIME but not considered significant in RF/LR.</p>
      </fn>
    </table-wrap-foot>
  </table-wrap>
  <table-wrap position="float" id="T2">
    <label>Table 2.</label>
    <caption>
      <p id="P74">Performance comparison of Zoish with other feature selectors</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Selector</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Regression</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Binary Classification</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Multi-label Classification</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Zoish</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>77%</bold>
          </td>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>57%</bold>
          </td>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>53%</bold>
          </td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">VarianceThreshold</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">6%</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SelectKBest</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">6%</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SelectPercentile</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2%</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">RFE</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">10%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">6%</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">RFECV</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">7%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">10%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">11%</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SelectFromModel</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">15%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">16%</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD with MathML3 v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1-mathml3.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?properties manuscript?>
<?origin nihpa?>
<?iso-abbr Pac Symp Biocomput?>
<?submitter-system nihms?>
<?submitter-canonical-name World Scientific Publishing?>
<?submitter-canonical-id WORLDSCIENCE?>
<?submitter-userid 8068881?>
<?submitter-authority myNCBI?>
<?submitter-login worldscience?>
<?submitter-name World Scientific Publishing?>
<?domain nihpa?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">9711271</journal-id>
    <journal-id journal-id-type="pubmed-jr-id">20660</journal-id>
    <journal-id journal-id-type="nlm-ta">Pac Symp Biocomput</journal-id>
    <journal-id journal-id-type="iso-abbrev">Pac Symp Biocomput</journal-id>
    <journal-title-group>
      <journal-title>Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">2335-6928</issn>
    <issn pub-type="epub">2335-6936</issn>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10764073</article-id>
    <article-id pub-id-type="pmid">38160271</article-id>
    <article-id pub-id-type="manuscript">nihpa1952171</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Zoish: A Novel Feature Selection Approach Leveraging Shapley Additive Values for Machine Learning Applications in Healthcare</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Sadaei</surname>
          <given-names>Hossein Javedani</given-names>
        </name>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Loguercio</surname>
          <given-names>Salvatore</given-names>
        </name>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Neyestanak</surname>
          <given-names>Mahdi Shafiei</given-names>
        </name>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Torkamani</surname>
          <given-names>Ali</given-names>
        </name>
        <xref rid="CR1" ref-type="corresp">†</xref>
        <aff id="A1">Scripps Research Translational Institute, and Department of Integrative Structural and Computational Biology Scripps Research, La Jolla, CA 92037, USA</aff>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Prilutsky</surname>
          <given-names>Daria</given-names>
        </name>
        <aff id="A2">Takeda Development Center Americas, Inc., Cambridge, MA, 02139, USA</aff>
      </contrib>
    </contrib-group>
    <author-notes>
      <fn fn-type="con" id="FN1">
        <p id="P1">Author contributions statement</p>
        <p id="P2">Author contributions include; conceptualization (HJS, AT), methodology and software (HJS), validation and data curation (HJS, SL, MSN, AT, and DP), writing (HJS, SL, MSN, and AT), and funding (AT).</p>
      </fn>
      <corresp id="CR1">
        <label>†</label>
        <email>atorkama@scripps.edu</email>
      </corresp>
    </author-notes>
    <pub-date pub-type="nihms-submitted">
      <day>15</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>03</day>
      <month>1</month>
      <year>2024</year>
    </pub-date>
    <volume>29</volume>
    <fpage>81</fpage>
    <lpage>95</lpage>
    <permissions>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>Open Access chapter published by World Scientific Publishing Company and distributed under the terms of the Creative Commons Attribution Non-Commercial (CC BY-NC) 4.0 License.</license-p>
      </license>
    </permissions>
    <abstract id="ABS1">
      <p id="P3">In the intricate landscape of healthcare analytics, effective feature selection is a prerequisite for generating robust predictive models, especially given the common challenges of sample sizes and potential biases. Zoish uniquely addresses these issues by employing Shapley additive values—an idea rooted in cooperative game theory—to enable both transparent and automated feature selection. Unlike existing tools, Zoish is versatile, designed to seamlessly integrate with an array of machine learning libraries including scikit-learn, XGBoost, CatBoost, and imbalanced-learn.</p>
      <p id="P4">The distinct advantage of Zoish lies in its dual algorithmic approach for calculating Shapley values, allowing it to efficiently manage both large and small datasets. This adaptability renders it exceptionally suitable for a wide spectrum of healthcare-related tasks. The tool also places a strong emphasis on interpretability, providing comprehensive visualizations for analyzed features. Its customizable settings offer users fine-grained control over feature selection, thus optimizing for specific predictive objectives.</p>
      <p id="P5">This manuscript elucidates the mathematical framework underpinning Zoish and how it uniquely combines local and global feature selection into a single, streamlined process. To validate Zoish’s efficiency and adaptability, we present case studies in breast cancer prediction and Montreal Cognitive Assessment (MoCA) prediction in Parkinson’s disease, along with evaluations on 300 synthetic datasets. These applications underscore Zoish’s unparalleled performance in diverse healthcare contexts and against its counterparts.</p>
    </abstract>
    <kwd-group>
      <kwd>Feature Selectors</kwd>
      <kwd>Zoish</kwd>
      <kwd>SHapley Additive exPlanations</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <label>1.</label>
    <title>Introduction</title>
    <p id="P6">Healthcare datasets, despite being typically sparse and heterogeneous, are a treasure trove of rich information. However, their high-dimensionality, often paired with smaller sizes, presents obstacles to building predictive models, with overfitting and extensive training time being common concerns.<sup><xref rid="R1" ref-type="bibr">1</xref>–<xref rid="R3" ref-type="bibr">3</xref></sup> Feature selection becomes an essential strategy in this context, aimed at pruning redundant or less important features. This helps to minimize information loss, enhance model interpretability, and curtail computational demands.</p>
    <p id="P7">Although traditional feature selection methods, grounded in statistical concepts like correlation analysis or chi-square tests, are widely used, they tend to fall short in offering detailed insights into feature importance. This shortcoming, along with the manual effort required, can lead to a time-intensive cycle of feature selection and performance evaluation, thus requiring expert intervention.<sup><xref rid="R4" ref-type="bibr">4</xref>,<xref rid="R5" ref-type="bibr">5</xref></sup></p>
    <p id="P8">Our proposed feature selection tool, Zoish, aims to overcome these limitations by utilizing the mathematical framework of additive Shapley values. Originating from cooperative game theory, Shapley values offer detailed understanding of feature importance,<sup><xref rid="R6" ref-type="bibr">6</xref></sup> thereby enhancing both local (instance-level) and global interpretability. Moreover, the integration of Zoish with our scalable hyperparameter optimization package, Lohrasb,<sup><xref rid="R7" ref-type="bibr">7</xref></sup> facilitates building models with optimal feature sets, all the while maintaining an industry-ready, user-friendly design.</p>
    <p id="P9">The structure of the paper is as follows. The first section sheds light on the core concepts of additive Shapley values and delves into the mathematical principles vital to Zoish. Subsequent to this, a section introducing a user guide for Zoish is presented. Lastly, we demonstrate the adaptability of Zoish through a variety of use-cases, experiments on large synthetic datasets, and a closing discussion.</p>
  </sec>
  <sec id="S2">
    <label>2.</label>
    <title>Theoretical Foundations of Zoish</title>
    <p id="P10">Our exploration into Zoish begins with the foundation of its theoretical structure, built upon Shapley additive values. First proposed in the field of cooperative game theory, Shapley additive values have proven to be a potent tool for understanding the contribution of each feature to a prediction made by a machine learning model. Simply put, the Shapley value of a feature represents the average marginal contribution of that feature, factored across all possible combinations of features.</p>
    <sec id="S3">
      <label>2.1.</label>
      <title>Shapley Additive Values and Feature Selection: A Game Theoretical Approach</title>
      <p id="P11">In the realm of cooperative game theory, the Shapley value denotes each player’s payoff based on their marginal contribution across all possible coalitions. In machine learning, ‘players’ correspond to the features and ‘game’ to the prediction task.<sup><xref rid="R6" ref-type="bibr">6</xref></sup></p>
      <p id="P12">An additive cooperative game assumes that the value of any coalition equals the sum of its members’ independent values. This idea leads us to Shapley additive values, where the Shapley value of a feature equals its average marginal contribution across all feature subsets.</p>
      <p id="P13">Mathematically, the Shapley value for a feature <italic toggle="yes">i</italic> in an additive game is:
<disp-formula id="FD1"><mml:math id="M1" display="block"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>S</mml:mi><mml:mo>⊆</mml:mo><mml:mi>N</mml:mi><mml:mo>∖</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>i</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>!</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>N</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>!</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>N</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>!</mml:mo></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo>∪</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>i</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>i</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p id="P14">In this formula, <italic toggle="yes">|N|</italic> denotes the total number of features, <italic toggle="yes">S</italic> a subset of features excluding feature <italic toggle="yes">i</italic>, and <italic toggle="yes">|S|</italic> the number of features in subset <italic toggle="yes">S</italic>. The terms <inline-formula><mml:math id="M2" display="inline"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>!</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>N</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>!</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M30" display="inline"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>N</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>!</mml:mo></mml:mrow></mml:math></inline-formula> calculate the number of possible permutations of features. The expression <inline-formula><mml:math id="M3" display="inline"><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo>∪</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>i</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> computes the marginal contribution of feature i when added to subset <italic toggle="yes">S</italic>.</p>
      <p id="P15">In feature selection, the Shapley value presents a way to distribute the model’s prediction among the features, based on their marginal contribution.<sup><xref rid="R8" ref-type="bibr">8</xref>,<xref rid="R9" ref-type="bibr">9</xref></sup> Shapley Additive exPlanations (SHAP) values, maintaining additivity, provide a unified measure of feature importance, attributing the difference between the model’s actual and expected output to each influencing feature.<sup><xref rid="R10" ref-type="bibr">10</xref></sup> High Shapley or SHAP values indicate significant feature importance, while values near zero suggest negligible predictive power.<sup><xref rid="R11" ref-type="bibr">11</xref></sup> This correlation aids in reducing data dimensionality and enhances model interpretability, marking a significant stride in feature selection.</p>
    </sec>
    <sec id="S4">
      <label>2.2.</label>
      <title>Properties of Shapley Additive Values</title>
      <p id="P16">The Shapley additive values satisfy a number of properties that make them particularly useful for interpreting machine learning models:
<list list-type="bullet" id="L2"><list-item><p id="P17"><bold>Efficiency</bold>: The sum of the Shapley values of all features is equal to the difference between the prediction for an instance and the average prediction over all instances.</p></list-item><list-item><p id="P18"><bold>Symmetry</bold>: If two features contribute equally to all possible combinations of features, they have the same Shapley value.</p></list-item><list-item><p id="P19"><bold>Additivity</bold>: Given two games (or in our context, two models), the Shapley value of the combined game is the sum of the Shapley values of the individual games.</p></list-item><list-item><p id="P20"><bold>Nullity (Dummy):</bold> If a feature does not improve the prediction for any combination of features, its Shapley value is zero.</p></list-item></list></p>
      <sec id="S5">
        <label>2.2.1.</label>
        <title>Proof of Nullity (Dummy)</title>
        <p id="P21">The Nullity (Dummy) property states that if a feature does not change the prediction model, i.e., its contribution is always zero, then its Shapley value is also zero. Let <italic toggle="yes">f</italic> be the prediction model and <italic toggle="yes">d</italic> be such a dummy feature.</p>
        <p id="P22">According to the definition of Shapley values, the Shapley value of a feature is the average of its marginal contributions across all possible subsets of features. Therefore, the Shapley value <italic toggle="yes">s</italic><sub><italic toggle="yes">f</italic></sub> (<italic toggle="yes">d</italic>) for the dummy feature <italic toggle="yes">d</italic> is:
<disp-formula id="FD2"><mml:math id="M4" display="block"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:munder><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>S</mml:mi><mml:mo>⊆</mml:mo><mml:mi>N</mml:mi><mml:mo>∖</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>d</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>!</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>N</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>!</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo>∪</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>d</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="P23">In the above formula, <italic toggle="yes">M</italic> is the total number of possible subsets of <italic toggle="yes">N</italic> that can be formed when the dummy feature <italic toggle="yes">d</italic> is excluded. The term <inline-formula><mml:math id="M5" display="inline"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>!</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>N</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>!</mml:mo></mml:mrow></mml:math></inline-formula> is the number of permutations of <italic toggle="yes">N</italic> in which the dummy feature <italic toggle="yes">d</italic> and the features in subset <italic toggle="yes">S</italic> appear together, and <inline-formula><mml:math id="M6" display="inline"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo>∪</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>d</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <italic toggle="yes">f</italic>(<italic toggle="yes">S</italic>) are the values of the game <italic toggle="yes">f</italic> when the dummy feature <italic toggle="yes">d</italic> is added to subset <italic toggle="yes">S</italic> and when it is not, respectively.</p>
        <p id="P24">Since <italic toggle="yes">d</italic> is a dummy feature, adding it to any subset <italic toggle="yes">S</italic> does not change the value of the game <italic toggle="yes">f</italic>. Thus, we have <inline-formula><mml:math id="M7" display="inline"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo>∪</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>d</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> for all <inline-formula><mml:math id="M8" display="inline"><mml:mrow><mml:mi>S</mml:mi><mml:mo>⊆</mml:mo><mml:mi>N</mml:mi><mml:mo>∖</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>d</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> which simplifies the above formula to:
<disp-formula id="FD3"><mml:math id="M9" display="block"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:munder><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>S</mml:mi><mml:mo>⊆</mml:mo><mml:mi>N</mml:mi><mml:mo>∖</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>d</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>!</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>N</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>!</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD4"><mml:math id="M10" display="block"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></disp-formula></p>
        <p id="P25">This confirms that the Shapley value of a feature that does not contribute to the prediction model is indeed zero, thus proving the Nullity (Dummy) property.<sup><xref rid="R9" ref-type="bibr">9</xref></sup></p>
      </sec>
    </sec>
    <sec id="S6">
      <label>2.3.</label>
      <title>Leveraging Shapley Additive Values for Efficient Feature Selection</title>
      <p id="P26">Shapley additive values have become increasingly prominent in feature selection for machine learning due to their robustness, efficiency, and power. Verdinelli et al. examined the explainability of machine learning models, focusing on methods such as LOCO and Shapley Values for assessing feature importance. Although their research indicated that Shapley Values do not eliminate feature correlation, they proposed new, statistically sound axioms for measuring feature importance.<sup><xref rid="R12" ref-type="bibr">12</xref></sup> In a separate study, Karczmarz et al. compared Shapley and Banzhaf values in the context of explaining tree ensemble models. They found Banzhaf values to be more intuitive, efficient, and numerically robust, and introduced faster algorithms for both methods to improve computational efficiency.<sup><xref rid="R13" ref-type="bibr">13</xref></sup> The SHAP (SHapley Additive exPlanation) library serves as a prime example of effectively leveraging the additivity and efficiency inherent in Shapley values.<sup><xref rid="R14" ref-type="bibr">14</xref></sup> A fundamental advantage of Shapley values lies in their additivity, which enables fast and efficient computation, especially in the context of tree-based models. The SHAP and FastTreeShap libraries employ Tree SHAP, a highly efficient and accurate algorithm designed for tree ensembles.<sup><xref rid="R14" ref-type="bibr">14</xref>,<xref rid="R15" ref-type="bibr">15</xref></sup> Given the innate additivity of ensemble tree models, which amalgamate multiple decision trees, this characteristic ensures swift and precise computation of Shapley values.<sup><xref rid="R6" ref-type="bibr">6</xref></sup> The efficacy of Shapley values is further underscored by their intrinsic efficiency. This is manifested in the fact that the sum of the Shapley values for all features equals the difference between the prediction for a specific instance and the average prediction across all instances. This aspect permits a meaningful distribution of the ”credit” for a prediction across features, hence illuminating their relative importance. The Zoish package,<sup><xref rid="R16" ref-type="bibr">16</xref></sup> designed to optimize feature selection, taps into the beneficial properties of Shapley values. It employs the Nullity (or Dummy) property to eliminate features with Shapley values close to or exactly zero, indicating their minimal predictive relevance. To assist users in setting the cut-off level, two methods are offered: one involves setting an internal parameter called <italic toggle="yes">threshold</italic>, while the other entails defining the number of desired features to retain in the model. By removing these non-influential features, the package enables dimensionality reduction of the model without sacrificing prediction quality. The symmetry property of Shapley values is also exploited by Zoish. This property mandates that if two features contribute equally to all possible subsets of other features, they must have identical Shapley values. By identifying and discarding these redundant features, Zoish facilitates the construction of models that are simpler and more interpretable, with no compromise on predictive power. By utilizing the SHAP and FastTreeShap libraries to incorporate Shapley additive values, Zoish implicitly benefits from its advantages, including the mathematical robustness and beneficial properties of Shapley values. Therefore, these libraries and the Zoish package present themselves as potent instruments for feature selection, spanning a wide range of machine learning tasks.</p>
    </sec>
  </sec>
  <sec id="S7">
    <label>3.</label>
    <title>Feature Selection Approaches</title>
    <p id="P27">Zoish is a versatile package designed to enhance the evaluation of feature importance and improve the overall performance of machine learning models.<sup><xref rid="R16" ref-type="bibr">16</xref></sup> While Zoish can function effectively as a standalone tool for feature selection, it is engineered to be highly extensible and can seamlessly integrate with hyperparameter optimization packages to further refine its capabilities. One such potent integration is with the Lohrasb package,<sup><xref rid="R7" ref-type="bibr">7</xref></sup> which provides advanced tuning methods to optimize the feature selection process. However, it’s worth noting that users are not confined to using Lohrasb; Zoish’s flexible architecture allows for easy integration with other hyperparameter optimization tools as well.</p>
    <sec id="S8">
      <label>3.1.</label>
      <title>Optimization and Flexibility in Zoish</title>
      <p id="P28">Zoish’s integration with Lohrasb serves a dual purpose: it not only optimizes the tree-based estimator used for feature selection but also offers a choice of hyperparameter tuning methods, including Optuna, GridSearchCV,<sup><xref rid="R17" ref-type="bibr">17</xref></sup> RandomizedSearchCV,<sup><xref rid="R18" ref-type="bibr">18</xref></sup> OptunaSearchCV, tune-sklearn,<sup><xref rid="R19" ref-type="bibr">19</xref></sup> and Ray’s Tune.<sup><xref rid="R20" ref-type="bibr">20</xref></sup> This optimization is crucial for enhancing Zoish’s feature selection capabilities, as represented in <xref rid="F1" ref-type="fig">Fig 1</xref>. However, the use of Lohrasb is optional, giving users the freedom to employ other tree-based estimators or hyperparameter tuning engines. Even without hyperparameter optimization, Zoish maintains its core functionality, allowing for a balance between efficiency and interpretability. The importance of hyperparameter optimization for feature selection is further elaborated in <xref rid="S14" ref-type="sec">Section 6</xref>. Therefore, while Lohrasb’s role is significant for optimal performance, users have the flexibility to choose an approach that best suits their specific needs.</p>
    </sec>
    <sec id="S9">
      <label>3.2.</label>
      <title>Workflow explanation</title>
      <p id="P29">Within a machine learning pipeline, Zoish functions as a feature selection component. The pipeline commences by cleaning and splitting the original dataset into training and validation subsets. A tree-based estimator, which is compatible with Zoish, is trained on the training subset. If hyperparameter tuning is applied, tools such as Lohrasb optimize the estimator against a specific metric, as shown in <xref rid="F1" ref-type="fig">Fig 1</xref>.</p>
      <p id="P30">Once the estimator is optimized, it becomes an input to Zoish along with a set of parameters, such as cross-validation settings, Shapley value calculation algorithms, and feature importance thresholds. Zoish computes Shapley values via either the SHAP library for smaller datasets, due to its exhaustive computational approach, or FastTreeShap for larger datasets, owing to its computational efficiency.</p>
      <p id="P31">Based on the calculated Shapley values, Zoish automatically selects the highest-ranking features. The training set is then narrowed down to these selected features. Subsequently, these refined training and validation sets are channeled to the next steps in the pipeline, which usually involve fitting another predictive model.</p>
      <p id="P32">To ensure robustness in feature selection, Zoish employs multiple rounds of cross-validation on the same training set, regulated by a parameter named <italic toggle="yes">n</italic><sub>-</sub><italic toggle="yes">iter</italic>.</p>
      <p id="P33">Documentation and code examples elucidating these operational details can be found in the Zoish repository.</p>
    </sec>
  </sec>
  <sec id="S10">
    <label>4.</label>
    <title>Source Code, installation and usage example</title>
    <p id="P34">The public repository of Zoish is available on GitHub alongside examples for end users is <ext-link xlink:href="https://github.com/TorkamaniLab/zoish" ext-link-type="uri">https://github.com/TorkamaniLab/zoish</ext-link>. Zoish package is available on PyPI and can be installed with pip:
<preformat position="float" xml:space="preserve">
pip install zoish
</preformat></p>
    <p id="P401">A straightforward example demonstrates how Zoish can be effectively combined with hyperparameter optimizers. Both this example and the comprehensive documentation in the repository highlight the package’s flexibility and adaptability across various scenarios.</p>
    <preformat position="float" xml:space="preserve">
import xgboost as xgb
from sklearn.model_selection import KFold, GridSearchCV
from zoish.feature_selectors.shap_selectors import ShapFeatureSelector
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

X_train, X_test, y_train, y_test = … # Your dataset here

grid = GridSearchCV(xgb.XGBClassifier(), {‘n_estimators’: [100, 150], ‘max_depth’:\
[6, 10], ‘gamma’: [0.5, 1.0]}, cv=5, n_jobs=−1, scoring=‘accuracy’).\
fit(X_train, y_train)
shap_selector = ShapFeatureSelector(grid.best_estimator_, \
num_features=15, cv=KFold(10), n_iter=5, direction=“maximum”, \
scoring=“accuracy”, algorithm=‘auto’, use_faster_algorithm=True)
pipeline = Pipeline(steps=[(“s”, shap_selector), \
(“m”, LogisticRegression())]).fit(X_train, y_train)
</preformat>
  </sec>
  <sec id="S11">
    <label>5.</label>
    <title>Use cases and applications</title>
    <sec id="S12">
      <label>5.1.</label>
      <title>Use case 1: Application to UCI breast cancer dataset - comparison with related XAI work</title>
      <p id="P51">To demonstrate the value of the Zoish feature selector in a real use case from the biomedical domain, we applied it to the openly available breast cancer dataset from the UCI Archive,<sup><xref rid="R21" ref-type="bibr">21</xref></sup> and compared the results with a recent study evaluating different feature importance measures for the same dataset.<sup><xref rid="R22" ref-type="bibr">22</xref></sup></p>
      <p id="P52">The UCI dataset includes benign and malignant samples from 569 patients, 212 with cancer and 157 with fibrocystic breast masses. Each sample includes thirty features - ten real valued features for each cell nucleus (<italic toggle="yes">radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, fractal dimension</italic>) each reported as Mean, Standard Error (SE) and Worst.<sup><xref rid="R23" ref-type="bibr">23</xref></sup> As the classes in this dataset are almost linearly separable, classification per se is not a difficult task; however, the most important features generally differ depending on the technique used.<sup><xref rid="R24" ref-type="bibr">24</xref></sup> To further investigate this aspect, Saarela et al.<sup><xref rid="R22" ref-type="bibr">22</xref></sup> compared different feature importance measures using both linear (logistic regression) and non-linear (random forest) models and local interpretable model-agnostic explanations for the same dataset. In <xref rid="F2" ref-type="fig">Fig. 2</xref> we show the top 20 important features for the UCI Breast Cancer dataset computed with Zoish by training a XGboost classifier over ten folds of cross validation. The AUC for the trained classifier was 0.96, similar to the mean AUC reported in<sup><xref rid="R22" ref-type="bibr">22</xref></sup> (0.99+−002). Overall, the most important features in Zoish/XGboost agree well with the set of nine statistically significant features for both RF and LR reported in<sup><xref rid="R22" ref-type="bibr">22</xref></sup> – where, for each method, significance was computed through a procedure based on permutation tests – i.e. by shuffling class labels in the training data over hundreds of runs. Seven out of nine features deemed statistically significant in<sup><xref rid="R22" ref-type="bibr">22</xref></sup> were found in <xref rid="F2" ref-type="fig">Fig. 2</xref> (<italic toggle="yes">Mean concave points, Worst concave points, Worst Area, Worst Radius, Worst Perimeter, Mean Concavity, Mean Area</italic>), with five of the most significant features near the top of the list (<xref rid="T1" ref-type="table">Table 1</xref>). Only one feature was labeled as not significant by both RF and LR (<italic toggle="yes">Worst Compactness</italic>) and such feature has consistently a zero Shap value in Zoish/XGboost (not shown).</p>
      <p id="P53">It is worth noting that certain features which rank very high in Zoish/XGboost (<italic toggle="yes">Worst concavity, Worst texture, Compactness SE</italic>) appeared not to be significant in RF classification, hence not reported in the set of nine common, statistically significant features for breast cancer classification in the UCI dataset. Why did Zoish / XGboost select them up then? A very interesting hint comes from the analysis of local importance measures for a specific set of observations in the UCI dataset. Again in,<sup><xref rid="R22" ref-type="bibr">22</xref></sup> LIME (local interpretable model-agnostic explanations,<sup><xref rid="R25" ref-type="bibr">25</xref></sup>) was used to estimate local importances for the four most interesting observations, (i.e. correctly classified as benign with highest probability, correctly classified as malignant with highest probability, misclassified as benign with highest probability, and misclassified as malignant with highest probability). Strikingly, the features ranking high in Zoish/XGboost but absent in RF were also important features in LIME, especially for the observations misclassified as benign (false negatives), which are critical for medical purposes (<xref rid="T1" ref-type="table">Table 1</xref>). The final recommendation in<sup><xref rid="R22" ref-type="bibr">22</xref></sup> was to combine several explanation techniques in order to provide more reliable and trustworthy results, but this advice can often be impractical. Conversely, The Zoish/XGboost feature selector appears to select relevant features both at the global and local level, adding more detailed explanations of feature importance (i.e., not just the magnitude but also the direction of change), while being fast and straightforward to use.</p>
    </sec>
    <sec id="S13">
      <label>5.2.</label>
      <title>Use case 2: Predict short-term PD progression status using the Montreal Cognitive Assessment (MoCA)</title>
      <p id="P54">Our model, Zoish, was put to another practical test where we aimed to predict short-term PD progression status using the Montreal Cognitive Assessment (MoCA) total scores for patients in baseline. MoCA was developed as a tool to screen patients who present with mild cognitive complaints and usually perform in the normal range on the MMSE (Mini-Mental State Examination).<sup><xref rid="R26" ref-type="bibr">26</xref></sup> For this prediction task, we utilized the AMP-PD dataset, which is a comprehensive collection of data from various sources, including clinical information, genetic data, imaging data, and other biomarkers from individuals with Parkinson’s disease. The dataset consists of eight cohorts, making it a large and harmonized resource. Access to the data was obtained under the AMP-PD Data Use Agreement, and the information was retrieved from the website: <ext-link xlink:href="https://amp-pd.org/" ext-link-type="uri">https://amp-pd.org/</ext-link>. Our prediction model incorporated several essential features from the datasets, such as ”family history,” genetic information (PRS), ”medical history,” ”smoking and alcohol history,” and demographic information of the participants from the eight cohorts. After fitting the model, we evaluated its performance using the coefficient of determination, commonly known as R-squared, and achieved an R-squared value of 23.6 percent on the test dataset. Additionally, we calculated the Mean Squared Error (MSE) of the model to be 2.49 for the test dataset. Furthermore, the Mean Absolute Error (MAE) was found to be 13.04. The MAE represents the average absolute difference between the predicted values and the actual total score values in the test dataset. List of selected features by Zoish can be seen in <xref rid="F3" ref-type="fig">Fig. 3</xref></p>
      <p id="P55">In order to draw a comparison with another prevalent feature selector, specifically, <bold>SelectFromModel</bold> from the sklearn library, we applied it to the same dataset under identical conditions. This application yielded a Mean Absolute Error (MAE) of 15.91, a Mean Squared Error (MSE) of 2.69, and an R-squared value of 0.12. The features selected by this approach are depicted in <xref rid="F4" ref-type="fig">Fig. 4</xref>.</p>
      <p id="P56">As observed, Zoish not only outperforms its counterparts in terms of prediction accuracy, but it also excels in the selection of meaningful features. Notably, the Polygenic Risk Scores (PRSs) selected by Zoish have demonstrated substantial relevance to the Montreal Cognitive Assessment (MoCA). A prime example among these is <italic toggle="yes">PGS001641</italic>, which is renowned for its strong correlation with the volume of white matter, normalised for head size. This particular PRS underscores the genetic predisposition towards the volume of white matter, a crucial neuroimaging measurement that relates directly to cognitive functions evaluated in the MoCA test. Therefore, the selection of this PRS by Zoish validates its capability in discerning features with profound implications for cognitive assessment.</p>
    </sec>
  </sec>
  <sec id="S14">
    <label>6.</label>
    <title>Evaluations and Performance Analysis</title>
    <p id="P57">To offer a comprehensive evaluation of Zoish, we performed an array of tests ranging from comparative analyses to hyperparameter optimization and scalability assessments.</p>
    <sec id="S15">
      <title>Comparative Analysis:</title>
      <p id="P58">We initiated our evaluation with a rigorous comparison involving 300 synthetic datasets tailored to mirror the complexities of healthcare data. These datasets span regression, binary classification, and multi-label classification tasks. Zoish was compared against six established feature selection techniques from Scikit-learn under identical conditions.Our findings suggested that Zoish surpassed other selectors in 77% of regression problems, while in multi-label classification and binary classification tasks, Zoish outperformed in 53% and 57% of the cases, respectively (refer to <xref rid="T2" ref-type="table">Table 2</xref>).</p>
    </sec>
    <sec id="S16">
      <title>Hyperparameter Optimization:</title>
      <p id="P59">While Zoish itself is powerful, coupling it with a hyperparameter optimization tool like Lohrasb significantly improves performance. We performed 100 runs comparing Zoish’s efficacy with and without Lohrasb, and found marked improvements when paired with Lohrasb (see <xref rid="F5" ref-type="fig">Fig. 5</xref>).</p>
    </sec>
    <sec id="S17">
      <title>Scalability:</title>
      <p id="P60">Our most recent update introduces a faster algorithm for Shapley value computation, making Zoish efficient on large datasets. In our trials, Zoish selected 500 features from a dataset with 10,000 samples in under 2 minutes on a machine with a 2.3 GHz Quad-Core Intel Core i7 processor and 32 GB RAM.</p>
      <p id="P61">All the code for our tests is available in the public repository, allowing for independent verification and further exploration of Zoish’s capabilities.</p>
    </sec>
  </sec>
  <sec id="S18">
    <label>7.</label>
    <title>Discussion and Limitations</title>
    <p id="P62">This paper introduces Zoish, a feature selection tool built on cooperative game theory principles.<sup><xref rid="R16" ref-type="bibr">16</xref></sup> Zoish has gained traction in the community, as evidenced by a significant number of downloads from pip-trends (<ext-link xlink:href="https://piptrends.com/package/zoish" ext-link-type="uri">https://piptrends.com/package/zoish</ext-link>). The tool specializes in optimizing predictive models, particularly in the healthcare sector, and leverages Shapley additive values for a comprehensive view of feature importance at both local and global scales.<sup><xref rid="R10" ref-type="bibr">10</xref></sup> Through its Nullity property, Zoish effectively minimizes model complexity by omitting features with negligible Shapley values, thereby retaining model performance.<sup><xref rid="R3" ref-type="bibr">3</xref></sup> The tool is further enriched by integration with the Lohrasb package, which aids in achieving optimal estimators and hyperparameter settings.<sup><xref rid="R7" ref-type="bibr">7</xref></sup></p>
    <p id="P63">While Zoish’s capabilities are robust, some limitations are noteworthy. Firstly, its computational efficiency may be compromised when dealing with exceptionally large datasets. Secondly, the Shapley values employed assume feature independence and local linearity—assumptions that may not be fully met in complex applications like healthcare. These limitations are partially mitigated by Zoish’s tree-based modeling approach, which is robust to feature correlation and can capture non-linear relationships.<sup><xref rid="R6" ref-type="bibr">6</xref></sup></p>
    <p id="P64">The flexibility and interpretability of Zoish make it a promising tool for future applications in other high-dimensional data fields, including finance and e-commerce. Additional functionalities could be incorporated to broaden its applicability further.</p>
    <p id="P65">Future work will focus on extending Zoish’s utility to various high-dimensional domains and incorporating more algorithms and tools for an even more robust feature selection process. We have conducted extensive tests on synthetic datasets mimicking real-world complexities in healthcare, which are detailed in <xref rid="S14" ref-type="sec">Section 6</xref>. These tests demonstrate Zoish’s reliability and adaptability, even under challenging conditions.</p>
  </sec>
</body>
<back>
  <ack id="S19">
    <title>Acknowledgments</title>
    <p id="P66">This work is supported by UL1RR025774 and R01HG010881. Also We would like to express our gratitude to Dua, D. and Graff, C.<sup><xref rid="R21" ref-type="bibr">21</xref></sup> for generously providing us with the Breast Cancer Data used in this study. Without their contribution, this research would not have been possible.</p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="R1">
      <label>1.</label>
      <mixed-citation publication-type="other"><name><surname>Johnstone</surname><given-names>IM</given-names></name> and <name><surname>Titterington</surname><given-names>DM</given-names></name>, <source>Statistical challenges of high-dimensional data</source> (<year>2009</year>).</mixed-citation>
    </ref>
    <ref id="R2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><name><surname>Muja</surname><given-names>M</given-names></name> and <name><surname>Lowe</surname><given-names>DG</given-names></name>, <article-title>Scalable nearest neighbor algorithms for high dimensional data</article-title>, <source>IEEE transactions on pattern analysis and machine intelligence</source>
<volume>36</volume>, <fpage>2227</fpage> (<year>2014</year>).<pub-id pub-id-type="pmid">26353063</pub-id>
</mixed-citation>
    </ref>
    <ref id="R3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><name><surname>Sadaei</surname><given-names>HJ</given-names></name>, <name><surname>Cordova-Palomera</surname><given-names>A</given-names></name>, <name><surname>Lee</surname><given-names>J</given-names></name>, <name><surname>Padmanabhan</surname><given-names>J</given-names></name>, <name><surname>Chen</surname><given-names>S-F</given-names></name>, <name><surname>Wineinger</surname><given-names>NE</given-names></name>, <name><surname>Dias</surname><given-names>R</given-names></name>, <name><surname>Prilutsky</surname><given-names>D</given-names></name>, <name><surname>Szalma</surname><given-names>S</given-names></name> and <name><surname>Torkamani</surname><given-names>A</given-names></name>, <article-title>Genetically-informed prediction of short-term parkinson’s disease progression</article-title>, <source>npj Parkinson’s Disease</source>
<volume>8</volume>, p. <fpage>143</fpage> (<year>2022</year>).</mixed-citation>
    </ref>
    <ref id="R4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><name><surname>Tang</surname><given-names>J</given-names></name>, <name><surname>Alelyani</surname><given-names>S</given-names></name> and <name><surname>Liu</surname><given-names>H</given-names></name>, <article-title>Feature selection for classification: A review</article-title>, <source>Data classification: Algorithms and applications</source>, p. <fpage>37</fpage> (<year>2014</year>).</mixed-citation>
    </ref>
    <ref id="R5">
      <label>5.</label>
      <mixed-citation publication-type="other"><name><surname>Hall</surname><given-names>MA</given-names></name>, <source>Correlation-based feature selection for machine learning</source> (<year>1999</year>).</mixed-citation>
    </ref>
    <ref id="R6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><name><surname>Fryer</surname><given-names>D</given-names></name>, <name><surname>Strümke</surname><given-names>I</given-names></name> and <name><surname>Nguyen</surname><given-names>H</given-names></name>, <article-title>Shapley values for feature selection: The good, the bad, and the axioms</article-title>, <source>Ieee Access</source>
<volume>9</volume>, <fpage>144352</fpage> (<year>2021</year>).</mixed-citation>
    </ref>
    <ref id="R7">
      <label>7.</label>
      <mixed-citation publication-type="other"><name><surname>Javedani Sadaei</surname><given-names>H</given-names></name> and <name><surname>Torkamani</surname><given-names>A</given-names></name>, <source>Lohrasb: A scalable estimator optimization tool compatible with scikit-learn APIs</source> (<month>April</month>
<year>2023</year>).</mixed-citation>
    </ref>
    <ref id="R8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><name><surname>Michalak</surname><given-names>TP</given-names></name>, <name><surname>Aadithya</surname><given-names>KV</given-names></name>, <name><surname>Szczepański</surname><given-names>PL</given-names></name>, <name><surname>Jennings</surname><given-names>NR</given-names></name> and <name><surname>Narayanam</surname><given-names>R</given-names></name>, <article-title>Efficient computation of the shapley value for game-theoretic network centrality</article-title>, <source>Journal of Artificial Intelligence Research</source>
<volume>46</volume>, <fpage>607</fpage> (<year>2013</year>).</mixed-citation>
    </ref>
    <ref id="R9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><name><surname>Shapley</surname><given-names>LS</given-names></name>, <article-title>A value for n-person games</article-title>, <source>Contributions to the Theory of Games</source><volume>2</volume>, <fpage>307</fpage> (<year>1953</year>).</mixed-citation>
    </ref>
    <ref id="R10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><name><surname>Lundberg</surname><given-names>SM</given-names></name> and <name><surname>Lee</surname><given-names>S-I</given-names></name>, <article-title>A unified approach to interpreting model predictions</article-title>, <source>Advances in neural information processing systems</source>
<volume>30</volume> (<year>2017</year>).</mixed-citation>
    </ref>
    <ref id="R11">
      <label>11.</label>
      <mixed-citation publication-type="book"><name><surname>Molnar</surname><given-names>C</given-names></name>, <source>Interpretable machine learning</source> (<publisher-name>Lulu.com</publisher-name>, <year>2020</year>).</mixed-citation>
    </ref>
    <ref id="R12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><name><surname>Verdinelli</surname><given-names>I</given-names></name> and <name><surname>Wasserman</surname><given-names>L</given-names></name>, <article-title>Feature importance: A closer look at shapley values and loco</article-title>, <source>arXiv preprint arXiv:2303.05981</source> (<year>2023</year>).</mixed-citation>
    </ref>
    <ref id="R13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><name><surname>Karczmarz</surname><given-names>A</given-names></name>, <name><surname>Mukherjee</surname><given-names>A</given-names></name>, <name><surname>Sankowski</surname><given-names>P</given-names></name> and <name><surname>Wygocki</surname><given-names>P</given-names></name>, <article-title>Improved feature importance computations for tree models: Shapley vs. banzhaf</article-title>, <source>arXiv preprint arXiv:2108.04126</source> (<year>2021</year>).</mixed-citation>
    </ref>
    <ref id="R14">
      <label>14.</label>
      <mixed-citation publication-type="webpage"><name><surname>Lundberg</surname><given-names>S</given-names></name>, <name><surname>Lee</surname><given-names>S-I</given-names></name><etal/>, <source>SHAP (SHapley Additive exPlanations)</source><comment><ext-link xlink:href="https://github.com/slundberg/shap" ext-link-type="uri">https://github.com/slundberg/shap</ext-link></comment>, (<year>2023</year>), <comment>Accessed</comment>: <date-in-citation>2023-09-25</date-in-citation>.</mixed-citation>
    </ref>
    <ref id="R15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><name><surname>Yang</surname><given-names>J</given-names></name>, <article-title>Fast treeshap: Accelerating shap value computation for trees</article-title>, <source>arXiv preprint arXiv:2109.09847</source> (<year>2021</year>).</mixed-citation>
    </ref>
    <ref id="R16">
      <label>16.</label>
      <mixed-citation publication-type="other"><name><surname>Javedani Sadaei</surname><given-names>H</given-names></name> and <name><surname>Torkamani</surname><given-names>A</given-names></name>, <source>Zoish: Automated feature selectoion tools (July 2023), If you use this software, please cite it as below</source>.</mixed-citation>
    </ref>
    <ref id="R17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><name><surname>Varma</surname><given-names>S</given-names></name> and <name><surname>Simon</surname><given-names>R</given-names></name>, <article-title>Parameter estimation using grid search with cross-validation</article-title>, <source>XGBoost: A Scalable Tree Boosting System</source>
<volume>12</volume>, p. <fpage>48</fpage> (<year>2012</year>).</mixed-citation>
    </ref>
    <ref id="R18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><name><surname>Bergstra</surname><given-names>J</given-names></name> and <name><surname>Bengio</surname><given-names>Y</given-names></name>, <article-title>Random search for hyper-parameter optimization</article-title>, <source>Journal of Machine Learning Research</source>
<volume>13</volume>, <fpage>281</fpage> (<year>2012</year>).</mixed-citation>
    </ref>
    <ref id="R19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><name><surname>Liaw</surname><given-names>R</given-names></name>, <name><surname>Liang</surname><given-names>E</given-names></name>, <name><surname>Nishihara</surname><given-names>R</given-names></name>, <name><surname>Moritz</surname><given-names>P</given-names></name>, <name><surname>Gonzalez</surname><given-names>JE</given-names></name> and <name><surname>Stoica</surname><given-names>I</given-names></name>, <article-title>Tune: A research platform for distributed model selection and training in pytorch</article-title>, <source>arXiv preprint arXiv:1807.05118</source> (<year>2018</year>).</mixed-citation>
    </ref>
    <ref id="R20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><name><surname>Liaw</surname><given-names>R</given-names></name>, <name><surname>Liang</surname><given-names>E</given-names></name>, <name><surname>Nishihara</surname><given-names>R</given-names></name>, <name><surname>Moritz</surname><given-names>P</given-names></name>, <name><surname>Gonzalez</surname><given-names>JE</given-names></name> and <name><surname>Stoica</surname><given-names>I</given-names></name>, <article-title>Tune: A research platform for distributed model selection and training in pytorch</article-title>, <source>arXiv preprint arXiv:2002.02613</source> (<year>2020</year>).</mixed-citation>
    </ref>
    <ref id="R21">
      <label>21.</label>
      <mixed-citation publication-type="other"><name><surname>Dua</surname><given-names>D</given-names></name> and <name><surname>Graff</surname><given-names>C</given-names></name>, <source>UCI machine learning repository</source> (<year>2017</year>).</mixed-citation>
    </ref>
    <ref id="R22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><name><surname>Mirka Saarela</surname><given-names>SJ</given-names></name>, <article-title>Comparison of feature importance measures as explanations for classification models</article-title>, <source>SN Applied Sciences</source><volume>3</volume> (<year>2021</year>).</mixed-citation>
    </ref>
    <ref id="R23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><name><surname>WH Wolberg</surname><given-names>OM</given-names></name>, <article-title>WN Street, Machine learning techniques to diagnose breast cancer from image-processed nuclear features of fine needle aspirates</article-title>, <source>Cancer Letters</source><volume>77</volume>, p. <fpage>163</fpage>–<lpage>171</lpage> (<year>1994</year>).<pub-id pub-id-type="pmid">8168063</pub-id>
</mixed-citation>
    </ref>
    <ref id="R24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><name><surname>E Aličković</surname><given-names>AS</given-names></name>, <article-title>Breast cancer diagnosis using ga feature selection and rotation forest</article-title>, <source>Neural Computation Applications</source><volume>28</volume>, p. <fpage>753</fpage>–<lpage>763</lpage> (<year>2017</year>).</mixed-citation>
    </ref>
    <ref id="R25">
      <label>25.</label>
      <mixed-citation publication-type="other"><name><surname>Ribeiro</surname><given-names>MT</given-names></name>, <name><surname>Singh</surname><given-names>S</given-names></name> and <name><surname>Guestrin</surname><given-names>C</given-names></name>, <source>”why should i trust you?”: Explaining the predictions of any classifier</source> (<year>2016</year>).</mixed-citation>
    </ref>
    <ref id="R26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><name><surname>Nasreddine</surname><given-names>ZS</given-names></name>, <name><surname>Phillips</surname><given-names>NA</given-names></name>, <name><surname>Bédirian</surname><given-names>V</given-names></name>, <name><surname>Charbonneau</surname><given-names>S</given-names></name>, <name><surname>Whitehead</surname><given-names>V</given-names></name>, <name><surname>Collin</surname><given-names>I</given-names></name>, <name><surname>Cummings</surname><given-names>JL</given-names></name> and <name><surname>Chertkow</surname><given-names>H</given-names></name>, <article-title>The montreal cognitive assessment, moca: a brief screening tool for mild cognitive impairment</article-title>, <source>Journal of the American Geriatrics Society</source>
<volume>53</volume>, <fpage>695</fpage> (<year>2005</year>).<pub-id pub-id-type="pmid">15817019</pub-id>
</mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig position="float" id="F1">
    <label>Fig. 1.</label>
    <caption>
      <p id="P67">Zoish workflow</p>
    </caption>
    <graphic xlink:href="nihms-1952171-f0001" position="float"/>
  </fig>
  <fig position="float" id="F2">
    <label>Fig. 2.</label>
    <caption>
      <p id="P68">Shap summary plot of the top 20 important features for the UCI Breast Cancer dataset - computed with Zoish by training a XGboost classifier.</p>
    </caption>
    <graphic xlink:href="nihms-1952171-f0002" position="float"/>
  </fig>
  <fig position="float" id="F3">
    <label>Fig. 3.</label>
    <caption>
      <p id="P69">List of selected features and their importance by Zoish</p>
    </caption>
    <graphic xlink:href="nihms-1952171-f0003" position="float"/>
  </fig>
  <fig position="float" id="F4">
    <label>Fig. 4.</label>
    <caption>
      <p id="P70">List of selected features and their importance by SelectFromModel</p>
    </caption>
    <graphic xlink:href="nihms-1952171-f0004" position="float"/>
  </fig>
  <fig position="float" id="F5">
    <label>Fig. 5.</label>
    <caption>
      <p id="P71">The importance of Hyperparameter Optimization for Better Feature Selection</p>
    </caption>
    <graphic xlink:href="nihms-1952171-f0005" position="float"/>
  </fig>
  <table-wrap position="float" id="T1">
    <label>Table 1.</label>
    <caption>
      <p id="P72">Comparison of top features in the UCI Breast Cancer dataset</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Zoish features</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Significant features in LR and RF</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">LIME - Correctly classified benign (RF)</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">LIME - Misclassified benign (RF)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">concave points 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">concave points 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">perimeter 3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">concavity 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">perimeter 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">concave points 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">concave points 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">radius 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">radius 3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">concave points 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">texture 3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">texture 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">perimeter 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">texture 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">concavity 3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">compactness 2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">radius 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">concave points 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 2</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">radius 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">concavity 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">concavity 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">smoothness 3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">perimeter 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 1</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">perimeter 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">radius 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">texture 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">concave points 1</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">symmetry 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">concave points 2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">concavity 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">texture 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">texture 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">symmetry 2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">texture 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">smoothness 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">compactness 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">a</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">smoothness 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">radius 2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">concavity 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">perimeter 2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">radius 2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">compactness 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">fractal dimension 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">smoothness 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">compactness 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">symmetry 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">area 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">fractal dimension 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
      </tbody>
    </table>
    <table-wrap-foot>
      <fn id="TFN1">
        <p id="P73">This Table is about comparison of top features in the UCI Breast Cancer dataset computed by Zoish/XGboost vs. Random Forest / Logistic Regression / LIME. For all features, 1=Mean, 2=Standard Error, 3=Worst. Top 20 features in Column 2 are ranked based on permutation p-value for RF. In red are features found in LIME but not considered significant in RF/LR.</p>
      </fn>
    </table-wrap-foot>
  </table-wrap>
  <table-wrap position="float" id="T2">
    <label>Table 2.</label>
    <caption>
      <p id="P74">Performance comparison of Zoish with other feature selectors</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Selector</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Regression</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Binary Classification</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Multi-label Classification</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Zoish</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>77%</bold>
          </td>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>57%</bold>
          </td>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>53%</bold>
          </td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">VarianceThreshold</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">6%</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SelectKBest</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">6%</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SelectPercentile</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2%</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">RFE</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">10%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">6%</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">RFECV</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">7%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">10%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">11%</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SelectFromModel</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">15%</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">16%</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
