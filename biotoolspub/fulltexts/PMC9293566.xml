<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.0 20120330//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.0?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Healthc Eng</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Healthc Eng</journal-id>
    <journal-id journal-id-type="publisher-id">JHE</journal-id>
    <journal-title-group>
      <journal-title>Journal of Healthcare Engineering</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">2040-2295</issn>
    <issn pub-type="epub">2040-2309</issn>
    <publisher>
      <publisher-name>Hindawi</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9293566</article-id>
    <article-id pub-id-type="doi">10.1155/2022/9016401</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>TiM-Net: Transformer in M-Net for Retinal Vessel Segmentation</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-8375-0039</contrib-id>
        <name>
          <surname>Zhang</surname>
          <given-names>Hongbin</given-names>
        </name>
        <email>zhanghongbin@whu.edu.cn</email>
        <xref rid="I1" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-2418-4548</contrib-id>
        <name>
          <surname>Zhong</surname>
          <given-names>Xiang</given-names>
        </name>
        <xref rid="I1" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Zhijie</given-names>
        </name>
        <xref rid="I1" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Chen</surname>
          <given-names>Yanan</given-names>
        </name>
        <xref rid="I2" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhu</surname>
          <given-names>Zhiliang</given-names>
        </name>
        <xref rid="I1" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lv</surname>
          <given-names>Jingqin</given-names>
        </name>
        <xref rid="I1" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Chuanxiu</given-names>
        </name>
        <xref rid="I3" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhou</surname>
          <given-names>Ying</given-names>
        </name>
        <xref rid="I4" ref-type="aff">
          <sup>4</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-3068-2869</contrib-id>
        <name>
          <surname>Li</surname>
          <given-names>Guangli</given-names>
        </name>
        <xref rid="I3" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="I1"><sup>1</sup>School of Software, East China Jiaotong University, Nanchang, China</aff>
    <aff id="I2"><sup>2</sup>School of International, East China Jiaotong University, Nanchang, China</aff>
    <aff id="I3"><sup>3</sup>School of Information Engineering, East China Jiaotong University, Nanchang, China</aff>
    <aff id="I4"><sup>4</sup>Medical School, Nanchang University, Nanchang, China</aff>
    <author-notes>
      <fn fn-type="other">
        <p>Academic Editor: Rajesh Kaluri</p>
      </fn>
    </author-notes>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>11</day>
      <month>7</month>
      <year>2022</year>
    </pub-date>
    <volume>2022</volume>
    <elocation-id>9016401</elocation-id>
    <history>
      <date date-type="received">
        <day>30</day>
        <month>3</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>4</day>
        <month>6</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>21</day>
        <month>6</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2022 Hongbin Zhang et al.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>retinal image is a crucial window for the clinical observation of cardiovascular, cerebrovascular, or other correlated diseases. Retinal vessel segmentation is of great benefit to the clinical diagnosis. Recently, the convolutional neural network (CNN) has become a dominant method in the retinal vessel segmentation field, especially the U-shaped CNN models. However, the conventional encoder in CNN is vulnerable to noisy interference, and the long-rang relationship in fundus images has not been fully utilized. In this paper, we propose a novel model called Transformer in M-Net (TiM-Net) based on M-Net, diverse attention mechanisms, and weighted side output layers to efficaciously perform retinal vessel segmentation. First, to alleviate the effects of noise, a dual-attention mechanism based on channel and spatial is designed. Then the self-attention mechanism in Transformer is introduced into skip connection to re-encode features and model the long-range relationship explicitly. Finally, a weighted SideOut layer is proposed for better utilization of the features from each side layer. Extensive experiments are conducted on three public data sets to show the effectiveness and robustness of our TiM-Net compared with the state-of-the-art baselines. Both quantitative and qualitative results prove its clinical practicality. Moreover, variants of TiM-Net also achieve competitive performance, demonstrating its scalability and generalization ability. The code of our model is available at <ext-link xlink:href="https://github.com/ZX-ECJTU/TiM-Net" ext-link-type="uri">https://github.com/ZX-ECJTU/TiM-Net</ext-link>.</p>
    </abstract>
    <funding-group>
      <award-group>
        <funding-source xlink:href="http://dx.doi.org/10.13039/501100001809">National Natural Science Foundation of China</funding-source>
        <award-id>62161011</award-id>
        <award-id>61861016</award-id>
      </award-group>
      <award-group>
        <funding-source xlink:href="http://dx.doi.org/10.13039/501100010857">Jiangxi Provincial Department of Science and Technology</funding-source>
        <award-id>20212BAB202006</award-id>
        <award-id>20202BABL202044</award-id>
        <award-id>20192BBE50071</award-id>
        <award-id>20202BBEL53003</award-id>
      </award-group>
      <award-group>
        <funding-source xlink:href="http://dx.doi.org/10.13039/501100009102">Education Department of Jiangxi Province</funding-source>
        <award-id>GJJ190323</award-id>
        <award-id>GJJ200644</award-id>
      </award-group>
      <award-group>
        <funding-source>Humanity and Social Science Foundation of Jiangxi University</funding-source>
        <award-id>TQ20108</award-id>
        <award-id>TQ21203</award-id>
      </award-group>
    </funding-group>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <title>1. Introduction</title>
    <p>Artificial intelligence (AI) models have promoted the interactions between humans and computers greatly [<xref rid="B1" ref-type="bibr">1</xref>–<xref rid="B3" ref-type="bibr">3</xref>]. This phenomenon is more evident in the computer-aided diagnosis field. Recently, owing to the unhealthy living habits and growing pressure of life, the probability of people suffering from cardiovascular or cerebrovascular or other diseases has generally increased. From the medical perspective, the human eye is the only organ of the body that can directly observe the blood vessels and nerves. The retinal circulation has the same anatomical and physiological characteristics as the brain and coronary circulation. Hence, the retina of the human eyes has become an important window to diagnose cardiovascular, cerebrovascular, or other correlated diseases more efficiently. Traditionally, ophthalmologists make clinical diagnoses manually, which needs sufficient diagnostic experience and time. So the traditional diagnostic method is time-consuming and low efficient, which extends the corresponding diagnostic cycle with much financial and mental pressure on the patients. With the rapid development of AI technologies, more and more doctors began to use computer-aided diagnosis (CAD) methods to alleviate this problem. The realization of the CAD-based retinal vessel segmentation method helps the ophthalmologists more accurately and efficiently observe retinal diseases [<xref rid="B4" ref-type="bibr">4</xref>] and also allows the patients to receive higher quality treatments. Since 2012, deep learning methods, such as convolutional neural network (CNN) [<xref rid="B5" ref-type="bibr">5</xref>] and recurrent neural network, have greatly promoted the development of the computer vision (CV) field. More and more CV tasks using specific CNN structures can obtain state-of-the-art performance. Recently, fully connected networks [<xref rid="B6" ref-type="bibr">6</xref>], U-Net [<xref rid="B7" ref-type="bibr">7</xref>], and U-Net++ [<xref rid="B8" ref-type="bibr">8</xref>] have become the dominant methods in medical image segmentation. The U-Net and U-Net++ models, usually use a symmetric encoder-decoder framework with skip connections to enhance the quality of detail retention. U-Net is simple, but it builds a firm foundation for the subsequent correlated research. Hence, many methods based on U-shaped networks were proposed to complete medical image segmentation, and they achieved great success in numerous tasks, such as retinal vessel segmentation [<xref rid="B9" ref-type="bibr">9</xref>–<xref rid="B12" ref-type="bibr">12</xref>], heart segmentation [<xref rid="B13" ref-type="bibr">13</xref>], and organ segmentation [<xref rid="B10" ref-type="bibr">10</xref>, <xref rid="B14" ref-type="bibr">14</xref>].</p>
    <p>Recently, deep learning models have played a very important role in retinal vessel segmentation [<xref rid="B15" ref-type="bibr">15</xref>, <xref rid="B16" ref-type="bibr">16</xref>] owing to their high practicality. Fu et al. [<xref rid="B17" ref-type="bibr">17</xref>] added a multiscale input layer to U-Net as well as a side output layer. However, feature filtering was not implemented in the skip connections of the M-Net model, and each side output layer uses the same weight. Guo et al. [<xref rid="B18" ref-type="bibr">18</xref>] placed the spatial attention module behind the encoder to extract significant features. Only using spatial attention loses the key information across different feature channels. Fu et al. [<xref rid="B19" ref-type="bibr">19</xref>] used parallel channel and spatial attention to suppress the negative influence of noisy features. Zhang et al. [<xref rid="B20" ref-type="bibr">20</xref>] absorbed a gate attention mechanism into the skip connection for filtering noisy information. Wang et al. [<xref rid="B21" ref-type="bibr">21</xref>] designed a hard attention network (HA-Net) consisting of three decoders for retinal vessel segmentation. Li et al. [<xref rid="B22" ref-type="bibr">22</xref>] adopted the weight-sharing and skip-connection features to facilitate training. The pyramid U-Net [<xref rid="B23" ref-type="bibr">23</xref>] acquires aggregated features at higher, current, and lower levels in its encoder and decoder. Recently, owing to the great success of Transformer in the CV field, TransUNet [<xref rid="B24" ref-type="bibr">24</xref>] and TransFuse [<xref rid="B25" ref-type="bibr">25</xref>] have been proposed by combining Transformer and U-Net. Similarly, Chen proposed the patches convolution attention-based Transformer U-Net (PCAT-UNet) [<xref rid="B26" ref-type="bibr">26</xref>] model that inserts a modified Transformer module into U-Net. Although better performance can be observed, these Transformer-based models are complex and time-consuming, which will affect their practicalities to some degree.</p>
    <p>Based on the above analysis, we found the following difficulties of previous work: (1) it is difficult to obtain the best performance on each evaluation metric; (2) it is difficult to combine the Transformer module and U-Net model owing to the high complexity; (3) feature maps are prone to noise interference; (4) it is difficult to effectively model the long-range relationships in the fundus images; and (5) the output layers only use one single layer, which did not exploit the utility of other layers.</p>
    <p>This study focuses on the (3), (4), and (5) problems. We first design a novel dual-attention mechanism and then apply it to our model. The dual-attention mechanism can effectively alleviate the interference of noisy information. Second, we explicitly model the long-rang relationship in the fundus images by using a pure Transformer module. Both the proposed dual-attention mechanism and Transformer module are plug-and-play, making our model simple and easy to implement. Finally, we assign a suitable weight to each side output layer based on its real importance. We are striving to make full use of the complementarity of multiple output layers. Conceptually and empirically, the main contributions of this paper can be summarized as follows:<list list-type="order"><list-item><p>We propose a novel model called Transformer in M-Net (TiM-Net), which is simple but effective for retinal vessel segmentation. TiM-Net takes multiscale input, feature refinement strategies, and long-range relationship into account, which can strengthen the discriminative abilities of image features. TiM-Net achieves satisfactory segmentation results, which provides firm technical support for clinical human-computer interaction diagnosis.</p></list-item><list-item><p>Extensive experiments were conducted on three public benchmark data sets. The corresponding results demonstrate the superior segmentation performance of TiM-Net over other state-of-the-art methods. The code of our model is available at <ext-link xlink:href="https://github.com/ZX-ECJTU/TiM-Net" ext-link-type="uri">https://github.com/ZX-ECJTU/TiM-Net</ext-link>.</p></list-item><list-item><p>Owing to a relatively flexible structure, TiM-Net has several model variants. These model variants also obtain competitive segmentation performance, demonstrating the powerful scalability and generalization ability of TiM-Net.</p></list-item><list-item><p>We complete both coarse- and fine-grained ablation analysis to evaluate the real contribution of each module in TiM-Net, which provides a new idea for evaluating the segmentation model comprehensively.</p></list-item></list></p>
    <p>The remainder of this paper is organized as follows: <xref rid="sec2" ref-type="sec">Section 2</xref> presents related work and our research motivations. TiM-Net is described in <xref rid="sec3" ref-type="sec">Section 3</xref>. Experiments on three well-known retinal image data sets and the corresponding results are discussed in <xref rid="sec4" ref-type="sec">Section 4</xref>. Finally, <xref rid="sec5" ref-type="sec">Section 5</xref> provides the conclusions and our future work.</p>
  </sec>
  <sec id="sec2">
    <title>2. Related Work</title>
    <sec id="sec2.1">
      <title>2.1. Medical Image Segmentation</title>
      <p>In the traditional U-shaped segmentation models, the encoders usually employ two methods, including superimposed convolutional layers and continuous down-sampling, to generate a sufficiently large receptive field, thus improving the efficiency of global context modeling. However, these methods bring the following drawbacks: (1) The features extracted from the encoders contain many noises, which affect the final segmentation performance, and (2) their models using too many parameters are prone to overfitting when the corresponding medical image data set is relatively small. To address these problems, some researchers used additional expansion paths to better extract both coarse- and fine-grained features for segmentation. For example, Zhang et al. [<xref rid="B27" ref-type="bibr">27</xref>] introduced three different dense connections in multiscale densely connected U-Net to combine the features from different scales. Feature fusion was carried out, in turn, to strengthen the discriminative ability of the features and reduce the risk of overfitting. Chen et al. [<xref rid="B28" ref-type="bibr">28</xref>] proposed a bridging method to connect two U-Net structures, which can make full use of the features extracted from the two networks. Devi et al. [<xref rid="B29" ref-type="bibr">29</xref>] embedded a multiscale dilated convolution module in the decoders to fuse multiscale features for automatic instrument segmentation. In summary, the U-shaped model is the mainstream model in the medical image segmentation field.</p>
    </sec>
    <sec id="sec2.2">
      <title>2.2. Attention Mechanism</title>
      <p>Owing to noisy interference, some important edge information is ignored by the segmentation model. And the corresponding performance is unsatisfactory, especially for retinal vessel segmentation. To address this problem, more and more researchers added the well-known attention mechanism [<xref rid="B30" ref-type="bibr">30</xref>] to U-Net. They want to capture the most correlated features for effective medical image segmentation. Li et al. [<xref rid="B31" ref-type="bibr">31</xref>] inserted a gate attention mechanism to the skip connection of U-Net. It focuses on the position of the encoded features in the target area. Unlike the single attention mechanism, the dual-attention mechanism [<xref rid="B19" ref-type="bibr">19</xref>, <xref rid="B32" ref-type="bibr">32</xref>, <xref rid="B33" ref-type="bibr">33</xref>] has been proposed to choose the most significant channel features and suppress irrelevant spatial features. Wang et al. [<xref rid="B34" ref-type="bibr">34</xref>] used the dual-attention mechanism combined with residual connection in the encoder and decoder structures. Experiments demonstrate that the combination of channel and spatial attention outperforms a single attention mechanism. Fu et al. [<xref rid="B17" ref-type="bibr">17</xref>] used parallel channel and spatial attention to suppress the negative influence of noisy features. Amer et al. [<xref rid="B35" ref-type="bibr">35</xref>] proposed a multiscale spatial attention module in which the spatial attention graph is derived from a hybrid hierarchical dilated convolution module. This module can capture multiscale context information for lung image segmentation. Summarily, extensive experiments have validated the effectiveness of the attention mechanism in medical image segmentation.</p>
      <p>The Transformer uses another kind of attention mechanism. It has pioneered new technologies in the fields of machine translation [<xref rid="B36" ref-type="bibr">36</xref>] and natural language processing [<xref rid="B37" ref-type="bibr">37</xref>]. Evident performance improvement can be observed on numerous tasks. Notably, lots of studies have demonstrated that Transformer is also suitable for CV tasks. Dosovitskiy et al. [<xref rid="B38" ref-type="bibr">38</xref>] implemented the well-known vision Transformer (ViT), directly applying the Transformer and the global self-attention mechanism to classify full-size images. Ye et al. [<xref rid="B39" ref-type="bibr">39</xref>] proposed a cross-modal self-attention mechanism that incorporates image and text features for query, key, and value. Yang et al. [<xref rid="B40" ref-type="bibr">40</xref>] proposed a cross-scale feature integration module that learns more powerful feature representations by stacking multiple texture Transformers. Liu et al. [<xref rid="B41" ref-type="bibr">41</xref>] proposed a hierarchical Transformer that limited self-attention computing to nonoverlapping local windows while allowing the cross-window connection.</p>
      <p>Recently, some researchers began to introduce Transformer into the medical image segmentation field and obtained satisfactory performance. The Transformer converts each image into a one-dimensional sequence and focuses on modeling the global context. Chen et al. [<xref rid="B24" ref-type="bibr">24</xref>] proposed the TransUNet model that replaces the encoder of the U-Net model with a Transformer structure. Zhang et al. [<xref rid="B25" ref-type="bibr">25</xref>] designed the TransFuse method that fuses Transformer with CNN. The two models obtained evident performance improvement in medical image segmentation. However, the two models are very complex.</p>
    </sec>
    <sec id="sec2.3">
      <title>2.3. Retinal Image Segmentation</title>
      <p>This study focuses on retinal vessel segmentation. For this task, Fu et al. [<xref rid="B17" ref-type="bibr">17</xref>] added a multiscale input layer into U-Net as well as a side output layer, which solves the segmentation problem of the optic disc and optic cup. Wang et al. [<xref rid="B42" ref-type="bibr">42</xref>] proposed a double-coded U-Net model and placed the channel attention on the skip connection to choose effective features. Ma et al. [<xref rid="B43" ref-type="bibr">43</xref>] proposed a multitask CNN with a spatial activation mechanism, which can simultaneously segment retinal blood vessels, arteries, and veins. Guo et al. [<xref rid="B18" ref-type="bibr">18</xref>] put a spatial attention module at the bottom-most layer of an encoder for adaptive feature refinement. This attention module can suppress the uncorrelated features to some degree. Zhang et al. [<xref rid="B20" ref-type="bibr">20</xref>] absorbed a gate attention mechanism to the skip connection. Wang et al. [<xref rid="B21" ref-type="bibr">21</xref>] designed the HA-Net model consisting of three decoders. The first decoder can dynamically analyze the “hard” and “easy” regions of the image, while the other two decoders are responsible for distinguishing the “hard” and “easy” regions of the retinal blood vessels. Tong et al. [<xref rid="B44" ref-type="bibr">44</xref>] proposed a side attention network that integrated side-attention and dense atrous convolutional blocks, preserving more features of the encoder and contextual information of the fundus image, respectively. Li et al. [<xref rid="B22" ref-type="bibr">22</xref>] adopted the weight-sharing and skip-connection features to facilitate training. Jiang et al. [<xref rid="B45" ref-type="bibr">45</xref>] used both multiscale dilated convolution and skip connection to reduce the loss of feature information. Zhai et al. [<xref rid="B46" ref-type="bibr">46</xref>] used multiple pyramid pooling modules to combine more contextual information in the decoding process. Zhang et al. [<xref rid="B47" ref-type="bibr">47</xref>] proposed a structure-texture demixing network for separating structure and texture components, which can better handle structure and texture in different ways. Cao [<xref rid="B48" ref-type="bibr">48</xref>] proposed a pure Transformer network to classify and segment images with great success. Chen et al. [<xref rid="B26" ref-type="bibr">26</xref>] proposed the PCAT-UNet model that absorbs a modified Transformer module into U-Net. However, due to the lack of the ability to capture the long-range relationship, noisy features are obtained after multiple convolutions, which affects the final performance.</p>
    </sec>
    <sec id="sec2.4">
      <title>2.4. Motivations</title>
      <p>Reviewing the work of [<xref rid="B17" ref-type="bibr">17</xref>, <xref rid="B18" ref-type="bibr">18</xref>, <xref rid="B24" ref-type="bibr">24</xref>–<xref rid="B26" ref-type="bibr">26</xref>], we found the following problems: (1) most studies use complex structures, which may lower their practicalities; (2) traditional encoders cannot model long-range relationships and are prone to noisy interference; and (3) the side output layer only uses a single-layer output, which cannot make full use of the complementarity of different layers. The complementarity helps recover the feature maps well.</p>
      <p>Hence, our motivations are threefold. First, to lower complexity, we consider a plug-and-play approach that only adds a Transformer to skip connection. Second, unlike the M-Net model, we incorporate diverse attention mechanisms, including the self-attention of Transformer and dual-attention mechanism, into two different positions of our model. On the one hand, we absorb the Transformer module into the skip connection to re-encode the image features extracted from the encoder. This can refine the encoded features to a certain degree. More importantly, this helps explicitly model the long-range relationship in the fundus images. On the other hand, we propose the dual-attention mechanism including spatial attention and channel attention to reduce the negative effect of the noisy features. Lastly, we make full use of each side layer through a suitable weight assignment strategy. All these modifications are easy to implement and cannot increase the complexity of the segmentation model, which also contributes to promoting the practicality of TiM-Net.</p>
    </sec>
  </sec>
  <sec id="sec3">
    <title>3. Method</title>
    <p>Problem definition: Our goal is to predict the corresponding label map with the size of <italic>H</italic> × <italic>W</italic> × <italic>C</italic> of an arbitrary retinal image. <italic>H</italic> is the height of the image. <italic>W</italic> is the width of the image. <italic>C</italic> is the corresponding channel number. Our model is illustrated in <xref rid="fig1" ref-type="fig">Figure 1</xref>.</p>
    <p>First, TiM-Net uses multiscale images as its input. This can leverage the multiscale information for retinal vessel segmentation. Second, TiM-Net incorporates the Transformer module into its skip connection. The built-in self-attention mechanism of the Transformer models the long-range relationship in the fundus images and makes effective feature refinement. This builds a firm foundation for the subsequent upsampling. Third, the dual-attention module including spatial and channel attention is placed behind the last encoder layer to prevent gradient degradation and make another kind of feature refinement. Finally, we make full use of each side layer to complete the final segmentation. We introduce each component as follows.</p>
    <sec id="sec3.1">
      <title>3.1. Transformer in Skip Connection</title>
      <p>The traditional attention mechanism uses different input sources and output targets, which has a certain negative influence on feature decoding. Moreover, it cannot model the long-range relationship in the fundus images. As we know, the Transformer employs the self-attention mechanism, which has the same target and source. More importantly, this self-attention mechanism can better model the long-range relationship across a whole image. Hence, we absorb the Transformer module into the skip connection at a suitable position. The corresponding structure of the Transformer is shown in <xref rid="fig2" ref-type="fig">Figure 2</xref>.</p>
      <p>In <xref rid="fig1" ref-type="fig">Figure 1</xref>, the second layer features extracted from the encoder are input into the Transformer to implement self-attention computing. The corresponding results are transferred into the decoder. Hence, we absorb the Transformer module into the skip connection. The Transformer divides the input feature maps (256 × 256) into 16 patches, namely <italic>p</italic><sup><italic>i</italic></sup>, averagely, and each patch size is <italic>P</italic> × <italic>P</italic>. Then these patches are serialized and passed into the embedding layer to obtain the original embedding sequence. They are linearly projected to a <italic>D</italic>-dimensional embedding space in turn.</p>
      <p>To learn specific spatial information about these patches, position embeddings are first added to the patches to preserve position information. Then the built-in self-attention mechanism in the Transformer module calculates the correlation between each patch pair. Finally, the spatial correlation of given patches is obtained through a multilayer perceptron (MLP) layer. Position embeddings are used as follows:<disp-formula id="EEq1"><label>(1)</label><mml:math id="M1" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="bold">X</mml:mi><mml:mo>;</mml:mo><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="bold">X</mml:mi><mml:mo>;</mml:mo><mml:mo>…</mml:mo><mml:mo>;</mml:mo><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>16</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mtext>pos</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>X</italic> ∈ <italic>ℜ</italic><sup>(<italic>P</italic> × <italic>P</italic> × <italic>C</italic>)×<italic>D</italic></sup> denotes the matrix that implements the corresponding linear projection as illustrated in <xref rid="fig2" ref-type="fig">Figure 2(a)</xref>. <italic>p</italic><sup><italic>i</italic></sup><italic>X</italic>(i ∈ {1,&amp;,16}) denotes a linear projection result of <italic>p</italic><sup><italic>i</italic></sup>. <bold>X</bold><sub>pos</sub> represents the corresponding position embedding of the given patches, so the position information of each patch is reserved by marking the original position serial number of <italic>p</italic><sup><italic>i</italic></sup>. This contributes to learning global long-range relationships in the fundus images. According to equation (<xref rid="EEq1" ref-type="disp-formula">1</xref>), the Transformer makes a linear projection of <italic>p</italic><sup><italic>i</italic></sup> and forms a <italic>D</italic>-dimensional <italic>z</italic><sub>0</sub> together with position embedding. This can also be regarded as a kind of preprocessing step for the subsequent self-attention computing. It builds a foundation for capturing long-range relationships in the fundus images.</p>
      <p>The projections are input into the encoder layer of the Transformer, which contains <italic>n</italic> layers of multihead self-attention (MSA) and MLP. The detailed structure of the encoder layer is illustrated in <xref rid="fig2" ref-type="fig">Figure 2(b)</xref>. Therefore, the output of the <italic>n</italic>-th layer can be calculated as follows:<disp-formula id="EEq2"><label>(2)</label><mml:math id="M2" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mi>z</mml:mi><mml:mi>n</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mtext>MSA</mml:mtext><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>L</mml:mi><mml:mi>N</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="EEq3"><label>(3)</label><mml:math id="EEq3EAAAIACCCA" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext>MLP</mml:mtext><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>L</mml:mi><mml:mi>N</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msubsup><mml:mi>z</mml:mi><mml:mi>n</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>n</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>z</italic><sub><italic>n</italic></sub> denotes the encoded image sequence and <italic>LN</italic> denotes the normalization layer. Finally, the output feature is reshaped to its original size. Owing to MSA and MLP, the Transformer can model the long-range relationship well and further refine the extracted features. Summarily, equation (<xref rid="EEq3" ref-type="disp-formula">3</xref>) makes the MLP projection of the results of self-attention computing and generates the refined image features for the subsequent upsampling operations.</p>
    </sec>
    <sec id="sec3.2">
      <title>3.2. Dual-Attention Mechanism in Encoder</title>
      <p>Deep learning features acquired through multiple convolutions inevitably mix numerous noisy features. Meanwhile, gradient degradation usually occurs when the segmentation model is too deep. Hence, we need to make feature refinement and strengthen the feature propagation procedure. To resolve the two problems, we place the dual-attention mechanism behind the encoder layer to suppress the noises and promote model optimization. The proposed dual-attention module is shown in <xref rid="fig3" ref-type="fig">Figure 3</xref>.</p>
      <p>As shown in <xref rid="fig3" ref-type="fig">Figure 3</xref>, channel attention focuses on “what” is meaningful in the input image, whereas spatial attention focuses on “where” is the most informative region. The two attentions complement each other. Experiments in [<xref rid="B33" ref-type="bibr">33</xref>] have demonstrated that sequential channel attention and spatial attention are effective. Unlike [<xref rid="B33" ref-type="bibr">33</xref>], we add the residual links after the convolution layer rather than before. This strategy has two evident advantages: First, it ensures the same number of channels. Second, it makes the whole procedure more efficient. Hence, each kind of attention mechanism captures the most important features from its perspective. And they complement each other to make more effective feature refinement.</p>
      <p>Here, we give the formal description of the dual-attention mechanism. It first obtains the feature map <italic>F</italic> ∈ <italic>ℜ</italic><sup><italic>C</italic>×<italic>H</italic>×<italic>W</italic></sup> through a 1 × 1 convolution and then gets the channel attention feature map <italic>F</italic><sub><italic>c</italic></sub> ∈ <italic>ℜ</italic><sup><italic>C</italic>×1×1</sup>. After <italic>F</italic><sub><italic>c</italic></sub> multiplying <italic>F</italic>, the spatial attention feature map <italic>F</italic><sub><italic>s</italic></sub> ∈ <italic>ℜ</italic><sup>1×<italic>H</italic>×<italic>W</italic></sup> is acquired. Finally, we get the last feature map <italic>F</italic>′ ∈ <italic>ℜ</italic><sup><italic>C</italic>×<italic>H</italic>×<italic>W</italic></sup>. It represents the final output of the dual-attention mechanism. The detailed equation of the dual-attention mechanism is shown as follows:<disp-formula id="EEq4"><label>(4)</label><mml:math id="M3" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi>F</mml:mi><mml:msup><mml:mrow/><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>F</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
      <p>Equation (<xref rid="EEq4" ref-type="disp-formula">4</xref>) represents the output of the dual-attention mechanism, which can suppress the noisy information in the encoded features. The following two subsections present channel attention and spatial attention, respectively.</p>
      <sec id="sec3.2.1">
        <title>3.2.1. Channel Attention</title>
        <p>In this subsection, we mine the relationship between different channels to obtain channel attention. We intend to find the channels that contain more valuable information for retinal vessel segmentation. Hence, the channel attention mechanism can retain the key information to the most extent. The channel attention employs the global average pooling and global max-pooling layers to squeeze the feature maps in the spatial dimension. The global average pooling layer captures the overall information of image features, whereas the global max-pooling layer obtains the difference information of these features. <xref rid="fig4" ref-type="fig"> Figure 4</xref> illustrates the core idea of channel attention.</p>
        <p>As shown in <xref rid="fig4" ref-type="fig">Figure 4</xref>, we implement global max-pooling (Maxpool) and global average pooling (Avgpool) on the input feature map <italic>F</italic> ∈ <italic>ℜ</italic><sup><italic>C</italic>×<italic>H</italic>×<italic>W</italic></sup>, respectively. Two feature maps, namely <italic>F</italic><sub>avg</sub><sup><italic>c</italic></sup> and <italic>F</italic><sub>max</sub><sup><italic>c</italic></sup>, are obtained. The two feature maps are input into a two-layer MLP. The neuron number of the first layer in the MLP is <italic>C</italic>/<italic>r</italic>, where <italic>r</italic> is the decay rate. The neuron number of the second layer in the MLP is <italic>C</italic>. The MLP model uses RELU as its activation function. Finally, the element-wise summation is implemented based on the two outputs of the MLP model, namely <italic>F</italic><sub>1</sub> and <italic>F</italic><sub>2</sub>. Sigmoid is chosen to generate the final channel attention feature maps <italic>F</italic><sub><italic>c</italic></sub>. Hence, the whole procedure of the channel attention can be formulated as follows:<disp-formula id="EEq5"><label>(5)</label><mml:math id="M4" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mtext>MLP</mml:mtext><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mtext>Avgpool</mml:mtext><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mtext>MLP</mml:mtext><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mtext>Maxpool</mml:mtext><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mtext>avg</mml:mtext></mml:mrow><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>F</italic><sub>avg</sub><sup><italic>c</italic></sup> and <italic>F</italic><sub>max</sub><sup><italic>c</italic></sup> represent the output of the two pooling layers, respectively. <italic>σ</italic> is the Sigmoid activation function. <italic>W</italic><sub>0</sub> ∈ <italic>ℜ</italic><sup><italic>C</italic>/<italic>r</italic>×<italic>C</italic></sup> and <italic>W</italic><sub>1</sub> ∈ <italic>ℜ</italic><sup><italic>C</italic>×<italic>C</italic>/<italic>r</italic></sup> represent the corresponding weights of the MLP model, which are shared for each output of the pooling layer. <italic>F</italic><sub><italic>c</italic></sub> represents the acquired channel attention feature map, which mainly depicts the valuable information in different feature channels.</p>
      </sec>
      <sec id="sec3.2.2">
        <title>3.2.2. Spatial Attention</title>
        <p>As described above, the channel attention module focuses on capturing the key information among different channels. As illustrated in <xref rid="fig5" ref-type="fig">Figure 5</xref>, unlike the channel attention module, spatial attention emphasizes the key segmentation information hidden in the spatial dimension more.</p>
        <p>We implement global max-pooling and global average pooling on the input feature maps <italic>F</italic>′ generated by the channel attention module. Two feature maps, namely <italic>F</italic><sub>avg</sub><sup><italic>s</italic></sup> and <italic>F</italic><sub>max</sub><sup><italic>s</italic></sup>, are obtained in turn. They pay more attention to the local key regions in the fundus images. We concatenate the two feature maps and implement a 7 × 7 convolutional operation named <italic>f</italic><sup>7×7</sup>, where the padding is 3. The sigmoid function is chosen to generate the final spatial attention feature maps. Hence, the whole procedure of the spatial attention can be formulated as follows:<disp-formula id="EEq6"><label>(6)</label><mml:math id="M5" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn><mml:mo>×</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msup><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtext>Avgpool</mml:mtext><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>F</mml:mi><mml:msup><mml:mrow/><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtext>Maxpool</mml:mtext><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:msup><mml:mrow/><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn><mml:mo>×</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msup><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>;</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>F</italic><sub>avg</sub><sup><italic>s</italic></sup> and <italic>F</italic><sub>max</sub><sup><italic>s</italic></sup> represent the output of the two pooling layers. <italic>σ</italic> is the Sigmoid activation function. <italic>F</italic><sub><italic>s</italic></sub> represents the feature map generated by spatial attention, which mainly highlights the key spatial information hidden in feature maps for blood vessel segmentation.</p>
      </sec>
    </sec>
    <sec id="sec3.3">
      <title>3.3. TiM-Net</title>
      <p>TiM-Net derives from M-Net [<xref rid="B17" ref-type="bibr">17</xref>]. Hence, it consists of the M-Net architecture, a new encoder combined with the dual-attention mechanism (left side), the Transformer-based skip connection that transfers the refined features to the decoder, and a new decoder combined with a group of weighted side output layers. Please refer to <xref rid="fig1" ref-type="fig">Figure 1</xref> to get the detailed structure of TiM-Net.</p>
      <p>In our encoder, we first use max-pooling to downsample the retinal vessel images and construct multiscale inputs for encoding. This strategy has two advantages: (1) multiscale images offer more sufficient information to depict vessel details and (2) it avoids the large growth of parameters and makes TiM-Net prone to reproduce. Then we place the dual-attention mechanism behind the encoder to suppress noisy information. The dual-attention mechanism is made up of the channel and spatial attention modules. They complement each other and adaptively reassign suitable weights to the corresponding encoded features.</p>
      <p>Unlike TransUNet and TransFuse, we need not modify the whole encoder. We only absorb the Transformer module to the skip connection. We intend to re-encode feature maps and capture the long-range relationship in the fundus images. Multiple image blocks are extracted and input into the modified skip connection to complete feature re-encoding. We make full use of the MSA mechanism of the Transformer module to further re-encode these feature maps. Owing to the self-attention characteristic, the long-range relationship among diverse feature patches is mined out. It is a significant complementarity to the local region-based convolutional information.</p>
      <p>In our decoder, we use four side layers to construct different outputs. Each side layer depicts the segmented results from its perspective. They complement each other. To directly utilize the predicted maps of each side layer, we combine the loss <italic>L</italic><sub><italic>i</italic></sub> of each side layer and create the final loss <italic>L</italic> as shown in (<xref rid="EEq7" ref-type="disp-formula">7</xref>). Each side layer is weighted by <italic>α</italic><sub><italic>i</italic></sub>(<italic>i</italic>=1,2,3,4), respectively. We tune these weights carefully (please refer to <xref rid="tab7" ref-type="table">Table 7</xref>). This can backpropagate the loss of each side layer and the final loss to the earlier layers of the decoder, which helps alleviate the gradient degradation problem. Moreover, we take full advantage of each side layer to obtain better segmentation results. The output loss function <italic>L</italic> is defined as follows:<disp-formula id="EEq7"><label>(7)</label><mml:math id="M6" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi>L</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo stretchy="true">∑</mml:mo><mml:mi>i</mml:mi><mml:mi>M</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>M</italic> is the output number. <italic>L</italic><sub><italic>i</italic></sub> is the loss of the <italic>i</italic>-th side output layer. Accordingly, <italic>v</italic><sup>(<italic>i</italic>)</sup> denotes the weight of the <italic>i</italic>-th side output layer. <italic>V</italic> represents the parameters of all the standard convolutional layers.</p>
    </sec>
  </sec>
  <sec id="sec4">
    <title>4. Experiments</title>
    <sec id="sec4.1">
      <title>4.1. Data Sets and Evaluation Metrics</title>
      <p>In this section, extensive experiments are conducted to verify the effectiveness and generalization ability of TiM-Net on three public data sets, including STARE [<xref rid="B49" ref-type="bibr">49</xref>], CHASEDB1 [<xref rid="B50" ref-type="bibr">50</xref>], and DRIVE [<xref rid="B51" ref-type="bibr">51</xref>].<list list-type="order"><list-item><p>STARE: It is a color image data set used for retinal vessel segmentation, which includes 20 retinal images. Ten images of this data set are diseased, whereas another 10 images have no disease. The image resolution is 605 × 700. We randomly select 14 images for training and other 6 images for evaluation. From the perspective of disease distribution, STARE is a balanced data set, which indicates that it is relatively easier to train the corresponding segmentation model.</p></list-item><list-item><p>CHASEDB1: It is a 999 × 960 image data set containing 28 retinal images of the central nervous vascular reflex. No image contains disease. We use 20 images for training and other 8 images for evaluation. Unlike the other two data sets, the corresponding image size of CHASEDB1 is larger, which indicates that we need to capture sufficient long-range relationships for better segmentation.</p></list-item><list-item><p>DRIVE: It includes 40 images. Seven images in this data set are early diabetic retinopathy, whereas another 33 samples are the fundus images without diabetic retinopathy. The resolution of each image is 565 × 584. We divide the training set and test set into 1:1. Unlike the above two data sets, it is an imbalanced data set, which means a relatively more challenging segmentation task. But it is closer to clinical conditions.</p></list-item></list></p>
      <p>According to the above presentation, all the data sets cover diverse diseases, data distributions, and image sizes. This setting has two advantages: (1) this can firmly validate the effectiveness and robustness of our segmentation model and (2) this can objectively mimic the real clinical diagnosis procedure to some degree.</p>
      <p>Similar to most methods of retinal image segmentation, we use the accuracy (Acc), sensitivity (Se), specificity (Sp), and area under ROC (AUC) metrics to evaluate each segmentation model. Acc is used to evaluate the overall segmentation performance of the model. Larger Acc means that both objects (vessel or background) can be segmented accurately. It is shown as follows:<disp-formula id="EEq8"><label>(8)</label><mml:math id="M7" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mtext>Acc</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>TN</mml:mtext></mml:mrow><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>TN</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
      <p>Se is another important metric of retinal vessel segmentation. It is the ratio of correct positive predictions to the total number of positive predictions in the predicted results. This metric mainly evaluates the ability to recognize retinal vessels (positive) in retinal images. The better the Se value, the lower the false negative rate (FNR). The Se metric is shown as follows:<disp-formula id="EEq9"><label>(9)</label><mml:math id="M8" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mtext>Se</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>TP</mml:mtext></mml:mrow><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
      <p>Sp is another mainstream metric of retinal vessel segmentation. It is the ratio of correct negative predictions to the total number of negative predictions. It mainly evaluates the ability to recognize background (negative) in retinal images. The better the Sp value, the lower the false positive rate (FPR). Hence, the Sp metric is shown as follows:<disp-formula id="EEq10"><label>(10)</label><mml:math id="M9" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mtext>Sp</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>TN</mml:mtext></mml:mrow><mml:mrow><mml:mtext>TN</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FP</mml:mtext></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
      <p>Here, TP, TN, FP, and FN denote the number of true positives, true negatives, false positives, and false negatives, respectively.</p>
      <p>In addition, we introduce the AUC metric to evaluate the segmentation performance of each model. It is an important overall metric. A larger AUC indicates satisfactory performance, which indicates that the corresponding ROC curve is very close to the (0, 1) point and far from the 45° diagonal of the coordinate axis.</p>
      <p>We used the PyTorch backend to implement all networks. We conducted all the experiments on our computer server with four NVIDIA GeForce GTX 2080Ti GPUs. We only need to resize each original image to 512 × 512. The learning rate is 0.0015, and the batch size is 2. We compare the TiM-Net model with numerous state-of-the-art methods. We use Acc, Se, Sp, and AUC metrics to evaluate each model more comprehensively.</p>
    </sec>
    <sec id="sec4.2">
      <title>4.2. Experimental Results</title>
      <sec id="sec4.2.1">
        <title>4.2.1. Quantitative Results on STARE</title>
        <p>In this section, we make detailed performance comparisons. We first show the corresponding comparisons on STARE in <xref rid="tab1" ref-type="table">Table 1</xref>. We use two variants, namely TiM-Net-1 and TiM-Net-2, in this experiment. TiM-Net-1 means that only Side7 is chosen as the final prediction. TiM-Net-2 represents that SideOut is the final prediction layer.</p>
        <p>As shown in <xref rid="tab1" ref-type="table">Table 1</xref>, TiM-Net-2 obtains the best Se and Acc on STARE. Highly competitive Sp and AUC can be observed too. First, the best Se, especially for TiM-Net-1, means that TiM-Net can more accurately identify retinal vessels (positive), which represents the best FNR among all the models. More blood vessels can be offered for clinical diagnosis and produce the effect. As described above, the MSA mechanism of the Transformer module focuses on capturing the foreground global vessel details. And the dual-attention mechanism can suppress noisy interference well. These two factors positively boost the FNR value. Second, highly competitive Sp indicates that TiM-Net has a very competitive FPR. Noisy information is suppressed to a certain degree, which can improve the practicality of TiM-Net and effectively assist in doctors' clinical diagnoses. Although reference [<xref rid="B54" ref-type="bibr">54</xref>] gets the best AUC, TiM-Net-2 outperforms it on any other metric. Compared with [<xref rid="B17" ref-type="bibr">17</xref>, <xref rid="B54" ref-type="bibr">54</xref>, <xref rid="B55" ref-type="bibr">55</xref>], relatively higher overall performance is obtained using TiM-Net-2. Certainly, the AUC value of our model needs further improvement. Summarily, each model variant is effective for retinal vessel segmentation on STARE, demonstrating its better scalability and generalization ability.</p>
      </sec>
      <sec id="sec4.2.2">
        <title>4.2.2. Quantitative Results on CHASEDB1</title>
        <p>We show the corresponding comparisons on the CHASEDB1 data set in <xref rid="tab2" ref-type="table">Table 2</xref>. We also use the two variants introduced above.</p>
        <p>As shown in <xref rid="tab2" ref-type="table">Table 2</xref>, TiM-Net-2 gets the best Acc and Sp, competitive Se, and AUC. The best Acc means that both objects (vessels or background) can be segmented accurately. More vessel details are offered for clinical diagnosis. The best Sp indicates that TiM-Net has the best FPR. The background (negative) of the CHASEDB1 images is better segmented. And doctors can get more evident pathological observations. Although reference [<xref rid="B21" ref-type="bibr">21</xref>] obtains the best AUC, TiM-Net-2 outperforms it on both Acc and Sp metrics. Compared with [<xref rid="B55" ref-type="bibr">55</xref>], TiM-Net-2 achieves superior performance on the other three metrics except for Se. Our model is relatively competitive for retinal vessel segmentation on CHASEDB1. However, the Se metric of TiM-Net needs further improvement. Some vessels are wrong and recognized as the background. This is mostly due to the visual similarity between the background and vessels. To solve this issue, we may do some data preprocessing steps. Currently, we have achieved satisfactory results without such preprocessing steps. Moreover, we will further focus on feature learning using some state-of-the-art methods, such as MAE [<xref rid="B57" ref-type="bibr">57</xref>] and ViT [<xref rid="B38" ref-type="bibr">38</xref>]. Summarily, our model is effective for retinal vessel segmentation on the challenging CHASEDB1 data set.</p>
      </sec>
      <sec id="sec4.2.3">
        <title>4.2.3. Quantitative Results on DRIVE</title>
        <p>We show the corresponding comparisons on the DRIVE data set in <xref rid="tab3" ref-type="table">Table 3</xref>. We use the two variants introduced above.</p>
        <p>As shown in <xref rid="tab3" ref-type="table">Table 3</xref>, TiM-Net-2 obtains the best Acc and the other three competitive values on DRIVE. More vessel details are offered for clinical diagnosis. And the background (negative) of the DRIVE image is better segmented by TiM-Net. Although reference [<xref rid="B7" ref-type="bibr">7</xref>] obtains the best Sp, TiM-Net-2 outperforms it on all other metrics. Similarly, although reference [<xref rid="B21" ref-type="bibr">21</xref>] gets the best Se and AUC, TiM-Net-2 beats it on other metrics. Our model is relatively competitive on the imbalance data set. It can generate sufficient effective information for clinical diagnosis. Certainly, some vessels are segmented as the background, which leads to low Se (please refer to TiM-Net-1; the best Se will be obtained if we choose the Side7 layer, which demonstrates the scalability of TiM-Net to some degree).</p>
        <p>Summarily, the above results demonstrate the effectiveness, robustness, and scalability of TiM-Net. It achieves the best overall performance on three public data sets. Unlike other models, such as [<xref rid="B7" ref-type="bibr">7</xref>, <xref rid="B9" ref-type="bibr">9</xref>, <xref rid="B21" ref-type="bibr">21</xref>, <xref rid="B41" ref-type="bibr">41</xref>, <xref rid="B55" ref-type="bibr">55</xref>] and so on, which need preprocessing steps, our model achieves satisfactory results without such steps. Owing to very competitive performance, TiM-Net offers sufficient information for the actual diagnosis.</p>
      </sec>
      <sec id="sec4.2.4">
        <title>4.2.4. Qualitative Results</title>
        <p>In this section, we use one representative retinal vessel image from each data set as an example to more intuitively show the corresponding qualitative segmentation performance. Similar results can be observed when we use other images. The morphological characteristics of the segmented retina can be used to assist doctors in the diagnosis of diabetic retinopathy, glaucoma, and age-related macular degeneration. The qualitative results are shown in Figures <xref rid="fig6" ref-type="fig">6</xref> and <xref rid="fig7" ref-type="fig">7</xref>. We compare our model with M-Net and U-Net.</p>
        <p>As shown in <xref rid="fig6" ref-type="fig">Figure 6</xref>, we choose some representative local regions to zoom in. The hard regions are mainly composed of thinner blood vessel boundaries, whereas the easy regions are made up of thicker blood vessel boundaries. Owing to MSA, sufficient long-range relationship in the fundus images is captured accurately to decode the key blood vessel boundaries, especially for the CHASEDB1 data set. Compared with U-Net, TiM-Net owns superior performance for both easy and hard regions on STARE. Similar results can be observed on DRIVE and CHASEDB1. Compared with M-Net, our model has obvious advantages for thinner blood vessels on DRIVE. More vessel details are precisely segmented by TiM-Net, which can assist doctors in observing lesion areas and making accurate diagnosis decisions. Certainly, the corresponding performance on small blood vessels needs further improvement. Summarily, TiM-Net obtains the best overall qualitative segmentation performance, which firmly supports the clinical diagnosis.</p>
        <p>As shown in <xref rid="fig7" ref-type="fig">Figure 7</xref>, to further explore the clinical practicality of TiM-Net, we compare the segmentation results of disease and nondisease cases on DRIVE. We choose some representative local regions to zoom in. The disease cases usually have more noise, and the blood vessels in the hard regions are more blurred than those in the nondisease images. Hence, accurate blood vessel segmentation has significant clinical value. Moreover, this has a certain influence on the segmentation result. First, by observing the segmentation results of the disease images, we found that our model had an advantage in obtaining more vascular details, which can assist doctors in observing the lesions and making correct diagnostic results. We conclude that this is mostly due to the combination of the MSA and dual-attention mechanisms. Second, by observing the segmentation results of the nondisease images, we found that although TiM-Net owns better segmentation results than other models, there is no evident advantage because each nondisease image has clearer vessel details. Summarily, our model can better obtain the implicit relationship between feature channels and long-range relationship in the fundus images, to segment disease images accurately, which has significant clinical value.</p>
      </sec>
    </sec>
    <sec id="sec4.3">
      <title>4.3. Other Optimizations</title>
      <p>We use a group of side output layers, including Side5, Side6, Side7, Side8, and SideOut, to complete the final segmentation prediction. Each side layer can be employed to make segmentation independently. Then, we complete feature fusion using all the layers. So the SideOut layer represents the weighted sum of each side layer. The weight <italic>α</italic><sub><italic>i</italic></sub> (<italic>i</italic> = 1, 2, 3, 4) of each side layer is set equally (0.25). And we obtain the corresponding experimental results shown in <xref rid="tab4" ref-type="table">Table 4</xref>. We use Acc, Se, Sp, and AUC metrics to evaluate the model.</p>
      <p>As shown in <xref rid="tab4" ref-type="table">Table 4</xref>, on each data set, Side8 outperforms Side5, Side6, and Side7 on the Sp and Acc metrics. The phenomena are more evident on CHASEDB1. This means that more significant information is decoded at the last side layer, which helps improve the final performance. However, the AUC of Side8 is unsatisfactory, especially for CHASEDB1. Side8 is a relatively better choice if we only need a lower FPR. Second, Side7 outperforms Side5 and Side6 on the Acc, Se, and Sp metrics. The phenomena are more evident on CHASEDB1. Much valuable information is still retained in the Side7 layer. It is a firm foundation for the final prediction. Similar to Side8, Side7 is another good choice if we focus on a specific metric, such as Se or Acc. Moreover, SideOut gets the best AUC and competitive Acc and Sp. All these results demonstrate that different side layers complement each other and they create a kind of joint force to boost the final performance. Overall, the SideOut layer obtains a more balanced performance among all the side layers.</p>
      <p>According to the results of <xref rid="tab4" ref-type="table">Table 4</xref>, we get the following to ascend rank order of all the side layers: “Side5 &lt; Side6 = Side7 &lt; Side8.” Hence, we must set different weights for different side layers to further improve segmentation performance. We tune <italic>α</italic><sub><italic>i</italic></sub> (<italic>i</italic> = 1, 2, 3, 4) to 0.10, 0.25, 0.25, and 0.40, respectively. We use Acc, Se, Sp, and AUC metrics to evaluate each model. All the results are shown in <xref rid="tab5" ref-type="table">Table 5</xref>. Moreover, to observe the performance improvement of each layer, we average the corresponding performance improvement of each metric on all the data sets compared to <xref rid="tab4" ref-type="table">Table 4</xref> and draw <xref rid="fig8" ref-type="fig">Figure 8</xref>.</p>
      <p>As presented in <xref rid="tab5" ref-type="table">Table 5</xref>, on each data set, Side8 outperforms Side5, Side6, and Side7 on the Sp and Acc metrics. The phenomena are more evident in the STARE and DRIVE data sets. This means that sufficient important information is decoded accurately at the last side layer. Certainly, the AUC of Side8 is unsatisfactory. Side8 is a relatively optimal choice if we need the best overall performance or a lower FPR. Second, Side7 outperforms Side5 and Side6 on most metrics. The phenomena are more evident in DRIVE and STARE. Much valuable information is retained in Side7. It is another firm foundation for weighted prediction. Moreover, the SideOut layer gets the best AUC and Acc. The best overall performance is obtained by assigning a suitable weight to each side layer. Different side layers complement each other and create a kind of joint force to boost the final performance.</p>
      <p>It is worth noting that compared with <xref rid="tab4" ref-type="table">Table 4</xref>, more improvements of SideOut are found in <xref rid="tab5" ref-type="table">Table 5</xref>. Four metrics get performance improvements on STARE and CHASEDB1, whereas three metrics get more evident improvements on DRIVE. These results validate that we must use those significant features to complete the final segmentation. Meanwhile, different side layers complement each other and contribute to boosting the final performance from their views. As another suitable choice, we can choose Side7 if we focus on improving a specific metric, such as Se or Acc (please refer to the results of TiM-Net-1 in Tables <xref rid="tab1" ref-type="table">1</xref><xref rid="tab2" ref-type="table"/>–<xref rid="tab3" ref-type="table">3</xref>). This demonstrates the effectiveness of TiM-Net from another perspective.</p>
      <p>As shown in <xref rid="fig8" ref-type="fig">Figure 8</xref>, from the perspective of Se, the best performance improvement was achieved when Side8 was chosen. From the perspective of Sp and Acc, Side5 achieves the best improvement, which indicates that it has better segmentation accuracy and a lower FPR. The model variant using Side5 segments blood vessels more correctly. However, this model requires a great sacrifice of vascular segmentation performance. In terms of AUC, Side7 achieves the best performance improvement, which implies that Side7 can distinguish negative and positive objects well. Overall, SideOut achieves relatively better and more balanced performance improvement, and its performance is more robust and satisfactory, which could firmly support clinical diagnosis.</p>
      <p>Summarily, on the one hand, the SideOut layer obtains a more balanced segmentation performance. On the other hand, the best overall performance is obtained by setting a suitable weight for each side layer. Different side layers complement each other to boost the final performance. Therefore, the TiM-Net model employs the new weighted SideOut layer to make the final retinal vessel segmentation.</p>
    </sec>
    <sec id="sec4.4">
      <title>4.4. Ablation Analysis</title>
      <p>In this section, we complete a group of detailed ablation analyses, including the application of the Transformer module (<xref rid="sec4.4.1" ref-type="sec">Subsection 4.4.1</xref>), and the real contribution of each module in TiM-Net (<xref rid="sec4.4.2" ref-type="sec">Subsection 4.4.2</xref>).</p>
      <sec id="sec4.4.1">
        <title>4.4.1. Application of Transformer</title>
        <p>To validate the effectiveness of the modified skip connection, we make the following experiments. We add the Transformer module into the second layer (TransL2), third layer (TransL3), and fourth layer (TransL4). We want to know where the best position is to apply the Transformer module and how many Transformer modules are needed for TiM-Net. We use Acc, Se, Sp, and AUC metrics to complete our experiments. All the results are shown in <xref rid="tab6" ref-type="table">Table 6</xref>.</p>
        <p>As shown in <xref rid="tab6" ref-type="table">Table 6</xref>, for DRIVE and CHASEDB1, the largest performance improvement can be observed in TransL2. This means that effective feature learning or feature selection by the Transformer module is obtained at the top layer, which contains much more valuable discriminative information and long-range relationship in the fundus images. And this information can better depict vessel details. Contrarily, this information may be lost at the bottom layers (i.e., TransL3). This phenomenon is more evident in the Se and AUC metrics. Similar results can be observed on STARE. Second, we need not add too many Transformer modules into the skip connection. The worst performance is observed when we use three Transformer modules, especially for DRIVE and STARE. On the other hand, too many Transformer modules also need extra computing resources. Certainly, the combination of TransL2 and TransL4 is a good choice if we intend to use many more Transformer modules. This indicates that we should consider both top and bottom information to better complete vessel segmentation. It is a valuable conclusion that is closer to people's objective cognition.</p>
        <p>In summary, we should tune the number and position of the plug-and-play Transformer module carefully to obtain the best segmentation performance.</p>
      </sec>
      <sec id="sec4.4.2">
        <title>4.4.2. Real Contribution of Each Module</title>
        <p>TiM-Net consists of several key components, such as the backbone, dual-attention (DA) mechanism, and Transformer module. Each component acts its role in retinal vessel segmentation. In this subsection, we evaluate the real contribution of each component. And we get a group of model variants by ablation analysis. This helps us recognize the bottleneck of TiM-Net and light our future research. We use Acc, Se, Sp, and AUC metrics to evaluate each model variant. All the results are shown in <xref rid="tab7" ref-type="table">Table 7</xref>. We call this procedure coarse-grained ablation analysis. Here, “Backbone1” represents U-Net [<xref rid="B7" ref-type="bibr">7</xref>]. “Backbone2” represents M-Net [<xref rid="B17" ref-type="bibr">17</xref>]. “DA” represents the dual-attention mechanism. “TransL2” is the Transformer module. Meanwhile, fine-grained ablation analysis results are shown in Figures <xref rid="fig9" ref-type="fig">9</xref> and <xref rid="fig10" ref-type="fig">10</xref>. <xref rid="fig9" ref-type="fig"> Figure 9</xref> illustrates the average performance improvement of each model variant on each metric relative to “Backbone1.” <xref rid="fig10" ref-type="fig">Figure 10</xref> illustrates the corresponding performance improvement relative to the “Backbone2.” For example, the average improvement of “DA” on the Se metric relative to “Backbone1” is calculated as follows: ((0.7787 − 0.7042) + (0.7303 − 0.7430) + (0.8132 − 0.7371))/3 = 0.0498. Other values are computed in the same way.</p>
        <p>As shown in <xref rid="tab7" ref-type="table">Table 7</xref>, for DRIVE, using different backbones leads to different segmentation performances. Compared with “Backbone1,” the corresponding Acc, Se, Sp, and AUC of “Backbone2” improve about 0.36%, 4.72%, −0.08%, and 4.89%, respectively. Similar results can be found on the other two data sets, especially for STARE. These results validate that M-Net is a better and more robust backbone for retinal vessel segmentation.</p>
        <p>Second, for the challenging DRIVE data set, using the dual-attention mechanism leads to evident improvements. Compared with “Backbone1,” the corresponding Acc, Se, Sp, and AUC of “Backbone1 + DA” improve about 0.34%, 7.45%, −0.37%, and 2.28%, respectively. Compared with “Backbone2,” the corresponding Acc, Se, Sp, and AUC of “Backbone2 + DA” improve about 0.29%, 3.55%, −0.36%, and 0.15%, respectively. Similar results can be observed on the other two data sets, especially for STAR. For STARE, compared with “Backbone2,” the corresponding Acc, Se, Sp, and AUC of “Backbone2 + DA” improve about 0.21%, 1.83%, 0.07%, and 0.47%, respectively. Hence, similar to M-Net, the dual-attention mechanism also plays an important role in TiM-Net.</p>
        <p>Third, applying the Transformer module leads to evident performance improvements. For STARE, compared with “Backbone2,” the corresponding Acc, Se, Sp, and AUC of “Backbone2 + TransL2” improve about 0.14%, 0.94%, 0.05%, and 0.67%, respectively. Similar results can be observed in the other two data sets. However, compared to the “Backbone2” and dual-attention mechanism, the Transformer module plays a relatively secondary role in our model. Hence, the “Backbone2” and dual-attention modules are more important for retinal vessel segmentation. This also informs us to modify the pure Transformer structure in our future work. All the above discussions belong to the scope of coarse-grained ablation analysis.</p>
        <p>Besides coarse-grained ablation analysis, we also make fine-grained ablation analyses to better understand the real contribution of each module. As shown in <xref rid="fig9" ref-type="fig">Figure 9</xref>, in terms of AUC, using the “DA” or “TransL2” module can attain more evident performance improvement in Backbone1. Each module improves a specific evaluation metric. In terms of Acc, adding both the “DA” and “TransL2” modules leads to more evident performance improvement. Summarily, each module contributes to promoting the final performance in the Backbone1.</p>
        <p>As shown in <xref rid="fig10" ref-type="fig">Figure 10</xref>, in terms of Se, using the “DA” module causes the largest performance improvement in Backbone2. This indicates that the “DA” module improves the FNR of the proposed segmentation model. More vessels are segmented accurately by TiM-Net. This may offer more detailed vessel information for the clinical diagnosis. According to Sp, using the Transformer module obtains the best performance. More background pixels are segmented accurately by TiM-Net. We infer this is mostly due to the long-range relationship captured by the MSA mechanism. The combination of the DA and Transformer modules achieves the best AUC improvement. Notably, compared with <xref rid="fig9" ref-type="fig">Figure 9</xref>, more balanced improvements are observed by using the “DA” and “TransL2” modules. Hence, we combine the two modules arbitrarily to obtain the best performance. The Transformer and “DA” modules are plug-and-play, which firmly supports this requirement.</p>
        <p>Summarily, according to the fine-grained ablation analysis, second only to Backbone2, “DA” plays a more significant role in TiM-Net. Certainly, the combination of the “DA,” “TransL2” modules gets the best overall performance in Backbone2. This can firmly support clinical diagnosis. Moreover, these results are consistent with those of coarse-grained ablation analysis.</p>
      </sec>
    </sec>
  </sec>
  <sec id="sec5">
    <title>5. Conclusion and Future Work</title>
    <p>We propose a novel model, called TiM-Net, for effective retinal vessel segmentation. To fully use multiscale information, TiM-Net employs the multiscale images after maximum pooling as its inputs. Then the dual-attention mechanism is placed behind the encoder to lower the negative influence of noisy features. Meanwhile, we make feature re-coding using the MSA mechanism of the Transformer module to capture the long-range relationship in the fundus images. Finally, we create a weighted SideOut layer to complete the final segmentation.</p>
    <p>We evaluate TiM-Net on the DRIVE, STARE, and CHASEDB1 data sets. They cover diverse diseases, data distributions, and image sizes, which have certain clinical and technological values. Compared with state-of-the-arts, TiM-Net, including its variants, achieves competitive segmentation performance. We make detailed ablation analyses from coarse- and fine-grained perspectives. The descending order of the real contribution of all the modules is “Backbone2 &gt; DA &gt; TransL2.” Notably, we can obtain satisfactory results without any data preprocessing steps, which have certain practicality for clinical diagnosis. Last but not least, in terms of qualitative results, our model has an evident advantage in the segmentation of the disease images, which will be beneficial for the clinical diagnosis. Summarily, owing to satisfactory performance, TiM-Net provides firm technical support for clinical human-computer interaction diagnosis. And it shows clinically satisfactory accuracy and sensitivity to some degree.</p>
    <p>Certainly, current researches including the proposed TiM-Net have the following shortcomings: (1) it is difficult to obtain the best performance on each metric; (2) they inevitably lose some vessel details owing to continuous upsampling. Hence, in the future, we plan to modify the internal structure of the Transformer module to improve the corresponding FPR. We intend to get a trade-off between all metrics. Additionally, we will combine the symmetric pattern in Swin-Unet [<xref rid="B48" ref-type="bibr">48</xref>] with the coding pattern in MAE [<xref rid="B57" ref-type="bibr">57</xref>], to retain sufficient vessel details and make the performance of our model more outstanding on each metric.</p>
  </sec>
</body>
<back>
  <ack>
    <title>Acknowledgments</title>
    <p>This research was partly funded by the National Natural Science Foundation of China (Grant nos. 62161011 and 61861016), the Natural Science Foundation of Jiangxi Provincial Department of Science and Technology (Grant nos. 20212BAB202006 and 20202BABL202044), the Key Research and Development Plan of Jiangxi Provincial Science and Technology Department (Grant nos. 20192BBE50071 and 20202BBEL53003), the Science and Technology Projects of Jiangxi Provincial Department of Education (Grant nos. GJJ190323 and GJJ200644), and the Humanity and Social Science Foundation of Jiangxi University (Grant nos. TQ20108 and TQ21203). The authors should give thanks to the authors for collecting and organizing the three data sets [<xref rid="B49" ref-type="bibr">49</xref>–<xref rid="B51" ref-type="bibr">51</xref>]. The authors also give thanks to Qipeng Xiong, Haowei Shi, and Yiwei Zhou who also gave us some good advice about the TiM-Net model. The authors also would like to thank the editor and the reviewers for their helpful suggestions.</p>
  </ack>
  <sec sec-type="data-availability">
    <title>Data Availability</title>
    <p>The data that support the findings of this study are openly available at <ext-link xlink:href="http://cecas.clemson.edu/%7Eahoover/stare/,%20https://blogs.kingston.ac.uk/retinal/chasedb1/" ext-link-type="uri">http://cecas.clemson.edu/∼ahoover/stare/, https://blogs.kingston.ac.uk/retinal/chasedb1/</ext-link>, and <ext-link xlink:href="https://drive.grand-challenge.org/" ext-link-type="uri">https://drive.grand-challenge.org/</ext-link> [<xref rid="B48" ref-type="bibr">48</xref>–<xref rid="B50" ref-type="bibr">50</xref>].</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Conflicts of Interest</title>
    <p>The authors declare that they have no conflicts of interest.</p>
  </sec>
  <sec>
    <title>Authors' Contributions</title>
    <p>Hongbin Zhang contributed to conceptualization, validation, investigation, data curation, writing the original draft, reviewing and editing the manuscript, supervision, project administration, and funding acquisition. Xiang Zhong provided the software and resources and contributed to validation, investigation, visualization, writing the original draft, and reviewing and editing the manuscript. Zhijie Li contributed to investigation, visualization, software, and validation. Yanan Chen contributed to methodology, validation, and reviewing and editing the manuscript. Zhiliang Zhu provided resources and contributed to formal analysis. Jingqin Lv contributed to investigation and validation. Chuanxiu Li contributed to investigation and visualization. Ying Zhou contributed to pathology guidance. Guangli Li contributed to investigation, formal analysis, and funding acquisition.</p>
  </sec>
  <ref-list>
    <ref id="B1" content-type="article">
      <label>1</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Reddy</surname>
            <given-names>G. T.</given-names>
          </name>
          <name>
            <surname>Kaluri</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Reddy</surname>
            <given-names>P. K.</given-names>
          </name>
          <name>
            <surname>Lakshmanna</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Koppu</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Rajput</surname>
            <given-names>D. S.</given-names>
          </name>
        </person-group>
        <article-title>A novel approach for home surveillance system using IoT adaptive security</article-title>
        <source>
          <italic>SSRN Electronic Journal</italic>
        </source>
        <year>2019</year>
        <pub-id pub-id-type="doi">10.2139/ssrn.3356525</pub-id>
      </element-citation>
    </ref>
    <ref id="B2" content-type="article">
      <label>2</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kaluri</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Ch</surname>
            <given-names>P. R.</given-names>
          </name>
        </person-group>
        <article-title>Optimized feature extraction for precise sign gesture recognition using self-improved genetic algorithm</article-title>
        <source>
          <italic>International Journal of Engineering and Technology Innovation</italic>
        </source>
        <year>2018</year>
        <volume>8</volume>
        <fpage>25</fpage>
        <lpage>37</lpage>
      </element-citation>
    </ref>
    <ref id="B3" content-type="article">
      <label>3</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>C.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>DCML: deep contrastive mutual learning for COVID-19 recognition</article-title>
        <source>
          <italic>Biomedical Signal Processing and Control</italic>
        </source>
        <year>2022</year>
        <volume>77</volume>
        <pub-id pub-id-type="publisher-id">103770</pub-id>
        <pub-id pub-id-type="doi">10.1016/j.bspc.2022.103770</pub-id>
      </element-citation>
    </ref>
    <ref id="B4" content-type="article">
      <label>4</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fan</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Cai</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>A hierarchical image matting model for blood vessel segmentation in fundus images</article-title>
        <source>
          <italic>IEEE Transactions on Image Processing</italic>
        </source>
        <year>2019</year>
        <volume>28</volume>
        <issue>5</issue>
        <fpage>2367</fpage>
        <lpage>2377</lpage>
        <pub-id pub-id-type="doi">10.1109/tip.2018.2885495</pub-id>
        <pub-id pub-id-type="other">2-s2.0-85058898267</pub-id>
      </element-citation>
    </ref>
    <ref id="B5" content-type="incollection">
      <label>5</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Krizhevsky</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G. E.</given-names>
          </name>
        </person-group>
        <article-title>ImageNet classification with deep convolutional neural networks</article-title>
        <source>
          <italic>Neural Information Processing Systems</italic>
        </source>
        <year>2017</year>
        <volume>60</volume>
        <issue>6</issue>
        <fpage>84</fpage>
        <lpage>90</lpage>
        <pub-id pub-id-type="doi">10.1145/3065386</pub-id>
        <pub-id pub-id-type="other">2-s2.0-85020126914</pub-id>
      </element-citation>
    </ref>
    <ref id="B6" content-type="inproceedings">
      <label>6</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Long</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Shelhamer</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Long</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Darrell</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Fully convolutional networks for semantic segmentation</article-title>
        <conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition</conf-name>
        <conf-date>June 2015</conf-date>
        <conf-loc>Boston, MA, USA</conf-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>3431</fpage>
        <lpage>3440</lpage>
      </element-citation>
    </ref>
    <ref id="B7" content-type="inproceedings">
      <label>7</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Ronneberger</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Fischer</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Brox</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>U-net: convolutional networks for biomedical image segmentation</article-title>
        <conf-name>Proceedings of the International Conference on Medical image computing and computer assisted intervention</conf-name>
        <conf-date>October 2015</conf-date>
        <conf-loc>Munich, Germany</conf-loc>
        <publisher-name>Springer</publisher-name>
        <fpage>234</fpage>
        <lpage>241</lpage>
      </element-citation>
    </ref>
    <ref id="B8" content-type="misc">
      <label>8</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Siddiquee</surname>
            <given-names>M. M. R.</given-names>
          </name>
          <name>
            <surname>Tajbakhsh</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Unet++: a nested U- net architecture for medical image segmentation</article-title>
        <source>
          <italic>Deep Learning in Medical Image Analysis and Multi-modal Learning for Clinical Decision Support</italic>
        </source>
        <conf-date>2018</conf-date>
        <publisher-loc>New York , NY, USA</publisher-loc>
        <publisher-name>Springer</publisher-name>
      </element-citation>
    </ref>
    <ref id="B9" content-type="article">
      <label>9</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jin</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Meng</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Pham</surname>
            <given-names>T. D.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Su</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>DUNet: a deformable network for retinal vessel segmentation</article-title>
        <source>
          <italic>Knowledge-Based Systems</italic>
        </source>
        <year>2019</year>
        <volume>178</volume>
        <fpage>149</fpage>
        <lpage>162</lpage>
        <pub-id pub-id-type="doi">10.1016/j.knosys.2019.04.025</pub-id>
        <pub-id pub-id-type="other">2-s2.0-85065243868</pub-id>
      </element-citation>
    </ref>
    <ref id="B10" content-type="article">
      <label>10</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Dense U-net based on patch-based learning for retinal vessel segmentation</article-title>
        <source>
          <italic>Entropy</italic>
        </source>
        <year>2019</year>
        <volume>21</volume>
        <pub-id pub-id-type="doi">10.3390/e21020168</pub-id>
        <pub-id pub-id-type="other">2-s2.0-85061958683</pub-id>
      </element-citation>
    </ref>
    <ref id="B11" content-type="article">
      <label>11</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Khan</surname>
            <given-names>T. M.</given-names>
          </name>
          <name>
            <surname>Alhussein</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Aurangzeb</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Arsalan</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Naqvi</surname>
            <given-names>S. S.</given-names>
          </name>
          <name>
            <surname>Nawaz</surname>
            <given-names>S. J.</given-names>
          </name>
        </person-group>
        <article-title>Residual connection-based encoder decoder network (rced-net) for retinal vessel segmentation</article-title>
        <source>
          <italic>IEEE Access</italic>
        </source>
        <year>2020</year>
        <volume>8</volume>
        <fpage>131257</fpage>
        <lpage>131272</lpage>
        <pub-id pub-id-type="doi">10.1109/access.2020.3008899</pub-id>
      </element-citation>
    </ref>
    <ref id="B12" content-type="inproceedings">
      <label>12</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Xia</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>Y.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Vessel-Net: retinal vessel segmentation under multi-path supervision</article-title>
        <conf-name>Proceedings of the International Conference Med. Image Computing and Computer-Assisted Intervention</conf-name>
        <conf-date>October 2019</conf-date>
        <conf-loc>Shenzhen, China</conf-loc>
        <fpage>264</fpage>
        <lpage>272</lpage>
      </element-citation>
    </ref>
    <ref id="B13" content-type="inproceedings">
      <label>13</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Yu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>J. Z.</given-names>
          </name>
          <name>
            <surname>Dou</surname>
            <given-names>Q.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Automatic 3D cardiovascular MR segmentation with densely-connected volumetric convnets</article-title>
        <conf-name>Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention</conf-name>
        <conf-date>September 2017</conf-date>
        <conf-loc>Quebec, Canada</conf-loc>
        <publisher-name>Springer</publisher-name>
        <fpage>287</fpage>
        <lpage>295</lpage>
      </element-citation>
    </ref>
    <ref id="B14" content-type="article">
      <label>14</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Seo</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Bassenne</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Xing</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Modified U-net (mU-Net) with incorporation of object-dependent high level features for improved liver and liver-tumor segmentation in CT images</article-title>
        <source>
          <italic>IEEE Transactions on Medical Imaging</italic>
        </source>
        <year>2020</year>
        <volume>39</volume>
        <issue>5</issue>
        <fpage>1316</fpage>
        <lpage>1325</lpage>
        <pub-id pub-id-type="doi">10.1109/tmi.2019.2948320</pub-id>
        <pub-id pub-id-type="pmid">31634827</pub-id>
      </element-citation>
    </ref>
    <ref id="B15" content-type="article">
      <label>15</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>Y.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Retinal vessel automatic segmentation using SegNet</article-title>
        <source>
          <italic>Computational and Mathematical Methods in Medicine</italic>
        </source>
        <year>2022</year>
        <volume>2022</volume>
        <fpage>11</fpage>
        <pub-id pub-id-type="publisher-id">3117455</pub-id>
        <pub-id pub-id-type="doi">10.1155/2022/3117455</pub-id>
      </element-citation>
    </ref>
    <ref id="B16" content-type="article">
      <label>16</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Gu</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>Multiscale U-net with spatial positional attention for retinal vessel segmentation</article-title>
        <source>
          <italic>Journal of Healthcare Engineering</italic>
        </source>
        <year>2022</year>
        <volume>2022</volume>
        <fpage>10</fpage>
        <pub-id pub-id-type="publisher-id">5188362</pub-id>
        <pub-id pub-id-type="doi">10.1155/2022/5188362</pub-id>
      </element-citation>
    </ref>
    <ref id="B17" content-type="article">
      <label>17</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fu</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Wong</surname>
            <given-names>D. W. K.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>Joint optic disc and cup segmentation based on multi-label deep network and polar transformation</article-title>
        <source>
          <italic>IEEE Transactions on Medical Imaging</italic>
        </source>
        <year>2018</year>
        <volume>37</volume>
        <fpage>1597</fpage>
        <lpage>160</lpage>
        <pub-id pub-id-type="doi">10.1109/tmi.2018.2791488</pub-id>
        <pub-id pub-id-type="other">2-s2.0-85041202159</pub-id>
        <pub-id pub-id-type="pmid">29969410</pub-id>
      </element-citation>
    </ref>
    <ref id="B18" content-type="inproceedings">
      <label>18</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Guo</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Szemenyei</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Yi</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Fan</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>Spatial attention U-net for retinal vessel segmentation</article-title>
        <conf-name>Proceedings of the 2020 25th International Conference on Pattern Recognition (ICPR)</conf-name>
        <conf-date>January 2021</conf-date>
        <conf-loc>Milan, Italy</conf-loc>
        <fpage>1236</fpage>
        <lpage>1242</lpage>
      </element-citation>
    </ref>
    <ref id="B19" content-type="inproceedings">
      <label>19</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Fu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Tian</surname>
            <given-names>H.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Dual attention network for scene segmentation</article-title>
        <conf-name>Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>
        <conf-date>June 2019</conf-date>
        <publisher-name>IEEE, America</publisher-name>
      </element-citation>
    </ref>
    <ref id="B20" content-type="misc">
      <label>20</label>
      <element-citation publication-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Fu</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>Y.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Attention guided network for retinal image segmentation</article-title>
        <year>2019</year>
        <comment>
          <ext-link xlink:href="https://arxiv.org/abs/1907.12930" ext-link-type="uri">https://arxiv.org/abs/1907.12930</ext-link>
        </comment>
        <pub-id pub-id-type="doi">10.1007/978-3-030-32239-7_88</pub-id>
      </element-citation>
    </ref>
    <ref id="B21" content-type="article">
      <label>21</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Haytham</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Pottenburgh</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Saeedi</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Tao</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Hard attention net for automatic retinal vessel segmentation</article-title>
        <source>
          <italic>IEEE Journal of Biomedical and Health Informatics</italic>
        </source>
        <year>2020</year>
        <volume>24</volume>
        <issue>12</issue>
        <fpage>3384</fpage>
        <lpage>3396</lpage>
        <pub-id pub-id-type="doi">10.1109/jbhi.2020.3002985</pub-id>
        <pub-id pub-id-type="pmid">32750941</pub-id>
      </element-citation>
    </ref>
    <ref id="B22" content-type="inproceedings">
      <label>22</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Verma</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Nakashima</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Nagahara</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Kawasaki</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Iternet: retinal image segmentation utilizing structural redundancy in vessel networks</article-title>
        <conf-name>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</conf-name>
        <conf-date>August 2020</conf-date>
        <conf-loc>Waikoloa, HI, USA</conf-loc>
        <fpage>3656</fpage>
        <lpage>3665</lpage>
      </element-citation>
    </ref>
    <ref id="B23" content-type="inproceedings">
      <label>23</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>Pyramid U-net for retinal vessel segmentation</article-title>
        <conf-name>Proceedings of the ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</conf-name>
        <conf-date>June 2021</conf-date>
        <conf-loc>Toronto, ON, Canada</conf-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>1125</fpage>
        <lpage>1129</lpage>
      </element-citation>
    </ref>
    <ref id="B24" content-type="misc">
      <label>24</label>
      <element-citation publication-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>Q.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>TransUNet: transformers make strong encoders for medical image segmentation</article-title>
        <year>2021</year>
        <comment>
          <ext-link xlink:href="http://arXiv.org/abs/2102.04306" ext-link-type="uri">http://arXiv.org/abs/2102.04306</ext-link>
        </comment>
      </element-citation>
    </ref>
    <ref id="B25" content-type="article">
      <label>25</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Huiye</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Qiang</surname>
            <given-names>Hu</given-names>
          </name>
        </person-group>
        <article-title>TransFuse: fusing transformers and CNNs for medical image segmentation</article-title>
        <source>
          <italic>MICCAI</italic>
        </source>
        <year>2021</year>
        <pub-id pub-id-type="doi">10.1007/978-3-030-87193-2_2</pub-id>
      </element-citation>
    </ref>
    <ref id="B26" content-type="article">
      <label>26</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Tan</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Bu</surname>
            <given-names>W. B.</given-names>
          </name>
        </person-group>
        <article-title>PCAT-UNet: UNet-like network fused convolution and transformer for retinal vessel segmentation</article-title>
        <source>
          <italic>PLoS One</italic>
        </source>
        <year>2022</year>
        <volume>17</volume>
        <pub-id pub-id-type="publisher-id">0262689</pub-id>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0262689</pub-id>
      </element-citation>
    </ref>
    <ref id="B27" content-type="misc">
      <label>27</label>
      <element-citation publication-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Jin</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>MDU-Net: multi-scale Densely Connected U-Net for biomedical image segmentation</article-title>
        <year>2018</year>
        <comment>
          <ext-link xlink:href="http://arXiv.org/abs/1812.00352" ext-link-type="uri">http://arXiv.org/abs/1812.00352</ext-link>
        </comment>
      </element-citation>
    </ref>
    <ref id="B28" content-type="inproceedings">
      <label>28</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>J.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Prostate segmentation using 2D bridged U-net</article-title>
        <conf-name>Proceedings of the 2019 International Joint Conference on Neural Networks (IJCNN)</conf-name>
        <conf-date>July 2019</conf-date>
        <conf-loc>Budapest, Hungary</conf-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>1</fpage>
        <lpage>7</lpage>
      </element-citation>
    </ref>
    <ref id="B29" content-type="article">
      <label>29</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Devi</surname>
            <given-names>W. V.</given-names>
          </name>
          <name>
            <surname>Roy</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Thongam</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Multi-scale dilated fusion network (MSDFN) for automatic instrument segmentation</article-title>
        <source>
          <italic>Journal of Computer Science and Technology Studies</italic>
        </source>
        <year>2022</year>
        <volume>4</volume>
        <fpage>66</fpage>
        <lpage>72</lpage>
        <pub-id pub-id-type="doi">10.32996/jcsts.2022.4.1.7</pub-id>
      </element-citation>
    </ref>
    <ref id="B30" content-type="article">
      <label>30</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Ba</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Kiros</surname>
            <given-names>R.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Show, attend and tell: neural image caption generation with visual attention</article-title>
        <source>
          <italic>Computer Science</italic>
        </source>
        <year>2015</year>
        <volume>3</volume>
        <fpage>2048</fpage>
        <lpage>2057</lpage>
      </element-citation>
    </ref>
    <ref id="B31" content-type="article">
      <label>31</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Tan</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>W.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>ANU-Net: attention-based nested U-Net to exploit full resolution features for medical image segmentation</article-title>
        <source>
          <italic>Computers &amp; Graphics</italic>
        </source>
        <year>2020</year>
        <volume>90</volume>
        <fpage>11</fpage>
        <lpage>20</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cag.2020.05.003</pub-id>
      </element-citation>
    </ref>
    <ref id="B32" content-type="inproceedings">
      <label>32</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Lan</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Jin</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>Relation-aware global attention for person Re-identification</article-title>
        <conf-name>Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>
        <conf-date>June 2020</conf-date>
        <conf-loc>Seattle, Washington, USA</conf-loc>
      </element-citation>
    </ref>
    <ref id="B33" content-type="misc">
      <label>33</label>
      <element-citation publication-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Woo</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>J. Y.</given-names>
          </name>
          <name>
            <surname>Kweon</surname>
            <given-names>I. S.</given-names>
          </name>
        </person-group>
        <article-title>CBAM: convolutional block Attention module</article-title>
        <year>2018</year>
        <comment>
          <ext-link xlink:href="https://arxiv.org/abs/1807.06521" ext-link-type="uri">https://arxiv.org/abs/1807.06521</ext-link>
        </comment>
        <pub-id pub-id-type="doi">10.1007/978-3-030-01234-2_1</pub-id>
        <pub-id pub-id-type="other">2-s2.0-85055111544</pub-id>
      </element-citation>
    </ref>
    <ref id="B34" content-type="inproceedings">
      <label>34</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Qian</surname>
            <given-names>C.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Residual attention network for image classification</article-title>
        <conf-name>Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>
        <conf-date>July 2017</conf-date>
        <conf-loc>Honolulu, HI, USA</conf-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>6450</fpage>
        <lpage>6458</lpage>
      </element-citation>
    </ref>
    <ref id="B35" content-type="article">
      <label>35</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Amer</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Lambrou</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Ye</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>MDA-unet: a multi-scale dilated attention U-net for medical image segmentation</article-title>
        <source>
          <italic>Applied Sciences</italic>
        </source>
        <year>2022</year>
        <volume>12</volume>
        <issue>7</issue>
        <fpage>p. 3676</fpage>
        <pub-id pub-id-type="doi">10.3390/app12073676</pub-id>
      </element-citation>
    </ref>
    <ref id="B36" content-type="inproceedings">
      <label>36</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Behnke</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Heafield</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Losing heads in the lottery: pruning transformer attention in neural machine translation</article-title>
        <conf-name>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</conf-name>
        <conf-date>November 2020</conf-date>
        <conf-loc>Punta Cana, Dominican Republic</conf-loc>
        <fpage>2664</fpage>
        <lpage>2674</lpage>
      </element-citation>
    </ref>
    <ref id="B37" content-type="misc">
      <label>37</label>
      <element-citation publication-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>HAT: hardware-aware transformers for efficient natural language processing</article-title>
        <year>2020</year>
        <comment>
          <ext-link xlink:href="http://arXiv.org/abs/2005.14187" ext-link-type="uri">http://arXiv.org/abs/2005.14187</ext-link>
        </comment>
        <pub-id pub-id-type="doi">10.18653/v1/2020.acl-main.686</pub-id>
      </element-citation>
    </ref>
    <ref id="B38" content-type="misc">
      <label>38</label>
      <element-citation publication-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Dosovitskiy</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Beyer</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Kolesnikov</surname>
            <given-names>A.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>An image is worth 16x16 words: transformers for image recognition at scale</article-title>
        <year>2021</year>
        <comment>
          <ext-link xlink:href="https://arxiv.org/abs/2010.11929" ext-link-type="uri">https://arxiv.org/abs/2010.11929</ext-link>
        </comment>
      </element-citation>
    </ref>
    <ref id="B39" content-type="inproceedings">
      <label>39</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Ye</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Rochan</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Cross-modal self-attention network for referring image segmentation</article-title>
        <volume>1</volume>
        <conf-name>Proceedings of the 2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>
        <conf-date>June 2019</conf-date>
        <conf-loc>Long Beach, CA, USA</conf-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>9</fpage>
        <lpage>21</lpage>
      </element-citation>
    </ref>
    <ref id="B40" content-type="inproceedings">
      <label>40</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Fu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>Learning texture transformer network for image super-resolution</article-title>
        <conf-name>Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>
        <conf-date>June 2020</conf-date>
        <conf-loc>Seattle, Washington, USA</conf-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>5790</fpage>
        <lpage>5799</lpage>
      </element-citation>
    </ref>
    <ref id="B41" content-type="book">
      <label>41</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>Y.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Swin transformer: hierarchical vision transformer using shifted windows</article-title>
        <year>2021</year>
        <comment>
          <ext-link xlink:href="https://arxiv.org/abs/2103.14030" ext-link-type="uri">https://arxiv.org/abs/2103.14030</ext-link>
        </comment>
      </element-citation>
    </ref>
    <ref id="B42" content-type="inproceedings">
      <label>42</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Qiu</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Dual encoding U-net for retinal vessel segmentation</article-title>
        <conf-name>Proceedings of the Proc. Int. Conf. Med. Image Comput. Comput. -Assisted Intervention</conf-name>
        <conf-date>October 2019</conf-date>
        <conf-loc>Semarang, Indonesia</conf-loc>
        <fpage>84</fpage>
        <lpage>92</lpage>
      </element-citation>
    </ref>
    <ref id="B43" content-type="inproceedings">
      <label>43</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Ma</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Multi-task neural networks with spatial activation for retinal vessel segmentation and artery/vein classification</article-title>
        <conf-name>Proceedings of the Proc. Int. Conf. Med. Image Comput. Comput. - Assisted Intervention</conf-name>
        <conf-date>October 2019</conf-date>
        <conf-loc>Shenzhen, China</conf-loc>
        <fpage>769</fpage>
        <lpage>778</lpage>
      </element-citation>
    </ref>
    <ref id="B44" content-type="article">
      <label>44</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tong</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Fang</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Cai</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>SAT-Net: a side attention network for retinal image segmentation</article-title>
        <source>
          <italic>Applied Intelligence</italic>
        </source>
        <year>2021</year>
        <volume>51</volume>
        <issue>7</issue>
        <fpage>5146</fpage>
        <lpage>5156</lpage>
        <pub-id pub-id-type="doi">10.1007/s10489-020-01966-z</pub-id>
      </element-citation>
    </ref>
    <ref id="B45" content-type="article">
      <label>45</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jiang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Yao</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Multi-scale and multi-branch convolutional neural network for retinal image segmentation</article-title>
        <source>
          <italic>Symmetry</italic>
        </source>
        <year>2021</year>
        <volume>13</volume>
        <issue>3</issue>
        <fpage>p. 365</fpage>
        <pub-id pub-id-type="doi">10.3390/sym13030365</pub-id>
      </element-citation>
    </ref>
    <ref id="B46" content-type="article">
      <label>46</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhai</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Yao</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Retinal vessel image segmentation algorithm based on encoder-decoder structure</article-title>
        <source>
          <italic>Multimedia Tools and Applications</italic>
        </source>
        <year>2022</year>
        <pub-id pub-id-type="doi">10.1007/s11042-022-13176-5</pub-id>
      </element-citation>
    </ref>
    <ref id="B47" content-type="misc">
      <label>47</label>
      <element-citation publication-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Fu</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Tan</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Retinal image segmentation with a structure-texture demixing network</article-title>
        <year>2020</year>
        <comment>
          <ext-link xlink:href="https://arxiv.org/abs/2008.00817" ext-link-type="uri">https://arxiv.org/abs/2008.00817</ext-link>
        </comment>
        <pub-id pub-id-type="doi">10.1007/978-3-030-59722-1_74</pub-id>
      </element-citation>
    </ref>
    <ref id="B48" content-type="misc">
      <label>48</label>
      <element-citation publication-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Cao</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Tian</surname>
            <given-names>Q.</given-names>
          </name>
        </person-group>
        <article-title>Swin-unet: unet-like pure transformer for medical image segmentation</article-title>
        <year>2021</year>
        <comment>
          <ext-link xlink:href="http://arXiv.org/abs/210505537" ext-link-type="uri">http://arXiv.org/abs/210505537</ext-link>
        </comment>
      </element-citation>
    </ref>
    <ref id="B49" content-type="article">
      <label>49</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hoover</surname>
            <given-names>A. D.</given-names>
          </name>
          <name>
            <surname>Kouznetsova</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Goldbaum</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response</article-title>
        <source>
          <italic>IEEE Transactions on Medical Imaging</italic>
        </source>
        <year>2000</year>
        <volume>19</volume>
        <issue>3</issue>
        <fpage>203</fpage>
        <lpage>210</lpage>
        <pub-id pub-id-type="doi">10.1109/42.845178</pub-id>
        <pub-id pub-id-type="other">2-s2.0-0033623974</pub-id>
        <pub-id pub-id-type="pmid">10875704</pub-id>
      </element-citation>
    </ref>
    <ref id="B50" content-type="article">
      <label>50</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Owen</surname>
            <given-names>C. G.</given-names>
          </name>
          <name>
            <surname>Rudnicka</surname>
            <given-names>A. R.</given-names>
          </name>
          <name>
            <surname>Mullen</surname>
            <given-names>R.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Measuring retinal vessel tortuosity in 10-year-old children: validation of the computer-assisted image analysis of the retina (CAIAR) program</article-title>
        <source>
          <italic>Investigative Opthalmology &amp; Visual Science</italic>
        </source>
        <year>2009</year>
        <volume>50</volume>
        <issue>5</issue>
        <fpage>2004</fpage>
        <lpage>2010</lpage>
        <pub-id pub-id-type="doi">10.1167/iovs.08-3018</pub-id>
        <pub-id pub-id-type="other">2-s2.0-65549125943</pub-id>
      </element-citation>
    </ref>
    <ref id="B51" content-type="article">
      <label>51</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Staal</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Abramoff</surname>
            <given-names>M. D.</given-names>
          </name>
          <name>
            <surname>Niemeijer</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Viergever</surname>
            <given-names>M. A.</given-names>
          </name>
          <name>
            <surname>van Ginneken</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>Ridge-based vessel segmentation in color images of the retina</article-title>
        <source>
          <italic>IEEE Transactions on Medical Imaging</italic>
        </source>
        <year>2004</year>
        <volume>23</volume>
        <issue>4</issue>
        <fpage>501</fpage>
        <lpage>509</lpage>
        <pub-id pub-id-type="doi">10.1109/tmi.2004.825627</pub-id>
        <pub-id pub-id-type="other">2-s2.0-1942454910</pub-id>
        <pub-id pub-id-type="pmid">15084075</pub-id>
      </element-citation>
    </ref>
    <ref id="B52" content-type="article">
      <label>52</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Orlando</surname>
            <given-names>J. I.</given-names>
          </name>
          <name>
            <surname>Prokofyeva</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Blaschko</surname>
            <given-names>M. B.</given-names>
          </name>
        </person-group>
        <article-title>A discriminatively trained fully connected conditional random field model for blood vessel segmentation in fundus images</article-title>
        <source>
          <italic>IEEE Transactions on Biomedical Engineering</italic>
        </source>
        <year>2017</year>
        <volume>64</volume>
        <issue>1</issue>
        <fpage>16</fpage>
        <lpage>27</lpage>
        <pub-id pub-id-type="doi">10.1109/tbme.2016.2535311</pub-id>
        <pub-id pub-id-type="other">2-s2.0-85008466267</pub-id>
        <pub-id pub-id-type="pmid">26930672</pub-id>
      </element-citation>
    </ref>
    <ref id="B53" content-type="article">
      <label>53</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yan</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>K. T.</given-names>
          </name>
        </person-group>
        <article-title>Joint segment-level and pixel-wise losses for deep learning based retinal vessel segmentation</article-title>
        <source>
          <italic>IEEE Transactions on Biomedical Engineering</italic>
        </source>
        <year>2018</year>
        <volume>65</volume>
        <issue>9</issue>
        <fpage>1912</fpage>
        <lpage>1923</lpage>
        <pub-id pub-id-type="doi">10.1109/tbme.2018.2828137</pub-id>
        <pub-id pub-id-type="other">2-s2.0-85045728445</pub-id>
        <pub-id pub-id-type="pmid">29993396</pub-id>
      </element-citation>
    </ref>
    <ref id="B54" content-type="article">
      <label>54</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yan</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>K. T.</given-names>
          </name>
        </person-group>
        <article-title>A three-stage deep learning model for accurate retinal vessel segmentation</article-title>
        <source>
          <italic>IEEE Journal of Biomedical and Health Informatics</italic>
        </source>
        <year>2019</year>
        <volume>23</volume>
        <issue>4</issue>
        <fpage>1427</fpage>
        <lpage>1436</lpage>
        <pub-id pub-id-type="doi">10.1109/jbhi.2018.2872813</pub-id>
        <pub-id pub-id-type="other">2-s2.0-85054348275</pub-id>
        <pub-id pub-id-type="pmid">30281503</pub-id>
      </element-citation>
    </ref>
    <ref id="B55" content-type="inproceedings">
      <label>55</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Mathews</surname>
            <given-names>M. R.</given-names>
          </name>
          <name>
            <surname>Anzar</surname>
            <given-names>S. M.</given-names>
          </name>
          <name>
            <surname>Krishnan</surname>
            <given-names>R. K.</given-names>
          </name>
        </person-group>
        <article-title>EfficientNet for retinal blood vessel segmentation</article-title>
        <conf-name>Proceedings of the 2020 3rd International Conference on Signal Processing and Information Security (ICSPIS)</conf-name>
        <conf-date>November 2020</conf-date>
        <conf-loc>virtual, United Arab Emirates</conf-loc>
        <fpage>1</fpage>
        <lpage>4</lpage>
      </element-citation>
    </ref>
    <ref id="B56" content-type="inproceedings">
      <label>56</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Xia</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Cai</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <article-title>Multiscale network followed network model for retinal vessel segmentation</article-title>
        <conf-name>Proceedings of the Proc. Int. Conf. Med. Image Comput. Comput. - Assisted Intervention</conf-name>
        <conf-date>September 2018</conf-date>
        <conf-loc>Granada, Spain</conf-loc>
        <fpage>119</fpage>
        <lpage>126</lpage>
      </element-citation>
    </ref>
    <ref id="B57" content-type="misc">
      <label>57</label>
      <element-citation publication-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Dollár</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Girshick</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Masked autoencoders are scalable vision learners</article-title>
        <year>2021</year>
        <comment>
          <ext-link xlink:href="http://arXiv.org/abs/2111.06377" ext-link-type="uri">http://arXiv.org/abs/2111.06377</ext-link>
        </comment>
      </element-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig position="float" id="fig1">
    <label>Figure 1</label>
    <caption>
      <p>Structure of TiM-Net. Each layer is marked with the corresponding feature map size and the number of channels. The green block on the left side represents the continuous encoding, and the green block on the right side represents the continuous decoding.</p>
    </caption>
    <graphic xlink:href="JHE2022-9016401.001" position="float"/>
  </fig>
  <fig position="float" id="fig2">
    <label>Figure 2</label>
    <caption>
      <p>Overview of the Transformer: (a) the module structure of the Transformer and (b) the internal structure of each Transformer layer.</p>
    </caption>
    <graphic xlink:href="JHE2022-9016401.002" position="float"/>
  </fig>
  <fig position="float" id="fig3">
    <label>Figure 3</label>
    <caption>
      <p>The proposed dual-attention module. Feature maps pass through serial channel attention and spatial attention one by one after convolution. ⊗ represents the multiplication operation of the corresponding elements, and ⊕ denotes the addition operation of the corresponding elements.</p>
    </caption>
    <graphic xlink:href="JHE2022-9016401.003" position="float"/>
  </fig>
  <fig position="float" id="fig4">
    <label>Figure 4</label>
    <caption>
      <p>The channel attention module.</p>
    </caption>
    <graphic xlink:href="JHE2022-9016401.004" position="float"/>
  </fig>
  <fig position="float" id="fig5">
    <label>Figure 5</label>
    <caption>
      <p>The spatial attention module.</p>
    </caption>
    <graphic xlink:href="JHE2022-9016401.005" position="float"/>
  </fig>
  <fig position="float" id="fig6">
    <label>Figure 6</label>
    <caption>
      <p>Qualitative comparisons with baseline approaches. Our method obtains fewer FPR and retains finer vessel details.</p>
    </caption>
    <graphic xlink:href="JHE2022-9016401.006" position="float"/>
  </fig>
  <fig position="float" id="fig7">
    <label>Figure 7</label>
    <caption>
      <p>Qualitative comparisons between disease and nondisease cases on DRIVE.</p>
    </caption>
    <graphic xlink:href="JHE2022-9016401.007" position="float"/>
  </fig>
  <fig position="float" id="fig8">
    <label>Figure 8</label>
    <caption>
      <p>The average performance improvement of each metric in all the data sets.</p>
    </caption>
    <graphic xlink:href="JHE2022-9016401.008" position="float"/>
  </fig>
  <fig position="float" id="fig9">
    <label>Figure 9</label>
    <caption>
      <p>The average performance improvement of each metric using the Backbone1 (U-Net) in all the data sets.</p>
    </caption>
    <graphic xlink:href="JHE2022-9016401.009" position="float"/>
  </fig>
  <fig position="float" id="fig10">
    <label>Figure 10</label>
    <caption>
      <p>The average performance improvement of each metric using the Backbone2 (M-Net) in all the data sets.</p>
    </caption>
    <graphic xlink:href="JHE2022-9016401.010" position="float"/>
  </fig>
  <table-wrap position="float" id="tab1">
    <label>Table 1</label>
    <caption>
      <p>Performance comparisons on STARE. The best result of each metric is shown as <bold>0.9711</bold>. “—” means that the corresponding value was not provided.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="left" rowspan="1" colspan="1">Model</th>
          <th align="center" rowspan="1" colspan="1">Acc ↑</th>
          <th align="center" rowspan="1" colspan="1">Se ↑</th>
          <th align="center" rowspan="1" colspan="1">Sp ↑</th>
          <th align="center" rowspan="1" colspan="1">AUC ↑</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" rowspan="1" colspan="1">U-Net [<xref rid="B7" ref-type="bibr">7</xref>] (2015)</td>
          <td align="center" rowspan="1" colspan="1">0.9674</td>
          <td align="center" rowspan="1" colspan="1">0.7371</td>
          <td align="center" rowspan="1" colspan="1">0.9878</td>
          <td align="center" rowspan="1" colspan="1">0.8855</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">Orlando's model [<xref rid="B52" ref-type="bibr">52</xref>] (2017)</td>
          <td align="center" rowspan="1" colspan="1">—</td>
          <td align="center" rowspan="1" colspan="1">0.7680</td>
          <td align="center" rowspan="1" colspan="1">0.9738</td>
          <td align="center" rowspan="1" colspan="1">—</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">Yan's model [<xref rid="B53" ref-type="bibr">53</xref>] (2018)</td>
          <td align="center" rowspan="1" colspan="1">0.9612</td>
          <td align="center" rowspan="1" colspan="1">0.7581</td>
          <td align="center" rowspan="1" colspan="1">0.9846</td>
          <td align="center" rowspan="1" colspan="1">0.9801</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">Yan's model [<xref rid="B54" ref-type="bibr">54</xref>] (2018)</td>
          <td align="center" rowspan="1" colspan="1">0.9638</td>
          <td align="center" rowspan="1" colspan="1">0.7735</td>
          <td align="center" rowspan="1" colspan="1">0.9857</td>
          <td align="center" rowspan="1" colspan="1">0.9833</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">M-Net [<xref rid="B17" ref-type="bibr">17</xref>] (2018)</td>
          <td align="center" rowspan="1" colspan="1">0.9701</td>
          <td align="center" rowspan="1" colspan="1">0.7446</td>
          <td align="center" rowspan="1" colspan="1">0.9908</td>
          <td align="center" rowspan="1" colspan="1">0.8848</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">DUNet [<xref rid="B9" ref-type="bibr">9</xref>] (2019)</td>
          <td align="center" rowspan="1" colspan="1">0.9641</td>
          <td align="center" rowspan="1" colspan="1">0.7595</td>
          <td align="center" rowspan="1" colspan="1">0.9878</td>
          <td align="center" rowspan="1" colspan="1">0.9832</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">IterNet [<xref rid="B22" ref-type="bibr">22</xref>] (2020)</td>
          <td align="center" rowspan="1" colspan="1">0.9701</td>
          <td align="center" rowspan="1" colspan="1">0.7715</td>
          <td align="center" rowspan="1" colspan="1">0.9886</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9881</bold>
          </td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">EfficientNet [<xref rid="B55" ref-type="bibr">55</xref>] (2020)</td>
          <td align="center" rowspan="1" colspan="1">0.9569</td>
          <td align="center" rowspan="1" colspan="1">0.7554</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9970</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">—</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">TiM-Net-1</td>
          <td align="center" rowspan="1" colspan="1">0.9674</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.8109</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.9819</td>
          <td align="center" rowspan="1" colspan="1">0.9454</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">TiM-Net-2</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9711</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.7867</td>
          <td align="center" rowspan="1" colspan="1">0.9880</td>
          <td align="center" rowspan="1" colspan="1">0.9670</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="tab2">
    <label>Table 2</label>
    <caption>
      <p>Performance comparisons on CHASEDB1. The best result of each metric is shown as <bold>0.9711</bold>.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="left" rowspan="1" colspan="1">Model</th>
          <th align="center" rowspan="1" colspan="1">Acc ↑</th>
          <th align="center" rowspan="1" colspan="1">Se ↑</th>
          <th align="center" rowspan="1" colspan="1">Sp ↑</th>
          <th align="center" rowspan="1" colspan="1">AUC ↑</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" rowspan="1" colspan="1">U-Net [<xref rid="B7" ref-type="bibr">7</xref>] (2015)</td>
          <td align="center" rowspan="1" colspan="1">0.9684</td>
          <td align="center" rowspan="1" colspan="1">0.7430</td>
          <td align="center" rowspan="1" colspan="1">0.9842</td>
          <td align="center" rowspan="1" colspan="1">0.8902</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">Wu's model [<xref rid="B56" ref-type="bibr">56</xref>] (2018)</td>
          <td align="center" rowspan="1" colspan="1">0.9637</td>
          <td align="center" rowspan="1" colspan="1">0.7538</td>
          <td align="center" rowspan="1" colspan="1">0.9847</td>
          <td align="center" rowspan="1" colspan="1">0.9825</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">M-Net [<xref rid="B17" ref-type="bibr">17</xref>] (2018)</td>
          <td align="center" rowspan="1" colspan="1">0.9709</td>
          <td align="center" rowspan="1" colspan="1">0.7606</td>
          <td align="center" rowspan="1" colspan="1">0.9855</td>
          <td align="center" rowspan="1" colspan="1">0.8917</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">DUNet [<xref rid="B9" ref-type="bibr">9</xref>] (2019)</td>
          <td align="center" rowspan="1" colspan="1">0.9610</td>
          <td align="center" rowspan="1" colspan="1">0.8155</td>
          <td align="center" rowspan="1" colspan="1">0.9752</td>
          <td align="center" rowspan="1" colspan="1">0.9804</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">Wang's model [<xref rid="B42" ref-type="bibr">42</xref>] (2019)</td>
          <td align="center" rowspan="1" colspan="1">0.9661</td>
          <td align="center" rowspan="1" colspan="1">0.8074</td>
          <td align="center" rowspan="1" colspan="1">0.9821</td>
          <td align="center" rowspan="1" colspan="1">0.9812</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">HANet [<xref rid="B21" ref-type="bibr">21</xref>] (2020)</td>
          <td align="center" rowspan="1" colspan="1">0.9670</td>
          <td align="center" rowspan="1" colspan="1">0.8239</td>
          <td align="center" rowspan="1" colspan="1">0.9813</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9871</bold>
          </td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">IterNet [<xref rid="B22" ref-type="bibr">22</xref>] (2020)</td>
          <td align="center" rowspan="1" colspan="1">0.9655</td>
          <td align="center" rowspan="1" colspan="1">0.7970</td>
          <td align="center" rowspan="1" colspan="1">0.9823</td>
          <td align="center" rowspan="1" colspan="1">0.9851</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">EfficientNet [<xref rid="B55" ref-type="bibr">55</xref>] (2020)</td>
          <td align="center" rowspan="1" colspan="1">0.9643</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.8477</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.9825</td>
          <td align="center" rowspan="1" colspan="1">0.9448</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">Pyramid U-Net [<xref rid="B23" ref-type="bibr">23</xref>] (2021)</td>
          <td align="center" rowspan="1" colspan="1">0.9639</td>
          <td align="center" rowspan="1" colspan="1">0.8035</td>
          <td align="center" rowspan="1" colspan="1">0.9787</td>
          <td align="center" rowspan="1" colspan="1">0.9832</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">TiM-Net-1</td>
          <td align="center" rowspan="1" colspan="1">0.9695</td>
          <td align="center" rowspan="1" colspan="1">0.7933</td>
          <td align="center" rowspan="1" colspan="1">0.9814</td>
          <td align="center" rowspan="1" colspan="1">0.9384</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">TiM-Net-2</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9711</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.7697</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9865</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.9648</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="tab3">
    <label>Table 3</label>
    <caption>
      <p>Performance comparisons on DRIVE. The best result of each metric is shown as <bold>0.9638</bold>.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="left" rowspan="1" colspan="1">Model</th>
          <th align="center" rowspan="1" colspan="1">Acc ↑</th>
          <th align="center" rowspan="1" colspan="1">Se ↑</th>
          <th align="center" rowspan="1" colspan="1">Sp ↑</th>
          <th align="center" rowspan="1" colspan="1">AUC ↑</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" rowspan="1" colspan="1">U-Net [<xref rid="B7" ref-type="bibr">7</xref>] (2015)</td>
          <td align="center" rowspan="1" colspan="1">0.9604</td>
          <td align="center" rowspan="1" colspan="1">0.7042</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9854</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.9130</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">Wu's model [<xref rid="B56" ref-type="bibr">56</xref>] (2018)</td>
          <td align="center" rowspan="1" colspan="1">0.9567</td>
          <td align="center" rowspan="1" colspan="1">0.7844</td>
          <td align="center" rowspan="1" colspan="1">0.9807</td>
          <td align="center" rowspan="1" colspan="1">0.9819</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">M-Net [<xref rid="B17" ref-type="bibr">17</xref>] (2018)</td>
          <td align="center" rowspan="1" colspan="1">0.9634</td>
          <td align="center" rowspan="1" colspan="1">0.7559</td>
          <td align="center" rowspan="1" colspan="1">0.9835</td>
          <td align="center" rowspan="1" colspan="1">0.8985</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">DUNet [<xref rid="B9" ref-type="bibr">9</xref>] (2019)</td>
          <td align="center" rowspan="1" colspan="1">0.9566</td>
          <td align="center" rowspan="1" colspan="1">0.7963</td>
          <td align="center" rowspan="1" colspan="1">0.9800</td>
          <td align="center" rowspan="1" colspan="1">0.9802</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">Ma's model [<xref rid="B41" ref-type="bibr">41</xref>] (2019)</td>
          <td align="center" rowspan="1" colspan="1">0.9570</td>
          <td align="center" rowspan="1" colspan="1">0.7916</td>
          <td align="center" rowspan="1" colspan="1">0.9811</td>
          <td align="center" rowspan="1" colspan="1">0.9810</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">Wang's model [<xref rid="B42" ref-type="bibr">42</xref>] (2019)</td>
          <td align="center" rowspan="1" colspan="1">0.9567</td>
          <td align="center" rowspan="1" colspan="1">0.7940</td>
          <td align="center" rowspan="1" colspan="1">0.9816</td>
          <td align="center" rowspan="1" colspan="1">0.9772</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">IterNet [<xref rid="B22" ref-type="bibr">22</xref>] (2020)</td>
          <td align="center" rowspan="1" colspan="1">0.9573</td>
          <td align="center" rowspan="1" colspan="1">0.7735</td>
          <td align="center" rowspan="1" colspan="1">0.9838</td>
          <td align="center" rowspan="1" colspan="1">0.9816</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">HANet [<xref rid="B21" ref-type="bibr">21</xref>] (2020)</td>
          <td align="center" rowspan="1" colspan="1">0.9581</td>
          <td align="center" rowspan="1" colspan="1">0.7991</td>
          <td align="center" rowspan="1" colspan="1">0.9813</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9823</bold>
          </td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">TiM-Net-1</td>
          <td align="center" rowspan="1" colspan="1">0.9616</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.8033</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.9770</td>
          <td align="center" rowspan="1" colspan="1">0.9510</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">TiM-Net-2</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9638</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.7805</td>
          <td align="center" rowspan="1" colspan="1">0.9816</td>
          <td align="center" rowspan="1" colspan="1">0.9682</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="tab4">
    <label>Table 4</label>
    <caption>
      <p>The experimental results of assigning the same weight to each layer. The best value of each metric on each data set is shown as <bold>0.9608</bold>.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="left" rowspan="1" colspan="1">Data set</th>
          <th align="center" rowspan="1" colspan="1">Side layer</th>
          <th align="center" rowspan="1" colspan="1">Acc ↑</th>
          <th align="center" rowspan="1" colspan="1">Se ↑</th>
          <th align="center" rowspan="1" colspan="1">Sp ↑</th>
          <th align="center" rowspan="1" colspan="1">AUC ↑</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" rowspan="5" colspan="1">DRIVE</td>
          <td align="center" rowspan="1" colspan="1">Side5</td>
          <td align="center" rowspan="1" colspan="1">0.9233</td>
          <td align="center" rowspan="1" colspan="1">0.4562</td>
          <td align="center" rowspan="1" colspan="1">0.9682</td>
          <td align="center" rowspan="1" colspan="1">0.9026</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">Side6</td>
          <td align="center" rowspan="1" colspan="1">0.9471</td>
          <td align="center" rowspan="1" colspan="1">0.7107</td>
          <td align="center" rowspan="1" colspan="1">0.9702</td>
          <td align="center" rowspan="1" colspan="1">0.9437</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">Side7</td>
          <td align="center" rowspan="1" colspan="1">0.9597</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.7263</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.9826</td>
          <td align="center" rowspan="1" colspan="1">0.9106</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">Side8</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9608</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.6944</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9868</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.8944</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">SideOut</td>
          <td align="center" rowspan="1" colspan="1">0.9596</td>
          <td align="center" rowspan="1" colspan="1">0.7060</td>
          <td align="center" rowspan="1" colspan="1">0.9844</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9529</bold>
          </td>
        </tr>
        <tr>
          <td align="left" colspan="6" rowspan="1">
            <hr/>
          </td>
        </tr>
        <tr>
          <td align="left" rowspan="5" colspan="1">STARE</td>
          <td align="center" rowspan="1" colspan="1">Side5</td>
          <td align="center" rowspan="1" colspan="1">0.9117</td>
          <td align="center" rowspan="1" colspan="1">0.6406</td>
          <td align="center" rowspan="1" colspan="1">0.9355</td>
          <td align="center" rowspan="1" colspan="1">0.8963</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">Side6</td>
          <td align="center" rowspan="1" colspan="1">0.9421</td>
          <td align="center" rowspan="1" colspan="1">0.7790</td>
          <td align="center" rowspan="1" colspan="1">0.9567</td>
          <td align="center" rowspan="1" colspan="1">0.9451</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">Side7</td>
          <td align="center" rowspan="1" colspan="1">0.9610</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.7926</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.9761</td>
          <td align="center" rowspan="1" colspan="1">0.9191</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">Side8</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9668</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.7452</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9865</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.8966</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">SideOut</td>
          <td align="center" rowspan="1" colspan="1">0.9604</td>
          <td align="center" rowspan="1" colspan="1">0.7783</td>
          <td align="center" rowspan="1" colspan="1">0.9767</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9476</bold>
          </td>
        </tr>
        <tr>
          <td align="left" colspan="6" rowspan="1">
            <hr/>
          </td>
        </tr>
        <tr>
          <td align="left" rowspan="5" colspan="1">CHASEDB1</td>
          <td align="center" rowspan="1" colspan="1">Side5</td>
          <td align="center" rowspan="1" colspan="1">0.9369</td>
          <td align="center" rowspan="1" colspan="1">0.6162</td>
          <td align="center" rowspan="1" colspan="1">0.9582</td>
          <td align="center" rowspan="1" colspan="1">0.9239</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">Side6</td>
          <td align="center" rowspan="1" colspan="1">0.9569</td>
          <td align="center" rowspan="1" colspan="1">0.7523</td>
          <td align="center" rowspan="1" colspan="1">0.9704</td>
          <td align="center" rowspan="1" colspan="1">0.9491</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">Side7</td>
          <td align="center" rowspan="1" colspan="1">0.9675</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.7827</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.9798</td>
          <td align="center" rowspan="1" colspan="1">0.9295</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">Side8</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9702</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.7222</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9868</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.8721</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">SideOut</td>
          <td align="center" rowspan="1" colspan="1">0.9676</td>
          <td align="center" rowspan="1" colspan="1">0.7527</td>
          <td align="center" rowspan="1" colspan="1">0.9818</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9580</bold>
          </td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="tab5">
    <label>Table 5</label>
    <caption>
      <p>The corresponding results of using different weights. The best value of each metric on each data set is shown as <bold>0.9638</bold>. And the improved metric of SideOut compared with <xref rid="tab4" ref-type="table">Table 4</xref> is shown as <bold>0.9638.</bold></p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="left" rowspan="1" colspan="1">Data set</th>
          <th align="center" rowspan="1" colspan="1">Side layer</th>
          <th align="center" rowspan="1" colspan="1">Acc ↑</th>
          <th align="center" rowspan="1" colspan="1">Se ↑</th>
          <th align="center" rowspan="1" colspan="1">Sp ↑</th>
          <th align="center" rowspan="1" colspan="1">AUC ↑</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" rowspan="5" colspan="1">DRIVE</td>
          <td align="center" rowspan="1" colspan="1">Side5</td>
          <td align="center" rowspan="1" colspan="1">0.9278</td>
          <td align="center" rowspan="1" colspan="1">0.5225</td>
          <td align="center" rowspan="1" colspan="1">0.9666</td>
          <td align="center" rowspan="1" colspan="1">0.9195</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">Side6</td>
          <td align="center" rowspan="1" colspan="1">0.9511</td>
          <td align="center" rowspan="1" colspan="1">0.7578</td>
          <td align="center" rowspan="1" colspan="1">0.9698</td>
          <td align="center" rowspan="1" colspan="1">0.9552</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">Side7</td>
          <td align="center" rowspan="1" colspan="1">0.9616</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.8033</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.9770</td>
          <td align="center" rowspan="1" colspan="1">0.9510</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">Side8</td>
          <td align="center" rowspan="1" colspan="1">0.9636</td>
          <td align="center" rowspan="1" colspan="1">0.7704</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9824</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.8945</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">SideOut</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9638</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.7805</td>
          <td align="center" rowspan="1" colspan="1">0.9816</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9682</bold>
          </td>
        </tr>
        <tr>
          <td align="left" colspan="6" rowspan="1">
            <hr/>
          </td>
        </tr>
        <tr>
          <td align="left" rowspan="5" colspan="1">STARE</td>
          <td align="center" rowspan="1" colspan="1">Side5</td>
          <td align="center" rowspan="1" colspan="1">0.9273</td>
          <td align="center" rowspan="1" colspan="1">0.5732</td>
          <td align="center" rowspan="1" colspan="1">0.9594</td>
          <td align="center" rowspan="1" colspan="1">0.9158</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">Side6</td>
          <td align="center" rowspan="1" colspan="1">0.9522</td>
          <td align="center" rowspan="1" colspan="1">0.7844</td>
          <td align="center" rowspan="1" colspan="1">0.9676</td>
          <td align="center" rowspan="1" colspan="1">0.9549</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">Side7</td>
          <td align="center" rowspan="1" colspan="1">0.9674</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.8109</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.9819</td>
          <td align="center" rowspan="1" colspan="1">0.9454</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">Side8</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9712</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.7711</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9896</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.8846</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">SideOut</td>
          <td align="center" rowspan="1" colspan="1">0.9711</td>
          <td align="center" rowspan="1" colspan="1">0.7867</td>
          <td align="center" rowspan="1" colspan="1">0.9880</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9670</bold>
          </td>
        </tr>
        <tr>
          <td align="left" colspan="6" rowspan="1">
            <hr/>
          </td>
        </tr>
        <tr>
          <td align="left" rowspan="5" colspan="1">CHASEDB1</td>
          <td align="center" rowspan="1" colspan="1">Side5</td>
          <td align="center" rowspan="1" colspan="1">0.9450</td>
          <td align="center" rowspan="1" colspan="1">0.5985</td>
          <td align="center" rowspan="1" colspan="1">0.9683</td>
          <td align="center" rowspan="1" colspan="1">0.9332</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">Side6</td>
          <td align="center" rowspan="1" colspan="1">0.9617</td>
          <td align="center" rowspan="1" colspan="1">0.7557</td>
          <td align="center" rowspan="1" colspan="1">0.9756</td>
          <td align="center" rowspan="1" colspan="1">0.9519</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">Side7</td>
          <td align="center" rowspan="1" colspan="1">0.9695</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.7933</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.9814</td>
          <td align="center" rowspan="1" colspan="1">0.9384</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">Side8</td>
          <td align="center" rowspan="1" colspan="1">0.<bold>9711</bold></td>
          <td align="center" rowspan="1" colspan="1">0.7637</td>
          <td align="center" rowspan="1" colspan="1">0.9851</td>
          <td align="center" rowspan="1" colspan="1">0.9141</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">SideOut</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9711</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.7697</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9865</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9648</bold>
          </td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="tab6">
    <label>Table 6</label>
    <caption>
      <p>The ablation analysis results from the application of the Transformer. Our backbone is M-Net [<xref rid="B18" ref-type="bibr">18</xref>]. The best value of each metric is shown as <bold>0.9706</bold>.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="left" rowspan="1" colspan="1">Data set</th>
          <th align="center" rowspan="1" colspan="1">TransL2</th>
          <th align="center" rowspan="1" colspan="1">TransL3</th>
          <th align="center" rowspan="1" colspan="1">TransL4</th>
          <th align="center" rowspan="1" colspan="1">Acc ↑</th>
          <th align="center" rowspan="1" colspan="1">Se ↑</th>
          <th align="center" rowspan="1" colspan="1">Sp ↑</th>
          <th align="center" rowspan="1" colspan="1">AUC ↑</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" rowspan="7" colspan="1">DRIVE</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9629</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.7903</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.9797</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9130</bold>
          </td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">0.9627</td>
          <td align="center" rowspan="1" colspan="1">0.6997</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9882</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.8633</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">0.9628</td>
          <td align="center" rowspan="1" colspan="1">0.7316</td>
          <td align="center" rowspan="1" colspan="1">0.9852</td>
          <td align="center" rowspan="1" colspan="1">0.8917</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">0.9602</td>
          <td align="center" rowspan="1" colspan="1">0.7008</td>
          <td align="center" rowspan="1" colspan="1">0.9857</td>
          <td align="center" rowspan="1" colspan="1">0.8737</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">0.9625</td>
          <td align="center" rowspan="1" colspan="1">0.7277</td>
          <td align="center" rowspan="1" colspan="1">0.9853</td>
          <td align="center" rowspan="1" colspan="1">0.8717</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">0.9601</td>
          <td align="center" rowspan="1" colspan="1">0.6908</td>
          <td align="center" rowspan="1" colspan="1">0.9862</td>
          <td align="center" rowspan="1" colspan="1">0.8980</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">0.9532</td>
          <td align="center" rowspan="1" colspan="1">0.6618</td>
          <td align="center" rowspan="1" colspan="1">0.9813</td>
          <td align="center" rowspan="1" colspan="1">0.9052</td>
        </tr>
        <tr>
          <td align="left" colspan="8" rowspan="1">
            <hr/>
          </td>
        </tr>
        <tr>
          <td align="left" rowspan="7" colspan="1">CHASEDB1</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9706</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.7640</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.9846</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9088</bold>
          </td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">0.9702</td>
          <td align="center" rowspan="1" colspan="1">0.7325</td>
          <td align="center" rowspan="1" colspan="1">0.9863</td>
          <td align="center" rowspan="1" colspan="1">0.8847</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">0.9705</td>
          <td align="center" rowspan="1" colspan="1">0.7390</td>
          <td align="center" rowspan="1" colspan="1">0.9862</td>
          <td align="center" rowspan="1" colspan="1">0.8806</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">0.9686</td>
          <td align="center" rowspan="1" colspan="1">0.7467</td>
          <td align="center" rowspan="1" colspan="1">0.9837</td>
          <td align="center" rowspan="1" colspan="1">0.9113</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">0.9696</td>
          <td align="center" rowspan="1" colspan="1">0.7492</td>
          <td align="center" rowspan="1" colspan="1">0.8946</td>
          <td align="center" rowspan="1" colspan="1">0.8738</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">0.9678</td>
          <td align="center" rowspan="1" colspan="1">0.6925</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9864</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.8394</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">0.9638</td>
          <td align="center" rowspan="1" colspan="1">0.6771</td>
          <td align="center" rowspan="1" colspan="1">0.9832</td>
          <td align="center" rowspan="1" colspan="1">0.8604</td>
        </tr>
        <tr>
          <td align="left" colspan="8" rowspan="1">
            <hr/>
          </td>
        </tr>
        <tr>
          <td align="left" rowspan="7" colspan="1">STARE</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9700</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.7440</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.9907</td>
          <td align="center" rowspan="1" colspan="1">0.8705</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">0.9685</td>
          <td align="center" rowspan="1" colspan="1">0.7065</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9925</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.8280</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">0.9690</td>
          <td align="center" rowspan="1" colspan="1">0.7416</td>
          <td align="center" rowspan="1" colspan="1">0.9899</td>
          <td align="center" rowspan="1" colspan="1">0.8850</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">0.9658</td>
          <td align="center" rowspan="1" colspan="1">0.7008</td>
          <td align="center" rowspan="1" colspan="1">0.9901</td>
          <td align="center" rowspan="1" colspan="1">0.8755</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">0.9690</td>
          <td align="center" rowspan="1" colspan="1">0.7308</td>
          <td align="center" rowspan="1" colspan="1">0.9908</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.8937</bold>
          </td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">0.9667</td>
          <td align="center" rowspan="1" colspan="1">0.7028</td>
          <td align="center" rowspan="1" colspan="1">0.9906</td>
          <td align="center" rowspan="1" colspan="1">0.8824</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">0.9618</td>
          <td align="center" rowspan="1" colspan="1">0.6465</td>
          <td align="center" rowspan="1" colspan="1">0.9906</td>
          <td align="center" rowspan="1" colspan="1">0.8700</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="tab7">
    <label>Table 7</label>
    <caption>
      <p>The corresponding coarse-grained ablation analysis results. The best value of each metric is shown as <bold>0.9726</bold>.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="left" rowspan="1" colspan="1">Data set</th>
          <th align="center" rowspan="1" colspan="1">Backbone1</th>
          <th align="center" rowspan="1" colspan="1">Backbone2</th>
          <th align="center" rowspan="1" colspan="1">DA</th>
          <th align="center" rowspan="1" colspan="1">TransL2</th>
          <th align="center" rowspan="1" colspan="1">Acc ↑</th>
          <th align="center" rowspan="1" colspan="1">Se ↑</th>
          <th align="center" rowspan="1" colspan="1">Sp ↑</th>
          <th align="center" rowspan="1" colspan="1">AUC ↑</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" rowspan="8" colspan="1">DRIVE</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">0.9604</td>
          <td align="center" rowspan="1" colspan="1">0.7042</td>
          <td align="center" rowspan="1" colspan="1">0.9854</td>
          <td align="center" rowspan="1" colspan="1">0.9130</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">0.9638</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.7787</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.9817</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9358</bold>
          </td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">0.9617</td>
          <td align="center" rowspan="1" colspan="1">0.7136</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9858</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.9345</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9641</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.7523</td>
          <td align="center" rowspan="1" colspan="1">0.9847</td>
          <td align="center" rowspan="1" colspan="1">0.8858</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9640</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.7514</td>
          <td align="center" rowspan="1" colspan="1">0.9846</td>
          <td align="center" rowspan="1" colspan="1">0.9619</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">0.9638</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.7869</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.9810</td>
          <td align="center" rowspan="1" colspan="1">0.9634</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">0.9639</td>
          <td align="center" rowspan="1" colspan="1">0.7330</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9862</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.9620</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">0.9638</td>
          <td align="center" rowspan="1" colspan="1">0.7805</td>
          <td align="center" rowspan="1" colspan="1">0.9816</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9682</bold>
          </td>
        </tr>
        <tr>
          <td align="left" colspan="9" rowspan="1">
            <hr/>
          </td>
        </tr>
        <tr>
          <td align="left" rowspan="8" colspan="1">CHASEDB1</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">0.9684</td>
          <td align="center" rowspan="1" colspan="1">0.7430</td>
          <td align="center" rowspan="1" colspan="1">0.9842</td>
          <td align="center" rowspan="1" colspan="1">0.8902</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9713</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.7303</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9874</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.9172</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">0.9693</td>
          <td align="center" rowspan="1" colspan="1">0.7553</td>
          <td align="center" rowspan="1" colspan="1">0.9838</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9312</bold>
          </td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">0.9681</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.7617</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.9821</td>
          <td align="center" rowspan="1" colspan="1">0.9062</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">0.9711</td>
          <td align="center" rowspan="1" colspan="1">0.7523</td>
          <td align="center" rowspan="1" colspan="1">0.9860</td>
          <td align="center" rowspan="1" colspan="1">0.9643</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9719</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.7692</td>
          <td align="center" rowspan="1" colspan="1">0.9856</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9679</bold>
          </td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">0.9712</td>
          <td align="center" rowspan="1" colspan="1">0.7635</td>
          <td align="center" rowspan="1" colspan="1">0.9854</td>
          <td align="center" rowspan="1" colspan="1">0.9670</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">0.9711</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.7697</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9865</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.9648</td>
        </tr>
        <tr>
          <td align="left" colspan="9" rowspan="1">
            <hr/>
          </td>
        </tr>
        <tr>
          <td align="left" rowspan="8" colspan="1">STARE</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">0.9674</td>
          <td align="center" rowspan="1" colspan="1">0.7371</td>
          <td align="center" rowspan="1" colspan="1">0.9878</td>
          <td align="center" rowspan="1" colspan="1">0.8855</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9726</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.8132</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.9875</td>
          <td align="center" rowspan="1" colspan="1">0.9626</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">0.9700</td>
          <td align="center" rowspan="1" colspan="1">0.7681</td>
          <td align="center" rowspan="1" colspan="1">0.9878</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9677</bold>
          </td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">0.9697</td>
          <td align="center" rowspan="1" colspan="1">0.7351</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9911</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.8970</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">0.9686</td>
          <td align="center" rowspan="1" colspan="1">0.7665</td>
          <td align="center" rowspan="1" colspan="1">0.9871</td>
          <td align="center" rowspan="1" colspan="1">0.9633</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">0.9707</td>
          <td align="center" rowspan="1" colspan="1">0.7848</td>
          <td align="center" rowspan="1" colspan="1">0.9878</td>
          <td align="center" rowspan="1" colspan="1">0.9680</td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">0.9700</td>
          <td align="center" rowspan="1" colspan="1">0.7759</td>
          <td align="center" rowspan="1" colspan="1">0.9876</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9700</bold>
          </td>
        </tr>
        <tr>
          <td align="center" rowspan="1" colspan="1"> </td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">✓</td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9711</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.7867</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">
            <bold>0.9880</bold>
          </td>
          <td align="center" rowspan="1" colspan="1">0.9670</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
