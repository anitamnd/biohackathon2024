<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Behav Res Methods</journal-id>
    <journal-id journal-id-type="iso-abbrev">Behav Res Methods</journal-id>
    <journal-title-group>
      <journal-title>Behavior Research Methods</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1554-351X</issn>
    <issn pub-type="epub">1554-3528</issn>
    <publisher>
      <publisher-name>Springer US</publisher-name>
      <publisher-loc>New York</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8219581</article-id>
    <article-id pub-id-type="pmid">33037602</article-id>
    <article-id pub-id-type="publisher-id">1491</article-id>
    <article-id pub-id-type="doi">10.3758/s13428-020-01491-4</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>StimuliApp: Psychophysical tests on mobile devices</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Marin-Campos</surname>
          <given-names>Rafael</given-names>
        </name>
        <address>
          <email>marinraf@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Dalmau</surname>
          <given-names>Josep</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Compte</surname>
          <given-names>Albert</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7473-4184</contrib-id>
        <name>
          <surname>Linares</surname>
          <given-names>Daniel</given-names>
        </name>
        <address>
          <email>danilinares@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.10403.36</institution-id><institution>Institut d’Investigacions Biomèdiques August Pi i Sunyer (IDIBAPS), </institution></institution-wrap>Barcelona, Spain </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.452372.5</institution-id><institution-id institution-id-type="ISNI">0000 0004 1791 1185</institution-id><institution>Centro de Investigación Biomédica en Red de Enfermedades Raras (CIBERER), </institution></institution-wrap>Barcelona, Spain </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.5841.8</institution-id><institution-id institution-id-type="ISNI">0000 0004 1937 0247</institution-id><institution>Hospital Clinic, </institution><institution>University of Barcelona, </institution></institution-wrap>Barcelona, Spain </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.425902.8</institution-id><institution-id institution-id-type="ISNI">0000 0000 9601 989X</institution-id><institution>Catalan Institution for Research and Advanced Studies (ICREA), </institution></institution-wrap>Barcelona, Spain </aff>
      <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.25879.31</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 8972</institution-id><institution>Department of Neurology, </institution><institution>University of Pennsylvania, </institution></institution-wrap>Philadelphia, PA USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>9</day>
      <month>10</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>9</day>
      <month>10</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2021</year>
    </pub-date>
    <volume>53</volume>
    <issue>3</issue>
    <fpage>1301</fpage>
    <lpage>1307</lpage>
    <history>
      <date date-type="accepted">
        <day>22</day>
        <month>9</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Psychophysical tests are commonly carried out using software applications running on desktop or laptop computers, but running the software on mobile handheld devices such as smartphones or tablets could have advantages in some situations. Here, we present StimuliApp, an open-source application in which the user can create psychophysical tests on the iPad and the iPhone by means of a system of menus. A wide number of templates for creating stimuli are available including patches, gradients, gratings, checkerboards, random-dots, texts, tones or auditory noise. Images, videos and audios stored in files could also be presented. The application was developed natively for iPadOS and iOS using the low-level interface Metal for accessing the graphics processing unit, which results in high timing performance.</p>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Open source software</kwd>
      <kwd>Psychophysics</kwd>
      <kwd>Stimuli</kwd>
      <kwd>Mobile</kwd>
      <kwd>Smartphone</kwd>
      <kwd>Tablet</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Centro de Investigación Biomédica en Red de Enfermedades Raras (CIBERER)</institution>
        </funding-source>
        <award-id>15/00010</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010434</institution-id>
            <institution>“la Caixa” Foundation</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>CERCA Programme/Generalitat de Catalunya</institution>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Instituto Carlos III/FEDER</institution>
        </funding-source>
        <award-id>FIS 17/00234, PIE 16/00014</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002809</institution-id>
            <institution>Generalitat de Catalunya</institution>
          </institution-wrap>
        </funding-source>
        <award-id>PERIS-ICT SLT002/16/00338, PERIS SLT006/17/00362</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Spanish Ministry of Science, Innovation and Universities and European Regional Development Fund</institution>
        </funding-source>
        <award-id>BFU2015-65315-R, RTI2018-094190-B-I00</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100008050</institution-id>
            <institution>Fundación Cellex</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100008062</institution-id>
            <institution>Fundación Alicia Koplowitz</institution>
          </institution-wrap>
        </funding-source>
        <award-id>PSI2017-83493 R</award-id>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Psychonomic Society, Inc. 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">Psychophysical tests that present stimuli and record responses accurately are essential for studying perception and cognition. They are commonly carried out using software applications running on desktop or laptop computers, but running the software on mobile handheld devices such as smartphones or tablets could be advantageous in some situations. First, their small size and weight could facilitate their use outside the laboratory in places such as clinical environments (Kalia et al., <xref ref-type="bibr" rid="CR11">2014</xref>; Bastawrous et al., <xref ref-type="bibr" rid="CR3">2015</xref>; McKendrick, Chan, Vingrys, Turpin, &amp; Badcock, <xref ref-type="bibr" rid="CR18">2018</xref>; Linares et al., <xref ref-type="bibr" rid="CR15">2020</xref>). Second, their touchscreen interface could facilitate their use by people with little experience with more traditional computers. Third, the large number of people owning them, and the ease of installation of applications from common repositories for these devices, could help in the collection of data online. Fourth, they are generally less expensive than desktop or laptop devices.</p>
    <p id="Par3">Psychophysical tests have been carried out on mobile devices with custom applications that implement specific tests such the assessment of visual acuity (Black et al., <xref ref-type="bibr" rid="CR4">2013</xref>), contrast sensitivity (Dorr, Lesmes, Lu, &amp; Bex, <xref ref-type="bibr" rid="CR8">2013</xref>; Rodríguez-Vallejo, Remón, Monsoriu, &amp; Furlan, <xref ref-type="bibr" rid="CR24">2015</xref>; Kollbaum, Jansen, Kollbaum, &amp; Bullimore, <xref ref-type="bibr" rid="CR14">2014</xref>), chromatic contrast sensitivity (Bodduluri, Boon, Ryan, &amp; Dain, <xref ref-type="bibr" rid="CR5">2018</xref>) or stereoacuity (Rodríguez-Vallejo, Ferrando, Montagud, Monsoriu, &amp; Furlan, <xref ref-type="bibr" rid="CR23">2017</xref>), or using an application that allows the sequential presentation of images created offline (Turpin, Lawson, &amp; McKendrick, <xref ref-type="bibr" rid="CR25">2014</xref>; McKendrick et al., <xref ref-type="bibr" rid="CR18">2018</xref>; Nguyen et al., <xref ref-type="bibr" rid="CR20">2018</xref>). To our knowledge, there are no software applications for mobile devices that allow the flexible generation of stimuli to create a wide range of psychophysical tests, and this motivated us to create StimuliApp, an open-source application developed natively for iPadOS and iOS, in which the user can create psychophysical tests by means of a system of menus.</p>
  </sec>
  <sec id="Sec2">
    <title>Development</title>
    <p id="Par4">StimuliApp (<ext-link ext-link-type="uri" xlink:href="http://www.stimuliapp.com">www.stimuliapp.com</ext-link>) is a custom application developed natively for iPad and iOS in XCode (version 11.5). XCode is an integrated development environment (IDE) for developing software using the programming language Swift (version 5). Swift is a general-purpose programming language built using the high-performance and open-source LLVM compiler technology, which transforms Swift code into optimized native code.</p>
    <p id="Par5">Visual stimuli with the exception of texts (which are rendered by the Swift library) are rendered using a function written in Metal. Metal is a low-level hardware-accelerated 3D graphic and computer shader application programming interface (API) based on C++14. Similar to OpenGL, it uses the graphics processing unit (GPU) of the device to perform the calculations in parallel.</p>
    <p id="Par6">Auditory stimuli, taking advantage of the versatility of Swift to implement C code, are coded directly in C for better performance. They are generated at a high audio rate of 44.1 kHz.</p>
    <p id="Par7">Touch information is sampled at 120 Hz in all devices except for the iPad Pro 11-inch first generation (and later) and the iPad Pro 12.9-inch third generation (and later), in which the sampling rate is 240 Hz.</p>
  </sec>
  <sec id="Sec3">
    <title>Graphical user interface (GUI)</title>
    <p id="Par8">To generate a psychophysical test, StimuliApp uses a graphical user interface (GUI) consisting of a system of menus (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). Each test is a collection of sections. A section, for example, could be the instructions of the test or each of the trials to be presented. Each section will be a sequence of scenes. For example, the <italic>First trial</italic> section could have the scenes Fixation, Target and Feedback. Each scene could include several stimuli. For example, the <italic>Target</italic> scene might consist of the simultaneous presentation of two gratings and one sound.<fig id="Fig1"><label>Fig. 1</label><caption><p>Hierarchy of menus in StimuliApp</p></caption><graphic xlink:href="13428_2020_1491_Fig1_HTML" id="MO1"/></fig></p>
    <p id="Par9">StimuliApp incorporates several demonstration tests. Tutorials of how these tests were built can be found at <ext-link ext-link-type="uri" xlink:href="http://www.stimuliapp.com">www.stimuliapp.com</ext-link>. To make a copy and modify a demonstration test (or any test created by the user), it is necessary to perform a long tap on the name of the test in the <italic>Test</italic> menu.</p>
    <p id="Par10">To facilitate the interactive creation of a test, each stimulus, scene or section can be independently previewed. When previewing a scene, it is possible to advance it frame by frame, which could be particularly useful for monitoring rapidly changing stimuli.</p>
    <p id="Par11">Once a test is created, it can be run in the same device by going to the <italic>Run test</italic> menu. It can also be exported to any other device with StimuliApp installed (for example, a test created on an iPad could be run on an iPhone). To export a test, <italic>Export test</italic> should be pressed in the <italic>Test</italic> menu. The user will be able to email a <italic>.stimulitest</italic> file containing a <italic>.json</italic> description of the parameters of the test to any device. To import the test in the receiver device, the <italic>.stimulitest</italic> file should be opened with StimuliApp (if StimuliApp does not appear in the list of applications, the user should click the <italic>More</italic> option).</p>
    <p id="Par12">The <italic>.stimulitest</italic> files, as they contain a .<italic>json</italic> description, can be edited with any text editor. By default, the files are generated in a single line, but some editors (e.g. atom) have an automatic option to change it to multiple lines. There are also online tools (e.g. JSON Formatter) to change a single line to multiple lines. We think, however, that it is easy to get lost in the structure of a .<italic>json</italic> file and recommend the modification of tests within StimuliApp.</p>
    <p id="Par13">Once a test is run, the results can be accessed in the <italic>Results</italic> menu. There, the user will find two text files that can be sent by email. One file—with <italic>txt</italic> extension—includes information about the settings such as audio rate, screen resolution or frame rate. The other file—with <italic>csv</italic> extension—consists of a table in which each column is a variable of the test and each row is a section (trial). As this is a typical structure for data analysis, the <italic>csv</italic> file can be read by standard software to that end such as R, Python, Excel or SPSS.</p>
  </sec>
  <sec id="Sec4">
    <title>Stimuli</title>
    <p id="Par14">StimuliApp offers a large number of templates for creating stimuli commonly used in the study of perception and cognition (Fig. <xref rid="Fig2" ref-type="fig">2</xref>; Lu &amp; Dosher, <xref ref-type="bibr" rid="CR17">2013</xref>) including patches, gradients, gratings (Gabors), checkerboards (rectangular or radial), random-dots (linear, radial, expansive), texts, tones or auditory noise. Stimuli can also consist of images, videos and audios stored in files.<fig id="Fig2"><label>Fig. 2</label><caption><p>Examples of stimuli that can be presented using StimuliApp. (A) Gabor. (B) Grating with modulated carrier contrast. (C) Radial checkerboard. (D) Random-dots. (E) Image from a file. (F) Text</p></caption><graphic xlink:href="13428_2020_1491_Fig2_HTML" id="MO2"/></fig></p>
    <p id="Par15">Each stimulus has specific properties that define the type of stimulus and general properties such as duration, position, orientation, size, shape, noise filter, color or contrast, which are common. An interesting feature of StimuliApp is that contrast can be manipulated quasi-continuously as the noise-bit method is implemented (Allard &amp; Faubert, <xref ref-type="bibr" rid="CR1">2008</xref>). All properties can be modified dynamically in the course of the test.</p>
    <p id="Par16">The values of the properties can be specified independently for each stimulus in different units. Sizes and distances, for example, can be specified in pixels, centimeters, inches, or degrees of visual angle, which are calculated taking into account the viewing distance specified by the user and the pixel resolution of the screen of the device, which is directly detected by StimuliApp. Time can be specified in frames or seconds. Luminance is specified in fractions of the maximum brightness of the device. Interestingly, the value of the luminance in cd/m<sup>2</sup> is automatically displayed, since StimuliApp recognizes the model device and incorporates a table with the maximum luminance of each device. The nominal maximum luminance values were retrieved from <ext-link ext-link-type="uri" xlink:href="http://apple.com">apple.com</ext-link> and might slightly differ from the displayed values due to variations across series or the time in use of the displays (Caffery, Manthey, &amp; Sim, <xref ref-type="bibr" rid="CR7">2016</xref>). Importantly, for luminance—and also color—to be displayed consistently, the technologies Auto-Brightness, True Tone and Night Shift of the device should be disabled in System Preferences.</p>
    <p id="Par17">To facilitate the selection of the values across sections (for example, trials) the method of constant stimuli and several adaptive methods are implemented (Kingdom &amp; Prins, <xref ref-type="bibr" rid="CR12">2016</xref>).</p>
  </sec>
  <sec id="Sec5">
    <title>Responses</title>
    <p id="Par18">Responses can be taps, the movement of a finger or the movement of an electronic pencil on the screen. A virtual keyboard can also be displayed or an external keyboard used.</p>
  </sec>
  <sec id="Sec6">
    <title>Timing</title>
    <p id="Par19">To compute the RGB values of the stimuli in real-time, the computation should last less than the duration of a frame (16.67 ms for a refresh rate of 60 Hz and 8.33 ms for 120 Hz, for example). If the computation lasts longer than a frame, the previous image will be on the screen until the computation is finished (dropped frame).</p>
    <p id="Par20">Using the GPU Frame Capture tool and the Metal API validation tool, we calculated the number of dropped frames for several stimulus tests, which we presented 10 times during 120 s on several platforms (iPad 6th generation 2018, iPhone X 2017, iPad Pro 1st generation 10.5 inches 2017 at 60 and 120 Hz). We tried many simple stimuli typically used in visual psychophysics, and all resulted in zero dropped frames in all platforms. We were able to get dropped frames only when we displayed complex stimuli on the iPad Pro at 120 Hz (Fig. <xref rid="Fig3" ref-type="fig">3</xref>). All of these stimuli resulted in zero dropped frames in the other platforms or when the iPad Pro was run at 60 Hz (Fig. <xref rid="Fig3" ref-type="fig">3</xref>).<fig id="Fig3"><label>Fig. 3</label><caption><p>Percentage of dropped frames for different stimuli in different platforms. Each dot indicates the percentage of dropped frames when we displayed the stimulus for 120 s. We presented each stimulus 10 times. The Gabors were 300 × 300 pixels (excepting the fullscreen Gabor). The transparent Gabors had quasi-continuous contrast using the noise-bit method (Allard &amp; Faubert, <xref ref-type="bibr" rid="CR1">2008</xref>)</p></caption><graphic xlink:href="13428_2020_1491_Fig3_HTML" id="MO3"/></fig></p>
    <p id="Par21">When a test is previewed or run, and one or more frames are dropped, a report is generated indicating the duration of the dropped frames. This information could be used to reduce the computational costs of the test.</p>
    <p id="Par22">To assess the accuracy and precision reproducing sounds, we presented 1000 sounds in StimuliApp specifying a duration of 100 ms each. Connecting an output jack to an oscilloscope (Tektronix TDS 1012), we measured the generated durations and found a mean duration of 100 ms and a standard deviation of less than 1 ms.</p>
    <p id="Par23">To assess audiovisual synchrony, we presented 1000 times a visual and an auditory signal of 100 ms of duration specifying the same time onset in StimuliApp. Connecting an output jack and a photodiode to the oscilloscope, we measured the generated onset asynchronies and found that the mean asynchrony depended on the device and was between −10 ms and 10 ms (this value could be specified in StimuliApp to correct it). The standard deviation was less than 1 ms.</p>
  </sec>
  <sec id="Sec7">
    <title>Extending StimuliApp</title>
    <p id="Par24">Developers can add stimuli to StimuliApp by downloading its open source code and compiling it including a new independent Metal function for each new stimulus, and one descriptive class in Swift with the description of its new parameters. These new parameters will be added to the general parameters common to all stimuli such as size, contrast, angle, noise and temporal changes. For more information visit <ext-link ext-link-type="uri" xlink:href="https://github.com/marinraf/StimuliApp">https://github.com/marinraf/StimuliApp</ext-link>.</p>
  </sec>
  <sec id="Sec8">
    <title>Workflow for collecting data using StimuliApp: Examples</title>
    <p id="Par25">The first example describes a situation where the test is created and run on the same device. This possibility could be useful to administer tests in person. An experimenter installs StimuliApp on an iPad and uses the application to create a new test. Once the test is created, the experimenter goes to the <italic>Run</italic> section of the application and hands the iPad to a participant. After the participant finishes the test, the experimenter goes to the <italic>Results</italic> section and sends the two output data files with the results to the email of the experimenter.</p>
    <p id="Par26">The second example describes a situation where the test is created on one device and run on another device. This possibility could be useful to administer tests remotely. An experimenter installs StimuliApp on an iPhone and uses the application to create a new test. Once the test is created, the experimenter exports the test (see GUI section) and sends it by email to the participant. The participant installs StimuliApp on the iPad and then opens the file sent by the experimenter with StimuliApp. Then, the participant goes to the <italic>Run</italic> section and runs the test. After finishing, the participant goes to the <italic>Results</italic> section and sends the two files with the results to the experimenter.</p>
  </sec>
  <sec id="Sec9">
    <title>Tutorial</title>
    <p id="Par27">In this section, we describe how to build a simple test to measure orientation discrimination using the method of constant stimuli (Kingdom &amp; Prins, <xref ref-type="bibr" rid="CR12">2016</xref>). Each trial will consist of a fixation cross followed by a grating with a small clockwise or counterclockwise rotation relative to the vertical orientation. The participant will need to tap the right or left side of the screen to indicate clockwise or counterclockwise rotation, respectively. Further tests with more detailed information including screen captures can be found at <ext-link ext-link-type="uri" xlink:href="http://www.stimuliapp.com">www.stimuliapp.com</ext-link>.</p>
    <p id="Par28">First, we create a fixation cross:<list list-type="bullet"><list-item><p id="Par29">Go to the <italic>Test</italic> menu (Fig. <xref rid="Fig1" ref-type="fig">1</xref>) and press <italic>new test</italic>.</p></list-item><list-item><p id="Par30">Go to the <italic>Stimuli</italic> menu (Fig. <xref rid="Fig1" ref-type="fig">1</xref>) and press <italic>new stimulus</italic>.</p></list-item><list-item><p id="Par31">Change the stimulus <italic>shape</italic> to <italic>cross</italic>.</p></list-item></list></p>
    <p id="Par32">Then, we include the fixation as the first scene of a section:<list list-type="bullet"><list-item><p id="Par33">Go to the <italic>Scenes &amp; Sections</italic> menu (Fig. <xref rid="Fig1" ref-type="fig">1</xref>) and press <italic>new section</italic>.</p></list-item><list-item><p id="Par34">Within that section, press <italic>new scene</italic>.</p></list-item><list-item><p id="Par35">Within the scene, press <italic>new object</italic> and select <italic>stimulus1</italic>.</p></list-item><list-item><p id="Par36">Press <italic>durationValue</italic> and change the scene duration to 0.5 seconds.</p></list-item><list-item><p id="Par37">Go to the <italic>Test</italic> menu, press <italic>firstSection</italic> and select <italic>section1</italic>.</p></list-item></list></p>
    <p id="Par38">Then, we create the grating:<list list-type="bullet"><list-item><p id="Par39">Go to the <italic>Stimuli</italic> menu and press <italic>Stimuli</italic> in the top-left corner to go to the home screen of the <italic>Stimuli</italic> menu. Press <italic>new stimulus</italic>.</p></list-item><list-item><p id="Par40">Press <italic>type</italic> and change the stimulus type to <italic>grating</italic>.</p></list-item><list-item><p id="Par41">Change the stimulus <italic>shape</italic> to <italic>ellipse</italic>.</p></list-item></list></p>
    <p id="Par42">Then, we include the grating as the second scene within the section:<list list-type="bullet"><list-item><p id="Par43">Go to the <italic>Scenes &amp; Sections</italic> menu and press <italic>section1</italic> in the top-left corner to go to the <italic>Sections</italic> menu. Press <italic>new scene</italic>.</p></list-item><list-item><p id="Par44">Within the scene, press <italic>new object</italic> and select <italic>stimulus2</italic>.</p></list-item></list></p>
    <p id="Par45">Then, we tell StimuliApp to present the stimulus multiple times with a different orientation each time:<list list-type="bullet"><list-item><p id="Par46">Go to the <italic>Stimuli</italic> menu, press the property <italic>gratingRotation</italic> and change it to <italic>variable</italic>.</p></list-item><list-item><p id="Par47">Go to the <italic>Lists</italic> menu, press <italic>new list</italic> and select <italic>new list of numeric values.</italic></p></list-item><list-item><p id="Par48">Press <italic>add linear sequence</italic>. For the <italic>First value</italic>, <italic>Last value</italic> and <italic>Number of values</italic> assign the numbers -0.03, 0.03 and 7, respectively (the first two are angles in radians).</p></list-item><list-item><p id="Par49">Go to the <italic>Scenes &amp; Sections</italic> menu and press <italic>section1</italic> (in the top-left corner of the screen).</p></list-item><list-item><p id="Par50">Press the newly created property <italic>scene2_object1_gratingRotation</italic> and assign the list created above.</p></list-item><list-item><p id="Par51">From the <italic>selectionMethod</italic> select <italic>all values in random order</italic>.</p></list-item><list-item><p id="Par52">Press <italic>section1</italic> and change the number of repetitions to 20.</p></list-item></list></p>
    <p id="Par53">Then, we allow the user to make responses:<list list-type="bullet"><list-item><p id="Par54">Go to <italic>scene2</italic>, press <italic>response</italic> and add a <italic>left or right</italic> response.</p></list-item></list></p>
    <p id="Par55">Finally, we run the test:<list list-type="bullet"><list-item><p id="Par56">Go to the <italic>Main menu</italic> and then to the <italic>Run test</italic> menu. Press <italic>Run: test1.</italic></p></list-item></list></p>
    <p id="Par57">The text file with the results can be found in the <italic>Results</italic> menu.</p>
  </sec>
  <sec id="Sec10">
    <title>Discussion</title>
    <p id="Par58">StimuliApp is an open-source application that enables the generation of a wide range of psychophysical tests on mobile devices. As it was developed natively for iPadOS and iOS, the supported mobile devices are the iPad and the iPhone. We decided to program the application natively to try to achieve high timing performance, and our measurements suggest that this was achieved for stimulus presentation, as StimuliApp, for most situations, results in zero dropped frames. Future research should also test the temporal precision of touch responses. We chose the Apple ecosystem because it has a relatively small number of available models, which facilitates testing, and because the application, by recognizing the model, can present about the same stimuli independently of the device. The value of the luminance, for example, is directly displayed in cd/m<sup>2</sup> without the need for a photometer.</p>
    <p id="Par59">For desktop and laptop computers, the flexible generation of psychophysical tests could be performed among others using the platform-independent packages Psychtoolbox (Kleiner, Brainard, &amp; Pelli, <xref ref-type="bibr" rid="CR13">2007</xref>) and PsychoPy (Peirce, <xref ref-type="bibr" rid="CR22">2007</xref>), both resulting in high timing performance (Bridges, Pitiot, MacAskill, &amp; Peirce, <xref ref-type="bibr" rid="CR6">2020</xref>). PsychoPy offers the possibility of generating psychophysical tests using a GUI without the need for coding, similar to StimuliApp. It also allows the execution of psychophysical tests on the web browser, although a decrease in timing performance has been reported (Bridges et al., <xref ref-type="bibr" rid="CR6">2020</xref>; Anwyl-Irvine, Dalmaijer, Hodges, &amp; Evershed, <xref ref-type="bibr" rid="CR2">2020</xref>). Running PsychoPy on the browser of a mobile device could be an alternative to creating psychophysical tests on mobile devices.</p>
    <p id="Par60">In the <xref rid="Sec1" ref-type="sec">Introduction</xref> section we described a number of advantages of using mobile devices to run psychophysical tests, but there are also limitations. First, psychophysical tests have traditionally been conducted using CRT monitors, which have better spatiotemporal properties than the LCD screens incorporated in mobile devices (Ghodrati, Morris, &amp; Price, <xref ref-type="bibr" rid="CR10">2015</xref>; Elze &amp; Tanner, <xref ref-type="bibr" rid="CR9">2012</xref>; Packer et al., <xref ref-type="bibr" rid="CR21">2001</xref>). In a previous study, however, we compared motion sensitivity for very brief stimuli presented on an iPad (using an earlier version of StimuliApp) or a CRT monitor (using PsychoPy) and found comparable values (Linares, Marin-Campos, Dalmau, &amp; Compte, <xref ref-type="bibr" rid="CR16">2018</xref>). Second, chin rests—which are often used when the experimental set-up consists of an external monitor connected to a desktop computer—could hardly be combined with mobile devices, hindering the maintenance of a constant viewing distance. To ameliorate this issue, in our previous studies (Linares et al., <xref ref-type="bibr" rid="CR16">2018</xref>, <xref ref-type="bibr" rid="CR15">2020</xref>) the experimenter monitored the participants to ensure that they held the same approximate position during the test. A future solution to this issue could be the use of a facial recognition system like Face ID to measure the viewing distance and alert the participant if it changes during the test. This technology might be also used to track eye movements. Third, many mobile devices incorporate glossy screens, whose glare might be superimposed on the visual stimuli. To minimize this problem, the participant should run the test in a location where reflections are minimized.</p>
    <p id="Par61">Tablets and smartphones are increasingly used to acquire data in behavioural sciences (Miller, <xref ref-type="bibr" rid="CR19">2012</xref>; Woods, Velasco, Levitan, Wan, &amp; Spence, <xref ref-type="bibr" rid="CR26">2015</xref>). Here, we present an application for iPadOS and iOS devices to create psychophysical tests with high temporal precision.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <p>This work was supported by the Fundación Alicia Koplowitz, Fundació CELLEX, “La Caixa”; Foundation, CIBERER (Refs: 15/00010), Ministerio de ciencia e innovación, and the European Regional Development Fund (Refs: BFU2015-65315-R, RTI2018-094190-B-I00), the Generalitat de Catalunya (PERIS-ICT SLT002/16/00338; PERIS SLT006/17/00362) and Instituto Carlos III/FEDER (Refs: FIS 17/00234, PIE 16/00014), and the CERCA Programme/Generalitat de Catalunya. This work was developed at the Centro Esther Koplowitz facility, Barcelona.</p>
    <sec id="FPar1">
      <title>Open practices statement</title>
      <p id="Par62">The application (<ext-link ext-link-type="uri" xlink:href="https://www.stimuliapp.com/">https://www.stimuliapp.com/</ext-link>) described here is open source (<ext-link ext-link-type="uri" xlink:href="https://github.com/marinraf/StimuliApp">https://github.com/marinraf/StimuliApp</ext-link>). None of the data or materials for the experiments reported here are available, and none of the experiments were preregistered.</p>
    </sec>
  </ack>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Allard</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Faubert</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>The noisy-bit method for digital displays: Converting a 256 luminance resolution into a continuous resolution</article-title>
        <source>Behavior Research Methods</source>
        <year>2008</year>
        <volume>40</volume>
        <issue>3</issue>
        <fpage>735</fpage>
        <lpage>743</lpage>
        <pub-id pub-id-type="doi">10.3758/BRM.40.3.735</pub-id>
        <pub-id pub-id-type="pmid">18697669</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <mixed-citation publication-type="other">Anwyl-Irvine, A. L., Dalmaijer, E. S., Hodges, N., &amp; Evershed, J. (2020). <italic>Online timing accuracy and precision: A comparison of platforms, browsers, and participant’s devices</italic>. 10.31234/osf.io/jfeca</mixed-citation>
    </ref>
    <ref id="CR3">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bastawrous</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Rono</surname>
            <given-names>HK</given-names>
          </name>
          <name>
            <surname>Livingstone</surname>
            <given-names>IAT</given-names>
          </name>
          <name>
            <surname>Weiss</surname>
            <given-names>HA</given-names>
          </name>
          <name>
            <surname>Jordan</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kuper</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Burton</surname>
            <given-names>MJ</given-names>
          </name>
        </person-group>
        <article-title>Development and validation of a smartphone-based visual acuity test (peek acuity) for clinical practice and community-based fieldwork</article-title>
        <source>JAMA Ophthalmology</source>
        <year>2015</year>
        <volume>133</volume>
        <issue>8</issue>
        <fpage>930</fpage>
        <lpage>937</lpage>
        <pub-id pub-id-type="doi">10.1001/jamaophthalmol.2015.1468</pub-id>
        <pub-id pub-id-type="pmid">26022921</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <mixed-citation publication-type="other">Black, J. M., Jacobs, R. J., Phillips, G., Chen, L., Tan, E., Tran, A., &amp; Thompson, B. (2013). An assessment of the iPad as a testing platform for distance visual acuity in adults. <italic>BMJ Open, 3</italic>(6), e002730.</mixed-citation>
    </ref>
    <ref id="CR5">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bodduluri</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Boon</surname>
            <given-names>MY</given-names>
          </name>
          <name>
            <surname>Ryan</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Dain</surname>
            <given-names>SJ</given-names>
          </name>
        </person-group>
        <article-title>Normative values for a tablet computer-based application to assess chromatic contrast sensitivity</article-title>
        <source>Behavior Research Methods</source>
        <year>2018</year>
        <volume>50</volume>
        <issue>2</issue>
        <fpage>673</fpage>
        <lpage>683</lpage>
        <pub-id pub-id-type="doi">10.3758/s13428-017-0893-7</pub-id>
        <pub-id pub-id-type="pmid">28411334</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <mixed-citation publication-type="other">Bridges, D., Pitiot, A., MacAskill, M. R., &amp; Peirce, J. W. (2020). The timing mega-study: comparing a range of experiment generators, both lab-based and online. <italic>PeerJ, 8</italic>, e9414.</mixed-citation>
    </ref>
    <ref id="CR7">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Caffery</surname>
            <given-names>LJ</given-names>
          </name>
          <name>
            <surname>Manthey</surname>
            <given-names>KL</given-names>
          </name>
          <name>
            <surname>Sim</surname>
            <given-names>LH</given-names>
          </name>
        </person-group>
        <article-title>The effect of time in use on the display performance of the iPad</article-title>
        <source>The British Journal of Radiology</source>
        <year>2016</year>
        <volume>89</volume>
        <issue>1063</issue>
        <fpage>20150657</fpage>
        <pub-id pub-id-type="doi">10.1259/bjr.20150657</pub-id>
        <pub-id pub-id-type="pmid">27181625</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dorr</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lesmes</surname>
            <given-names>LA</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z-L</given-names>
          </name>
          <name>
            <surname>Bex</surname>
            <given-names>PJ</given-names>
          </name>
        </person-group>
        <article-title>Rapid and reliable assessment of the contrast sensitivity function on an iPad</article-title>
        <source>Investigative Ophthalmology &amp; Visual Science</source>
        <year>2013</year>
        <volume>54</volume>
        <issue>12</issue>
        <fpage>7266</fpage>
        <lpage>7273</lpage>
        <pub-id pub-id-type="doi">10.1167/iovs.13-11743</pub-id>
        <pub-id pub-id-type="pmid">24114545</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Elze</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tanner</surname>
            <given-names>TG</given-names>
          </name>
        </person-group>
        <article-title>Temporal properties of liquid crystal displays: Implications for vision science experiments</article-title>
        <source>PLoS ONE</source>
        <year>2012</year>
        <volume>7</volume>
        <issue>9</issue>
        <fpage>e44048</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0044048</pub-id>
        <pub-id pub-id-type="pmid">22984458</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ghodrati</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Morris</surname>
            <given-names>AP</given-names>
          </name>
          <name>
            <surname>Price</surname>
            <given-names>NSC</given-names>
          </name>
        </person-group>
        <article-title>The (un)suitability of modern liquid crystal displays (LCDs) for vision research</article-title>
        <source>Frontiers in Psychology</source>
        <year>2015</year>
        <volume>6</volume>
        <fpage>303</fpage>
        <pub-id pub-id-type="doi">10.3389/fpsyg.2015.00303</pub-id>
        <pub-id pub-id-type="pmid">25852617</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kalia</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Lesmes</surname>
            <given-names>LA</given-names>
          </name>
          <name>
            <surname>Dorr</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Gandhi</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Chatterjee</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Ganesh</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Development of pattern vision following early and extended blindness</article-title>
        <source>Proceedings of the National Academy of Sciences of the United States of America</source>
        <year>2014</year>
        <volume>111</volume>
        <issue>5</issue>
        <fpage>2035</fpage>
        <lpage>2039</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.1311041111</pub-id>
        <pub-id pub-id-type="pmid">24449865</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Kingdom</surname>
            <given-names>FAA</given-names>
          </name>
          <name>
            <surname>Prins</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <source>
          <italic>Psychophysics: A practical introduction</italic>
        </source>
        <year>2016</year>
        <publisher-loc>London</publisher-loc>
        <publisher-name>Elsevier Science</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR13">
      <mixed-citation publication-type="other">Kleiner, M., Brainard, D., &amp; Pelli, D. (2007). <italic>What’s new in Psychtoolbox-3</italic>?. <ext-link ext-link-type="uri" xlink:href="https://pure.mpg.de/rest/items/item_1790332/component/file_3136265/content">https://pure.mpg.de/rest/items/item_1790332/component/file_3136265/content</ext-link></mixed-citation>
    </ref>
    <ref id="CR14">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kollbaum</surname>
            <given-names>PS</given-names>
          </name>
          <name>
            <surname>Jansen</surname>
            <given-names>ME</given-names>
          </name>
          <name>
            <surname>Kollbaum</surname>
            <given-names>EJ</given-names>
          </name>
          <name>
            <surname>Bullimore</surname>
            <given-names>MA</given-names>
          </name>
        </person-group>
        <article-title>Validation of an iPad test of letter contrast sensitivity</article-title>
        <source>Optometry and Vision Science: Official Publication of the American Academy of Optometry</source>
        <year>2014</year>
        <volume>91</volume>
        <issue>3</issue>
        <fpage>291</fpage>
        <lpage>296</lpage>
        <pub-id pub-id-type="doi">10.1097/OPX.0000000000000158</pub-id>
        <pub-id pub-id-type="pmid">24413274</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <mixed-citation publication-type="other">Linares, D., Amoretti, S., Marin-Campos, R., Sousa, A., Prades, L., Dalmau, J., Compte, A. (2020). Spatial suppression and sensitivity for motion in schizophrenia. <italic>Schizophrenia Bulletin Open</italic>. sgaa045, 10.1093/schizbullopen/sgaa045.</mixed-citation>
    </ref>
    <ref id="CR16">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Linares</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Marin-Campos</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Dalmau</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Compte</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Validation of motion perception of briefly displayed images using a tablet</article-title>
        <source>Scientific Reports</source>
        <year>2018</year>
        <volume>8</volume>
        <issue>1</issue>
        <fpage>16056</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-018-34466-9</pub-id>
        <pub-id pub-id-type="pmid">30375459</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Lu</surname>
            <given-names>Z-L</given-names>
          </name>
          <name>
            <surname>Dosher</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <source>
          <italic>Visual psychophysics: From laboratory to theory</italic>
        </source>
        <year>2013</year>
        <publisher-loc>Cambridge, MA</publisher-loc>
        <publisher-name>MIT Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR18">
      <mixed-citation publication-type="other">McKendrick, A. M., Chan, Y. M., Vingrys, A. J., Turpin, A., &amp; Badcock, D. R. (2018). Daily vision testing can expose the prodromal phase of migraine. <italic>Cephalalgia, 38</italic>(9), 1575–1584.</mixed-citation>
    </ref>
    <ref id="CR19">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Miller</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>The smartphone psychology manifesto</article-title>
        <source>Perspectives on Psychological Science: A Journal of the Association for Psychological Science</source>
        <year>2012</year>
        <volume>7</volume>
        <issue>3</issue>
        <fpage>221</fpage>
        <lpage>237</lpage>
        <pub-id pub-id-type="doi">10.1177/1745691612441215</pub-id>
        <pub-id pub-id-type="pmid">26168460</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nguyen</surname>
            <given-names>BN</given-names>
          </name>
          <name>
            <surname>Hew</surname>
            <given-names>S-A</given-names>
          </name>
          <name>
            <surname>Ly</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Shin</surname>
            <given-names>H-Y</given-names>
          </name>
          <name>
            <surname>Wong</surname>
            <given-names>JC</given-names>
          </name>
          <name>
            <surname>Yeung</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>McKendrick</surname>
            <given-names>AM</given-names>
          </name>
        </person-group>
        <article-title>Acute caffeine ingestion affects surround suppression of perceived contrast</article-title>
        <source>Journal of Psychopharmacology</source>
        <year>2018</year>
        <volume>32</volume>
        <issue>1</issue>
        <fpage>81</fpage>
        <lpage>88</lpage>
        <pub-id pub-id-type="doi">10.1177/0269881117725684</pub-id>
        <pub-id pub-id-type="pmid">28879800</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Packer</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Diller</surname>
            <given-names>LC</given-names>
          </name>
          <name>
            <surname>Verweij</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>BB</given-names>
          </name>
          <name>
            <surname>Pokorny</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Williams</surname>
            <given-names>DR</given-names>
          </name>
          <name>
            <surname>Dacey</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>Brainard</surname>
            <given-names>DH</given-names>
          </name>
        </person-group>
        <article-title>Characterization and use of a digital light projector for vision research</article-title>
        <source>Vision Research</source>
        <year>2001</year>
        <volume>41</volume>
        <issue>4</issue>
        <fpage>427</fpage>
        <lpage>439</lpage>
        <pub-id pub-id-type="doi">10.1016/S0042-6989(00)00271-6</pub-id>
        <pub-id pub-id-type="pmid">11166046</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Peirce</surname>
            <given-names>JW</given-names>
          </name>
        </person-group>
        <article-title>PsychoPy—Psychophysics software in Python</article-title>
        <source>Journal of Neuroscience Methods</source>
        <year>2007</year>
        <volume>162</volume>
        <issue>1–2</issue>
        <fpage>8</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jneumeth.2006.11.017</pub-id>
        <pub-id pub-id-type="pmid">17254636</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rodríguez-Vallejo</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ferrando</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Montagud</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Monsoriu</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Furlan</surname>
            <given-names>WD</given-names>
          </name>
        </person-group>
        <article-title>Stereopsis assessment at multiple distances with an iPad application</article-title>
        <source>Displays</source>
        <year>2017</year>
        <volume>50</volume>
        <fpage>35</fpage>
        <lpage>40</lpage>
        <pub-id pub-id-type="doi">10.1016/j.displa.2017.09.001</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rodríguez-Vallejo</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Remón</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Monsoriu</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Furlan</surname>
            <given-names>WD</given-names>
          </name>
        </person-group>
        <article-title>Designing a new test for contrast sensitivity function measurement with iPad</article-title>
        <source>Journal of Optometry</source>
        <year>2015</year>
        <volume>8</volume>
        <issue>2</issue>
        <fpage>101</fpage>
        <lpage>108</lpage>
        <pub-id pub-id-type="doi">10.1016/j.optom.2014.06.003</pub-id>
        <pub-id pub-id-type="pmid">25890826</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Turpin</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Lawson</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>McKendrick</surname>
            <given-names>AM</given-names>
          </name>
        </person-group>
        <article-title>PsyPad: A platform for visual psychophysics on the iPad</article-title>
        <source>Journal of Vision</source>
        <year>2014</year>
        <volume>14</volume>
        <issue>3</issue>
        <fpage>16</fpage>
        <pub-id pub-id-type="doi">10.1167/14.3.16</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Woods</surname>
            <given-names>AT</given-names>
          </name>
          <name>
            <surname>Velasco</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Levitan</surname>
            <given-names>CA</given-names>
          </name>
          <name>
            <surname>Wan</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Spence</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Conducting perception research over the internet: A tutorial review</article-title>
        <source>PeerJ</source>
        <year>2015</year>
        <volume>3</volume>
        <fpage>e1058</fpage>
        <pub-id pub-id-type="doi">10.7717/peerj.1058</pub-id>
        <pub-id pub-id-type="pmid">26244107</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
