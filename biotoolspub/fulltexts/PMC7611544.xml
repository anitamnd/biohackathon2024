<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName nihms2pmcx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin ukpmcpa?>
<?ManuscriptPrefix new?>
<?iso-abbr Nat Methods?>
<?submitter-system ukmss?>
<?submitter-canonical-name Nature Publishing Group?>
<?submitter-canonical-id NATURE-STRUCTUR?>
<?submitter-userid 0?>
<?submitter-authority publisher?>
<?submitter-login NPG?>
<?submitter-name Nature Publishing Group?>
<?domain wtpa?>
<?pmc-id-preallocated 7611544?>
<?properties manuscript?>
<?origin ukpmcpa?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">101215604</journal-id>
    <journal-id journal-id-type="nlm-ta">Nat Methods</journal-id>
    <journal-id journal-id-type="iso-abbrev">Nat Methods</journal-id>
    <journal-title-group>
      <journal-title>Nature methods</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1548-7091</issn>
    <issn pub-type="epub">1548-7105</issn>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7611544</article-id>
    <article-id pub-id-type="manuscript">ems129592</article-id>
    <article-id pub-id-type="pmid">34354294</article-id>
    <article-id pub-id-type="doi">10.1038/s41592-021-01226-z</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>LiftPose3D, a deep learning-based approach for transforming 2D to 3D pose in laboratory animals</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Gosztolai</surname>
          <given-names>Adam</given-names>
        </name>
        <xref ref-type="corresp" rid="CR1">*</xref>
        <xref ref-type="aff" rid="A1">1</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Günel</surname>
          <given-names>Semih</given-names>
        </name>
        <xref ref-type="corresp" rid="CR1">*</xref>
        <xref ref-type="aff" rid="A1">1</xref>
        <xref ref-type="aff" rid="A2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ríos</surname>
          <given-names>Victor Lobato</given-names>
        </name>
        <xref ref-type="aff" rid="A1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Abrate</surname>
          <given-names>Marco Pietro</given-names>
        </name>
        <xref ref-type="aff" rid="A1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Morales</surname>
          <given-names>Daniel</given-names>
        </name>
        <xref ref-type="aff" rid="A1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Rhodin</surname>
          <given-names>Helge</given-names>
        </name>
        <xref ref-type="aff" rid="A3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Fua</surname>
          <given-names>Pascal</given-names>
        </name>
        <xref ref-type="aff" rid="A2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ramdya</surname>
          <given-names>Pavan</given-names>
        </name>
        <xref ref-type="corresp" rid="CR1">*</xref>
        <xref ref-type="aff" rid="A1">1</xref>
      </contrib>
    </contrib-group>
    <aff id="A1"><label>1</label>Neuroengineering Laboratory, Brain Mind Institute &amp; Interfaculty Institute of Bioengineering, EPFL, Lausanne, Switzerland</aff>
    <aff id="A2"><label>2</label>Computer Vision Laboratory, EPFL, Lausanne, Switzerland</aff>
    <aff id="A3"><label>3</label>Department of Computer Science, UBC, Vancouver, Canada</aff>
    <author-notes>
      <corresp id="CR1"><label>*</label>corresponding authors: <email>adam.gosztolai@epfl.ch</email>; <email>semih.gunel@epfl.ch</email>; <email>pavan.ramdya@epfl.ch</email></corresp>
    </author-notes>
    <pub-date pub-type="nihms-submitted">
      <day>07</day>
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <day>01</day>
      <month>8</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>05</day>
      <month>8</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>20</day>
      <month>8</month>
      <year>2021</year>
    </pub-date>
    <volume>18</volume>
    <issue>8</issue>
    <fpage>975</fpage>
    <lpage>981</lpage>
    <permissions>
      <ali:free_to_read xmlns:ali="http://www.niso.org/schemas/ali/1.0/"/>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/">https://www.springernature.com/gp/open-research/policies/accepted-manuscript-terms</ali:license_ref>
        <license-p>Users may view, print, copy, and download text and data-mine the content in such documents, for the purposes of academic research, subject always to the full Conditions of use: <ext-link ext-link-type="uri" xlink:href="https://www.springernature.com/gp/open-research/policies/accepted-manuscript-terms">https://www.springernature.com/gp/open-research/policies/accepted-manuscript-terms</ext-link></license-p>
      </license>
    </permissions>
    <abstract>
      <p id="P1">Markerless 3D pose estimation has become an indispensable tool for kinematic studies of laboratory animals. Most current methods recover 3D pose by multi-view triangulation of deep network-based 2D pose estimates. However, triangulation requires multiple, synchronized cameras and elaborate calibration protocols that hinder its widespread adoption in laboratory studies. Here we describe LiftPose3D, a deep network-based method that overcomes these barriers by reconstructing 3D poses from a single 2D camera view. We illustrate LiftPose3D’s versatility by applying it to multiple experimental systems using flies, mice, rats, and macaque monkeys and in circumstances where 3D triangulation is impractical or impossible. Our framework achieves accurate lifting for stereotyped and non-stereotyped behaviors from different camera angles. Thus, LiftPose3D permits high-quality 3D pose estimation in the absence of complex camera arrays, tedious calibration procedures, and despite occluded body parts in freely behaving animals.</p>
    </abstract>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="S1">
    <label>1</label>
    <title>Introduction</title>
    <p id="P2">To identify how actions arise from neural circuit dynamics, one must first make accurate measurements of behavior in laboratory experiments. Recent innovations in 3-dimensional (3D) pose estimation promise to accelerate the discovery of these neural control principles. 3D pose estimation is typically accomplished by triangulating 2-dimensional (2D) poses acquired using multiple, synchronized cameras and deep learning-based tracking algorithms [<xref rid="R1" ref-type="bibr">1</xref>–<xref rid="R9" ref-type="bibr">9</xref>]. Notably, triangulation requires that every tracked keypoint (body landmark) be visible from at least two synchronized cameras [<xref rid="R10" ref-type="bibr">10</xref>] and that each camera is calibrated. These requirements are often difficult to meet in space-constrained experimental systems that also house sensory stimulation devices [<xref rid="R11" ref-type="bibr">11</xref>–<xref rid="R13" ref-type="bibr">13</xref>], or when imaging untethered, freely behaving animals like fur-covered rodents [<xref rid="R14" ref-type="bibr">14</xref>] for whom keypoints can sometimes be occluded.</p>
    <p id="P3">Because of these challenges, most animal studies have favored 2D pose estimation using one camera [<xref rid="R1" ref-type="bibr">1</xref>, <xref rid="R2" ref-type="bibr">2</xref>, <xref rid="R6" ref-type="bibr">6</xref>, <xref rid="R15" ref-type="bibr">15</xref>–<xref rid="R17" ref-type="bibr">17</xref>]. Nevertheless, 3D poses are desirable because they eliminate a problematic camera-angle dependence that can arise during behavioral analyses [<xref rid="R3" ref-type="bibr">3</xref>]. Research on human pose estimation has long been interested in “lifting” 2D poses by regressing them to a library of 3D poses [<xref rid="R18" ref-type="bibr">18</xref>–<xref rid="R21" ref-type="bibr">21</xref>] but only recently have achieved high accuracy using deep learning [<xref rid="R22" ref-type="bibr">22</xref>–<xref rid="R34" ref-type="bibr">34</xref>]. These techniques have not been adapted to the study of animals due to the relative lack of large and diverse training datasets.</p>
    <p id="P4">Here, we introduce LiftPose3D, a tool for 3D pose estimation of tethered and freely behaving laboratory animals from a single camera view. Our method builds on a recent neural network architecture designed to lift human poses [<xref rid="R30" ref-type="bibr">30</xref>]. We develop data transformations and network training augmentation methods that enable accurate 3D pose estimation across a wide range of animals, camera angles, experimental systems, and behaviors using relatively little data. We find that (i) a library of 3D poses can be used to train our network to lift 2D poses from one camera, with minimal constraints on camera hardware and positioning and, consequently, no calibration, (ii) by aligning animal poses, our network can overcome occlusions and outliers in ground truth data, and (iii) pretrained networks can generalize across experimental setups using linear domain adaptation.</p>
  </sec>
  <sec sec-type="results" id="S2">
    <label>2</label>
    <title>Results</title>
    <sec id="S3">
      <label>2.1</label>
      <title>Predicting 3D pose with fewer cameras at arbitrary positions</title>
      <p id="P5">Rather than taking independent 2D keypoints as inputs, as for triangulation-based 3D pose estimation, LiftPose3D uses a deep-neural network to regress an ensmeble of 2D keypoints viewed from a camera—the 2D pose—to a ground truth library of 3D poses. Considering all keypoints simultaneously allows the network to learn geometric relationships intrinsic to animal poses.</p>
      <p id="P6">First, we illustrate how this approach can reduce the number of cameras needed for 3D pose estimation on a tethered adult <italic>Drosophila</italic> dataset [<xref rid="R3" ref-type="bibr">3</xref>]. Here, 15 keypoints are visible from three synchronized cameras on each side of the animal (<xref ref-type="fig" rid="F1">Figure 1A</xref>). These keypoints were annotated and triangulated using DeepFly3D [<xref rid="R3" ref-type="bibr">3</xref>]. Using this dataset as a 3D pose library we aimed to train a LiftPose3D network that lifts half-body 2D poses from any side camera without knowing the camera’s orientation. First, we ensured that the output of LiftPose3D was translation invariant by predicting the keypoints of the respective legs relative to six ”root” immobile thorax-coxa joints (green circles, <xref ref-type="fig" rid="F1">Figure 1B</xref>). Second, to avoid the network having to learn perspective distortion, we assumed that the focal length (intrinsic matrix) of the camera and the animal-to-camera distance were known, or that one of them is large enough to assume weak perspective effects. In the latter case, we normalized 2D input poses by their Frobenius norm during both training and testing. Third, to facilitate lifting from any angle, we assumed that camera extrinsic matrices, which could be obtained by calibration, might also be unknown. Instead, we parametrized them by Euler angles <italic>ψ<sub>z</sub>, ψ<sub>y</sub>, ψ<sub>x</sub></italic> representing ordered rotations around the <italic>z, y</italic> and <italic>x</italic> axes of a coordinate system centered around the fly (<xref ref-type="fig" rid="F1">Figure 1D</xref>). During training, we took as inputs 2D poses (from 3D poses randomly projected to virtual camera planes, rather than 2D pose estimates), and as outputs 3D poses triangulated from three cameras. To measure lifting accuracy, we tested the network on software-annotated 2D poses (<xref ref-type="fig" rid="F1">Figure 1B</xref>) from two independent animals and computed the mean absolute error (MAE), <inline-formula><mml:math id="M1"><mml:mrow><mml:msubsup><mml:mi>e</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mtext>te</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, for each joint <italic>j</italic> as well as the MAE across all joints <inline-formula><mml:math id="M2"><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mtext>te</mml:mtext></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mi>∑</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:msubsup><mml:mi>e</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mtext>te</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> relative to triangulated 3D poses.</p>
      <p id="P7">We found that LiftPose3D could predict 3D poses using only one camera per side (<xref ref-type="fig" rid="F1">Figure 1C</xref>). When the virtual projections during training were performed using known intrinsic and extrinsic matrices, the network’s accuracy was at least as good as triangulation using two cameras per keypoint (<xref ref-type="fig" rid="F1">Figure 1E</xref>, white). Surprisingly, the accuracy did not suffer when the network was trained (i) using virtual 2D projections around an approximate camera location (<xref ref-type="fig" rid="F1">Figure 1E</xref>, green, narrow range) rather than with known instrinsic matrices, and (ii) using normalized 2D poses rather than with known intrinsic matrices. Accuracy remained excellent when virtual projections extended to all possible angles around the meridian (<xref ref-type="fig" rid="F1">Figure 1E</xref>, red, wide-range). Lifting could be performed for optogenetically-induced backward walking (<xref ref-type="supplementary-material" rid="SD7">Video 1</xref>), antennal grooming (<xref ref-type="supplementary-material" rid="SD8">Video 2</xref>), and spontaneous, irregular limb movements (<xref ref-type="supplementary-material" rid="SD9">Video 3</xref>). Because the network predicts joint coordinates with respect to thoracic root joints, the MAE was larger for distal joints that move within a larger kinematic volume. By contrast, the error for triangulation depended only on the accuracy of 2D annotations because it treats each keypoint independently. We also assessed camera-angle dependence for our wide angle-range network by lifting virtual 2D poses projected onto the meridian of the unit sphere, or 2D poses captured from each of the six cameras (<xref ref-type="fig" rid="F1">Figure 1F</xref>). The test MAE was low (&lt; 0.05 mm) and had no camera-angle dependence. Because we make no assumptions about camera placement when training our angle-invariant networks, these pretrained networks might also be used to predict accurate 3D poses for tethered <italic>Drosophila</italic> recorded in other laboratories.</p>
      <p id="P8">We next explored how the similarity between animal behaviors used for training and testing might influence lifting accuracy. Our tethered <italic>Drosophila</italic> dataset contained optogenetically-induced antennal grooming (<italic>aDN</italic>), and backward walking (<italic>MDN</italic>), as well as spontaneous behaviors like forward walking (control). We trained a network using poses from only one behavior (not including rest frames) and evaluated it on all three behaviors while keeping the amount of training data fixed (2.5 × 10<sup>4</sup> poses). As expected, the MAE was higher when test data included untrained behaviors than when test data included trained behaviors (<xref ref-type="fig" rid="F1">Figure 1G</xref>). Furthermore, training on all three behaviors led to comparable or lower MAE (<xref ref-type="fig" rid="F1">Figure 1E</xref>, orange) than training and testing on one single behavior (<xref ref-type="fig" rid="F1">Figure 1G</xref>). Thus, higher training data diversity improves lifting accuracy.</p>
      <p id="P9">To illustrate the advantage of using lifted 3D poses versus 2D poses in downstream analyses, we derived joint angles during forward walking from lifted 3D poses and from 2D poses projected from 3D poses in the ventral plane (<xref ref-type="fig" rid="F4">Extended Data Figure 1</xref>, green). Joint angles derived from lifted and triangulated 3D poses were in close agreement. On the other hand, we found spurious dynamics in the distal joints when viewed from a projected plane, likely due to rotations upstream in the kinematic chain (proximal joints) that cause movements of the whole leg. Thus, 3D poses predicted by LiftPose3D can help to decouple underlying physical degrees-of-freedom.</p>
      <p id="P10">We also tested LiftPose3D in freely behaving animals where the effective camera angle dynamically changes, and in animals without exoskeletons whose neighboring keypoints are less constrained. Specifically, we considered freely behaving macaque monkeys [<xref rid="R4" ref-type="bibr">4</xref>] where 3D poses were triangulated using 2D poses from 62 synchronized cameras (<xref ref-type="fig" rid="F1">Figure 1H</xref>). After training LiftPose3D with only 6’571 3D poses, we could lift 3D poses from test images with diverse animal poses (<xref ref-type="supplementary-material" rid="SD10">Video 4</xref>), acquired from any camera (<xref ref-type="fig" rid="F1">Figure 1I</xref>), and with relatively low body length-normalized MAE (<xref ref-type="fig" rid="F1">Figure 1J</xref>).</p>
      <p id="P11">Taken together, these results demonstrate that, using simple data preprocessing and a relatively small but diverse training dataset, LiftPose3D can reduce the number of cameras required to perform accurate 3D pose estimation.</p>
    </sec>
    <sec id="S4">
      <label>2.2</label>
      <title>Predicting 3D pose despite occluded keypoints</title>
      <p id="P12">In freely behaving animals, keypoints are often missing from certain camera angles due to self-occlusions and, therefore, only partial 3D ground truth can be obtained by triangulation. We asked how the global nature of lifting—all keypoints are lifted simultaneously—might be leveraged to reconstruct information lost by occlusions, allowing one to predict full 3D poses.</p>
      <p id="P13">To address this question, we built an experimental system similar to others used for flies and mice [<xref rid="R14" ref-type="bibr">14</xref>, <xref rid="R35" ref-type="bibr">35</xref>, <xref rid="R36" ref-type="bibr">36</xref>] that consisted of a transparent enclosure coupled to a right-angle prism mirror and with a camera beneath to record ventral and side views of a freely behaving fly (<xref ref-type="fig" rid="F2">Figure 2A</xref>). Due to the right-angle prism and the long focal length camera (i.e., negligible perspective effects), the ventral and side views are orthographic projections of the true 3D pose. Triangulation thus consisted of estimating the z-axis depth of keypoints from the side view. Although keypoints closer to the prism were simultaneously visible in both views and could be triangulated, other joints had only ventral 2D information. We therefore aligned flies in the same reference frame in the ventral view (<xref ref-type="fig" rid="F2">Figure 2B</xref>), turning lifting into a regression problem similar to that for tethered animals. During training we took ventral view 2D poses as inputs, but penalized only those keypoints with complete 3D information. By also aligning these data, we found that the network could implicitly augment unseen coordinates by learning geometric relationships between keypoints. The network could predict 3D positions for every joint at test time, including those occluded in the side view (<xref ref-type="fig" rid="F2">Figure 2D</xref> and <xref ref-type="supplementary-material" rid="SD11">Video 5</xref>). Notably, owing to the high spatial resolution of this setup, the accuracy, based on available triangulation-derived 3D positions (<xref ref-type="fig" rid="F2">Figure 2E</xref>), was better than that obtained for tethered flies triangulated using four cameras (<xref ref-type="fig" rid="F1">Figure 1E</xref>). Thus, LiftPose3D can estimate 3D poses from 2D images in cases where keypoints are occluded and cannot be triangulated.</p>
      <p id="P14">These results suggested an opportunity to apply lifting to potentially correct inaccurate 3D poses obtained using other tracking approaches. To test this, we used a dataset consisting of freely behaving mice traversing a narrow corridor [<xref rid="R14" ref-type="bibr">14</xref>] and tracked using the LocoMouse software from ventral and side views [<xref rid="R14" ref-type="bibr">14</xref>]. We triangulated and aligned incomplete 3D ground truth poses as we did for <italic>Drosophila</italic> and then trained a LiftPose3D network using ventral 2D poses as inputs. Predictions were in good agreement with the LocoMouse’s side view tracking (<xref ref-type="fig" rid="F2">Figure 2E</xref> and <xref ref-type="supplementary-material" rid="SD12">Video 6</xref>) and could recover expected cycloid-like kinematics between strides (<xref ref-type="fig" rid="F2">Figure 2F</xref>). Remarkably, LiftPose3D predictions could also correct poorly labeled or missing side-view poses (<xref ref-type="fig" rid="F2">Figure 2F</xref>, bottom, white arrowheads). However, lifting accuracy depended on the fidelity of input 2D poses: incorrect ventral 2D poses generated false side view predictions (<xref ref-type="fig" rid="F2">Figure 2F</xref>, bottom, white asterisks). These errors were always localized to the joint-of-interest and were relatively infrequent. Overall, LiftPose3D and LocoMouse performed similarly compared with manual human annotation (<xref ref-type="fig" rid="F2">Figure 2G</xref>) demonstrating that LiftPose3D can be used to test the consistency of ground truth datasets.</p>
      <p id="P15">To assess how well spatial relationships learned by LiftPose3D could generalize to animals with more complex behaviors and larger variations in body proportions, we next considered the CAPTURE dataset of six cameras recording freely behaving rats within a circular arena [<xref rid="R37" ref-type="bibr">37</xref>] (<xref ref-type="fig" rid="F2">Figure 2H</xref>, left). Animal poses were intermittently self-occluded during a variety of complex behaviors (<xref ref-type="fig" rid="F2">Figure 2I</xref>). Therefore, to allow the network to learn the skeletal geometry, we aligned animals in the camera-coordinate frame and replaced missing input data with zeros. Furthermore, to make the network robust to bone length variability within and across animals (<xref ref-type="fig" rid="F2">Figure 2J</xref>) we assumed that bone lengths were normally distributed and generated, for each triangulated 3D pose, rescaled 3D poses by sampling from bone-length distributions while preserving joint angles. Then, we obtained corresponding 2D poses via a virtual projection within the Euler angle range of ±10° with respect to the known camera locations (to augment the range of camera-to-animal angles). Finally, we normalized 2D poses by their Frobenius norm, as before, assuming a large enough camera-to-animal distance.</p>
      <p id="P16">To show that the network generalizes across new experimental setups, we used two experiments from this dataset (i.e., two animals and two camera arrangements) for training and tested with a third experiment (a different animal, camera focal length, and animal-to-camera distance). By replacing low confidence or missing coordinates with zeros, LiftPose3D could accurately predict the nonzero coordinates (<xref ref-type="fig" rid="F2">Figure 2H, K</xref> and <xref ref-type="supplementary-material" rid="SD13">Video 7</xref>). Thus, this is a viable way to correct for erroneous input keypoints and makes our network directly applicable to other rat movement studies.</p>
    </sec>
    <sec id="S5">
      <label>2.3</label>
      <title>Lifting diverse experimental data without 3D ground truth</title>
      <p id="P17">Although our angle-invariant networks for lifting 3D poses in tethered flies (<xref ref-type="fig" rid="F1">Figure 1D-F</xref>) and freely behaving rats (<xref ref-type="fig" rid="F2">Figure 2H-K</xref>) can already be used in similar experimental systems without the need for additional training data, small variations resulting from camera distortions or postural differences may limit the accuracy of lifted poses. Therefore, we explored how domain adaptation might enable pretrained networks to lift poses in new experimental systems despite small postural variations.</p>
      <p id="P18">We assessed the possibility of domain adaptation by training a network in domain <italic>A</italic>—tethered flies—and predicting 3D poses in domain <italic>B</italic>—freely-moving flies (<xref ref-type="fig" rid="F3">Figure 3A</xref>). To do so, we identified two linear transformations <italic>d</italic>
<sub>2</sub> and <italic>d</italic>
<sub>3</sub>. <italic>d</italic>
<sub>2</sub> is used to map 2D poses from domain <italic>B</italic> as inputs to the pre-trained network in domain <italic>A</italic>, while <inline-formula><mml:math id="M3"><mml:mrow><mml:msubsup><mml:mi>d</mml:mi><mml:mn>3</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is used to transform lifted 3D poses back to domain <italic>B</italic>. These linear transformations were found as best-fit mappings from every pose in a training dataset <italic>B</italic>’ to their <italic>k</italic> nearest neighbors <italic>A</italic>’ (<xref ref-type="fig" rid="F3">Figure 3B</xref>). They are expected to generalize as long as the poses in domain <italic>A</italic> are rich enough to cover the pose repertoire in domain <italic>B</italic> and are sufficiently similar between domains. We found by 10-fold cross-validation that the error associated with the transformations converged after less than 500 poses (<xref ref-type="fig" rid="F3">Figure 3C</xref>). The final lifted poses were also in good agreement with the triangulated poses in domain <italic>B</italic> (<xref ref-type="fig" rid="F3">Figure 3D</xref>) having accuracies comparable to a network lifting purely in domain <italic>A</italic> (<xref ref-type="fig" rid="F3">Figure 3E</xref>, compare dark with light gray).</p>
      <p id="P19">To demonstrate the full potential of linear domain adaptation, we next lifted <italic>Drosophila</italic> 2D poses from a single ventral camera. This experimental system is common due to its simplicity, low cost, and increased throughput and has been used to study <italic>C. elegans</italic> [<xref rid="R38" ref-type="bibr">38</xref>], larval zebrafish [<xref rid="R39" ref-type="bibr">39</xref>], larval <italic>Drosophila</italic> [<xref rid="R40" ref-type="bibr">40</xref>], adult <italic>Drosophila</italic> [<xref rid="R41" ref-type="bibr">41</xref>], and mice [<xref rid="R42" ref-type="bibr">42</xref>]. Because depth sensors [<xref rid="R43" ref-type="bibr">43</xref>, <xref rid="R44" ref-type="bibr">44</xref>] cannot resolve small laboratory animals, 3D pose estimation from a single 2D view remains unsolved, but has the potential to enrich behavioral datasets and improve downstream analysis.</p>
      <p id="P20">We developed an experimental system with a square-shaped arena in which multiple freely-behaving flies were recorded ventrally using a single camera (<xref ref-type="fig" rid="F3">Figure 3F</xref>, left) at four-fold lower spatial resolution (26 px mm<sup>−1</sup>) than in our prism-mirror system. We pretrained a network using prism-mirror training data for keypoints present in both datasets and then augmented these data using a Gaussian noise term with standard deviation of ~ 4. We adapted annotated 2D poses into the network’s domain before lifting (<xref ref-type="fig" rid="F3">Figure 3B</xref>). We found that the network could predict physiologically realistic 3D poses in this new dataset using only ventral 2D poses (<xref ref-type="fig" rid="F3">Figure 3G</xref> and <xref ref-type="supplementary-material" rid="SD14">Video 8</xref>). This is remarkable because ventrally-viewed swing and stance phases are difficult to distinguish, particularly at lower resolution. During walking, 2D tracking of the tarsal claws traced out stereotypical trajectories in the x-y plane (<xref ref-type="fig" rid="F3">Figure 3H</xref>, top) [<xref rid="R45" ref-type="bibr">45</xref>] and circular movements in the unmeasured x-z plane (<xref ref-type="fig" rid="F3">Figure 3H</xref>, bottom) whose amplitudes were consistent with real kinematic measurements during forward walking [<xref rid="R46" ref-type="bibr">46</xref>].</p>
      <p id="P21">Another exciting possibility offered by LiftPose3D is to ’resurrect’ previously published 2D pose data for new 3D kinematic analyses. We applied our network that was trained on prism-mirror data to lift published video data of a fly walking through a capsule-shaped arena [<xref rid="R16" ref-type="bibr">16</xref>] (<xref ref-type="fig" rid="F3">Figure 3I</xref>). Using a similar processing pipeline as before (<xref ref-type="fig" rid="F3">Figure 3B,F,G</xref>), including registration and domain adaptation but not noise perturbations (the target data were of similarly high resolution as the training data), LiftPose3D could predict 3D poses from this dataset (<xref ref-type="fig" rid="F3">Figure 3J</xref>). We again observed physiologically realistic cyclical movements of the pretarsi during forward walking (<xref ref-type="fig" rid="F3">Figure 3K</xref>, bottom; <xref ref-type="supplementary-material" rid="SD15">Video 9</xref>). These data illustrate that linear domain-adaptation and LiftPose3D can be combined to lift 3D poses from previously published 2D video data for which 3D triangulation would be otherwise impossible.</p>
    </sec>
    <sec id="S6">
      <label>2.4</label>
      <title><italic>Drosophila</italic> LiftPose3D station</title>
      <p id="P22">These domain adaptation results suggested that one could make 3D pose acquisition cheaper and more accessible by designing a <italic>“Drosophila</italic> LiftPose3D station”—an inexpensive (~$150) open-source hardware system including a 3D printed rig supporting a rectangular arena (<xref ref-type="fig" rid="F6">Extended Data Figure 3</xref>, <xref ref-type="supplementary-material" rid="SD6">Supplementary Note 1</xref>). A common hardware solution like this overcomes potential variability across different experimental systems that arise from camera distortions and perspective effects. Using pre-trained DeepLabCut and LiftPose3D networks we found that one can effectively lift <italic>Drosophila</italic> 3D poses with this system (<xref ref-type="supplementary-material" rid="SD16">Video 10</xref>). We envision that a similar low-cost approach might, in the future, also be taken to facilitate cross-laboratory 3D lifting of mouse 2D poses from a single camera.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="S7">
    <label>3</label>
    <title>Discussion</title>
    <p id="P23">Here we have introduced LiftPose3D, a deep learning-based tool that dramatically simplifies 3D pose estimation across a wide variety of laboratory contexts. LiftPose3D can take as inputs 2D poses from any of a variety of annotation softwares [<xref rid="R2" ref-type="bibr">2</xref>, <xref rid="R3" ref-type="bibr">3</xref>]. Through input data preprocessing, training augmentation, and domain adaptation one can train a lifting network [<xref rid="R30" ref-type="bibr">30</xref>] with several orders of magnitude less data as well as incomplete or innacurate ground truth poses. LiftPose3D is invariant to camera hardware and positioning, making it possible to use the same networks across laboratories and experimental systems. We provide an intuitive Python notebook that serves as an interface for data preprocessing, network training, 3D predictions, and data visualization.</p>
    <p id="P24">Several factors must be considered when optimizing LiftPose3D for new experimental systems. First, because predicting depth from a 2D projection depends on comparing the lengths of body parts, input poses must be sufficiently well-resolved to discriminate between 3D poses with similar 2D projections. Second, prediction accuracy depends on training data diversity: previously untrained behaviors may not be as accurately lifted. Further work may improve LiftPose3D by constraining 3D poses using body priors [<xref rid="R47" ref-type="bibr">47</xref>–<xref rid="R51" ref-type="bibr">51</xref>] and temporal information [<xref rid="R31" ref-type="bibr">31</xref>].</p>
    <p id="P25">Using our domain adaptation methodology, networks with the largest and most diverse training data, like those for the tethered fly, may be sufficiently robust to accurately lift 2D to 3D pose in other laboratories. In the future, similarly robust lifting networks might be generated for other animals through a cross-laboratory aggregation of diverse 3D pose ground truth datasets. In summary, LiftPose3D can accelerate 3D pose estimation in laboratory research by reducing the need for complex and expensive synchronized multi-camera systems, and arduous calibration procedures. This, enables the acquisition of rich behavioral data and can accelerate our understanding of the neuromechanical control of behavior.</p>
  </sec>
  <sec sec-type="materials | methods" id="S8" specific-use="web-only">
    <label>10</label>
    <title>Materials and Methods</title>
    <sec id="S9">
      <label>10.1</label>
      <title>Theoretical basis for LiftPose3D</title>
      <p id="P26">LiftPose3D aims to estimate the 3D pose <bold>X</bold> = (<bold>X</bold>
<sub>1</sub>, …, <bold>X</bold>
<sub><italic>n</italic></sub>), i.e., an ensemble of keypoints, by learning a nonlinear mapping between triangulated ground truth 3D poses and corresponding 2D poses <bold>x</bold>
<sub><italic>c</italic></sub> = (<bold>x</bold>
<sub><italic>c</italic>,1</sub>, …, <bold>x</bold>
<sub><italic>c,n</italic></sub>). Formally, this operation is encoded in a <italic>lifting</italic> function <italic>f</italic> mapping a 2D pose from any camera c to their corresponding 3D pose in camera-centered coordinates, <bold>Y</bold>
<sub><italic>c</italic></sub> = <italic>f</italic>(<bold>x</bold>
<sub><italic>c</italic></sub>), and a camera transformation ϕ<sub><italic>c</italic></sub>, encoding a rotation and translation operation (see <xref ref-type="disp-formula" rid="FD2">Eq. (2)</xref>), mapping from camera-centered coordinates to world coordinates <inline-formula><mml:math id="M4"><mml:mrow><mml:mtext mathvariant="bold">X</mml:mtext><mml:mo>=</mml:mo><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mtext mathvariant="bold">Y</mml:mtext><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The lifting function <italic>f</italic> can be approximated using a deep neural network <italic>F</italic>(x<sub><italic>c</italic></sub>; Θ), where Θ represents the network weights controlling the behavior of <italic>F</italic>. In a specific application, Θ are trained by minimizing the discrepancy between 3D poses predicted by lifting from any camera and ground truth 3D poses, <disp-formula id="FD1"><label>(1)</label><mml:math id="M5"><mml:mrow><mml:msub><mml:mtext mathvariant="script">J</mml:mtext><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>Θ</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>≔</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mi>c</mml:mi></mml:munder><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>χ</mml:mi><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mtext>x</mml:mtext><mml:mi>c</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mo>Θ</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mtext mathvariant="bold">Y</mml:mtext><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula> where <inline-formula><mml:math id="M6"><mml:mrow><mml:msub><mml:mi>χ</mml:mi><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is an indicator function of the set <italic>V<sub>c</sub></italic> of visible points from camera <italic>c</italic>. For <italic>F</italic>(x<sub><italic>c</italic></sub>; Θ), we adapted a network architecture from [<xref rid="R57" ref-type="bibr">57</xref>] composed of fully connected layers regularized by batch-norm and dropout [<xref rid="R58" ref-type="bibr">58</xref>] and linked with skip connections (<xref ref-type="fig" rid="F1">Figure 1B</xref>). This network was developed to perform human-pose estimation following training on approximately 10<sup>6</sup> fully annotated 2D-3D human pose pairs for many different behaviors. We demonstrate that training augmentation methods allow this network to (i) work with a vastly smaller training dataset (between 10<sup>3</sup>-10<sup>4</sup> poses acquired automatically using 2D pose estimation approaches [<xref rid="R52" ref-type="bibr">52</xref>, <xref rid="R59" ref-type="bibr">59</xref>]), (ii) predict 3D poses from a single camera view at arbitrary angles, (iii) be trained with only partially annotated ground truth 3D poses suffering from occlusions, and (iv) generalize a single pretrained network across experimental systems and domains by linear domain adaptation.</p>
      <p id="P27">Note that our approach implicitly assumes that the network learns two operations: lifting the 2D pose x<sub><italic>c</italic></sub> to camera-centered 3D coordinates Y<sub><italic>c</italic></sub> by predicting the depth component of the pose, and learning perspective effects encoded in the animal-to-camera distance and the intrinsic camera matrix (see <xref ref-type="disp-formula" rid="FD2">Eqs. (2)</xref>–(<xref ref-type="disp-formula" rid="FD4">5</xref>)). Notably, the intrinsic camera matrix is camera-specific, suggesting that a trained network can only lift poses from cameras used during training and that application to new settings with strong perspective effects (short focal lengths) may require camera calibration. We show that this is not necessarily the case and that one can generalize pre-trained networks to new settings by weakening perspective effects. This can be accomplished by either using a large focal length camera, or by increasing the animal-to-camera distance and normalizing the scale of 2D poses. We demonstrate that a weak perspective assumption can, in many practical scenarios, enable lifting 2D poses from different cameras without calibration. These contributions enable 3D pose estimation in otherwise inaccessible experimental scenarios.</p>
    </sec>
    <sec id="S10">
      <label>10.2</label>
      <title>Obtaining 3D pose ground truth data by triangulation</title>
      <p id="P28">Triangulated 3D positions served as ground truth data for assessing the accuracy of LiftPose3D. If a keypoint <italic>j</italic> of interest is visible from at least two cameras, with corresponding 2D coordinates <inline-formula><mml:math id="M7"><mml:msub><mml:mtext mathvariant="bold">x</mml:mtext><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> in camera <italic>c</italic> and camera parameters (extrinsic and intrinsic matrices), then its 3D coordinates <inline-formula><mml:math id="M8"><mml:msub><mml:mtext mathvariant="bold">X</mml:mtext><mml:mi>j</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mn>3</mml:mn></mml:msup></mml:math></inline-formula> in a global world reference frame can be obtained by triangulation. Let us express <inline-formula><mml:math id="M9"><mml:msub><mml:mtext mathvariant="bold">X</mml:mtext><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mn>3</mml:mn></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> in homogeneous coordinates as <inline-formula><mml:math id="M10"><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold">X</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mn>3</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. The projection from the 3D points in the global coordinate system to 2D points in a local coordinate system centered on camera c is performed by the function <inline-formula><mml:math id="M11"><mml:msub><mml:mi>π</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mn>4</mml:mn></mml:msup><mml:mo>→</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mn>3</mml:mn></mml:msup></mml:math></inline-formula> defined as <inline-formula><mml:math id="M12"><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold">X</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. This function can be expressed as a composition <italic>π<sub>c</sub></italic> = proj<sub>1,2</sub> ∘ <italic>ϕ<sub>c</sub></italic> of an affine transformation <inline-formula><mml:math id="M13"><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mn>4</mml:mn></mml:msup><mml:mo>→</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mn>4</mml:mn></mml:msup></mml:math></inline-formula> from global coordinates to camera-centered coordinates and a projection <inline-formula><mml:math id="M14"><mml:msub><mml:mtext>proj</mml:mtext><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mn>4</mml:mn></mml:msup><mml:mo>→</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mn>3</mml:mn></mml:msup></mml:math></inline-formula> to the first two coordinates. Both functions can be parametrized using the pinhole camera model [<xref rid="R61" ref-type="bibr">61</xref>]. On the one hand, we have <disp-formula id="FD2"><label>(2)</label><mml:math id="M15"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mtext mathvariant="bold">X</mml:mtext><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>≔</mml:mo><mml:msub><mml:mtext mathvariant="bold">C</mml:mtext><mml:mi>c</mml:mi></mml:msub><mml:msubsup><mml:mover accent="true"><mml:mtext mathvariant="bold">X</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mi>j</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold">Y</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where C<sub><italic>c</italic></sub> is the extrinsic camera matrix corresponding to the <italic>ϕ<sub>c</sub></italic> and can be written as <fig id="F7" orientation="portrait" position="anchor"><graphic xlink:href="EMS129592-f007"/></fig> where <inline-formula><mml:math id="M16"><mml:mrow><mml:msub><mml:mtext mathvariant="bold">R</mml:mtext><mml:mi>c</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is a matrix corresponding to rotation around the origin and <inline-formula><mml:math id="M17"><mml:mrow><mml:msub><mml:mtext mathvariant="bold">T</mml:mtext><mml:mi>c</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> is a translation vector representing the distance of the origin of the world coordinate system to the camera center. Likewise, the projection function can be expressed as <disp-formula id="FD3"><label>(4)</label><mml:math id="M18"><mml:mrow><mml:msub><mml:mrow><mml:mtext>proj</mml:mtext></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold">Y</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>≔</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="bold">K</mml:mtext><mml:mover accent="true"><mml:mtext mathvariant="bold">Y</mml:mtext><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <bold>K</bold> is the intrinsic camera transformation <disp-formula id="FD4"><label>(5)</label><mml:math id="M19"><mml:mrow><mml:mtext mathvariant="bold">K</mml:mtext><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>f<sub>x</sub>, f<sub>y</sub></italic> denote the focal lengths and <italic>c<sub>x</sub>, c<sub>y</sub></italic> denote the image center. The coordinates projected to the camera plane can be obtained by converting back to Euclidean coordinates <inline-formula><mml:math id="M20"><mml:mrow><mml:msub><mml:mtext mathvariant="bold">x</mml:mtext><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msubsup></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:msubsup></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
      <p id="P29">Triangulation of the coordinate <bold>X</bold>
<sub><italic>j</italic></sub> of joint <italic>j</italic> with respect to <italic>π<sub>c</sub></italic> is obtained by minimizing the reprojection error, that is, the discrepancy between the 2D camera coordinate, <bold>x</bold>
<sub><italic>c,j</italic></sub>, and the 3D coordinate projected to the camera frame, <italic>π<sub>c</sub></italic>(<bold>X</bold>
<sub><italic>j</italic></sub>). Let <italic>V<sub>c</sub></italic> be the set of visible joints from camera <italic>c</italic>. The reprojection error for joint <italic>j</italic> is taken to be <disp-formula id="FD5"><label>(6)</label><mml:math id="M21"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mtext>RP</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>;</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mi>c</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>χ</mml:mi><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:msub><mml:mtext mathvariant="bold">x</mml:mtext><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mtext mathvariant="bold">X</mml:mtext><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>χ<sub>V<sub>c</sub></sub></italic> (·) is the indicator function of set <italic>V<sub>c</sub></italic> of visible keypoints from camera <italic>c</italic>. The camera projection functions <italic>π<sub>c</sub></italic> are initially unknown. To avoid having to use a calibration grid, we jointly minimize with respect to the 3D location of all joints and to the camera parameters, a procedure known as bundle adjustment [<xref rid="R61" ref-type="bibr">61</xref>]. Given a set of 2D observations, we seek <disp-formula id="FD6"><label>(7)</label><mml:math id="M22"><mml:mrow><mml:munder><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mtext mathvariant="bold">X</mml:mtext><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mi>j</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mtext>RP</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>;</mml:mo><mml:mspace width="0.2em"/><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula> using a second-order optimization method. For further details, we refer the interested reader to [<xref rid="R3" ref-type="bibr">3</xref>].</p>
    </sec>
    <sec id="S11">
      <label>10.3</label>
      <title>LiftPose3D network architecture and optimization</title>
      <p id="P30">The core LiftPose3D network architecture is similar to the one of [<xref rid="R57" ref-type="bibr">57</xref>] and is depicted in <xref ref-type="fig" rid="F1">Figure 1B</xref>. Its main module includes two linear layers of dimension 1024 rectified linear units (ReLU [<xref rid="R62" ref-type="bibr">62</xref>]), dropout [<xref rid="R58" ref-type="bibr">58</xref>] and residual connections [<xref rid="R63" ref-type="bibr">63</xref>]. The inputs and outputs of each block are connected during each forward pass using a skip connection. The model contains 4 × 10<sup>6</sup> trainable parameters, which are optimized by stochastic gradient descent using the Adam optimizer [<xref rid="R64" ref-type="bibr">64</xref>]. We also perform batch normalization [<xref rid="R65" ref-type="bibr">65</xref>].</p>
      <p id="P31">In all cases, the parameters were set using Kaiming initialization [<xref rid="R63" ref-type="bibr">63</xref>] and the optimizer was run until convergence—typically within 30 epochs—with the following training hyperparameters: Batch-size of 64 and an initial learning rate of 10<sup>–3</sup> that was dropped by 4% every 5000 steps. We implemented our network in PyTorch on a desktop workstation running on an Intel Core i9-7900X CPU with 32 GB of DDR4 RAM, and a GeForce RTX 2080 Ti Dual O11G GPU. Training time was less than 10 minutes for all cases studied.</p>
    </sec>
    <sec id="S12">
      <label>10.4</label>
      <title>Weak perspective augmentation</title>
      <p id="P32">To project 2D poses from 3D poses, one needs to know the camera transformation <italic>ϕ<sub>c</sub></italic> (<xref ref-type="disp-formula" rid="FD2">Eq. (2)</xref>), encoded by the extrinsic matix <bold>C</bold>
<sub><italic>c</italic></sub> (<xref ref-type="fig" rid="F7">Eq. (3)</xref>) and the projection function projļ <sub>2</sub> (<xref ref-type="disp-formula" rid="FD3">Eq. (4)</xref>), encoded by the intrinsic matrix <bold>K</bold> (<xref ref-type="disp-formula" rid="FD4">Eq. (5)</xref>). In the previous section, we described how to deal with the case when <bold>C</bold>
<sub><italic>c</italic></sub> is unknown. In addition, <bold>K</bold> may also be unknown <italic>a priori</italic> at test time. Alternatively, one may want to use one of our pre-trained networks on a novel dataset without having to match the camera positioning (focal length, camera-to-animal distance) used to collect the training data. In this case, one may still be able to predict the 3D pose in a fixed camera-centered coordinate frame by assuming that either the camera-to-animal distance or the focal length are large enough to neglect perspective effects and by normalizing the scale of 2D poses. Following Ref. [<xref rid="R60" ref-type="bibr">60</xref>], we chose the Frobenius norm to perform normalization on the input 2D poses x<sub><italic>c,j</italic></sub>/‖x<sub><italic>c,j</italic></sub>‖<sub><italic>F</italic></sub>, which is the diagonal distance of the smallest bounding box around the 2D pose. Note, that if the 2D poses are obtained via projections, one may use the unit intrinsic matrix <xref ref-type="disp-formula" rid="FD4">Eq. (5)</xref> with <italic>f<sub>x</sub></italic> = <italic>f<sub>y</sub></italic> and <italic>c<sub>x</sub></italic> = <italic>c<sub>y</sub></italic> = 0 before performing normalization. Here, using <italic>c<sub>x</sub></italic> = <italic>c<sub>y</sub></italic> =0 assumes that the 2D poses are centered, which in each of our examples is achieved by considering coordinates relative to root joints placed at the origin. Importantly, the 2D poses must be normalized both at training and test times.</p>
    </sec>
    <sec id="S13">
      <label>10.5</label>
      <title>Camera-angle augmentation</title>
      <p id="P33">The object-to-camera orientation is encoded by the extrinsic matrix <bold>C</bold>
<sub><italic>c</italic></sub> of <xref ref-type="fig" rid="F7">Eq. (3)</xref>. When it is unavailable, one can still use our framework by taking 3D poses from the ground truth library and, during training, performing virtual 2D projections around the approximate camera location or for all possible angles. To this end, we assume that the rotation matrix <bold>R</bold> is unknown, but that the intrinsic matrix <bold>K</bold> and the object-to-camera distance d are known such that we may take <bold>T</bold> = (0,0, <italic>d</italic>)<sup><italic>T</italic></sup>. When <bold>K</bold> or <italic>d</italic> are also unknown, or dynamically changing, one can make the weak-perspective assumption as described in the next section. Then, instead of training the LiftPose3D network with pairs of 3D poses and 2D poses at fixed angles, we perform random 2D projections of the 3D pose to obtain virtual camera planes whose centers <italic>c<sub>x</sub>, c<sub>y</sub></italic> lie on the sphere of radius <italic>d</italic>. To define the projections we require a parametric representation of the rotations. Rotating a point in 3D space can be achieved using three consecutive rotations around the three Cartesian coordinate axes <italic>x, y, z</italic> commonly referred to as Euler angles and denoted by <italic>ψ<sub>x</sub>, ψ<sub>y</sub></italic>, and <italic>ψ<sub>y</sub></italic>. The rotation matrix can then be written as <disp-formula id="FD7"><label>(8)</label><mml:math id="M23"><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mtext mathvariant="bold">R</mml:mtext><mml:mo>=</mml:mo><mml:msub><mml:mtext mathvariant="bold">R</mml:mtext><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mtext mathvariant="bold">R</mml:mtext><mml:mi>x</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msub><mml:mtext mathvariant="bold">R</mml:mtext><mml:mi>y</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msub><mml:mtext mathvariant="bold">R</mml:mtext><mml:mi>z</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mspace width="0.1em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mtext>cos</mml:mtext><mml:mspace width="0.2em"/><mml:msub><mml:mi>ψ</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mtext>sin</mml:mtext><mml:mspace width="0.2em"/><mml:msub><mml:mi>ψ</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mtext>sin</mml:mtext><mml:mspace width="0.2em"/><mml:msub><mml:mi>ψ</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>cos</mml:mtext><mml:mspace width="0.2em"/><mml:msub><mml:mi>ψ</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mtext>cos</mml:mtext><mml:mspace width="0.2em"/><mml:msub><mml:mi>ψ</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mtext>sin</mml:mtext><mml:mspace width="0.2em"/><mml:msub><mml:mi>ψ</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mtext>sin</mml:mtext><mml:mspace width="0.2em"/><mml:msub><mml:mi>ψ</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mtext>cos</mml:mtext><mml:mspace width="0.2em"/><mml:msub><mml:mi>ψ</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mtext>cos</mml:mtext><mml:mspace width="0.2em"/><mml:msub><mml:mi>ψ</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mtext>sin</mml:mtext><mml:mspace width="0.2em"/><mml:msub><mml:mi>ψ</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext>sin</mml:mtext><mml:mspace width="0.2em"/><mml:msub><mml:mi>ψ</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>cos</mml:mtext><mml:mspace width="0.2em"/><mml:msub><mml:mi>ψ</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
</p>
      <p id="P34">Given <xref ref-type="disp-formula" rid="FD2">Eq. (2)</xref>–(<xref ref-type="disp-formula" rid="FD4">5</xref>) we may then define a random projection <inline-formula><mml:math id="M24"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> on the sphere of radius <italic>d</italic> of a keypoint with homogeneous coordinate <inline-formula><mml:math id="M25"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold">X</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> as <fig id="F8" orientation="portrait" position="anchor"><graphic xlink:href="EMS129592-f008"/></fig> where <bold>T</bold> = (0, 0, <italic>d</italic>)<sup><italic>T</italic></sup>. Likewise, the 3D pose in camera coordinates can be expressed as <fig id="F9" orientation="portrait" position="anchor"><graphic xlink:href="EMS129592-f009"/></fig>
</p>
      <p id="P35">Before training, we fix <italic>d, f<sub>x</sub>, f<sub>y</sub>, c<sub>y</sub>, c<sub>y</sub></italic> and define intervals for the Euler angle rotations. We then obtain the mean and standard deviation in each dimension for both 2D and 3D poses in the training dataset by performing random projections within these angle ranges. The obtained means and standard deviations are then used to normalize both the training and test datasets.</p>
    </sec>
    <sec id="S14">
      <label>10.6</label>
      <title>Linear domain adaptation</title>
      <p id="P36">Here we describe the process of adapting a network trained on data from experiment <italic>A</italic> to lift 2D poses in experiment <italic>B</italic>. Domain adaptation is also useful if the camera parameters or the distance from the camera are not known and the weak perspective assumption cannot be invoked. Before performing domain adaptation, we first estimate 2D poses from ventral images in domain <italic>B</italic>, as before. This allowed us to circumvent the difficulties arising from differences in appearance and illumination that are present in the more general image domain adaptation problem [?, 66]. Thus, adapting poses became a purely geometric problem of adjusting proportions and postural differences across domains.</p>
      <p id="P37">The basis for domain adaptation is to first find a function <italic>d</italic>
<sub>2</sub> : <italic>B</italic>|<sub>2</sub> → <italic>A</italic>|<sub>2</sub>, where <italic>A</italic>|<sub>2</sub> and <italic>B</italic>|<sub>2</sub> are restrictions of 3D poses in the two domains, to the corresponding 2n-dimensional spaces of 2D poses. This function maps poses in domain <italic>B</italic> to domain <italic>A</italic> and makes them compatible inputs for the network trained on poses in domain <italic>A</italic>. In the scenario that 3D data is available in domain <italic>B</italic>, we can also find a function <italic>d</italic>
<sub>3</sub> : <italic>B</italic> → <italic>A</italic> where <italic>A</italic> and <italic>B</italic> are 3<italic>n</italic>-dimensional spaces of 3D poses in the two experimental domains. After 3D poses have been obtained in domain <italic>A</italic>, we map these poses back to domain <italic>B</italic> by inverting this function.</p>
      <p id="P38">We now describe how to obtain the functions <italic>d</italic>
<sub>2</sub> and <italic>d</italic>
<sub>3</sub>, which we denote collectively as <italic>d</italic>. To find <italic>d</italic>, we assume that poses in domain <italic>B</italic> can be obtained by small perturbations of poses in domain <italic>A</italic>. This allows us to set up a matching between the two domains by finding nearest neighbor 2D poses in domain <italic>A</italic> for each 2D pose in domain <italic>B</italic>, <inline-formula><mml:math id="M26"><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mi>i</mml:mi><mml:mi>B</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mi>B</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We use 2D rather than 3D poses to find a match because 3D poses may not always be available in domain <italic>B</italic>. Moreover, the nearest poses in 3D space will necessarily be among the nearest poses in 2D space. Specifically, for each <inline-formula><mml:math id="M27"><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mi>i</mml:mi><mml:mi>B</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, we find a set of <italic>k</italic> nearest poses in domain <italic>A</italic>, <inline-formula><mml:math id="M28"><mml:msubsup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mtext mathvariant="script">N</mml:mtext><mml:msub><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mi>i</mml:mi><mml:mi>B</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:msubsup></mml:math></inline-formula> such that <inline-formula><mml:math id="M29"><mml:msub><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mtext mathvariant="script">N</mml:mtext><mml:msub><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mi>i</mml:mi><mml:mi>B</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mi>i</mml:mi><mml:mi>B</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mtext mathvariant="script">N</mml:mtext><mml:msub><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mi>i</mml:mi><mml:mi>B</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mi>i</mml:mi><mml:mi>B</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>. We then use these poses to learn a linear mapping <inline-formula><mml:math id="M30"><mml:msub><mml:mtext mathvariant="bold">W</mml:mtext><mml:mrow><mml:mi>B</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> from domain <italic>B</italic> to <italic>A</italic>, where <italic>n</italic> is the number of keypoints, as before. We can find this linear mapping by first defining a set of <italic>p</italic> training poses in domain <italic>B</italic>, <inline-formula><mml:math id="M31"><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mrow><mml:mtext>tr</mml:mtext></mml:mrow><mml:mi>B</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mn>1</mml:mn><mml:mi>B</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mi>p</mml:mi><mml:mi>B</mml:mi></mml:msubsup></mml:math></inline-formula> and writing <inline-formula><mml:math id="M32"><mml:msub><mml:mtext mathvariant="bold">W</mml:mtext><mml:mrow><mml:mi>B</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mrow><mml:mtext>tr</mml:mtext></mml:mrow><mml:mi>B</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mrow><mml:mtext>tr</mml:mtext></mml:mrow><mml:mi>A</mml:mi></mml:msubsup></mml:math></inline-formula>, where <inline-formula><mml:math id="M33"><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mrow><mml:mtext>tr</mml:mtext></mml:mrow><mml:mi>B</mml:mi></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>k</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="M34"><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mrow><mml:mtext>tr</mml:mtext></mml:mrow><mml:mi>A</mml:mi></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>k</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> with <italic>d</italic> = 2 or 3 are matrices defined according to <disp-formula id="FD8"><label>(11)</label><mml:math id="M35"><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mtext mathvariant="bold">W</mml:mtext><mml:mrow><mml:mi>B</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:munder><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo stretchy="false">|</mml:mo></mml:mtd><mml:mtd><mml:mspace width="0.1em"/></mml:mtd><mml:mtd><mml:mo stretchy="false">|</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mn>1</mml:mn><mml:mi>B</mml:mi></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mn>1</mml:mn><mml:mi>B</mml:mi></mml:msubsup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo stretchy="false">|</mml:mo></mml:mtd><mml:mtd><mml:mspace width="0.1em"/></mml:mtd><mml:mtd><mml:mo stretchy="false">|</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="true">︸</mml:mo></mml:munder><mml:mi>k</mml:mi></mml:munder><mml:mspace width="0.3em"/><mml:mo>⋯</mml:mo><mml:mspace width="0.3em"/><mml:munder><mml:munder><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo stretchy="false">|</mml:mo></mml:mtd><mml:mtd><mml:mspace width="0.1em"/></mml:mtd><mml:mtd><mml:mo stretchy="false">|</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mi>p</mml:mi><mml:mi>B</mml:mi></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mi>p</mml:mi><mml:mi>B</mml:mi></mml:msubsup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo stretchy="false">|</mml:mo></mml:mtd><mml:mtd><mml:mspace width="0.1em"/></mml:mtd><mml:mtd><mml:mo stretchy="false">|</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="true">︸</mml:mo></mml:munder><mml:mi>k</mml:mi></mml:munder></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2em"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext mathvariant="script">N</mml:mtext><mml:munder><mml:munder><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo stretchy="false">|</mml:mo></mml:mtd><mml:mtd><mml:mspace width="0.1em"/></mml:mtd><mml:mtd><mml:mo stretchy="false">|</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mn>1</mml:mn><mml:mi>B</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mtext mathvariant="script">N</mml:mtext><mml:msub><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mn>1</mml:mn><mml:mi>B</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo stretchy="false">|</mml:mo></mml:mtd><mml:mtd><mml:mspace width="0.1em"/></mml:mtd><mml:mtd><mml:mo stretchy="false">|</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="true">︸</mml:mo></mml:munder><mml:mi>k</mml:mi></mml:munder><mml:mspace width="0.1em"/><mml:mo>⋯</mml:mo><mml:mspace width="0.1em"/><mml:mtext mathvariant="script">N</mml:mtext><mml:munder><mml:munder><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo stretchy="false">|</mml:mo></mml:mtd><mml:mtd><mml:mspace width="0.1em"/></mml:mtd><mml:mtd><mml:mo stretchy="false">|</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mi>p</mml:mi><mml:mi>B</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mtext mathvariant="script">N</mml:mtext><mml:msub><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mi>p</mml:mi><mml:mi>B</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo stretchy="false">|</mml:mo></mml:mtd><mml:mtd><mml:mspace width="0.1em"/></mml:mtd><mml:mtd><mml:mo stretchy="false">|</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="true">︸</mml:mo></mml:munder><mml:mi>k</mml:mi></mml:munder></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p>
      <p id="P39">Transposing this linear equation yields the linear problem <inline-formula><mml:math id="M36"><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mrow><mml:mtext>tr</mml:mtext></mml:mrow><mml:mi>B</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mtext mathvariant="bold">W</mml:mtext><mml:mrow><mml:mi>B</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mrow><mml:mtext>tr</mml:mtext></mml:mrow><mml:mi>A</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:math></inline-formula>. Given that the <italic>p</italic> training poses are different, <inline-formula><mml:math id="M37"><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mrow><mml:mtext>tr</mml:mtext></mml:mrow><mml:mi>B</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> has linearly independent columns and this problem is overdetermined as long as <italic>kp</italic> &gt; <italic>dn</italic>. Thus, by least-squares minimization, we obtain <inline-formula><mml:math id="M38"><mml:msubsup><mml:mtext mathvariant="bold">W</mml:mtext><mml:mrow><mml:mi>B</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mrow><mml:mtext>tr</mml:mtext></mml:mrow><mml:mi>B</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mrow><mml:mtext>tr</mml:mtext></mml:mrow><mml:mi>B</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mrow><mml:mtext>tr</mml:mtext></mml:mrow><mml:mi>B</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mrow><mml:mtext>tr</mml:mtext></mml:mrow><mml:mi>A</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:math></inline-formula>.</p>
    </sec>
    <sec id="S15">
      <label>10.7</label>
      <title>Experimental systems and conditions</title>
      <p id="P40">All adult <italic>Drosophila melanogaster</italic> experiments were performed on female flies raised at 25°C on a 12 h light/dark cycle at 2-3 days post-eclosion (dpe). Before each experiment, wild-type (<italic>PR</italic>) animals were anaesthetized using CO<sub>2</sub> or in ice-cooled vials and left to acclimate for 10 min. DeepFly3D tethered fly data were taken from [<xref rid="R52" ref-type="bibr">52</xref>]. OpenMonkeyStudio macaque data were taken from [<xref rid="R53" ref-type="bibr">53</xref>]. LocoMouse mouse data were taken from [<xref rid="R54" ref-type="bibr">54</xref>]. CAPTURE rat data were taken from [<xref rid="R55" ref-type="bibr">55</xref>]. FlyLimbTracker freely-behaving fly data were taken from [<xref rid="R56" ref-type="bibr">56</xref>]. See these publications for detailed experimental procedures. For more information on the datasets including the number of keypoints, poses, animals, resolution, framerate we refer the reader to <xref rid="T1" ref-type="table">Table 1</xref>.</p>
      <sec id="S16">
        <label>10.7.1</label>
        <title>Freely behaving <italic>Drosophila</italic> recorded from two high-resolution views using one camera and a right-angle prism mirror</title>
        <p id="P41">We constructed a transparent arena coupled to a right-angle prism mirror [<xref rid="R35" ref-type="bibr">35</xref>, <xref rid="R69" ref-type="bibr">69</xref>]. The enclosed arena consists of three vertically stacked layers of 1/16” thick acrylic sheets laser-cut to be 15 mm long, 3 mm wide, and 1.6 mm high. The arena ceiling and walls were coated with Sigmacote (Sigma-Aldrich, Merck, Darmstadt, Germany) to discourage animals from climbing onto the walls and ceilings. One side of the enclosure was physically coupled to a right-angled prism (Thorlabs PS915). The arena and prism were placed on a kinematic mounting platform (Thorlabs KM100B/M), permitting their 3D adjustment with respect to a camera (Basler acA1920-150um) outfitted with a lens (Computar MLM3X-MP, Cary, NC USA). Data were acquired using the Basler Pylon software (pylon Application 1.2.0.8206, pylon Viewer 6.2.0.8206). The camera was oriented vertically upwards below the arena to provide two views of the fly: a direct ventral view, and an indirect, prism mirror-reflected side view. The arena was illuminated by four Infrared LEDs (Thorlabs, fibre-coupled LED M850F2 with driver LEDD1B T-Cube and collimator F810SMA-780): two from above and two from below. To elicit locomotor activity, the platform was acoustically and mechanically stimulated using a mobile phone speaker. Flies were then allowed to behave freely, without optogenetic stimulation.</p>
      </sec>
      <sec id="S17">
        <label>10.7.2</label>
        <title>Freely behaving <italic>Drosophila</italic> recorded from one ventral view at low-resolution</title>
        <p id="P42">We constructed a square arena consisting of three vertically stacked layers of 1/16” thick acrylic sheets laser-cut to be 30 mm long, 30 mm wide, and 1.6 mm high. This arena can house multiple flies at once, increasing throughput at the expense of spatial resolution (26 px mm<sup>–1</sup>). Before each experiment the arena ceiling was coated with 10 uL Sigmacote (Sigma-Aldrich, Merck, Darmstadt, Germany) to discourage animals from climbing onto the ceiling. A camera (pco.panda 4.2 M-USB-PCO, Gloor Instruments, Switzerland, with a Milvus 2/100M ZF.2 lens, Zeiss, Switzerland) was oriented with respect to a 45 ° mirror below the arena to capture a ventral view of the fly. An 850 nm infrared LED ring light (CCS Inc. LDR2-74IR2-850-LA) was placed above the arena to provide illumination. Although the experiment contained optogenetically elicited behaviors interspersed with periods of spontaneous behavior, here we focused only on spontaneously generated forward walking.</p>
        <p id="P43">The positions and orientations of individual flies were tracked using custom software including a modified version of Tracktor [<xref rid="R70" ref-type="bibr">70</xref>]. Using these data, a 138 × 138 px image was cropped around each fly and registered for subsequent analyses.</p>
      </sec>
    </sec>
    <sec id="S18">
      <label>10.8</label>
      <title>2D pose estimation</title>
      <p id="P44">DeepFly3D 2D poses were taken from [<xref rid="R52" ref-type="bibr">52</xref>]. OpenMonkeyStudio 2D poses were taken from [<xref rid="R53" ref-type="bibr">53</xref>]. CAPTURE 2D poses were taken from [<xref rid="R55" ref-type="bibr">55</xref>]. LocoMouse 2D poses were taken from [<xref rid="R54" ref-type="bibr">54</xref>]. See these publications for detailed 2D pose estimation procedures. In the prism-mirror setup, we split the data acquired from a single camera into ventral and side view images. We hand-annotated the location of all 30 leg joints (five joints per leg) on 640 images from the ventral view and up to 15 visible unilateral joints on 640 images of the side view. We used these manual annotations to train two separate DeepLabCut [<xref rid="R59" ref-type="bibr">59</xref>] 2D pose estimation networks (root-mean-squared errors for training and testing were 0.02 mm and 0.04 mm for ventral and side views, respectively). We ignored frames in which flies were climbing the enclosure walls (thus exhibiting large yaw and roll orientation angles). We also removed keypoints with &lt; 0.95 DeepLabCut confidence and higher than 10 px mismatch along the <italic>x</italic>-coordinate of ventral and side views. FlyLimbTracker data [<xref rid="R56" ref-type="bibr">56</xref>] was manually annotated. Images acquired in the new low-resolution ventral view setup were annotated using DeepLabCut [<xref rid="R59" ref-type="bibr">59</xref>] trained on 160 hand-annotated images. Due to the low resolution of images, the coxa-femur joints were not distinguishable. Therefore, we treated the thorax-coxa and coxa-femur joints as a single entity.</p>
    </sec>
    <sec id="S19">
      <label>10.9</label>
      <title>Training the LiftPose3D network</title>
      <p id="P45">An important step in constructing LiftPose3D training data is to choose r root joints (see the specific use cases below for how these root joints were selected), and a target set corresponding to each root joint. The location of joints in the target set are predicted relative to the root joint to ensure translation invariance of the 2D poses.</p>
      <p id="P46">The training dataset consisted of input-output pose pairs <inline-formula><mml:math id="M39"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mi>c</mml:mi><mml:mrow><mml:mtext>tr</mml:mtext></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msup><mml:mtext mathvariant="bold">X</mml:mtext><mml:mrow><mml:mtext>tr</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> with dimensionality equal to the number of keypoints visible from a given camera c minus the number of root joints <italic>r</italic>, namely <inline-formula><mml:math id="M40"><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mi>c</mml:mi><mml:mrow><mml:mtext>tr</mml:mtext></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="M41"><mml:msup><mml:mtext mathvariant="bold">X</mml:mtext><mml:mrow><mml:mtext>tr</mml:mtext></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula>. Then, the training data was standardized with respect to the mean and standard deviation of a given keypoint across all poses.</p>
      <sec id="S20">
        <label>10.9.1</label>
        <title>Tethered <italic>Drosophila melanogaster</italic>
</title>
        <p id="P47">Of the 38 original keypoints in Ref. [<xref rid="R52" ref-type="bibr">52</xref>], here we focused on the 30 leg joints. Specifically, for each leg we estimated 3D position for the thorax-coxa, coxa-femur, femur-tibia, and tibia-tarsus joints and the tarsal tips (claws). Thus, the training data consisted of input-output coordinate pairs for 24 joints (30 minus six thorax-coxa root joints) from all cameras. The training convergence is shown on <xref ref-type="fig" rid="F5">Extended Data Figure 2A</xref>).</p>
      </sec>
      <sec id="S21">
        <label>10.9.2</label>
        <title>Freely behaving macaque monkeys</title>
        <p id="P48">The OpenMonkeyStudio dataset [<xref rid="R53" ref-type="bibr">53</xref>] consists of images of freely behaving macaque monkeys inside a 2.45 × 2.45 × 2.75 m arena in which 62 cameras are equidistant horizontally at two heights along the arena perimeter. We extracted all five available experiments (7, 9, 9a, 9b and 11) for training and testing. Since 2D pose annotations were not available for all cameras, we augmented this dataset during training by projecting triangulated 3D poses onto cameras lacking 2D annotation using the provided camera matrix. We removed fisheye lens-related distortions of 2D poses using the provided radial distortion parameters. We normalized each 2D pose to unit length, by dividing it by its Euclidean norm as well as the 3D pose with respect to bone lengths to reduce the large scale variability of the OpenMonkeyStudio annotations (animals ranged between 5.5 and 12 kg). We set the neck as the root joint during training. We compare our absolute errors to the total body length, calculated as the sum of the mean lengths of the nose-neck, neck-hip, hip-knee, knee-foot joints pairs. Over multiple epochs, we observed rapid convergence of our trained network (<xref ref-type="fig" rid="F5">Extended Data Figure 2B</xref>).</p>
      </sec>
      <sec id="S22">
        <label>10.9.3</label>
        <title>Freely behaving mice and <italic>Drosophila</italic> recorded from two views using a right-angle mirror</title>
        <p id="P49">Freely behaving mouse data [<xref rid="R54" ref-type="bibr">54</xref>] consisted of recordings of animals traversing a 66.5 cm long, 4.5 cm wide, and 20 cm high glass corridor. A 45° mirror was used to obtain both ventral and side views with a single camera beneath the corridor. 2D keypoint positions were previously tracked using the LocoMouse software [<xref rid="R54" ref-type="bibr">54</xref>]. We considered six major keypoints—the four paws, the proximal tail, and the nose. Keypoint positions were taken relative to a virtual root keypoint placed on the ground midway between the nose and the tail. The networks were trained on partial ground truth data following pose alignment, as described in the main text. The networks for <italic>Drosophila</italic> and mouse training data converged within 30 and 10 training epochs (<xref ref-type="fig" rid="F5">Extended Data Figure 2C,D</xref>).</p>
      </sec>
      <sec id="S23">
        <label>10.9.4</label>
        <title>Freely behaving rat in a naturalistic enclosure</title>
        <p id="P50">The CAPTURE dataset contains recordings of freely behaving rats in a 2-foot diameter cylindrical enclosure video recorded using six cameras. Motion capture markers on the animal were tracked using a commercial motion capture acquisition program [<xref rid="R55" ref-type="bibr">55</xref>] to obtain 2D poses. Out of 20 possible joints, we limited our scope to the 15 joints that were not redundant and provided most of the information about the animal’s pose. The dataset includes 4 experiments recording 3 rats from two different camera setups. Before using LiftPose3D, we removed the distortion from 2D poses using radial distortion parameters provided by the authors. The CAPTURE dataset has many missing 3D pose instances which we handle by not computing the loss corresponding to these keypoints during back-propagation. We selected the neck joint as the single root joint and predicted all of the other joints with respect to this root joint. We observed that LiftPose3D converged within 15 training epochs (<xref ref-type="fig" rid="F5">Extended Data Figure 2E</xref>).</p>
      </sec>
      <sec id="S24">
        <label>10.9.5</label>
        <title>Freely behaving adult <italic>Drosophila melanogaster</italic> recorded from one ventral camera view</title>
        <p id="P51">For both the newly acquired low-resolution and previously published high-resolution [<xref rid="R56" ref-type="bibr">56</xref>] images of freely behaving flies taken using one ventral view camera, we trained a LiftPose3D network on partial ground truth data acquired from the prism mirror system. For the high-resolution data, we considered the thorax-coxa joints as roots. For the low resolution data, the coxa-femur joints were imperceptible. Therefore, the thorax-coxa joints were selected as roots. The training dataset consisted of coordinate pairs <inline-formula><mml:math id="M42"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mrow><mml:mtext>ventral</mml:mtext></mml:mrow><mml:mrow><mml:mtext>tr</mml:mtext></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mtext mathvariant="bold">z</mml:mtext><mml:mrow><mml:mtext>side</mml:mtext></mml:mrow><mml:mrow><mml:mtext>tr</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="M43"><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mrow><mml:mtext>ventral</mml:mtext></mml:mrow><mml:mrow><mml:mtext>tr</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M44"><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold">z</mml:mtext><mml:mrow><mml:mtext>side</mml:mtext></mml:mrow><mml:mrow><mml:mtext>tr</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> were chosen to represent the annotated ventral coordinates and z-axis depth for the visible joints, as before. Meanwhile, <italic>η</italic> was a zero-mean Gaussian noise term with a joint-independent standard deviation of 4 px. The role of this noise term was to account for the keypoint position degeneracy inherent in the transformation from high-resolution prism training data to lower-resolution testing data. For the high resolution dataset this noise term was set to zero.</p>
      </sec>
    </sec>
    <sec id="S25">
      <label>10.10</label>
      <title>Comparing joint angles derived from lifted 3D and 2D poses</title>
      <p id="P52">To illustrate the benefits of using lifted 3D coordinates versus 2D coordinates for kinematic analyis, we derived the joint angles obtained from 3D coordinates along with projected 2D coordinates. Consider the (2D or 3D) coordinates of three consecutive joints in the kinematic chain of one leg with coordinates u, v, w. Then, vectors s<sub>1</sub> = u – v and s<sub>2</sub> = u – w describe adjacent bones. Their enclosed angle is found by the cosine rule, cos<sup>–1</sup>(s<sub>1</sub> · s<sub>2</sub>/(‖s<sub>1</sub>‖ ‖s<sub>2</sub>‖)). Due to the uncertainty of 2D and 3D pose estimation, we assumed that keypoint coordinates are Gaussian distributed around the estimated coordinate. As a proxy for the variance we took the variation of bone lengths ||s<sub>1</sub>|| and ||s<sub>2</sub>|| because they are expected to remain approximately constant owing to the low mechanical compliance of the fly’s exoskeleton (with the exception of the flexible tarsal segments). This allowed us to predict 3D joint angles by Monte Carlo sampling (using 5 × 10<sup>3</sup> samples), drawing one sample from each of three distributions and then computing the corresponding joint angle by the cosine rule.</p>
      <p id="P53">The joint angles derived from lifted and triangulated 3D poses were in close agreement (<xref ref-type="fig" rid="F4">Extended Data Figure 1</xref>, red and blue). The errors were low when comparing angle estimate variances to the amount of joint rotation during locomotor cycles. This shows that that our network learned and preserved body proportions—a remarkable fact given the absence of any skeletal constraints, or temporal information. Furthermore, when comparing the joint angles derived from 3D and 2D poses, we found that the predicted coxa-femur 3D joint angles, <italic>β</italic>, in the front and hindlegs were of larger amplitude than <italic>β</italic>’, derived from projected 2D poses. This is expected since the action of these joints has a large out-of-plane component relative to the x-y plane during walking. In the front leg, the predicted tibia-tarsus 3D joint angles, <italic>ω</italic>, were of smaller amplitude than <italic>ω</italic>’. Indeed, rotations upstream in the kinematic chain (proximal joints) cause the movement of the whole leg, introducing spurious dynamics in the distal joints when viewed from a projected plane. These results illustrate that 3D poses predicted by LiftPose3D can decouple the underlying physical degrees-of-freedom and avoid spurious correlations introduced by 2D projected joint angles.</p>
    </sec>
  </sec>
  <sec sec-type="extended-data" id="S26">
    <title>Extended Data</title>
    <fig id="F4" orientation="portrait" position="anchor">
      <label>Extended data figure 1</label>
      <caption>
        <title>Joint angles resulting from lifting compared with 3D triangulated ground truth and 2D projections.</title>
        <p id="P54">Joint angles <italic>α, β, γ</italic>, and <italic>ω</italic> for the front, mid, and hind left legs during forward walking. Shown are angles computed from 3D triangulation using DeepFly3D (blue), LiftPose3D predictions (red), and ventral 2D projections <italic>α′, β′, γ′</italic>, and <italic>ω′</italic> (green). The mean (solid lines) and standard deviation of joint error distributions (transparency) are shown. Joint angles were computed by Monte Carlo sampling and errors were computed using fluctuations in bone lengths.</p>
      </caption>
      <graphic xlink:href="EMS129592-f004"/>
    </fig>
    <fig id="F5" orientation="portrait" position="anchor">
      <label>Extended data figure 2</label>
      <caption>
        <title>Training and test loss convergence of the LiftPose3D network when applied to a variety of datasets.</title>
        <p id="P55">Shown are the absolute test errors of LiftPose3D for all joints as a function of optimization epoch. Note that the test error is sometimes lower than the training error because we do not apply dropout at test time. <bold>A</bold> Two-camera data of <italic>Drosophila</italic> on a spherical treadmill (each color denotes a different pair of diametrically opposed cameras). <bold>B</bold> OpenMonkeyStudio dataset (each color denotes a different training run). <bold>C</bold> Single-camera data of <italic>Drosophila</italic> behaving freely in the right-angle prism mirror system. <bold>D</bold> LocoMouse dataset. <bold>E</bold> CAPTURE dataset.</p>
      </caption>
      <graphic xlink:href="EMS129592-f005"/>
    </fig>
    <fig id="F6" orientation="portrait" position="anchor">
      <label>Extended data figure 3</label>
      <caption>
        <title><italic>Drosophila</italic> LiftPose3D station.</title>
        <p id="P56"><bold>A</bold> CAD drawing of the LiftPose3D station indicating major components (color-coded). <bold>B</bold> Photo of the LiftPose3D station. <bold>C</bold> Electronic circuit for building the illumination module on a pre-fabricated prototyping board. Electronic components and additional wiring are color-coded. <bold>D</bold> Printed circuit board provided as an alternative to a pre-fabricated board for constructing the illumination module.</p>
      </caption>
      <graphic xlink:href="EMS129592-f006"/>
    </fig>
  </sec>
  <sec sec-type="supplementary-material" id="SM">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="SD1">
      <label>Source data for Extended Data Figure 1</label>
      <media xlink:href="EMS129592-supplement-Source_data_for_Extended_Data_Figure_1.xlsx" mimetype="application" mime-subtype="vnd.openxmlformats-officedocument.spreadsheetml.sheet" orientation="portrait" id="d40e3219" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD2">
      <label>Source data for Extended Data Figure 2</label>
      <media xlink:href="EMS129592-supplement-Source_data_for_Extended_Data_Figure_2.xlsx" mimetype="application" mime-subtype="vnd.openxmlformats-officedocument.spreadsheetml.sheet" orientation="portrait" id="d40e3222" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD3">
      <label>Source data for Figure 1</label>
      <media xlink:href="EMS129592-supplement-Source_data_for_Figure_1.xlsx" mimetype="application" mime-subtype="vnd.openxmlformats-officedocument.spreadsheetml.sheet" orientation="portrait" id="d40e3225" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD4">
      <label>Source data for Figure 2</label>
      <media xlink:href="EMS129592-supplement-Source_data_for_Figure_2.xlsx" mimetype="application" mime-subtype="vnd.openxmlformats-officedocument.spreadsheetml.sheet" orientation="portrait" id="d40e3228" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD5">
      <label>Source data for Figure 3</label>
      <media xlink:href="EMS129592-supplement-Source_data_for_Figure_3.xlsx" mimetype="application" mime-subtype="vnd.openxmlformats-officedocument.spreadsheetml.sheet" orientation="portrait" id="d40e3231" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD6">
      <label>Supp Info 6/29</label>
      <media xlink:href="EMS129592-supplement-Supp_Info_6_29.pdf" mimetype="application" mime-subtype="pdf" orientation="portrait" id="d40e3234" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD7">
      <label>Video 1</label>
      <caption>
        <title>3D poses lifted for tethered <italic>Drosophila</italic> walking backwards as viewed from two side cameras.</title>
        <p id="P57">Videos obtained from cameras 2 <bold>(top-left)</bold> and 5 <bold>(bottom-left)</bold>. DeepFly3D-derived 2D poses are superimposed. Orange circle indicates optogenetic stimulation of MDNs to elicit backward walking. <bold>(right)</bold> 3D poses obtained by triangulating six camera views using DeepFly3D (solid lines), or lifted using LiftPose3D and two camera views (dashed lines).</p>
      </caption>
      <media xlink:href="EMS129592-supplement-Video_1.mp4" mimetype="video" mime-subtype="mp4" orientation="portrait" id="d40e3251" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD8">
      <label>Video 2</label>
      <caption>
        <title>3D poses lifted for tethered <italic>Drosophila</italic> grooming as viewed from two side cameras.</title>
        <p id="P58">Videos obtained from cameras 2 <bold>(top-left)</bold> and 5 <bold>(bottom-left)</bold>. DeepFly3D-derived 2D poses are superimposed. Orange circle indicates optogenetic stimulation of aDNs to elicit antennal grooming. <bold>(right)</bold> 3D poses obtained by triangulating six camera views using DeepFly3D (solid lines), or lifted using LiftPose3D and two camera views (dashed lines).</p>
      </caption>
      <media xlink:href="EMS129592-supplement-Video_2.mp4" mimetype="video" mime-subtype="mp4" orientation="portrait" id="d40e3268" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD9">
      <label>Video 3</label>
      <caption>
        <title>3D poses lifted for tethered <italic>Drosophila</italic> behaving spontaneously with irregular limb movements as viewed from two side cameras.</title>
        <p id="P59">Videos obtained from cameras 2 <bold>(top-left)</bold> and 5 <bold>(bottom-left)</bold>. DeepFly3D-derived 2D poses are superimposed. <bold>(right)</bold> 3D poses obtained by triangulating six camera views using DeepFly3D (solid lines), or lifted using LiftPose3D and two camera views (dashed lines).</p>
      </caption>
      <media xlink:href="EMS129592-supplement-Video_3.mp4" mimetype="video" mime-subtype="mp4" orientation="portrait" id="d40e3285" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD10">
      <label>Video 4</label>
      <caption>
        <title>3D poses lifted for previously published OpenMonkeyStudio data of a freely moving macaque</title>
        <p id="P60"><bold>(left)</bold> Single image drawn randomly from one of 62 cameras. <bold>(middle)</bold> Ground truth 3D poses based on triangulation of 2D poses from up to 62 cameras (solid lines), or lifting from a single camera view using LiftPose3D (dashed lines). <bold>(right)</bold> Error distribution across the 62 cameras for a given pose. Camera locations (circles) are color-coded by error. Gray circles denote cameras for which an image was not available. Green circle denotes the camera from which the image was acquired.</p>
      </caption>
      <media xlink:href="EMS129592-supplement-Video_4.mp4" mimetype="video" mime-subtype="mp4" orientation="portrait" id="d40e3297" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD11">
      <label>Video 5</label>
      <caption>
        <title>3D poses lifted for freely behaving <italic>Drosophila</italic> when triangulation is only partially possible.</title>
        <p id="P61">Single camera images of the ventral <bold>(top-left)</bold> and side <bold>(bottom-left)</bold> views. DeepLabCut-derived 2D poses are superimposed. <bold>(right)</bold> 3D poses obtained by triangulating partially available multi-view 2D poses (solid lines), or by lifting the ventral 2D pose using LiftPose3D (dashed lines).</p>
      </caption>
      <media xlink:href="EMS129592-supplement-Video_5.mp4" mimetype="video" mime-subtype="mp4" orientation="portrait" id="d40e3315" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD12">
      <label>Video 6</label>
      <caption>
        <title>3D poses lifted for previously published freely behaving mouse data when triangulation is only partially possible.</title>
        <p id="P62">Side <bold>(top-left)</bold> and ventral <bold>(bottom-left)</bold> views of a freely walking mouse. Superimposed are keypoints on the paws, mouth, and proximal tail tracked using the LocoMouse software (blue circles). Using only the ventral view 2D pose, a trained LiftPose3D network can accurately track keypoints in the side view (orange circles).</p>
      </caption>
      <media xlink:href="EMS129592-supplement-Video_6.mp4" mimetype="video" mime-subtype="mp4" orientation="portrait" id="d40e3326" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD13">
      <label>Video 7</label>
      <caption>
        <title>3D poses lifted for previously published freely behaving rat data with self-occlusions</title>
        <p id="P63"><bold>(left)</bold> Ground truth 3D poses triangulated from six cameras (solid lines) superimposed with LiftPose3D’s predictions using 2D poses from one camera (dashed lines). <bold>(right)</bold> Images from one camera with 2D poses acquired using CAPTURE are superimposed.</p>
      </caption>
      <media xlink:href="EMS129592-supplement-Video_7.mp4" mimetype="video" mime-subtype="mp4" orientation="portrait" id="d40e3336" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD14">
      <label>Video 8</label>
      <caption>
        <title>3D poses lifted for low-resolution videos of freely behaving flies when triangulation is impossible.</title>
        <p id="P64"><bold>(top)</bold> Three freely behaving <italic>Drosophila</italic> in a rounded square arena and recorded ventrally using a single low-resolution camera. Of these, fly 0 is tracked, cropped, and rotated leftward. Superimposed are 2D poses for 24 visible joints. (bottom) 3D poses lifted from ventral view 2D poses (<italic>x</italic> − <italic>y</italic> plane) permit analysis of leg kinematics in the otherwise unobserved <italic>x</italic> − <italic>z</italic> plane.</p>
      </caption>
      <media xlink:href="EMS129592-supplement-Video_8.mp4" mimetype="video" mime-subtype="mp4" orientation="portrait" id="d40e3359" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD15">
      <label>Video 9</label>
      <caption>
        <title>3D poses lifted for previously published ventral view videos of freely behaving flies when triangulation is impossible.</title>
        <p id="P65"><bold>(top)</bold> Video of a freely behaving fly within a pill-shaped arena and recorded ventrally using a single high-resolution camera. <bold>(bottom-left)</bold> Following tracking, a region-of-interest containing the fly was cropped and rotated to maintain a leftward orientation. Superimposed are 2D poses estimated for 24 visible joints. <bold>(bottom-middle)</bold> 3D poses obtained by lifting ventral view 2D poses. <bold>(bottom-right)</bold> 3D poses lifted from ventral view 2D poses (top) permit analysis of leg kinematics in the otherwise unobserved <italic>x</italic> − <italic>z</italic> plane (bottom).</p>
      </caption>
      <media xlink:href="EMS129592-supplement-Video_9.mp4" mimetype="video" mime-subtype="mp4" orientation="portrait" id="d40e3379" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD16">
      <label>Video 10</label>
      <caption>
        <title>3D poses lifted for data from the <italic>Drosophila</italic> LiftPose3D station.</title>
        <p id="P66"><bold>(left)</bold> Video of a freely behaving fly in the LiftPose3D station arena. <bold>(middle)</bold> Cropped video around the centroid of the tracked fly, registered, and superimposed with 2D pose predictions. <bold>(right)</bold> Lifted 3D poses obtained using ventral 2D poses.</p>
      </caption>
      <media xlink:href="EMS129592-supplement-Video_10.mp4" mimetype="video" mime-subtype="mp4" orientation="portrait" id="d40e3395" position="anchor"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="S27">
    <title>Acknowledgments</title>
    <p>PR acknowledges support from an SNSF Project grant (175667), and an SNSF Eccellenza grant (181239). AG acknowledges support from an HFSP Cross-disciplinary Postdoctoral Fellowship (LT000669/2020-C). SG acknowledges support from an EPFL SV iPhD Grant. DM holds a Marie Curie EuroTech postdoctoral fellowship and acknowledges that this project has received funding from the European Union’s Horizon 2020 research and innovation program under the Marie Sklodowska-Curie grant agreement No 754462. VLR acknowledges support from the Mexican National Council for Science and Technology, CONACYT, under the grant number 709993.</p>
    <p>We thank the Ölvecky lab (Center for Brain Science, Harvard University, Boston, USA) for providing us with the CAPTURE dataset. We thank the Carey lab (Champalimaud Centre for the Unknown, Lisbon, Portugal) for the LocoMouse dataset.</p>
  </ack>
  <fn-group>
    <fn id="FN1" fn-type="con">
      <label>5</label>
      <p id="P70">
        <bold>Author Contributions</bold>
      </p>
      <p id="P71">A.G. - Conceptualization, methodology, software, hardware (<italic>Drosophila</italic> prism mirror system), formal analysis, data curation, writing—original draft, writing—review &amp; editing.</p>
      <p id="P72">S.G. - Conceptualization, methodology, software, formal analysis, data curation, writing—original draft, writing—review &amp; editing.</p>
      <p id="P73">V.L.R. - Software and hardware (LiftPose3D station, low-resolution <italic>Drosophila</italic> ventral view system), data curation, writing—review &amp; editing.</p>
      <p id="P74">M.A. - Methodology, software (LiftPose3D), preliminary analysis of DeepFly3D dataset, data curation, writing—review &amp; editing.</p>
      <p id="P75">D.M. - Investigation (low-resolution <italic>Drosophila</italic> experiments), writing—review &amp; editing.</p>
      <p id="P76">H.R. - Conceptualization, writing—review &amp; editing.</p>
      <p id="P77">P.F. - Writing—review &amp; editing, funding acquisition.</p>
      <p id="P78">P.R. - Conceptualization, hardware (<italic>Drosophila</italic> prism mirror system), resources, writing—original draft, writing—review &amp; editing, supervision, project administration, funding acquisition.</p>
    </fn>
    <fn fn-type="COI-statement" id="FN2">
      <label>6</label>
      <p id="P79">
        <bold>Competing interests</bold>
      </p>
      <p id="P80">The authors declare that no competing interests exist.</p>
    </fn>
  </fn-group>
  <sec id="S28" sec-type="data-availability">
    <label>10.11</label>
    <title>Code availability</title>
    <p id="P67">LiftPose3D code can be installed as a pip package, see <ext-link ext-link-type="uri" xlink:href="https://pypi.org/project/liftpose/">https://pypi.org/project/liftpose/</ext-link>, or downloaded at <ext-link ext-link-type="uri" xlink:href="https://github.com/NeLy-EPFL/LiftPose3D">https://github.com/NeLy-EPFL/LiftPose3D</ext-link>.</p>
    <p id="P68">Custom software to acquire images using the LiftPose3D station is also available at <ext-link ext-link-type="uri" xlink:href="https://github.com/NeLy-EPFL/LiftPose3D">https://github.com/NeLy-EPFL/LiftPose3D</ext-link>.</p>
  </sec>
  <sec id="S29" sec-type="data-availability">
    <label>10.12</label>
    <title>Data availability</title>
    <p id="P69">The experimental data collected for this study can be downloaded at: <ext-link ext-link-type="uri" xlink:href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/KHFAEI">https://doi.org/10.7910/DVN/KHFAEI</ext-link>
</p>
  </sec>
  <ref-list>
    <ref id="R1">
      <label>[1]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pereira</surname>
            <given-names>TD</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Fast animal pose estimation using deep neural networks</article-title>
        <source>Nat Methods</source>
        <year>2019</year>
        <volume>16</volume>
        <fpage>117</fpage>
        <lpage>125</lpage>
        <pub-id pub-id-type="pmid">30573820</pub-id>
      </element-citation>
    </ref>
    <ref id="R2">
      <label>[2]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mathis</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title>
        <source>Nat Neurosci</source>
        <year>2018</year>
        <volume>21</volume>
        <fpage>1281</fpage>
        <lpage>1289</lpage>
        <pub-id pub-id-type="pmid">30127430</pub-id>
      </element-citation>
    </ref>
    <ref id="R3">
      <label>[3]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Günel</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>DeepFly3D, a deep learning-based approach for 3D limb and appendage tracking in tethered, adult <italic>Drosophila</italic>
</article-title>
        <source>eLife</source>
        <year>2019</year>
        <volume>8</volume>
        <fpage>3686</fpage>
      </element-citation>
    </ref>
    <ref id="R4">
      <label>[4]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bala</surname>
            <given-names>PC</given-names>
          </name>
          <name>
            <surname>Eisenreich</surname>
            <given-names>BR</given-names>
          </name>
          <name>
            <surname>Yoo</surname>
            <given-names>S Bum Michael</given-names>
          </name>
          <name>
            <surname>Hayden</surname>
            <given-names>Benjamin Y</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>H Soo</given-names>
          </name>
          <name>
            <surname>Zimmermann</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Automated markerless pose estimation in freely moving macaques with Open-MonkeyStudio</article-title>
        <source>Nat Commun</source>
        <year>2020</year>
        <volume>11</volume>
        <elocation-id>4560</elocation-id>
        <pub-id pub-id-type="pmid">32917899</pub-id>
      </element-citation>
    </ref>
    <ref id="R5">
      <label>[5]</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Newell</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <source>Stacked hourglass networks for human pose estimation</source>
        <conf-name>European Conference on Computer Vision (ECCV)</conf-name>
        <year>2016</year>
      </element-citation>
    </ref>
    <ref id="R6">
      <label>[6]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Graving</surname>
            <given-names>JM</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deepposekit, a software toolkit for fast and robust animal pose estimation using deep learning</article-title>
        <source>eLife</source>
        <year>2019</year>
        <volume>8</volume>
        <elocation-id>e47994</elocation-id>
        <pub-id pub-id-type="pmid">31570119</pub-id>
      </element-citation>
    </ref>
    <ref id="R7">
      <label>[7]</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Fang</surname>
            <given-names>HS</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Tai</surname>
            <given-names>YW</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <source>RMPE: Regional multi-person pose estimation</source>
        <conf-name>IEEE International Conference on Computer Vision (ICCV)</conf-name>
        <year>2017</year>
      </element-citation>
    </ref>
    <ref id="R8">
      <label>[8]</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>SE</given-names>
          </name>
          <name>
            <surname>Ramakrishna</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Kanade</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Sheikh</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <source>Convolutional pose machines</source>
        <conf-name>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>
        <year>2016</year>
      </element-citation>
    </ref>
    <ref id="R9">
      <label>[9]</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Cao</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Simon</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>SE</given-names>
          </name>
          <name>
            <surname>Sheikh</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <source>Realtime multi-person 2D pose estimation using part affinity fields</source>
        <conf-name>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>
        <year>2017</year>
      </element-citation>
    </ref>
    <ref id="R10">
      <label>[10]</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Hartley</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Zisserman</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <source>Multiple View Geometry in Computer Vision</source>
        <edition>2 edn</edition>
        <publisher-name>Cambridge University Press</publisher-name>
        <publisher-loc>USA</publisher-loc>
        <year>2003</year>
      </element-citation>
    </ref>
    <ref id="R11">
      <label>[11]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dombeck</surname>
            <given-names>DA</given-names>
          </name>
          <name>
            <surname>Khabbaz</surname>
            <given-names>AN</given-names>
          </name>
          <name>
            <surname>Collman</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Adelman</surname>
            <given-names>TL</given-names>
          </name>
          <name>
            <surname>Tank</surname>
            <given-names>DW</given-names>
          </name>
        </person-group>
        <article-title>Imaging large-scale neural activity with cellular resolution in awake, mobile mice</article-title>
        <source>Neuron</source>
        <year>2007</year>
        <volume>56</volume>
        <fpage>43</fpage>
        <lpage>57</lpage>
        <pub-id pub-id-type="pmid">17920014</pub-id>
      </element-citation>
    </ref>
    <ref id="R12">
      <label>[12]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Seelig</surname>
            <given-names>JD</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Two-photon calcium imaging from head-fixed <italic>Drosophila</italic> during optomotor walking behavior</article-title>
        <source>Nat Methods</source>
        <year>2010</year>
        <volume>7</volume>
        <fpage>535</fpage>
        <lpage>540</lpage>
        <pub-id pub-id-type="pmid">20526346</pub-id>
      </element-citation>
    </ref>
    <ref id="R13">
      <label>[13]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gaudry</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Hong</surname>
            <given-names>EJ</given-names>
          </name>
          <name>
            <surname>Kain</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>de Bivort</surname>
            <given-names>BL</given-names>
          </name>
          <name>
            <surname>Wilson</surname>
            <given-names>RI</given-names>
          </name>
        </person-group>
        <article-title>Asymmetric neurotransmitter release enables rapid odour lateralization in <italic>Drosophila</italic>
</article-title>
        <source>Nature</source>
        <year>2013</year>
        <volume>493</volume>
        <fpage>424</fpage>
        <lpage>428</lpage>
        <pub-id pub-id-type="pmid">23263180</pub-id>
      </element-citation>
    </ref>
    <ref id="R14">
      <label>[14]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Machado</surname>
            <given-names>AS</given-names>
          </name>
          <name>
            <surname>Darmohray</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>Fayad</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Marques</surname>
            <given-names>HG</given-names>
          </name>
          <name>
            <surname>Carey</surname>
            <given-names>MR</given-names>
          </name>
        </person-group>
        <article-title>A quantitative framework for whole-body coordination reveals specific deficits in freely walking ataxic mice</article-title>
        <source>eLife</source>
        <year>2015</year>
        <volume>4</volume>
        <elocation-id>e07892</elocation-id>
        <pub-id pub-id-type="pmid">26433022</pub-id>
      </element-citation>
    </ref>
    <ref id="R15">
      <label>[15]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Isakov</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Recovery of locomotion after injury in <italic>Drosophila melanogaster</italic> depends on proprioception</article-title>
        <source>J Exp Biol</source>
        <year>2016</year>
        <volume>219</volume>
        <fpage>1760</fpage>
        <lpage>1771</lpage>
        <pub-id pub-id-type="pmid">26994176</pub-id>
      </element-citation>
    </ref>
    <ref id="R16">
      <label>[16]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Uhlmann</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Ramdya</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Delgado-Gonzalo</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Benton</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Unser</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Flylimbtracker: an active contour based approach for leg segment tracking in unmarked, freely behaving <italic>Drosophila</italic>
</article-title>
        <source>PLoS One</source>
        <year>2017</year>
        <volume>12</volume>
        <elocation-id>e0173433</elocation-id>
        <pub-id pub-id-type="pmid">28453566</pub-id>
      </element-citation>
    </ref>
    <ref id="R17">
      <label>[17]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>DeAngelis</surname>
            <given-names>BD</given-names>
          </name>
          <name>
            <surname>Zavatone-Veth</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Clark</surname>
            <given-names>DA</given-names>
          </name>
        </person-group>
        <article-title>The manifold structure of limb coordination in walking <italic>Drosophila</italic>
</article-title>
        <source>eLife</source>
        <year>2019</year>
        <volume>8</volume>
        <fpage>137</fpage>
      </element-citation>
    </ref>
    <ref id="R18">
      <label>[18]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>HJ</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Determination of 3D human body postures from a single view</article-title>
        <source>Computer Vision, Graphics, and Image Processing</source>
        <year>1985</year>
        <volume>30</volume>
        <fpage>148</fpage>
        <lpage>168</lpage>
      </element-citation>
    </ref>
    <ref id="R19">
      <label>[19]</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Taylor</surname>
            <given-names>CJ</given-names>
          </name>
        </person-group>
        <source>Reconstruction of articulated objects from point correspondences in a single uncalibrated image</source>
        <conf-name>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>
        <year>2000</year>
      </element-citation>
    </ref>
    <ref id="R20">
      <label>[20]</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Ramanan</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <source>3D human pose estimation = 2D pose estimation + matching</source>
        <conf-name>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>
        <year>2017</year>
      </element-citation>
    </ref>
    <ref id="R21">
      <label>[21]</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Gupta</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Martinez</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Little</surname>
            <given-names>JJ</given-names>
          </name>
          <name>
            <surname>Woodham</surname>
            <given-names>RJ</given-names>
          </name>
        </person-group>
        <source>3D pose from motion for cross-view action recognition via non-linear circulant temporal encoding</source>
        <conf-name>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>
        <year>2014</year>
      </element-citation>
    </ref>
    <ref id="R22">
      <label>[22]</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Sun</surname>
            <given-names>JJ</given-names>
          </name>
          <etal/>
        </person-group>
        <chapter-title>View-invariant probabilistic embedding for human pose</chapter-title>
        <person-group person-group-type="editor">
          <name>
            <surname>Vedaldi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bischof</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Brox</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Frahm</surname>
            <given-names>JM</given-names>
          </name>
        </person-group>
        <source>Computer Vision - ECCV2020</source>
        <publisher-name>Springer International Publishing</publisher-name>
        <publisher-loc>Cham</publisher-loc>
        <year>2020</year>
        <fpage>53</fpage>
        <lpage>70</lpage>
      </element-citation>
    </ref>
    <ref id="R23">
      <label>[23]</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Nibali</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Morgan</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Prendergast</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <source>3D human pose estimation with 2D marginal heatmaps</source>
        <conf-name>IEEE Winter Conference on Applications of Computer Vision (WACV)</conf-name>
        <year>2019</year>
      </element-citation>
    </ref>
    <ref id="R24">
      <label>[24]</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Tian</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Kapadia</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Metaxas</surname>
            <given-names>DN</given-names>
          </name>
        </person-group>
        <source>Semantic graph convolutional networks for 3D human pose regression</source>
        <conf-name>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>
        <year>2019</year>
      </element-citation>
    </ref>
    <ref id="R25">
      <label>[25]</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Iskakov</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Burkov</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Lempitsky</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Malkov</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <source>Learnable triangulation of human pose</source>
        <conf-name>International Conference on Computer Vision (ICCV)</conf-name>
        <year>2019</year>
      </element-citation>
    </ref>
    <ref id="R26">
      <label>[26]</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Kanazawa</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>JY</given-names>
          </name>
          <name>
            <surname>Felsen</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Malik</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <source>Learning 3D human dynamics from video</source>
        <conf-name>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>
        <year>2019</year>
      </element-citation>
    </ref>
    <ref id="R27">
      <label>[27]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mehta</surname>
            <given-names>D</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>XNect: Real-time multi-person 3D motion capture with a single RGB camera</article-title>
        <source>ACM Transactions on Graphics</source>
        <year>2020</year>
      </element-citation>
    </ref>
    <ref id="R28">
      <label>[28]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rematas</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Nguyen</surname>
            <given-names>CH</given-names>
          </name>
          <name>
            <surname>Ritschel</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Fritz</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Tuytelaars</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Novel views of objects from a single image</article-title>
        <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>
        <year>2017</year>
        <volume>39</volume>
        <fpage>1576</fpage>
        <lpage>1590</lpage>
        <pub-id pub-id-type="pmid">27541489</pub-id>
      </element-citation>
    </ref>
    <ref id="R29">
      <label>[29]</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Rhodin</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Constantin</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Katircioglu</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Salzmann</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Fua</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <source>Neural scene decomposition for multi-person motion capture</source>
        <conf-name>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>
        <year>2019</year>
      </element-citation>
    </ref>
    <ref id="R30">
      <label>[30]</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Martinez</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hossain</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Romero</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Little</surname>
            <given-names>JJ</given-names>
          </name>
        </person-group>
        <source>A simple yet effective baseline for 3D human pose estimation</source>
        <conf-name>IEEE International Conference on Computer Vision (ICCV)</conf-name>
        <year>2017</year>
      </element-citation>
    </ref>
    <ref id="R31">
      <label>[31]</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Pavllo</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Feichtenhofer</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Grangier</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Auli</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <source>3D human pose estimation in video with temporal convolutions and semi-supervised training</source>
        <conf-name>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>
        <year>2019</year>
      </element-citation>
    </ref>
    <ref id="R32">
      <label>[32]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Guang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Rojas</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>GAST-Net: Graph attention spatio-temporal convolutional networks for 3D human pose estimation in video</article-title>
        <year>2020</year>
        <comment>Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2003.14179">https://arxiv.org/abs/2003.14179</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="R33">
      <label>[33]</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Cai</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <source>Exploiting spatial-temporal relationships for 3D pose estimation via graph convolutional networks</source>
        <conf-name>IEEE International Conference on Computer Vision (ICCV)</conf-name>
        <year>2019</year>
      </element-citation>
    </ref>
    <ref id="R34">
      <label>[34]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yiannakides</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Aristidou</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Chrysanthou</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Real-time 3D human pose and motion reconstruction from monocular rgb videos</article-title>
        <source>Comput Animat Virtual Worlds</source>
        <year>2019</year>
        <volume>30</volume>
        <fpage>1</fpage>
        <lpage>12</lpage>
      </element-citation>
    </ref>
    <ref id="R35">
      <label>[35]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Card</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Dickinson</surname>
            <given-names>MH</given-names>
          </name>
        </person-group>
        <article-title>Visually mediated motor planning in the escape response of <italic>Drosophila</italic>
</article-title>
        <source>Curr Biol</source>
        <year>2008</year>
        <volume>18</volume>
        <fpage>1300</fpage>
        <lpage>1307</lpage>
        <pub-id pub-id-type="pmid">18760606</pub-id>
      </element-citation>
    </ref>
    <ref id="R36">
      <label>[36]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wosnitza</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bockemühl</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Dübbert</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Scholz</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Büschges</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Inter-leg coordination in the control of walking speed in <italic>Drosophila</italic>
</article-title>
        <source>J Exp Biol</source>
        <year>2013</year>
        <volume>216</volume>
        <fpage>480</fpage>
        <lpage>491</lpage>
        <pub-id pub-id-type="pmid">23038731</pub-id>
      </element-citation>
    </ref>
    <ref id="R37">
      <label>[37]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Marshall</surname>
            <given-names>JD</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Continuous whole-body 3D kinematic recordings across the rodent behavioral repertoire</article-title>
        <source>Neuron</source>
        <year>2021</year>
        <volume>109</volume>
        <fpage>420</fpage>
        <lpage>437</lpage>
        <elocation-id>e8</elocation-id>
        <pub-id pub-id-type="pmid">33340448</pub-id>
      </element-citation>
    </ref>
    <ref id="R38">
      <label>[38]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>De Bono</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Bargmann</surname>
            <given-names>CI</given-names>
          </name>
        </person-group>
        <article-title>Natural variation in a neuropeptide y receptor homolog modifies social behavior and food response in <italic>C. elegans</italic>
</article-title>
        <source>Cell</source>
        <year>1998</year>
        <volume>94</volume>
        <fpage>679</fpage>
        <lpage>689</lpage>
        <pub-id pub-id-type="pmid">9741632</pub-id>
      </element-citation>
    </ref>
    <ref id="R39">
      <label>[39]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Budick</surname>
            <given-names>SA</given-names>
          </name>
          <name>
            <surname>O’Malley</surname>
            <given-names>DM</given-names>
          </name>
        </person-group>
        <article-title>Locomotor repertoire of the larval zebrafish: swimming, turning and prey capture</article-title>
        <source>J Exp Biol</source>
        <year>2000</year>
        <volume>203</volume>
        <fpage>2565</fpage>
        <lpage>2579</lpage>
        <pub-id pub-id-type="pmid">10934000</pub-id>
      </element-citation>
    </ref>
    <ref id="R40">
      <label>[40]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Louis</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Huber</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Benton</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Sakmar</surname>
            <given-names>TP</given-names>
          </name>
          <name>
            <surname>Vosshall</surname>
            <given-names>LB</given-names>
          </name>
        </person-group>
        <article-title>Bilateral olfactory sensory input enhances chemotaxis behavior</article-title>
        <source>Nat Neurosci</source>
        <year>2008</year>
        <volume>11</volume>
        <fpage>187</fpage>
        <lpage>199</lpage>
        <pub-id pub-id-type="pmid">18157126</pub-id>
      </element-citation>
    </ref>
    <ref id="R41">
      <label>[41]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Strauss</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Heisenberg</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Coordination of legs during straight walking and turning in <italic>Drosophila melanogaster</italic>
</article-title>
        <source>J Comp Physiol A</source>
        <year>1990</year>
        <volume>167</volume>
        <fpage>403</fpage>
        <lpage>412</lpage>
        <pub-id pub-id-type="pmid">2121965</pub-id>
      </element-citation>
    </ref>
    <ref id="R42">
      <label>[42]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Clarke</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Still</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Gait analysis in the mouse</article-title>
        <source>Physiol Behav</source>
        <year>1999</year>
        <volume>66</volume>
        <fpage>723</fpage>
        <lpage>729</lpage>
        <pub-id pub-id-type="pmid">10405098</pub-id>
      </element-citation>
    </ref>
    <ref id="R43">
      <label>[43]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wiltschko</surname>
            <given-names>AB</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Mapping sub-second structure in mouse behavior</article-title>
        <source>Neuron</source>
        <year>2015</year>
        <volume>88</volume>
        <fpage>1121</fpage>
        <lpage>1135</lpage>
        <pub-id pub-id-type="pmid">26687221</pub-id>
      </element-citation>
    </ref>
    <ref id="R44">
      <label>[44]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hong</surname>
            <given-names>W</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Automated measurement of mouse social behaviors using depth sensing, video tracking, and machine learning</article-title>
        <source>Proc Natl Acad Sci USA</source>
        <year>2015</year>
        <volume>112</volume>
        <fpage>E5351</fpage>
        <lpage>E5360</lpage>
        <pub-id pub-id-type="pmid">26354123</pub-id>
      </element-citation>
    </ref>
    <ref id="R45">
      <label>[45]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mendes</surname>
            <given-names>CS</given-names>
          </name>
          <name>
            <surname>Bartos</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Akay</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Márka</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Mann</surname>
            <given-names>RS</given-names>
          </name>
        </person-group>
        <article-title>Quantification of gait parameters in freely walking wild type and sensory deprived <italic>Drosophila melanogaster</italic>
</article-title>
        <source>eLife</source>
        <year>2013</year>
        <volume>2</volume>
        <fpage>231</fpage>
      </element-citation>
    </ref>
    <ref id="R46">
      <label>[46]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Feng</surname>
            <given-names>K</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Distributed control of motor circuits for backward walking in <italic>Drosophila</italic>
</article-title>
        <source>Nat Commun</source>
        <year>2020</year>
        <volume>11</volume>
        <fpage>1</fpage>
        <lpage>17</lpage>
        <pub-id pub-id-type="pmid">31911652</pub-id>
      </element-citation>
    </ref>
    <ref id="R47">
      <label>[47]</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Alp Güler</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Neverova</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Kokkinos</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <source>Densepose: Dense human pose estimation in the wild</source>
        <conf-name>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>
        <year>2018</year>
      </element-citation>
    </ref>
    <ref id="R48">
      <label>[48]</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Güler</surname>
            <given-names>RA</given-names>
          </name>
          <name>
            <surname>Kokkinos</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <source>Holopose: Holistic 3D human reconstruction in-the-wild</source>
        <conf-name>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>
        <year>2019</year>
      </element-citation>
    </ref>
    <ref id="R49">
      <label>[49]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Loper</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Mahmood</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Romero</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Pons-Moll</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Black</surname>
            <given-names>MJ</given-names>
          </name>
        </person-group>
        <article-title>SMPL: A skinned multi-person linear model</article-title>
        <source>ACM Trans Graphics (Proc SIGGRAPH Asia)</source>
        <year>2015</year>
        <volume>34</volume>
        <fpage>248:1</fpage>
        <lpage>248:16</lpage>
      </element-citation>
    </ref>
    <ref id="R50">
      <label>[50]</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>JY</given-names>
          </name>
          <name>
            <surname>Felsen</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Kanazawa</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Malik</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <source>Predicting 3D human dynamics from video</source>
        <conf-name>IEEE International Conference on Computer Vision (ICCV)</conf-name>
        <year>2019</year>
      </element-citation>
    </ref>
    <ref id="R51">
      <label>[51]</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Zuffi</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kanazawa</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Berger-Wolf</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Black</surname>
            <given-names>MJ</given-names>
          </name>
        </person-group>
        <source>Three-d safari: Learning to estimate zebra pose, shape, and texture from images ”in the wild”</source>
        <conf-name>IEEE International Conferene on Computer Vision (ICCV)</conf-name>
        <year>2019</year>
      </element-citation>
    </ref>
    <ref id="R52">
      <label>[52]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Günel</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>DeepFly3D, a deep learning-based approach for 3D limb and appendage tracking in tethered, adult <italic>Drosophila</italic>
</article-title>
        <source>eLife</source>
        <year>2019</year>
        <volume>8</volume>
        <fpage>3686</fpage>
      </element-citation>
    </ref>
    <ref id="R53">
      <label>[53]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bala</surname>
            <given-names>PC</given-names>
          </name>
          <name>
            <surname>Eisenreich</surname>
            <given-names>BR</given-names>
          </name>
          <name>
            <surname>Yoo</surname>
            <given-names>SB</given-names>
          </name>
          <name>
            <surname>Hayden</surname>
            <given-names>HY</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>BS</given-names>
          </name>
          <name>
            <surname>Zimmermann</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Automated markerless pose estimation in freely moving macaques with OpenMonkeyStudio</article-title>
        <source>Nat Commun</source>
        <year>2020</year>
        <volume>11</volume>
        <elocation-id>4560</elocation-id>
        <pub-id pub-id-type="pmid">32917899</pub-id>
      </element-citation>
    </ref>
    <ref id="R54">
      <label>[54]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Machado</surname>
            <given-names>AS</given-names>
          </name>
          <name>
            <surname>Darmohray</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>Fayad</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Marques</surname>
            <given-names>HG</given-names>
          </name>
          <name>
            <surname>Carey</surname>
            <given-names>MR</given-names>
          </name>
        </person-group>
        <article-title>A quantitative framework for whole-body coordination reveals specific deficits in freely walking ataxic mice</article-title>
        <source>eLife</source>
        <year>2015</year>
        <volume>4</volume>
        <elocation-id>e07892</elocation-id>
        <pub-id pub-id-type="pmid">26433022</pub-id>
      </element-citation>
    </ref>
    <ref id="R55">
      <label>[55]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Marshall</surname>
            <given-names>JD</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Continuous whole-body 3D kinematic recordings across the rodent behavioral repertoire</article-title>
        <source>Neuron</source>
        <year>2021</year>
        <volume>109</volume>
        <fpage>420</fpage>
        <lpage>437</lpage>
        <elocation-id>e8</elocation-id>
        <pub-id pub-id-type="pmid">33340448</pub-id>
      </element-citation>
    </ref>
    <ref id="R56">
      <label>[56]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Uhlmann</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Ramdya</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Delgado-Gonzalo</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Benton</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Unser</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Flylimbtracker: An active contour based approach for leg segment tracking in unmarked, freely behaving <italic>Drosophila</italic>
</article-title>
        <source>PLoS One</source>
        <year>2017</year>
        <volume>12</volume>
        <elocation-id>e0173433</elocation-id>
        <pub-id pub-id-type="pmid">28453566</pub-id>
      </element-citation>
    </ref>
    <ref id="R57">
      <label>[57]</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Martinez</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hossain</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Romero</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Little</surname>
            <given-names>JJ</given-names>
          </name>
        </person-group>
        <source>A simple yet effective baseline for 3D human pose estimation</source>
        <conf-name>IEEE International Conference on Computer Vision (ICCV)</conf-name>
        <year>2017</year>
      </element-citation>
    </ref>
    <ref id="R58">
      <label>[58]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Srivastava</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Krizhevsky</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Salakhutdinov</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title>
        <source>The Journal of Machine Learning Research</source>
        <year>2014</year>
        <volume>15</volume>
        <fpage>1929</fpage>
        <lpage>1958</lpage>
      </element-citation>
    </ref>
    <ref id="R59">
      <label>[59]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mathis</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title>
        <source>Nat Neurosci</source>
        <year>2018</year>
        <volume>21</volume>
        <fpage>1281</fpage>
        <lpage>1289</lpage>
        <pub-id pub-id-type="pmid">30127430</pub-id>
      </element-citation>
    </ref>
    <ref id="R60">
      <label>[60]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wandt</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Rudolph</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Zell</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Rhodin</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Rosenhahn</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>CanonPose: Self-supervised monocular 3D human pose estimation in the wild</article-title>
        <year>2020</year>
        <comment>Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2011.14679">https://arxiv.org/abs/2011.14679</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="R61">
      <label>[61]</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Hartley</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Zisserman</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <source>Multiple View Geometry in Computer Vision</source>
        <edition>2 edn</edition>
        <publisher-name>Cambridge University Press</publisher-name>
        <publisher-loc>USA</publisher-loc>
        <year>2003</year>
      </element-citation>
    </ref>
    <ref id="R62">
      <label>[62]</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Nair</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>GE</given-names>
          </name>
        </person-group>
        <source>Rectified linear units improve restricted boltzmann machines</source>
        <conf-name>International Conference on Machine Learning (ICML)</conf-name>
        <year>2010</year>
        <fpage>807</fpage>
        <lpage>814</lpage>
      </element-citation>
    </ref>
    <ref id="R63">
      <label>[63]</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <source>Deep residual learning for image recognition</source>
        <conf-name>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>
        <year>2016</year>
      </element-citation>
    </ref>
    <ref id="R64">
      <label>[64]</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Kingma</surname>
            <given-names>DP</given-names>
          </name>
          <name>
            <surname>Ba</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <source>Adam: A method for stochastic optimization</source>
        <conf-name>The International Conference on Learning Representations (ICLR)</conf-name>
        <year>2015</year>
      </element-citation>
    </ref>
    <ref id="R65">
      <label>[65]</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Ioffe</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Szegedy</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <source>Batch normalization: Accelerating deep network training by reducing internal covariate shift</source>
        <conf-name>International Conference on Machine Learning (ICML)</conf-name>
        <year>2015</year>
        <fpage>448</fpage>
        <lpage>456</lpage>
      </element-citation>
    </ref>
    <ref id="R66">
      <label>[66]</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Cao</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <source>Cross-domain adaptation for animal pose estimation</source>
        <conf-name>IEEE International Conference on Computer Vision (ICCV)</conf-name>
        <year>2019</year>
      </element-citation>
    </ref>
    <ref id="R67">
      <label>[67]</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Sanakoyeu</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Khalidov</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>McCarthy</surname>
            <given-names>MS</given-names>
          </name>
          <name>
            <surname>Vedaldi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Neverova</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <source>Transferring Dense Pose to Proximal Animal Classes</source>
        <conf-name>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>
        <year>2020</year>
      </element-citation>
    </ref>
    <ref id="R68">
      <label>[68]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Card</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Dickinson</surname>
            <given-names>MH</given-names>
          </name>
        </person-group>
        <article-title>Visually mediated motor planning in the escape response of <italic>Drosophila</italic>
</article-title>
        <source>Curr Biol</source>
        <year>2008</year>
        <volume>18</volume>
        <fpage>1300</fpage>
        <lpage>1307</lpage>
        <pub-id pub-id-type="pmid">18760606</pub-id>
      </element-citation>
    </ref>
    <ref id="R69">
      <label>[69]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wosnitza</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bockemühl</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Dübbert</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Scholz</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Büschges</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Inter-leg coordination in the control of walking speed in <italic>Drosophila</italic>
</article-title>
        <source>J Exp Biol</source>
        <year>2013</year>
        <volume>216</volume>
        <fpage>480</fpage>
        <lpage>491</lpage>
        <pub-id pub-id-type="pmid">23038731</pub-id>
      </element-citation>
    </ref>
    <ref id="R70">
      <label>[70]</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sridhar</surname>
            <given-names>VH</given-names>
          </name>
          <name>
            <surname>Roche</surname>
            <given-names>DG</given-names>
          </name>
          <name>
            <surname>Gingins</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Tracktor: Image-based automated tracking of animal movement and behaviour</article-title>
        <source>Methods in Ecology and Evolution</source>
        <year>2019</year>
        <volume>10</volume>
        <fpage>815</fpage>
        <lpage>820</lpage>
      </element-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig id="F1" orientation="portrait" position="float">
    <label>Figure 1</label>
    <caption>
      <title>LiftPose3D predicts 3D pose with fewer cameras and flexible camera positioning</title>
      <p><bold>A</bold> Ground truth 3D poses of tethered <italic>Drosophila</italic> are triangulated using six camera views (3 cameras per keypoint). <bold>B</bold> LiftPose3D predicts 3D poses using deep network-derived 2D poses from only two cameras (red and blue, 1 camera per keypoint). The coordinates are considered relative to a set of root joints (green). The inputs are scaled up and passed twice through the main processing unit (gray rectangle) comprising batch norm, dropout and ReLU wrapped by a skip connection. <bold>C</bold> The output, 3D half-body poses (blue/red), are compared with triangulated 3D poses. Limbs are labeled by left (L)/right (R) and front (1), mid (2), or hind (3) positions. <bold>D</bold> LiftPose3D can be trained using virtual camera projections of 3D poses to lift from cameras within the angles <italic>ψ<sub>z</sub>, ψ<sub>y</sub>, ψ<sub>x</sub></italic> (representing ordered yaw, roll, pitch rotations). <bold>E</bold> Error of 3D poses relative to triangulation using three cameras per keypoint. We compare triangulation error using 2 cameras per keypoint (white), test error for a network trained with known camera parameters (orange) and two angle-invariant networks with narrow (green, <italic>ψ<sub>z</sub></italic> = ±10°, <italic>ψ<sub>y</sub></italic> = ±5°, <italic>ψ<sub>x</sub></italic> = ±5° with respect to a known camera orientation), or wide ranges (red, <italic>ψ<sub>z</sub></italic> = ±180°, <italic>ψ<sub>y</sub></italic> = ±5°, <italic>ψ<sub>x</sub></italic> = ±5°). <bold>F</bold> Error of lifted 3D poses at different virtual camera orientations of the wide-range lifter network and a network with known camera parameters. Blue dots represent lifting errors for a given projected 2D pose. Orange circles represent averages over the test dataset for a given camera. <bold>G</bold> Error of estimated 3D poses for a network trained and tested on different combinations behavioral data including optogenetically-induced backward walking (<italic>MDN,</italic> left), antennal grooming (<italic>aDN,</italic> middle), or spontaneous, unstimulated behaviors (<italic>control</italic>, right). <bold>H</bold> Two representative images from the OpenMonkeyStudio dataset. 2D poses are superimposed (black). <bold>I</bold> 3D poses obtained by triangulating up to 62 cameras (red lines), or using a single camera and LiftPose3D (dashed black lines). <bold>J</bold> Absolute errors for different body parts with respect to total body length. Violin plots represent Gaussian kernel density estimates with bandwidth 0.5, truncated at the 99th percentile and superimposed with the median (gray dot), 25th, and 50th percentiles (black line).</p>
    </caption>
    <graphic xlink:href="EMS129592-f001"/>
  </fig>
  <fig id="F2" orientation="portrait" position="float">
    <label>Figure 2</label>
    <caption>
      <title>LiftPose3D performs 3D pose estimation on freely behaving animals with occluded keypoints.</title>
      <p><bold>A</bold><italic>Drosophila</italic> behaving freely within a narrow, transparent enclosure. Using a right-angle prism mirror, ventral (top) and side (bottom) views are recorded with one camera. Afterwards, 2D poses are annotated (colored lines). Ventral 2D poses (green box) are used to lift 3D poses. <bold>B</bold> Keypoints near the prism mirror (red and blue) can be tracked in both views and triangulated. Other keypoints (gray) are only visible ventrally and thus have no 3D ground truth. Unilateral ground truth for both sides are obtained by registering the orientation and position of ventral images of the fly. <bold>C</bold> Training data consist of full ventral 2D poses and their corresponding partial 3D poses. <bold>D</bold> Following training, LiftPose3D can predict 3D poses for new ventral view 2D poses. <bold>E</bold> Joint-wise and overall absolute errors of the network’s 3D pose predictions for freely behaving <italic>Drosophila</italic>. <bold>F</bold> A similar data preprocessing approach is used to lift ventral view 2D poses of mice (green boxes) walking within a narrow enclosure and tracked using LocoMouse software. LocoMouse ground truth (blue and red) and LiftPose3D (orange) pose trajectories are shown for the right forepaw (top) and hindpaw (bottom) during one walking epoch. Arrowheads indicate where LiftPose3D lifting of the ventral view can be used to correct LocoMouse side view tracking errors (red). Asterisks indicate where inaccuracies in the LocoMouse ventral view ground truth (red) disrupt LiftPose3D’s side view predictions (orange). <bold>G</bold> Absolute errors of LiftPose3D and LocoMouse side view predictions for six keypoints with respect to manually-annotated ground truth data. <bold>H</bold> Camera image from the CAPTURE dataset superimposed with the annotated 2D pose (left). LiftPose3D uses this 2D pose to recover the full 3D pose (right). <bold>I</bold> LiftPose3D can be trained to lift 3D poses of a freely moving rat with occluded keypoints (open circles). <bold>J</bold> Histograms of the measured lengths of the spinal segment for two different animals reveal large animal-to-animal skeletal variations. <bold>K</bold> Error distribution over all keypoints for the CAPTURE dataset.</p>
    </caption>
    <graphic xlink:href="EMS129592-f002"/>
  </fig>
  <fig id="F3" orientation="portrait" position="float">
    <label>Figure 3</label>
    <caption>
      <title>A pretrained LiftPose3D network predicts 3D poses for diverse data and when triangulation is impossible.</title>
      <p><bold>A</bold> Linear domain adaptation between domain A (fly on a spherical treadmill) and domain B (fly on a flat surface). 2D poses in B are mapped to A by a linear transformation <italic>d</italic>
<sub>2</sub> then lifted with a network trained only on domain A poses. After lifting, the 3D poses are mapped back to B by another linear transformation <italic>d</italic>
<sub>3</sub>. <bold>B</bold> A typical 2D pose in domain B mapped into domain A by the best-fit linear transformation <italic>d</italic>
<sub>2</sub> between poses in B and their nearest neighbors in A. <bold>C</bold> Error between mapped pose and <italic>k</italic> nearest neighbor poses for <italic>d</italic>
<sub>2</sub>, <italic>d</italic>
<sub>3</sub> against the number of poses used to train them (<italic>k</italic> =1 for <italic>d</italic>
<sub>2</sub> and <italic>k</italic> = 2 for <italic>d</italic>
<sub>3</sub>). <bold>D</bold> Lifted 3D pose following domain adapation of a ventral domain B 2D pose and lifting with a network trained on domain A data. The prediction is superimposed with the incomplete ground truth 3D pose in domain B. <bold>E</bold> Lifting error following domain adaptation of domain B poses compared with lifting error in the domain A with no domain adaptation. <bold>F</bold> Freely behaving flies were recorded from below using a low-resolution camera. Following body tracking, the region-of-interest containing the fly was cropped and registered. 2D pose estimation was then performed for 24 visible joints. <bold>G</bold> 2D poses are adapted to the prism-mirror domain. These are then lifted to 3D poses using a network pre-trained with prism-mirror data and coarse-grained to match the lower resolution 2D images in the new experimental system. <bold>H</bold> These 3D poses permit the analysis of claw movements in an otherwise unobserved <italic>x</italic> – <italic>z</italic> plane (bottom). <bold>I</bold> Freely behaving fly recorded from below using one high-resolution camera. 2D pose estimation was performed for all 30 joints. Following tracking, a region-of-interest containing the fly was cropped and registered. The same LiftPose3D network trained in panel B—but without coarse-graining—was used to predict <bold>J</bold> 3D poses and <bold>K</bold> unobserved claw movements in the <italic>x</italic> – <italic>z</italic> plane (bottom).</p>
    </caption>
    <graphic xlink:href="EMS129592-f003"/>
  </fig>
  <table-wrap id="T1" position="float" orientation="portrait">
    <label>Table 1</label>
    <caption>
      <title>List of datasets used</title>
    </caption>
    <table frame="void" rules="cols">
      <thead>
        <tr>
          <th align="left" valign="middle" style="border-bottom: solid thin" rowspan="1" colspan="1">Dataset</th>
          <th align="left" valign="middle" style="border-bottom: solid thin" rowspan="1" colspan="1">Views (#)</th>
          <th align="left" valign="middle" style="border-bottom: solid thin" rowspan="1" colspan="1">Lifted keypoints (#)</th>
          <th align="left" valign="middle" style="border-bottom: solid thin" rowspan="1" colspan="1">3D poses (# train/test)</th>
          <th align="left" valign="middle" style="border-bottom: solid thin" rowspan="1" colspan="1">Resolution (px/mm)</th>
          <th align="left" valign="middle" style="border-bottom: solid thin" rowspan="1" colspan="1">Framerate (Hz)</th>
          <th align="left" valign="middle" style="border-bottom: solid thin" rowspan="1" colspan="1">Animals (# train/test)</th>
          <th align="center" valign="middle" style="border-bottom: solid thin" rowspan="1" colspan="1">Source</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">DeepFly3D (spherical treadmill)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">6</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">24</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.56 × 10<sup>5</sup>/1.98 × 10<sup>4</sup>
</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">117</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">100</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">6/2</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="R3" ref-type="bibr">3</xref>]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">OpenMonkeyStudio</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">62</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">12</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">6’581/710</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.15</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">30</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">5/1</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="R4" ref-type="bibr">4</xref>]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Fly in a prism-mirror setup</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">24</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">8’362/3’416</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">112</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">100</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3/1</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">this paper</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">LocoMouse</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">6</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">28’840/10’814</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.5</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">400</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">30/4</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="R14" ref-type="bibr">14</xref>]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">CAPTURE</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">6</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">20</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.58 × 10<sup>5</sup>/5.17 × 10<sup>4</sup>
</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">300</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3/1</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="R37" ref-type="bibr">37</xref>]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Fly in a rounded square arena</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">18</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">n.a.</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">26</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">80</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">n.a./1</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">this paper</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Fly in a pill-shaped arena</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">18</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">n.a.</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">203</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">200</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">n.a./1</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="R16" ref-type="bibr">16</xref>]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"><italic>Drosophila</italic> LiftPose3D station</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">18</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">n.a.</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">56</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">80</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">n.a./1</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">this paper</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
