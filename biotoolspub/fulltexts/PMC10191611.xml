<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10191611</article-id>
    <article-id pub-id-type="pmid">36961334</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btad151</article-id>
    <article-id pub-id-type="publisher-id">btad151</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Paper</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Genome Analysis</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Scrooge: a fast and memory-frugal genomic sequence aligner for CPUs, GPUs, and ASICs</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-2581-8637</contrib-id>
        <name>
          <surname>Lindegger</surname>
          <given-names>Joël</given-names>
        </name>
        <aff><institution>Department of Information Technology and Electrical Engineering, ETH Zurich</institution>, Zurich 8006, <country country="CH">Switzerland</country></aff>
        <xref rid="btad151-cor1" ref-type="corresp"/>
        <!--omutlu@gmail.com-->
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Senol Cali</surname>
          <given-names>Damla</given-names>
        </name>
        <aff><institution>Bionano Genomics</institution>, San Diego, CA 92121, <country country="US">United States</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Alser</surname>
          <given-names>Mohammed</given-names>
        </name>
        <aff><institution>Department of Information Technology and Electrical Engineering, ETH Zurich</institution>, Zurich 8006, <country country="CH">Switzerland</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Gómez-Luna</surname>
          <given-names>Juan</given-names>
        </name>
        <aff><institution>Department of Information Technology and Electrical Engineering, ETH Zurich</institution>, Zurich 8006, <country country="CH">Switzerland</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ghiasi</surname>
          <given-names>Nika Mansouri</given-names>
        </name>
        <aff><institution>Department of Information Technology and Electrical Engineering, ETH Zurich</institution>, Zurich 8006, <country country="CH">Switzerland</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Mutlu</surname>
          <given-names>Onur</given-names>
        </name>
        <aff><institution>Department of Information Technology and Electrical Engineering, ETH Zurich</institution>, Zurich 8006, <country country="CH">Switzerland</country></aff>
        <xref rid="btad151-cor1" ref-type="corresp"/>
        <!--jmlindegger@gmail.com-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Robinson</surname>
          <given-names>Peter</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btad151-cor1">Corresponding author. Department of Information Technology and Electrical Engineering, ETH Zurich, Zurich 8006, Switzerland. E-mail: <email>jmlindegger@gmail.com</email> (J.L.), <email>omutlu@gmail.com</email> (O.M.)</corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>5</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2023-03-24">
      <day>24</day>
      <month>3</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>24</day>
      <month>3</month>
      <year>2023</year>
    </pub-date>
    <volume>39</volume>
    <issue>5</issue>
    <elocation-id>btad151</elocation-id>
    <history>
      <date date-type="received">
        <day>21</day>
        <month>8</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>11</day>
        <month>1</month>
        <year>2023</year>
      </date>
      <date date-type="editorial-decision">
        <day>11</day>
        <month>2</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>23</day>
        <month>3</month>
        <year>2023</year>
      </date>
      <date date-type="corrected-typeset">
        <day>17</day>
        <month>5</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btad151.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Pairwise sequence alignment is a very time-consuming step in common bioinformatics pipelines. Speeding up this step requires heuristics, efficient implementations, and/or hardware acceleration. A promising candidate for all of the above is the recently proposed GenASM algorithm. We identify and address three inefficiencies in the GenASM algorithm: it has a high amount of data movement, a large memory footprint, and does some unnecessary work.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We propose <italic toggle="yes">Scrooge</italic>, a fast and memory-frugal genomic sequence aligner. Scrooge includes three novel algorithmic improvements which reduce the data movement, memory footprint, and the number of operations in the GenASM algorithm. We provide efficient open-source implementations of the Scrooge algorithm for CPUs and GPUs, which demonstrate the significant benefits of our algorithmic improvements. For long reads, the CPU version of Scrooge achieves a 20.1<inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula>, 1.7<inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula>, and 2.1<inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula> speedup over KSW2, Edlib, and a CPU implementation of GenASM, respectively. The GPU version of Scrooge achieves a 4.0<inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula>, 80.4<inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula>, 6.8<inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula>, 12.6<inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula>, and 5.9<inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula> speedup over the CPU version of Scrooge, KSW2, Edlib, Darwin-GPU, and a GPU implementation of GenASM, respectively. We estimate an ASIC implementation of Scrooge to use 3.6<inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula> less chip area and 2.1<inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula> less power than a GenASM ASIC while maintaining the same throughput. Further, we systematically analyze the throughput and accuracy behavior of GenASM and Scrooge under various configurations. As the best configuration of Scrooge depends on the computing platform, we make several observations that can help guide future implementations of Scrooge.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p><ext-link xlink:href="https://github.com/CMU-SAFARI/Scrooge" ext-link-type="uri">https://github.com/CMU-SAFARI/Scrooge</ext-link>.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Semiconductor Research Corporation</institution>
            <institution-id institution-id-type="DOI">10.13039/100000028</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>ETH Future Computing Laboratory</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>BioPIM</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="10"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p><italic toggle="yes">Pairwise sequence alignment</italic> is a computational step commonly required in bioinformatics pipelines (<xref rid="btad151-B5" ref-type="bibr">Alser et al. 2022</xref>), such as in <italic toggle="yes">read mapping</italic> (<xref rid="btad151-B3" ref-type="bibr">Alser et al. 2020a</xref>) and <italic toggle="yes">de novo assembly</italic> (<xref rid="btad151-B30" ref-type="bibr">Li et al. 2011</xref>). We formulate the problem as: (i) finding the <italic toggle="yes">edit distance</italic> between two sequences (<xref rid="btad151-B28" ref-type="bibr">Levenshtein 1966</xref>) and (ii) determining the sequence of corresponding edits. Efficient algorithms for solving this problem optimally are based on <italic toggle="yes">dynamic programming (DP)</italic>, such as the Smith–Waterman–Gotoh algorithm (<xref rid="btad151-B44" ref-type="bibr">Smith and Waterman 1981</xref>; <xref rid="btad151-B23" ref-type="bibr">Gotoh 1982</xref>), and have a runtime that grows quadratically with sequence length (<xref rid="btad151-B7" ref-type="bibr">Alser et al. 2021</xref>). <xref rid="btad151-B9" ref-type="bibr">Backurs and Indyk (2015)</xref> proves no strongly subquadratic time solutions can exist, provided the strong exponential time hypothesis (<xref rid="btad151-B26" ref-type="bibr">Impagliazzo and Paturi 2001</xref>) holds. Hence, recent works focus on approaches such as prealignment filtering <italic toggle="yes">(</italic>e.g. <xref rid="btad151-B53" ref-type="bibr">Xin et al. 2013</xref>, <xref rid="btad151-B52" ref-type="bibr">2015</xref>; <xref rid="btad151-B6" ref-type="bibr">Alser et al. 2019</xref>, <xref rid="btad151-B4" ref-type="bibr">2020b</xref>; <xref rid="btad151-B43" ref-type="bibr">Singh et al. 2021</xref>; <xref rid="btad151-B33" ref-type="bibr">Mansouri Ghiasi et al. 2022</xref>), constant factor algorithmic speedups <italic toggle="yes">(</italic>e.g. <xref rid="btad151-B45" ref-type="bibr">Šošić and Šikić 2017</xref>; <xref rid="btad151-B29" ref-type="bibr">Li 2018</xref>; <xref rid="btad151-B46" ref-type="bibr">Suzuki and Kasahara 2018</xref>; <xref rid="btad151-B34" ref-type="bibr">Marco-Sola et al. 2020</xref>), GPU-based acceleration <italic toggle="yes">(</italic>e.g. <xref rid="btad151-B32" ref-type="bibr">Liu et al. 2013</xref>; <xref rid="btad151-B16" ref-type="bibr">de Oliveira Sandes et al. 2016</xref>; <xref rid="btad151-B1" ref-type="bibr">Ahmedet al. 2019</xref>, <xref rid="btad151-B2" ref-type="bibr">2020</xref>; <xref rid="btad151-B8" ref-type="bibr">Awan et al. 2020</xref>), FPGA-based acceleration <italic toggle="yes">(</italic>e.g. <xref rid="btad151-B12" ref-type="bibr">Benkrid et al. 2009</xref>; <xref rid="btad151-B24" ref-type="bibr">Hoffmann et al. 2016</xref>; <xref rid="btad151-B19" ref-type="bibr">Feiet al. 2018</xref>), or using specialized hardware accelerators <italic toggle="yes">(</italic>e.g. <xref rid="btad151-B21" ref-type="bibr">Fujiki et al. 2018</xref>, <xref rid="btad151-B22" ref-type="bibr">2020</xref>; <xref rid="btad151-B47" ref-type="bibr">Turakhiaet al. 2018</xref>, <xref rid="btad151-B48" ref-type="bibr">2019</xref>; <xref rid="btad151-B41" ref-type="bibr">Senol Cali et al. 2020</xref>, <xref rid="btad151-B42" ref-type="bibr">2022</xref>).</p>
    <p>We observe that GenASM (<xref rid="btad151-B41" ref-type="bibr">Senol Cali et al. 2020</xref>), a recent state-of-the-art sequence alignment algorithm, has a large space for improvement. GenASM uses only cheap bitwise operations and breaks the lower complexity bound of pairwise sequence alignment through its powerful <italic toggle="yes">windowing heuristic</italic>. <xref rid="btad151-B41" ref-type="bibr">Senol Cali et al. (2020)</xref> has already proven the effectiveness of the GenASM algorithm and its accelerator implementation, thus we are motivated to further improve the GenASM algorithm and explore its potential on commodity hardware.</p>
    <p>We identify three inefficiencies in the GenASM algorithm: (i) it has a <italic toggle="yes">large memory footprint</italic> due to the large size of the dynamic programming (DP) table, (ii) it has a <italic toggle="yes">high amount of data movement</italic> between registers and memory due to frequent accesses to the DP table, and (iii) it does some <italic toggle="yes">unnecessary</italic> work by calculating DP cells that are not useful for finding the final result. The three inefficiencies negatively impact both (i) software implementations running on commodity hardware (e.g. CPUs or GPUs) and (ii) custom hardware (e.g. ASIC) implementations.</p>
    <p><italic toggle="yes">Software implementations</italic> on commodity hardware typically cannot fit all the data into fast on-chip memories (e.g. L1, scratchpad memory) due to the large memory footprint. This increases the latency and limits the bandwidth with which the DP table can be accessed. The high amount of data movement puts high pressure on this bandwidth, limiting performance.</p>
    <p>In contrast, <italic toggle="yes">custom hardware implementations</italic> can use arbitrarily large amounts of on-chip memory, but such a large on-chip memory with the high bandwidth requirement is costly. For example, the hardware accelerator described in <xref rid="btad151-B41" ref-type="bibr">Senol Cali et al. (2020)</xref> requires 76% and 54% of the total chip area and power consumption for the on-chip memory that stores the DP table.</p>
    <p>The unnecessary work stems from computing cells that do not contain useful information for finding the final result. This applies to at least 25% of cells on an average for uncorrelated string pairs, and more for correlated string pairs, as we show in Section 2.4.3. Doing unnecessary work affects software and hardware implementations equally because both could use the wasted time to do useful work instead.</p>
    <p>Our goal is to develop a fast and memory-frugal alignment algorithm by addressing the inefficiencies in the GenASM algorithm, and demonstrate its benefits with high-performance CPU and GPU implementations.</p>
    <p>To this end we propose Scrooge, which includes improvements to the GenASM algorithm based on three key ideas:</p>
    <list list-type="bullet">
      <list-item>
        <p>The DP table can be <italic toggle="yes">compressed</italic> by storing only the bitwise AND of multiple values (see Section 2.4.1). The required regions of the DP table can then be decompressed on-demand during traceback with a small computational overhead.</p>
      </list-item>
      <list-item>
        <p>Part of the DP table <italic toggle="yes">does not need to be stored</italic> because the traceback operation cannot reach these entries (see Section 2.4.2).</p>
      </list-item>
      <list-item>
        <p>Part of the DP table can opportunistically be <italic toggle="yes">excluded from calculation</italic> if previous rows of the DP table already contain the information needed for finding the final result (see Section 2.4.3).</p>
      </list-item>
    </list>
    <p>These improvements (i) reduce the number of accesses to GenASMs DP table, (ii) reduce the memory footprint of the DP table, and (iii) eliminate unnecessary work. <italic toggle="yes">Scrooge</italic> is a name for miserly or frugal fictional characters (e.g., <xref rid="btad151-B17" ref-type="bibr">Dickens 1843</xref>), similar to how our proposed algorithm aims to be as resource-efficient as possible.</p>
    <p>We experimentally demonstrate that our improvements yield significant benefits across multiple computing platforms and multiple baseline sequence alignment methods. The CPU version of Scrooge achieves a 20.1<inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula>, 1.7<inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula>, and 2.1<inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula> speedup over CPU-based implementations of KSW2 (<xref rid="btad151-B29" ref-type="bibr">Li 2018</xref>; <xref rid="btad151-B46" ref-type="bibr">Suzuki and Kasahara 2018</xref>), Edlib (<xref rid="btad151-B45" ref-type="bibr">Šošić and Šikić 2017</xref>), GenASM, respectively. The GPU version of Scrooge achieves a 4.0<inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula>, 80.4<inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula>, 6.8<inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula>, 12.6<inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula>, and 5.9<inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula> speedup over CPU-based implementations of Scrooge, KSW2, and Edlib, and GPU-based implementations of Darwin-GPU (<xref rid="btad151-B2" ref-type="bibr">Ahmed et al. 2020</xref>) and GenASM, respectively. We analytically estimate an ASIC implementation of Scrooge to use 3.6<inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula> less chip area and consume 2.1<inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula> less power compared to the prior state-of-the-art ASIC implementation of GenASM (<xref rid="btad151-B41" ref-type="bibr">Senol Cali et al. 2020</xref>) while maintaining the same throughput.</p>
    <p>The contributions of this paper are as follows:</p>
    <list list-type="bullet">
      <list-item>
        <p>We develop three novel algorithmic improvements that are applicable to software and custom hardware implementations of Scrooge, collectively reducing the memory footprint by 24<inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula>, the number of memory accesses by 12<inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula>, and the number of entries of the DP table calculated by at least 25% on an average compared to GenASM.</p>
      </list-item>
      <list-item>
        <p>We experimentally demonstrate the significant throughput (i.e. alignments per second) increase of our improvements for CPU and GPU implementations of Scrooge.</p>
      </list-item>
      <list-item>
        <p>We analytically estimate that an ASIC implementation of Scrooge significantly reduces the chip area and power consumption compared to the prior state-of-the-art ASIC implementation of GenASM.</p>
      </list-item>
      <list-item>
        <p>We open-source all code, including high-performance CPU and GPU implementations of Scrooge, which can be readily used as a sequence alignment library, and all evaluation scripts.</p>
      </list-item>
      <list-item>
        <p>We systematically analyze the throughput and accuracy behavior of GenASM and Scrooge across a range of configurations based on real and simulated datasets for long and short reads. As the best configuration of Scrooge depends on the computing platform, we make several observations that can help guide future implementations of Scrooge.</p>
      </list-item>
    </list>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Overview</title>
      <p>The primary purpose of Scrooge is to accelerate pairwise sequence alignment through (i) a memory-frugal and efficient algorithm, and (ii) optimized CPU, GPU, and ASIC implementations.</p>
      <p>Scrooge solves the <italic toggle="yes">approximate string matching (ASM)</italic> problem with the <italic toggle="yes">edit distance</italic> (<xref rid="btad151-B28" ref-type="bibr">Levenshtein 1966</xref>) as the cost metric. That is, given two strings, <monospace>text</monospace> and <monospace>pattern</monospace>, Scrooge finds the minimum number of single-letter substitutions, insertions, and deletions to convert <monospace>text</monospace> into <monospace>pattern</monospace>. Additionally, the sequence of edits that corresponds the edit distance is reported, which is called <italic toggle="yes">CIGAR string</italic>.</p>
      <p>The Scrooge algorithm is based on the GenASM algorithm (see Section 2.2). <xref rid="btad151-B41" ref-type="bibr">Senol Cali et al. (2020)</xref> first proposed the GenASM algorithm as an algorithm/hardware co-design targeted for an ASIC accelerator, and demonstrated GenASMs potential for very high throughput and resource efficiency. However, as we show in Section 2.3, the GenASM algorithm: (i) requires large amounts of memory bandwidth, (ii) exhibits a large memory footprint, and (iii) does some unnecessary work. These inefficiencies limit GenASMs throughput and resource efficiency on both commodity and custom hardware, and addressing them is critical.</p>
      <p>To this end, we propose Scrooge’s three novel algorithmic improvements to GenASM in Section 2.4. In Section 3.2, we experimentally demonstrate that these improvements significantly increase performance on recent CPUs and GPUs. In Section 3.4, we explore the throughput behavior of GenASM with and without the proposed improvements across various configurations. We show in Section 3.5 that an ASIC implementation of Scrooge will have significantly reduced chip area and power consumption compared to the ASIC designed for GenASM (<xref rid="btad151-B41" ref-type="bibr">Senol Cali et al. 2020</xref>) while maintaining the same throughput. In Section 3.6, we explore the accuracy behavior of GenASM and Scrooge across various configurations.</p>
    </sec>
    <sec>
      <title>2.2 GenASM algorithm</title>
      <p>The GenASM algorithm (<xref rid="btad151-B41" ref-type="bibr">Senol Cali et al. 2020</xref>) consists of two subalgorithms: <italic toggle="yes">GenASM-DC</italic> and <italic toggle="yes">GenASM-TB</italic>. GenASM-DC (see Section 2.2.1) fills a bitvector-based dynamic programming table. The last column of the table indicates the edit distance between the two input strings. GenASM-TB (see Section 2.2.2) re-traces this optimal solution in the constructed table. To better scale with longer input sequences, GenASM uses a <italic toggle="yes">windowing heuristic</italic> (see Section 2.2.3).</p>
      <sec>
        <title>2.2.1 GenASM-DC algorithm</title>
        <p>GenASM-DC uses only cheap bitwise operations to calculate the edit distance between two strings <monospace>text</monospace> and <monospace>pattern</monospace> (<xref rid="btad151-B41" ref-type="bibr">Senol Cali et al. 2020</xref>). It builds an (<monospace>n</monospace><bold><sc> </sc></bold>+<bold><sc> </sc></bold>1)<inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula>(<monospace>k</monospace><bold><sc> </sc></bold>+<bold><sc> </sc></bold>1) dynamic programming (DP) table <monospace>R</monospace>, where <monospace>n</monospace>=<italic toggle="yes">length</italic>(<monospace>text</monospace>) and <monospace>k</monospace> is the maximum number of edits considered. The entries of <monospace>R</monospace> are <monospace>m</monospace>-bit bitvectors, where <monospace>m</monospace>=<italic toggle="yes">length</italic>(<monospace>pattern</monospace>). <xref rid="btad151-F1" ref-type="fig">Figure 1</xref> shows an example of <monospace>R</monospace> after it is constructed by GenASM-DC.</p>
        <fig position="float" id="btad151-F1">
          <label>Figure 1.</label>
          <caption>
            <p>An example of DP table R with <monospace>text=ACGT</monospace> and <monospace>k</monospace> = 4. The bitmasks for <monospace>pattern=ACGA</monospace> are shown on the right. The colored arrows show the possible origins and data dependencies of the 0 at <monospace>d</monospace> = 1, <monospace>i</monospace> = 2, <monospace>j</monospace> = 2. The values in the red marked diagonal are mutually independent and thus can be computed in parallel.</p>
          </caption>
          <graphic xlink:href="btad151f1" position="float"/>
        </fig>
        <statement id="mthst1">
          <p><sc>Theorem</sc> 1. <italic toggle="yes">The entries (bitvectors) of <monospace>R</monospace> can be interpreted as follows:</italic></p>
        </statement>
        <disp-formula id="E1">
          <mml:math id="M1" display="block" overflow="scroll">
            <mml:mtable>
              <mml:mtr>
                <mml:mtd>
                  <mml:mi mathvariant="monospace">j</mml:mi>
                  <mml:mo>-</mml:mo>
                  <mml:mtext>th bit of</mml:mtext>
                  <mml:mo> </mml:mo>
                  <mml:mi mathvariant="monospace">R</mml:mi>
                  <mml:mo stretchy="false">[</mml:mo>
                  <mml:mi mathvariant="monospace">i</mml:mi>
                  <mml:mo stretchy="false">]</mml:mo>
                  <mml:mo stretchy="false">[</mml:mo>
                  <mml:mi mathvariant="monospace">d</mml:mi>
                  <mml:mo stretchy="false">]</mml:mo>
                  <mml:mo>=</mml:mo>
                  <mml:mn>0</mml:mn>
                  <mml:mo>⇔</mml:mo>
                </mml:mtd>
              </mml:mtr>
              <mml:mtr>
                <mml:mtd>
                  <mml:mi>d</mml:mi>
                  <mml:mi mathvariant="italic">istance</mml:mi>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mi mathvariant="monospace">text</mml:mi>
                  <mml:mo stretchy="false">[</mml:mo>
                  <mml:mi mathvariant="monospace">i</mml:mi>
                  <mml:mo>:</mml:mo>
                  <mml:mi mathvariant="monospace">n</mml:mi>
                  <mml:mo stretchy="false">)</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mi mathvariant="monospace">pattern</mml:mi>
                  <mml:mo stretchy="false">[</mml:mo>
                  <mml:mi mathvariant="monospace">j</mml:mi>
                  <mml:mo>:</mml:mo>
                  <mml:mi mathvariant="monospace">m</mml:mi>
                  <mml:mo stretchy="false">)</mml:mo>
                  <mml:mo stretchy="false">)</mml:mo>
                  <mml:mo>≤</mml:mo>
                  <mml:mi mathvariant="monospace">d</mml:mi>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
        <p>In natural language, Theorem 1 states that the <monospace>j</monospace>th bit of the bitvector <monospace>R[i][d]</monospace> is 0 exactly if the suffix of <monospace>text</monospace> starting at character <monospace>i</monospace> and the suffix of <monospace>pattern</monospace> starting at character <monospace>j</monospace> differ by at most <monospace>d</monospace> edits. Following this interpretation, the first row <monospace>d</monospace><bold><sc> </sc></bold><monospace>=</monospace><bold><sc> </sc></bold><monospace>d</monospace><inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi mathvariant="italic">OPT</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> that has a <monospace>0</monospace> in the first bit (<monospace>j</monospace><bold><sc> </sc></bold><monospace>=</monospace><bold><sc> </sc></bold><monospace>0</monospace>) of the leftmost column (<monospace>i</monospace><bold><sc> </sc></bold><monospace>=</monospace><bold><sc> </sc></bold><monospace>0</monospace>) indicates that the edit distance between <monospace>text</monospace> and <monospace>pattern</monospace> is <monospace>d</monospace><inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi mathvariant="italic">OPT</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. This bit is marked in pink in <xref rid="btad151-F1" ref-type="fig">Fig. 1</xref>.</p>
        <p>GenASM-DC (<xref rid="btad151-BOX1" ref-type="boxed-text">Algorithm 1</xref>) starts by preprocessing <monospace>pattern</monospace> into four <italic toggle="yes">pattern masks</italic>, one per character in the alphabet. The pattern mask for character <monospace>X</monospace><inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mo>∈</mml:mo></mml:math></inline-formula><inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> is a bitvector of length <monospace>m</monospace>=<italic toggle="yes">length</italic>(<monospace>pattern</monospace>), with a 0 in the <monospace>i</monospace>th bit if <monospace>pattern</monospace>[<monospace>i</monospace>]==<monospace>X</monospace>. See <xref rid="btad151-F1" ref-type="fig">Fig. 1</xref> for an example.</p>
        <p>GenASM-DC populates the rightmost column (Line 5) and topmost row (Line 11) of <monospace>R</monospace>. The remaining entries are then calculated from their respective neighbors in the north (Line 13, insertion), north-east (Lines 14–15, deletion and substitution), and east (Line 16, match) through simple bitwise update rules. We refer to (<xref rid="btad151-B10" ref-type="bibr">Baeza-Yates and Gonnet 1992</xref>; <xref rid="btad151-B51" ref-type="bibr">Wu and Manber 1992</xref>; <xref rid="btad151-B41" ref-type="bibr">Senol Cali et al. 2020</xref>) for detailed arguments on the correctness of GenASM-DC. To follow the rest of this paper, it is sufficient to consider: (i) the interpretation of <monospace>R</monospace> given in Theorem 1, and (ii) the north-east data dependencies imposed by <xref rid="btad151-BOX1" ref-type="boxed-text">Algorithm 1</xref> and shown in <xref rid="btad151-F1" ref-type="fig">Fig. 1</xref>.<boxed-text id="btad151-BOX1" position="float"><label>Algorithm 1.</label><caption><p>GenASM-DC Algorithm</p></caption><p><inline-graphic xlink:href="btad151ilf1.jpg"/></p></boxed-text><bold>Intra-Task Parallelism.</bold><xref rid="btad151-B41" ref-type="bibr">Senol Cali et al. (2020)</xref> enables efficient intra-task parallelism by identifying that the DP entries within each north-west to south-east diagonal (one such diagonal is marked in red in <xref rid="btad151-F1" ref-type="fig">Fig. 1</xref>) do <italic toggle="yes">not</italic> depend on each other, hence they can be computed in parallel.</p>
      </sec>
      <sec>
        <title>2.2.2 GenASM-TB algorithm</title>
        <p>For use-cases like read mapping, the pairwise sequence alignment algorithm should report both the edit distance and the corresponding sequence of edits, which is called the <italic toggle="yes">CIGAR string</italic>. Obtaining the CIGAR string involves retracing the origin of the edit distance value as a linear path through DP entries in their reverse construction order; this process is called <italic toggle="yes">traceback</italic>.</p>
        <p>GenASM enables efficient traceback operations based on two key observations: First, if <italic toggle="yes">all</italic> intermediate values of variables <monospace>I</monospace>, <monospace>D</monospace>, <monospace>S</monospace>, and <monospace>M</monospace> in <xref rid="btad151-BOX1" ref-type="boxed-text">Algorithm 1</xref> are stored, then one can follow the path of <monospace>0</monospace> s in these variables, starting from <monospace>0</monospace> in the west of <monospace>R</monospace> that indicates the edit distance (highlighted in pink in <xref rid="btad151-F1" ref-type="fig">Fig. 1</xref>) and go towards the north-east corner of <monospace>R</monospace>. Whenever a 0 in one of these variables is traversed, the name of that variable is recorded as an edit (e.g. ’<monospace>I</monospace>’ for an insertion). Second, it is sufficient to store only three out of the four variables (because <monospace>S</monospace> can be obtained by shifting <monospace>D</monospace>), saving both memory footprint and bandwidth.</p>
      </sec>
      <sec>
        <title>2.2.3 GenASMs windowing heuristic</title>
        <p>To provide a linear runtime complexity, <xref rid="btad151-B41" ref-type="bibr">Senol Cali et al. (2020)</xref> proposes a greedy <italic toggle="yes">windowing heuristic</italic>. Instead of aligning <monospace>text</monospace> and <monospace>pattern</monospace> in a single run of GenASM-DC, the windowing heuristic runs GenASM-DC multiple times as a subroutine in <italic toggle="yes">windows</italic> of size <monospace>W</monospace>. In each window, a prefix of size <monospace>W</monospace> characters of each sequence (i.e. <inline-formula id="IE50"><mml:math id="IM50" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="monospace">text</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi mathvariant="monospace">W</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE51"><mml:math id="IM51" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="monospace">pattern</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi mathvariant="monospace">W</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>) are aligned. The first <inline-formula id="IE52"><mml:math id="IM52" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="monospace">W</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="monospace">O</mml:mi></mml:mrow></mml:math></inline-formula> characters of the window are greedily considered aligned optimally, where we call <monospace>O</monospace> the window <italic toggle="yes">overlap</italic>. The smaller strings <inline-formula id="IE53"><mml:math id="IM53" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="monospace">text</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="monospace">W</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="monospace">O</mml:mi><mml:mo>:</mml:mo><mml:mi mathvariant="monospace">n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE54"><mml:math id="IM54" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="monospace">pattern</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="monospace">W</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="monospace">O</mml:mi><mml:mo>:</mml:mo><mml:mi mathvariant="monospace">m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> then remain to be aligned in the next window.</p>
        <p>This approach has three advantages. First, instead of constructing a large table of <monospace>n</monospace><inline-formula id="IE55"><mml:math id="IM55" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula><monospace>m</monospace><inline-formula id="IE56"><mml:math id="IM56" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula><monospace>k</monospace> bits, only <inline-formula id="IE57"><mml:math id="IM57" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mi mathvariant="monospace">m</mml:mi><mml:mrow><mml:mi mathvariant="monospace">W</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="monospace">O</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> tables of <inline-formula id="IE58"><mml:math id="IM58" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="monospace">W</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> bits must be constructed, saving memory footprint, data movement, and computation. Second, the GenASM-DC subroutine now runs over constant-sized sequences, simplifying its implementation. For example, DP entries can be statically assigned to processing elements (<xref rid="btad151-B41" ref-type="bibr">Senol Cali <italic toggle="yes">et al.</italic> 2020</xref>), and the data movement and exact memory footprint are known at compile time, even if the length of the input sequences is unknown. Third, the program flow (e.g. the number of loop iterations per window) is entirely known at compile time, giving the compiler the ability to optimize.</p>
        <p>The windowing strategy is greedy and heuristic, so it is possible that it could miss the optimal alignment and produce a suboptimal one instead. This is a key limitation of GenASM and Scrooge. Note that several state-of-the-art tools do not give any optimality guarantees either, and instead experimentally demonstrate their practical accuracy, as Scrooge does. This includes greedy alignment techniques like SeGraM (<xref rid="btad151-B42" ref-type="bibr">Senol Cali et al. 2022</xref>), Darwin (<xref rid="btad151-B47" ref-type="bibr">Turakhia et al. 2018</xref>), and WFA-adaptive (<xref rid="btad151-B34" ref-type="bibr">Marco-Sola et al. 2020</xref>), as well as mappers based on sparse dynamic programming, like minimap2 (<xref rid="btad151-B29" ref-type="bibr">Li 2018</xref>). To balance performance and accuracy, the tunable parameters <monospace>W</monospace> (window size) and <monospace>O</monospace> (window overlap) must be selected appropriately. The parameter <monospace>W</monospace> can be understood as the <italic toggle="yes">range</italic> of solutions considered, similar to the <italic toggle="yes">band width</italic> (<xref rid="btad151-B49" ref-type="bibr">Ukkonen 1985</xref>) in popular alignment implementations (e.g. <xref rid="btad151-B45" ref-type="bibr">Šošić and Šikić 2017</xref>; <xref rid="btad151-B29" ref-type="bibr">Li 2018</xref>; <xref rid="btad151-B46" ref-type="bibr">Suzuki and Kasahara 2018</xref>). The parameter <monospace>O</monospace> can be understood as the <italic toggle="yes">globality</italic> of the solutions or <italic toggle="yes">inverse greediness</italic>. We demonstrate in Section 3: (i) that higher <monospace>W</monospace> and <monospace>O</monospace> generally improve accuracy, at the cost of lowering throughput, (ii) that the best choice of <monospace>W</monospace> and <monospace>O</monospace> depends on the input dataset (e.g. its error distribution and read lengths), and (iii) that <monospace>W</monospace> = 64 and <monospace>O</monospace> = 33 achieve a good throughput/accuracy tradeoff for long and short read mapping.</p>
      </sec>
    </sec>
    <sec>
      <title>2.3 Inefficiencies in the GenASM algorithm</title>
      <p>We identify three inefficiencies in the GenASM algorithm: (i) it has a large amount of data movement, (ii) it has a large memory footprint, and (iii) it does some unnecessary work.</p>
      <p>The combination of large amount of data movement and large memory footprint, which we quantify in Sections 2.3.1 and 2.3.2, respectively, affects both software implementations running on commodity hardware, as well as custom hardware implementations. Commodity hardware (e.g. CPUs or GPUs) has a fixed amount of on-chip memory. The DP table might not fit into this on-chip memory, which introduces three inefficiencies: Data have to be moved a larger distance, which increases (i) access latency and (ii) access energy (<xref rid="btad151-B13" ref-type="bibr">Boroumand et al. 2018</xref>). (iii) The high amount of data movement puts high pressure on memory bandwidth, which is scarce when accessing data residing off-chip. This causes the entire application to become memory bandwidth-bound, thus wasting compute resources and achieving suboptimal performance. Custom hardware implementations (e.g. ASICs) can have as large on-chip memory as needed, but such a large and high-bandwidth on-chip memory comes at the cost of a large chip area and power consumption (<xref rid="btad151-B14" ref-type="bibr">Boroumand et al. 2021</xref>).</p>
      <p>Doing unnecessary work trivially wastes runtime and energy. In Section 2.4.3, we identify the DP entries that are calculated needlessly by GenASM, and quantify how frequent they are.</p>
      <sec>
        <title>2.3.1 Roofline model</title>
        <p>We use the <italic toggle="yes">roofline model</italic> (<xref rid="btad151-B50" ref-type="bibr">Williams et al. 2009</xref>; <xref rid="btad151-B39" ref-type="bibr">Ofenbeck et al. 2014</xref>) to visualize that GenASM has a large amount of data movement, and that its operational intensity (i.e. the number of operations per byte) is too low to saturate the compute resources of modern CPUs and GPUs. The roofline model plots the upper limit of achievable compute throughput for different operational intensities for a given processor. It consists of horizontal peak compute throughput rooflines, and sloped memory bandwidth rooflines.</p>
        <p><xref rid="btad151-F2" ref-type="fig">Figure 2</xref> shows the roofline plots for an Intel Xeon Gold 5118 CPU (<xref rid="btad151-B27" ref-type="bibr">Intel 2017</xref>) and an NVIDIA A6000 GPU (<xref rid="btad151-B37" ref-type="bibr">NVIDIA 2020</xref>), including their respective on-chip memory (<italic toggle="yes">shared memory</italic> in CUDA), cache, and off-chip memory (<italic toggle="yes">global memory</italic> in CUDA) bandwidths (drawn in shades of blue) and peak compute throughputs (draw in shades of green). GenASMs operational intensity is drawn in red. We derive the roofline parameters in Section 8 of the <xref rid="sup1" ref-type="supplementary-material">Supplementary Materials</xref>.</p>
        <fig position="float" id="btad151-F2">
          <label>Figure 2.</label>
          <caption>
            <p>The roofline models of (a) an Intel Xeon Gold 5118 CPU and (b) an NVIDIA A6000 GPU.</p>
          </caption>
          <graphic xlink:href="btad151f2" position="float"/>
        </fig>
        <p>From <xref rid="btad151-F2" ref-type="fig">Fig. 2</xref>, we make three observations. First, if the data resides off-chip, GenASM is heavily memory bandwidth-bound for a modern CPU and GPU. This is evidenced by the red (algorithm) and dark blue (off-chip memory bandwidth) lines intersecting far below the green (peak compute throughput) line. Second, GenASM would no longer be memory bandwidth-bound if its computational intensity were <inline-formula id="IE59"><mml:math id="IM59" display="inline" overflow="scroll"><mml:mo>≥</mml:mo></mml:math></inline-formula>10<inline-formula id="IE60"><mml:math id="IM60" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula> higher, because then the red (algorithm) line would be shifted to the right and intersect with the dark blue (off-chip memory bandwidth) line above the green (peak compute throughput) line. The operational intensity could be increased by reducing GenASMs data movement. Third, if the data reside in the fastest on-chip memory, GenASM <italic toggle="yes">can</italic> reach peak compute throughput, even with the high amount of data movement in the baseline algorithm. This is evidenced by the red (algorithm) and light blue (L1/shared memory bandwidth) lines intersecting above the green (peak compute throughput) line. However, as we show in Section 2.3.2, GenASMs memory footprint is too large for the typical capacity of such fast on-chip memories in commodity hardware, and building large enough on-chip memories is costly.</p>
        <p>Based on these observations, we conclude that: (i) GenASM cannot saturate commodity hardware with computation, and (ii) data movement should be reduced to address this inefficiency.</p>
      </sec>
      <sec>
        <title>2.3.2 Memory footprint</title>
        <p>In this section, we demonstrate the overheads associated with GenASMs large memory footprint.</p>
        <p>We derive GenASMs working set memory footprint to be 96.5<italic toggle="yes">KiB</italic> in Section 9 of the <xref rid="sup1" ref-type="supplementary-material">Supplementary Materials</xref>. For comparison, the Intel Xeon Gold 5118 has 32<italic toggle="yes">KiB</italic> of L1D cache per core (<xref rid="btad151-B27" ref-type="bibr">Intel 2017</xref>) and NVIDIAs <italic toggle="yes">Ampere</italic> GPU microarchitecture provides up to 99<italic toggle="yes">KiB</italic> of high-bandwidth on-chip memory per GPU core (<italic toggle="yes">streaming multiprocessor</italic>, <italic toggle="yes">SM</italic> in CUDA) (<xref rid="btad151-B38" ref-type="bibr">NVIDIA 2023</xref>). Thus, one SM can hold the DP table for exactly one GenASM problem instance in its on-chip memory. One thread block of two warps (i.e. 2<inline-formula id="IE61"><mml:math id="IM61" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula>32 threads) can work on a single GenASM problem instance, but this does not saturate the compute resources in the SM. This is because modern GPUs are designed to alternate between executing <italic toggle="yes">multiple</italic> independent instruction streams for the purpose of hiding the latency of instructions (<xref rid="btad151-B31" ref-type="bibr">Lindholm et al. 2008</xref>). Underutilization of the compute resources in an SM due to too few independent instruction streams is called <italic toggle="yes">low occupancy</italic> and causes the unused computational resources to be wasted (<xref rid="btad151-B38" ref-type="bibr">NVIDIA 2023</xref>). Hence, the occupancy should be increased by working on multiple problem instances per SM. Multiple problem instances can fit into memory by <italic toggle="yes">either</italic> reducing the memory footprint per problem instance, <italic toggle="yes">or</italic> placing the DP tables into the GPUs off-chip memory, which has a much larger capacity. Our goal is the former, as we show in Section 2.3.1 that the latter is <italic toggle="yes">not</italic> an efficient a solution due to the off-chip memory’s limited bandwidth.</p>
        <p>Custom hardware implementations (e.g. ASICs) can potentially have as large on-chip memory as needed. For example, the GenASM ASIC (<xref rid="btad151-B41" ref-type="bibr">Senol Cali <italic toggle="yes">et al.</italic> 2020</xref>) uses scratchpads of 96.5 <italic toggle="yes">KiB</italic> each to hold the DP tables. However, these scratchpads occupy 76% of the total chip area and consume over 54% of the chip power. This limits the performance achievable with a given chip area and power budget.</p>
        <p>In summary, GenASM has a large memory footprint compared to typical on-chip memory capacities in commodity hardware, and while sufficiently large on-chip scratchpads can be designed for custom hardware implementations, it is costly to do so.</p>
      </sec>
    </sec>
    <sec>
      <title>2.4 Scrooge</title>
      <p>We have shown in Section 2.3 that GenASM has a high amount of data movement <italic toggle="yes">and</italic> high memory footprint per problem instance. We have elaborated that this combination either limits performance (on commodity hardware), or requires expensive large on-chip memories (on custom hardware), both of which are undesirable. Thus, our strategy is to reduce the GenASM algorithm’s memory footprint as much as possible while introducing minimal computational overhead. We present three novel algorithmic improvements that collectively achieve a 24<inline-formula id="IE62"><mml:math id="IM62" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula> reduction in memory footprint, as well as a 12<inline-formula id="IE63"><mml:math id="IM63" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula> reduction in data movement from the memory that holds the DP table.</p>
      <sec>
        <title>2.4.1 Improvement 1—store entries, not edges</title>
        <p>As we explain in Section 2.2.2, GenASM stores 3 bitvectors per entry of the table <monospace>R</monospace> to enable traceback. If we imagine a graph where the entries of <monospace>R</monospace> are nodes and the intermediate bitvectors are edges connecting their source and target entries, GenASM stores 3 ingoing edges for most nodes (<xref rid="btad151-F3" ref-type="fig">Fig. 3</xref>). We propose to trade off the majority of this memory footprint for a small increase in computation with the store entries, not edges (<italic toggle="yes">SENE</italic>) improvement. SENE regenerates the required edges on-demand during traceback from stored nodes (entries of table <monospace>R</monospace>) by applying the update rules in <xref rid="btad151-BOX1" ref-type="boxed-text">Algorithm 1</xref> on requested neighbor entries.</p>
        <fig position="float" id="btad151-F3">
          <label>Figure 3.</label>
          <caption>
            <p>Per cell, GenASM stores three edges for traceback. Scrooge with SENE stores only the DP entry itself instead; the needed edges are regenerated on the fly during traceback.</p>
          </caption>
          <graphic xlink:href="btad151f3" position="float"/>
        </fig>
        <p><bold>Cost and benefits.</bold> Since traceback explores only a single path across the table <monospace>R</monospace>, only <inline-formula id="IE64"><mml:math id="IM64" display="inline" overflow="scroll"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="monospace">W</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> edges are regenerated, making the overhead of this extra computation small compared to computing the table of <inline-formula id="IE65"><mml:math id="IM65" display="inline" overflow="scroll"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="monospace">W</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> entries. Storing <monospace>R</monospace> requires storing <inline-formula id="IE66"><mml:math id="IM66" display="inline" overflow="scroll"><mml:mrow><mml:mn>65</mml:mn><mml:mo>×</mml:mo><mml:mn>65</mml:mn></mml:mrow></mml:math></inline-formula> entries of 64 bits each, for a total of <inline-formula id="IE67"><mml:math id="IM67" display="inline" overflow="scroll"><mml:mrow><mml:mn>33</mml:mn><mml:mo>,</mml:mo><mml:mn>800</mml:mn><mml:mi>B</mml:mi><mml:mo>≈</mml:mo><mml:mn>33</mml:mn><mml:mi>k</mml:mi><mml:mi>i</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:math></inline-formula>. The previous memory footprint was 96.5<italic toggle="yes">kiB</italic> as derived in Section 2.3.2, yielding a <inline-formula id="IE68"><mml:math id="IM68" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>96.5</mml:mn></mml:mrow><mml:mrow><mml:mn>33</mml:mn></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>2.92</mml:mn><mml:mo>≈</mml:mo><mml:mn>3</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> improvement in memory footprint. Since each of these locations is still only written to once during the construction of <monospace>R</monospace>, SENE also reduces the data movement from the memory that holds the DP table by 3<inline-formula id="IE69"><mml:math id="IM69" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula>.</p>
      </sec>
      <sec>
        <title>2.4.2 Improvement 2—discard entries not used by traceback</title>
        <p>The windowing heuristic (see Section 2.2.3) mandates that traceback covers only the first <inline-formula id="IE70"><mml:math id="IM70" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="monospace">W</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="monospace">O</mml:mi></mml:mrow></mml:math></inline-formula> characters of each window. This means that traceback never reads the table entries of the last <monospace>O</monospace> characters in each window.</p>
        <p>We propose to discard the entries that can never be reached by traceback, an improvement we call discard entries not used by traceback (<italic toggle="yes">DENT</italic>). These include the last <monospace>O</monospace> columns of <monospace>R</monospace> and the last <monospace>O</monospace>−1 bits of every bitvector. The resulting DP table consists of <inline-formula id="IE71"><mml:math id="IM71" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="monospace">W</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="monospace">O</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> columns, <inline-formula id="IE72"><mml:math id="IM72" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="monospace">W</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> rows, and <inline-formula id="IE73"><mml:math id="IM73" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="monospace">W</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="monospace">O</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> bits per entry. <xref rid="btad151-F4" ref-type="fig">Figure 4</xref> shows an example for <monospace>W</monospace> = 4 and <monospace>O</monospace> = 3, where Scrooge stores only the leftmost two columns and leftmost 2 bits per entry, because traceback does not reach the rightmost three columns and rightmost 2 bits per entry.</p>
        <fig position="float" id="btad151-F4">
          <label>Figure 4.</label>
          <caption>
            <p>DENT exploits that the windowing heuristic stops traceback after the first <inline-formula id="IE74"><mml:math id="IM74" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="monospace">W</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="monospace">O</mml:mi></mml:mrow></mml:math></inline-formula> edges are crossed (here <monospace>W</monospace> = 4 and <monospace>O</monospace> = 3). The area never reached by traceback can be discarded.</p>
          </caption>
          <graphic xlink:href="btad151f4" position="float"/>
        </fig>
        <p>Due to the fixed word sizes and word alignment requirements of commodity hardware, the number of bits stored for each bitvector cannot be chosen freely. We show in Section 3.2 that for a modern GPU <inline-formula id="IE75"><mml:math id="IM75" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="monospace">O</mml:mi><mml:mo>=</mml:mo><mml:mn>33</mml:mn></mml:mrow></mml:math></inline-formula> achieves the best throughput results for <inline-formula id="IE76"><mml:math id="IM76" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="monospace">W</mml:mi><mml:mo>=</mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:math></inline-formula>, because the stored bitvectors perfectly fit into a 32-bit word. In contrast, <xref rid="btad151-B41" ref-type="bibr">Senol Cali et al. (2020)</xref> use <monospace>O</monospace> = 24 for its ASIC design, which we show to be suboptimal on commodity hardware. Note that increasing <monospace>O</monospace><italic toggle="yes">improves</italic> accuracy, see Section 2.2.3 for an intuition and Section 3.6 for experimental results.</p>
        <p><bold>Cost and benefits.</bold> DENT incurs two computational overheads: First, the bits to store have to be determined and extracted from the bitvectors. Second, increasing <monospace>O</monospace> from 24 to 33 means the algorithm makes nine characters less progress per window.</p>
        <p>By discarding the right half of each bitvector and the rightmost <monospace>O</monospace> columns of <monospace>R</monospace>, DENT improves the memory footprint by <inline-formula id="IE77"><mml:math id="IM77" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mi mathvariant="monospace">W</mml:mi><mml:mrow><mml:mi mathvariant="monospace">W</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="monospace">O</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="monospace">W</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="monospace">W</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="monospace">O</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>64</mml:mn></mml:mrow><mml:mrow><mml:mn>32</mml:mn></mml:mrow></mml:mfrac><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mn>65</mml:mn></mml:mrow><mml:mrow><mml:mn>32</mml:mn></mml:mrow></mml:mfrac><mml:mo>≈</mml:mo><mml:mn>4</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula>. We describe in Section 1 of the <xref rid="sup1" ref-type="supplementary-material">Supplementary Materials</xref> how DENT can be extended to store only half the rows of <monospace>R</monospace> for a total 8<inline-formula id="IE78"><mml:math id="IM78" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula> memory footprint reduction.</p>
        <p>By the same calculation, the number of writes to table <monospace>R</monospace> is reduced by approximately 4<inline-formula id="IE79"><mml:math id="IM79" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula>, assuming the forefront diagonal (marked red in <xref rid="btad151-F1" ref-type="fig">Fig. 1</xref>) is kept in registers and communicated directly.</p>
      </sec>
      <sec>
        <title>2.4.3 Improvement 3—early termination</title>
        <p>The edit distance is determined by the highest row of <monospace>R</monospace> that contains a 0 in the most significant bit in the leftmost column. Traceback starts from this entry. Since entries are constructed from their north, north-east, and east neighbors, the traceback path can only go to the north, north-east, and east. It can <italic toggle="yes">never</italic> go south. Thus, at no point do the rows of higher cost than <inline-formula id="IE80"><mml:math id="IM80" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">distance</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="monospace">pattern</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="monospace">text</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> contain useful information for traceback (see <xref rid="btad151-F5" ref-type="fig">Fig. 5</xref> for an example). We propose building <monospace>R</monospace> row-wise, and terminating the algorithm early as soon as the most significant bit in the first entry of the current row is a 0.</p>
        <fig position="float" id="btad151-F5">
          <label>Figure 5.</label>
          <caption>
            <p>The colored edges indicate the path taken by traceback for <monospace>W</monospace> = 4 and <monospace>O</monospace> = 0. Rows below the edit distance do not contain useful information for traceback. Thus, they do not need to be computed (Early Termination).</p>
          </caption>
          <graphic xlink:href="btad151f5" position="float"/>
        </fig>
        <p><bold>Cost and benefits.</bold> Early termination (ET) does not yield a constant factor improvement in either memory footprint or runtime: If <inline-formula id="IE81"><mml:math id="IM81" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">distance</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="monospace">pattern</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="monospace">text</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="monospace">W</mml:mi></mml:mrow></mml:math></inline-formula>, we are not able to terminate early at all. However, typical input pairs incur fewer than <monospace>W</monospace> edits in a single window. For correct candidate pairs, the edit distance will be low, e.g. up to 15% for long reads (<xref rid="btad151-B7" ref-type="bibr">Alser et al. 2021</xref>). Even uncorrelated random sequence pairs of length <monospace>W</monospace> over a 4-letter alphabet have an edit distance of at most <inline-formula id="IE82"><mml:math id="IM82" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula><monospace>W</monospace> on an average, as we prove in Section 10 of the <xref rid="sup1" ref-type="supplementary-material">Supplementary Materials</xref>. Thus, on an average, Scrooge can skip at least 25% of the entries of <monospace>R</monospace>, saving computation as well as data movement (see Section 2.3.1).</p>
        <p><bold>Conflict with intra-task parallelism.</bold> Recall from Section 1 that GenASM provides the option for intra-task parallelism. Exploiting this parallelism requires the available processing elements to build <monospace>R</monospace> in a <italic toggle="yes">diagonal-wise</italic> fashion, as shown in <xref rid="btad151-F1" ref-type="fig">Fig. 1</xref>. However, as we describe in Section 2.4.3, to make full use of ET, <monospace>R</monospace> should be built <italic toggle="yes">row-wise</italic>. As a compromise, we implement ET in a diagonal-wise fashion in our GPU implementation. As in the row-wise version, construction on <monospace>R</monospace> stops as soon as the leftmost processing element finds a 0 in the most significant bit. Due to the diagonal-wise computation, the other processing elements have already computed several rows ahead at this point, i.e. done unnecessary work. For this reason, the benefit of ET is limited in intra-task parallel implementations, such as our GPU implementation, while being much more significant in row-wise implementations, such as our CPU implementation. We reaffirm these effects experimentally in Section 3.4.</p>
      </sec>
    </sec>
    <sec>
      <title>2.5 Implementation</title>
      <p>We implement C++ versions of our algorithm for x86 CPUs and NVIDIA GPUs. They are exposed as simple library functions for pairwise sequence alignment. Each improvement and implementation constant can be easily configured at compile time through preprocessor macros. The implementations, as well as baselines and evaluation scripts, are available at <ext-link xlink:href="https://github.com/cmu-safari/Scrooge" ext-link-type="uri">https://github.com/cmu-safari/Scrooge</ext-link>.</p>
      <p><bold>CPU.</bold> The CPU version converts the input pairs to a two-bit-per-basepair encoding, but padded to 8 bits. Each thread works on a single pairwise alignment at a time and obtains sequence pairs from a global queue. During each call to the GenASM-DC subroutine, the thread calculates the DP table <monospace>R</monospace> in a row-wise fashion.</p>
      <p><bold>GPU.</bold> The GPU version is implemented using CUDA 11.1 (<xref rid="btad151-B38" ref-type="bibr">NVIDIA 2023</xref>) and targets GPUs of compute capability 7.0 and higher (<ext-link xlink:href="https://developer.nvidia.com/cuda-gpus" ext-link-type="uri">https://developer.nvidia.com/cuda-gpus</ext-link>). The input sequence pairs are converted to a two-bit-per-basepair encoding and transferred to the GPU. Each thread block works on a single pairwise alignment at a time and obtains sequence pairs from a global queue. During each call to the GenASM-DC subroutine, the thread block calculates the DP table <monospace>R</monospace> in a diagonal-wise fashion, and each of the <monospace>W</monospace> threads in the thread block calculates a single column of <monospace>R</monospace>. Threads resolve their mutual data dependencies using warp shuffle instructions within a warp and using shared memory across warps. A single thread per warp executes the traceback operation. The size of the CIGAR string is not known ahead of time, hence it is stored as a linked list in global memory.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 Evaluation methodology</title>
      <p>We demonstrate the benefits of Scrooge (along with each of our three algorithmic improvements) using both CPU and GPU implementations by comparing it to the recent WFA lm (<xref rid="btad151-B18" ref-type="bibr">Eizenga and Paten 2022</xref>), WFA (<xref rid="btad151-B34" ref-type="bibr">Marco-Sola et al. 2020</xref>), KSW2 (<xref rid="btad151-B46" ref-type="bibr">Suzuki and Kasahara 2018</xref>) (the state-of-the-art aligner used in minimap2; <xref rid="btad151-B29" ref-type="bibr">Li 2018</xref>), Edlib (<xref rid="btad151-B45" ref-type="bibr">Šošić and Šikić 2017</xref>) [the state-of-the-art implementation of Myers’ bitvector algorithm (<xref rid="btad151-B36" ref-type="bibr">Myers 1999</xref>) used in Medaka (<ext-link xlink:href="https://github.com/nanoporetech/medaka" ext-link-type="uri">https://github.com/nanoporetech/medaka</ext-link>) and Dysgu (<xref rid="btad151-B15" ref-type="bibr">Cleal and Baird 2022</xref>)], CUDASW++3.0 (<xref rid="btad151-B32" ref-type="bibr">Liu et al. 2013</xref>), Darwin-GPU (<xref rid="btad151-B2" ref-type="bibr">Ahmed et al. 2020</xref>), and our CPU and GPU implementations of the GenASM algorithm.</p>
      <p>We evaluate the throughput and accuracy of Scrooge via three classes of experiments. First, we compare the throughputs of all evaluated tools and show that Scrooge outperforms state-of-the-art aligners. Second, we evaluate the throughput benefits of Scrooge’s algorithmic improvements and its sensitivity to different choices for <monospace>W</monospace> and <monospace>O</monospace>. Third, we evaluate Scrooge’s accuracy. We define throughput as the number of pairwise sequence alignments per second for a given dataset.</p>
      <p>We run all CPU evaluations on a dual-socket Intel Xeon Gold 5118 (2<inline-formula id="IE83"><mml:math id="IM83" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula> 12 physical cores, 2<inline-formula id="IE84"><mml:math id="IM84" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula> 24 logical cores) (<xref rid="btad151-B27" ref-type="bibr">Intel 2017</xref>) at 3.2 GHz with 196GiB DDR4 RAM. We run all GPU evaluations on an NVIDIA A6000 (<xref rid="btad151-B37" ref-type="bibr">NVIDIA 2020</xref>). We repeat all CPU and GPU experiments 10 times and 5 times, respectively, and average the results.</p>
      <sec>
        <title>3.1.1 Datasets</title>
        <p>We simulate 115 240 PacBio reads from the human genome using PBSIM2 (<xref rid="btad151-B40" ref-type="bibr">Ono et al. 2020</xref>), each of length 10 kilobases and with a target error rate of 5%. We obtain the ground truth location in the reference genome, and the alignment (CIGAR string) of each read from PBSIM2, thus obtaining 115 240 candidate pairs for our <italic toggle="yes">long read groundtruth</italic> dataset. We map 500 of the simulated PacBio reads to the human genome using minimap2 (<xref rid="btad151-B29" ref-type="bibr">Li 2018</xref>) and obtain all chains (candidate locations) it generates using the <monospace>-P</monospace> flag, 138 929 locations in total. This constitutes our <italic toggle="yes">long read</italic> dataset. We map 100 000 Illumina short reads from the dataset with accession number SRR13278681 to the human genome using minimap2 (<xref rid="btad151-B29" ref-type="bibr">Li 2018</xref>) and obtain all chains (candidate locations) it generates using the <monospace>-P</monospace> flag, 9 612 222 locations in total. This constitutes our <italic toggle="yes">short read</italic> dataset. We show further statistics of the datasets in Section 2 of the <xref rid="sup1" ref-type="supplementary-material">Supplementary Materials</xref>, including error, error rate, and sequence length distributions. The exact datasets and command lines that produced all our results, including those in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Materials</xref>, are available at our GitHub repository: <ext-link xlink:href="https://github.com/cmu-safari/Scrooge" ext-link-type="uri">https://github.com/cmu-safari/Scrooge</ext-link>.</p>
      </sec>
    </sec>
    <sec>
      <title>3.2 Throughput</title>
      <p>We run the CPU-based tools using 48 threads. We set the band width (i.e. the edit distance threshold) of Edlib and KSW2 to 15% of the read length. We configure WFA-adaptive as recommended by its authors. We take the fastest configuration from a parameter sweep for Darwin-GPU. For a meaningful comparison, we ensure that Darwin-GPUs alignment component fully aligns all sequence pairs. We explain our changes to Darwin-GPU in Section 7 of the <xref rid="sup1" ref-type="supplementary-material">Supplementary Materials</xref>. We empirically configure Scrooge’s CPU and GPU implementations with <monospace>W</monospace> = 64, <monospace>O</monospace> = 33 for the long read dataset and <monospace>W</monospace> = 32, <monospace>O</monospace> = 17 for the short read dataset, and enable the combinations of improvements that yield the best throughput. The exact function calls and parameters we used for each tool can be found in our GitHub repository and in Section 7 of the <xref rid="sup1" ref-type="supplementary-material">Supplementary Materials</xref>. <xref rid="btad151-F6" ref-type="fig">Figure 6</xref> shows that Scrooge significantly speeds up the alignment of long and short reads over <italic toggle="yes">all</italic> baselines. In particular, the CPU implementation of Scrooge has 2.1<inline-formula id="IE85"><mml:math id="IM85" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula> higher throughput (i.e. pairwise sequence alignments per second) than our CPU implementation of GenASM for long reads and 3.8<inline-formula id="IE86"><mml:math id="IM86" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula> higher throughput for short reads. The GPU implementation of Scrooge has 5.9<inline-formula id="IE87"><mml:math id="IM87" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula> higher throughput than our GPU implementation of GenASM for long reads and 2.4<inline-formula id="IE88"><mml:math id="IM88" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula> higher throughput for short reads. The CPU and GPU speedups over GenASM are entirely due to Scrooge’s algorithmic improvements (i.e. SENE, DENT, ET) since our Scrooge and GenASM implementations are similarly optimized.</p>
      <fig position="float" id="btad151-F6">
        <label>Figure 6.</label>
        <caption>
          <p>Scrooge’s alignment throughput relative to various CPU and GPU baselines.</p>
        </caption>
        <graphic xlink:href="btad151f6" position="float"/>
      </fig>
      <p>Note that WFA, KSW2, CUDASW++3.0, and Darwin solve a more general formulation of the alignment problem with affine gap scores (<xref rid="btad151-B23" ref-type="bibr">Gotoh 1982</xref>). This puts them at a performance disadvantage. In contrast, Edlib (<xref rid="btad151-B45" ref-type="bibr">Šošić and Šikić 2017</xref>), GenASM (<xref rid="btad151-B41" ref-type="bibr">Senol Cali et al. 2020</xref>), and Scrooge solve a less general but more efficient formulation of the alignment problem with unit costs (edit distance or Levenshtein distance; <xref rid="btad151-B28" ref-type="bibr">Levenshtein 1966</xref>). We list the capabilities of each tool in Section 6 of the <xref rid="sup1" ref-type="supplementary-material">Supplementary Materials</xref>.</p>
    </sec>
    <sec>
      <title>3.3 Thread scaling</title>
      <p>We explore the scaling of each CPU tool as the number of CPU threads increases. For each evaluated CPU tool, we sweep the number of CPU threads and measure the throughput on the long read and short read datasets. <xref rid="btad151-F7" ref-type="fig">Figure 7</xref> shows the results normalized to each tool’s throughput with four threads (for readability). We make three key observations. First, most tools scale almost linearly up to 24 threads for both datasets, but do not scale significantly from 24 to 48 threads. The system we perform our experiments on has 24 physical cores and 48 logical cores (<xref rid="btad151-B27" ref-type="bibr">Intel 2017</xref>), thus we hypothesize that the tools do not benefit from simultaneous multithreading (<italic toggle="yes">Hyper-Threading</italic> in Intel terminology) (<xref rid="btad151-B35" ref-type="bibr">Marr et al. 2002</xref>) due to the low latencies of simple arithmetic and bitwise instructions (<xref rid="btad151-B20" ref-type="bibr">Fog 2021</xref>), which is what the underlying alignment algorithms of the tools primarily consist of. Second, we observe that Edlib’s performance <italic toggle="yes">decreases</italic> from 16 to 20 threads in the long read dataset. Since this does not occur in the short read dataset, we hypothesize that Edlib suffers from cache thrashing in the long read dataset and that the data fits into the cache for the short read dataset. Third, we observe that both evaluated functions of KSW2 do not scale at all past 24 threads in the long read dataset. We hypothesize that KSW2 is bandwidth-bound in this case.</p>
      <fig position="float" id="btad151-F7">
        <label>Figure 7.</label>
        <caption>
          <p>Speedup of each CPU tool as the number of CPU threads increases.</p>
        </caption>
        <graphic xlink:href="btad151f7" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.4 Sensitivity analysis</title>
      <p>We explore the throughput benefits of our algorithmic improvements in parameter sweeps over (i) the number of GPU and CPU threads, (ii) the window size (<monospace>W</monospace>) parameter, and (iii) the window overlap (<monospace>O</monospace>) parameter.</p>
      <p><bold>GPU threads.</bold> First, we run a scaling experiment on a GPU for GenASM, Scrooge with the SENE improvement, Scrooge with the DENT improvement, and Scrooge with all three proposed improvements, with the DP table placed in either shared memory (<xref rid="btad151-F8" ref-type="fig">Fig. 8a</xref>) or global memory (<xref rid="btad151-F8" ref-type="fig">Fig. 8b</xref>). Based on <xref rid="btad151-F8" ref-type="fig">Fig. 8</xref>, we make five observations. First, we observe that SENE and DENT individually improve performance when the DP table is placed in either shared or global memory. Second, we observe that SENE, DENT, and ET can be combined for greater benefits. Third, we observe that placing the DP table in shared (on-chip) memory achieves the best performance, but only when both proposed memory footprint improvements (i.e. SENE and DENT) are applied. This is because only with SENE and DENT is the memory footprint small enough to keep sufficiently many problem instances in the shared memory to utilize the compute resources in each SM (see Section 2.3.2) well. Fourth, in configurations where the memory footprint is not reduced sufficiently (e.g. with only DENT or SENE), using global (off-chip) memory can be faster than using shared memory, because global memory has sufficient capacity to fit many problem instances, utilizing compute resources better than shared memory despite the global memory’s limited bandwidth. Finally, we observe that the baseline GenASM algorithm cannot run using shared memory at all, although we showed in Section 2.3.2 that a single instance of the baseline DP table has a footprint of 98.5KiB and thus should use fit into the 99KiB of shared memory. This is because our implementation requires some additional memory, such as for communication between processing elements. Thus, we cannot fit even a single instance into shared memory with GenASM.</p>
      <fig position="float" id="btad151-F8">
        <label>Figure 8.</label>
        <caption>
          <p>Scaling experiments of our GPU implementation with <monospace>W</monospace> = 64, <monospace>O</monospace> = 33, when the DP table placed in (a) shared memory and (b) global memory.</p>
        </caption>
        <graphic xlink:href="btad151f8" position="float"/>
      </fig>
      <p>We ran the experiment for all seven possible combinations of our three improvements (i.e. SENE, DENT, and ET). Full results are shown in Section 11 of the <xref rid="sup1" ref-type="supplementary-material">Supplementary Materials</xref>. In particular, we observe no significant benefits for GPUs from ET, which is why we omit it in <xref rid="btad151-F8" ref-type="fig">Fig. 8</xref> for readability.</p>
      <p><bold>CPU threads.</bold> We run a similar scaling experiment on a CPU for GenASM, Scrooge with the SENE improvement, Scrooge with the ET improvement, and Scrooge with the SENE and ET improvements. From <xref rid="btad151-F9" ref-type="fig">Fig. 9a</xref>, we make three observations: First, we observe that ET improves performance significantly. This contrasts with our GPU implementation, where ET did not show significant benefits. This is because our CPU implementation builds the DP table <monospace>R</monospace> row-wise, while our GPU implementation builds <monospace>R</monospace> diagonal-wise (see Section 2.4.3). Second, SENE improves performance consistently, but less significantly than in the GPU case. This is because modern CPUs have relatively large on-chip cache capacities (e.g. 1MiB L2 cache per core on the Xeon Gold 5118 we evaluated on <xref rid="btad151-B27" ref-type="bibr">Intel 2017</xref>). Thus, the DP table easily fits into the L2 cache even without Scrooge’s algorithmic improvements, and hence reducing the memory footprint is not as important. Third, Scrooge scales linearly up to 24 threads but does not scale at all from 24 to 48 threads, a trend we observe for all evaluated tools (see Section 3.3).</p>
      <fig position="float" id="btad151-F9">
        <label>Figure 9.</label>
        <caption>
          <p>(a) Scaling and (b) sensitivity to window size of our CPU implementation.</p>
        </caption>
        <graphic xlink:href="btad151f9" position="float"/>
      </fig>
      <p>We ran the experiment for all seven possible combinations of our three improvements (i.e. SENE, DENT, and ET). Full results are shown in Section 12 of the <xref rid="sup1" ref-type="supplementary-material">Supplementary Materials</xref>. In particular, we observe no significant benefits for CPUs from DENT, and in some cases even a slowdown, which is why we omit it in <xref rid="btad151-F9" ref-type="fig">Fig. 9</xref> for readability.</p>
      <p>The three key takeaways from these experiments are that: (i) the SENE and DENT memory improvements yield significant benefits if performance is limited by memory bandwidth or capacity (e.g. in the GPU experiment), (ii) some of the algorithmic improvements can cause slight performance loss in practice (e.g. DENT in the CPU experiment), and (iii) the ideal combination of improvements depends on the computation platform (e.g. the available on-chip cache capacity) and the exact implementation (e.g. row-wise or diagonal-wise).</p>
      <p><bold>Window size (<monospace>W</monospace>) and overlap (<monospace>O</monospace>).</bold> We explore the sensitivity of Scrooge’s throughput to the window size parameter <monospace>W</monospace> (see Section 2.2.3) on CPUs. We vary <monospace>W</monospace> and set <monospace>O</monospace> = <monospace>W</monospace>//2 + 1. Note that larger <monospace>W</monospace> improves accuracy (see Section 2.2.3).</p>
      <p>From the CPU results in <xref rid="btad151-F9" ref-type="fig">Fig. 9b</xref> we make two observations: First, we observe that performance generally reduces as <monospace>W</monospace> increases. This is because the number of calculated bits per window increases cubically with increasing <monospace>W</monospace>. Second, we observe a sudden throughput dropoff when <monospace>W</monospace> increases past 64. This is because the word size of the Xeon Gold 5118 CPU is 64 bits; thus, if <monospace>W</monospace><inline-formula id="IE89"><mml:math id="IM89" display="inline" overflow="scroll"><mml:mo>&gt;</mml:mo></mml:math></inline-formula> 64, each bitvector operation has to be emulated using multiple word-sized machine instructions. This emulation is conceptually simple (e.g. carry over shifted bits) but requires several additional instructions, causing the performance dropoff. For example, in our implementation, a single 65-bit left shift is performed using two 64-bit left shifts, a 64-bit right shift, and a bitwise or operation.</p>
      <p>We repeat the same study on a GPU and observe the same trends: Increasing <monospace>W</monospace> reduces performance, and if the bitvectors are longer than the machine word (32 bits on the evaluated GPU), bit operations become significantly more expensive. We plot the GPU results and give detailed explanations in Section 4 of the <xref rid="sup1" ref-type="supplementary-material">Supplementary Materials</xref>.</p>
      <p>We repeat a similar study for the window overlap (<monospace>O</monospace>) in Section 5 of the <xref rid="sup1" ref-type="supplementary-material">Supplementary Materials</xref>. We observe that as <monospace>O</monospace> increases, performance generally reduces. However, with Scrooge’s optimizations, larger values of <monospace>O</monospace> can sometimes <italic toggle="yes">increase</italic> performance. <monospace>O</monospace> = 33 gives the best result. Thus, we choose it as the default operating point of Scrooge for CPUs and GPUs.</p>
    </sec>
    <sec>
      <title>3.5 Area and power consumption of an ASIC implementation</title>
      <p>The GenASM ASIC designed in <xref rid="btad151-B41" ref-type="bibr">Senol Cali et al. (2020)</xref> uses a large on-chip scratchpad to store bitvectors for traceback. This scratchpad alone accounts for 0.256 mm<sup>2</sup> (76%) of silicon area and 0.055 W (54%) of power out of a total of 0.334 mm<sup>2</sup> and 0.101 <italic toggle="yes">W</italic> per accelerator core. Our proposed algorithmic improvements can be applied to that ASIC design through minor modifications. We estimate the area and power cost of such an ASIC implementation of Scrooge analytically as follows:</p>
      <list list-type="order">
        <list-item>
          <p>We start with the DC-logic, DC-SRAM, and TB-logic area and power numbers reported in <xref rid="btad151-B41" ref-type="bibr">Senol Cali et al. (2020)</xref></p>
        </list-item>
        <list-item>
          <p>We estimate Scrooge’s TB-SRAM area and power cost with CACTI 7 (<xref rid="btad151-B11" ref-type="bibr">Balasubramonian et al. 2017</xref>), as in <xref rid="btad151-B41" ref-type="bibr">Senol Cali et al. (2020)</xref>, but with Scrooge’s reduced memory footprint and data movement numbers.</p>
        </list-item>
        <list-item>
          <p>We account for the logic overhead of SENE by adding the area and power of a single DC processing element (<xref rid="btad151-B41" ref-type="bibr">Senol Cali et al. 2020</xref>) to the traceback (TB) logic cost, which accounts for recomputing edges during traceback. We assume no overhead for SENE during the construction of R, since the ANDed bitvectors are already computed.</p>
        </list-item>
        <list-item>
          <p>We assume no overheads for applying DENT since it simply masks out bits when storing the bitvectors, which is trivial in hardware.</p>
        </list-item>
      </list>
      <p><xref rid="btad151-T1" ref-type="table">Table 1</xref> lists the area and power breakdowns obtained with this methodology, and the breakdown of (<xref rid="btad151-B41" ref-type="bibr">Senol Cali et al. 2020</xref>) as a comparison point. In particular, we observe a 3.6<inline-formula id="IE90"><mml:math id="IM90" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula> reduction in chip area and a 2.1<inline-formula id="IE91"><mml:math id="IM91" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula> reduction in chip power consumption, while maintaining the same throughput. These improvements come from (i) the reduced TB SRAM capacity, and (ii) the reduced TB SRAM bandwidth.</p>
      <table-wrap position="float" id="btad151-T1">
        <label>Table 1.</label>
        <caption>
          <p>Estimated area and power of a Scrooge ASIC with <monospace>W</monospace> = 64 and <monospace>O</monospace> = 33.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th colspan="5" rowspan="1">Area (<inline-formula id="IE92"><mml:math id="IM92" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mtext>mm</mml:mtext></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>)<hr/></th>
              <th colspan="5" rowspan="1">Power (<italic toggle="yes">W</italic>)<hr/></th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">ASIC implementation</th>
              <th rowspan="1" colspan="1">DC logic</th>
              <th rowspan="1" colspan="1">TB logic</th>
              <th rowspan="1" colspan="1">DC SRAM</th>
              <th rowspan="1" colspan="1">TB SRAM</th>
              <th rowspan="1" colspan="1">Total</th>
              <th rowspan="1" colspan="1">DC logic</th>
              <th rowspan="1" colspan="1">TB logic</th>
              <th rowspan="1" colspan="1">DC SRAM</th>
              <th rowspan="1" colspan="1">TB SRAM</th>
              <th rowspan="1" colspan="1">Total</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">
                <xref rid="btad151-B41" ref-type="bibr">Senol Cali <italic toggle="yes">et al.</italic> (2020)</xref>
              </td>
              <td rowspan="1" colspan="1">0.049</td>
              <td rowspan="1" colspan="1">0.016</td>
              <td rowspan="1" colspan="1">0.013</td>
              <td rowspan="1" colspan="1">0.256</td>
              <td rowspan="1" colspan="1">0.334</td>
              <td rowspan="1" colspan="1">0.033</td>
              <td rowspan="1" colspan="1">0.004</td>
              <td rowspan="1" colspan="1">0.009</td>
              <td rowspan="1" colspan="1">0.055</td>
              <td rowspan="1" colspan="1">0.101</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Scrooge</td>
              <td rowspan="1" colspan="1">0.049</td>
              <td rowspan="1" colspan="1">0.016</td>
              <td rowspan="1" colspan="1">0.013</td>
              <td rowspan="1" colspan="1">0.014</td>
              <td rowspan="1" colspan="1">0.093</td>
              <td rowspan="1" colspan="1">0.033</td>
              <td rowspan="1" colspan="1">0.004</td>
              <td rowspan="1" colspan="1">0.009</td>
              <td rowspan="1" colspan="1">0.003</td>
              <td rowspan="1" colspan="1">0.049</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>The key takeaway from this estimate is that Scrooge’s algorithmic improvements (i) are directly applicable to and (ii) yield significant benefits over an ASIC implementation of GenASM.</p>
    </sec>
    <sec>
      <title>3.6 Accuracy</title>
      <p>The GenASM algorithm (<xref rid="btad151-B41" ref-type="bibr">Senol Cali et al. 2020</xref>), which Scrooge is based on, is a greedy heuristic algorithm, as explained in Section 2.2. Our improvements do <italic toggle="yes">not</italic> introduce additional inaccuracy. In fact, Scrooge’s default operating point of <monospace>W</monospace> = 64 <monospace>O</monospace> = 33 <italic toggle="yes">increases</italic> accuracy (see Section 2.2.3) over GenASM’s default operating point of <monospace>W</monospace> = 64 <monospace>O</monospace> = 24 (<xref rid="btad151-B41" ref-type="bibr">Senol Cali et al. 2020</xref>). The following analysis explores the accuracy of both Scrooge and GenASM across different operating points. At any given operating point, Scrooge produces the same alignments (and hence accuracy) as GenASM at that operating point. We run three types of experiments. First, we evaluate the alignment quality of Scrooge compared to all evaluated baseline tools. Second, we explore in detail the sensitivity of accuracy to the window size <monospace>W</monospace>. Third, we explore in detail the sensitivity of accuracy to the window overlap <monospace>O</monospace>.</p>
      <p><bold>Alignment quality compared to baseline tools.</bold> We explore the quality of the alignments (CIGAR strings) generated by Scrooge, compared to the baseline tools. To measure alignment quality, we count the number of correctly aligned bases according to the ground truth alignments reported by the PBSIM2 simulator for the long read groundtruth dataset. For Scrooge we repeat the evaluation for multiple values of <monospace>W</monospace> and set <monospace>O</monospace> = <monospace>W</monospace>//2 + 1. We make three observations from <xref rid="btad151-F10" ref-type="fig">Fig. 10</xref>. First, the number of bases correctly aligned by Scrooge increases as the window size <monospace>W</monospace> increases. Second, Scrooge correctly aligns approximately the same number of bases as all of the baselines if <monospace>W</monospace><inline-formula id="IE93"><mml:math id="IM93" display="inline" overflow="scroll"><mml:mo>≥</mml:mo></mml:math></inline-formula> 64. Third, no tool can consistently produce the exact ground truth alignment. By manually inspecting such misalignments of each tool, we determine this is because of two reasons. First, indels in homopolymers are ambiguous and cannot reliably be retrieved with any aligner. Second, sometimes the ground truth alignment is suboptimal in terms of alignment score and/or edit distance. In these cases, the aligners’ goal of finding the optimal scoring alignment produces high-scoring but wrong alignments.</p>
      <fig position="float" id="btad151-F10">
        <label>Figure 10.</label>
        <caption>
          <p>Fraction of correctly aligned bases according to the ground truth alignments in the long read groundtruth dataset.</p>
        </caption>
        <graphic xlink:href="btad151f10" position="float"/>
      </fig>
      <p>We explore the sensitivity of Scrooge’s accuracy to the window size parameter <monospace>W</monospace> (see Section 2.2.3). We analyze the accuracy compared to optimal edit distance solutions, such as Edlib (<xref rid="btad151-B45" ref-type="bibr">Šošić and Šikić 2017</xref>). We evaluate the generated alignments based on minimap2’s default affine gap scoring model. We vary <monospace>W</monospace> and set <monospace>O</monospace> = <monospace>W</monospace>//2 + 1. For each experiment, we record the 0.5, 0.1, 0.01, and 0.001 percentile alignment scores (i.e. for a dataset of 1000 pairs, the 0.5 percentile would be the 500th worst alignment score, the 0.01 percentile would be the 10th worst alignment score) of Scrooge and GenASM (which produce the same results for the same choice of <monospace>W</monospace> and <monospace>O</monospace>) and compare to Edlib as an ideal upper bound.</p>
      <p><bold>Sensitivity to window size (<monospace>W</monospace>).</bold> From <xref rid="btad151-F11" ref-type="fig">Fig. 11</xref>, we make three observations: First, accuracy depends on the dataset. Second, small window sizes are sufficient for Scrooge and GenASM to find the optimal edit distance alignment for <italic toggle="yes">most</italic> of the sequence pairs. For example, the median alignment score is already optimal at <monospace>W</monospace> = 32 for the long read groundtruth dataset and at <monospace>W</monospace> = 8 for the short read dataset. Third, to find the optimal alignment for a few worst-case pairs, large window sizes are required: For example, the optimal alignment for the 0.001 percentile in the long read groundtruth dataset is only found for <monospace>W</monospace><inline-formula id="IE94"><mml:math id="IM94" display="inline" overflow="scroll"><mml:mo>≥</mml:mo></mml:math></inline-formula> 80. We manually inspect several of these ‘difficult’ sequence pairs to find the reason for their apparent difficulty. We observe sequence pairs are aligned poorly if they contain extremely noisy and repetitive sub-sequences. However, these pairs <italic toggle="yes">will</italic> be aligned optimally if the window size is larger than the length of the noisy sub-sequences. We illustrate this observation with an example sequence pair from the long read groundtruth dataset in Section 13 of the <xref rid="sup1" ref-type="supplementary-material">Supplementary Materials</xref>.</p>
      <fig position="float" id="btad151-F11">
        <label>Figure 11.</label>
        <caption>
          <p>Sensitivity of Scrooge’s accuracy to W. We show the achieved alignment score of the 0.001, 0.01, 0.1, and 0.5 (median) quantiles, and compare to Edlib as an upper bound for the accuracy achievable with the edit distance metric.</p>
        </caption>
        <graphic xlink:href="btad151f11" position="float"/>
      </fig>
      <p><bold>Sensitivity to window overlap (<monospace>O</monospace>).</bold> We explore the sensitivity of Scrooge’s accuracy to the window overlap parameter <monospace>O</monospace> (see Section 2.2.3). We sweep over <monospace>O</monospace> and run experiments for each <inline-formula id="IE95"><mml:math id="IM95" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="monospace">W</mml:mi><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mn>32</mml:mn><mml:mo>,</mml:mo><mml:mn>64</mml:mn><mml:mo>,</mml:mo><mml:mn>96</mml:mn><mml:mo>,</mml:mo><mml:mn>128</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>. For each experiment, we record the 0.01 percentile alignment score of Scrooge/GenASM and compare to Edlib as an ideal upper bound.</p>
      <p>From <xref rid="btad151-F12" ref-type="fig">Fig. 12</xref>, we make two observations: First, accuracy improves as <monospace>O</monospace> increases. Second, we observe that <monospace>W</monospace> and <monospace>O</monospace> need to be <italic toggle="yes">balanced</italic> to achieve good accuracy. For example, the accuracy loss of a too small <monospace>W</monospace> = 32 for the long read groundtruth dataset cannot be overcome with even large <monospace>O</monospace> = 30. Similarly, choosing <monospace>O</monospace> close to 0 hurts accuracy for both datasets, even when <monospace>W</monospace> is large.</p>
      <fig position="float" id="btad151-F12">
        <label>Figure 12.</label>
        <caption>
          <p>Sensitivity of accuracy to O, reporting the first percentile (worst 1%) alignment score for each configuration. Edlib is an upper bound for the scores achievable with the edit distance metric.</p>
        </caption>
        <graphic xlink:href="btad151f12" position="float"/>
      </fig>
      <p>The two key takeaways from these experiments are that (i) <monospace>W</monospace> and <monospace>O</monospace> need to be chosen per dataset, and (ii) <monospace>W</monospace> and <monospace>O</monospace> should be increased or reduced together for the best accuracy.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Discussion and conclusion</title>
    <p>To our knowledge, this is the first paper to: (i) demonstrate the computational inefficiencies in the GenASM algorithm, (ii) address them with three improvements in our new Scrooge algorithm, (iii) rigorously demonstrate the computational benefits of Scrooge over GenASM for CPU, GPU, ASIC implementations, and (iv) rigorously analyze the accuracy of GenASM and Scrooge under multiple different configurations.</p>
    <p>We have already extensively compared to WFA (<xref rid="btad151-B34" ref-type="bibr">Marco-Sola et al. 2020</xref>), KSW2 (<xref rid="btad151-B29" ref-type="bibr">Li 2018</xref>; <xref rid="btad151-B46" ref-type="bibr">Suzuki and Kasahara 2018</xref>), Edlib (<xref rid="btad151-B45" ref-type="bibr">Šošić and Šikić 2017</xref>), CUDASW++3.0 (<xref rid="btad151-B32" ref-type="bibr">Liu et al. 2013</xref>), and Darwin-GPU (<xref rid="btad151-B2" ref-type="bibr">Ahmed et al. 2020</xref>). Several other works accelerate sequence alignment: NVBIO (<ext-link xlink:href="https://github.com/NVlabs/nvbio" ext-link-type="uri">https://github.com/NVlabs/nvbio</ext-link>) is a multipurpose library for accelerating bioinformatics applications using GPUs, but is no longer maintained. Gasal2 (<xref rid="btad151-B1" ref-type="bibr">Ahmed et al. 2019</xref>) is a recent GPU aligner limited to short reads. CUDAlign4.0 (<xref rid="btad151-B16" ref-type="bibr">de Oliveira Sandes et al. 2016</xref>) can efficiently align a single pair of extremely long (chromosome-sized) sequences, with use cases such as whole genome alignment. Adept (<xref rid="btad151-B8" ref-type="bibr">Awan et al. 2020</xref>) is a recent GPU aligner for short and long reads but does not support traceback, i.e. only reports the alignment score.</p>
    <p>The Darwin accelerator (<xref rid="btad151-B47" ref-type="bibr">Turakhia et al. 2018</xref>) implements a Smith–Waterman–Gotoh accelerator for long reads using a similar greedy strategy to GenASM called <italic toggle="yes">tiling</italic>. We have compared Scrooge to the GPU implementation of this algorithm, Darwin-GPU. GenASM, Scrooge, and Darwin demonstrate the significant benefits of greedy algorithms, based on which there are at least two interesting future directions to explore. First, a suitability study of different algorithms to greedy heuristics, such as Myers’ bitvector algorithm (<xref rid="btad151-B36" ref-type="bibr">Myers 1999</xref>), Hyyrö’s banded bitvector algorithm (<xref rid="btad151-B25" ref-type="bibr">Hyyrö 2003</xref>), or the recently proposed wavefront algorithm (<xref rid="btad151-B34" ref-type="bibr">Marco-Sola et al. 2020</xref>). Second, an exploration of the effectiveness of our algorithmic improvements for other implementations of greedy windowing or tiling, like Darwin. We believe the DENT improvement can be applied directly to Darwin.</p>
    <p>We have demonstrated the computational benefits of Scrooge over a variety of state-of-the-art baselines for both commodity hardware (i.e. CPUs and GPUs) and custom hardware (i.e. ASICs). We have demonstrated the accuracy of Scrooge for multiple datasets. We conclude that Scrooge has clear benefits across a wide range of computing platforms.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btad151_Supplementary_Data</label>
      <media xlink:href="btad151_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>The authors acknowledge the generous gifts of our industrial partners, especially Google, Huawei, Intel, Microsoft, VMware, and Xilinx.</p>
  </ack>
  <sec>
    <title>Supplementary data</title>
    <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> is available at <italic toggle="yes">Bioinformatics</italic> online.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Conflict of interest</title>
    <p>None declared.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was partially supported by the Semiconductor Research Corporation, the ETH Future Computing Laboratory, and the BioPIM project.</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btad151-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ahmed</surname><given-names>N</given-names></string-name></person-group>, <string-name><surname>Lévy</surname><given-names>J</given-names></string-name>, <string-name><surname>Ren</surname><given-names>S</given-names></string-name><etal>et al</etal><article-title>GASAL2: a GPU accelerated sequence alignment library for high-throughput NGS data</article-title>. <source>BMC Bioinformatics</source><year>2019</year>;<volume>20</volume>:<fpage>520</fpage>.<pub-id pub-id-type="pmid">31653208</pub-id></mixed-citation>
    </ref>
    <ref id="btad151-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ahmed</surname><given-names>N</given-names></string-name>, <string-name><surname>Qiu</surname><given-names>TD</given-names></string-name>, <string-name><surname>Bertels</surname><given-names>K</given-names></string-name></person-group><etal>et al</etal><article-title>GPU acceleration of Darwin read overlapper for de novo assembly of long DNA reads</article-title>. <source>BMC Bioinformatics</source><year>2020</year>;<volume>21</volume>:<fpage>388</fpage>.<pub-id pub-id-type="pmid">32938392</pub-id></mixed-citation>
    </ref>
    <ref id="btad151-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alser</surname><given-names>M</given-names></string-name>, <string-name><surname>Bingol</surname><given-names>Z</given-names></string-name>, <string-name><surname>Cali</surname><given-names>DS</given-names></string-name></person-group><etal>et al</etal><article-title>Accelerating genome analysis: a primer on an ongoing journey</article-title>. <source>IEEE Micro</source><year>2020a</year>;<volume>40</volume>:<fpage>65</fpage>–<lpage>75</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alser</surname><given-names>M</given-names></string-name><collab>, Shahroodi , Gómez-Luna J</collab></person-group><etal>et al</etal><article-title>SneakySnake: a fast and accurate universal genome pre-alignment filter for CPUs, GPUs and FPGAs</article-title>. <source>Bioinformatics</source><year>2020b</year>;<volume>36</volume>:<fpage>5282</fpage>–<lpage>90</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alser</surname><given-names>M</given-names></string-name><collab>, Lindegger J, Firtina C</collab></person-group><etal>et al</etal><article-title>From molecules to genomic variations: accelerating genome analysis via intelligent algorithms and architectures</article-title>. <source>Comput Struct Biotechnol J</source><year>2022</year>;<volume>20</volume>:<fpage>4579</fpage>–<lpage>99</lpage>.<pub-id pub-id-type="pmid">36090814</pub-id></mixed-citation>
    </ref>
    <ref id="btad151-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alser</surname><given-names>M</given-names></string-name>, <string-name><surname>Hassan</surname><given-names>H</given-names></string-name>, <string-name><surname>Kumar</surname><given-names>A</given-names></string-name></person-group><etal>et al</etal><article-title>Shouji: a fast and efficient pre-alignment filter for sequence alignment</article-title>. <source>Bioinformatics</source><year>2019</year>;<volume>35</volume>:<fpage>4255</fpage>–<lpage>63</lpage>.<pub-id pub-id-type="pmid">30923804</pub-id></mixed-citation>
    </ref>
    <ref id="btad151-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alser</surname><given-names>M</given-names></string-name>, <string-name><surname>Rotman</surname><given-names>J</given-names></string-name>, <string-name><surname>Deshpande</surname><given-names>D</given-names></string-name></person-group><etal>et al</etal><article-title>Technology dictates algorithms: recent developments in read alignment</article-title>. <source>Genome Biol</source><year>2021</year>;<volume>22</volume>:<fpage>249</fpage>.<pub-id pub-id-type="pmid">34446078</pub-id></mixed-citation>
    </ref>
    <ref id="btad151-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Awan</surname><given-names>MG</given-names></string-name>, <string-name><surname>Deslippe</surname><given-names>J</given-names></string-name>, <string-name><surname>Buluc</surname><given-names>A</given-names></string-name></person-group><etal>et al</etal><article-title>ADEPT: a domain independent sequence alignment strategy for GPU architectures</article-title>. <source>BMC Bioinformatics</source><year>2020</year>;<volume>21</volume>:<fpage>406</fpage>.<pub-id pub-id-type="pmid">32933482</pub-id></mixed-citation>
    </ref>
    <ref id="btad151-B9">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Backurs</surname><given-names>A</given-names></string-name>, <string-name><surname>Indyk</surname><given-names>P.</given-names></string-name></person-group><article-title>Edit distance cannot be computed in strongly subquadratic time (unless SETH is false)</article-title>. <source>STOC</source><year>2015</year>:<fpage>51</fpage>–<lpage>8</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baeza-Yates</surname><given-names>R</given-names></string-name>, <string-name><surname>Gonnet</surname><given-names>GH.</given-names></string-name></person-group><article-title>A new approach to text searching</article-title>. <source>Commun ACM</source><year>1992</year>;<volume>35</volume>:<fpage>74</fpage>–<lpage>82</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Balasubramonian</surname><given-names>R</given-names></string-name>, <string-name><surname>Kahng</surname><given-names>AB</given-names></string-name>, <string-name><surname>Muralimanohar</surname><given-names>N</given-names></string-name></person-group><etal>et al</etal><article-title>CACTI 7: new tools for interconnect exploration in innovative off-chip memories</article-title>. <source>ACM Trans Archit Code Optim</source><year>2017</year>;<volume>14</volume>:<fpage>1</fpage>–<lpage>25</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Benkrid</surname><given-names>K</given-names></string-name><collab>, Liu Y, Benkrid A.</collab></person-group><article-title>A highly parameterized and efficient FPGA-based skeleton for pairwise biological sequence alignment</article-title>. <source>IEEE Trans VLSI Syst</source><year>2009</year>;<volume>17</volume>:<fpage>561</fpage>–<lpage>70</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B13">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Boroumand</surname><given-names>A</given-names></string-name><collab>, Ghose S, Kim Y</collab></person-group><etal>et al</etal> Google workloads for consumer devices: mitigating data movement bottlenecks. In: <source>ASPLOS</source>. Vol. 53. <year>2018.</year><fpage>316</fpage>–<lpage>31</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B14">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Boroumand</surname><given-names>A</given-names></string-name><collab>, Ghose S, Akin B</collab></person-group><etal>et al</etal> Google neural network models for edge devices: analyzing and mitigating machine learning inference bottlenecks. In: PACT <year>2021</year>. <fpage>159</fpage>–<lpage>72</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cleal</surname><given-names>K</given-names></string-name>, <string-name><surname>Baird</surname><given-names>DM.</given-names></string-name></person-group><article-title>Dysgu: efficient structural variant calling using short or long reads</article-title>. <source>Nucleic Acids Res</source><year>2022</year>;<volume>50</volume>:<fpage>e53</fpage>.<pub-id pub-id-type="pmid">35100420</pub-id></mixed-citation>
    </ref>
    <ref id="btad151-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>de Oliveira Sandes</surname><given-names>EF</given-names></string-name><collab>, Miranda G, Martorell X</collab></person-group><etal>et al</etal><article-title>CUDAlign 4.0: incremental speculative traceback for exact chromosome-wide alignment in GPU clusters</article-title>. <source>IEEE Trans Parallel Distrib Syst</source><year>2016</year>;<volume>27</volume>:<fpage>2838</fpage>–<lpage>50</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B17">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Dickens</surname><given-names>C.</given-names></string-name></person-group><source>A Christmas Carol</source>. London: <publisher-name>Chapman &amp; Hall</publisher-name>, <year>1843</year>.</mixed-citation>
    </ref>
    <ref id="btad151-B18">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Eizenga</surname><given-names>JM</given-names></string-name>, <string-name><surname>Paten</surname><given-names>B.</given-names></string-name></person-group> Improving the time and space complexity of the WFA algorithm and generalizing its scoring. biorXiv, <year>2022</year>, preprint not peer reviewed.</mixed-citation>
    </ref>
    <ref id="btad151-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fei</surname><given-names>X</given-names></string-name></person-group><etal>et al</etal><article-title>FPGASW: accelerating large-scale Smith–Waterman sequence alignment application with backtracking on FPGA linear systolic array</article-title>. <source>Interdiscip Sci</source><year>2018</year>;<volume>10</volume>:<fpage>176</fpage>–<lpage>88</lpage>.<pub-id pub-id-type="pmid">28432608</pub-id></mixed-citation>
    </ref>
    <ref id="btad151-B20">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Fog</surname><given-names>A.</given-names></string-name></person-group><year>2021</year>. Lists of instruction latencies, throughputs and micro-operation breakdowns for Intel, AMD, and VIA CPUs.</mixed-citation>
    </ref>
    <ref id="btad151-B21">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Fujiki</surname><given-names>D</given-names></string-name><collab>, Subramaniyan A, Zhang T</collab></person-group><etal>et al</etal><article-title>GenAx: a genome sequencing accelerator</article-title>. <source>ISCA</source><year>2018</year>;<fpage>69</fpage>–<lpage>82</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B22">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Fujiki</surname><given-names>D</given-names></string-name><collab>, Wu S, Ozog N</collab></person-group><etal>et al</etal><article-title>SeedEx: a genome sequencing accelerator for optimal alignments in subminimal space</article-title>. <source>MICRO</source><year>2020</year>;<fpage>937</fpage>–<lpage>50</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gotoh</surname><given-names>O.</given-names></string-name></person-group><article-title>An improved algorithm for matching biological sequences</article-title>. <source>J Mol Biol</source><year>1982</year>;<volume>162</volume>:<fpage>705</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">7166760</pub-id></mixed-citation>
    </ref>
    <ref id="btad151-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hoffmann</surname><given-names>J</given-names></string-name><collab>, Zeckzer D, Bogdan M.</collab></person-group><article-title>Using FPGAs to accelerate Myers bit-vector algorithm</article-title>. <source>MEDICON</source><year>2016</year>;<volume>57</volume>:<fpage>535</fpage>–<lpage>41</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hyyrö</surname><given-names>H.</given-names></string-name></person-group><article-title>A bit-vector algorithm for computing Levenshtein and Damerau edit distances</article-title>. <source>Nord J Comput</source><year>2003</year>;<volume>10</volume>:<fpage>29</fpage>–<lpage>39</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Impagliazzo</surname><given-names>R</given-names></string-name>, <string-name><surname>Paturi</surname><given-names>R.</given-names></string-name></person-group><article-title>On the complexity of k-SAT</article-title>. <source>J Comput Syst Sci</source><year>2001</year>;<volume>62</volume>:<fpage>367</fpage>–<lpage>75</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B27">
      <mixed-citation publication-type="other"><collab>Intel</collab>. <year>2017</year>. Intel Xeon Gold 5118 datasheet.</mixed-citation>
    </ref>
    <ref id="btad151-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Levenshtein</surname><given-names>VI.</given-names></string-name></person-group><article-title>Binary codes capable of correcting deletions, insertions, and reversals</article-title>. <source>Soviet Physics Doklady</source><year>1966</year>;<volume>10</volume>:<fpage>707</fpage>–<lpage>10</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>H.</given-names></string-name></person-group><article-title>Minimap2: pairwise alignment for nucleotide sequences</article-title>. <source>Bioinformatics</source><year>2018</year>;<volume>34</volume>:<fpage>3094</fpage>–<lpage>100</lpage>.<pub-id pub-id-type="pmid">29750242</pub-id></mixed-citation>
    </ref>
    <ref id="btad151-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>Z</given-names></string-name><collab>, Chen Y Mu D</collab></person-group><etal>et al</etal><article-title>Comparison of the two major classes of assembly algorithms: overlap–layout–consensus and De-Bruijn-graph</article-title>. <source>Brief Funct Genomics</source><year>2011</year>;<volume>11</volume>:<fpage>25</fpage>–<lpage>37</lpage>.<pub-id pub-id-type="pmid">22184334</pub-id></mixed-citation>
    </ref>
    <ref id="btad151-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lindholm</surname><given-names>E</given-names></string-name>, <string-name><surname>Nickolls</surname><given-names>J</given-names></string-name>, <string-name><surname>Oberman</surname><given-names>S</given-names></string-name></person-group><etal>et al</etal><article-title>NVIDIA tesla: a unified graphics and computing architecture</article-title>. <source>IEEE Micro</source><year>2008</year>;<volume>28</volume>:<fpage>39</fpage>–<lpage>55</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>Y</given-names></string-name><collab>, Wirawan A, Schmidt B.</collab></person-group><article-title>CUDASW++ 3.0: accelerating Smith-Waterman protein database search by coupling CPU and GPU SIMD instructions</article-title>. <source>BMC Bioinformatics</source><year>2013</year>;<volume>14</volume>:<fpage>117</fpage>.<pub-id pub-id-type="pmid">23557111</pub-id></mixed-citation>
    </ref>
    <ref id="btad151-B33">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Mansouri Ghiasi</surname><given-names>N</given-names></string-name><collab>, Park J, Mustafa H</collab></person-group><etal>et al</etal><article-title>GenStore: a high-performance in-storage processing system for genome sequence analysis</article-title>. <source>ASPLOS</source><year>2022</year>;<fpage>635</fpage>–<lpage>54</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marco-Sola</surname><given-names>S</given-names></string-name><collab>, Moure JC, Moreto M, Espinosa A.</collab></person-group><article-title>Fast gap-affine pairwise alignment using the wavefront algorithm</article-title>. <source>Bioinformatics</source><year>2020</year>;<volume>37</volume>:<fpage>456</fpage>–<lpage>63</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marr</surname><given-names>DT</given-names></string-name><collab>, Bins F, Hill DL</collab></person-group><etal>et al</etal><article-title>Hyper-threading technology architecture and microarchitecture</article-title>. <source>Intel Technol J</source><year>2002</year>;<volume>6</volume>:<fpage>1</fpage>–<lpage>12</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Myers</surname><given-names>G.</given-names></string-name></person-group><article-title>A fast bit-vector algorithm for approximate string matching based on dynamic programming</article-title>. <source>J ACM</source><year>1999</year>;<volume>46</volume>:<fpage>395</fpage>–<lpage>415</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B37">
      <mixed-citation publication-type="other"><collab>NVIDIA</collab>. <year>2020</year>. NVIDIA RTX A6000 datasheet.</mixed-citation>
    </ref>
    <ref id="btad151-B38">
      <mixed-citation publication-type="other"><collab>NVIDIA</collab>. <year>2023</year>. CUDA programming guide release 12.0.</mixed-citation>
    </ref>
    <ref id="btad151-B39">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Ofenbeck</surname><given-names>G</given-names></string-name><collab>, Steinmann R, Caparros V</collab></person-group><etal>et al</etal><article-title>Applying the roofline model</article-title>. <source>ISPASS</source><year>2014</year>;<fpage>76</fpage>–<lpage>85</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ono</surname><given-names>Y</given-names></string-name><collab>, Asai K, Hamada M.</collab></person-group><article-title>PBSIM2: a simulator for long-read sequencers with a novel generative model of quality scores</article-title>. <source>Bioinformatics</source><year>2020</year>;<volume>37</volume>:<fpage>589</fpage>–<lpage>95</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B41">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Senol Cali</surname><given-names>D</given-names></string-name><collab>, Kalsi GS, Bingöl Z</collab></person-group><etal>et al</etal><article-title>GenASM: a high-performance, low-power approximate string matching acceleration framework for genome sequence analysis</article-title>. <source>MICRO</source><year>2020</year>;<fpage>951</fpage>–<lpage>66</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B42">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Senol Cali</surname><given-names>D</given-names></string-name><collab>, Kanellopoulos K, Lindegger J</collab></person-group><etal>et al</etal><article-title>SeGraM: a universal hardware accelerator for genomic sequence-to-graph and sequence-to-sequence mapping</article-title>. <source>ISCA</source><year>2022</year>;<fpage>638</fpage>–<lpage>55</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Singh</surname><given-names>G</given-names></string-name>, <string-name><surname>Alser</surname><given-names>M</given-names></string-name>, <string-name><surname>Cali</surname><given-names>DS</given-names></string-name></person-group><etal>et al</etal><article-title>FPGA-based near-memory acceleration of modern data-intensive applications</article-title>. <source>IEEE Micro</source><year>2021</year>;<volume>41</volume>:<fpage>39</fpage>–<lpage>48</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname><given-names>TF</given-names></string-name>, <string-name><surname>Waterman</surname><given-names>MS.</given-names></string-name></person-group><article-title>Identification of common molecular subsequences</article-title>. <source>J Mol Biol</source><year>1981</year>;<volume>147</volume>:<fpage>195</fpage>–<lpage>7</lpage>.<pub-id pub-id-type="pmid">7265238</pub-id></mixed-citation>
    </ref>
    <ref id="btad151-B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Šošić</surname><given-names>M</given-names></string-name>, <string-name><surname>Šikić</surname><given-names>M.</given-names></string-name></person-group><article-title>Edlib: a C/C++ library for fast, exact sequence alignment using edit distance</article-title>. <source>Bioinformatics</source><year>2017</year>;<volume>33</volume>:<fpage>1394</fpage>–<lpage>5</lpage>.<pub-id pub-id-type="pmid">28453688</pub-id></mixed-citation>
    </ref>
    <ref id="btad151-B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Suzuki</surname><given-names>H</given-names></string-name>, <string-name><surname>Kasahara</surname><given-names>M.</given-names></string-name></person-group><article-title>Introducing difference recurrence relations for faster semi-global alignment of long sequences</article-title>. <source>BMC Bioinformatics</source><year>2018</year>;<volume>19</volume>:<fpage>45</fpage>.<pub-id pub-id-type="pmid">29504909</pub-id></mixed-citation>
    </ref>
    <ref id="btad151-B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Turakhia</surname><given-names>Y</given-names></string-name><collab>, Bejerano G, Dally WJ.</collab></person-group><article-title>Darwin: a genomics co-processor provides up to 15,000× acceleration on long read assembly</article-title>. <source>ASPLOS</source><year>2018</year>;<volume>53</volume>:<fpage>199</fpage>–<lpage>213</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B48">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Turakhia</surname><given-names>Y</given-names></string-name><collab>, Goenka SD, Bejerano G, Dally WJ.</collab></person-group> Darwin-WGA: a co-processor provides increased sensitivity in whole genome alignments with high speedup. In: HPCA <year>2019</year>;<fpage>359</fpage>–<lpage>72</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B49">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ukkonen</surname><given-names>E.</given-names></string-name></person-group><article-title>Algorithms for approximate string matching</article-title>. <source>Inf Control</source><year>1985</year>;<volume>64</volume>:<fpage>100</fpage>–<lpage>18</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B50">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Williams</surname><given-names>S</given-names></string-name><collab>, Waterman A, Patterson D.</collab></person-group><article-title>Roofline: an insightful visual performance model for multicore architectures</article-title>. <source>Commun ACM</source><year>2009</year>;<volume>52</volume>:<fpage>65</fpage>–<lpage>76</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B51">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wu</surname><given-names>S</given-names></string-name>, <string-name><surname>Manber</surname><given-names>U.</given-names></string-name></person-group><article-title>Fast text searching: allowing errors</article-title>. <source>Commun ACM</source><year>1992</year>;<volume>35</volume>:<fpage>83</fpage>–<lpage>91</lpage>.</mixed-citation>
    </ref>
    <ref id="btad151-B52">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xin</surname><given-names>H</given-names></string-name>, <string-name><surname>Greth</surname><given-names>J</given-names></string-name>, <string-name><surname>Emmons</surname><given-names>J</given-names></string-name></person-group><etal>et al</etal><article-title>Shifted hamming distance: a fast and accurate SIMD-friendly filter to accelerate alignment verification in read mapping</article-title>. <source>Bioinformatics</source><year>2015</year>;<volume>31</volume>:<fpage>1553</fpage>–<lpage>60</lpage>.<pub-id pub-id-type="pmid">25577434</pub-id></mixed-citation>
    </ref>
    <ref id="btad151-B53">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xin</surname><given-names>H</given-names></string-name>, <string-name><surname>Lee</surname><given-names>D</given-names></string-name>, <string-name><surname>Hormozdiari</surname><given-names>F</given-names></string-name></person-group><etal>et al</etal><article-title>Accelerating read mapping with FastHASH</article-title>. <source>BMC Genomics</source><year>2013</year>;<volume>14</volume>:<fpage>S13</fpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
