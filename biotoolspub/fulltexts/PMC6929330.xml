<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Genomics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Genomics</journal-id>
    <journal-title-group>
      <journal-title>BMC Genomics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2164</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6929330</article-id>
    <article-id pub-id-type="publisher-id">6335</article-id>
    <article-id pub-id-type="doi">10.1186/s12864-019-6335-4</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Classification of adaptor proteins using recurrent neural networks and PSSM profiles</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Khanh Le</surname>
          <given-names>Nguyen Quoc</given-names>
        </name>
        <address>
          <email>khanhle@ntu.edu.sg</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Nguyen</surname>
          <given-names>Quang H.</given-names>
        </name>
        <address>
          <email>quangnh@soict.hust.edu.vn</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Chen</surname>
          <given-names>Xuan</given-names>
        </name>
        <address>
          <email>chenxuan@genomics.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Rahardja</surname>
          <given-names>Susanto</given-names>
        </name>
        <address>
          <email>susantorahardja@ieee.org</email>
        </address>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Nguyen</surname>
          <given-names>Binh P.</given-names>
        </name>
        <address>
          <email>binh.p.nguyen@vuw.ac.nz</email>
        </address>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9337 0481</institution-id><institution-id institution-id-type="GRID">grid.412896.0</institution-id><institution>Professional Master Program in Artificial Intelligence in Medicine, Taipei Medical University, </institution></institution-wrap>Keelung Road, Da’an Distric, Taipei City 106, Taiwan (R.O.C.) </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.440792.c</institution-id><institution>School of Information and Communication Technology, Hanoi University of Science and Technology, </institution></institution-wrap>1 Dai Co Viet, Hanoi 100000, Vietnam </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2034 1839</institution-id><institution-id institution-id-type="GRID">grid.21155.32</institution-id><institution>Beijing Genomics Institute, </institution></institution-wrap>21 Hongan 3rd Street, Shenzhen 518083, China </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0307 1240</institution-id><institution-id institution-id-type="GRID">grid.440588.5</institution-id><institution>School of Marine Science and Technology, Northwestern Polytechnical University, </institution></institution-wrap>127 West Youyi Road, Xi’an 710072, China </aff>
      <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2292 3111</institution-id><institution-id institution-id-type="GRID">grid.267827.e</institution-id><institution>School of Mathematics and Statistics, Victoria University of Wellington, </institution></institution-wrap>Gate 7, Kelburn Parade, Wellington 6140, New Zealand </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>24</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>24</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>20</volume>
    <issue>Suppl 9</issue>
    <elocation-id>966</elocation-id>
    <history>
      <date date-type="received">
        <day>24</day>
        <month>10</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>25</day>
        <month>11</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Adaptor proteins are carrier proteins that play a crucial role in signal transduction. They commonly consist of several modular domains, each having its own binding activity and operating by forming complexes with other intracellular-signaling molecules. Many studies determined that the adaptor proteins had been implicated in a variety of human diseases. Therefore, creating a precise model to predict the function of adaptor proteins is one of the vital tasks in bioinformatics and computational biology. Few computational biology studies have been conducted to predict the protein functions, and in most of those studies, position specific scoring matrix (PSSM) profiles had been used as the features to be fed into the neural networks. However, the neural networks could not reach the optimal result because the sequential information in PSSMs has been lost. This study proposes an innovative approach by incorporating recurrent neural networks (RNNs) and PSSM profiles to resolve this problem.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">Compared to other state-of-the-art methods which had been applied successfully in other problems, our method achieves enhancement in all of the common measurement metrics. The area under the receiver operating characteristic curve (AUC) metric in prediction of adaptor proteins in the cross-validation and independent datasets are 0.893 and 0.853, respectively.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">This study opens a research path that can promote the use of RNNs and PSSM profiles in bioinformatics and computational biology. Our approach is reproducible by scientists that aim to improve the performance results of different protein function prediction problems. Our source code and datasets are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/ngphubinh/adaptors">https://github.com/ngphubinh/adaptors</ext-link>.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Adaptor proteins</kwd>
      <kwd>Prediction</kwd>
      <kwd>Classification</kwd>
      <kwd>Deep learning</kwd>
      <kwd>RNN</kwd>
      <kwd>GRU</kwd>
      <kwd>PSSM</kwd>
    </kwd-group>
    <conference xlink:href="https://incob2019.org/">
      <conf-name>International Conference on Bioinformatics (InCoB 2019)</conf-name>
      <conf-acronym>InCoB 2019</conf-acronym>
      <conf-loc>Jakarta, Indonesia</conf-loc>
      <conf-date>10-12 September 2019</conf-date>
    </conference>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p>Protein function prediction is a technique that assigns biological or biochemical roles to proteins with regards to their genome sequences. The essential of understanding the protein function has drawn researchers’ attentions on enhancing the predictive performance of protein functions. Numerous solutions have been proposed in the past decades for this purpose. Two most effective solutions are finding strong feature sets and adopting powerful neural network models. Previous studies have revealed that using strong feature sets alone, for example, position specific scoring matrix (PSSM) [<xref ref-type="bibr" rid="CR1">1</xref>], biochemical properties (AAindex) [<xref ref-type="bibr" rid="CR2">2</xref>], and PseAAC [<xref ref-type="bibr" rid="CR3">3</xref>], can achieve satisfactory prediction results. With the popularity of deep learning, many researchers in the field of bioinformatics attempted to apply the technique to protein function prediction. Some of the recent works like [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR5">5</xref>] have demonstrated some successes. Motivated by these two observations, we intend to take the advantages of strong feature sets and deep neural network to further improve the performance by deriving a novel approach for protein function prediction. In this work, we put special focus to the prediction of adaptor protein, which is one of the most vital molecule function in signal transduction.</p>
    <p>Signal transduction, so-called cell signaling, is the transmission from a cell’s outside to inside of molecular signals. Received signals must be transported viably into cells to guarantee a proper reaction. This progression is started by cell-surface receptors. One of the primary objectives of researchers who conduct their experiments on signal transduction is to decide the mechanisms that regulate cross-talk between signaling cascades and to decide the accomplishment of signaling. A rising class of proteins that much contributes to the signal transduction process are adaptor (or adapter) proteins. In adaptor proteins, there are numerious protein-binding modules linking protein-binding partners together. In addition, they are able to facilitate the signaling complexes creation [<xref ref-type="bibr" rid="CR6">6</xref>]. They are vital in intermolecular interactions and play a role in the control of signal transduction started by commitment of surface receptors on all cell types.</p>
    <p>In detail, adaptor proteins have been shown to be associated with a lot of human diseases. For instance, Gab adaptor proteins play an important role as therapeutic targets for hematologic disease [<xref ref-type="bibr" rid="CR7">7</xref>]. XB130, a specific adaptor protein, plays an important role in cancer [<xref ref-type="bibr" rid="CR8">8</xref>]. Likewise, Src-like adaptor proteins (SLAP-1 and SLAP-2) are important in the pathogenesis of osteoporosis, type I hypersensitivity, and numerous malignant diseases [<xref ref-type="bibr" rid="CR9">9</xref>]. In [<xref ref-type="bibr" rid="CR10">10</xref>], adaptor protein is also noted to be a therapeutic target in chronic kidney disease. Moreover, a review paper from [<xref ref-type="bibr" rid="CR11">11</xref>] showed the association of adapter proteins with the regulation of heart diseases. Further, the involvement of adaptor protein complex 4 in hypersensitive cell death induced by avirulent bacteria has been shown in [<xref ref-type="bibr" rid="CR12">12</xref>].</p>
    <p>Given the significance of adaptor proteins to the functions and structures of signal transduction, elucidating the molecular mechanisms of adaptor proteins is therefore a very important research area which has recently gained rapid advancement. However, it is costly and time-consuming with these experimental techniques. Therefore, it is highly desired to develop automated prediction methods for quick and accurate identification of adaptor proteins.</p>
    <p>PSSM is one of the most strong feature sets in biology to decode the evolutionary information of a protein sequence. Many computational studies have investigated the protein function prediction using PSSM profiles such as protein fold recognition [<xref ref-type="bibr" rid="CR13">13</xref>], phosphoglycerylation prediction [<xref ref-type="bibr" rid="CR14">14</xref>], succinylation prediction [<xref ref-type="bibr" rid="CR15">15</xref>], and protein subcellular localization prediction [<xref ref-type="bibr" rid="CR16">16</xref>]. However, among the existing approaches, none of them has found a solution to prevent the loss of amino acid sequence information in PSSM profiles. Here, to address this problem, we present an innovative approach via the use of a Recurrent Neural Network (RNN) architecture.</p>
    <p>Standard neural network typically assumes independent relationship between input signals, but this is usually not the case in real world. Likewise, utilizing the co-relationship between genome sequences can help in protein function prediction.</p>
    <p>We thus present a novel deep learning framework which utilizes RNNs and PSSM profiles to classify adaptor proteins. RNNs have been recently demonstrated to extract sequential information from sequences to predict various properties of protein sequences in several studies [<xref ref-type="bibr" rid="CR17">17</xref>–<xref ref-type="bibr" rid="CR19">19</xref>]. However, how to apply it on PSSM profiles to address the ordering information of them is still an open research question. The main contributions of this paper include (1) introducing a first sequence-based model for distinguishing adaptor proteins from general proteins, (2) proposing an efficient deep learning architecture constructed from RNNs and PSSM profiles for protein function prediction, (3) presenting a benchmark dataset and newly discovered data for adaptor proteins, and (4) providing valuable information to biologists and researchers for better understanding the adaptor protein structures.</p>
  </sec>
  <sec id="Sec2">
    <title>Results and discussion</title>
    <sec id="Sec3">
      <title>Experiment setup</title>
      <p>Given an unknown sequence, the objective is to determine if the sequence is an adaptor protein and thus this can be treated as a supervised learning classification. As a representation, we defined adaptor protein as positive data with label “Positive”, and otherwise, non-adaptor protein as negative data with label “Negative”. We applied 5-fold cross-validation method in our training dataset with hyper-parameter optimization techniques. Finally, the independent dataset was used to evaluate the correctness as well as overfitting in our model.</p>
      <p>Our proposed RNN model was implemented using PyTorch library with a Titan Xp GPU. We trained the RNN model from scratch using Adam optimizer for 30 epochs. The learning rate was fixed to 1×10<sup>−4</sup> in the entire training process. Due to the significant imbalance in the sample numbers of adaptor proteins and non-adaptor proteins in the dataset, we adopted weighted binary cross-entropy loss in the training process. The weighting factors were the inverse class frequency.</p>
      <p>Sensitivity, specificity, accuracy, and MCC (Matthew’s correlation coefficient) were used to measure the prediction performance. TP, FP, TN, FN are true positives, false positives, true negatives, and false negatives, respectively.
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \text{Sensitivity} = \frac{{TP}}{{TP + FN}}  $$ \end{document}</tex-math><mml:math id="M2"><mml:mtext>Sensitivity</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext mathvariant="italic">TP</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">FN</mml:mtext></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="12864_2019_6335_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>
        <disp-formula id="Equ2">
          <label>2</label>
          <alternatives>
            <tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \text{Specificity} = \frac{{TN}}{{TN + FP}}  $$ \end{document}</tex-math>
            <mml:math id="M4">
              <mml:mtext>Specificity</mml:mtext>
              <mml:mo>=</mml:mo>
              <mml:mfrac>
                <mml:mrow>
                  <mml:mtext mathvariant="italic">TN</mml:mtext>
                </mml:mrow>
                <mml:mrow>
                  <mml:mtext mathvariant="italic">TN</mml:mtext>
                  <mml:mo>+</mml:mo>
                  <mml:mtext mathvariant="italic">FP</mml:mtext>
                </mml:mrow>
              </mml:mfrac>
            </mml:math>
            <graphic xlink:href="12864_2019_6335_Article_Equ2.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>
        <disp-formula id="Equ3">
          <label>3</label>
          <alternatives>
            <tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \text{Accuracy} = \frac{{TP+TN}}{{TP + TN + FP + FN}}  $$ \end{document}</tex-math>
            <mml:math id="M6">
              <mml:mspace width="-12.0pt"/>
              <mml:mtext>Accuracy</mml:mtext>
              <mml:mo>=</mml:mo>
              <mml:mfrac>
                <mml:mrow>
                  <mml:mtext mathvariant="italic">TP</mml:mtext>
                  <mml:mo>+</mml:mo>
                  <mml:mtext mathvariant="italic">TN</mml:mtext>
                </mml:mrow>
                <mml:mrow>
                  <mml:mtext mathvariant="italic">TP</mml:mtext>
                  <mml:mo>+</mml:mo>
                  <mml:mtext mathvariant="italic">TN</mml:mtext>
                  <mml:mo>+</mml:mo>
                  <mml:mtext mathvariant="italic">FP</mml:mtext>
                  <mml:mo>+</mml:mo>
                  <mml:mtext mathvariant="italic">FN</mml:mtext>
                </mml:mrow>
              </mml:mfrac>
            </mml:math>
            <graphic xlink:href="12864_2019_6335_Article_Equ3.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>
        <disp-formula id="Equ4">
          <label>4</label>
          <alternatives>
            <tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \textrm{MCC} \,=\, \frac{{TP \times TN - FP \times FN}}{{\sqrt {(TP + FP)(TP + FN)(TN + FP)(TN + FN)} }}  $$ \end{document}</tex-math>
            <mml:math id="M8">
              <mml:mspace width="-15.0pt"/>
              <mml:mtext>MCC</mml:mtext>
              <mml:mspace width="0.3em"/>
              <mml:mo>=</mml:mo>
              <mml:mspace width="0.3em"/>
              <mml:mfrac>
                <mml:mrow>
                  <mml:mtext mathvariant="italic">TP</mml:mtext>
                  <mml:mo>×</mml:mo>
                  <mml:mtext mathvariant="italic">TN</mml:mtext>
                  <mml:mo>−</mml:mo>
                  <mml:mtext mathvariant="italic">FP</mml:mtext>
                  <mml:mo>×</mml:mo>
                  <mml:mtext mathvariant="italic">FN</mml:mtext>
                </mml:mrow>
                <mml:mrow>
                  <mml:msqrt>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mtext mathvariant="italic">TP</mml:mtext>
                      <mml:mo>+</mml:mo>
                      <mml:mtext mathvariant="italic">FP</mml:mtext>
                      <mml:mo>)</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mtext mathvariant="italic">TP</mml:mtext>
                      <mml:mo>+</mml:mo>
                      <mml:mtext mathvariant="italic">FN</mml:mtext>
                      <mml:mo>)</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mtext mathvariant="italic">TN</mml:mtext>
                      <mml:mo>+</mml:mo>
                      <mml:mtext mathvariant="italic">FP</mml:mtext>
                      <mml:mo>)</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mtext mathvariant="italic">TN</mml:mtext>
                      <mml:mo>+</mml:mo>
                      <mml:mtext mathvariant="italic">FN</mml:mtext>
                      <mml:mo>)</mml:mo>
                    </mml:mrow>
                  </mml:msqrt>
                </mml:mrow>
              </mml:mfrac>
            </mml:math>
            <graphic xlink:href="12864_2019_6335_Article_Equ4.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>In addition, we also utilized Receiver Operating Characteristic (ROC) curves to examine the predictive performance of our model. In the ROC curve, the Area Under the Curve (AUC) metric is a floating point value ranging from 0 to 1 in which higher value represents better model. ROC curve and AUC are reliable metrics to compare the performance results among different models.</p>
      <p>We first investigated the composition of amino acid in adaptor proteins and non-adaptor proteins to understand how we could better utilize the dataset for protein function prediction. We also studied how different hyper-parameters affected the performance of the RNN model. Besides, comparison between the proposed model and existing methods was based on the provided PSSM profiles.</p>
    </sec>
    <sec id="Sec4">
      <title>Comparison between adaptor proteins and non-adaptor proteins</title>
      <p>We computed the amino acid frequency of adaptor and non-adaptor proteins in the whole dataset to analyze the differences between the two types. It can be seen from Fig. <xref rid="Fig1" ref-type="fig">1</xref> that there are differences in amino acid composition surrounding adaptor and non-adaptor proteins. For example, the amino acid E, F, G, or V had higher variations to separate between two classes. The significant differences show that our model can distinguish adaptor proteins from general proteins according to some amino acid distributions.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Different compositions of amino acid in adaptor proteins and non-adaptor proteins. <italic>x</italic>-axis represents 20 amino acids, <italic>y</italic>-axis represents the frequency (%) of each amino acid</p></caption><graphic xlink:href="12864_2019_6335_Fig1_HTML" id="MO1"/></fig></p>
    </sec>
    <sec id="Sec5">
      <title>Study on selection of hyper-parameters</title>
      <p>In this section, the selection of hyper-parameters is studied. Specifically, we have examined our model with different hyper-parameters, i.e., number of convolution filters, fully connected layer size, kernel size, and so on. We performed 5-fold cross-validation and varied the number of filters of the fully connected layer from 32 to 1,024 to find the optimal number. Our model has been selected based on the optimal performance results on validation dataset at a specific random seed value (i.e., random_seed = 7 in this study).</p>
      <p>In our experiments, among different tested sizes, the fully connected layer size of 512 reached the maximum performance when discriminating the adaptor proteins in different validation settings. When testing our model in the independent dataset, the performance results were also consistent with the 5-fold cross-validation. It means that our model did not suffer from the over-fitting problem and can be applied in most of unseen data. A reason to explain this point is that we applied dropout, which is the regulation technique to prevent over-fitting in deep learning models.</p>
      <p>The next important hyper-parameter that needs to be examined is the gated recurrent unit (GRU) hidden layer size. After several steps, we observed that the GRU with 256 hidden layer sizes was superior. Finally, these optimal parameters were used on our best model.</p>
    </sec>
    <sec id="Sec6">
      <title>Comparison between the current method and state-of-the-art techniques using pSSM profiles</title>
      <p>After tuning up the hyper-parameters, we identified 512 filters and GRU size of 256 as the best performing architecture. We then used our optimized model to compare with the previous state-of-the-art methods. To use PSSM profiles, most recent techniques summed up all the same amino acids to produce a 400-dimensional vector and then fed to neural networks. A number of bioinformatics researchers have used this technique in their applications and obtained promising results [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR5">5</xref>]. We also conducted experiments according to widely used machine learning algorithms including <italic>k</italic>-NN [<xref ref-type="bibr" rid="CR20">20</xref>], the Random Forests (RF) [<xref ref-type="bibr" rid="CR21">21</xref>] and the Support Vector Machines (SVM) [<xref ref-type="bibr" rid="CR22">22</xref>]. Besides, we also compared our proposed method with a two-dimensional convolutional neural network (2-D CNN), which is a method treating PSSM profiles as images and successfully applied in sequence analysis [<xref ref-type="bibr" rid="CR5">5</xref>].</p>
      <p>Overall, the comparison between our proposed method and the other methods is shown in Table <xref rid="Tab1" ref-type="table">1</xref>. Note that we used grid search cross-validation to find the optimal parameters of all algorithms. This ensures that our comparison is fair and reliable among these methods. The optimal results were: <italic>k</italic>=10 nearest neighbors in <italic>k</italic>-NN, 500 trees in RF, <italic>c</italic>=8 and <italic>g</italic>=0.5 in SVM, and 128 filters with each filter size of 3×3 in 2-D CNN. We easily observe that our RNN also exhibited the higher performance than the other techniques at the same level comparison. This was also supported by our preliminary work when testing with other classification algorithms including kernel dictionary learning [<xref ref-type="bibr" rid="CR23">23</xref>–<xref ref-type="bibr" rid="CR25">25</xref>] and an enhanced k-NN method [<xref ref-type="bibr" rid="CR26">26</xref>]. It can be concluded that the sequential information of PSSM plays a vital role in predicting the adaptor protein as well as the other protein functions in general. Using 1D-CNN in our method helps to prevent the loss of sequential information compared to other embedding methods (e.g., 2D-CNN).
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Performance results of distinguishing adaptor proteins with different methods</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left" colspan="5">Cross Validation</th><th align="left" colspan="5">Independent Test</th></tr><tr><th align="left"/><th align="left">Sensitivity</th><th align="left">Specificity</th><th align="left">Accuracy</th><th align="left">AUC</th><th align="left">MCC</th><th align="left">Sensitivity</th><th align="left">Specificity</th><th align="left">Accuracy</th><th align="left">AUC</th><th align="left">MCC</th></tr></thead><tbody><tr><td align="left"><italic>k</italic>-NN</td><td align="left">0.635</td><td align="left">0.750</td><td align="left">0.738</td><td align="left">0.770</td><td align="left">0.254</td><td align="left">0.671</td><td align="left">0.751</td><td align="left">0.743</td><td align="left">0.791</td><td align="left">0.280</td></tr><tr><td align="left">RF</td><td align="left">0.185</td><td align="left"><bold>0.968</bold></td><td align="left"><bold>0.890</bold></td><td align="left">0.837</td><td align="left">0.214</td><td align="left">0.290</td><td align="left">0.923</td><td align="left">0.860</td><td align="left">0.838</td><td align="left">0.216</td></tr><tr><td align="left">SVM</td><td align="left">0.397</td><td align="left">0.934</td><td align="left">0.881</td><td align="left">0.818</td><td align="left">0.332</td><td align="left">0.426</td><td align="left"><bold>0.932</bold></td><td align="left"><bold>0.881</bold></td><td align="left">0.806</td><td align="left">0.353</td></tr><tr><td align="left">CNN</td><td align="left">0.532</td><td align="left">0.875</td><td align="left">0.841</td><td align="left">0.774</td><td align="left">0.328</td><td align="left">0.548</td><td align="left">0.873</td><td align="left">0.841</td><td align="left">0.783</td><td align="left">0.339</td></tr><tr><td align="left">RNN</td><td align="left"><bold>0.812</bold></td><td align="left">0.751</td><td align="left">0.757</td><td align="left"><bold>0.853</bold></td><td align="left"><bold>0.373</bold></td><td align="left"><bold>0.856</bold></td><td align="left">0.798</td><td align="left">0.804</td><td align="left"><bold>0.893</bold></td><td align="left"><bold>0.446</bold></td></tr></tbody></table><table-wrap-foot><p>(<italic>k</italic>-NN: <italic>k</italic>=10; RF: num_stimators=500; SVM: <italic>c</italic>=8.0, <italic>g</italic>=0.5; CNN: 128 filters; RNN: 512 filters)</p></table-wrap-foot></table-wrap></p>
      <p>Specifically, our sensitivity was significantly higher than that of the other methods. This is a very important point because our model aims to predict as much as adaptor proteins as possible. Via this high sensitivity, a large number of adaptor proteins could be discovered with a high accuracy. It provides a lot of information for biologists as well as researchers to understand and conduct their works on adaptor proteins.</p>
      <p>The results in Table <xref rid="Tab1" ref-type="table">1</xref> are based on the default decision threshold value of each algorithm and this is not sufficiently significant. Hence, we show the ROC Curve and AUC to evaluate the performance results at different threshold levels. They are the most important evaluation metrics for checking the performance of most supervised learning classification. The ROC curve is plotted from True Positive Rate and False Positive Rate. As the value of AUC approaches to unity, the corresponding model is regarded to have shown optimal performance. As shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, our model could predict the adaptor proteins with AUC of 0.893 and this is a significant level to show that our model performed well in this kind of dataset. It also determines that our results did not only perform well in a specific point but also at different levels. We can use this model to predict adaptor proteins with high performance and superior to the previous techniques (Table <xref rid="Tab1" ref-type="table">1</xref>).
<fig id="Fig2"><label>Fig. 2</label><caption><p>The receiver operating characteristic (ROC) curve of one fold in our experiments</p></caption><graphic xlink:href="12864_2019_6335_Fig2_HTML" id="MO2"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec7" sec-type="conclusion">
    <title>Conclusions</title>
    <p>In this study, we proposed an innovative method using RNN and PSSM profiles for distinguishing the adaptor proteins using sequence information only. It is also the first computational model that applies this combination to adaptor protein prediction. Via this method, we can conserve all the PSSM information in training process and to prevent the missing information as much as possible. The performance using 5-fold cross validation and independent testing dataset (including 245 adaptor proteins and 2,202 non-adaptor proteins) is evaluated. The proposed method could predict adaptor proteins with a 5-fold cross validation accuracy and MCC of 80.4% and 44.5%, respectively. To evaluate the correctness of our model, we applied an independent dataset testing and its accuracy and MCC achieved 75.7% and 37.3%, respectively. Our performance results are superior to the state-of-the-art methods in term of accuracy, MCC, as well as the other metrics.</p>
    <p>This study discussed a powerful model for discovering new proteins that belong to adaptor proteins or not. This study opens a research path that can promote the use of RNN and PSSM profiles in bioinformatics and computational biology. Our approach is able to be reproduced by scientists that aim to improve the performance results of different protein function prediction problems.</p>
    <p>Finally, physicochemical properties had been successfully used in a number of bioinformatics applications with high performance [<xref ref-type="bibr" rid="CR27">27</xref>–<xref ref-type="bibr" rid="CR29">29</xref>]. Therefore, it is possible to combine PSSM profiles and physicochemical proteins into a set of hybrid features. Subsequently, these hybrid features could be fed directly into our proposed architecture. We hope that the future studies will consider these hybrid features to help improving the performance results of protein function prediction.</p>
  </sec>
  <sec id="Sec8">
    <title>Methods</title>
    <sec id="Sec9">
      <title>Benchmark dataset</title>
      <p>Figure <xref rid="Fig3" ref-type="fig">3</xref> illustrates the flowchart of the study. A detailed description on the construction of the benchmark dataset is provided as follows.
<fig id="Fig3"><label>Fig. 3</label><caption><p>Flowchart of the study</p></caption><graphic xlink:href="12864_2019_6335_Fig3_HTML" id="MO3"/></fig></p>
      <p>Because our study is the first computational study to classify adaptor proteins, therefore, we manually created a dataset from well-known protein data sources. We collected data from UniProt [<xref ref-type="bibr" rid="CR30">30</xref>] and Gene Ontology (GO) [<xref ref-type="bibr" rid="CR31">31</xref>], which provide high quality resources for research on gene products. We collected all the proteins from UniProt with GO molecular function annotations related to adaptor proteins. An important selection criteria is that we had to select the reviewed sequences, which means they had been published in scientific papers. Thus, the full query for collecting data was:</p>
      <p>“keyword:“adaptor” OR goa:(“adaptor”)) AND reviewed:yes”
</p>
      <p>After this step, we received 4,049 adaptor proteins in all species.</p>
      <p>We solved the proposed problem as a binary classification problem, thus we collected a set of general proteins as negative samples. Actually, our classifier aimed to classify between adaptor proteins and non-adaptor proteins. So we needed a real set of adaptors and non-adaptors to train the model. However, in practice, if we collect all non-adaptor proteins as negative data, the number of negative dataset will reach hundred thousands of data. This will result in serious data imbalance and affect the model’s performance. Therefore, in most of the related problems in bioinformatics, scientists can only select a subset of negative data and treat them as general proteins. In this study, we chose membrane protein, which is a general protein including a big enough number of sequences and functions. Briefly, we extracted all of the membrane proteins in UniProt and excluded the adaptor proteins. Similar to the previous step, only reviewed proteins were retained.</p>
      <p>Subsequently, BLAST [<xref ref-type="bibr" rid="CR32">32</xref>] was applied to all the collected data to remove redundant sequences with sequence identity level of more than 30%. This was an important step to prevent over-fitting in training model. The remaining sequences were regarded as valid for the benchmark dataset and were naturally divided into 1,224 adaptor proteins and 11,078 non-adaptor proteins. For fair comparison, we held up one-fifths of both the adaptor proteins and the non-adaptor proteins as the test set to evaluate model performance. The rest of the valid sequences were used as a cross-validation (Train-Val) set for model training. Table <xref rid="Tab2" ref-type="table">2</xref> lists the statistics of the benchmark dataset.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Statistics of the benchmark dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Original</th><th align="left" colspan="3">Non-Redundant</th></tr><tr><th align="left"/><th align="left"/><th align="left">Total</th><th align="left">Train-Val</th><th align="left">Test</th></tr></thead><tbody><tr><td align="left">Adaptor</td><td align="left">4049</td><td align="left">1224</td><td align="left">1069</td><td align="left">155</td></tr><tr><td align="left">Non-Adaptor</td><td align="left">23,917</td><td align="left">11,078</td><td align="left">9695</td><td align="left">1383</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec10">
      <title>RNN model</title>
      <p>In this study, we propose an RNN model for distinguishing adaptor proteins from non-adaptor proteins. An overview of the proposed RNN model is shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. The RNN model takes PSSM profiles as inputs and extracts their features by several one dimensional (1-D) convolution layers and 1-D average pooling layers. The extracted features are then fed forward to gated recurrent units (GRUs), where the spatial context within the entire PSSM profile is explored and utilized for final prediction. The input sequence has a length of <italic>N</italic>. After going through two layers of 1-D CNN and 1-D Max-Pool, the length became <italic>N</italic>/9. Subsequently, this <italic>N</italic>/9 vector was fed into GRU, taking the output of GRU (256 features) to the input of the last vector for which the characteristic of the sequence was formed. Finally, our model took this output through a Fully Connected (FC) layer (512 nodes), and passed to a Sigmoid layer to produce a prediction probability value.
<fig id="Fig4"><label>Fig. 4</label><caption><p>Architecture of the RNN model</p></caption><graphic xlink:href="12864_2019_6335_Fig4_HTML" id="MO4"/></fig></p>
      <sec id="Sec11">
        <title>Preventing information missing by preserving ordering of pSSM profiles</title>
        <p>A PSSM profile for a query protein is an <italic>N</italic>×20 matrix (<italic>N</italic> is the length of the query sequence), in which a score <italic>P</italic><sub><italic>ij</italic></sub> is assigned for the <italic>j</italic><sup><italic>t</italic><italic>h</italic></sup> amino acid in the <italic>i</italic><sup><italic>t</italic><italic>h</italic></sup> position of the query sequence with a large value and a small value indicating a highly conservative position and a weakly conservative position, respectively.</p>
        <p>PSSM was first proposed by [<xref ref-type="bibr" rid="CR1">1</xref>] and applied to various bioinformatics applications with promising improvements. The acquired protein sequences in the benchmark dataset are in FASTA format. From these FASTA sequences, we used PSI-BLAST [<xref ref-type="bibr" rid="CR32">32</xref>] to generate PSSM profiles by searching them in the non-redundant (NR) database with two iterations.</p>
        <p>Some studies attempted to predict the protein functions by summing up all of the same amino acids [<xref ref-type="bibr" rid="CR2">2</xref>]. It helped to convert PSSM profiles with 20×<italic>N</italic> matrix to 20×20 matrix and all of the sequences had the same input length that can be easily used in supervised classification learning. However, important information could be lost since the ordering of PSSM profiles would be discarded. Therefore, an RNN architecture was presented to not only input PSSM profiles but also preserve the ordering. As the proposed RNN network accepts PSSM sequences with different lengths, we were thus able to well utilize their spatial context for better protein function prediction.</p>
      </sec>
      <sec id="Sec12">
        <title>Feature extraction via CNN</title>
        <p>The proposed RNN model first extracts convolutional features maps from PSSM profiles via an 1-D CNN. The CNN contains two 1-D convolution layers, each followed by a Rectified Linear Unit (ReLU) as non-linear activation, and two average pooling layers to reduce the dimension of the feature maps as well as enlarge the receptive field of the CNN network. The extracted feature maps are then fed forward to the RNN module for exploring the spatial relationship within the entire PSSM profile before final prediction.</p>
      </sec>
      <sec id="Sec13">
        <title>Learning and classification using RNN</title>
        <p>RNN is a neural network which had been shown to perform very well in various fields such as time series prediction [<xref ref-type="bibr" rid="CR33">33</xref>], speech recognition [<xref ref-type="bibr" rid="CR34">34</xref>], and language model [<xref ref-type="bibr" rid="CR35">35</xref>]. Since RNN can memorize parts of sequential data, we used GRU which is an advanced architecture of RNN in this study.</p>
        <p>After using the aforementioned CNN to create feature maps, we applied a multi-layer GRU to the extracted features. The standard RNN has a major drawback called the gradient vanishing problem, leading to that the network fails in memorizing information which is far away from the sequence and it makes predictions based on the most recent information only. Therefore, more powerful recurrent units, like GRU and Long Short-Term Memory (LSTM), were explored and introduced.</p>
        <p>GRU is an advanced version of the standard RNN, in which the gradient vanishing problem is resolved by the introduction of an update gate and a reset gate for determining what information should be passed or discarded. GRU enables the possibility of long dependencies between the current input and far away information.</p>
        <p>Basically, the structure of GRU is similar to LSTM. However, the fact that GRU requires less parameters than LSTM so it is more suitable for small datasets. This eases the training procedure and motivates us to adopt GRU as the basic unit in our RNN module. In the RNN module, a GRU layer consists of two gates:</p>
        <p>(1) Update gate decides what information to throw away and what new information to add. To calculate the update gate <italic>z</italic><sub><italic>t</italic></sub>, we used the following formula:
<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ z_{t} = \sigma \left(W_{iz} x_{t} + b_{iz} + W_{hz} h_{(t - 1)} + b_{hz} \right),  $$ \end{document}</tex-math><mml:math id="M10"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">iz</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">iz</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">hz</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">hz</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math><graphic xlink:href="12864_2019_6335_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>where <italic>t</italic> is the time step, <italic>σ</italic> represents the sigmoid function, <italic>W</italic> represents weight, <italic>x</italic><sub><italic>t</italic></sub> represents the input at time <italic>t</italic>, <italic>h</italic><sub>(<italic>t</italic>−1)</sub> represents the hidden state of the previous layer at time <italic>t</italic>−1 or the initial hidden state at time 0, and <italic>b</italic> represents bias.</p>
        <p>(2) Reset gate is applied in the model to determine how much past information to forget. The following formula is used:
<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ r_{t} = \sigma \left({W_{ir} x_{t} + b_{ir} + W_{hr} h_{(t - 1)} + b_{hr}} \right).  $$ \end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">ir</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">ir</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">hr</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">hr</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi>.</mml:mi></mml:math><graphic xlink:href="12864_2019_6335_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>Moreover, to save the past information from the reset gate, GRU uses a current memory content. It can be calculated using the following equation:
<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ n_{t} = \tanh \left({W_{in} x_{t} + b_{in} + r_{t} \circ \left({W_{hn} h_{(t - 1)} + b_{hn}} \right)} \right).  $$ \end{document}</tex-math><mml:math id="M14"><mml:mspace width="-5.0pt"/><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>tanh</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">in</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">in</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∘</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">hn</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">hn</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mi>.</mml:mi></mml:math><graphic xlink:href="12864_2019_6335_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>Finally, the last step is final memory, to determine what to collect from the current memory content and the previous steps at the last step. To perform this step, GRU calculates vector <italic>h</italic><sub><italic>t</italic></sub> as follows:
<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ h_{t} = (1 - z_{t})\circ n_{t} + z_{t} \circ h_{(t - 1)}.  $$ \end{document}</tex-math><mml:math id="M16"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>∘</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∘</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msub><mml:mi>.</mml:mi></mml:math><graphic xlink:href="12864_2019_6335_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>The final output from the RNN module is then mapped to the prediction with a fully connected layer and the sigmoid function. The output from the RNN model is a scalar in [0,1] representing the probability that the PSSM profile belongs to the adaptor protein category (close to 1) or the non-adaptor protein category (close to 0).</p>
      </sec>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>AUC</term>
        <def>
          <p>Area under the ROC curve</p>
        </def>
      </def-item>
      <def-item>
        <term>CNN</term>
        <def>
          <p>Convolutional neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>GRU</term>
        <def>
          <p>Gated recurrent unit</p>
        </def>
      </def-item>
      <def-item>
        <term>LSTM</term>
        <def>
          <p>Long short-term memory</p>
        </def>
      </def-item>
      <def-item>
        <term>MCC</term>
        <def>
          <p>Matthew’s correlation coefficient</p>
        </def>
      </def-item>
      <def-item>
        <term>PSSM</term>
        <def>
          <p>Position specific scoring matrix</p>
        </def>
      </def-item>
      <def-item>
        <term>ReLU</term>
        <def>
          <p>Rectified linear unit</p>
        </def>
      </def-item>
      <def-item>
        <term>RF</term>
        <def>
          <p>Random forest</p>
        </def>
      </def-item>
      <def-item>
        <term>RNN</term>
        <def>
          <p>Recurrent neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>ROC</term>
        <def>
          <p>Receiver operating characteristic</p>
        </def>
      </def-item>
      <def-item>
        <term>SVM</term>
        <def>
          <p>Support vector machine</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
    <fn>
      <p>Nguyen Quoc Khanh Le and Quang H. Nguyen contributed equally to this work.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>BPN and QHN gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan V and Titan Xp GPUs used for this research.</p>
    <sec id="d29e1660">
      <title>About this supplement</title>
      <p>This article has been published as part of BMC Genomics, Volume 20 Supplement 9, 2019: 18th International Conference on Bioinformatics. The full contents of the supplement are available at <ext-link ext-link-type="uri" xlink:href="https://bmcgenomics.biomedcentral.com/articles/supplements/volume-20-supplement-9">https://bmcgenomics.biomedcentral.com/articles/supplements/volume-20-supplement-9</ext-link>.</p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>NQKL contributed to data preparation and the first draft of the work. QHN and BPN designed the method and implemented the code. The experiments were performed by QHN and NQKL. XC contributed to the second draft of the work. SR and BPN supervised the study and substantively revised the manuscript. Interpretation of data and experimental results were done by all authors. All authors have read and approved the manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>The authors received no specific funding for this work. The publication costs were covered by the authors.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>Our source code and datasets are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/ngphubinh/adaptors">https://github.com/ngphubinh/adaptors</ext-link>.</p>
  </notes>
  <notes>
    <title>Ethics approval and consent to participate</title>
    <p>Not applicable.</p>
  </notes>
  <notes>
    <title>Consent for publication</title>
    <p>Not applicable.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Competing interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jones</surname>
            <given-names>DT</given-names>
          </name>
        </person-group>
        <article-title>Protein secondary structure prediction based on position-specific scoring matrices</article-title>
        <source>J Mole Biol</source>
        <year>1999</year>
        <volume>292</volume>
        <issue>2</issue>
        <fpage>195</fpage>
        <lpage>202</lpage>
        <pub-id pub-id-type="doi">10.1006/jmbi.1999.3091</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>S-A</given-names>
          </name>
          <name>
            <surname>Ou</surname>
            <given-names>Y-Y</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>T-Y</given-names>
          </name>
          <name>
            <surname>Gromiha</surname>
            <given-names>MM</given-names>
          </name>
        </person-group>
        <article-title>Prediction of transporter targets using efficient RBF networks with PSSM profiles and biochemical properties</article-title>
        <source>Bioinformatics</source>
        <year>2011</year>
        <volume>27</volume>
        <issue>15</issue>
        <fpage>2062</fpage>
        <lpage>7</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btr340</pub-id>
        <pub-id pub-id-type="pmid">21653515</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cheng</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Chou</surname>
            <given-names>K-C</given-names>
          </name>
        </person-group>
        <article-title>pLoc-mHum: predict subcellular localization of multi-location human proteins via general PseAAC to winnow out the crucial GO information</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>34</volume>
        <issue>9</issue>
        <fpage>1448</fpage>
        <lpage>56</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx711</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Taju</surname>
            <given-names>SW</given-names>
          </name>
          <name>
            <surname>Nguyen</surname>
            <given-names>T-T-D</given-names>
          </name>
          <name>
            <surname>Le</surname>
            <given-names>N-Q-K</given-names>
          </name>
          <name>
            <surname>Kusuma</surname>
            <given-names>RMI</given-names>
          </name>
          <name>
            <surname>Ou</surname>
            <given-names>Y-Y</given-names>
          </name>
        </person-group>
        <article-title>DeepEfflux: a 2-D convolutional neural network model for identifying families of efflux proteins in transporters</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>34</volume>
        <issue>18</issue>
        <fpage>3111</fpage>
        <lpage>7</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty302</pub-id>
        <pub-id pub-id-type="pmid">29668844</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5</label>
      <mixed-citation publication-type="other">Le N-Q-K, Nguyen BP. Prediction of FMN binding sites in electron transport chains based on 2-D CNN and PSSM profiles. IEEE/ACM Trans Comput Biol Bioinforma. 2019:1–9. 10.1109/TCBB.2019.2932416.</mixed-citation>
    </ref>
    <ref id="CR6">
      <label>6</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Flynn</surname>
            <given-names>DC</given-names>
          </name>
        </person-group>
        <article-title>Adaptor proteins</article-title>
        <source>Oncogene</source>
        <year>2001</year>
        <volume>20</volume>
        <issue>44</issue>
        <fpage>6270</fpage>
        <pub-id pub-id-type="doi">10.1038/sj.onc.1204769</pub-id>
        <pub-id pub-id-type="pmid">11607828</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7</label>
      <mixed-citation publication-type="other">Verma S, Vaughan T, Bunting KD. Gab adapter proteins as therapeutic targets for hematologic disease. Adv Hematol. 2012; 2012. 10.1155/2012/380635. Accessed 01 Apr 2019.</mixed-citation>
    </ref>
    <ref id="CR8">
      <label>8</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shiozaki</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Roles of XB130, a novel adaptor protein, in cancer</article-title>
        <source>J Clin Bioinforma</source>
        <year>2011</year>
        <volume>1</volume>
        <issue>1</issue>
        <fpage>10</fpage>
        <pub-id pub-id-type="doi">10.1186/2043-9113-1-10</pub-id>
        <pub-id pub-id-type="pmid">21884627</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9</label>
      <mixed-citation publication-type="other">Marton N, Baricza E, Érsek B, Buzás EI, Nagy G. The emerging and diverse roles of src-like adaptor proteins in health and disease. Mediators Inflam. 2015; 2015. 10.1155/2015/952536. Accessed 01 Apr 2019.</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Isaka</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Adaptor protein is a new therapeutic target in chronic kidney disease</article-title>
        <source>Kidney Int</source>
        <year>2017</year>
        <volume>92</volume>
        <issue>6</issue>
        <fpage>1312</fpage>
        <lpage>4</lpage>
        <pub-id pub-id-type="doi">10.1016/j.kint.2017.06.012</pub-id>
        <pub-id pub-id-type="pmid">29153133</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tao</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Jia</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Recent advances of adapter proteins in the regulation of heart diseases</article-title>
        <source>Heart Fail Rev</source>
        <year>2017</year>
        <volume>22</volume>
        <issue>1</issue>
        <fpage>99</fpage>
        <lpage>107</lpage>
        <pub-id pub-id-type="doi">10.1007/s10741-016-9582-3</pub-id>
        <pub-id pub-id-type="pmid">27623843</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hatsugai</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Nakatsuji</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Unten</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Ogasawara</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Kondo</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Nishimura</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Shimada</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Katagiri</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Hara-Nishimura</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>Involvement of adapter protein complex 4 in hypersensitive cell death induced by avirulent bacteria</article-title>
        <source>Plant Physiol</source>
        <year>2018</year>
        <volume>176</volume>
        <issue>2</issue>
        <fpage>1824</fpage>
        <lpage>34</lpage>
        <pub-id pub-id-type="doi">10.1104/pp.17.01610</pub-id>
        <pub-id pub-id-type="pmid">29242374</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Paliwal</surname>
            <given-names>KK</given-names>
          </name>
          <name>
            <surname>Sharma</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Lyons</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Dehzangi</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>A tri-gram based feature extraction technique using linear probabilities of position specific scoring matrix for protein fold recognition</article-title>
        <source>IEEE Trans NanoBiosci</source>
        <year>2014</year>
        <volume>13</volume>
        <issue>1</issue>
        <fpage>44</fpage>
        <lpage>50</lpage>
        <pub-id pub-id-type="doi">10.1109/TNB.2013.2296050</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chandra</surname>
            <given-names>AA</given-names>
          </name>
          <name>
            <surname>Sharma</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Dehzangi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Tsunoda</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>EvolStruct-Phogly: incorporating structural properties and evolutionary information from profile bigrams for the phosphoglycerylation prediction</article-title>
        <source>BMC Genomics</source>
        <year>2019</year>
        <volume>19</volume>
        <issue>9</issue>
        <fpage>984</fpage>
        <pub-id pub-id-type="doi">10.1186/s12864-018-5383-5</pub-id>
        <pub-id pub-id-type="pmid">30999859</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dehzangi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>López</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Lal</surname>
            <given-names>SP</given-names>
          </name>
          <name>
            <surname>Taherzadeh</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Sattar</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Tsunoda</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Sharma</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Improving succinylation prediction accuracy by incorporating the secondary structure via helix, strand and coil, and evolutionary information from profile bigrams</article-title>
        <source>PLOS ONE</source>
        <year>2018</year>
        <volume>13</volume>
        <fpage>1</fpage>
        <lpage>16</lpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0191900</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dehzangi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Heffernan</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Sharma</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Lyons</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Paliwal</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Sattar</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Gram-positive and gram-negative protein subcellular localization by incorporating evolutionary-based descriptors into Chou’s general PseAAC</article-title>
        <source>J Theoret Biol</source>
        <year>2015</year>
        <volume>364</volume>
        <fpage>284</fpage>
        <lpage>94</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jtbi.2014.09.029</pub-id>
        <pub-id pub-id-type="pmid">25264267</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Deep recurrent neural network for protein function prediction from sequence</article-title>
        <source>CoRR</source>
        <year>2017</year>
        <volume>abs/1701.08318</volume>
        <fpage>1</fpage>
        <lpage>9</lpage>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cao</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Freitas</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Chan</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>ProLanGO: protein function prediction using neural machine translation based on a recurrent neural network</article-title>
        <source>Molecules</source>
        <year>2017</year>
        <volume>22</volume>
        <issue>10</issue>
        <fpage>1732</fpage>
        <pub-id pub-id-type="doi">10.3390/molecules22101732</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hanson</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Paliwal</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Improving protein disorder prediction by deep bidirectional long short-term memory recurrent neural networks</article-title>
        <source>Bioinformatics</source>
        <year>2016</year>
        <volume>33</volume>
        <issue>5</issue>
        <fpage>685</fpage>
        <lpage>92</lpage>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Keller</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Gray</surname>
            <given-names>MR</given-names>
          </name>
          <name>
            <surname>Givens</surname>
            <given-names>JA</given-names>
          </name>
        </person-group>
        <article-title>A fuzzy k-nearest neighbor algorithm</article-title>
        <source>IEEE Trans Syst, Man, Cybernet</source>
        <year>1985</year>
        <volume>SMC-15</volume>
        <issue>4</issue>
        <fpage>580</fpage>
        <lpage>5</lpage>
        <pub-id pub-id-type="doi">10.1109/TSMC.1985.6313426</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Breiman</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Random forests</article-title>
        <source>Mach Learn</source>
        <year>2001</year>
        <volume>45</volume>
        <issue>1</issue>
        <fpage>5</fpage>
        <lpage>32</lpage>
        <pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chang</surname>
            <given-names>C-C</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>C-J</given-names>
          </name>
        </person-group>
        <article-title>LIBSVM: A library for support vector machines</article-title>
        <source>ACM Transactions on Intelligent Systems and Technology</source>
        <year>2011</year>
        <volume>2</volume>
        <issue>3</issue>
        <fpage>27</fpage>
        <lpage>12727</lpage>
        <pub-id pub-id-type="doi">10.1145/1961189.1961199</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23</label>
      <mixed-citation publication-type="other">Chen X, Nguyen BP, Chui C-K, Ong S-H. Automated brain tumor segmentation using kernel dictionary learning and superpixel-level features. In: Proceedings of the 2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC 2016): 2016. p. 2547–52. 10.1109/SMC.2016.7844622.</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Nguyen</surname>
            <given-names>BP</given-names>
          </name>
          <name>
            <surname>Chui</surname>
            <given-names>C-K</given-names>
          </name>
          <name>
            <surname>Ong</surname>
            <given-names>S-H</given-names>
          </name>
        </person-group>
        <article-title>An automatic framework for multi-label brain tumor segmentation based on kernel sparse representation</article-title>
        <source>Acta Polytechnica Hungarica</source>
        <year>2017</year>
        <volume>14</volume>
        <issue>1</issue>
        <fpage>25</fpage>
        <lpage>43</lpage>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Nguyen</surname>
            <given-names>BP</given-names>
          </name>
          <name>
            <surname>Chui</surname>
            <given-names>C-K</given-names>
          </name>
          <name>
            <surname>Ong</surname>
            <given-names>S-H</given-names>
          </name>
        </person-group>
        <article-title>Reworking multilabel brain tumor segmentation – an automated framework using structured kernel sparse representation</article-title>
        <source>IEEE Syst, Man, Cybernet Mag</source>
        <year>2017</year>
        <volume>3</volume>
        <issue>2</issue>
        <fpage>18</fpage>
        <lpage>22</lpage>
        <pub-id pub-id-type="doi">10.1109/MSMC.2017.2664158</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nguyen</surname>
            <given-names>BP</given-names>
          </name>
          <name>
            <surname>Tay</surname>
            <given-names>W-L</given-names>
          </name>
          <name>
            <surname>Chui</surname>
            <given-names>C-K</given-names>
          </name>
        </person-group>
        <article-title>Robust biometric recognition from palm depth images for gloved hands</article-title>
        <source>IEEE Trans Human-Mach Syst</source>
        <year>2015</year>
        <volume>45</volume>
        <issue>6</issue>
        <fpage>799</fpage>
        <lpage>804</lpage>
        <pub-id pub-id-type="doi">10.1109/THMS.2015.2453203</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sarda</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Chua</surname>
            <given-names>GH</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>K-B</given-names>
          </name>
          <name>
            <surname>Krishnan</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>pSLIP: SVM based protein subcellular localization prediction using multiple physicochemical properties</article-title>
        <source>BMC Bioinformatics</source>
        <year>2005</year>
        <volume>6</volume>
        <issue>1</issue>
        <fpage>152</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-6-152</pub-id>
        <pub-id pub-id-type="pmid">15963230</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Du</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Prediction of protein submitochondria locations by hybridizing pseudo-amino acid composition with various physicochemical features of segmented sequence</article-title>
        <source>BMC Bioinformatics</source>
        <year>2006</year>
        <volume>7</volume>
        <issue>1</issue>
        <fpage>518</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-7-518</pub-id>
        <pub-id pub-id-type="pmid">17134515</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nanni</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Lumini</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>MppS: An ensemble of support vector machine based on multiple physicochemical properties of amino acids</article-title>
        <source>Neurocomputing</source>
        <year>2006</year>
        <volume>69</volume>
        <issue>13</issue>
        <fpage>1688</fpage>
        <lpage>90</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neucom.2006.04.001</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Consortium</surname>
            <given-names>U</given-names>
          </name>
        </person-group>
        <article-title>UniProt: a hub for protein information</article-title>
        <source>Nucleic Acids Res</source>
        <year>2014</year>
        <volume>43</volume>
        <issue>D1</issue>
        <fpage>204</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gku989</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ashburner</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ball</surname>
            <given-names>CA</given-names>
          </name>
          <name>
            <surname>Blake</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Botstein</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Butler</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Cherry</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Davis</surname>
            <given-names>AP</given-names>
          </name>
          <name>
            <surname>Dolinski</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Dwight</surname>
            <given-names>SS</given-names>
          </name>
          <name>
            <surname>Eppig</surname>
            <given-names>JT</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Gene ontology: tool for the unification of biology</article-title>
        <source>Nature Genet</source>
        <year>2000</year>
        <volume>25</volume>
        <issue>1</issue>
        <fpage>25</fpage>
        <pub-id pub-id-type="doi">10.1038/75556</pub-id>
        <pub-id pub-id-type="pmid">10802651</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Altschul</surname>
            <given-names>SF</given-names>
          </name>
          <name>
            <surname>Madden</surname>
            <given-names>TL</given-names>
          </name>
          <name>
            <surname>Schäffer</surname>
            <given-names>AA</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Miller</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Lipman</surname>
            <given-names>DJ</given-names>
          </name>
        </person-group>
        <article-title>Gapped BLAST and PSI-BLAST: a new generation of protein database search programs</article-title>
        <source>Nucleic Acids Res</source>
        <year>1997</year>
        <volume>25</volume>
        <issue>17</issue>
        <fpage>3389</fpage>
        <lpage>402</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/25.17.3389</pub-id>
        <pub-id pub-id-type="pmid">9254694</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Connor</surname>
            <given-names>JT</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>RD</given-names>
          </name>
          <name>
            <surname>Atlas</surname>
            <given-names>LE</given-names>
          </name>
        </person-group>
        <article-title>Recurrent neural networks and robust time series prediction</article-title>
        <source>IEEE Trans Neural Netw</source>
        <year>1994</year>
        <volume>5</volume>
        <issue>2</issue>
        <fpage>240</fpage>
        <lpage>54</lpage>
        <pub-id pub-id-type="doi">10.1109/72.279188</pub-id>
        <pub-id pub-id-type="pmid">18267794</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34</label>
      <mixed-citation publication-type="other">Graves A, Mohamed A-r, Hinton G. Speech recognition with deep recurrent neural networks. In: Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing: 2013. p. 6645–9. 10.1109/ICASSP.2013.6638947.</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Mikolov</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Karafiát</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Burget</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Černockỳ</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Khudanpur</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Recurrent neural network based language model</article-title>
        <source>Proceedings of the 11th Annual Conference of the International Speech Communication Association</source>
        <year>2010</year>
        <publisher-loc>Baixas</publisher-loc>
        <publisher-name>International Speech Communication Association</publisher-name>
      </element-citation>
    </ref>
  </ref-list>
</back>
