<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sci Rep</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id>
    <journal-title-group>
      <journal-title>Scientific Reports</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2045-2322</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8055887</article-id>
    <article-id pub-id-type="publisher-id">86912</article-id>
    <article-id pub-id-type="doi">10.1038/s41598-021-86912-w</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>PathoNet introduced as a deep neural network backend for evaluation of Ki-67 and tumor-infiltrating lymphocytes in breast cancer</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Negahbani</surname>
          <given-names>Farzin</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff7">7</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Sabzi</surname>
          <given-names>Rasool</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Pakniyat Jahromi</surname>
          <given-names>Bita</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Firouzabadi</surname>
          <given-names>Dena</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Movahedi</surname>
          <given-names>Fateme</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kohandel Shirazi</surname>
          <given-names>Mahsa</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Majidi</surname>
          <given-names>Shayan</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Dehghanian</surname>
          <given-names>Amirreza</given-names>
        </name>
        <address>
          <email>adehghan@sums.ac.ir</email>
        </address>
        <xref ref-type="aff" rid="Aff5">5</xref>
        <xref ref-type="aff" rid="Aff6">6</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.412573.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 0745 1259</institution-id><institution>Department of Computer Science and Engineering, </institution><institution>Shiraz University, </institution></institution-wrap>Shiraz, Iran </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.412571.4</institution-id><institution-id institution-id-type="ISNI">0000 0000 8819 4698</institution-id><institution>Department of Pathology, </institution><institution>Shiraz University of Medical science, </institution></institution-wrap>Shiraz, Iran </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.412571.4</institution-id><institution-id institution-id-type="ISNI">0000 0000 8819 4698</institution-id><institution>Student Research Committee, </institution><institution>Shiraz University of Medical Science, </institution></institution-wrap>Shiraz, Iran </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.412571.4</institution-id><institution-id institution-id-type="ISNI">0000 0000 8819 4698</institution-id><institution>Department of Clinical Pharmacy, </institution><institution>Shiraz University of Medical Sciences, </institution></institution-wrap>Shiraz, Iran </aff>
      <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.412571.4</institution-id><institution-id institution-id-type="ISNI">0000 0000 8819 4698</institution-id><institution>Molecular Pathology and Cytogenetics Division, Department of Pathology, </institution><institution>Shiraz University of Medical Sciences, </institution></institution-wrap>Shiraz, Iran </aff>
      <aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="GRID">grid.412571.4</institution-id><institution-id institution-id-type="ISNI">0000 0000 8819 4698</institution-id><institution>Trauma Research Center, </institution><institution>Shiraz University of Medical Sciences, </institution></institution-wrap>Shiraz, Iran </aff>
      <aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="GRID">grid.15876.3d</institution-id><institution-id institution-id-type="ISNI">0000000106887552</institution-id><institution>Present Address: Department of Computer Science and Engineering, </institution><institution>Koc University, </institution></institution-wrap>Istanbul, Turkey </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>19</day>
      <month>4</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>19</day>
      <month>4</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>11</volume>
    <elocation-id>8489</elocation-id>
    <history>
      <date date-type="received">
        <day>23</day>
        <month>10</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>16</day>
        <month>3</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">The nuclear protein Ki-67 and Tumor infiltrating lymphocytes (TILs) have been introduced as prognostic factors in predicting both tumor progression and probable response to chemotherapy. The value of Ki-67 index and TILs in approach to heterogeneous tumors such as Breast cancer (BC) that is the most common cancer in women worldwide, has been highlighted in literature. Considering that estimation of both factors are dependent on professional pathologists’ observation and inter-individual variations may also exist, automated methods using machine learning, specifically approaches based on deep learning, have attracted attention. Yet, deep learning methods need considerable annotated data. In the absence of publicly available benchmarks for BC Ki-67 cell detection and further annotated classification of cells, In this study we propose SHIDC-BC-Ki-67 as a dataset for the aforementioned purpose. We also introduce a novel pipeline and backend, for estimation of Ki-67 expression and simultaneous determination of intratumoral TILs score in breast cancer cells. Further, we show that despite the challenges that our proposed model has encountered, our proposed backend, PathoNet, outperforms the state of the art methods proposed to date with regard to harmonic mean measure acquired. Dataset is publicly available in <ext-link ext-link-type="uri" xlink:href="http://shiraz-hidc.com">http://shiraz-hidc.com</ext-link> and all experiment codes are published in <ext-link ext-link-type="uri" xlink:href="https://github.com/SHIDCenter/PathoNet">https://github.com/SHIDCenter/PathoNet</ext-link>.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Cancer</kwd>
      <kwd>Molecular medicine</kwd>
      <kwd>Oncology</kwd>
      <kwd>Mathematics and computing</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">The nuclear protein Ki-67 was first detected in Hodgkin lymphoma cell line and introduced as a proliferative marker<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. It was further confirmed that the monoclonal antibody Ki-67 is present during all cell cycle phases except for the G0<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. However variation in its extent of expression has been reported during different phases, as for the G1 accounting for the lowest<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. Knowing that excessive cellular proliferation correlates with progression of malignancy, precise estimation of this protein marker can benefit physicians in identifying high-grade tumors and can also convey prognostic value in approach to tumor management<sup><xref ref-type="bibr" rid="CR4">4</xref>–<xref ref-type="bibr" rid="CR6">6</xref></sup>. Also tumoral cells may suppress the body’s immune mechanisms, however tumor infiltrating lymphocytes (TILs), introduced as an immune component against tumor progression, have been found beneficial in improving outcome of breast cancer patients<sup><xref ref-type="bibr" rid="CR7">7</xref>–<xref ref-type="bibr" rid="CR10">10</xref></sup>. On the other hand, heterogeneity of breast cancer complicates approach to its treatment.Accurate quantitative determination of markers such as Ki-67 and TILs can simplify this approach to some extent. The established method for Ki-67 detection is Immunohistochemical (IHC) analysis using MIB-1 or SP6 as monoclonal antibodies used in the staining process performed on paraffin embedded tissue<sup><xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR12">12</xref></sup>. TILs scoring is also evaluated using the same tissue blocks using recommendations by the international TILs working group<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>. Considering that both markers’ scoring is based on an expert pathologist’s decision, inter-observer result variations are inevitable. To increase the accuracy of estimation, it has been suggested to count all tumor cells from different fields of a breast tissue section. If impossible to do so, at least 500–1000 cells in the representative areas of the whole section are recommended to be counted by the pathologist<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. This, in turn, can be very time consuming for large numbers of samples The mentioned limitations prompt the need for an exact continuous calculation of both markers, which could be assessed by means of Artificial intelligence (AI).
</p>
    <p id="Par3">AI has significantly improved the speed and precision of clinical diagnosis and is becoming an inseparable part of different aspects of medicine<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>. Before introducing deep learning, conventional AI algorithms were commonly used; however, designing a generalized and robust method requires field experts to extract handcrafted features. By the advent of deep neural networks, having the facility of automatically learning the best features from the input data, this issue has almost been resolved. In addition, if these algorithms are developed and trained with diverse and adequate data, they are generalizable and robust. Convolutional neural network (CNN) is a class of deep neural networks widely used in different areas such as Robotics, Bioinformatics, Computer Vision, etc. and have shown to be highly efficacious specifically, in image processing<sup><xref ref-type="bibr" rid="CR14">14</xref>–<xref ref-type="bibr" rid="CR16">16</xref></sup>.</p>
    <p id="Par4">Although Neocognitron introduced by Fukushima et al. in 1979 and later CNNs were introduced by LeCun et al. in 1989 for handwritten digit classification, CNNs failed to be much successful due to computational barriers and lack of sufficient data. In 2012, improvement of CNN’s computational capability was observed in introduction of AlexNet<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>, which won the ImageNet competition using a CNN. To overcome the shortcomings of manual assessment of Ki-67 and TILs and yet to take advantage of the probable favorable role of both markers in approach to breast cancer, in this experimental study we have designed and suggested the use of AI assisted methods with emphasis on CNN for the more accurate detection of tumoral cells along with Ki-67 and TILs. Different aspects of this study can be summarized into four categories. First, a dataset with detection and classification annotation has been introduced that provides a benchmark for Ki-67 stained cell detection, classification, proliferation index and tumor infilterating lymphocytes (TILs) estimation. Second, we suggest a novel pipeline that can achieve cell detection and classification and further examined the proposed pipeline on our benchmark. Third, we recommend a deep network, named PathoNet, that outperforms the state of the art backends with the proposed pipeline in Ki-67 immunopositive, immunonegative, and lymphocyte detection and classification. Lastly, we introduce a residual inception module that provides higher accuracy without causing vanishing gradient or overfitting issues.</p>
  </sec>
  <sec id="Sec2">
    <title>Literature review</title>
    <p id="Par5">Data regarding detection and estimation of Ki-67 by means of deep learning and conventional machine learning algorithms are present in the literature. Some conventional methods have been suggested in this regard; A study on neuroendocrine tumors (NET) presented a framework for Ki-67 assessment of NET samples that can differentiate tumoral from non-tumoral cells (such as lymphocytes) and has furthermore classified immunopositive and immunonegative tumor cells to achieve automatic Ki-67 scoring<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>. For the tumor biopsies of meningiomas and oligodendrogliomas based on immunohistochemical (IHC) Ki-67 stained images, Swiderska et al., introduced a combination of morphological methods, texture analysis, classification, and thresholding<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>. Shi et al. carried out a study based on morphological methods to address color distribution inconsistency of different cell types in the IHC Ki-67 staining of nasopharyngeal carcinoma images. They suggested classifying image pixels using local pixel correlations taken from specific color spaces<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. Geread et al. proposed a robust unsupervised method for discriminating between brown and blue colors<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>.</p>
    <p id="Par6">Despite improvements, conventional methods not only lack generalization and accuracy compared to direct interpretation by pathologists, they are also complex to develop because of having handcrafted features. As for Deep Learning methods, different aspects including image classification, cell detection, nuclei detection, and Ki-67 estimation in histopathological images have been studied and reported. Xu et al. suggested using deep learning features in multiple instance learning (MIL) framework for colon cancer classification<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>. Weidi et al. proposed a cell spatial density map using CNNs to overcome interference of cell clumping or overlapping while performing task of automated cell counting and detection<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>.</p>
    <p id="Par7">On the other hand, Cohen et al. suggested redundant counting instead of proposing a density map. Moreover, they introduced a network derived from inception networks called Count-ception for cell counting<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. Spanhol et al. compared conventional methods with deep features in breast cancer evaluation<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. In another study on breast cancer Ki-67 scoring by Saha et al., decision layers were used in detecting hotspots using gamma mixture model assisted deep learning<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. Zhang et al. used CNN to classify images as benign or malignant and a single shot multibox detector as an object detector to assess Ki-67 proliferation score in breast biopsies<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. Sornapudi et al. extracted localized features by taking advantage of superpixels generated using clustering algorithms and thereafter applied a CNN for nuclei detection on extracted features<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. Due to the restriction of manually labeled Ki-67 datasets, Jiang et al. proposed a new model consisting of residual modules and Squeeze-and-Excitation(SE) block named small SE-ResNet, which has fewer parameters in order to prevent the model from over-fitting. Similar classification accuracy was reported for SE-ResNet compared to the ResNet in classifying samples into benign and malignant<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. Liu et al. addressed cell counting problem as a regression problem by producing cell density map in a preprocessing step and further utilized a stacked deep CNN model for counting<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>.</p>
    <p id="Par8">Available datasets in form of publicly presented benchmarks can be divided into the benign-malignant classification and cell counting categories. For benign-malignant image classification, Spanhol et al. introduced BreakHis, which consists of breast cancer histopathological images obtained from partial mastectomy specimens from 82 patients with four different magnifications<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>. Diverse cell counting and nuclei detection datasets such as synthetically generated VGG-CellS<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>, real samples of human bone marrow by Kainz et al.<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>, Modified Bone Marrow (MBM) and human subcutaneous adipose tissue (ADI) datasets by Cohen et al.<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>, and Dublin Cell Counting (DCC) proposed by Marsden et al.<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> are all of the many examples of datasets presented. However, none of the mentioned benchmarks provide facilities for both cell detection and classification. To the best of our knowledge, SHIDC-B-Ki-67 is the first benchmark introducing IHC marked breast cancer specimens that has cell annotations in three different classes of immunopositive, immuno negative, and tumor infiltrating lymphocytes.</p>
  </sec>
  <sec id="Sec3">
    <title>Dataset</title>
    <p id="Par9">The critical role of providing the accurate data for developing deep learning models is evident to experts in this field. In this study the unavailability of a comprehensive Ki-67 marked dataset, lead us to gathering SHIDC-B-Ki-67 by using numerous and various data labeled by expert pathologists. This dataset contains microscopic tru-cut biopsy images of malignant breast tumors exclusively of the invasive ductal carcinoma type (Table <xref rid="Tab1" ref-type="table">1</xref>). Images were taken from biopsy specimens gathered during a clinical study from 2017 to 2020. SHIDC-B-Ki-67 contains 1656 training and 701 test data. Detailed statistics of the annotated cells have been elaborated in Table <xref rid="Tab2" ref-type="table">2</xref>. All patients who participated in this study were patients with a pathologically confirmed diagnosis of breast cancer whose breast tru-cut biopsies were taken at Shiraz University of Medical Sciences’ affiliated hospitals’ pathology Laboratories in Shiraz, Iran. Shiraz University of Medical Sciences institutional review and ethical board committee approved the study (ethics approval ID: IR.SUMS.REC.1399.756) and written informed consent was gathered from all patients willing to take part in the study. All procedures including slide preparation, staining and image acquirement’s were performed according to institutional policies and regulations. Moreover, all the data were anonymized.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Tumor characteristics of breast cancer patients enrolled for sample collection.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Pt</th><th align="left">S</th><th align="left">A (y)</th><th align="left">Tumor laterality</th><th align="left">Tumor type</th><th align="left">Tumor size (cm)</th><th align="left">Nottingham grade</th><th align="left">Node involvement</th><th align="left">TNM status</th><th align="left">Tumor stage</th><th align="left">C Ki-67 index (%)</th><th align="left">C TILs score (%)</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">F</td><td align="left">54</td><td align="left">RIGHT</td><td align="left">IDC</td><td align="left">3</td><td align="left">II/III</td><td align="left">YES</td><td align="left">T2N2M0</td><td align="left">IIIA</td><td char="." align="char">10.66</td><td char="." align="char">9.58</td></tr><tr><td align="left">2</td><td align="left">F</td><td align="left">41</td><td align="left">LEFT</td><td align="left">IDC</td><td align="left">2</td><td align="left">II/III</td><td align="left">NO</td><td align="left">T1N0M0</td><td align="left">IA</td><td char="." align="char">19.98</td><td char="." align="char">1.91</td></tr><tr><td align="left">3</td><td align="left">F</td><td align="left">55</td><td align="left">RIGHT</td><td align="left">IDC</td><td align="left">1.2</td><td align="left">I/III</td><td align="left">NO</td><td align="left">T1N0M0</td><td align="left">IA</td><td char="." align="char">11.17</td><td char="." align="char">0.53</td></tr><tr><td align="left">4</td><td align="left">F</td><td align="left">59</td><td align="left">RIGHT</td><td align="left">IDC</td><td align="left">2</td><td align="left">I/III</td><td align="left">NO</td><td align="left">T1N0M0</td><td align="left">IA</td><td char="." align="char">29.39</td><td char="." align="char">0.75</td></tr><tr><td align="left">5</td><td align="left">F</td><td align="left">57</td><td align="left">LEFT</td><td align="left">IDC</td><td align="left">3.5</td><td align="left">I/III</td><td align="left">NO</td><td align="left">T2N0M0</td><td align="left">IIA</td><td char="." align="char">28.82</td><td char="." align="char">2.52</td></tr><tr><td align="left">6</td><td align="left">F</td><td align="left">30</td><td align="left">RIGHT</td><td align="left">IDC</td><td align="left">4</td><td align="left">II/III</td><td align="left">NO</td><td align="left">T2N0M0</td><td align="left">IIA</td><td char="." align="char">75.06</td><td char="." align="char">10.98</td></tr><tr><td align="left">7</td><td align="left">F</td><td align="left">44</td><td align="left">RIGHT</td><td align="left">IDC</td><td align="left">2</td><td align="left">III/III</td><td align="left">NO</td><td align="left">T1N0M0</td><td align="left">IA</td><td char="." align="char">78.81</td><td char="." align="char">0.63</td></tr><tr><td align="left">8</td><td align="left">F</td><td align="left">29</td><td align="left">LEFT</td><td align="left">IDC</td><td align="left">2</td><td align="left">III/III</td><td align="left">NO</td><td align="left">T1N0M0</td><td align="left">IA</td><td char="." align="char">35.24</td><td char="." align="char">1.76</td></tr><tr><td align="left">9</td><td align="left">F</td><td align="left">50</td><td align="left">RIGHT</td><td align="left">IDC</td><td align="left">2.3</td><td align="left">II/III</td><td align="left">NO</td><td align="left">T2N0M0</td><td align="left">IIA</td><td char="." align="char">22.97</td><td char="." align="char">3.7</td></tr><tr><td align="left">10</td><td align="left">F</td><td align="left">29</td><td align="left">LEFT</td><td align="left">IDC</td><td align="left">2</td><td align="left">III/III</td><td align="left">YES</td><td align="left">T1N1M0</td><td align="left">IIA</td><td char="." align="char">52.51</td><td char="." align="char">5.81</td></tr><tr><td align="left">11</td><td align="left">F</td><td align="left">43</td><td align="left">RIGHT</td><td align="left">IDC</td><td align="left">3.6</td><td align="left">I/III</td><td align="left">YES</td><td align="left">T2N1M0</td><td align="left">IIB</td><td char="." align="char">56.98</td><td char="." align="char">0.71</td></tr><tr><td align="left">12</td><td align="left">F</td><td align="left">45</td><td align="left">LEFT</td><td align="left">IDC</td><td align="left">1.8</td><td align="left">I/III</td><td align="left">YES</td><td align="left">T1N2M0</td><td align="left">IIIA</td><td char="." align="char">22.67</td><td char="." align="char">0.72</td></tr><tr><td align="left">13</td><td align="left">F</td><td align="left">42</td><td align="left">LEFT</td><td align="left">IDC</td><td align="left">2</td><td align="left">III/III</td><td align="left">YES</td><td align="left">T2N2M0</td><td align="left">IIIA</td><td char="." align="char">23.14</td><td char="." align="char">0.82</td></tr><tr><td align="left">14</td><td align="left">F</td><td align="left">51</td><td align="left">RIGHT</td><td align="left">IDC</td><td align="left">2</td><td align="left">II/III</td><td align="left">NO</td><td align="left">T1N0M0</td><td align="left">IA</td><td char="." align="char">7.93</td><td char="." align="char">3.66</td></tr><tr><td align="left">15</td><td align="left">F</td><td align="left">53</td><td align="left">RIGHT</td><td align="left">IDC</td><td align="left">2.5</td><td align="left">I/III</td><td align="left">NO</td><td align="left">T2N0M0</td><td align="left">IIA</td><td char="." align="char">39.75</td><td char="." align="char">1.58</td></tr><tr><td align="left">16</td><td align="left">F</td><td align="left">33</td><td align="left">RIGHT</td><td align="left">IDC</td><td align="left">3</td><td align="left">III/III</td><td align="left">NO</td><td align="left">T2N0M0</td><td align="left">IIA</td><td char="." align="char">28.51</td><td char="." align="char">1.65</td></tr><tr><td align="left">17</td><td align="left">F</td><td align="left">46</td><td align="left">LEFT</td><td align="left">IDC</td><td align="left">1.8</td><td align="left">III/III</td><td align="left">NO</td><td align="left">T1N0M0</td><td align="left">IA</td><td char="." align="char">52.96</td><td char="." align="char">0.8</td></tr><tr><td align="left">18</td><td align="left">F</td><td align="left">28</td><td align="left">RIGHT</td><td align="left">IDC</td><td align="left">3.5</td><td align="left">II/III</td><td align="left">NO</td><td align="left">T2N0M0</td><td align="left">IIA</td><td char="." align="char">32.58</td><td char="." align="char">0.82</td></tr><tr><td align="left">19</td><td align="left">F</td><td align="left">52</td><td align="left">RIGHT</td><td align="left">IDC</td><td align="left">3.8</td><td align="left">II/III</td><td align="left">NO</td><td align="left">T2N0M0</td><td align="left">IIA</td><td char="." align="char">11.65</td><td char="." align="char">11.59</td></tr><tr><td align="left">20</td><td align="left">F</td><td align="left">40</td><td align="left">RIGHT</td><td align="left">IDC</td><td align="left">2.5</td><td align="left">III/III</td><td align="left">YES</td><td align="left">T2N1M0</td><td align="left">IIB</td><td char="." align="char">26.13</td><td char="." align="char">1.03</td></tr><tr><td align="left">21</td><td align="left">F</td><td align="left">55</td><td align="left">LEFT</td><td align="left">IDC</td><td align="left">1.2</td><td align="left">II/III</td><td align="left">NO</td><td align="left">T1N0M0</td><td align="left">IA</td><td char="." align="char">55.87</td><td char="." align="char">1.22</td></tr><tr><td align="left">22</td><td align="left">F</td><td align="left">46</td><td align="left">RIGHT</td><td align="left">IDC</td><td align="left">2</td><td align="left">I/III</td><td align="left">NO</td><td align="left">T1N0M0</td><td align="left">IA</td><td char="." align="char">62.16</td><td char="." align="char">4.5</td></tr><tr><td align="left">23</td><td align="left">F</td><td align="left">51</td><td align="left">LEFT</td><td align="left">IDC</td><td align="left">3.5</td><td align="left">III/III</td><td align="left">YES</td><td align="left">T2N3M0</td><td align="left">IIIC</td><td char="." align="char">15.02</td><td char="." align="char">2.19</td></tr></tbody></table><table-wrap-foot><p>Pt = patient ID; S = sex; A (y) = age in years; IDC = invasive ductal carcinoma; C Ki-67 index = cumulative Ki-67 index of patient calculated by the expert annotations in percent; C TILs score = cumulative TILs score of patient calculated by the expert annotations in percent.</p></table-wrap-foot></table-wrap><table-wrap id="Tab2"><label>Table 2</label><caption><p>Statistics of the annotated cells.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Cell type</th><th align="left" colspan="2">Total set (2357 IMG)</th><th align="left" colspan="2">Training set (1656 IMG)</th><th align="left" colspan="2">Test set (701 IMG)</th></tr><tr><th align="left"># Cells</th><th align="left">Avg./IMG</th><th align="left"># Cells</th><th align="left">Avg./IMG</th><th align="left"># Cells</th><th align="left">Avg./IMG</th></tr></thead><tbody><tr><td align="left">Immunopositive</td><td char="." align="char">50,861</td><td char="." align="char">21.58</td><td char="." align="char">35,106</td><td char="." align="char">21.19</td><td char="." align="char">15,755</td><td char="." align="char">22.50</td></tr><tr><td align="left">Immunonegative</td><td char="." align="char">107,647</td><td char="." align="char">45.69</td><td char="." align="char">75,008</td><td char="." align="char">45.29</td><td char="." align="char">32,639</td><td char="." align="char">46.62</td></tr><tr><td align="left">Lymphocyte</td><td char="." align="char">4490</td><td char="." align="char">1.90</td><td char="." align="char">3112</td><td char="." align="char">1.87</td><td char="." align="char">1378</td><td char="." align="char">1.96</td></tr><tr><td align="left">Total cells</td><td char="." align="char">162,998</td><td char="." align="char">23.06</td><td char="." align="char">113,226</td><td char="." align="char">22.79</td><td char="." align="char">49,772</td><td char="." align="char">23.70</td></tr></tbody></table><table-wrap-foot><p>The # cells reports the number of annotated cells. avg./IMG relate average cells per image.</p></table-wrap-foot></table-wrap></p>
    <p id="Par11">Images were taken from slides prepared from breast mass tru-cut biopsies, which were further stained for Ki-67 by IHC method. Specific monoclonal antibodies (clone SP6) were obtained from Biocare Medical, Ca, USA. The adjuvant detection kit, named Master polymer plus detection system (peroxidase), was obtained from Master Diagnostica, Granada. Dimethylbenzene (Xylene 99.5%) obtained from Samchun Chemical Co., Ltd, South Korea, Ethanol 100% from JATA Co., Iran, and 96% from Kimia Alcohol Zanjan Co., Iran, EDTA and Tris (molecular biology grade) obtained from Pars Tous Biotechnology, Iran. Phosphate buffer saline (PBS <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1{\times}$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_86912_Article_IEq1.gif"/></alternatives></inline-formula>) 0.01M, pH 7.4 was prepared. At first, paraffin-embedded blocks of breast tissue were sectioned (4–5 microns) and fixed on glass slides. Prepared slides were further immunostained. Hematoxylin was used for counterstaining to perform nuclear staining and semi-quantify the extent of immunostaining that would further be evaluated. Accordingly, the expert pathologists identified the tumoral areas in each slide, by visual analysis of tissue sections under a light microscope. The final diagnosis of each case was also approved by two experienced pathologists and confirmed by some ancillary tests such as immune staining for more markers. An Olympus BX-51 system microscope with a relay lens with a magnification of <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10{\times}$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mn>10</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_86912_Article_IEq2.gif"/></alternatives></inline-formula> coupled to OMAX microscope digital color camera A35180U3 were used to get digital images of the tumoral tissue slides. Complementary details of the camera and setup are provided in the supplementary information document. Images acquired in RGB (24-bit color depth, 8 bits per color channel) color space using <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$400\times$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mn>400</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_86912_Article_IEq3.gif"/></alternatives></inline-formula> magnification, corresponding to objective lens <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$40\times$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mn>40</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_86912_Article_IEq4.gif"/></alternatives></inline-formula>. The stepwise acquisition of images is as follows: first, the pathologist identifies the tumor and defines a region of interest (ROI). In order to cover the whole ROI, several images are captured that may be overlapping. The pathologist preferentially selects images of the tumoral area, but some of the images also include transitional parts, e.g., tumoral/non-tumoral areas. A final visual (i.e., manual) inspection discards out-of-focus images. Then, stained images were labeled by expert pathologists as Ki-67 positive tumor cells, Ki-67 negative tumor cells, and tumor cells with positive infiltrating lymphocytes. Figure <xref rid="Fig1" ref-type="fig">1</xref> depicts some samples of SHIDC-B-Ki-67 dataset.<fig id="Fig1"><label>Figure 1</label><caption><p>SHIDC-B-Ki-67 dataset samples.</p></caption><graphic xlink:href="41598_2021_86912_Fig1_HTML" id="MO1"/></fig></p>
    <p id="Par12"><italic>Labeling</italic> Precision and quality of expert labels play a crucial role in the correct learning process and methods’ accuracy. However, labeling real world data is a challenging and labor-intensive task. In SHIDC-B-Ki-67, each image contains 69 cells on average and a total of 162,998 cells. Manually labeling all cells, requires the time, effort and precision of experts that may be tiresome and error-prone for large numbers of samples. Another big challenge in the process is choosing a label type. In the segmentation task, labeling requires determining a class for each pixel; however, due to overlapping pixels in many cells and the infeasibility of annotating each pixel on histopathological images scale, this approach is not applicable in our case. In addition, annotations of detection tasks are usually a bounding box around the object of interest. Utilizing this type of annotation in our case where cells are small and abundant with different sizes, makes the network design procedure more complicated. To overcome this issue, cell center plus cell type are selected as the annotation. Figure <xref rid="Fig2" ref-type="fig">2</xref> demonstrates labels in this study.<fig id="Fig2"><label>Figure 2</label><caption><p>SHIDC-B-Ki-67 labeling process. (<bold>a</bold>) Capturing and cropping raw images, (<bold>b</bold>) specifying cell centers along with cell types by experts, (<bold>c</bold>) generating density maps from cell centers.</p></caption><graphic xlink:href="41598_2021_86912_Fig2_HTML" id="MO2"/></fig></p>
    <p id="Par13">Although this type of annotations hastens labeling procedure, it is not without limitations. Since just one pixel is picked as the center of each cell, many pixels exist without a label. This makes the data unbalanced and, in turn, a more laborious learning process is needed. Furthermore, this labeling approach is not appropriate for most of the ordinary neural network loss functions because they cannot be a suitable representative of loss in the task. To clarify this issue, we bring an example in which a network predicts the center of a cell with either 2-pixel or 200-pixel drift compared to the center pixel picked by experts. Normal loss functions cannot discriminate between these two pixels and scores them equally. Also, experts’ annotations are error-prone that may cause the same problem. This issue can be addressed by considering an uncertainty for center pixels annotated by the experts. The uncertainty is modeled as a Gaussian distribution with the labeled pixel as the center and n-pixel variance for each cell. As a result, instead of having a 3-channel pixel as the label, a density map for each class is used. Consequently, the nature of the problem is converted into a density map estimation problem.
</p>
  </sec>
  <sec id="Sec4">
    <title>Methodology</title>
    <p id="Par14">In the following sections, we explain our suggested pipeline for cell classification and detection of Ki-67 and TILs. The pipeline takes advantage of using CNN to extract features and estimate density maps from an input RGB image.</p>
    <p id="Par15"><italic>U-Net</italic> is one of the most commonly used architectures in biomedical image segmentation<sup><xref ref-type="bibr" rid="CR35">35</xref></sup> that consists of symmetric U-shaped architecture with two paths named decoder and encoder. In each layer of the decoder, an up-sampling layer increases the feature map dimension until it reaches to the input image size. U-Net is a fully convolutional model made from 19 layers. The novelty of this method is in using skip connections between corresponding encoder and decoder layers, transmitting high detail features from encoder layers to the same size layers of the decoder. This approach leads to achieving accurate location results. Similar to many studies motivated and designed based on U-Net<sup><xref ref-type="bibr" rid="CR36">36</xref>,<xref ref-type="bibr" rid="CR37">37</xref></sup>, we suggest PathoNet as a backend for the proposed pipeline based on U-Net architecture.
</p>
    <p id="Par16"><italic>Residual dilated inception module</italic> Cell detection, classification, and counting in histopathological images is a specialized and error-prone task and results may face inter-individual differences due to the nature of tissues with a variety of cell types and the high possibility of overlapping cells unless pathologists are very well experienced in the field. Nevertheless, detection of the aforementioned tumor features are crucial for the accurate diagnosis of disease and a physicians’ approach to its management. This explains the need for accuracy in developing such networks, which can mostly be done by designing a deeply structured network involving plenty of parameters, yet this mostly causes vanishing gradient issues.</p>
    <p id="Par17">On the other hand, cell size may vary from image to image, and since the same cell types usually sit together, picking a suitable kernel size is crucial. Szegedy et al.<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> proposed an inception module to provide a wider field of view without having exponential parameter growth. In the inception model, instead of building a deeper network by stacking convolutional layers, parallel convolutional layers were added to make the network wider. Therefore, by increasing the number of kernels in a layer, higher accuracy can be achieved without facing the vanishing gradient problem. Also, by utilizing different kernel sizes in one module, the problem of choosing a fixed kernel size was solved, and as a result field of view increased. In the inception module proposed by Szegedy et al., three parallel kernels with 5 × 5, 3 × 3, and 1 × 1 sizes were used before using a max-pooling layer. Also, to prevent immense computation growth, input tensor channels were decreased using 1 × 1 kernels before 5 × 5 and 3 × 3 kernels. Still, their method increases network parameters, which means the network needs more data to train and is more likely to be over-fitted. To overcome this issue, Yang et al.<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> employed dilated convolutions instead of regular convolutional layers inside the inception module. D-Dilated convolution has a D distance between each kernel element that can cover a wider region. Figure <xref rid="Fig3" ref-type="fig">3</xref> shows this operator with different dilation rates. Equation (<xref rid="Equ1" ref-type="">1</xref>) shows a K × K dilated convolution with step size D.<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} output[i] = \sum _{n=1}^K Input[i+d\cdot n]\cdot Kernel[n] \end{aligned}$$\end{document}</tex-math><mml:math id="M10" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mo>·</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>·</mml:mo><mml:mi>K</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2021_86912_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>In other words, a convolution operation is a dilated convolution with a dilation rate of one. By using dilated convolution in the inception module, Yang et al.<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> maintained the accuracy and reduced the number of model parameters. Also, instead of using multiple 1 × 1 kernels before other kernels, a 1 × 1 kernel that shares output with the next kernels was used. In the parallel convolution, outputs add together while in the inception module proposed by Szegedy et al., these outputs concat together, which consumes more memory. In a study by He et al.<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>, the authors used ResNet blocks to design deeper architectures without facing overfitting or vanishing gradient effect. In the ResNet blocks, the activation function output of layers sums up with the previous layers’ output. Motivated by the mentioned studies, in this article, we used a new inception module called residual dilated inception module (RDIM). The RDIM consists of two parallel paths where the first path has two convolution layers with kernel size 3 × 3, and the second one is built by stacking two 3 × 3 dilated convolution layers with dilation rates equal to 4. In the end, the two paths’ output sum up with the module input. However, since the number of the input channels must be the same with the output to be able to perform summation, two inception modules are used in the encoder and the decoder. In the encoder part, where the input channels are half of the output, duplicated input sums with the result of the two paths. In contrast, in the decoder, a 1 × 1 convolution with the same kernel size as the two paths applies to the input then sums with the results of parallel routes. Figure <xref rid="Fig4" ref-type="fig">4</xref> shows inception and residual dilated inception module structures.<fig id="Fig3"><label>Figure 3</label><caption><p>Dilated convolution visualization. Dilated convolution kernels presented with dilation rates (<bold>a</bold>) 1, (<bold>b</bold>) 2, and (<bold>c</bold>) 4. Increasing the dilation rate increases the kernel’s field of view.</p></caption><graphic xlink:href="41598_2021_86912_Fig3_HTML" id="MO3"/></fig><fig id="Fig4"><label>Figure 4</label><caption><p>Inception modules. Comparison of conventional and proposed inception module (<bold>a</bold>) conventional inception module, (<bold>b</bold>) residual dilated inception module (encoder path), (<bold>c</bold>) residual dilated inception module (decoder path).</p></caption><graphic xlink:href="41598_2021_86912_Fig4_HTML" id="MO4"/></fig></p>
    <p id="Par18"><italic>PathoNet</italic> PathoNet first extracts features from input images then predicts candidate pixels for Ki-67 immunopositive and immunonegative cells, and also lymphocytes with their corresponding density values. The proposed backend utilizes the U-Net-like backbone, where except for the first layer, convolutional layers are replaced by RDIM. In PathoNet, first, input passes through two convolutional layers. Then in the encoder, three and the decoder four RDIMs are used. In the end, a layer consists of three 1 × 1 convolution layers, and a linear activation function produces a three-channel output of the model. Figure <xref rid="Fig5" ref-type="fig">5</xref> demonstrates PathoNet architecture. This network results in a three-channel, 256 by 256 matrix that each channel corresponds to the density map of Ki-67 immunopositive, immunonegative, or lymphocyte class.<fig id="Fig5"><label>Figure 5</label><caption><p>PathoNet architecture.</p></caption><graphic xlink:href="41598_2021_86912_Fig5_HTML" id="MO6"/></fig></p>
    <p id="Par19"><italic>Watershed</italic> Watershed algorithm is a conventional method that is useful in medical and material science image segmentation<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>. Watershed was first introduced in 1978<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>, but over the past decades, different other versions of the method have been proposed<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>. This algorithm maps grayscale images to a topographic relief space. In a 3-dimensional relief space, each point corresponds to a pixel in the input image with height value equal to the pixel intensity. This relief space consists of different regions, namely low-lying valleys (minimums), high-altitude ridges (watershed lines), and slopes (catchment basins). These regions are demonstrated in Fig. <xref rid="Fig6" ref-type="fig">6</xref>. Watershed algorithm’s objective is to find catchment basins or watershed lines. Watershed is based on a simple yet useful theory. Lines connecting these points are called watershed lines that clarify the segment borders, and the holes are catchment basins or image segments. Algorithm 1 describes a simple Watershed algorithm.<fig id="Fig6"><label>Figure 6</label><caption><p>Watershed Algorithm maps grayscale images to a topographic relief space.</p></caption><graphic xlink:href="41598_2021_86912_Fig6_HTML" id="MO7"/></fig><graphic position="anchor" xlink:href="41598_2021_86912_Figa_HTML" id="MO600"/></p>
    <p id="Par20"><italic>Proposed pipeline</italic> The proposed pipeline consists of three components: (1) PathoNet network, (2) post-processing, and (3) Watershed algorithm. The Watershed algorithm and post-processing components do not contain trainable elements, therefore in the training phase, we train the PathoNet component. During the test phase, PathoNet generates a 3-channel density map from an input image where each channel corresponds to the density map of a class. Since there can be multiple pixels in a small region of the map with close or equal densities, choosing a single pixel as the center is rather ambiguous. Besides, due to the presence of noise and low-density points, the amount of false-positive predictions increases. Hence, a post-processing stage is added to the pipeline. Within the post-processing stage, at first, points with less than the specified threshold are removed and points more than the threshold maps to 255. Second, distance transformation is applied on each channel, producing a grayscale image in which the value of points in continuous regions shows the distance from region borders. After applying distance transformation, areas with only one maximum point are produced. For non-circular regions or where we have overlapping cells, there might be multiple maximum points. Finally, we apply the Watershed algorithm that produces cell center coordinates to segment these regions and the overlapping cells. It is notable that because Watershed finds minimum values, an inverse operation is applied before performing the Watershed method in the third step of the post-processing phase. Figure <xref rid="Fig7" ref-type="fig">7</xref> presents the proposed pipeline.<fig id="Fig7"><label>Figure 7</label><caption><p>The proposed pipeline. First, a backend, which is a density map estimator based on CNNs, predicts a density map for each class. Then thresholding applies to density maps and produces binary images. Further, inversed distance transformation scores region centers with low and borders with high values. Finally, the watershed algorithm predicts cell centers, and the pipeline outputs the cell coordinates.</p></caption><graphic xlink:href="41598_2021_86912_Fig7_HTML" id="MO8"/></fig></p>
    <p id="Par21"><italic>Experimental setup</italic> In order to obtain a balanced train and test set, 70% of each patient’s images were randomly selected for the training and 30% for the test set. The resulted splits are included in the dataset files. Then all the methods are trained on the training set, and results are reported on the test set. MSE loss function by means of the ADAM optimizer is used for training. Learning rate was empirically set to 0.0001 with a 0.1 decrease rate every ten epochs. Keras framework<sup><xref ref-type="bibr" rid="CR44">44</xref></sup> was also used to train the networks using two NVIDIA Geforce GTX 1060 and an Intel Core-i5 6400 processor.</p>
  </sec>
  <sec id="Sec5">
    <title>Results and discussion</title>
    <p id="Par22">To the best of our knowledge, most of the methods introduced for automated detection of Ki-67 have reported their results on datasets that are not publicly available, Therefore in this study, the authors not only presented a publicly available dataset, but also introduced a backend for a more precise estimation Ki-67 index. Our presented method overrules others also in generating pixel coordinates in addition to cell class types, as for other studies, only prediction of cell nuclei or Ki-67 score is reported<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR29">29</xref></sup>. Since no previously reported method exists that can be applied directly on our classification and detection benchmark, comparison of the currently presented method was made with state of the art methods as the backend in our pipeline and results were reported accordingly. DeepLabV3<sup><xref ref-type="bibr" rid="CR45">45</xref></sup> that has the best results on the VOC-PASCAL 2012<sup><xref ref-type="bibr" rid="CR46">46</xref></sup> was used. The last layer of DeepLabV3 was replaced with a 3-channel convolution layer that has a linear activation function and can have outputs similar to PathoNet. The DeepLab method has Mobilenet and Xeption implementations, which we have provided results for both. Similar to PathoNet, FCRN-A, and FCRN-B<sup><xref ref-type="bibr" rid="CR47">47</xref></sup> were designed for density estimation but at a single class. For evaluating FCRN-A and FCRN-B methods as our pipeline backend, their last layer was changed from one kernel to three kernels. Since PathoNet backbone is similar to U-Net, we trained it in the same setting as the other methods did, yet, U-Net proved to be underfitting and, therefore, could not be evaluated. Though, by following the methods presented the by Zhou et al.<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>, a modified version of U-Net with batch normalization<sup><xref ref-type="bibr" rid="CR49">49</xref></sup> layers of this study were evaluated. Evaluation results of the proposed pipeline with different backends is provided in Table <xref rid="Tab3" ref-type="table">3</xref>.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Cell detection and classification results.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Backend</th><th align="left" colspan="3">Immunopositive</th><th align="left" colspan="3">Immunonegative</th><th align="left" colspan="3">Lymphocyte</th><th align="left" colspan="3">Average</th></tr><tr><th align="left">Prec.</th><th align="left">Rec.</th><th align="left">F1</th><th align="left">Prec.</th><th align="left">Rec.</th><th align="left">F1</th><th align="left">Prec.</th><th align="left">Rec.</th><th align="left">F1</th><th align="left">Prec.</th><th align="left">Rec.</th><th align="left">F1</th></tr></thead><tbody><tr><td align="left">Modified DeepLabv3-Mobilenetv2<sup><xref ref-type="bibr" rid="CR45">45</xref></sup></td><td align="left">0.8398</td><td align="left">0.8252</td><td align="left">0.8324</td><td align="left">0.7201</td><td align="left">0.7468</td><td align="left">0.7332</td><td align="left">0.1170</td><td align="left">0.3471</td><td align="left">0.2344</td><td align="left">0.7430</td><td align="left">0.7606</td><td align="left">0.7508</td></tr><tr><td align="left">Modified DeepLabv3-Xeption<sup><xref ref-type="bibr" rid="CR45">45</xref></sup></td><td align="left">0.8366</td><td align="left">0.8659</td><td align="left">0.8510</td><td align="left">0.7168</td><td align="left"><bold>0.8330</bold></td><td align="left">0.7705</td><td align="left">0.4227</td><td align="left"><bold>0.4717</bold></td><td align="left"><bold>0.4458</bold></td><td align="left">0.7467</td><td align="left"><bold>0.8334</bold></td><td align="left">0.7871</td></tr><tr><td align="left">Modified FCRN-A<sup><xref ref-type="bibr" rid="CR47">47</xref></sup></td><td align="left">0.8287</td><td align="left">0.8556</td><td align="left">0.8419</td><td align="left">0.7190</td><td align="left">0.7887</td><td align="left">0.7523</td><td align="left">0.3950</td><td align="left">0.4101</td><td align="left">0.4028</td><td align="left">0.7449</td><td align="left">0.7994</td><td align="left">0.7710</td></tr><tr><td align="left">Modified FCRN-B<sup><xref ref-type="bibr" rid="CR47">47</xref></sup></td><td align="left">0.8270</td><td align="left"><bold>0.8726</bold></td><td align="left">0.8492</td><td align="left">0.7367</td><td align="left">0.7908</td><td align="left">0.7628</td><td align="left"><bold>0.4253</bold></td><td align="left">0.7376</td><td align="left">0.4314</td><td align="left">0.7568</td><td align="left">0.8069</td><td align="left">0.7810</td></tr><tr><td align="left">Modified U-Net<sup><xref ref-type="bibr" rid="CR35">35</xref></sup></td><td align="left">0.8415</td><td align="left">0.8436</td><td align="left">0.8426</td><td align="left">0.7351</td><td align="left">0.7899</td><td align="left">0.7615</td><td align="left">0.4146</td><td align="left">0.4630</td><td align="left">0.4375</td><td align="left">0.7600</td><td align="left">0.7972</td><td align="left">0.7783</td></tr><tr><td align="left">Proposed method</td><td align="left"><bold>0.8436</bold></td><td align="left">0.8611</td><td align="left"><bold>0.8523</bold></td><td align="left"><bold>0.7466</bold></td><td align="left">0.8198</td><td align="left"><bold>0.7815</bold></td><td align="left">0.3424</td><td align="left">0.4246</td><td align="left">0.379</td><td align="left"><bold>0.7766</bold></td><td align="left">0.8219</td><td align="left"><bold>0.7928</bold></td></tr></tbody></table><table-wrap-foot><p>Measurements on the test set in terms of precision (Prec.), recall (Rec.), and F1-score (F1.) reported based on different backends. Best results in each column have been shown in bold font.</p></table-wrap-foot></table-wrap></p>
    <p id="Par23"><italic>Measurements</italic> To evaluate our pipeline, first, we need to define true and false predictions. We count an estimation as True positive (TP) when the predicted center has a less than R pixel distance with the corresponding ground truth; otherwise, it is marked as a False positive(FP). If more than one detected center is within an R-pixel distance with the same cell type in the ground truth, estimation with lower distance counts as TP and otherwise as FP. Finally, cells are defined in the ground truth, but without any prediction for False Negatives (FN). With the given definitions, the precision and recall formulas are shown in Eq. (<xref rid="Equ2" ref-type="">2</xref>).<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} Precision = \frac{TP}{TP+FP} \quad Recall = \frac{TP}{TP+FN} \end{aligned}$$\end{document}</tex-math><mml:math id="M12" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mspace width="1em"/><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2021_86912_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>A model with high recall and low precision rates detects most of the pixels as cell centers, while most of them are FP. On the other hand, low recall and high precision rates in a model bring about the detection of few cells, while most detected cells are TP. None of the mentioned cases is useful; therefore, our goal is to develop a model that holds a trade-off between precision and recall. F1 score or harmonic mean is an appropriate measure that can be used for this evaluation that can be calculated using Eq. (<xref rid="Equ3" ref-type="">3</xref>).<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} F1-score = 2 \cdot \frac{Precision \cdot Recall}{ Precision + Recall} \end{aligned}$$\end{document}</tex-math><mml:math id="M14" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>·</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>·</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2021_86912_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>Considering the importance of precise estimation of both markers, we also evaluated the introduced pipeline with using different backends and the performance of the proposed model in terms of TILs and Ki-67 index calculation was evaluated using RMSE.<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned}&amp;Ki-67-score=\frac{Immunopositive}{Immunopositive+Immunonegative} \end{aligned}$$\end{document}</tex-math><mml:math id="M16" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mi>K</mml:mi><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>67</mml:mn><mml:mo>-</mml:mo><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">Immunopositive</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2021_86912_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned}&amp;TIL-score = \frac{Lymphocyte}{Lymphocyte + Immunopositive+Immunonegative} \end{aligned}$$\end{document}</tex-math><mml:math id="M18" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mi>T</mml:mi><mml:mi>I</mml:mi><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">Lymphocyte</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>y</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2021_86912_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>Also, since each raw image is cropped into smaller ones, we grouped all images that belonged to a patient into one group and classified the Ki-67 ane TILs estimation into different cut-off categories that have been presented in previous studies (Fig. <xref rid="Fig8" ref-type="fig">8</xref>). This aggregated-image classification results are presented under TILs and Ki-67 cut-off accuracy column of Table <xref rid="Tab4" ref-type="table">4</xref>. As suggested by Saha et al.<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>, cases with Ki-67 scores below 16 percent are accounted as less proliferative, between 16 and 30 as having average proliferation rate, and higher than 30 as highly proliferative. Also, based on TILs score, the cut-off ranges presented in literature is between 0 and 10, between 11 and 39, and higher than 40%<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>. Table <xref rid="Tab4" ref-type="table">4</xref> compares RMSE and accuracy of different backends based on the proposed pipeline for Ki-67 and TILs scoring. <table-wrap id="Tab4"><label>Table 4</label><caption><p>RMSE and aggregated cut-off accuracy.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Backend</th><th align="left">Ki-67 index (RMSE)</th><th align="left">TILs score (RMSE)</th><th align="left">Ki-67 cut-off accuracy</th><th align="left">TILs cut-off accuracy</th></tr></thead><tbody><tr><td align="left">Modified DeepLabv3-Mobilenetv2<sup><xref ref-type="bibr" rid="CR45">45</xref></sup></td><td align="left">0.05123</td><td align="left">0.05568</td><td align="left">0.9565</td><td align="left">0.8260</td></tr><tr><td align="left">Modified DeepLabv3-Xeption<sup><xref ref-type="bibr" rid="CR45">45</xref></sup></td><td align="left">0.06341</td><td align="left">0.01639</td><td align="left">0.9130</td><td align="left">0.9565</td></tr><tr><td align="left">Modified FCRN-A<sup><xref ref-type="bibr" rid="CR47">47</xref></sup></td><td align="left">0.05836</td><td align="left">0.01580</td><td align="left">0.9565</td><td align="left">1.0</td></tr><tr><td align="left">Modified FCRN-B<sup><xref ref-type="bibr" rid="CR47">47</xref></sup></td><td align="left">0.06285</td><td align="left">0.01674</td><td align="left">0.9565</td><td align="left">1.0</td></tr><tr><td align="left">Modified U-Net<sup><xref ref-type="bibr" rid="CR35">35</xref></sup></td><td align="left">0.05206</td><td align="left">0.01497</td><td align="left">0.9556</td><td align="left">0.9565</td></tr><tr><td align="left">Ours(PathoNet)</td><td align="left">0.04803</td><td align="left">0.02530</td><td align="left">0.9565</td><td align="left">0.9565</td></tr></tbody></table><table-wrap-foot><p>Here, Ki-67 index and TILs score estimation were evaluated in terms of root mean squared error (RMSE) and classification accuracy of cut-offs presented for Ki-67 and TILs using the proposed pipeline on the test set.</p></table-wrap-foot></table-wrap><fig id="Fig8"><label>Figure 8</label><caption><p>From whole mount slide image preparation to Ki-67 and TILs estimation. The process starts by taking pictures (<bold>b</bold>) from a patient’s tumor section (<bold>a</bold>) and cropping them into 256 × 256 pixels sub-images. Next, each cropped image (<bold>c</bold>) is fed to the proposed pipeline (<bold>d</bold>), resulting in predicted cell centers, corresponding class labels (<bold>e</bold>). Then, the Ki-67 index and TILs score can easily be calculated. Figure <xref rid="Fig10" ref-type="fig">10</xref> depicts a prediction sample image with <inline-formula id="IEq5"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$4912\times 3684$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:mn>4912</mml:mn><mml:mo>×</mml:mo><mml:mn>3684</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_86912_Article_IEq5.gif"/></alternatives></inline-formula> pixels size.</p></caption><graphic xlink:href="41598_2021_86912_Fig8_HTML" id="MO13"/></fig></p>
    <p id="Par24"><italic>Quantitative results</italic> As shown in Table <xref rid="Tab3" ref-type="table">3</xref>, DeepLabv3-Xeption performed better in Ki-67 immunopositive cell detection. However, our introduced model outperforms the others in the detection of Ki-67 immunonegative cells in terms of precision and harmonic mean (F1 score). The suggested pipeline using FCRN-B has better precision and harmonic mean in the lymphocyte class of cells. In contrast, our model has a better recall rate, meaning that the pipeline using PathoNet could detect more labeled lymphocytes than the other methods. The proposed backend performed better in terms of precision by having the lowest FP and outperforming the others in terms of overall F1. Although DeepLabv3-Xception is very close to ours in the overall F1, as shown in Table <xref rid="Tab5" ref-type="table">5</xref>, it has 12 times more parameters compared to the proposed method. So, not only DeepLabv3-Xception needs more computation resources for training, but also, the proposed method can be processed faster and provides higher FPS while maintaining a better F1 score than DeepLabv3-Xception.
<table-wrap id="Tab5"><label>Table 5</label><caption><p>Inference speed and model parameters.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Backend</th><th align="left"># Parameters</th><th align="left">FPS</th><th align="left">Avg. F1</th></tr></thead><tbody><tr><td align="left">Modified DeepLabv3-Mobilenetv2<sup><xref ref-type="bibr" rid="CR45">45</xref></sup></td><td char="." align="char">3,236,907</td><td char="." align="char">20.52</td><td align="left">0.7508</td></tr><tr><td align="left">Modified DeepLabv3-Xeption<sup><xref ref-type="bibr" rid="CR45">45</xref></sup></td><td char="." align="char">41,253,587</td><td char="." align="char">8.76</td><td align="left">0.7871</td></tr><tr><td align="left">Modified FCRN-A<sup><xref ref-type="bibr" rid="CR47">47</xref></sup></td><td char="." align="char">2,142,019</td><td char="." align="char">22.03</td><td align="left">0.7710</td></tr><tr><td align="left">Modified FCRN-B<sup><xref ref-type="bibr" rid="CR47">47</xref></sup></td><td char="." align="char">1,365,888</td><td char="." align="char">14.6</td><td align="left">0.7810</td></tr><tr><td align="left">Modified U-Net<sup><xref ref-type="bibr" rid="CR35">35</xref></sup></td><td char="." align="char">31,036,323</td><td char="." align="char">12.34</td><td align="left">0.7783</td></tr><tr><td align="left">Ours (PathoNet)</td><td char="." align="char">3,142,208</td><td char="." align="char">14.86</td><td align="left">0.7928</td></tr></tbody></table><table-wrap-foot><p>Run-time in terms of frame per seconds and models’ parameter counts of different backends used in the proposed pipeline.</p></table-wrap-foot></table-wrap></p>
    <p id="Par25"><italic>Qualitative results</italic> We elaborated the qualitative results on the SHIDC-BC-Ki-67 test set in Fig. <xref rid="Fig9" ref-type="fig">9</xref>. We have compared our proposed pipeline’s detection and classification results with different backends in addition to the ground truth and the input images that have been presented. The proposed pipeline leads to the detection of highly overlapped cells with different colors, sizes, and lighting conditions as shown in Fig. <xref rid="Fig9" ref-type="fig">9</xref>. Compared to the required clinical ROI, <inline-formula id="IEq6"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$255 \times 255$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:mn>255</mml:mn><mml:mo>×</mml:mo><mml:mn>255</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_86912_Article_IEq6.gif"/></alternatives></inline-formula> images are small. Considering the quantitative aggregated scores of patients presented in Table <xref rid="Tab4" ref-type="table">4</xref>, by following the process in Fig. <xref rid="Fig8" ref-type="fig">8</xref>, qualitative results of aggregated sub-images of a patient is presented in Fig. <xref rid="Fig10" ref-type="fig">10</xref>.<fig id="Fig9"><label>Figure 9</label><caption><p>Qualitative results. Samples from SHIDC-B-Ki-67 test set. Points in red show negative Ki-67 stained tumor cells. Blue points depict positive Ki-67 stained tumor cells and dots with cyan color, show tumor infiltrating lymphocytes. In this image, ground truth column is expert annotation.</p></caption><graphic xlink:href="41598_2021_86912_Fig9_HTML" id="MO14"/></fig><fig id="Fig10"><label>Figure 10</label><caption><p>Aggregated sub-image qualitative results. Input raw image is cropped into smaller sub-images and, after prediction, aggregates to build the input image. This sample is visualized from the test set.</p></caption><graphic xlink:href="41598_2021_86912_Fig10_HTML" id="MO15"/></fig></p>
    <p id="Par26"><italic>Limitations</italic> Variations of tumoral cell size and color from patient to patient complicates labeling and annotation of cells. Also using only Ki-67 marked images for TILs scoring leads to masking of Ki-67 positive TILs. This issue could be resolved by dual IHC staining which was unavailable at our center. However, we believe that the powerful deep model feature extractors presented, have shown promising results in overcoming this issue. We should mention that the recommended staining for evaluation of TILs is Hematoxylin and Eosin (H&amp;E), but since during IHC staining, a counter staining process is performed using Hematoxylin, we expected the lymphocytes’ nuclei be stained sufficiently as has been recommended by guidelines.</p>
  </sec>
  <sec id="Sec6">
    <title>Conclusion</title>
    <p id="Par28">In this article we introduced a new benchmark for cell detection, classification , proliferation index estimation, and TILs scoring using histopathological images. We further proposed a new pipeline to be used in cell detection and classification that can utilize different deep models for feature extraction. We evaluated this pipeline on immunonegative, immunopositive, and TILs cell detection on Ki-67 stained images. Also, we suggested a residual inception and designed a new light-weight backbone called PathoNet that achieved state of the art results on the proposed dataset. The suggested pipeline showed high accuracy in Ki-67 and TILs cut-off classification compared with all backends. Finally, we showed that, designing PathoNet using RDIM provides high accuracy while slightly increasing model parameters.
</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Information</title>
    <sec id="Sec7">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="41598_2021_86912_MOESM1_ESM.pdf">
            <caption>
              <p>Supplementary Informations.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary Information</title>
    <p>The online version contains supplementary material available at 10.1038/s41598-021-86912-w.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>We would like to acknowledge Maryam Amin Safae for her efforts and support.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>B.P.J. and A.D. prepared the slides and determined hotspots. F.N. and R.S. performed machine learning experiments plus slide imaging. A.D. and B.P.J. supervised and verified image gathering. B.P.J., M.K.S., F.M. and S.M carried out labeling and A.D. and D.F. further validated labels. A.D., F.N., R.S. and D.F wrote the manuscript and all authors reviewed the manuscript. A.D., F.N., R.S. and D.F. contributed to the major revision of the manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>All analysis results and ablations withing this study are included is this article. In addition, dataset is publicly available and can be accessed from <ext-link ext-link-type="uri" xlink:href="http://www.shiraz-hidc.com">http://www.shiraz-hidc.com</ext-link>.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Code availability</title>
    <p>Codes and scripts to replicate the results is provided on the related repository <ext-link ext-link-type="uri" xlink:href="https://github.com/SHIDCenter/PathoNet">https://github.com/SHIDCenter/PathoNet</ext-link>.</p>
  </notes>
  <notes id="FPar1" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par32">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gerdes</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Schwab</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Lemke</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Stein</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Production of a mouse monoclonal antibody reactive with a human nuclear antigen associated with cell proliferation</article-title>
        <source>Int. J. Cancer</source>
        <year>1983</year>
        <volume>31</volume>
        <fpage>13</fpage>
        <lpage>20</lpage>
        <pub-id pub-id-type="doi">10.1002/ijc.2910310104</pub-id>
        <pub-id pub-id-type="pmid">6339421</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gerdes</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Cell cycle analysis of a cell proliferation-associated human nuclear antigen defined by the monoclonal antibody ki-67</article-title>
        <source>J. Immunol.</source>
        <year>1984</year>
        <volume>133</volume>
        <fpage>1710</fpage>
        <lpage>1715</lpage>
        <?supplied-pmid 6206131?>
        <pub-id pub-id-type="pmid">6206131</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lopez</surname>
            <given-names>F</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Modalities of synthesis of ki67 antigen during the stimulation of lymphocytes</article-title>
        <source>Cytom. J. Int. Soc. Anal. Cytol.</source>
        <year>1991</year>
        <volume>12</volume>
        <fpage>42</fpage>
        <lpage>49</lpage>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dowsett</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Dunbier</surname>
            <given-names>AK</given-names>
          </name>
        </person-group>
        <article-title>Emerging biomarkers and new understanding of traditional markers in personalized therapy for breast cancer</article-title>
        <source>Clin. Cancer Res.</source>
        <year>2008</year>
        <volume>14</volume>
        <fpage>8019</fpage>
        <lpage>8026</lpage>
        <pub-id pub-id-type="doi">10.1158/1078-0432.CCR-08-0974</pub-id>
        <pub-id pub-id-type="pmid">19088018</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jones</surname>
            <given-names>RL</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The prognostic significance of ki67 before and after neoadjuvant chemotherapy in breast cancer</article-title>
        <source>Breast Cancer Res. Treat.</source>
        <year>2009</year>
        <volume>116</volume>
        <fpage>53</fpage>
        <lpage>68</lpage>
        <pub-id pub-id-type="doi">10.1007/s10549-008-0081-7</pub-id>
        <pub-id pub-id-type="pmid">18592370</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Taneja</surname>
            <given-names>P</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Classical and novel prognostic markers for breast cancer and their clinical significance</article-title>
        <source>Clin. Med. Insights Oncol.</source>
        <year>2010</year>
        <volume>4</volume>
        <fpage>CMO-S4773</fpage>
        <pub-id pub-id-type="doi">10.4137/CMO.S4773</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Denkert</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Tumor-associated lymphocytes as an independent predictor of response to neoadjuvant chemotherapy in breast cancer</article-title>
        <source>J. Clin. Oncol.</source>
        <year>2010</year>
        <volume>28</volume>
        <fpage>105</fpage>
        <lpage>113</lpage>
        <pub-id pub-id-type="doi">10.1200/JCO.2009.23.7370</pub-id>
        <pub-id pub-id-type="pmid">19917869</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Denkert</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Tumour-infiltrating lymphocytes and prognosis in different subtypes of breast cancer: a pooled analysis of 3771 patients treated with neoadjuvant therapy</article-title>
        <source>Lancet Oncol.</source>
        <year>2018</year>
        <volume>19</volume>
        <fpage>40</fpage>
        <lpage>50</lpage>
        <pub-id pub-id-type="doi">10.1016/S1470-2045(17)30904-X</pub-id>
        <pub-id pub-id-type="pmid">29233559</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mao</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The value of tumor infiltrating lymphocytes (TILs) for predicting response to neoadjuvant chemotherapy in breast cancer: a systematic review and meta-analysis</article-title>
        <source>PLoS ONE</source>
        <year>2014</year>
        <volume>9</volume>
        <fpage>e115103</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0115103</pub-id>
        <pub-id pub-id-type="pmid">25501357</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mao</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The prognostic value of tumor-infiltrating lymphocytes in breast cancer: a systematic review and meta-analysis</article-title>
        <source>PLoS ONE</source>
        <year>2016</year>
        <volume>11</volume>
        <fpage>e0152500</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0152500</pub-id>
        <pub-id pub-id-type="pmid">27073890</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Urruticoechea</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>IE</given-names>
          </name>
          <name>
            <surname>Dowsett</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Proliferation marker ki-67 in early breast cancer</article-title>
        <source>J. Clin. Oncol.</source>
        <year>2005</year>
        <volume>23</volume>
        <fpage>7212</fpage>
        <lpage>7220</lpage>
        <pub-id pub-id-type="doi">10.1200/JCO.2005.07.501</pub-id>
        <pub-id pub-id-type="pmid">16192605</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dowsett</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Assessment of ki67 in breast cancer: recommendations from the international ki67 in breast cancer working group</article-title>
        <source>J. Natl. Cancer Inst.</source>
        <year>2011</year>
        <volume>103</volume>
        <fpage>1656</fpage>
        <lpage>1664</lpage>
        <pub-id pub-id-type="doi">10.1093/jnci/djr393</pub-id>
        <pub-id pub-id-type="pmid">21960707</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kononenko</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Bratko</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Kukar</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Application of machine learning to medical diagnosis</article-title>
        <source>Mach. Learn. Data Min. Methods Appl.</source>
        <year>1997</year>
        <volume>389</volume>
        <fpage>408</fpage>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Soans, N., Asali, E., Hong, Y. &amp; Doshi, P. Sa-Net: robust state-action recognition for learning from observations. In <italic>IEEE International Conference on Robotics and Automation (ICRA)</italic>, 2153–2159 (2020).</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Haskins</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Kruger</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Deep learning in medical image registration: a survey</article-title>
        <source>Mach. Vis. Appl.</source>
        <year>2020</year>
        <volume>31</volume>
        <fpage>8</fpage>
        <pub-id pub-id-type="doi">10.1007/s00138-020-01060-x</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Hafiz, A. M. &amp; Bhat, G. M. A survey of deep learning techniques for medical diagnosis. In Tuba, M., Akashe, S. &amp; Joshi, A. (eds) <italic>Information and Communication Technology for Sustainable Development</italic>, 161–170 (Springer, 2020).</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <mixed-citation publication-type="other">Krizhevsky, A., Sutskever, I. &amp; Hinton, G. E. Imagenet classification with deep convolutional neural networks. <italic>Advances in Neural Information Processing Systems</italic>, 1097–1105 (2012).</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xing</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Su</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Neltner</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Automatic ki-67 counting using robust cell detection and online dictionary learning</article-title>
        <source>IEEE Trans. Biomed. Eng.</source>
        <year>2013</year>
        <volume>61</volume>
        <fpage>859</fpage>
        <lpage>870</lpage>
        <pub-id pub-id-type="doi">10.1109/TBME.2013.2291703</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Swiderska, Z., Markiewicz, T., Grala, B. &amp; Slodkowska, J. Hot-spot selection and evaluation methods for whole slice images of meningiomas and oligodendrogliomas. In <italic>2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</italic>, 6252–6256 (IEEE, 2015).</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shi</surname>
            <given-names>P</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Automated ki-67 quantification of immunohistochemical staining image of human nasopharyngeal carcinoma xenografts</article-title>
        <source>Sci. Rep.</source>
        <year>2016</year>
        <volume>6</volume>
        <fpage>32127</fpage>
        <pub-id pub-id-type="doi">10.1038/srep32127</pub-id>
        <pub-id pub-id-type="pmid">27562647</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Geread</surname>
            <given-names>RS</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Ihc colour histograms for unsupervised ki67 proliferation index calculation</article-title>
        <source>Front. Bioeng. Biotechnol.</source>
        <year>2019</year>
        <volume>7</volume>
        <fpage>226</fpage>
        <pub-id pub-id-type="doi">10.3389/fbioe.2019.00226</pub-id>
        <pub-id pub-id-type="pmid">31632956</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Xu, Y. <italic>et al.</italic> Deep learning of feature representation with multiple instance learning for medical image analysis. In <italic>2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</italic>, 1626–1630 (IEEE, 2014).</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Weidi, X., Noble, J. A. &amp; Zisserman, A. Microscopy cell counting with fully convolutional regression networks. In <italic>1st Deep Learning Workshop, Medical Image Computing and Computer-Assisted Intervention (MICCAI)</italic> (2015).</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Paul Cohen, J., Boucher, G., Glastonbury, C. A., Lo, H. Z. &amp; Bengio, Y. Count-ception: counting by fully convolutional redundant counting. In <italic>Proceedings of the IEEE International Conference on Computer Vision</italic>, 18–26 (2017).</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Spanhol, F. A., Oliveira, L. S., Cavalin, P. R., Petitjean, C. &amp; Heutte, L. Deep features for breast cancer histopathological image classification. In <italic>2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)</italic>, 1868–1873 (IEEE, 2017).</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Saha</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Chakraborty</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Arun</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Ahmed</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Chatterjee</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>An advanced deep learning approach for ki-67 stained hotspot detection and proliferation rate scoring for prognostic evaluation of breast cancer</article-title>
        <source>Sci. Rep.</source>
        <year>2017</year>
        <volume>7</volume>
        <fpage>3213</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-017-03405-5</pub-id>
        <pub-id pub-id-type="pmid">28607456</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Tumor cell identification in ki-67 images on deep learning</article-title>
        <source>Mol. Cell. Biomech.</source>
        <year>2018</year>
        <volume>15</volume>
        <fpage>177</fpage>
        <lpage>187</lpage>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sornapudi</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep learning nuclei detection in digitized histology images by superpixels</article-title>
        <source>J. Pathol. Inform.</source>
        <year>2018</year>
        <volume>9</volume>
        <fpage>5</fpage>
        <pub-id pub-id-type="doi">10.4103/jpi.jpi_74_17</pub-id>
        <pub-id pub-id-type="pmid">29619277</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jiang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Breast cancer histopathological image classification using convolutional neural networks with small SE-ResNet module</article-title>
        <source>PLoS ONE</source>
        <year>2019</year>
        <volume>14</volume>
        <fpage>e0214587</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0214587</pub-id>
        <pub-id pub-id-type="pmid">30925170</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">Liu, Q., Junker, A., Murakami, K. &amp; Hu, P. A novel convolutional regression network for cell counting. In <italic>2019 IEEE 7th International Conference on Bioinformatics and Computational Biology (ICBCB)</italic>, 44–49 (IEEE, 2019).</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Spanhol, F. A., Oliveira, L. S., Petitjean, C. &amp; Heutte, L. Breast cancer histopathological image classification using convolutional neural networks. In <italic>2016 International Joint Conference on Neural Networks (IJCNN)</italic>, 2560–2567 (IEEE, 2016).</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Lempitsky, V. &amp; Zisserman, A. Learning to count objects in images. In <italic>Advances in Neural Information Processing Systems</italic>, 23, 1324–1332 (2010).</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">Kainz, P., Urschler, M., Schulter, S., Wohlhart, P. &amp; Lepetit, V. You should use regression to detect cells. In <italic>International Conference on Medical Image Computing and Computer-Assisted Intervention</italic>, 276–283 (Springer, 2015).</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Marsden, M., McGuinness, K., Little, S., Keogh, C. E. &amp; O’Connor, N. E. People, penguins and petri dishes: adapting object counting models to new visual domains and object types without forgetting. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic>, 8070–8079 (2018).</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Ronneberger, O., Fischer, P. &amp; Brox, T. U-net: convolutional networks for biomedical image segmentation. In <italic>International Conference on Medical Image Computing and Computer-assisted Intervention</italic>, 234–241 (Springer, 2015).</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Myronenko, A. 3D MRI brain tumor segmentation using autoencoder regularization. In <italic>International MICCAI Brainlesion Workshop</italic>, 311–320 (Springer, 2018).</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Dolz, J., Desrosiers, C. &amp; Ayed, I. B. IVD-Net: Intervertebral disc localization and segmentation in MRI with a multi-modal UNet. In <italic>International Workshop and Challenge on Computational Methods and Clinical Applications for Spine Imaging</italic>, 130–143 (Springer, 2018).</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Szegedy, C. <italic>et al.</italic> Going deeper with convolutions. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic>, 1–9 (2015).</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Yang, S., Lin, G., Jiang, Q. &amp; Lin, W. A dilated inception network for visual saliency prediction (2019). arXiv:1904.03571.</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">He, K., Zhang, X., Ren, S. &amp; Sun, J. Deep residual learning for image recognition. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic>, 770–778 (2016).</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Atta-Fosu</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>3D clumped cell segmentation using curvature based seeded watershed</article-title>
        <source>J. Imaging</source>
        <year>2016</year>
        <volume>2</volume>
        <fpage>31</fpage>
        <pub-id pub-id-type="doi">10.3390/jimaging2040031</pub-id>
        <pub-id pub-id-type="pmid">28280723</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <mixed-citation publication-type="other">Lantuéjoul, C. La squelettisation et son application aux mesures topologiques des mosaiques polycristallines (1978).</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kornilov</surname>
            <given-names>AS</given-names>
          </name>
          <name>
            <surname>Safonov</surname>
            <given-names>IV</given-names>
          </name>
        </person-group>
        <article-title>An overview of watershed algorithm implementations in open source libraries</article-title>
        <source>J. Imaging</source>
        <year>2018</year>
        <volume>4</volume>
        <fpage>123</fpage>
        <pub-id pub-id-type="doi">10.3390/jimaging4100123</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <mixed-citation publication-type="other">Chollet, F. <italic>et al.</italic> Keras. <ext-link ext-link-type="uri" xlink:href="https://keras.io/">https://keras.io/</ext-link>. Accessed 30 May 2019 (2019).</mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <mixed-citation publication-type="other">Chen, L.-C., Papandreou, G., Schroff, F. &amp; Adam, H. Rethinking atrous convolution for semantic image segmentation. <italic>arXiv preprint</italic><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1706.05587">arXiv:1706.05587</ext-link> (2017).</mixed-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Everingham</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Van Gool</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Williams</surname>
            <given-names>CKI</given-names>
          </name>
          <name>
            <surname>Winn</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zisserman</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>The pascal visual object classes (VOC) challenge</article-title>
        <source>Int. J. Comput. Vis.</source>
        <year>2010</year>
        <volume>88</volume>
        <fpage>303</fpage>
        <lpage>338</lpage>
        <pub-id pub-id-type="doi">10.1007/s11263-009-0275-4</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xie</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Noble</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Zisserman</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Microscopy cell counting and detection with fully convolutional regression networks</article-title>
        <source>Comput. Methods Biomech. Biomed. Eng. Imaging Vis.</source>
        <year>2018</year>
        <volume>6</volume>
        <fpage>283</fpage>
        <lpage>292</lpage>
        <pub-id pub-id-type="doi">10.1080/21681163.2016.1149104</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Ruan</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Canu</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>A review: Deep learning for medical image segmentation using multi-modality fusion</article-title>
        <source>Array</source>
        <year>2019</year>
        <volume>3–4</volume>
        <fpage>100004</fpage>
        <pub-id pub-id-type="doi">10.1016/j.array.2019.100004</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <mixed-citation publication-type="other">Ioffe, S. &amp; Szegedy, C. Batch normalization: accelerating deep network training by reducing internal covariate shift. <italic>arXiv preprint</italic><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1502.03167">arXiv:1502.03167</ext-link> (2015).</mixed-citation>
    </ref>
  </ref-list>
</back>
