<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-id journal-id-type="pmc">plosone</journal-id>
    <journal-title-group>
      <journal-title>PLoS ONE</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1932-6203</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6504038</article-id>
    <article-id pub-id-type="publisher-id">PONE-D-19-07217</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pone.0216456</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Physiology</subject>
          <subj-group>
            <subject>Physiological Processes</subject>
            <subj-group>
              <subject>Sleep</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Physiology</subject>
          <subj-group>
            <subject>Physiological Processes</subject>
            <subj-group>
              <subject>Sleep</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Bioassays and Physiological Analysis</subject>
          <subj-group>
            <subject>Electrophysiological Techniques</subject>
            <subj-group>
              <subject>Brain Electrophysiology</subject>
              <subj-group>
                <subject>Electroencephalography</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Physiology</subject>
          <subj-group>
            <subject>Electrophysiology</subject>
            <subj-group>
              <subject>Neurophysiology</subject>
              <subj-group>
                <subject>Brain Electrophysiology</subject>
                <subj-group>
                  <subject>Electroencephalography</subject>
                </subj-group>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Physiology</subject>
          <subj-group>
            <subject>Electrophysiology</subject>
            <subj-group>
              <subject>Neurophysiology</subject>
              <subj-group>
                <subject>Brain Electrophysiology</subject>
                <subj-group>
                  <subject>Electroencephalography</subject>
                </subj-group>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Neurophysiology</subject>
            <subj-group>
              <subject>Brain Electrophysiology</subject>
              <subj-group>
                <subject>Electroencephalography</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Brain Mapping</subject>
            <subj-group>
              <subject>Electroencephalography</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Clinical Medicine</subject>
          <subj-group>
            <subject>Clinical Neurophysiology</subject>
            <subj-group>
              <subject>Electroencephalography</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Imaging Techniques</subject>
          <subj-group>
            <subject>Neuroimaging</subject>
            <subj-group>
              <subject>Electroencephalography</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Neuroimaging</subject>
            <subj-group>
              <subject>Electroencephalography</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Neurology</subject>
          <subj-group>
            <subject>Sleep Disorders</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
            <subj-group>
              <subject>Deep Learning</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Neural Networks</subject>
          <subj-group>
            <subject>Recurrent Neural Networks</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Neural Networks</subject>
            <subj-group>
              <subject>Recurrent Neural Networks</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Neural Networks</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Neural Networks</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Bioassays and Physiological Analysis</subject>
          <subj-group>
            <subject>Electrophysiological Techniques</subject>
            <subj-group>
              <subject>Muscle Electrophysiology</subject>
              <subj-group>
                <subject>Electromyography</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Applied Mathematics</subject>
            <subj-group>
              <subject>Algorithms</subject>
              <subj-group>
                <subject>Machine Learning Algorithms</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Simulation and Modeling</subject>
          <subj-group>
            <subject>Algorithms</subject>
            <subj-group>
              <subject>Machine Learning Algorithms</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
            <subj-group>
              <subject>Machine Learning Algorithms</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>SleepEEGNet: Automated sleep stage scoring with sequence to sequence deep learning approach</article-title>
      <alt-title alt-title-type="running-head">SleepEEGNet: Automated sleep stage scoring with sequence to sequence deep learning approach</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-3806-8487</contrib-id>
        <name>
          <surname>Mousavi</surname>
          <given-names>Sajad</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Conceptualization</role>
        <role content-type="http://credit.casrai.org/">Data curation</role>
        <role content-type="http://credit.casrai.org/">Formal analysis</role>
        <role content-type="http://credit.casrai.org/">Investigation</role>
        <role content-type="http://credit.casrai.org/">Methodology</role>
        <role content-type="http://credit.casrai.org/">Visualization</role>
        <role content-type="http://credit.casrai.org/">Writing – original draft</role>
        <xref ref-type="aff" rid="aff001">
          <sup>1</sup>
        </xref>
        <xref ref-type="corresp" rid="cor001">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Afghah</surname>
          <given-names>Fatemeh</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Formal analysis</role>
        <role content-type="http://credit.casrai.org/">Funding acquisition</role>
        <role content-type="http://credit.casrai.org/">Supervision</role>
        <role content-type="http://credit.casrai.org/">Validation</role>
        <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff001">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Acharya</surname>
          <given-names>U. Rajendra</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Validation</role>
        <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff002">
          <sup>2</sup>
        </xref>
        <xref ref-type="aff" rid="aff003">
          <sup>3</sup>
        </xref>
        <xref ref-type="aff" rid="aff004">
          <sup>4</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <label>1</label>
      <addr-line>School of Informatics, Computing and Cyber Systems, Northern Arizona University, Flagstaff, Arizona, United States of America</addr-line>
    </aff>
    <aff id="aff002">
      <label>2</label>
      <addr-line>Department of Electronics and Computer Engineering, Ngee Ann Polytechnic, Singapore, Singapore</addr-line>
    </aff>
    <aff id="aff003">
      <label>3</label>
      <addr-line>Department of Biomedical Engineering, School of Science and Technology, Singapore University of Social Sciences, Singapore, Singapore</addr-line>
    </aff>
    <aff id="aff004">
      <label>4</label>
      <addr-line>Department of Biomedical Engineering, Faculty of Engineering, University of Malaya, Kuala Lumpur, Malaysia</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Pławiak</surname>
          <given-names>Paweł</given-names>
        </name>
        <role>Editor</role>
        <xref ref-type="aff" rid="edit1"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>Politechnika Krakowska im Tadeusza Kosciuszki, POLAND</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>SajadMousavi@nau.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>7</day>
      <month>5</month>
      <year>2019</year>
    </pub-date>
    <volume>14</volume>
    <issue>5</issue>
    <elocation-id>e0216456</elocation-id>
    <history>
      <date date-type="received">
        <day>12</day>
        <month>3</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>23</day>
        <month>4</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2019 Mousavi et al</copyright-statement>
      <copyright-year>2019</copyright-year>
      <copyright-holder>Mousavi et al</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pone.0216456.pdf"/>
    <abstract>
      <p>Electroencephalogram (EEG) is a common base signal used to monitor brain activities and diagnose sleep disorders. Manual sleep stage scoring is a time-consuming task for sleep experts and is limited by inter-rater reliability. In this paper, we propose an automatic sleep stage annotation method called <italic>SleepEEGNet</italic> using a single-channel EEG signal. The <italic>SleepEEGNet</italic> is composed of deep convolutional neural networks (CNNs) to extract time-invariant features, frequency information, and a sequence to sequence model to capture the complex and long short-term context dependencies between sleep epochs and scores. In addition, to reduce the effect of the class imbalance problem presented in the available sleep datasets, we applied novel loss functions to have an equal misclassified error for each sleep stage while training the network. We evaluated the performance of the proposed method on different single-EEG channels (i.e., Fpz-Cz and Pz-Oz EEG channels) from the Physionet Sleep-EDF datasets published in 2013 and 2018. The evaluation results demonstrate that the proposed method achieved the best annotation performance compared to current literature, with an overall accuracy of 84.26%, a macro F1-score of 79.66% and <italic>κ</italic> = 0.79. Our developed model can be applied to other sleep EEG signals and aid the sleep specialists to arrive at an accurate diagnosis. The source code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/SajadMo/SleepEEGNet">https://github.com/SajadMo/SleepEEGNet</ext-link>.</p>
    </abstract>
    <funding-group>
      <award-group id="award001">
        <funding-source>
          <institution>National Institute On Minority Health and Health Disparities of the National Institutes of Health</institution>
        </funding-source>
        <award-id>U54MD012388</award-id>
        <principal-award-recipient>
          <name>
            <surname>Afghah</surname>
            <given-names>Fatemeh</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award002">
        <funding-source>
          <institution>National Science Foundation (NSF)</institution>
        </funding-source>
        <award-id>1657260</award-id>
        <principal-award-recipient>
          <name>
            <surname>Afghah</surname>
            <given-names>Fatemeh</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <funding-statement>This material is based upon work supported by the National Science Foundation under Grant Number 1657260. Research reported in this publication was also supported by the National Institute On Minority Health and Health Disparities of the National Institutes of Health under Award Number U54MD012388. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="6"/>
      <table-count count="4"/>
      <page-count count="15"/>
    </counts>
    <custom-meta-group>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>All EEG files are available from the physiobank database (doi:<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.13026/C2C30J">10.13026/C2C30J</ext-link>). <ext-link ext-link-type="uri" xlink:href="https://physionet.org/physiobank/database/sleep-edf/">https://physionet.org/physiobank/database/sleep-edf/</ext-link>.</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>All EEG files are available from the physiobank database (doi:<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.13026/C2C30J">10.13026/C2C30J</ext-link>). <ext-link ext-link-type="uri" xlink:href="https://physionet.org/physiobank/database/sleep-edf/">https://physionet.org/physiobank/database/sleep-edf/</ext-link>.</p>
  </notes>
</front>
<body>
  <sec sec-type="intro" id="sec001">
    <title>Introduction</title>
    <p>The electroencephalogram (EEG), electrooculogram (EOG), and electromyogram (EMG) signals are widely used to diagnose the sleep disorders (e.g., sleep apnea, parasomnias, and hypersomnia). These signals are typically recorded by placing sensors on different parts of the patient’s body. In an overnight polysomnography (PSG) (also called as sleep study), the EEG signal is usually the main collected signal being used to monitor the brain activities to diagnose different sleep disorders [<xref rid="pone.0216456.ref001" ref-type="bibr">1</xref>] and other common disorders such as epilepsy [<xref rid="pone.0216456.ref002" ref-type="bibr">2</xref>].</p>
    <p>The EEG signals are split into a number of predefined fixed length segments which are termed as epochs. Then, a sleep expert manually labels each epoch according to sleep scoring standards provided by the American Academy of Sleep Medicine (AASM) [<xref rid="pone.0216456.ref003" ref-type="bibr">3</xref>] or the Rechtschaffen and Kales standard [<xref rid="pone.0216456.ref004" ref-type="bibr">4</xref>]. Each EEG recording is around 8-hour long on average. Therefore, the manual scoring of such a long signal for a sleep expert is a tedious and time-consuming task. The human-based annotation methods also highly rely on an inter-rater agreement in place. Therefore, such restrictions call for automated sleep stage classification system that is able to score each epoch automatically with a high accuracy.</p>
    <p>Several studies have focused on developing automated sleep stage scoring algorithms. Generally, they can be divided into two different categories in terms of the feature extraction approaches. First, the hand-engineered feature-based methods that require a prior knowledge of EEG analysis to extract the most relevant features. These approaches first extract the most common features such as time, frequency and time-frequency domain features of single channel-EEG waveforms [<xref rid="pone.0216456.ref005" ref-type="bibr">5</xref>–<xref rid="pone.0216456.ref007" ref-type="bibr">7</xref>]. Then, they apply conventional machine learning algorithms such as support vector machines (SVM) [<xref rid="pone.0216456.ref008" ref-type="bibr">8</xref>], random forests [<xref rid="pone.0216456.ref009" ref-type="bibr">9</xref>] and neural networks [<xref rid="pone.0216456.ref010" ref-type="bibr">10</xref>] to train the model for sleep stage classification based on the extracted features. Although these methods have achieved a reasonable performance, they carry several limitations including the need for a prior knowledge of sleep analysis and are not able to generalize to larger datasets from various patients with different sleep patterns. The second category includes the automated feature extraction-based methods such as deep learning algorithms, in which the machine extracts the pertaining features automatically (e.g., the CNNs to extract time-invariant features from raw EEG signals).</p>
    <p>In recent years, deep neural networks have shown impressive results in various domains ranging from computer vision and reinforcement learning to natural language processing [<xref rid="pone.0216456.ref011" ref-type="bibr">11</xref>–<xref rid="pone.0216456.ref014" ref-type="bibr">14</xref>]. One key reason for the success of deep learning based methods in these domains is the availability of large amounts of data to learn the underlying complex pattern in the data sets. Due to availability of a large number of sleep EEG recordings [<xref rid="pone.0216456.ref015" ref-type="bibr">15</xref>], deep learning algorithms have also been applied for sleep stage classification [<xref rid="pone.0216456.ref001" ref-type="bibr">1</xref>, <xref rid="pone.0216456.ref016" ref-type="bibr">16</xref>–<xref rid="pone.0216456.ref019" ref-type="bibr">19</xref>]. However, in spite of the remarkable achievements in using deep learning models compared to the shallow machine learning methods for sleep stage scoring task, they still suffer from the class imbalance problem present in the sleep datasets. Thus, this problem can limit the use of deep learning techniques and in general machine learning techniques toward reaching an expert-level performance for sleep stage classification.</p>
    <p>The sleep is a cyclical process. Typically, a sleeper experiences five main sleep stages during his sleep time, including <italic>wake</italic>, <italic>N1</italic>, <italic>N2</italic>, <italic>N3</italic>, and rapid eye movement (<italic>REM</italic>) stages. Usually, each sleep cycle goes through the Non-REM (Stages 1, 2 and 3) sleep to REM sleep. In most cases, the cycle takes 90-120 minutes resulting in four to five cycles per night [<xref rid="pone.0216456.ref020" ref-type="bibr">20</xref>]. Hence, we believe the sleep stage classification problem is sequential in nature and taking into account this sequential characteristic by considering the correlation between different stages can enhance the accuracy of sleep stage scoring process. Therefore, it is essential to propose a sleep stage scoring system with the capability of extracting non-linear dependencies present in the consecutive stages during scoring different stages. In this paper, we introduced a novel deep learning approach, called <italic>SleepEEGNet</italic>, for automated sleep stage scoring using a single-channel EEG. In this model, we applied a sequence to sequence deep learning model with the following building blocks: (1) CNNs to perform the feature extraction, (2) a bidirectional recurrent neural network (BiRNN) to capture temporal information from sequences and consider the previous and future input information simultaneously, and (3) an attention network to let the model learn the most relevant parts of the input sequence while training. Also, we utilized new loss functions to reduce the effect of imbalance class problem on the model by treating the error of each misclassified sample equally regardless of being a member of the majority class or minority class.</p>
    <p>The main contributions of our study are as follows:</p>
    <list list-type="bullet">
      <list-item>
        <p>We propose a sequence to sequence deep learning approach along with the BiRNN and attention mechanism that suits best for the sleep stage scoring problem.</p>
      </list-item>
      <list-item>
        <p>We apply novel loss functions to address the imbalance class problem.</p>
      </list-item>
      <list-item>
        <p>The proposed model is an end-to-end deep learning approach that uses raw single-channel EEGs as its input without using any handcrafted features and significant signal pre-processing such as filtering or noise removal methods.</p>
      </list-item>
    </list>
    <p>The rest of the paper is structured as follows: Methodology section introduces the proposed method. Dataset and Data Preparation section describes the utilized datasets and the data preparation techniques. Experimental Results section presents the experimental design and shows the achieved results by the proposed method along with a performance comparison to the state-of-the-art algorithms. Finally, Conclusion section concludes the paper.</p>
  </sec>
  <sec sec-type="materials|methods" id="sec002">
    <title>Methodology</title>
    <p>In the following sections, we present a detailed description of our proposed model developed to automatically score each sleep stage from a given EEG signal.</p>
    <sec id="sec003">
      <title>Pre-processing</title>
      <p>The input to this method is a sequence of 30-s EEG epochs. In order to extract the EEG epochs from a given EEG signal, we follow two simple steps:</p>
      <list list-type="order">
        <list-item>
          <p>Segmenting the continuous raw single-channel EEG to a sequence of 30-s epochs and assigning a label to each epoch (i.e., sleep stage) based on the annotation file.</p>
        </list-item>
        <list-item>
          <p>Normalizing 30-s EEG epochs such that each one has a zero mean and unit variance.</p>
        </list-item>
      </list>
      <p>It is worth mentioning that, these pre-processing steps for the sleep stage extraction are very simple and do not involve any form of filtering or noise removal methods.</p>
    </sec>
    <sec id="sec004">
      <title>The architecture</title>
      <p>The sequence to sequence models have shown very impressive results in neural machine translation applications, nearly similar to human-level performance [<xref rid="pone.0216456.ref021" ref-type="bibr">21</xref>]. The architecture of sequence to sequence networks is usually composed of two main parts: the encoder and decoder which are types of recurrent neural network (RNN). In this study, we used an RNN sequence to sequence model along with a convolutional neural network (CNN) to perform automatic sleep stage scoring task.</p>
      <p><xref ref-type="fig" rid="pone.0216456.g001">Fig 1</xref> illustrates the proposed network architecture for automatic sleep stage classification. We applied almost the same CNN architecture provided by [<xref rid="pone.0216456.ref017" ref-type="bibr">17</xref>]. The CNN consists of two sections, one with small filters to extract temporal information and another one with large filters to extract frequency information. The idea behind these variable sizes of filters comes from the signal processing community to have a trade-off between extracting time domain (i.e., time-invariant) and frequency domain features [<xref rid="pone.0216456.ref022" ref-type="bibr">22</xref>]. This helps get benefit from both time and frequency domain features in the classification task.</p>
      <fig id="pone.0216456.g001" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0216456.g001</object-id>
        <label>Fig 1</label>
        <caption>
          <title>Illustration of the proposed sequence to sequence deep learning network architecture for automated sleep stage scoring.</title>
          <p>The input signal is a sequence of 30-s EEG epochs and the outputs are their corresponding stages (or classes) generated by our proposed method.</p>
        </caption>
        <graphic xlink:href="pone.0216456.g001"/>
      </fig>
      <p>Each CNN part consists of four consecutive one-dimensional convolutional layers. Each convolutional layer is passed to a rectified linear unit (ReLU) nonlinearity. The first layer is followed by a max pooling layer and a dropout block, and just a dropout block comes after the last convolutional layer. At each time-step of training/testing the model, a sequence (size of <italic>maxtime</italic>) of 30-s EEG epochs is fed into the CNN for feature extraction. In the end, the outputs of CNN parts are concatenated serially and followed by a dropout block in order for the encoder network to encode the sequence input. <xref ref-type="fig" rid="pone.0216456.g002">Fig 2</xref> depicts the detailed CNN structure.</p>
      <fig id="pone.0216456.g002" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0216456.g002</object-id>
        <label>Fig 2</label>
        <caption>
          <title>Detailed sketch of the utilized CNN model in the proposed work.</title>
        </caption>
        <graphic xlink:href="pone.0216456.g002"/>
      </fig>
      <p>The sequence to sequence model is designed based on the encoder-decoder abstract ideas. The encoder encodes the input sequence, while the decoder computes the category of each single channel 30-s EEG of the input sequence. The encoder is composed of long short-term memory (LSTM) units which capture the complex and long short-term context dependencies between the inputs and the targets [<xref rid="pone.0216456.ref023" ref-type="bibr">23</xref>]. They capture non-linear dependencies present in the entire time series while predicting a target. The (time) sequence of input feature vectors herein are fed to the LSTMs and then the hidden states, (<italic>e</italic><sub>0</sub>, <italic>e</italic><sub>1</sub>, <italic>e</italic><sub>2</sub>, …), calculated by the LSTM are considered as the encoder representation, and are fed to the attention network (or to initialize the first hidden state of the decoder, if the basic decoder is used), as depicted in <xref ref-type="fig" rid="pone.0216456.g001">Fig 1</xref>.</p>
    </sec>
    <sec id="sec005">
      <title>Bidirectional recurrent neural network</title>
      <p>We have utilized the bidirectional recurrent neural network (BiRNN) units in the network architecture instead of the standard LSTM (i.e., standard RNN). Standard RNNs are unidirectional, hence they are restricted to use the previous input state. To address this limitation, the BiRNN have been proposed [<xref rid="pone.0216456.ref024" ref-type="bibr">24</xref>], which can process data in both forward and backward directions. Thus, the current state has access to previous and future input information simultaneously. The BiRNN consists of forward and backward networks. The input sequence is fed in normal time order, <italic>t</italic> = 1, …, <italic>T</italic> for the forward network, and in reverse time order, <italic>t</italic> = <italic>T</italic>, …, 1 for the backward network. Finally, the weighted sum of the outputs of the two networks is computed as the output of the BiRNN. This mechanism can be formulated as follows:
<disp-formula id="pone.0216456.e001"><alternatives><graphic xlink:href="pone.0216456.e001.jpg" id="pone.0216456.e001g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M1"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>→</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo form="prefix">tanh</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>W</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mover accent="true"><mml:mi>V</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:msub><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mover accent="true"><mml:mi>b</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(1)</label></disp-formula>
<disp-formula id="pone.0216456.e002"><alternatives><graphic xlink:href="pone.0216456.e002.jpg" id="pone.0216456.e002g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M2"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>←</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo form="prefix">tanh</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>W</mml:mi><mml:mo>←</mml:mo></mml:mover><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mover accent="true"><mml:mi>V</mml:mi><mml:mo>←</mml:mo></mml:mover><mml:msub><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>←</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mover accent="true"><mml:mi>b</mml:mi><mml:mo>←</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(2)</label></disp-formula>
<disp-formula id="pone.0216456.e003"><alternatives><graphic xlink:href="pone.0216456.e003.jpg" id="pone.0216456.e003g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M3"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>U</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>→</mml:mo></mml:mover><mml:mo>;</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>←</mml:mo></mml:mover><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(3)</label></disp-formula>
where (<inline-formula id="pone.0216456.e004"><alternatives><graphic xlink:href="pone.0216456.e004.jpg" id="pone.0216456.e004g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M4"><mml:mover accent="true"><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>→</mml:mo></mml:mover></mml:math></alternatives></inline-formula>, <inline-formula id="pone.0216456.e005"><alternatives><graphic xlink:href="pone.0216456.e005.jpg" id="pone.0216456.e005g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M5"><mml:mover accent="true"><mml:mi>b</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:math></alternatives></inline-formula>) are the hidden state and the bias of the froward network, and (<inline-formula id="pone.0216456.e006"><alternatives><graphic xlink:href="pone.0216456.e006.jpg" id="pone.0216456.e006g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M6"><mml:mover accent="true"><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>←</mml:mo></mml:mover></mml:math></alternatives></inline-formula>, <inline-formula id="pone.0216456.e007"><alternatives><graphic xlink:href="pone.0216456.e007.jpg" id="pone.0216456.e007g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M7"><mml:mover accent="true"><mml:mi>b</mml:mi><mml:mo>←</mml:mo></mml:mover></mml:math></alternatives></inline-formula>) are the hidden state and the bias of the backward network. Also, <italic>x</italic><sub><italic>t</italic></sub> and <italic>y</italic><sub><italic>t</italic></sub> are the input and the output of the BiRNN, respectively. <xref ref-type="fig" rid="pone.0216456.g003">Fig 3</xref> illustrates a BiRNN architecture with T time steps.</p>
      <fig id="pone.0216456.g003" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0216456.g003</object-id>
        <label>Fig 3</label>
        <caption>
          <title>A schematic diagram of the bidirectional recurrent neural network.</title>
        </caption>
        <graphic xlink:href="pone.0216456.g003"/>
      </fig>
    </sec>
    <sec id="sec006">
      <title>Attention decoder</title>
      <p>The decoder is used to generate the target sequence epoch by epoch. Similar to the encoder, the building block of the decoder is an LSTM. In a basic decoder, at every step of decoding, the decoder gets a new representation of an input element of the sequence generated by the encoder and an element of the target input. The last element of the input sequence is usually the last influence to update the encoder’s hidden state. Therefore, the model can be biased to the last element. To address such a problem, we have applied an attention mechanism to the model to consider not only the whole encoder representation of the input but also to put more emphasis on different parts of the encoder outputs in each step of decoding. In other words, the attention mechanism makes the model to learn the most relevant parts of the input sequence in the decoding phase. In a sequence to sequence model without an attention approach, the decoder part relies on the hidden vector of the decoder’s RNN (or BiRNN), while the sequence to sequence model with the attention is more goal-oriented by putting attention on the most related input regions to produce the targets. It considers the combination of encoder’s representation and decoder hidden vector calling the context vector or the attention vector, (<italic>c</italic><sub><italic>t</italic></sub>).</p>
      <p>To calculate the <italic>c</italic><sub><italic>t</italic></sub> vector, we first computed a set of attention weights with a function <italic>f</italic>(.) followed by a softmax function. These attention weights are probabilities, (<italic>α</italic><sub><italic>i</italic></sub>), corresponding to the importance of each hidden state. Then, these scores are multiplied by the hidden states (i.e, the encoder output vectors) to calculate the weighted combination, (<italic>c</italic><sub><italic>t</italic></sub>).
<disp-formula id="pone.0216456.e008"><alternatives><graphic xlink:href="pone.0216456.e008.jpg" id="pone.0216456.e008g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M8"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo form="prefix">tanh</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(4)</label></disp-formula>
<disp-formula id="pone.0216456.e009"><alternatives><graphic xlink:href="pone.0216456.e009.jpg" id="pone.0216456.e009g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M9"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>softmax</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:mfrac><mml:mrow><mml:mo form="prefix">exp</mml:mo><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mo form="prefix">exp</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mspace width="3.33333pt"/><mml:mspace width="3.33333pt"/><mml:mo>∈</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(5)</label></disp-formula>
<disp-formula id="pone.0216456.e010"><alternatives><graphic xlink:href="pone.0216456.e010.jpg" id="pone.0216456.e010g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M10"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(6)</label></disp-formula>
where <italic>α</italic><sub><italic>i</italic></sub> is a parameter reflecting the importance of part <italic>i</italic> of hidden state <italic>e</italic><sub><italic>i</italic></sub>. In other words, at every time step <italic>t</italic>, the attention layer computes <italic>f</italic>(.), a combination of the values of <italic>e</italic><sub><italic>i</italic></sub> (the encoder’s hidden state) and <italic>h</italic><sub><italic>t</italic>−1</sub> (the decoder’s hidden state) followed by a tanh layer. Then, the <italic>f</italic>(.) output is fed into a softmax module to calculate <italic>α</italic><sub><italic>i</italic></sub> over <italic>n</italic> parts. Finally, the attention module computes <italic>c</italic><sub><italic>t</italic></sub>, a weighted sum of all vectors <italic>e</italic><sub><italic>i</italic></sub>, <italic>i</italic> ∈ 1, 2, …, <italic>n</italic> based on computed <italic>α</italic><sub><italic>i</italic></sub>’s. Thus, the model can learn to focus on the important regions of the input sequence when decoding.</p>
      <p>During the training phase, the decoder, in addition to the augmented version of the encoder’s hidden states, captures the given target sequence shifted by one starting with a special feature vector &lt; <italic>SOD</italic> &gt; (i.e., the start of decoding) as its input. Then, the decoder starts to generate outputs until it confronts the special label called &lt; <italic>EOD</italic> &gt; (i.e., the end of decoding). We should note that the target sequence is just used during the training phase and is not applied for the testing phase. During the testing phase, the decoder uses whatever label it generates at each step as the input for the next step. Finally, a softmax is applied to the output of the decoder to convert it to a vector of probabilities <italic>p</italic> ∈ [0, 1]<sup><italic>C</italic></sup>, where <italic>C</italic> represents the number of classes and each element of <italic>p</italic> indicates the probability of each class of the sleep stage.</p>
    </sec>
    <sec id="sec007">
      <title>Loss calculation</title>
      <p>Similar to other biomedical applications, the sleep stage classification also deals with the problem of class imbalance. To alleviate the effect of this problem on the model, we calculated new loss functions based on [<xref rid="pone.0216456.ref025" ref-type="bibr">25</xref>] to treat the error of each misclassified sample equally regardless of being a member of the majority or minority class.</p>
      <p>We extended the proposed loss functions, mean false error (MFE) and mean squared false error (MSFE), in [<xref rid="pone.0216456.ref025" ref-type="bibr">25</xref>] for the multi-class classification task. MFE and MSFE can be defined as follows:
<disp-formula id="pone.0216456.e011"><alternatives><graphic xlink:href="pone.0216456.e011.jpg" id="pone.0216456.e011g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M11"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>l</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(7)</label></disp-formula>
<disp-formula id="pone.0216456.e012"><alternatives><graphic xlink:href="pone.0216456.e012.jpg" id="pone.0216456.e012g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M12"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>F</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mi>l</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(8)</label></disp-formula>
<disp-formula id="pone.0216456.e013"><alternatives><graphic xlink:href="pone.0216456.e013.jpg" id="pone.0216456.e013g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M13"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>F</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mi>l</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(9)</label></disp-formula>
where <italic>c</italic><sub><italic>i</italic></sub> is the class label (e.g., W or N1), <italic>C</italic><sub><italic>i</italic></sub> is the number of the samples in class <italic>c</italic><sub><italic>i</italic></sub>, <italic>N</italic> is the number available classes (here sleep stage classes), and <italic>l</italic>(<italic>c</italic><sub><italic>i</italic></sub>) is the calculated error for the class <italic>c</italic><sub><italic>i</italic></sub>. In the most common used loss function, mean squared error (MSE), the loss is calculated by averaging the squared difference between predictions and targets. This way of computing the loss function makes the contribution of the majority classes be much more in comparison with the minorities classes in the imbalanced dataset. However, the MFE and MSFE try to consider the errors of all classes equally.</p>
    </sec>
  </sec>
  <sec id="sec008">
    <title>Dataset and data preparation</title>
    <p>In this study, we used the Physionet Sleep-EDF dataset [<xref rid="pone.0216456.ref015" ref-type="bibr">15</xref>, <xref rid="pone.0216456.ref026" ref-type="bibr">26</xref>]: version 1 contributed in 2013 with 61 polysomnograms (PSGs) and version 2 contributed in 2018 with 197 PSGs to evaluate the performance of the proposed method for the sleep stage scoring task. For simplicity’s sake, the Sleep-EDF-13 and the Sleep-EDF-18 are used for versions 1 and 2, respectively. Sleep-EDF 2013 to The Sleep-EDF dataset contains two different studies including (1) study of age effects on sleep in healthy individuals (SC = Sleep Cassette) and (2) study of temazepam effects on sleep (ST = Sleep Telemetry). The dataset includes whole-night polysomnograms (PSGs) sleep recordings at the sampling rate of 100 Hz. Each record contains EEG (from Fpz-Cz and Pz-Oz electrode locations), EOG, chin electromyography (EMG), and event markers. Few records often also contain oro-nasal respiration and rectal body temperature. The hypnograms (sleep stages; 30-s epochs) were manually labeled by well-trained technicians according to the Rechtschaffen and Kales standard [<xref rid="pone.0216456.ref004" ref-type="bibr">4</xref>].</p>
    <p>Each stage was considered to belong to a different class (stage). The classes include wake (W), rapid eye movement (REM), N1, N2, N3, N4, M (movement time) and ‘?’ (not scored). According to American Academy of Sleep Medicine (AASM) standard, we integrated the stages of N3 and N4 in one class named N3 and excluded M (movement time) and ? (not scored) stages to have five sleep stages [<xref rid="pone.0216456.ref003" ref-type="bibr">3</xref>]. Stages 1 and 2-3 are the light sleep time in which the stage N1 is the lightest stage and has a short period time. The stage N2-N3 takes longer than the stage N1, including approximately 40-60% of total sleep time. The stage N3 is called as deep sleep and the REM is known as the dreaming stage taking 90-120 minutes per night [<xref rid="pone.0216456.ref020" ref-type="bibr">20</xref>]. Considering different stage time periods results in having a imbalanced stage numbers in the sleep datasets. In addition, we considered Fpz-Cz/Pz-Oz EEG channels from SCs of both versions in our evaluations. <xref rid="pone.0216456.t001" ref-type="table">Table 1</xref> presents the number of sleep stages in two different versions.</p>
    <table-wrap id="pone.0216456.t001" orientation="portrait" position="float">
      <object-id pub-id-type="doi">10.1371/journal.pone.0216456.t001</object-id>
      <label>Table 1</label>
      <caption>
        <title>Details of number of sleep stages in each version of Sleep-EDF dataset.</title>
      </caption>
      <alternatives>
        <graphic id="pone.0216456.t001g" xlink:href="pone.0216456.t001"/>
        <table frame="box" rules="all" border="0">
          <colgroup span="1">
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th align="left" style="border-top:thick" rowspan="1" colspan="1">Dataset</th>
              <th align="center" style="border-top:thick" rowspan="1" colspan="1">W</th>
              <th align="center" style="border-top:thick" rowspan="1" colspan="1">N1</th>
              <th align="center" style="border-top:thick" rowspan="1" colspan="1">N2</th>
              <th align="center" style="border-top:thick" rowspan="1" colspan="1">N3-N4</th>
              <th align="center" style="border-top:thick" rowspan="1" colspan="1">REM</th>
              <th align="center" style="border-top:thick" rowspan="1" colspan="1">Total</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" rowspan="1" colspan="1">Sleep-EDF-13</td>
              <td align="center" rowspan="1" colspan="1">8,285</td>
              <td align="center" rowspan="1" colspan="1">2,804</td>
              <td align="center" rowspan="1" colspan="1">17,799</td>
              <td align="center" rowspan="1" colspan="1">5,703</td>
              <td align="center" rowspan="1" colspan="1">7,717</td>
              <td align="center" rowspan="1" colspan="1">42,308</td>
            </tr>
            <tr>
              <td align="left" style="border-bottom:thick" rowspan="1" colspan="1">Sleep-EDF-18</td>
              <td align="center" style="border-bottom:thick" rowspan="1" colspan="1">65,951</td>
              <td align="center" style="border-bottom:thick" rowspan="1" colspan="1">21,522</td>
              <td align="center" style="border-bottom:thick" rowspan="1" colspan="1">96,132</td>
              <td align="center" style="border-bottom:thick" rowspan="1" colspan="1">13,039</td>
              <td align="center" style="border-bottom:thick" rowspan="1" colspan="1">25,835</td>
              <td align="center" style="border-bottom:thick" rowspan="1" colspan="1">222,479</td>
            </tr>
          </tbody>
        </table>
      </alternatives>
    </table-wrap>
  </sec>
  <sec id="sec009">
    <title>Experimental results</title>
    <sec id="sec010">
      <title>Experimental design</title>
      <p>The distribution of sleep stages in the Sleep-EDF database is not uniform. Hence, the number of W and N2 stages are much greater than other stages. The machine learning approaches do not perform well with the class imbalance problem. To address this problem, in addition to using the novel loss functions described in Loss calculation section, the dataset is oversampled to nearly reaching a balanced number of sleep stages in each class. We have used the synthetic minority over-sampling technique (SMOTE) to generate the synthetic data points by considering the similarities between existing minority samples [<xref rid="pone.0216456.ref027" ref-type="bibr">27</xref>].</p>
      <p>Our proposed model was evaluated using a k-fold cross-validation. We set k to 20 and 10 for version 1 and version 2 of the Sleep-EDF dataset, respectively. In other words, we split the dataset into k folds. Then, for each unique fold, (1) the fold is taken as test set and the remaining folds as a training set and (2) trained the model using the training set and evaluated the model using the test set. Finally, all evaluation results were combined.</p>
      <p>The network was trained (for each dataset) with a maximum of 120 epochs. RMSProp optimizer was used to minimize the <italic>l</italic><sub><italic>MFE</italic></sub> loss with mini batches of size 20 and a learning rate of <italic>α</italic> = 0.001. We also applied an additional <italic>L</italic><sub>2</sub> regularization element with <italic>β</italic> = 0.001 to the loss function to mitigate the overfitting. Python programming language and Google Tensorflow deep learning library were utilized to implement our proposed approach. We ran the k-fold cross validation on a machine with 8 CPUs (Intel(R) Xeon(R) CPU @ 3.60 GHz), 32 GB memory and Ubuntu 16.04. The training time for each epoch was 98 seconds on average and the testing time for each batch of 20 EEG epochs was approximately 0.102 seconds.</p>
    </sec>
    <sec id="sec011">
      <title>Evaluation metrics</title>
      <p>We have used different metrics to evaluate the performance of the proposed approach including, overall accuracy, precision, recall (sensitivity), specificity, and F1-score. We also computed macro-averaging of F1-score (MF1) which is the sum of per-class F1-scores over the number of classes (i.e., sleep stages). These metrics are defined as follows: 
<disp-formula id="pone.0216456.e014"><alternatives><graphic xlink:href="pone.0216456.e014.jpg" id="pone.0216456.e014g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M14"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></alternatives><label>(10)</label></disp-formula>
<disp-formula id="pone.0216456.e015"><alternatives><graphic xlink:href="pone.0216456.e015.jpg" id="pone.0216456.e015g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M15"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>/</mml:mo><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></alternatives><label>(11)</label></disp-formula>
<disp-formula id="pone.0216456.e016"><alternatives><graphic xlink:href="pone.0216456.e016.jpg" id="pone.0216456.e016g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M16"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>/</mml:mo><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></alternatives><label>(12)</label></disp-formula>
<disp-formula id="pone.0216456.e017"><alternatives><graphic xlink:href="pone.0216456.e017.jpg" id="pone.0216456.e017g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M17"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></alternatives><label>(13)</label></disp-formula>
<disp-formula id="pone.0216456.e018"><alternatives><graphic xlink:href="pone.0216456.e018.jpg" id="pone.0216456.e018g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M18"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mspace width="0.166667em"/><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mfrac><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mo>×</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(14)</label></disp-formula>
where TP (True Positive), TN (True Negative), FP (False Positive) and FN (False Negative) indicate the number of sleep stages correctly labeled, the number of sleep stages correctly identified as not correspond to the sleep stages, the number of sleep stages that incorrectly labeled, and the number of sleep stages which were not identified as the sleep stages that they should have been, respectively. The other main metric that we have used for performance evaluation of our proposed method is Cohen’s Kappa coefficient (<italic>κ</italic>). When two persons (algorithms or raters) try to measure the same data, the Cohen’s Kappa coefficient, <italic>κ</italic>, is used as a measure of agreement between their decisions. For example, in this study, we aim to measure the amount of agreement between our algorithm as one rater and the provided labels for sleep stages by the dataset as another rater.</p>
    </sec>
    <sec id="sec012">
      <title>Results and discussion</title>
      <p>Tables <xref rid="pone.0216456.t002" ref-type="table">2</xref> and <xref rid="pone.0216456.t003" ref-type="table">3</xref> present the confusion matrices and the performances of each class achieved by the proposed method using Fpz-Cz and Pz-Oz channels of the EDF-Sleep-2013 data set, respectively. The main diagonals in each confusion matrix denote the true positive (TP) values which indicate the number of stages scored correctly. It can be seen from the tables (the confusion matrices’ parts) that TP values are higher than other values in the same rows and columns. These tables also show the prediction performance (i.e., the precision, recall, specificity and F1 score) of our model for each class (i.e., the stage). Among all stages, the model performance is better for W1, N2, N3, and REM stages than the N1 stage. This may be because the number of N1 stages in the dataset is smaller compared to the other stages. However, our results for stage N1 is better than other state-of-the-art algorithms listed in <xref rid="pone.0216456.t004" ref-type="table">Table 4</xref>.</p>
      <table-wrap id="pone.0216456.t002" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0216456.t002</object-id>
        <label>Table 2</label>
        <caption>
          <title>Confusion matrix and per-class performance achieved by the proposed method using Fpz-Cz EEG channel of the EDF-Sleep-2013 database.</title>
        </caption>
        <alternatives>
          <graphic id="pone.0216456.t002g" xlink:href="pone.0216456.t002"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="center" rowspan="2" colspan="1"/>
                <th align="center" colspan="5" rowspan="1">Predicted</th>
                <th align="center" colspan="4" rowspan="1">Per-class Performance (%)</th>
              </tr>
              <tr>
                <th align="center" rowspan="1" colspan="1">W1</th>
                <th align="center" rowspan="1" colspan="1">N1</th>
                <th align="center" rowspan="1" colspan="1">N2</th>
                <th align="center" rowspan="1" colspan="1">N3</th>
                <th align="center" rowspan="1" colspan="1">REM</th>
                <th align="center" rowspan="1" colspan="1">Pre</th>
                <th align="center" rowspan="1" colspan="1">Rec</th>
                <th align="center" rowspan="1" colspan="1">Spe</th>
                <th align="center" rowspan="1" colspan="1">F1</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">W1</td>
                <td align="center" rowspan="1" colspan="1">7161</td>
                <td align="center" rowspan="1" colspan="1">432</td>
                <td align="center" rowspan="1" colspan="1">67</td>
                <td align="center" rowspan="1" colspan="1">27</td>
                <td align="center" rowspan="1" colspan="1">219</td>
                <td align="char" char="." rowspan="1" colspan="1">87.84</td>
                <td align="char" char="." rowspan="1" colspan="1">90.58</td>
                <td align="char" char="." rowspan="1" colspan="1">96.97</td>
                <td align="char" char="." rowspan="1" colspan="1">89.19</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">N1</td>
                <td align="center" rowspan="1" colspan="1">442</td>
                <td align="center" rowspan="1" colspan="1">1486</td>
                <td align="center" rowspan="1" colspan="1">364</td>
                <td align="center" rowspan="1" colspan="1">25</td>
                <td align="center" rowspan="1" colspan="1">409</td>
                <td align="char" char="." rowspan="1" colspan="1">50.05</td>
                <td align="char" char="." rowspan="1" colspan="1">54.51</td>
                <td align="char" char="." rowspan="1" colspan="1">96.08</td>
                <td align="char" char="." rowspan="1" colspan="1">52.19</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">N2</td>
                <td align="center" rowspan="1" colspan="1">359</td>
                <td align="center" rowspan="1" colspan="1">735</td>
                <td align="center" rowspan="1" colspan="1">14187</td>
                <td align="center" rowspan="1" colspan="1">1035</td>
                <td align="center" rowspan="1" colspan="1">837</td>
                <td align="char" char="." rowspan="1" colspan="1">91.26</td>
                <td align="char" char="." rowspan="1" colspan="1">82.71</td>
                <td align="char" char="." rowspan="1" colspan="1">94.20</td>
                <td align="char" char="." rowspan="1" colspan="1">86.77</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">N3</td>
                <td align="center" rowspan="1" colspan="1">37</td>
                <td align="center" rowspan="1" colspan="1">9</td>
                <td align="center" rowspan="1" colspan="1">560</td>
                <td align="center" rowspan="1" colspan="1">4857</td>
                <td align="center" rowspan="1" colspan="1">2</td>
                <td align="char" char="." rowspan="1" colspan="1">81.69</td>
                <td align="char" char="." rowspan="1" colspan="1">88.87</td>
                <td align="char" char="." rowspan="1" colspan="1">96.90</td>
                <td align="char" char="." rowspan="1" colspan="1">85.13</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">REM</td>
                <td align="center" rowspan="1" colspan="1">153</td>
                <td align="center" rowspan="1" colspan="1">307</td>
                <td align="center" rowspan="1" colspan="1">368</td>
                <td align="center" rowspan="1" colspan="1">2</td>
                <td align="center" rowspan="1" colspan="1">6520</td>
                <td align="char" char="." rowspan="1" colspan="1">81.63</td>
                <td align="char" char="." rowspan="1" colspan="1">88.71</td>
                <td align="char" char="." rowspan="1" colspan="1">95.59</td>
                <td align="char" char="." rowspan="1" colspan="1">85.02</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <table-wrap id="pone.0216456.t003" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0216456.t003</object-id>
        <label>Table 3</label>
        <caption>
          <title>Confusion matrix and per-class performance achieved by the proposed method using Pz-Oz EEG channel of the EDF-Sleep-2013 database.</title>
        </caption>
        <alternatives>
          <graphic id="pone.0216456.t003g" xlink:href="pone.0216456.t003"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="center" rowspan="2" colspan="1"/>
                <th align="center" colspan="5" rowspan="1">Predicted</th>
                <th align="center" colspan="4" rowspan="1">Per-class Performance (%)</th>
              </tr>
              <tr>
                <th align="center" rowspan="1" colspan="1">W1</th>
                <th align="center" rowspan="1" colspan="1">N1</th>
                <th align="center" rowspan="1" colspan="1">N2</th>
                <th align="center" rowspan="1" colspan="1">N3</th>
                <th align="center" rowspan="1" colspan="1">REM</th>
                <th align="center" rowspan="1" colspan="1">Pre</th>
                <th align="center" rowspan="1" colspan="1">Rec</th>
                <th align="center" rowspan="1" colspan="1">Spe</th>
                <th align="center" rowspan="1" colspan="1">F1</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">W1</td>
                <td align="center" rowspan="1" colspan="1">7094</td>
                <td align="center" rowspan="1" colspan="1">398</td>
                <td align="center" rowspan="1" colspan="1">82</td>
                <td align="center" rowspan="1" colspan="1">41</td>
                <td align="center" rowspan="1" colspan="1">238</td>
                <td align="char" char="." rowspan="1" colspan="1">90.20</td>
                <td align="char" char="." rowspan="1" colspan="1">90.33</td>
                <td align="char" char="." rowspan="1" colspan="1">97.65</td>
                <td align="char" char="." rowspan="1" colspan="1">90.27</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">N1</td>
                <td align="center" rowspan="1" colspan="1">539</td>
                <td align="center" rowspan="1" colspan="1">1167</td>
                <td align="center" rowspan="1" colspan="1">455</td>
                <td align="center" rowspan="1" colspan="1">29</td>
                <td align="center" rowspan="1" colspan="1">492</td>
                <td align="char" char="." rowspan="1" colspan="1">45.84</td>
                <td align="char" char="." rowspan="1" colspan="1">43.51</td>
                <td align="char" char="." rowspan="1" colspan="1">96.36</td>
                <td align="char" char="." rowspan="1" colspan="1">44.64</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">N2</td>
                <td align="center" rowspan="1" colspan="1">114</td>
                <td align="center" rowspan="1" colspan="1">655</td>
                <td align="center" rowspan="1" colspan="1">14220</td>
                <td align="center" rowspan="1" colspan="1">1157</td>
                <td align="center" rowspan="1" colspan="1">971</td>
                <td align="char" char="." rowspan="1" colspan="1">88.58</td>
                <td align="char" char="." rowspan="1" colspan="1">83.07</td>
                <td align="char" char="." rowspan="1" colspan="1">92.19</td>
                <td align="char" char="." rowspan="1" colspan="1">85.74</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">N3</td>
                <td align="center" rowspan="1" colspan="1">17</td>
                <td align="center" rowspan="1" colspan="1">12</td>
                <td align="center" rowspan="1" colspan="1">791</td>
                <td align="center" rowspan="1" colspan="1">4658</td>
                <td align="center" rowspan="1" colspan="1">10</td>
                <td align="char" char="." rowspan="1" colspan="1">78.48</td>
                <td align="char" char="." rowspan="1" colspan="1">84.88</td>
                <td align="char" char="." rowspan="1" colspan="1">96.36</td>
                <td align="char" char="." rowspan="1" colspan="1">81.55</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">REM</td>
                <td align="center" rowspan="1" colspan="1">100</td>
                <td align="center" rowspan="1" colspan="1">314</td>
                <td align="center" rowspan="1" colspan="1">506</td>
                <td align="center" rowspan="1" colspan="1">50</td>
                <td align="center" rowspan="1" colspan="1">6489</td>
                <td align="char" char="." rowspan="1" colspan="1">79.13</td>
                <td align="char" char="." rowspan="1" colspan="1">87.00</td>
                <td align="char" char="." rowspan="1" colspan="1">94.84</td>
                <td align="char" char="." rowspan="1" colspan="1">82.88</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <table-wrap id="pone.0216456.t004" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0216456.t004</object-id>
        <label>Table 4</label>
        <caption>
          <title>Comparison of performance obtained by our approach with other state-of-the-art algorithms.</title>
        </caption>
        <alternatives>
          <graphic id="pone.0216456.t004g" xlink:href="pone.0216456.t004"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="center" rowspan="2" style="border-top:thick" colspan="1">Method</th>
                <th align="center" rowspan="2" style="border-top:thick" colspan="1">Dataset</th>
                <th align="center" rowspan="2" style="border-top:thick" colspan="1">CV</th>
                <th align="center" rowspan="2" style="border-top:thick" colspan="1">EEG Channel</th>
                <th align="center" colspan="3" style="border-top:thick" rowspan="1">Overall Performance</th>
                <th align="center" colspan="5" style="border-top:thick" rowspan="1">Per-class Performance (F1)</th>
              </tr>
              <tr>
                <th align="center" rowspan="1" colspan="1">ACC</th>
                <th align="center" rowspan="1" colspan="1">MF1</th>
                <th align="center" rowspan="1" colspan="1">
                  <italic>κ</italic>
                </th>
                <th align="center" rowspan="1" colspan="1">W</th>
                <th align="center" rowspan="1" colspan="1">N1</th>
                <th align="center" rowspan="1" colspan="1">N2</th>
                <th align="center" rowspan="1" colspan="1">N3</th>
                <th align="center" rowspan="1" colspan="1">REM</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="center" rowspan="1" colspan="1">
                  <italic>SleepEEGNet</italic>
                </td>
                <td align="center" rowspan="1" colspan="1">Sleep-EDF-13</td>
                <td align="center" rowspan="1" colspan="1">20-fold CV</td>
                <td align="center" rowspan="1" colspan="1">Fpz-Cz</td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>84.26</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>79.66</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>0.79</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>89.19</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>52.19</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>86.77</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>85.13</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>85.02</bold>
                </td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">Supratak et al. [<xref rid="pone.0216456.ref017" ref-type="bibr">17</xref>]</td>
                <td align="center" rowspan="1" colspan="1">Sleep-EDF-13</td>
                <td align="center" rowspan="1" colspan="1">20-fold CV</td>
                <td align="center" rowspan="1" colspan="1">Fpz-Cz</td>
                <td align="char" char="." rowspan="1" colspan="1">82.0</td>
                <td align="char" char="." rowspan="1" colspan="1">76.9</td>
                <td align="char" char="." rowspan="1" colspan="1">0.76</td>
                <td align="char" char="." rowspan="1" colspan="1">84.7</td>
                <td align="char" char="." rowspan="1" colspan="1">46.6</td>
                <td align="char" char="." rowspan="1" colspan="1">85.9</td>
                <td align="char" char="." rowspan="1" colspan="1">84.8</td>
                <td align="char" char="." rowspan="1" colspan="1">82.4</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">Tsinalis et al. [<xref rid="pone.0216456.ref018" ref-type="bibr">18</xref>]</td>
                <td align="center" rowspan="1" colspan="1">Sleep-EDF-13</td>
                <td align="center" rowspan="1" colspan="1">20-fold CV</td>
                <td align="center" rowspan="1" colspan="1">Fpz-Cz</td>
                <td align="char" char="." rowspan="1" colspan="1">78.9</td>
                <td align="char" char="." rowspan="1" colspan="1">73.7</td>
                <td align="center" rowspan="1" colspan="1">-</td>
                <td align="char" char="." rowspan="1" colspan="1">71.6</td>
                <td align="char" char="." rowspan="1" colspan="1">47.0</td>
                <td align="char" char="." rowspan="1" colspan="1">84.6</td>
                <td align="char" char="." rowspan="1" colspan="1">84.0</td>
                <td align="char" char="." rowspan="1" colspan="1">81.4</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">Tsinalis et al. [<xref rid="pone.0216456.ref028" ref-type="bibr">28</xref>]</td>
                <td align="center" rowspan="1" colspan="1">Sleep-EDF-13</td>
                <td align="center" rowspan="1" colspan="1">20-fold CV</td>
                <td align="center" rowspan="1" colspan="1">Fpz-Cz</td>
                <td align="char" char="." rowspan="1" colspan="1">74.8</td>
                <td align="char" char="." rowspan="1" colspan="1">69.8</td>
                <td align="center" rowspan="1" colspan="1">-</td>
                <td align="char" char="." rowspan="1" colspan="1">65.4</td>
                <td align="char" char="." rowspan="1" colspan="1">43.7</td>
                <td align="char" char="." rowspan="1" colspan="1">80.6</td>
                <td align="char" char="." rowspan="1" colspan="1">84.9</td>
                <td align="char" char="." rowspan="1" colspan="1">74.5</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">
                  <italic>SleepEEGNet</italic>
                </td>
                <td align="center" rowspan="1" colspan="1">Sleep-EDF-13</td>
                <td align="center" rowspan="1" colspan="1">20-fold CV</td>
                <td align="center" rowspan="1" colspan="1">Pz-Oz</td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>82.83</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>77.02</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>0.77</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>90.27</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>44.64</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>85.74</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>81.55</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>82.88</bold>
                </td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">Supratak et al. [<xref rid="pone.0216456.ref017" ref-type="bibr">17</xref>]</td>
                <td align="center" rowspan="1" colspan="1">Sleep-EDF-13</td>
                <td align="center" rowspan="1" colspan="1">20-fold CV</td>
                <td align="center" rowspan="1" colspan="1">Pz-Oz</td>
                <td align="char" char="." rowspan="1" colspan="1">79.8</td>
                <td align="char" char="." rowspan="1" colspan="1">73.1</td>
                <td align="char" char="." rowspan="1" colspan="1">0.72</td>
                <td align="char" char="." rowspan="1" colspan="1">88.1</td>
                <td align="char" char="." rowspan="1" colspan="1">37</td>
                <td align="char" char="." rowspan="1" colspan="1">82.7</td>
                <td align="char" char="." rowspan="1" colspan="1">77.3</td>
                <td align="char" char="." rowspan="1" colspan="1">80.3</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">
                  <italic>SleepEEGNet</italic>
                </td>
                <td align="center" rowspan="1" colspan="1">Sleep-EDF-18</td>
                <td align="center" rowspan="1" colspan="1">10-fold CV</td>
                <td align="center" rowspan="1" colspan="1">Fpz-Cz</td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <underline>80.03</underline>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <underline>73.55</underline>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <underline>0.73</underline>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <underline>91.72</underline>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <underline>44.05</underline>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <underline>82.49</underline>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <underline>73.45</underline>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <underline>76.06</underline>
                </td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">
                  <italic>SleepEEGNet</italic>
                </td>
                <td align="center" rowspan="1" colspan="1">Sleep-EDF-18</td>
                <td align="center" rowspan="1" colspan="1">10-fold CV</td>
                <td align="center" rowspan="1" colspan="1">Pz-Oz</td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <underline>77.56</underline>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <underline>70.00</underline>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <underline>68.94</underline>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <underline>90.26</underline>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <underline>42.21</underline>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <underline>79.71</underline>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <underline>94.83</underline>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <underline>72.19</underline>
                </td>
              </tr>
            </tbody>
          </table>
        </alternatives>
        <table-wrap-foot>
          <fn id="t004fn001">
            <p>Sleep-EDF-13: Sleep-EDF 2013; Sleep-EDF-18: Sleep-EDF 2018; CV: Cross Validation</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>Typically, there are two approaches to evaluate the proposed methods in the literature: (i) intra-patient paradigm in which the training and evaluation sets can include epochs from the same subjects, and (ii) inter-patient paradigm in which the epochs sets for test and training come from different individuals. As the inter-patient scheme seems to be a more realistic evaluation mechanism, the results and comparisons presented in this study are based on the inter-patient paradigm. <xref rid="pone.0216456.t004" ref-type="table">Table 4</xref> presents the comparison of stage sleep scoring results for the proposed method with the existing algorithms. It can be noted from <xref rid="pone.0216456.t004" ref-type="table">Table 4</xref> that the proposed model outperformed the state-of-the-art algorithms presented in the table. Our model has performed better in all listed channels (i.e., the Fpz-Cz and the Pz-Oz EEG channels) in terms of all evaluation metrics compared to others. According to <xref rid="pone.0216456.t004" ref-type="table">Table 4</xref>, the results for Fpz-Cz channel are better than Pz-Oz channel. The reason is that Fpz-Cz channel position captures most of the frequencies including delta activity, K-complexes, lower frequency sleep spindles (predominantly frontal phenomena) that are important for sleep staging. However, Pz-Oz channel position extracts Theta activity and higher frequency sleep spindles (predominantly parietal phenomena) [<xref rid="pone.0216456.ref018" ref-type="bibr">18</xref>].</p>
      <p>Furthermore, it may be noted that in spite of the imbalance-class problem, our model yielded desirable performance, especially for stage N1. In addition to the Sleep-EDF 2013 dataset, we also evaluated our model with the Sleep-EDF 2018 dataset. Since the dataset has been published recently, we could not find any work to compare the performance of our model. Therefore, we just reported our findings without any comparison.</p>
      <p><xref ref-type="fig" rid="pone.0216456.g004">Fig 4a</xref> (left) shows the performance graph of the accuracy. It is shown that the model can offer a comparable performance on both training and test sets. Also, we can see that the test accuracy is greater than the training accuracy meaning the network has generalized very well. <xref ref-type="fig" rid="pone.0216456.g004">Fig 4b</xref> (right) illustrates the performance graphs of the loss function. From <xref ref-type="fig" rid="pone.0216456.g004">Fig 4b</xref> (right), we can see that the loss curves grow constantly at the final epochs. This means that we should stop training.</p>
      <fig id="pone.0216456.g004" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0216456.g004</object-id>
        <label>Fig 4</label>
        <caption>
          <title>Graphs of the performance of the accuracy (a) and the loss function (b) of the proposed model in each epoch for a randomly selected fold (i.e., the fold 4).</title>
        </caption>
        <graphic xlink:href="pone.0216456.g004"/>
      </fig>
      <p><xref ref-type="fig" rid="pone.0216456.g005">Fig 5</xref> illustrates the hypnogram produced manually by a sleep expert and its corresponding hypnogram generated by our method for a subject for approximately 8 hours of sleep at night. It can be noted from the figure that around 85% the manually scored hypnogram and automatically scored correctly.</p>
      <fig id="pone.0216456.g005" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0216456.g005</object-id>
        <label>Fig 5</label>
        <caption>
          <title>A example of hypnograms generated by the machine (i.e., the proposed method) and a sleep expert of a subject from the Sleep-EDF-13 dataset; approximately 85% coverage.</title>
        </caption>
        <graphic xlink:href="pone.0216456.g005"/>
      </fig>
      <p>Furthermore, by employing the attention mechanism into the network, we are able to illustrate (in the form of attention maps) which input epochs are important to score the sleep stages. As shown in <xref ref-type="fig" rid="pone.0216456.g006">Fig 6</xref>, we can see the network used almost the exact input epoch to predict its corresponding sleep stage (on the diagonal of each figure). The higher brightness indicates more attention (i.e., the amount of each square brightness shows the importance of its corresponding input (on the x-axis in the figure)) to generate each sleep stage. For example, <xref ref-type="fig" rid="pone.0216456.g006">Fig 6a</xref> (left) shows that to score the input epoch 8 (on the x-axis) the important epochs are its previous (epoch 7, N1) and next (epoch 9, N2) epochs, and specially its epoch as the corresponding square in the attention map is brighter.</p>
      <fig id="pone.0216456.g006" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0216456.g006</object-id>
        <label>Fig 6</label>
        <caption>
          <title>Attention maps of two sequence inputs (EEG epochs) and their corresponding sleep stage scores provided by our proposed method.</title>
        </caption>
        <graphic xlink:href="pone.0216456.g006"/>
      </fig>
      <p>Our model has performed better than the rest of the works due to the following two reasons: First, the nature of the sleep stage scoring task is sequential in which each sleep stage has a relationship with the previous and next stage. Hence, applying a sequence to sequence deep learning model for such a problem would be a desirable choice. Also, using the attention model and BiRNNs as the building blocks of the sequence to sequence model enhanced the performance. Second, the sleep stage classification suffers from the imbalance-class problem. To reduce the effect of this problem, we applied new loss functions (i.e., the MFE and MSFE) to have an equal misclassified error effect for each sleep stage while training the network.</p>
      <p>One of the remarkable aspects of our proposed method is that, the model is generic in nature hence it generalizes for other problems in the biomedical signal processing applications that are inherently sequential and have the imbalance-class problem such as the heartbeat classification for arrhythmia detection [<xref rid="pone.0216456.ref029" ref-type="bibr">29</xref>, <xref rid="pone.0216456.ref030" ref-type="bibr">30</xref>].</p>
      <p>Even though our proposed model achieved significant results compared to the existing methods for the sleep stage classification, the model still carries several limitations. First, similar to other deep learning methods, our method needs a sufficient amount of sleep stage samples in training phase to learn discriminative features of each stage. Second, as our model is a sequence to sequence approach, at each time step, it requires to have a certain amount of 30-s EEG epochs (as input sequence) to be able to score the input epochs. Finally, our proposed method is evaluated with two available EEG channels (i.e., Fpz-Cz and Pz-Oz EEG channels) extracted from the Physionet Sleep-EDF datasets. Therefore, to evaluate its performance on other EEG channels, the network has to be trained with new EEG epochs.</p>
      <p>Furthermore, in future, we intend to extend this work using multimodal polysomnography (PSG) signals including EEG, EOG (electrooculography) and EMG (electromyogram) to boost the performance of the sleep stage classification.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec013">
    <title>Conclusion</title>
    <p>We have presented a novel algorithm for automated sleep stage annotation problem. The proposed method leverages the ability of deep convolutional neural network and encoder-decoder network in which we have used bidirectional recurrent neural networks and attention mechanisms as its building blocks. The proposed new loss calculation approaches helped to reduce the effect of the class-imbalance problem and boost the performance, especially the performance of our method on the stage N1, that is more difficult than other sleep stages to score. <xref rid="pone.0216456.t004" ref-type="table">Table 4</xref> presents that, our proposed model significantly outperformed the existing algorithms by yielding the highest performance for the sleep stage scoring task. While developing the automated systems, generally there will be imbalance data problem (normal class more data than diseased class). Our developed model can be applied to such biomedical applications such as arrhythmia detection using ECG signals, epilepsy detection using EEG signals and EMG signals to study the postures.</p>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="pone.0216456.ref001">
      <label>1</label>
      <mixed-citation publication-type="other">Biswal S, Kulas J, Sun H, Goparaju B, Westover MB, Bianchi MT, et al. SLEEPNET: automated sleep staging system via deep learning. arXiv preprint arXiv:170708262. 2017;.</mixed-citation>
    </ref>
    <ref id="pone.0216456.ref002">
      <label>2</label>
      <mixed-citation publication-type="journal"><name><surname>Acharya</surname><given-names>UR</given-names></name>, <name><surname>Oh</surname><given-names>SL</given-names></name>, <name><surname>Hagiwara</surname><given-names>Y</given-names></name>, <name><surname>Tan</surname><given-names>JH</given-names></name>, <name><surname>Adeli</surname><given-names>H</given-names></name>. <article-title>Deep convolutional neural network for the automated detection and diagnosis of seizure using EEG signals</article-title>. <source>Computers in biology and medicine</source>. <year>2018</year>;<volume>100</volume>:<fpage>270</fpage>–<lpage>278</lpage>. <pub-id pub-id-type="doi">10.1016/j.compbiomed.2017.09.017</pub-id><?supplied-pmid 28974302?><pub-id pub-id-type="pmid">28974302</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216456.ref003">
      <label>3</label>
      <mixed-citation publication-type="other">Berry RB, Brooks R, Gamaldo CE, Harding SM, Marcus C, Vaughn B, et al. The AASM manual for the scoring of sleep and associated events. Rules, Terminology and Technical Specifications, Darien, Illinois, American Academy of Sleep Medicine. 2012;.</mixed-citation>
    </ref>
    <ref id="pone.0216456.ref004">
      <label>4</label>
      <mixed-citation publication-type="other">Rechtschaffen A. A manual for standardized terminology, techniques and scoring system for sleep stages in human subjects. Brain information service. 1968;.</mixed-citation>
    </ref>
    <ref id="pone.0216456.ref005">
      <label>5</label>
      <mixed-citation publication-type="journal"><name><surname>Zafar</surname><given-names>R</given-names></name>, <name><surname>Dass</surname><given-names>SC</given-names></name>, <name><surname>Malik</surname><given-names>AS</given-names></name>. <article-title>Electroencephalogram-based decoding cognitive states using convolutional neural network and likelihood ratio based score fusion</article-title>. <source>PloS one</source>. <year>2017</year>;<volume>12</volume>(<issue>5</issue>):<fpage>e0178410</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0178410</pub-id><?supplied-pmid 28558002?><pub-id pub-id-type="pmid">28558002</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216456.ref006">
      <label>6</label>
      <mixed-citation publication-type="other">Zaeri-Amirani M, Afghah F, Mousavi S. A Feature Selection Method Based on Shapley Value to False Alarm Reduction in ICUs A Genetic-Algorithm Approach. In: 2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC); 2018. p. 319–323.</mixed-citation>
    </ref>
    <ref id="pone.0216456.ref007">
      <label>7</label>
      <mixed-citation publication-type="journal"><name><surname>Afghah</surname><given-names>F</given-names></name>., <name><surname>Razi</surname><given-names>A</given-names></name>., <name><surname>Soroushmehr</surname><given-names>R</given-names></name>., <name><surname>Ghanbari</surname><given-names>H</given-names></name>., <name><surname>Najarian</surname><given-names>K</given-names></name>. <article-title>Game Theoretic Approach for Systematic Feature Selection; Application in False Alarm Detection in Intensive Care Units</article-title>. <source>Entropy</source>. <year>2018</year>; <volume>3</volume>(<issue>190</issue>).</mixed-citation>
    </ref>
    <ref id="pone.0216456.ref008">
      <label>8</label>
      <mixed-citation publication-type="journal"><name><surname>Koley</surname><given-names>B</given-names></name>, <name><surname>Dey</surname><given-names>D</given-names></name>. <article-title>An ensemble system for automatic sleep stage classification using single channel EEG signal</article-title>. <source>Computers in biology and medicine</source>. <year>2012</year>;<volume>42</volume>(<issue>12</issue>):<fpage>1186</fpage>–<lpage>1195</lpage>. <pub-id pub-id-type="doi">10.1016/j.compbiomed.2012.09.012</pub-id><?supplied-pmid 23102750?><pub-id pub-id-type="pmid">23102750</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216456.ref009">
      <label>9</label>
      <mixed-citation publication-type="journal"><name><surname>Fraiwan</surname><given-names>L</given-names></name>, <name><surname>Lweesy</surname><given-names>K</given-names></name>, <name><surname>Khasawneh</surname><given-names>N</given-names></name>, <name><surname>Wenz</surname><given-names>H</given-names></name>, <name><surname>Dickhaus</surname><given-names>H</given-names></name>. <article-title>Automated sleep stage identification system based on time–frequency analysis of a single EEG channel and random forest classifier</article-title>. <source>Computer methods and programs in biomedicine</source>. <year>2012</year>;<volume>108</volume>(<issue>1</issue>):<fpage>10</fpage>–<lpage>19</lpage>. <pub-id pub-id-type="doi">10.1016/j.cmpb.2011.11.005</pub-id><?supplied-pmid 22178068?><pub-id pub-id-type="pmid">22178068</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216456.ref010">
      <label>10</label>
      <mixed-citation publication-type="journal"><name><surname>Hsu</surname><given-names>YL</given-names></name>, <name><surname>Yang</surname><given-names>YT</given-names></name>, <name><surname>Wang</surname><given-names>JS</given-names></name>, <name><surname>Hsu</surname><given-names>CY</given-names></name>. <article-title>Automatic sleep stage recurrent neural classifier using energy features of EEG signals</article-title>. <source>Neurocomputing</source>. <year>2013</year>;<volume>104</volume>:<fpage>105</fpage>–<lpage>114</lpage>. <pub-id pub-id-type="doi">10.1016/j.neucom.2012.11.003</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216456.ref011">
      <label>11</label>
      <mixed-citation publication-type="other">Sutskever I, Vinyals O, Le QV. Sequence to sequence learning with neural networks. In: Advances in neural information processing systems; 2014. p. 3104–3112.</mixed-citation>
    </ref>
    <ref id="pone.0216456.ref012">
      <label>12</label>
      <mixed-citation publication-type="other">Mousavi SS, Schukat M, Howley E. Deep reinforcement learning: an overview. In: Proceedings of SAI Intelligent Systems Conference. Springer; 2016. p. 426–440.</mixed-citation>
    </ref>
    <ref id="pone.0216456.ref013">
      <label>13</label>
      <mixed-citation publication-type="other">Mousavi S, Schukat M, Howley E, Borji A, Mozayani N. Learning to predict where to look in interactive environments using deep recurrent q-learning. arXiv preprint arXiv:161205753. 2016;.</mixed-citation>
    </ref>
    <ref id="pone.0216456.ref014">
      <label>14</label>
      <mixed-citation publication-type="journal"><name><surname>Mousavi</surname><given-names>SS</given-names></name>, <name><surname>Schukat</surname><given-names>M</given-names></name>, <name><surname>Howley</surname><given-names>E</given-names></name>. <article-title>Traffic light control using deep policy-gradient and value-function-based reinforcement learning</article-title>. <source>IET Intelligent Transport Systems</source>. <year>2017</year>;<volume>11</volume>(<issue>7</issue>):<fpage>417</fpage>–<lpage>423</lpage>. <pub-id pub-id-type="doi">10.1049/iet-its.2017.0153</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216456.ref015">
      <label>15</label>
      <mixed-citation publication-type="journal"><name><surname>Goldberger</surname><given-names>AL</given-names></name>, <name><surname>Amaral</surname><given-names>LA</given-names></name>, <name><surname>Glass</surname><given-names>L</given-names></name>, <name><surname>Hausdorff</surname><given-names>JM</given-names></name>, <name><surname>Ivanov</surname><given-names>PC</given-names></name>, <name><surname>Mark</surname><given-names>RG</given-names></name>, <etal>et al</etal><article-title>PhysioBank, PhysioToolkit, and PhysioNet: components of a new research resource for complex physiologic signals</article-title>. <source>Circulation</source>. <year>2000</year>;<volume>101</volume>(<issue>23</issue>):<fpage>e215</fpage>–<lpage>e220</lpage>. <pub-id pub-id-type="doi">10.1161/01.CIR.101.23.e215</pub-id><?supplied-pmid 10851218?><pub-id pub-id-type="pmid">10851218</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216456.ref016">
      <label>16</label>
      <mixed-citation publication-type="journal"><name><surname>Yildirim</surname><given-names>O</given-names></name>, <name><surname>Baloglu</surname><given-names>UB</given-names></name>, <name><surname>Acharya</surname><given-names>UR</given-names></name>. <article-title>A Deep Learning Model for Automated Sleep Stages Classification Using PSG Signals</article-title>. <source>International Journal of Environmental Research and Public Health</source>. <year>2019</year>;<volume>16</volume>(<issue>4</issue>):<fpage>599</fpage><pub-id pub-id-type="doi">10.3390/ijerph16040599</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216456.ref017">
      <label>17</label>
      <mixed-citation publication-type="journal"><name><surname>Supratak</surname><given-names>A</given-names></name>, <name><surname>Dong</surname><given-names>H</given-names></name>, <name><surname>Wu</surname><given-names>C</given-names></name>, <name><surname>Guo</surname><given-names>Y</given-names></name>. <article-title>DeepSleepNet: a model for automatic sleep stage scoring based on raw single-channel EEG</article-title>. <source>IEEE Transactions on Neural Systems and Rehabilitation Engineering</source>. <year>2017</year>;<volume>25</volume>(<issue>11</issue>):<fpage>1998</fpage>–<lpage>2008</lpage>. <pub-id pub-id-type="doi">10.1109/TNSRE.2017.2721116</pub-id><?supplied-pmid 28678710?><pub-id pub-id-type="pmid">28678710</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216456.ref018">
      <label>18</label>
      <mixed-citation publication-type="journal"><name><surname>Tsinalis</surname><given-names>O</given-names></name>, <name><surname>Matthews</surname><given-names>PM</given-names></name>, <name><surname>Guo</surname><given-names>Y</given-names></name>. <article-title>Automatic sleep stage scoring using time-frequency analysis and stacked sparse autoencoders</article-title>. <source>Annals of biomedical engineering</source>. <year>2016</year>;<volume>44</volume>(<issue>5</issue>):<fpage>1587</fpage>–<lpage>1597</lpage>. <pub-id pub-id-type="doi">10.1007/s10439-015-1444-y</pub-id><?supplied-pmid 26464268?><pub-id pub-id-type="pmid">26464268</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216456.ref019">
      <label>19</label>
      <mixed-citation publication-type="journal"><name><surname>Michielli</surname><given-names>N</given-names></name>, <name><surname>Acharya</surname><given-names>UR</given-names></name>, <name><surname>Molinari</surname><given-names>F</given-names></name>. <article-title>Cascaded LSTM recurrent neural network for automated sleep stage classification using single-channel EEG signals</article-title>. <source>Computers in biology and medicine</source>. <year>2019</year>;. <pub-id pub-id-type="doi">10.1016/j.compbiomed.2019.01.013</pub-id><?supplied-pmid 30685634?><pub-id pub-id-type="pmid">30685634</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216456.ref020">
      <label>20</label>
      <mixed-citation publication-type="other">Tuck. Stages of Sleep and Sleep Cycles; 2018. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.tuck.com/stages/">https://www.tuck.com/stages/</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0216456.ref021">
      <label>21</label>
      <mixed-citation publication-type="other">Johnson M, Schuster M, Le QV, Krikun M, Wu Y, Chen Z, et al. Google’s multilingual neural machine translation system: enabling zero-shot translation. arXiv preprint arXiv:161104558. 2016;.</mixed-citation>
    </ref>
    <ref id="pone.0216456.ref022">
      <label>22</label>
      <mixed-citation publication-type="book"><name><surname>Cohen</surname><given-names>MX</given-names></name>. <source>Analyzing neural time series data: theory and practice</source>. <publisher-name>MIT press</publisher-name>; <year>2014</year>.</mixed-citation>
    </ref>
    <ref id="pone.0216456.ref023">
      <label>23</label>
      <mixed-citation publication-type="other">Fernandez R, Rendel A, Ramabhadran B, Hoory R. Using deep bidirectional recurrent neural networks for prosodic-target prediction in a unit-selection text-to-speech system. In: Sixteenth Annual Conference of the International Speech Communication Association; 2015.</mixed-citation>
    </ref>
    <ref id="pone.0216456.ref024">
      <label>24</label>
      <mixed-citation publication-type="journal"><name><surname>Schuster</surname><given-names>M</given-names></name>, <name><surname>Paliwal</surname><given-names>KK</given-names></name>. <article-title>Bidirectional recurrent neural networks</article-title>. <source>IEEE Transactions on Signal Processing</source>. <year>1997</year>;<volume>45</volume>(<issue>11</issue>):<fpage>2673</fpage>–<lpage>2681</lpage>. <pub-id pub-id-type="doi">10.1109/78.650093</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216456.ref025">
      <label>25</label>
      <mixed-citation publication-type="other">Wang S, Liu W, Wu J, Cao L, Meng Q, Kennedy PJ. Training deep neural networks on imbalanced data sets. In: Neural Networks (IJCNN), 2016 International Joint Conference on. IEEE; 2016. p. 4368–4374.</mixed-citation>
    </ref>
    <ref id="pone.0216456.ref026">
      <label>26</label>
      <mixed-citation publication-type="journal"><name><surname>Kemp</surname><given-names>B</given-names></name>, <name><surname>Zwinderman</surname><given-names>AH</given-names></name>, <name><surname>Tuk</surname><given-names>B</given-names></name>, <name><surname>Kamphuisen</surname><given-names>HA</given-names></name>, <name><surname>Oberye</surname><given-names>JJ</given-names></name>. <article-title>Analysis of a sleep-dependent neuronal feedback loop: the slow-wave microcontinuity of the EEG</article-title>. <source>IEEE Transactions on Biomedical Engineering</source>. <year>2000</year>;<volume>47</volume>(<issue>9</issue>):<fpage>1185</fpage>–<lpage>1194</lpage>. <pub-id pub-id-type="doi">10.1109/10.867928</pub-id><?supplied-pmid 11008419?><pub-id pub-id-type="pmid">11008419</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216456.ref027">
      <label>27</label>
      <mixed-citation publication-type="journal"><name><surname>Chawla</surname><given-names>NV</given-names></name>, <name><surname>Bowyer</surname><given-names>KW</given-names></name>, <name><surname>Hall</surname><given-names>LO</given-names></name>, <name><surname>Kegelmeyer</surname><given-names>WP</given-names></name>. <article-title>SMOTE: synthetic minority over-sampling technique</article-title>. <source>Journal of artificial intelligence research</source>. <year>2002</year>;<volume>16</volume>:<fpage>321</fpage>–<lpage>357</lpage>. <pub-id pub-id-type="doi">10.1613/jair.953</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216456.ref028">
      <label>28</label>
      <mixed-citation publication-type="other">Tsinalis O, Matthews PM, Guo Y, Zafeiriou S. Automatic sleep stage scoring with single-channel EEG using convolutional neural networks. arXiv preprint arXiv:161001683. 2016;.</mixed-citation>
    </ref>
    <ref id="pone.0216456.ref029">
      <label>29</label>
      <mixed-citation publication-type="other">Mousavi S, Afghah F. Inter-and intra-patient ECG heartbeat classification for arrhythmia detection: a sequence to sequence deep learning approach. ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE; 2019;pp. 1308–1312.</mixed-citation>
    </ref>
    <ref id="pone.0216456.ref030">
      <label>30</label>
      <mixed-citation publication-type="other">Mousavi S, Afghah F, Razi A, Acharya UR. ECGNET: Learning where to attend for detection of atrial fibrillation with deep visual attention. arXiv preprint arXiv:181207422. 2018, to appear in IEEE BHI 2019;.</mixed-citation>
    </ref>
  </ref-list>
</back>
