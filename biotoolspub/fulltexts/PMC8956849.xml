<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?subarticle report123174?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">F1000Res</journal-id>
    <journal-id journal-id-type="iso-abbrev">F1000Res</journal-id>
    <journal-title-group>
      <journal-title>F1000Research</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2046-1402</issn>
    <publisher>
      <publisher-name>F1000 Research Limited</publisher-name>
      <publisher-loc>London, UK</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8956849</article-id>
    <article-id pub-id-type="doi">10.12688/f1000research.107925.1</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software Tool Article</subject>
      </subj-group>
      <subj-group>
        <subject>Articles</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>NIFtHool: an informatics program for identification of NifH proteins using deep neural networks</article-title>
      <fn-group content-type="pub-status">
        <fn>
          <p>[version 1; peer review: 2 approved]</p>
        </fn>
      </fn-group>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Suquilanda-Pesántez</surname>
          <given-names>Jefferson Daniel</given-names>
        </name>
        <role content-type="http://credit.niso.org/">Data Curation</role>
        <role content-type="http://credit.niso.org/">Formal Analysis</role>
        <role content-type="http://credit.niso.org/">Investigation</role>
        <role content-type="http://credit.niso.org/">Methodology</role>
        <role content-type="http://credit.niso.org/">Writing – Original Draft Preparation</role>
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5655-5038</contrib-id>
        <xref rid="a1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Aguiar Salazar</surname>
          <given-names>Evelyn Dayana</given-names>
        </name>
        <role content-type="http://credit.niso.org/">Data Curation</role>
        <role content-type="http://credit.niso.org/">Formal Analysis</role>
        <role content-type="http://credit.niso.org/">Investigation</role>
        <role content-type="http://credit.niso.org/">Methodology</role>
        <role content-type="http://credit.niso.org/">Writing – Original Draft Preparation</role>
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-4306-6216</contrib-id>
        <xref rid="a1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Almeida-Galárraga</surname>
          <given-names>Diego</given-names>
        </name>
        <role content-type="http://credit.niso.org/">Methodology</role>
        <role content-type="http://credit.niso.org/">Validation</role>
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-9196-335X</contrib-id>
        <xref rid="a1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Salum</surname>
          <given-names>Graciela</given-names>
        </name>
        <role content-type="http://credit.niso.org/">Methodology</role>
        <role content-type="http://credit.niso.org/">Validation</role>
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-0585-5377</contrib-id>
        <xref rid="a1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Villalba-Meneses</surname>
          <given-names>Fernando</given-names>
        </name>
        <role content-type="http://credit.niso.org/">Conceptualization</role>
        <role content-type="http://credit.niso.org/">Investigation</role>
        <role content-type="http://credit.niso.org/">Project Administration</role>
        <role content-type="http://credit.niso.org/">Resources</role>
        <role content-type="http://credit.niso.org/">Supervision</role>
        <role content-type="http://credit.niso.org/">Validation</role>
        <role content-type="http://credit.niso.org/">Writing – Review &amp; Editing</role>
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-7236-7499</contrib-id>
        <xref rid="c1" ref-type="corresp">a</xref>
        <xref rid="a1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Gudiño Gomezjurado</surname>
          <given-names>Marco Esteban</given-names>
        </name>
        <role content-type="http://credit.niso.org/">Conceptualization</role>
        <role content-type="http://credit.niso.org/">Investigation</role>
        <role content-type="http://credit.niso.org/">Project Administration</role>
        <role content-type="http://credit.niso.org/">Resources</role>
        <role content-type="http://credit.niso.org/">Supervision</role>
        <role content-type="http://credit.niso.org/">Validation</role>
        <role content-type="http://credit.niso.org/">Writing – Review &amp; Editing</role>
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-8609-0806</contrib-id>
        <xref rid="c2" ref-type="corresp">b</xref>
        <xref rid="a1" ref-type="aff">1</xref>
      </contrib>
      <aff id="a1"><label>1</label>Escuela de Ciencias Biológicas e Ingeniería, Universidad de Investigación de Tecnología Experimental Yachay, Urcuquí, Imbabura, 100115, Ecuador</aff>
    </contrib-group>
    <author-notes>
      <corresp id="c1">
        <label>a</label>
        <email xlink:href="mailto:gvillalba@yachaytech.edu.ec">gvillalba@yachaytech.edu.ec</email>
      </corresp>
      <corresp id="c2">
        <label>b</label>
        <email xlink:href="mailto:mgudino@yachaytech.edu.ec">mgudino@yachaytech.edu.ec</email>
      </corresp>
      <fn fn-type="COI-statement">
        <p>No competing interests were disclosed.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>9</day>
      <month>2</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>11</volume>
    <elocation-id>164</elocation-id>
    <history>
      <date date-type="accepted">
        <day>4</day>
        <month>2</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright: © 2022 Suquilanda-Pesántez JD et al.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the Creative Commons Attribution Licence, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="f1000research-11-119185.pdf"/>
    <abstract>
      <p>Atmospheric nitrogen fixation carried out by microorganisms has environmental and industrial importance, related to the increase of soil fertility and productivity. The present work proposes the development of a new high precision system that allows the recognition of amino acid sequences of the nitrogenase enzyme (NifH) as a promising way to improve the identification of diazotrophic bacteria. For this purpose, a database obtained from UniProt built a processed dataset formed by a set of 4911 and 4782 amino acid sequences of the NifH and non-NifH
proteins respectively. Subsequently, the feature extraction was developed using two methodologies: (i) k-mers counting and (ii) embedding layers to obtain numerical vectors of the amino acid chains. Afterward, for the embedding layer, the data was crossed by an external trainable convolutional layer, which received a uniform matrix and applied convolution using filters to obtain the feature maps of the model. Finally, a deep neural network was used as the primary model to classify the amino acid sequences as NifH protein or not. Performance evaluation experiments were carried out, and the results revealed an accuracy of 96.4%, a sensitivity of 95.2%, and a specificity of 96.7%. Therefore, an amino acid sequence-based feature extraction method that uses a neural network to detect N-fixing organisms is proposed and implemented. NIFtHool is available from:
<ext-link xlink:href="https://nifthool.anvil.app/" ext-link-type="uri">https://nifthool.anvil.app/</ext-link>
</p>
    </abstract>
    <kwd-group kwd-group-type="author">
      <kwd>Deep Neural Network</kwd>
      <kwd>Embedding Layers</kwd>
      <kwd>NifH protein</kwd>
      <kwd>Software</kwd>
      <kwd>k-mers</kwd>
    </kwd-group>
    <funding-group>
      <funding-statement>The author(s) declared that no grants were involved in supporting this work.</funding-statement>
    </funding-group>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="sec1">
    <title>Introduction</title>
    <p>Nitrogen is an essential nutrient for plants. Nitrogen fertilizers have the highest worldwide demand during agricultural practices.
<sup><xref rid="ref1" ref-type="bibr">1</xref></sup>
<sup>,</sup>
<sup><xref rid="ref2" ref-type="bibr">2</xref></sup> Among the vast diversity of microorganisms, different bacterial taxa have developed the capacity to use atmospheric nitrogen as a substrate to produce ammonia (NH
<sub>4</sub>) through the biological nitrogen fixation (BNF) process.
<sup><xref rid="ref3" ref-type="bibr">3</xref></sup> BNF is the most critical pathway of incorporating N-NH
<sub>4</sub> inside the biosphere.
<sup><xref rid="ref4" ref-type="bibr">4</xref></sup> Estimating that only 10% of the total nitrogen incomes proceed from atmospheric precipitation, the rest is through this biological process.
<sup><xref rid="ref5" ref-type="bibr">5</xref></sup>
</p>
    <p>The activity of the nitrogenase enzyme carry out the BNF. This enzyme is a molecular complex constituted by two subunits: (i) the dinitrogenase reductase (NifH), which is an iron protein that participates in the electrons transport from ferredoxins, to the (ii) dinitrogenase or molybdenum-iron-protein.
<sup><xref rid="ref3" ref-type="bibr">3</xref></sup>
<sup>,</sup>
<sup><xref rid="ref6" ref-type="bibr">6</xref></sup> Molybdenum-iron-protein (Mo-Fe) is the catalytic site, which catalyses the N
<sub>2</sub> reduction using 16 ATP molecules as an energy source.
<sup><xref rid="ref7" ref-type="bibr">7</xref></sup> Both units are encoded in the
<italic toggle="yes">nifHDK</italic> operon (genes encoding for the Fe/Mo-Fe nitrogenase protein complex) located on the bacterial chromosome or plasmids, depending on the bacterial species
<sup><xref rid="ref8" ref-type="bibr">8</xref></sup> (
<xref rid="f1" ref-type="fig">Figure 1</xref>). The principal role of NifH protein is donating electrons to molybdenum-iron proteins (NifD/NifK), favouring N
<sub>2</sub> reduction. Efficient and quick identification of NifH proteins is of relevant interest because Nitrogen engineering focuses on the development of new products during farming activities.
<sup><xref rid="ref9" ref-type="bibr">9</xref></sup>
</p>
    <fig position="float" fig-type="figure" id="f1">
      <label>Figure 1. </label>
      <caption>
        <title>Nitrogenase complex.</title>
        <p>The
<italic toggle="yes">nifHDK</italic> operon (genes encoding for the Fe/Mo-Fe nitrogenase protein complex) codifies for the subunits of the nitrogenase enzyme, which catalyzes the reduction of the N
<sub>2</sub> to NH
<sub>4</sub> in an ATP- dependent manner through the electron flux from the dinitrogenase reductase to the molybdenum-iron (Mo-Fe) protein subunit. Modified from Ref.
<xref rid="ref10" ref-type="bibr">10</xref>.</p>
      </caption>
      <graphic xlink:href="f1000research-11-119185-g0000" id="gr1" position="float"/>
    </fig>
    <p>Previous studies have focused on developing new informatics tools to identify
<italic toggle="yes">nifH</italic> genes from different bacterial genera. Frank
<italic toggle="yes">et al</italic>. (2016) retrieved the sequences of the
<italic toggle="yes">nifH</italic> genes in the genome of nitrogen-fixing microorganisms and achieved the classification of the nifH sequence at different clusters.
<sup><xref rid="ref11" ref-type="bibr">11</xref></sup> Likewise, Shinde
<italic toggle="yes">et al</italic>. (2019) developed his
<italic toggle="yes">nifH</italic> genes classification model based on image processing and convolutional neural network.
<sup><xref rid="ref12" ref-type="bibr">12</xref></sup> On the other hand, Frank (2014) designed a tool with sufficient information to carry out phylogenetic cluster membership predictions from 32954 NifH protein sequences.
<sup><xref rid="ref13" ref-type="bibr">13</xref></sup> These three studies obtained good results. However, their investigations did not produce a computer tool.</p>
    <p>Meher
<italic toggle="yes">et al</italic>. (2018) developed
<ext-link xlink:href="http://webapp.cabgrid.res.in/nifPred/dataset.html" ext-link-type="uri">nifPred</ext-link>, a machine learning (ML) software, to perform a sequence classification into NifH or non-NifH proteins. This informatics tool converts multi-categorical sort of gene sequences into one of the six types of the Nif proteins encoded by the
<italic toggle="yes">nif</italic> operon using a high computational performance.
<sup><xref rid="ref14" ref-type="bibr">14</xref></sup>
</p>
    <p>Constant supervision is necessary to guide the program in all phases of the system, which causes the increment of computational cost.
<sup><xref rid="ref15" ref-type="bibr">15</xref></sup> Nowadays, some algorithms are registered in literature to make predictions of NifH proteins based on gene sequences. However, there is still no tool to distinguish Nif proteins from amino acids sequences.</p>
    <p>Based on this background, the objective of this work was to develop an informatics tool that uses deep neural networks with the lowest computer cost to play an essential role in the improvement of the BNF process research through the identification of the NifH proteins among different bacterial genera in a reliable way.</p>
  </sec>
  <sec sec-type="methods" id="sec2">
    <title>Methods</title>
    <p>The proposed model was developed into five main stages: (i) acquisition of raw data from the UniProt protein databank
<sup><xref rid="ref16" ref-type="bibr">16</xref></sup> (Universal Protein Resource, RRID:SCR_002380); (ii) feature extraction stage, which allows defining numerical vectors as representation of amino acids (aa) sequences; (iii) development of a prediction model or classifier using a deep neural network (DNN); (iv) K-fold Cross-validation to evaluate the model and (v) the identification of the predicted label of the sequence (
<xref rid="f2" ref-type="fig">Figure 2</xref>).</p>
    <fig position="float" fig-type="figure" id="f2">
      <label>Figure 2. </label>
      <caption>
        <title>Description of the methodology applied in this work.</title>
        <p>i) Data acquisition, ii) Feature extraction, iii) Modeling of deep learning, iv) K-fold Cross-validation and v) prediction and providing information. Dinitrogenase reductase = NifH.</p>
      </caption>
      <graphic xlink:href="f1000research-11-119185-g0001" id="gr2" position="float"/>
    </fig>
    <p>Experimental processes were carried out using an Intel Core i5-8100 processor machine, feature extraction process and neural network code were written in Python v. 3.8 language (RRID:SCR_008394), and the classifier was implemented on
<ext-link xlink:href="https://anvil.works/" ext-link-type="uri">Anvil Works</ext-link>.
<sup><xref rid="ref17" ref-type="bibr">17</xref></sup>
</p>
    <sec id="sec3">
      <title>Acquisition and preprocessing of raw data</title>
      <p>The binary classification of the protein sequences was arranged under two types of labels: 1 (NifH) and 0 (non-NifH). The raw dataset was constructed by NifH and non_NifH protein sequences extracted from database UniProt
<sup><xref rid="ref18" ref-type="bibr">18</xref></sup> in the FASTA format up to March 10
<sup>th</sup> 2021.
<sup><xref rid="ref18" ref-type="bibr">18</xref></sup> NifH and non-NifH sequences were retrieved by searching for ‘NifH, nifH proteins’ and ‘non-NifH, non-nifH proteins’, respectively, considering the following parameters: organism identification, gen name, proteins names, and length. Uniprot provided 52942 NifH proteins and 5763 non-NifH at the end of the search.</p>
      <p>CD-HIT software (RRID:SCR_007105) analysed the raw dataset
<sup><xref rid="ref19" ref-type="bibr">19</xref></sup> to remove redundant sequences with 90% similarity to avoid undesirable biases. Next, positive, and negative sequences, 4939 and 4953, respectively, were obtained after the cleaning process. Sequences were filtered for lengths greater than 50 aa and shorter than 1173 aa, the maximum length of NifH sequence. These values were set up for two reasons: (i) the upper limit because non-nifH sequences are greater than 1173 aa and (ii) shorter sequences than a defined upper limit are padded with zeros until the limit is reached that guarantees the conversion of aa sequences into numeric vectors during the embedding layers (feature extraction stage). For instance, for a maximum value defined as 2000, a 125 aa sequence would have to be completed with 1875 zeros; for a sequence of 1750 aa, it would only be necessary to complete it with 250 zeros. The redundancy of a large amount of zeros can be a factor leading to undesirable bias.
<sup><xref rid="ref20" ref-type="bibr">20</xref></sup> Thus, 4911 NifH and 4782 non-NifH protein sequences were obtained after filtration of limits to build the final dataset of 9693 sequences (
<xref rid="f3" ref-type="fig">Figure 3</xref>).</p>
      <fig position="float" fig-type="figure" id="f3">
        <label>Figure 3. </label>
        <caption>
          <title>Correlation of the sequences number with the number of the amino acid (aa).</title>
          <p>The curves show the correlation between the number of sequences and the number of amino acids of dinitrogenase reductase (NifH) and non-NifH proteins.</p>
        </caption>
        <graphic xlink:href="f1000research-11-119185-g0002" id="gr3" position="float"/>
      </fig>
    </sec>
    <sec id="sec4">
      <title>Feature extraction</title>
      <p>This step results in 328-feature numeric vectors representing each of the sequences. The values were obtained through two different processes, k-mers, which are related to the presence of specific groups, and embedding vectors, related to the location of the amino acids in the sequence.</p>
      <p>
        <italic toggle="yes">k-mers</italic>
      </p>
      <p>This analysis allows the sequence representation based on the presence of specific groups called k-mers, where k is the length of that group.
<sup><xref rid="ref21" ref-type="bibr">21</xref></sup> For example, 5-mers represents a specific group of 5 amino acids. This stage begins with the generation of a list of the most common k-mers within the NifH dataset. Considering 5-mers, for each sequence of the dataset of 4939 NifH proteins, all the different 5-mers are obtained and their quantities are counted. Then, all the 5-mers in the entire dataset are identified and the k-mers with the highest frequency are defined, which form the general list (GL) of 5-mers where the order of each value is relevant. To generate a numerical vector for a certain sequence, GL is compared with that sequence and according to the presence of the GL 5-mers in the sequence, ‘1’ is recorded for presence and ‘0’ for absence according to the order of the GL
<sup><xref rid="ref22" ref-type="bibr">22</xref></sup> (
<xref rid="f4" ref-type="fig">Figure 4</xref>).</p>
      <fig position="float" fig-type="figure" id="f4">
        <label>Figure 4. </label>
        <caption>
          <title>Description of feature extraction.</title>
          <p>This process comprised some sequential steps: 1) Input of the amino acid (aa) sequences, 2) extraction, and 3) development of the convolutional layer. Where M is the length of the input sequence and L is the length of the embedding vector.</p>
        </caption>
        <graphic xlink:href="f1000research-11-119185-g0003" id="gr4" position="float"/>
      </fig>
      <p>For instance, in the sequence ‘GAHYTGGTPLNFH’ nine 5-mers can be identified, some examples of the 5-mers are GAHYT, AHYTG, HYTGG, PLNFH. If the GL of 5-mers were made up of 5 k-mers, for example: AGHLP, PLNFH, HJKLP, YTGGT and TGHHT the resulting vector for the example sequence would be [0, 1, 0, 1, 0]. The length of the vector is five because the length of the GL is five. According to the order, each GL 5-mer is searched in the sequence and if the 5-mer is present, 1 is recorded. In the example, the vector has ‘1’ at position 2 because PLNFH is present in the sequence and is the second most common of the GL.</p>
      <p>
        <italic toggle="yes">Embedding features</italic>
      </p>
      <p>This section was performed as described previously by Shadab
<italic toggle="yes">et al</italic>. (2020) and comprised the carrying on: (i) input sequences, (ii) embedding vectors, and (iii) convolutional layers
<sup><xref rid="ref23" ref-type="bibr">23</xref></sup> (
<xref rid="f4" ref-type="fig">Figure 4</xref>).</p>
      <p>
        <italic toggle="yes">Input sequences</italic>
      </p>
      <p>Protein sequences have different lengths depending on the number of aa. Deep learning requires the same length for all sequences.
<sup><xref rid="ref24" ref-type="bibr">24</xref></sup>
<sup>,</sup>
<sup><xref rid="ref25" ref-type="bibr">25</xref></sup> The maximum size determined from the data had a value of 1173. Each number sequence with a length less than this value goes through a filling process, and the sequence was filled with the number “0” (token) until the maximum size is achieved (
<xref rid="f4" ref-type="fig">Figure 4</xref>). These zero vectors do not affect the output of the subsequent layers, and M was defined as the length of the input sequence.
<sup><xref rid="ref23" ref-type="bibr">23</xref></sup>
<sup>,</sup>
<sup><xref rid="ref24" ref-type="bibr">24</xref></sup> In addition, the aa count per chain was performed, and this set presented a minimum of 50, a maximum of 1173, and an average of 278.43 (
<xref rid="f3" ref-type="fig">Figure 3</xref>).</p>
      <p>
        <italic toggle="yes">Embedding vectors</italic>
      </p>
      <p>Padded data were passed throughout an embedding layer to get a dense vector. This layer is used to transform discrete inputs into points in a vector space, called embedding vectors (L is the length of this vector).
<sup><xref rid="ref23" ref-type="bibr">23</xref></sup> Each aa of the protein sequences, both in the training and test sets, had a specific integer number, and the result of the encoding phase is the integer number vectors. For instance, in the sequence ‘ACLKIGAL’, a possible encoding would be ‘1-4-6-8-13-15-1-6’ (
<xref rid="f4" ref-type="fig">Figure 4</xref>). This algorithm randomly assigns the digits but maintains the same number for a specific aa. The order in which the numbers are assigned could be a relevant aspect regarding the model's performance. However, it has been proved that order does not influence the result.
<sup><xref rid="ref23" ref-type="bibr">23</xref></sup>
<sup>,</sup>
<sup><xref rid="ref25" ref-type="bibr">25</xref></sup> The final output of the embedding layers is a uniform matrix of size L×M.</p>
      <p>
        <italic toggle="yes">Convolutional layer</italic>
      </p>
      <p>A trainable convolutional layer was added, and its input was a uniform matrix (embedding vectors). This layer applied convolution using 128 trainable filters, each with a size of L×31. The result was 128 feature maps, each of the same size. To reduce overfitting and capture noise, we used a max-pooling (window size of 3×3) to subsample these feature maps.
<sup><xref rid="ref23" ref-type="bibr">23</xref></sup> Finally, this feature map was flattened into a 1×1 dimensional matrix, where each matrix represents the features of the input sequence (
<xref rid="f4" ref-type="fig">Figure 4</xref>). These features were used to train the classifiers.</p>
      <p>The advantage of this method relies on the results of the classification model that can be back-propagated to the convolution and embedding layers.
<sup><xref rid="ref23" ref-type="bibr">23</xref></sup>
<sup>,</sup>
<sup><xref rid="ref25" ref-type="bibr">25</xref></sup>
<sup>,</sup>
<sup><xref rid="ref26" ref-type="bibr">26</xref></sup> These layers were trained to extract better features.</p>
    </sec>
    <sec id="sec5">
      <title>Dataset methodologies</title>
      <p>This stage allows the determination of the the most efficient dataset to perform the training and evaluation of the neural network. For this purpose, different methodologies are used to create the datasets. Two techniques for feature extraction were considered, the methodologies will focus on the combination of these techniques and the number of features that are extracted. Due to the embedding vectors (EV) has 128 fixed features, the variation of dataset methodologies depends on the length of k-mers and number of k-mers in the GL. Some datasets were EV, 3-mers (100 features) + EV, 7-mers (300 features) + EV, 3-mers (100 f) + 5-mers (100 f) + EV, and so on. Some datasets were created by combining k-mers and EV, but others were created by combining two different k-mers and EV.
<xref rid="T2" ref-type="table">Table 2</xref> shows the different methodologies analysed. After a series of analysis with the dataset methodologies to determine the most optimal number of features to compose the numeric vectors the feature extraction stage results in vectors of 328 numeric values corresponding to 100 5-mer features, 100 features of 7-mers, and 128 values of embedding vectors (
<xref rid="f4" ref-type="fig">Figure 4</xref>).</p>
    </sec>
    <sec id="sec6">
      <title>Deep neural network</title>
      <p>For the prediction of the identity of the aa sequence, a Deep Neural Network (DNN) was designed. Its input corresponds to the representation of the sequences (array of 328 numbers) and the output is the class of each sequence: 1 (NifH) and 0 (non-NifH). This DNN was written in Python v.3.8 using the following libraries: i) Pandas (RRID:SCR_018214),
<sup><xref rid="ref27" ref-type="bibr">27</xref></sup> ii) Keras,
<sup><xref rid="ref28" ref-type="bibr">28</xref></sup> iii) Scikit-learn (RRID:SCR_002577),
<sup><xref rid="ref29" ref-type="bibr">29</xref></sup> iv) NumPy (RRID:SCR_008633),
<sup><xref rid="ref30" ref-type="bibr">30</xref></sup> and v) Matplolib.
<sup><xref rid="ref31" ref-type="bibr">31</xref></sup> First, the hyperparameters of our model were related to the learning algorithm level: training of 40 epochs at 6 seconds, using a batch size of 40 and a learning rate of 6x10-5. Second, hyperparameters related to structure and topology were the layers. The deep learning model consisted of 12 layers, excluding the input and output layers (
<xref rid="f5" ref-type="fig">Figure 5</xref>).</p>
      <fig position="float" fig-type="figure" id="f5">
        <label>Figure 5. </label>
        <caption>
          <title>Visualisation of the deep neural network architecture.</title>
          <p>It was composed of four blocks, and the number of neurons for each block was 128, 64, 32, and 2, from the first to last one, respectively.</p>
        </caption>
        <graphic xlink:href="f1000research-11-119185-g0004" id="gr5" position="float"/>
      </fig>
      <p>The model established the number of neurons in each defined block of layers, being 128 neurons for the first two layers, 64 neurons for the following four layers, followed by 32 neurons for the next four layers, and finally, two neurons at the last two. The number of neurons was placed according to the input parameters and the architecture of the DNN.
<sup><xref rid="ref32" ref-type="bibr">32</xref></sup> There were four layers in the neural network corresponding to the dense, activation, dropout, and batch normalization layers.
<sup><xref rid="ref26" ref-type="bibr">26</xref></sup>
<sup>,</sup>
<sup><xref rid="ref33" ref-type="bibr">33</xref></sup> Four layers were used: (i) four dense layers, (ii) three activation layers, (iii) three dropout Layer, and (iv) two batch normalisation layers. The detailed configuration and order of layers of the proposed DNN model are shown in
<xref rid="T1" ref-type="table">Table 1</xref>.</p>
      <table-wrap position="float" id="T1">
        <label>Table 1. </label>
        <caption>
          <title>Layers of the deep neural network implemented in this model.</title>
        </caption>
        <table frame="hsides" rules="groups" content-type="article-table">
          <thead>
            <tr>
              <th align="left" valign="top" rowspan="1" colspan="1">Layer</th>
              <th align="left" valign="top" rowspan="1" colspan="1">Type</th>
              <th align="left" valign="top" rowspan="1" colspan="1">Output shape</th>
              <th align="left" valign="top" rowspan="1" colspan="1">Param #</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">dense_108</td>
              <td align="left" valign="top" rowspan="1" colspan="1">Dense</td>
              <td align="left" valign="top" rowspan="1" colspan="1">(None, 128)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">42112</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">dropout_96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">Dropout</td>
              <td align="left" valign="top" rowspan="1" colspan="1">(None, 128)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">dense_109</td>
              <td align="left" valign="top" rowspan="1" colspan="1">Dense</td>
              <td align="left" valign="top" rowspan="1" colspan="1">(None, 64)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">8256</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">batch_normalization_56</td>
              <td align="left" valign="top" rowspan="1" colspan="1">Batch</td>
              <td align="left" valign="top" rowspan="1" colspan="1">(None, 64)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">256</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">activation_82</td>
              <td align="left" valign="top" rowspan="1" colspan="1">Activation</td>
              <td align="left" valign="top" rowspan="1" colspan="1">(None, 64)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">dropout_83</td>
              <td align="left" valign="top" rowspan="1" colspan="1">Dropout</td>
              <td align="left" valign="top" rowspan="1" colspan="1">(None, 64)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">dense_110</td>
              <td align="left" valign="top" rowspan="1" colspan="1">Dense</td>
              <td align="left" valign="top" rowspan="1" colspan="1">(None, 32)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">2080</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">batch_normalization_57</td>
              <td align="left" valign="top" rowspan="1" colspan="1">Batch</td>
              <td align="left" valign="top" rowspan="1" colspan="1">(None, 32)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">128</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">activation_83</td>
              <td align="left" valign="top" rowspan="1" colspan="1">Activation</td>
              <td align="left" valign="top" rowspan="1" colspan="1">(None, 32)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">dropout_84</td>
              <td align="left" valign="top" rowspan="1" colspan="1">Dropout</td>
              <td align="left" valign="top" rowspan="1" colspan="1">(None, 32)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">dense_111</td>
              <td align="left" valign="top" rowspan="1" colspan="1">Dense</td>
              <td align="left" valign="top" rowspan="1" colspan="1">(None, 2)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">66</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">activation_84</td>
              <td align="left" valign="top" rowspan="1" colspan="1">Activation</td>
              <td align="left" valign="top" rowspan="1" colspan="1">(None, 2)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="sec7">
      <title>k-fold cross validation</title>
      <p>To evaluate the classification accuracy of the several dataset methodologies considered to train the neural network (
<xref rid="T2" ref-type="table">Table 2</xref>), the k-fold cross-validation technique was performed. k-fold cross-validation divides datasets into k-subsets and requires that each subset is used to validate data exactly once.
<sup><xref rid="ref34" ref-type="bibr">34</xref></sup> In this validation process, k typically is 10, even though to reduce the computational time, this study used k = 4, giving 4-fold cross-validation (
<xref rid="f6" ref-type="fig">Figure 6</xref>).</p>
      <table-wrap position="float" id="T2">
        <label>Table 2. </label>
        <caption>
          <title>Performance metrics (precision, recall, F1-score, accuracy, and loss) calculated for each dataset methodology.</title>
        </caption>
        <table frame="hsides" rules="groups" content-type="article-table">
          <thead>
            <tr>
              <th align="left" rowspan="2" valign="bottom" colspan="1">Methodology</th>
              <th align="left" colspan="5" valign="top" rowspan="1">Statistical parameters (%)</th>
            </tr>
            <tr>
              <th align="left" valign="top" rowspan="1" colspan="1">Precision</th>
              <th align="left" valign="top" rowspan="1" colspan="1">Recall</th>
              <th align="left" valign="top" rowspan="1" colspan="1">F1-score</th>
              <th align="left" valign="top" rowspan="1" colspan="1">Accuracy</th>
              <th align="left" valign="top" rowspan="1" colspan="1">Loss</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">Embedding Vectors (EV)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">93</td>
              <td align="left" valign="top" rowspan="1" colspan="1">93</td>
              <td align="left" valign="top" rowspan="1" colspan="1">93</td>
              <td align="left" valign="top" rowspan="1" colspan="1">92.82</td>
              <td align="left" valign="top" rowspan="1" colspan="1">20.96</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">3-mers (100 features) + EV</td>
              <td align="left" valign="top" rowspan="1" colspan="1">95</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">95</td>
              <td align="left" valign="top" rowspan="1" colspan="1">95.46</td>
              <td align="left" valign="top" rowspan="1" colspan="1">17.34</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">3-mers (200 features) + EV</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">95.58</td>
              <td align="left" valign="top" rowspan="1" colspan="1">21.42</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">3-mers (300 features) + EV</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">95.71</td>
              <td align="left" valign="top" rowspan="1" colspan="1">19.32</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">5-mers (100 features) + EV</td>
              <td align="left" valign="top" rowspan="1" colspan="1">95</td>
              <td align="left" valign="top" rowspan="1" colspan="1">95</td>
              <td align="left" valign="top" rowspan="1" colspan="1">95</td>
              <td align="left" valign="top" rowspan="1" colspan="1">95.38</td>
              <td align="left" valign="top" rowspan="1" colspan="1">13.54</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">7-mers (100 features) + EV</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">95.5</td>
              <td align="left" valign="top" rowspan="1" colspan="1">13.51</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">7-mers (200 features) + EV</td>
              <td align="left" valign="top" rowspan="1" colspan="1">95</td>
              <td align="left" valign="top" rowspan="1" colspan="1">95</td>
              <td align="left" valign="top" rowspan="1" colspan="1">95</td>
              <td align="left" valign="top" rowspan="1" colspan="1">95.38</td>
              <td align="left" valign="top" rowspan="1" colspan="1">16.14</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">7-mers (300 features) + EV</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">95.38</td>
              <td align="left" valign="top" rowspan="1" colspan="1">15.43</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">15-mers (100 features) + EV</td>
              <td align="left" valign="top" rowspan="1" colspan="1">95</td>
              <td align="left" valign="top" rowspan="1" colspan="1">95</td>
              <td align="left" valign="top" rowspan="1" colspan="1">95</td>
              <td align="left" valign="top" rowspan="1" colspan="1">94.97</td>
              <td align="left" valign="top" rowspan="1" colspan="1">15.45</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">15-mers (200 features) + EV</td>
              <td align="left" valign="top" rowspan="1" colspan="1">94</td>
              <td align="left" valign="top" rowspan="1" colspan="1">94</td>
              <td align="left" valign="top" rowspan="1" colspan="1">94</td>
              <td align="left" valign="top" rowspan="1" colspan="1">94.39</td>
              <td align="left" valign="top" rowspan="1" colspan="1">16.71</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">20-mers (100 features) + EV</td>
              <td align="left" valign="top" rowspan="1" colspan="1">94</td>
              <td align="left" valign="top" rowspan="1" colspan="1">94</td>
              <td align="left" valign="top" rowspan="1" colspan="1">94</td>
              <td align="left" valign="top" rowspan="1" colspan="1">93.69</td>
              <td align="left" valign="top" rowspan="1" colspan="1">18.96</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">20-mers (200 features) + EV</td>
              <td align="left" valign="top" rowspan="1" colspan="1">93</td>
              <td align="left" valign="top" rowspan="1" colspan="1">93</td>
              <td align="left" valign="top" rowspan="1" colspan="1">93</td>
              <td align="left" valign="top" rowspan="1" colspan="1">93.48</td>
              <td align="left" valign="top" rowspan="1" colspan="1">19.12</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">3-mers (100 f) + 5-mers (100 f) + EV</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">95.71</td>
              <td align="left" valign="top" rowspan="1" colspan="1">19.17</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">3-mers (100 f) + 7-mers (300 f) + EV</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96.04</td>
              <td align="left" valign="top" rowspan="1" colspan="1">16.78</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">3-mers (300 f) + 7-mers (300 f) + EV</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96.29</td>
              <td align="left" valign="top" rowspan="1" colspan="1">20.31</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">5-mers (100 f) + 7-mers (100 f) + EV</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96.37</td>
              <td align="left" valign="top" rowspan="1" colspan="1">14.79</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">5-mers (100 f) + 7-mers (300 f) + EV</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">96</td>
              <td align="left" valign="top" rowspan="1" colspan="1">95.63</td>
              <td align="left" valign="top" rowspan="1" colspan="1">13.9</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <fig position="float" fig-type="figure" id="f6">
        <label>Figure 6. </label>
        <caption>
          <title>Four-fold cross-validation diagram.</title>
          <p>Data were divided into four folders, three were used to train the classifier, and one was allowed to test during each of four iterations.</p>
        </caption>
        <graphic xlink:href="f1000research-11-119185-g0005" id="gr6" position="float"/>
      </fig>
      <p>Datasets were partitioned into four equal parts; one group was used to test the methodologies (test dataset), and the remaining groups were used to train the software (Train Dataset). Four iterations were needed for each part to validate the methodologies throughout an accuracy and confusion matrix obtained for each iteration to discard the model. Then, each dataset methodology is processed by 4 iterations, where induvial iteration generates a classifier. Thus, when the classifier with the best performance is found, this is saved and used to predict NifH proteins.</p>
    </sec>
    <sec id="sec8">
      <title>Operation</title>
      <p>NIFtHool requires access to internet and to have any device capable of being a Web server. The device must have an operating system that can run as a Web server, capable of delivering HTML5 content. It must also have an Intel
<sup>®</sup> Celeron
<sup>®</sup> 847 Processor, 1.10 GHz, and a minimum Ram of 512 MB. Finally, this tool does not require the device to have a certain amount of storage or hard disk space as it works online.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="sec9">
    <title>Results</title>
    <sec id="sec10">
      <title>Evaluation of the model classification</title>
      <p>Performance of four-fold cross-validation and metrics such as accuracy, loss, F1-score, sensitivity or recall, and precision were obtained for each dataset methodology to evaluate the classification as summarized in
<xref rid="T2" ref-type="table">Table 2</xref>. Due to each dataset methodology works with four iteration producing four classifiers, only the one with the best performance is mentioned below.</p>
      <p>The use of the methodology that consists of EV had a high performance, since both its precision, recall and F1-score were greater than 90%. However, its loss still represents a significant value. For this reason, EV was merged with k-mers to increase the classification values. A large number of methodological datasets were tested, and all of them were higher than the EV. The results define that datasets with longer k-mers, such as 20-mers, have slightly lower performance than datasets with shorter k-mers, such as 7-mers. Because the computational cost is higher as the k-mers are longer, and because their performance decreases slightly, shorter k-mers were selected to improve the classification performance.</p>
      <p>Another factor considered when comparing the different methodologies was the number of features extracted. For the 3-mers + EV and 7-mers + EV datasets, the extraction of 100, 200 and 300 features were tested, but the results were similar between the groups. In the group of 3-mers + EV, the accuracy values were around 95% for the three cases, as well as recall and F1-score, which were the same (96%) for all. For the group of 7-mers + EV, despite being k-mers of greater length than the 3-mers, they obtained similar results to 3-mers + EV, where their greatest difference was the loss, which represents an advantage for the 7-mers with values between 13 and 17%, compared to the higher 17 to 21% of the 3-mers. In this sense, since a greater number of features implies a higher computational cost, and the values are similar, the extraction of 100 features for each k-mer was chosen. The analysis of 5-mers + EV was also recorded, which was similar to 3-mers + EV, however this analysis showed better performance in the loss, since 5-mers reached 13% while the best result of 3-mers was 17%.</p>
      <p>Methodologies that combine two k-mers + EV were experimented with, and due to the high performance of each k-mer together with EV, the performance of the combination of these k-mers with EV was analysed. Finally, five dataset methodologies were performed, and 5-mers (100 features) + 7-mers (100 features) + EV dataset had the best performance, which obtained precision, recall and F1-scores of 96%, an accuracy of 96.37%, and a loss of only 14.79%. Due to its performance, this methodology was selected to be used as the model classifier and to be implemented into an informatics tool.</p>
      <p>The validation error of model 2 was 0.2625. This value was obtained from the learning rate, ranging from 0 to 40 in which the red epochs were trained. Measurements showed suitable training convergences as shown in
<xref rid="f7" ref-type="fig">Figure 7</xref>. Accuracy evaluation of the training started with low values that increased in the epochs (
<xref rid="f7" ref-type="fig">Figure 7b</xref>). On the other hand, while training began with a high loss, this value decreases as they were trained (
<xref rid="f7" ref-type="fig">Figure 7a</xref>). Both graphs show similar behaviour during the training and validation as a reliable model learning with constant values obtained at the end of the assessment, which indicates that the maximum training point was reached.</p>
      <fig position="float" fig-type="figure" id="f7">
        <label>Figure 7. </label>
        <caption>
          <title>TensorBoard (RRID:SCR_016345) visualization of the distributed training metrics for the classifier after 30 epochs.</title>
          <p>Where x-axis represent the number of epochs and the y-axis represents the values of accuracy and loss as a function of unity (1= 100%). a) Loss evaluation. b) Accuracy evaluation.</p>
        </caption>
        <graphic xlink:href="f1000research-11-119185-g0006" id="gr7" position="float"/>
      </fig>
      <p>The efficiency of the neural network was assessed using a confusion matrix. Primary diagonal data was represented, which indicates the number of hits in the model (
<xref rid="f8" ref-type="fig">Figure 8</xref>). 1195 sequences were correctly classified as no-NifH, and 1128 as true NifH proteins. The value below the primary diagonal shows the false negatives or type II errors (the NifH is not detected), corresponding to 61 cases.</p>
      <fig position="float" fig-type="figure" id="f8">
        <label>Figure 8. </label>
        <caption>
          <title>Assessment of the efficiency of the Deep Neural Network by a Confusion matrix.</title>
          <p>a) Panel a shows the confusion matrix for the number of evaluated sequences, and panel b corresponds to the number of evaluated sequences normalized to one. TP: true positive =1188, TN: true negative = 1132, FP: false positive = 25, and FN: false negative = 48.</p>
        </caption>
        <graphic xlink:href="f1000research-11-119185-g0007" id="gr8" position="float"/>
      </fig>
      <p>In contrast, the value above the primary diagonal reflected the classifier errors: false positives or error type I (the NifH is detected but not present) was equal to 39 cases. The confusion matrix results evaluated the relevance through 3 metrics: accuracy rate (96.4%), specificity (96.7%), and sensitivity (95.2%).</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="sec11">
    <title>Discussion</title>
    <p>Deep learning was selected because it worked with protein sequence and conversion to arrays allowing better results than other methods, as previously reported (
<xref rid="T3" ref-type="table">Table 3</xref>). Our model had a high performance, as demonstrated high sensitivity (95.2%), high accuracy (96.4%), and specificity (96.7%), which is comparable to current deep learning techniques generating software
<sup><xref rid="ref23" ref-type="bibr">23</xref></sup>
<sup>,</sup>
<sup><xref rid="ref25" ref-type="bibr">25</xref></sup> that considered a vector of 128 features-values for each sequence protein to train the model.</p>
    <table-wrap position="float" id="T3">
      <label>Table 3. </label>
      <caption>
        <title>Performance of our model in comparisons with other methods of machine learning.</title>
      </caption>
      <table frame="hsides" rules="groups" content-type="article-table">
        <thead>
          <tr>
            <th align="left" rowspan="2" valign="top" colspan="1">Method</th>
            <th align="left" rowspan="2" valign="top" colspan="1">Learning technology</th>
            <th align="left" rowspan="2" valign="top" colspan="1">Purpose</th>
            <th align="left" rowspan="2" valign="top" colspan="1">Database</th>
            <th align="left" colspan="3" valign="top" rowspan="1">Metrics evaluated (%)</th>
            <th align="left" rowspan="2" valign="top" colspan="1">Reference</th>
          </tr>
          <tr>
            <th align="left" valign="top" rowspan="1" colspan="1">Sensitivity</th>
            <th align="left" valign="top" rowspan="1" colspan="1">Specificity</th>
            <th align="left" valign="top" rowspan="1" colspan="1">Accuracy</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left" valign="middle" rowspan="1" colspan="1">Embedding vectors and
<xref rid="tfn1" ref-type="table-fn"><sup>§</sup></xref>ANN</td>
            <td align="left" valign="middle" rowspan="1" colspan="1"><xref rid="tfn7" ref-type="table-fn">*</xref>NN</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">Identification of
<xref rid="tfn9" ref-type="table-fn"><sup>€</sup></xref>NifH proteins</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">9793</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">95.17</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">96.67</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">96.37</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">This work</td>
          </tr>
          <tr>
            <td align="left" valign="middle" rowspan="1" colspan="1">Image processing and
<xref rid="tfn2" ref-type="table-fn"><sup>¶</sup></xref>CNN</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">NN</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">Identification of NifH proteins</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">42767</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">98.26</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">88.79</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">99.00</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">
              <sup>
                <xref rid="ref12" ref-type="bibr">12</xref>
              </sup>
            </td>
          </tr>
          <tr>
            <td align="left" valign="middle" rowspan="1" colspan="1">Feature Generation and
<xref rid="tfn3" ref-type="table-fn"><sup>¥</sup></xref>SVM</td>
            <td align="left" valign="middle" rowspan="1" colspan="1"><xref rid="tfn8" ref-type="table-fn">**</xref>ML</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">Identification of Nif proteins: NifH,
<xref rid="tfn10" ref-type="table-fn"><sup>γ</sup></xref>NifD,
<xref rid="tfn11" ref-type="table-fn"><sup>ɑ</sup></xref>NifK,
<xref rid="tfn12" ref-type="table-fn"><sup>∞</sup></xref>NifE,
<xref rid="tfn13" ref-type="table-fn"><sup>∂</sup></xref>NifN and
<xref rid="tfn14" ref-type="table-fn"><sup>ℇ</sup></xref>NifB.</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">747</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">88.70</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">99.30</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">94.00</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">
              <sup>
                <xref rid="ref14" ref-type="bibr">14</xref>
              </sup>
            </td>
          </tr>
          <tr>
            <td align="left" valign="middle" rowspan="1" colspan="1">Embedding vectors and
<xref rid="tfn4" ref-type="table-fn"><sup>£</sup></xref>MBD-LSTM</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">NN</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">Identification of
<italic toggle="yes">Plasmodium falciparum</italic> mitochondrial proteins</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">3776</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">100</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">99.33</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">99.50</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">
              <sup>
                <xref rid="ref25" ref-type="bibr">25</xref>
              </sup>
            </td>
          </tr>
          <tr>
            <td align="left" valign="middle" rowspan="1" colspan="1">Embedding vectors and
<xref rid="tfn5" ref-type="table-fn"><sup>Þ</sup></xref>DeepDBP-ANN</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">NN</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">Identification of DNA-binding proteins</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">1261</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">98.00</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">97.00</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">99.02</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">
              <sup>
                <xref rid="ref23" ref-type="bibr">23</xref>
              </sup>
            </td>
          </tr>
          <tr>
            <td align="left" valign="middle" rowspan="1" colspan="1"><xref rid="tfn6" ref-type="table-fn"><sup>†</sup></xref>CART</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">ML</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">Classification of NifH Protein Sequences</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">32954</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">N/D</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">N/D</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">95-99</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">
              <sup>
                <xref rid="ref13" ref-type="bibr">13</xref>
              </sup>
            </td>
          </tr>
          <tr>
            <td align="left" valign="middle" rowspan="1" colspan="1">CART and decision trees</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">ML</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">Classification of NifH Protein Sequences</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">290</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">N/D</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">N/D</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">96-97</td>
            <td align="left" valign="middle" rowspan="1" colspan="1">
              <sup>
                <xref rid="ref11" ref-type="bibr">11</xref>
              </sup>
            </td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn id="tfn1">
          <label>
            <sup>§</sup>
          </label>
          <p>ANN: Artificial neural network.</p>
        </fn>
        <fn id="tfn2">
          <label>
            <sup>¶</sup>
          </label>
          <p>CNN: Convolutional neural network.</p>
        </fn>
        <fn id="tfn3">
          <label>
            <sup>¥</sup>
          </label>
          <p>SVM: Support vector machine.</p>
        </fn>
        <fn id="tfn4">
          <label>
            <sup>£</sup>
          </label>
          <p>MBD-LSTM: Multilayer bi-directional long short term memory.</p>
        </fn>
        <fn id="tfn5">
          <label>
            <sup>Þ</sup>
          </label>
          <p>DeepDBP-ANN: Deep neural networks for identification of DNA binding proteins.</p>
        </fn>
        <fn id="tfn6">
          <label>
            <sup>†</sup>
          </label>
          <p>CART: Classification and regression trees statistical models.</p>
        </fn>
        <fn id="tfn7">
          <label>*</label>
          <p>NN: Neural networks.</p>
        </fn>
        <fn id="tfn8">
          <label>**</label>
          <p>ML: Machine leaning.</p>
        </fn>
        <fn id="tfn9">
          <label>
            <sup>€</sup>
          </label>
          <p>NifH: Nitrogenase Iron Protein.</p>
        </fn>
        <fn id="tfn10">
          <label>
            <sup>γ</sup>
          </label>
          <p>NifD: Nitrogenase molybdenum-iron protein alpha chain.</p>
        </fn>
        <fn id="tfn11">
          <label>
            <sup>ɑ</sup>
          </label>
          <p>NifK: Nitrogenase molybdenum-iron protein beta chain.</p>
        </fn>
        <fn id="tfn12">
          <label>
            <sup>∞</sup>
          </label>
          <p>NifE: Nitrogenase iron-molybdenum cofactor biosynthesis protein NifE.</p>
        </fn>
        <fn id="tfn13">
          <label>
            <sup>∂</sup>
          </label>
          <p>NifN: Nitrogenase iron-molybdenum cofactor biosynthesis protein NifN.</p>
        </fn>
        <fn id="tfn14">
          <label>
            <sup>ℇ</sup>
          </label>
          <p>NifB: Nitrogenase iron-molybdenum cofactor biosynthesis protein NifB, N/D: No data.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <p>Our software showed a high performance as compared with previous apps. For instance, Shinde's model
<sup><xref rid="ref12" ref-type="bibr">12</xref></sup> is based on the analysis of gene sequences of NifH proteins, operated with a 32×32 matrix for each sequence as our model. Nonetheless, our software is an improvement on previous informatics tools as it uses other feature extraction methods, as described above. nifPred, a multi NifH proteins classifier, that uses 13,500 values per sequence, involves four manual methods to obtain the components and data to be trained, having a high specificity.
<sup><xref rid="ref14" ref-type="bibr">14</xref></sup>
</p>
    <p>NIFtHool was compared with two models that work with two different embedding vectors to identification of mitochondrial proteins of
<italic toggle="yes">Plasmodium falciparum</italic>
<sup><xref rid="ref25" ref-type="bibr">25</xref></sup> and DNA-binding proteins.
<sup><xref rid="ref23" ref-type="bibr">23</xref></sup> The three models showed positive results in sensitivity, specificity, and accuracy, so the selection of the embedding vector method was adequate. Additionally, a comparison was made with studies using machine learning techniques for the classification of NifH protein sequences. These studies used classification and regression trees statistical models and decision trees.
<sup><xref rid="ref11" ref-type="bibr">11</xref></sup>
<sup>,</sup>
<sup><xref rid="ref13" ref-type="bibr">13</xref></sup>
</p>
    <p>It is stated that our model obtained high accuracy results like these models but with reduced computational power. Thus, NIFtHool shows clear improvements compared to the studies in the literature as shown in
<xref rid="T3" ref-type="table">Table 3</xref>.</p>
  </sec>
  <sec sec-type="conclusion" id="sec12">
    <title>Conclusion</title>
    <p>A binary classification model of NifH protein sequences using artificial neural networks has been developed in the present work and hosted by
<ext-link xlink:href="https://nifthool.anvil.app/" ext-link-type="uri">Anvil</ext-link>. We tried the conventional approach of extracting features with specified algorithms through the novel feature extraction approach using deep learning techniques. Numerical features were obtained from two aspects: k-mers method considering the unique k-mer and an embedding layer related to aa position in the sequence. This methodology that was studied offers a performance similar to the best performances in the literature. Our tool offers better computational performance due to the classification process being based on the use of only NifH protein domain, resulting in less data processing for the software.</p>
    <p>Even though the entire nitrogenase complex is relevant for transforming atmospheric nitrogen into ammonia, only NifH protein has been considered because this subunit is paramount during the reduction from N
<sub>2</sub> to NH
<sub>4</sub>. NIFtHool not only represents a significant improvement compared to other computational methods, but it is also a tool for the identification of NifH Protein. Thus, researchers can easily use NIFtHool to identify NifH proteins as a reliable tool during the protein and nitrogen fixing bacteria analysis.</p>
  </sec>
  <sec sec-type="data-availability" id="sec13">
    <title>Data availability</title>
    <sec id="sec14">
      <title>Underlying data</title>
      <p>Zenodo: NIFTHool: Repository.
<ext-link xlink:href="https://doi.org/10.5281/zenodo.5913032" ext-link-type="uri">https://doi.org/10.5281/zenodo.5913032</ext-link>.
<sup><xref rid="ref18" ref-type="bibr">18</xref></sup>
</p>
      <p>This project contains the following underlying data:
<list list-type="simple"><list-item><label>-</label><p>List_kmers.csv (List of 5-mers and 7-mers obtained from dataset after it filtered sequences shorter than 50 aa and longer than 1173 aa)</p></list-item><list-item><label>-</label><p>RAW_data_NifH.fasta (52942 NifH proteins retrieved from Uniprot)</p></list-item><list-item><label>-</label><p>data_NifH.csv (4939 NifH sequences retrieved after CD-hit filtration)</p></list-item><list-item><label>-</label><p>data_nonNifH.txt (4953 non-NifH sequences)</p></list-item></list>
</p>
    </sec>
    <sec id="sec15">
      <title>Extended data</title>
      <p>Zenodo: NIFTHool: Repository.
<ext-link xlink:href="https://doi.org/10.5281/zenodo.5913032" ext-link-type="uri">https://doi.org/10.5281/zenodo.5913032</ext-link>.
<sup><xref rid="ref18" ref-type="bibr">18</xref></sup>
</p>
      <p>This project contains the following underlying data:
<list list-type="simple"><list-item><label>-</label><p>data_NifH_plus_nonNifH.txt (Combination of sequences from data_NifH.csv and data_nonNifH.txt).</p></list-item></list>
</p>
      <p>Data are available under the terms of the
<ext-link xlink:href="http://creativecommons.org/publicdomain/zero/1.0/" ext-link-type="uri">Creative Commons Zero “No rights reserved” data waiver</ext-link> (CC0 1.0 Public domain dedication).</p>
    </sec>
  </sec>
  <sec id="sec16">
    <title>Software availability</title>
    <p>NIFtHool available from:
<ext-link xlink:href="https://nifthool.anvil.app/" ext-link-type="uri">https://nifthool.anvil.app/</ext-link>
</p>
    <p>Source code available from:
<ext-link xlink:href="https://github.com/JefferDSP/NIFTHool/tree/v1.0" ext-link-type="uri">https://github.com/JefferDSP/NIFTHool/tree/v1.0</ext-link>
</p>
    <p>Archived source code as at time of publication:
<ext-link xlink:href="https://doi.org/10.5281/zenodo.5913032" ext-link-type="uri">https://doi.org/10.5281/zenodo.5913032</ext-link>.
<sup><xref rid="ref18" ref-type="bibr">18</xref></sup>
</p>
    <p>License:
<ext-link xlink:href="https://creativecommons.org/publicdomain/zero/1.0/" ext-link-type="uri">CC0-1.0</ext-link>.</p>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="ref1">
      <label>1</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cao</surname><given-names>P</given-names></name><name><surname>Lu</surname><given-names>C</given-names></name><name><surname>Yu</surname><given-names>Z</given-names></name></person-group>:
<article-title>Historical nitrogen fertilizer use in agricultural ecosystems of the contiguous United States during 1850-2015: Application rate, timing, and fertilizer types.</article-title><source><italic toggle="yes">Earth Syst. Sci. Data.</italic></source><year>2018</year>;<volume>10</volume>(<issue>2</issue>):<fpage>969</fpage>–<lpage>984</lpage>.
<pub-id pub-id-type="doi">10.5194/essd-10-969-2018</pub-id></mixed-citation>
    </ref>
    <ref id="ref2">
      <label>2</label>
      <mixed-citation publication-type="book"><collab>FAO</collab>:
<source><italic toggle="yes">World fertilizer trends and outlook to 2020.</italic></source><publisher-name>Food and Agriculture Organization of United Nations</publisher-name>;<year>2017</year>.
<ext-link xlink:href="https://www.fao.org/3/i6895e/i6895e.pdf" ext-link-type="uri">Reference Source</ext-link></mixed-citation>
    </ref>
    <ref id="ref3">
      <label>3</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahmud</surname><given-names>K</given-names></name><name><surname>Makaju</surname><given-names>S</given-names></name><name><surname>Ibrahim</surname><given-names>R</given-names></name><etal/></person-group>:
<article-title>Current progress in nitrogen fixing plants and microbiome research.</article-title><source><italic toggle="yes">Plants.</italic></source><year>2020</year>;<volume>9</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>17</lpage>.
<pub-id pub-id-type="doi">10.3390/plants9010097</pub-id></mixed-citation>
    </ref>
    <ref id="ref4">
      <label>4</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bhattacharjee</surname><given-names>RB</given-names></name><name><surname>Singh</surname><given-names>A</given-names></name><name><surname>Mukhopadhyay</surname><given-names>SN</given-names></name></person-group>:
<article-title>Use of nitrogen-fixing bacteria as biofertiliser for non-legumes: Prospects and challenges.</article-title><source><italic toggle="yes">Appl. Microbiol. Biotechnol.</italic></source><year>2008</year>;<volume>80</volume>(<issue>2</issue>):<fpage>199</fpage>–<lpage>209</lpage>.
<pub-id pub-id-type="doi">10.1007/s00253-008-1567-2</pub-id><?supplied-pmid 18600321?><pub-id pub-id-type="pmid">18600321</pub-id></mixed-citation>
    </ref>
    <ref id="ref5">
      <label>5</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davies-Barnard</surname><given-names>T</given-names></name><name><surname>Friedlingstein</surname><given-names>P</given-names></name></person-group>:
<article-title>The Global Distribution of Biological Nitrogen Fixation in Terrestrial Natural Ecosystems.</article-title><source><italic toggle="yes">Glob. Biogeochem. Cycles</italic></source><year>2020</year>;<volume>34</volume>:<fpage>1</fpage>–<lpage>14</lpage>.
<pub-id pub-id-type="doi">10.1029/2019GB006387</pub-id></mixed-citation>
    </ref>
    <ref id="ref6">
      <label>6</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>W</given-names></name><name><surname>Shahrajabian</surname><given-names>MH</given-names></name><name><surname>Cheng</surname><given-names>Q</given-names></name></person-group>:
<article-title>Nitrogen Fixation and Diazotrophs – A Review.</article-title><source><italic toggle="yes">Rom. Biotechnol. Lett.</italic></source><year>2021</year>;<volume>26</volume>(<issue>4</issue>):<fpage>2834</fpage>–<lpage>2845</lpage>.
<pub-id pub-id-type="doi">10.25083/rbl/26.4/2834-2845</pub-id></mixed-citation>
    </ref>
    <ref id="ref7">
      <label>7</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bellenger</surname><given-names>JP</given-names></name><name><surname>Darnajoux</surname><given-names>R</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><etal/></person-group>:
<article-title>Biological nitrogen fixation by alternative nitrogenases in terrestrial ecosystems: a review.</article-title><source><italic toggle="yes">Biogeochemistry.</italic></source><year>2020</year>;<volume>149</volume>(<issue>1</issue>):<fpage>53</fpage>–<lpage>73</lpage>.
<pub-id pub-id-type="doi">10.1007/s10533-020-00666-7</pub-id></mixed-citation>
    </ref>
    <ref id="ref8">
      <label>8</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poole</surname><given-names>P</given-names></name><name><surname>Ramachandran</surname><given-names>V</given-names></name><name><surname>Terpolilli</surname><given-names>J</given-names></name></person-group>:
<article-title>Rhizobia: From saprophytes to endosymbionts.</article-title><source><italic toggle="yes">Nat. Rev. Microbiol.</italic></source><year>2018</year>;<volume>16</volume>(<issue>5</issue>):<fpage>291</fpage>–<lpage>303</lpage>.
<pub-id pub-id-type="doi">10.1038/nrmicro.2017.171</pub-id><?supplied-pmid 29379215?><pub-id pub-id-type="pmid">29379215</pub-id></mixed-citation>
    </ref>
    <ref id="ref9">
      <label>9</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>X</given-names></name><name><surname>Payá-Tormo</surname><given-names>L</given-names></name><name><surname>Coroian</surname><given-names>D</given-names></name><etal/></person-group>:
<article-title>Exploiting genetic diversity and gene synthesis to identify superior nitrogenase NifH protein variants to engineer N2-fixation in plants.</article-title><source><italic toggle="yes">Commun. Biol.</italic></source><year>2021 Jan</year>;<volume>4</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>11</lpage>.
<pub-id pub-id-type="doi">10.1038/s42003-020-01536-6</pub-id><pub-id pub-id-type="pmid">33398033</pub-id></mixed-citation>
    </ref>
    <ref id="ref10">
      <label>10</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rascio</surname><given-names>N</given-names></name><name><surname>La Rocca</surname><given-names>N</given-names></name></person-group>:
<part-title>Biological Nitrogen Fixation</part-title>. In:
<person-group person-group-type="editor"><name><surname>Fath</surname><given-names>B</given-names></name></person-group>, editor.
<source><italic toggle="yes">Encyclopedia of Ecology.</italic></source><edition>Second edition.</edition><publisher-loc>Amsterdam</publisher-loc>:
<publisher-name>Elsevier</publisher-name>;<year>2019</year>; Volume<volume>2</volume>: p.<fpage>264</fpage>–<lpage>279</lpage>.
<pub-id pub-id-type="doi">10.1016/B978-0-444-63768-0.00685-5</pub-id></mixed-citation>
    </ref>
    <ref id="ref11">
      <label>11</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frank</surname><given-names>IE</given-names></name><name><surname>Turk-Kubo</surname><given-names>KA</given-names></name><name><surname>Zehr</surname><given-names>JP</given-names></name></person-group>:
<article-title>Rapid annotation of nifH gene sequences using classification and regression trees facilitates environmental functional gene analysis.</article-title><source><italic toggle="yes">Environ. Microbiol. Rep.</italic></source><year>2016 Oct</year>;<volume>8</volume>(<issue>5</issue>):<fpage>905</fpage>–<lpage>916</lpage>.
<pub-id pub-id-type="doi">10.1111/1758-2229.12455</pub-id><?supplied-pmid 27557869?><pub-id pub-id-type="pmid">27557869</pub-id></mixed-citation>
    </ref>
    <ref id="ref12">
      <label>12</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Shinde</surname><given-names>I</given-names></name></person-group>:
<source><italic toggle="yes">Nitrogenase Iron Protein Detection using Neural Network. Master’s Projects.</italic></source><publisher-loc>[San Jose, CA, USA]</publisher-loc>:
<publisher-name>San José State University</publisher-name>;<year>2019</year>.</mixed-citation>
    </ref>
    <ref id="ref13">
      <label>13</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Frank</surname><given-names>I</given-names></name></person-group>:
<source><italic toggle="yes">Rapid Classification of NifH Protein Sequences using Classification and Regression Trees.</italic></source><publisher-name>University of California Santa Cruz</publisher-name>;<year>2014</year>.</mixed-citation>
    </ref>
    <ref id="ref14">
      <label>14</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meher</surname><given-names>PK</given-names></name><name><surname>Sahu</surname><given-names>TK</given-names></name><name><surname>Mohanty</surname><given-names>J</given-names></name><etal/></person-group>:
<article-title>nifPred: Proteome-Wide Identification and Categorization of Nitrogen-Fixation Proteins of Diaztrophs Based on Composition-Transition-Distribution Features Using Support Vector Machine.</article-title><source><italic toggle="yes">Front. Microbiol.</italic></source><year>2018 May</year>;<volume>9</volume>:<fpage>1</fpage>–<lpage>16</lpage>.
<pub-id pub-id-type="doi">10.3389/fmicb.2018.01100</pub-id><pub-id pub-id-type="pmid">29403456</pub-id></mixed-citation>
    </ref>
    <ref id="ref15">
      <label>15</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>X-D</given-names></name></person-group>:
<article-title>Machine Learning. A Matrix Algebr Approach to Artif Intell.</article-title><year>2020</year>;<fpage>223</fpage>–<lpage>440</lpage>.</mixed-citation>
    </ref>
    <ref id="ref16">
      <label>16</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>The Uniprot Consortium</collab></person-group>:
<article-title>UniProt: The universal protein knowledgebase in 2021.</article-title><source><italic toggle="yes">Nucleic Acids Res.</italic></source><year>2021</year>;<volume>49</volume>(<issue>D1</issue>):<fpage>D480</fpage>–<lpage>D489</lpage>.
<pub-id pub-id-type="doi">10.1093/nar/gkaa1100</pub-id><?supplied-pmid 33237286?><pub-id pub-id-type="pmid">33237286</pub-id></mixed-citation>
    </ref>
    <ref id="ref17">
      <label>17</label>
      <mixed-citation publication-type="other">Anvil TM. Anvil Full Stack web apps with nothing but Phyton[Internet]. 2020 [cited 2022 Jan 31].
<ext-link xlink:href="https://anvil.works/" ext-link-type="uri">https://anvil.works/</ext-link>
</mixed-citation>
    </ref>
    <ref id="ref18">
      <label>18</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jefferson</surname><given-names>S-P</given-names></name></person-group>:
<article-title>JefferDSP/NIFTHool: NIFTHool repository.</article-title><source><italic toggle="yes">NIFTHool Repository</italic></source><year>2021 [cited 2022 Jan 31]</year>.
<pub-id pub-id-type="doi">10.5281/zenodo.5913032</pub-id></mixed-citation>
    </ref>
    <ref id="ref19">
      <label>19</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>L</given-names></name><name><surname>Niu</surname><given-names>B</given-names></name><name><surname>Zhu</surname><given-names>Z</given-names></name><etal/></person-group>:
<article-title>CD-HIT: Accelerated for clustering the next-generation sequencing data.</article-title><source><italic toggle="yes">Bioinformatics.</italic></source><year>2012</year>;<volume>28</volume>(<issue>23</issue>):<fpage>3150</fpage>–<lpage>3152</lpage>.
<pub-id pub-id-type="doi">10.1093/bioinformatics/bts565</pub-id><?supplied-pmid 23060610?><pub-id pub-id-type="pmid">23060610</pub-id></mixed-citation>
    </ref>
    <ref id="ref20">
      <label>20</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bursteinas</surname><given-names>B</given-names></name><name><surname>Britto</surname><given-names>R</given-names></name><name><surname>Bely</surname><given-names>B</given-names></name><etal/></person-group>:
<article-title>Minimizing proteome redundancy in the UniProt Knowledgebase.</article-title><source><italic toggle="yes">Database J. Biol. Databases Curation.</italic></source><year>2016 Jan</year>;<volume>2016</volume>:<fpage>1</fpage>–<lpage>9</lpage>.
<pub-id pub-id-type="doi">10.1093/database/baw139</pub-id></mixed-citation>
    </ref>
    <ref id="ref21">
      <label>21</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manekar</surname><given-names>SC</given-names></name><name><surname>Sathe</surname><given-names>SR</given-names></name></person-group>:
<article-title>Estimating the k-mer Coverage Frequencies in Genomic Datasets: A Comparative Assessment of the State-of-the-art.</article-title><source><italic toggle="yes">Curr. Genomics</italic></source><year>2019 Oct</year>;<volume>20</volume>(<issue>1</issue>):<fpage>2</fpage>–<lpage>15</lpage>.
<pub-id pub-id-type="doi">10.2174/1389202919666181026101326</pub-id><?supplied-pmid 31015787?><pub-id pub-id-type="pmid">31015787</pub-id></mixed-citation>
    </ref>
    <ref id="ref22">
      <label>22</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breitwieser</surname><given-names>FP</given-names></name><name><surname>Baker</surname><given-names>DN</given-names></name><name><surname>Salzberg</surname><given-names>SL</given-names></name></person-group>:
<article-title>KrakenUniq: confident and fast metagenomics classification using unique k-mer counts.</article-title><source><italic toggle="yes">Genome Biol.</italic></source><year>2018 Nov</year>;<volume>19</volume>(<issue>1</issue>):<fpage>198</fpage>.
<pub-id pub-id-type="doi">10.1186/s13059-018-1568-0</pub-id><?supplied-pmid 30445993?><pub-id pub-id-type="pmid">30445993</pub-id></mixed-citation>
    </ref>
    <ref id="ref23">
      <label>23</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadab</surname><given-names>S</given-names></name><name><surname>Alam Khan</surname><given-names>MT</given-names></name><name><surname>Neezi</surname><given-names>NA</given-names></name><etal/></person-group>:
<article-title>DeepDBP: Deep neural networks for identification of DNA-binding proteins.</article-title><source><italic toggle="yes">Informatics Med. Unlocked.</italic></source><year>2020</year>;<volume>19</volume>:<fpage>100317</fpage>–<lpage>100318</lpage>.
<pub-id pub-id-type="doi">10.1016/j.imu.2020.100318</pub-id></mixed-citation>
    </ref>
    <ref id="ref24">
      <label>24</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>ElAbd</surname><given-names>H</given-names></name><name><surname>Bromberg</surname><given-names>Y</given-names></name><name><surname>Hoarfrost</surname><given-names>A</given-names></name><etal/></person-group>:
<article-title>Amino acid encoding for deep learning applications.</article-title><source><italic toggle="yes">BMC Bioinformatics.</italic></source><year>2020 Jun</year>;<volume>21</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>14</lpage>.
<pub-id pub-id-type="doi">10.1186/s12859-020-03546-x</pub-id><pub-id pub-id-type="pmid">31898485</pub-id></mixed-citation>
    </ref>
    <ref id="ref25">
      <label>25</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khan</surname><given-names>SU</given-names></name><name><surname>Baik</surname><given-names>R</given-names></name></person-group>:
<article-title>MPPIF-Net: Identification of Plasmodium Falciparum Parasite Mitochondrial Proteins Using Deep Features with Multilayer Bi-directional LSTM.</article-title><source><italic toggle="yes">Processes.</italic></source><year>2020 Jun</year>;<volume>8</volume>(<issue>6</issue>):<fpage>1</fpage>–<lpage>16</lpage>.
<pub-id pub-id-type="doi">10.3390/pr8060725</pub-id></mixed-citation>
    </ref>
    <ref id="ref26">
      <label>26</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Abadi</surname><given-names>M</given-names></name><name><surname>Agarwal</surname><given-names>A</given-names></name><name><surname>Barham</surname><given-names>P</given-names></name><etal/></person-group>:
<part-title>TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems</part-title>.
<source><italic toggle="yes">OSDI’16: Proceedings of the 12th USENIX conference on Operating Systems Design and Implementation.</italic></source><publisher-loc>Savannah</publisher-loc>:
<publisher-name>USENIX Association</publisher-name>;<year>2016</year>; p.<fpage>265</fpage>–<lpage>283</lpage>.
<ext-link xlink:href="http://www.tensorflow.org" ext-link-type="uri">Reference Source</ext-link></mixed-citation>
    </ref>
    <ref id="ref27">
      <label>27</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McKinney</surname><given-names>W</given-names></name></person-group>:
<article-title>Data Structures for Statistical Computing in Python.</article-title><source><italic toggle="yes">Proc 9th Python Sci Conf.</italic></source><year>2010</year>;<volume>1</volume>:<fpage>56</fpage>–<lpage>61</lpage>.</mixed-citation>
    </ref>
    <ref id="ref28">
      <label>28</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Keras</surname><given-names>CF</given-names></name></person-group>.<year>2015</year>.
<ext-link xlink:href="https://github.com/fchollet/keras" ext-link-type="uri">Reference Source</ext-link></mixed-citation>
    </ref>
    <ref id="ref29">
      <label>29</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><etal/></person-group>:
<article-title>Scikit-learn: Machine Learning in Python.</article-title><source><italic toggle="yes">J. Mach. Learn. Res.</italic></source><year>2011</year>;<volume>12</volume>:<fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation>
    </ref>
    <ref id="ref30">
      <label>30</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>Walt</surname><given-names>SJ</given-names><prefix>van der</prefix></name><etal/></person-group>:
<article-title>Array programming with NumPy.</article-title><source><italic toggle="yes">Nature.</italic></source><year>2020</year>;<volume>585</volume>(<issue>7825</issue>):<fpage>357</fpage>–<lpage>62</lpage>.
<pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id><?supplied-pmid 32939066?><pub-id pub-id-type="pmid">32939066</pub-id></mixed-citation>
    </ref>
    <ref id="ref31">
      <label>31</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunter</surname><given-names>JD</given-names></name></person-group>:
<article-title>Matplotlib: A 2D graphics environment.</article-title><source><italic toggle="yes">Comput. Sci. Eng.</italic></source><year>2007</year>;<volume>9</volume>(<issue>3</issue>):<fpage>90</fpage>–<lpage>95</lpage>.
<pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id></mixed-citation>
    </ref>
    <ref id="ref32">
      <label>32</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name></person-group>:
<article-title>Deep Neural Networks as Scientific Models.</article-title><source><italic toggle="yes">Trends Cogn. Sci.</italic></source><year>2019 Apr</year>;<volume>23</volume>(<issue>4</issue>):<fpage>305</fpage>–<lpage>317</lpage>.
<pub-id pub-id-type="doi">10.1016/j.tics.2019.01.009</pub-id><pub-id pub-id-type="pmid">30795896</pub-id></mixed-citation>
    </ref>
    <ref id="ref33">
      <label>33</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srivastava</surname><given-names>N</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name><name><surname>Krizhevsky</surname><given-names>A</given-names></name><etal/></person-group>:
<article-title>Dropout: A Simple Way to Prevent Neural Networks from Overfitting.</article-title><source><italic toggle="yes">J. Mach. Learn. Res.</italic></source><year>2014</year>;<volume>15</volume>:<fpage>1929</fpage>–<lpage>1958</lpage>.</mixed-citation>
    </ref>
    <ref id="ref34">
      <label>34</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ali</surname><given-names>M</given-names></name><name><surname>Son</surname><given-names>D-H</given-names></name><name><surname>Kang</surname><given-names>S-H</given-names></name><etal/></person-group>:
<article-title>An Accurate CT Saturation Classification Using a Deep Learning Approach Based on Unsupervised Feature Extraction and Supervised Fine-Tuning Strategy.</article-title><source><italic toggle="yes">Energies.</italic></source><year>2017 Nov</year>;<volume>10</volume>(<issue>11</issue>):<fpage>1830</fpage>.
<pub-id pub-id-type="doi">10.3390/en10111830</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
<sub-article article-type="peer-review" id="report123174">
  <front-stub>
    <article-id pub-id-type="doi">10.5256/f1000research.119185.r123174</article-id>
    <title-group>
      <article-title>Reviewer response for version 1</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Barona López</surname>
          <given-names>Lorena Isabel</given-names>
        </name>
        <xref rid="r123174a1" ref-type="aff">1</xref>
        <role>Referee</role>
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5184-3759</contrib-id>
      </contrib>
      <aff id="r123174a1"><label>1</label>Artificial Intelligence and Computer Vision Research Lab, Departamento de Informática y Ciencias de la Computación (DICC), Escuela Politécnica Nacional, Quito, Ecuador</aff>
    </contrib-group>
    <author-notes>
      <fn fn-type="COI-statement">
        <p><bold>Competing interests: </bold>No competing interests were disclosed.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>25</day>
      <month>3</month>
      <year>2022</year>
    </pub-date>
    <permissions>
      <copyright-statement>Copyright: © 2022 Barona López LI</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access peer review report distributed under the terms of the Creative Commons Attribution Licence, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <related-article related-article-type="peer-reviewed-article" ext-link-type="doi" id="relatedArticleReport123174" xlink:href="10.12688/f1000research.107925.1">Version 1</related-article>
    <custom-meta-group>
      <custom-meta>
        <meta-name>recommendation</meta-name>
        <meta-value>approve</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>The authors have built a system that allows researchers the identification of the NifH proteins using deep learning. The paper is well written and explained clearly. Some unique suggestions would be to provide an explanation about how the number of samples was calculated, why was k-fold validation used and how were the hyperparameters determined? I think that these points will enhance the paper.</p>
    <p> Best regards</p>
    <p>Are the conclusions about the tool and its performance adequately supported by the findings presented in the article?</p>
    <p>Yes</p>
    <p>Is the rationale for developing the new software tool clearly explained?</p>
    <p>Yes</p>
    <p>Is the description of the software tool technically sound?</p>
    <p>Yes</p>
    <p>Are sufficient details of the code, methods and analysis (if applicable) provided to allow replication of the software development and its use by others?</p>
    <p>Yes</p>
    <p>Is sufficient information provided to allow interpretation of the expected output datasets and any results generated using the tool?</p>
    <p>Yes</p>
    <p>Reviewer Expertise:</p>
    <p>Security, Machine learning</p>
    <p>I confirm that I have read this submission and believe that I have an appropriate level of expertise to confirm that it is of an acceptable scientific standard.</p>
  </body>
</sub-article>
<sub-article article-type="peer-review" id="report123112">
  <front-stub>
    <article-id pub-id-type="doi">10.5256/f1000research.119185.r123112</article-id>
    <title-group>
      <article-title>Reviewer response for version 1</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Orozco-López</surname>
          <given-names>Juan Onofre</given-names>
        </name>
        <xref rid="r123112a1" ref-type="aff">1</xref>
        <role>Referee</role>
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-0711-6538</contrib-id>
      </contrib>
      <aff id="r123112a1"><label>1</label>Centro Universitario de los Lagos, Universidad de Guadalajara, Guadalajara, Mexico</aff>
    </contrib-group>
    <author-notes>
      <fn fn-type="COI-statement">
        <p><bold>Competing interests: </bold>No competing interests were disclosed.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>22</day>
      <month>3</month>
      <year>2022</year>
    </pub-date>
    <permissions>
      <copyright-statement>Copyright: © 2022 Orozco-López JO</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access peer review report distributed under the terms of the Creative Commons Attribution Licence, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <related-article related-article-type="peer-reviewed-article" ext-link-type="doi" id="relatedArticleReport123112" xlink:href="10.12688/f1000research.107925.1">Version 1</related-article>
    <custom-meta-group>
      <custom-meta>
        <meta-name>recommendation</meta-name>
        <meta-value>approve</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <bold>Is the rationale for developing the new software tool clearly explained?</bold>
    </p>
    <p> The authors explained the reasons to develop this new tool, and they also is showed a summary of existing ones and their disadvantages with the current proposal. But it would be better to justify the use of this new tool from the point of view of getting extra information together at the classification capability.</p>
    <p> On the other hand, there are enough technical details on the requirements, the authors also widely explained the internal structure of Deep Neural Network (DNN), but didn't include information on how such a DNN is performed, additionally to the number of neurons per layer and the layers. It would be great if the authors can share or show a smaller or simpler version of such DNN, in order to replicate their experiments locally, because I found this next error while using the tool on the platform:</p>
    <p>
      <bold>"
<italic toggle="yes">NoServerFunctionError: No server function matching "function_pred_seq" has been registered</italic>"</bold>
    </p>
    <p> Maybe this error is caused by the lack of information or instructions to use the tool.</p>
    <p>
      <bold>Minor comments:</bold>
      <list list-type="bullet">
        <list-item>
          <p>On page 4 it appears "NifH, nifH proteins" was written, where I understand that capital letters refer to specific chemical elements, and the lower case "n" in nifH could be a mistake.</p>
        </list-item>
        <list-item>
          <p>Figure 6 is detailed, how is the dataset divided into train or test datasets? It would be interesting to use an extra set or save an extra 10% of the training datasets to try the tool as a real test using real data. As the whole data set was used for training purposes at different iterations, the neural network has not been tested with any non-training data at any iteration. I propose to save some data sets and use them with the goal to test the performance of the neural network with unknown data by the algorithm.</p>
        </list-item>
      </list>
    </p>
    <p>Are the conclusions about the tool and its performance adequately supported by the findings presented in the article?</p>
    <p>Yes</p>
    <p>Is the rationale for developing the new software tool clearly explained?</p>
    <p>Partly</p>
    <p>Is the description of the software tool technically sound?</p>
    <p>Yes</p>
    <p>Are sufficient details of the code, methods and analysis (if applicable) provided to allow replication of the software development and its use by others?</p>
    <p>Partly</p>
    <p>Is sufficient information provided to allow interpretation of the expected output datasets and any results generated using the tool?</p>
    <p>Yes</p>
    <p>Reviewer Expertise:</p>
    <p>Artificial neural networks are used mainly to develop or improve the performance of the artificial pancreas in the treatment of diabetes Mellitus type 1. Also, I used to develop automatic control algorithms, some of them using artificial neural networks</p>
    <p>I confirm that I have read this submission and believe that I have an appropriate level of expertise to confirm that it is of an acceptable scientific standard.</p>
  </body>
</sub-article>
