<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6849186</article-id>
    <article-id pub-id-type="publisher-id">3087</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-019-3087-8</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>PyBDA: a command line tool for automated analysis of big biological data sets</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Dirmeier</surname>
          <given-names>Simon</given-names>
        </name>
        <address>
          <email>simon.dirmeier@bsse.ethz.ch</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Emmenlauer</surname>
          <given-names>Mario</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Dehio</surname>
          <given-names>Christoph</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Beerenwinkel</surname>
          <given-names>Niko</given-names>
        </name>
        <address>
          <email>niko.beerenwinkel@bsse.ethz.ch</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2156 2780</institution-id><institution-id institution-id-type="GRID">grid.5801.c</institution-id><institution>Department of Biosystems Science and Engineering, ETH Zurich, </institution></institution-wrap>Basel, Switzerland </aff>
      <aff id="Aff2"><label>2</label>SIB Swiss Institute of Bioinformatics, Basel, Switzerland </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1937 0642</institution-id><institution-id institution-id-type="GRID">grid.6612.3</institution-id><institution>Biozentrum, University of Basel, </institution></institution-wrap>Basel, Switzerland </aff>
      <aff id="Aff4"><label>4</label>BioDataAnalysis GmbH, Munich, 81669 Germany </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>12</day>
      <month>11</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>12</day>
      <month>11</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>20</volume>
    <elocation-id>564</elocation-id>
    <history>
      <date date-type="received">
        <day>29</day>
        <month>4</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>9</day>
        <month>9</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License(<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Analysing large and high-dimensional biological data sets poses significant computational difficulties for bioinformaticians due to lack of accessible tools that scale to hundreds of millions of data points.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">We developed a novel machine learning command line tool called PyBDA for automated, distributed analysis of big biological data sets. By using Apache Spark in the backend, PyBDA scales to data sets beyond the size of current applications. It uses Snakemake in order to automatically schedule jobs to a high-performance computing cluster. We demonstrate the utility of the software by analyzing image-based RNA interference data of 150 million single cells.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p id="Par3">PyBDA allows automated, easy-to-use data analysis using common statistical methods and machine learning algorithms. It can be used with simple command line calls entirely making it accessible to a broad user base. PyBDA is available at <ext-link ext-link-type="uri" xlink:href="https://pybda.rtfd.io">https://pybda.rtfd.io</ext-link>.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Big data</kwd>
      <kwd>Data analysis</kwd>
      <kwd>Command line</kwd>
      <kwd>Pipeline</kwd>
      <kwd>Computing cluster</kwd>
      <kwd>Grid engine</kwd>
      <kwd>Machine learning</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p>The advent of technologies that produce very large amounts of high-dimensional biological data is posing not only statistical, but primarily computational difficulties for researchers in bioinformatics, including in single-cell sequencing, genome-wide association studies, or imaging [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR3">3</xref>]. For statistical analysis and machine learning of gene expression data, tools such as Scanpy [<xref ref-type="bibr" rid="CR4">4</xref>] exist. However, they scale only up to a (few) million observations rendering them unsuitable for the analysis of, e.g., microscopy imaging data often comprising billions of cells. Approaches that scale to big data sets by using high-performance computing, such as reviewed in [<xref ref-type="bibr" rid="CR5">5</xref>], have been developed mainly for sequence analysis, but not statistical analysis for data derived from, for instance, imaging or mass spectrometry.</p>
    <p>Here, we introduce PyBDA, a Python command line tool for automated analysis of big biological data sets. PyBDA offers easily customizable machine learning pipelines that require only minimal programming knowledge. The main goal of PyBDA is to simplify the repetitive, time-consuming task of creating customized machine learning pipelines and combine it with distributed computation on high-performance clusters. The main contributions of PyBDA are (i) a command line tool for the analysis of big data sets with automated pipelines and generation of relevant plots after each analysis, (ii) various statistical and machine learning methods either using novel, custom implementations or interfacing to MLLib [<xref ref-type="bibr" rid="CR6">6</xref>] from Apache Spark [<xref ref-type="bibr" rid="CR7">7</xref>], and (iii) a modularized framework that can be easily extended to introduce new methods and algorithms. We built PyBDA with a special emphasis on ease of usability and automation of multiple machine learning tasks, such that minimal programming and implementation effort is required and tasks can be executed quickly.</p>
  </sec>
  <sec id="Sec2" sec-type="overview">
    <title>Overview</title>
    <p>PyBDA provides various statistical methods and machine learning algorithms that scale to very large, high-dimensional data sets. Since most machine learning algorithms are computationally expensive and big high-dimensional data does not fit into the memory of standard desktop computers, PyBDA uses Apache Spark’s DataFrame API for computation which automatically partitions data across nodes of a computing cluster, or, if no cluster environment is available, uses the resources available.</p>
    <p>In comparison to other data analysis libraries, for instance [<xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR9">9</xref>], where the user needs to use the provided API, PyBDA is a command line tool that does not require extensive programming knowledge. Instead the user only needs to define a config file in which they specify the algorithms to be used. PyBDA then automatically builds a workflow and executes the specified methods one after another. PyBDA uses Snakemake [<xref ref-type="bibr" rid="CR10">10</xref>] to automatically execute these workflows of methods.</p>
    <p>Specifically, PyBDA implements the following workflow to enable pipelining of multiple data analysis tasks (Fig. <xref rid="Fig1" ref-type="fig">1</xref>): PyBDA builds an abstract Petri net from a config file containing a list of statistical methods or machine learning algorithms to be executed. A Petri net is a bipartite, directed graph in which one set of nodes represents conditions (in our case data sets) and the other set represents transitions (in our case operations like machine learning methods and statistical models). A transition in a Petri net model can only be enabled if a condition is met, i.e., in our case when a data set that is used as input for a method exists on the file system. Firing a transition leads to the creation of a new condition, i.e., a new data set. Every operation in the Petri net, i.e., every triple of input file, method and output file, is then executed by Snakemake. The method of every triple is a Python module with the main functionality being implemented with Spark’s DataFrame and RDD API or MLLib. By using Spark, data sets are automatically chunked into smaller pieces, and executed on a distributed high performance computing (HPC) cluster in parallel on multiple cores. Through distributed, parallel computing it is possible to fit models and apply methods even to big, high-dimensional data sets.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Using PyBDA. (1) To use PyBDA, the user only requires to create a short config file that lists the different methods to be executed. (2) From the config file, PyBDA creates an abstract Petri net, i.e., a bipartite directed graph with <italic>data nodes</italic> (gray squares) and <italic>operation nodes</italic> (analysis methods, green rectangles). (3) PyBDA traverses the net and creates triples, i.e., subgraphs consisting of an input file, an associated analysis method, and an output file. It then uses Snakemake for execution of each triple. The associated method of every triple is implemented as a Python module, each developed against the DataFrame API from Apache Spark. Spark uses a master to chunk a method into several tasks and distributes these on worker nodes on a distributed HPC cluster</p></caption><graphic xlink:href="12859_2019_3087_Fig1_HTML" id="MO1"/></fig>
</p>
    <sec id="Sec3">
      <title>Comparison to other big data tools</title>
      <p>In the last decade several big data analysis and machine learning frameworks have been proposed, yet none of them allow for easy, automated pipelining of multiple data analysis or machine learning tasks. Here, we briefly compare the pros and cons of PyBDA with some of the most popular frameworks, including TensorFlow [<xref ref-type="bibr" rid="CR11">11</xref>], scikit-learn [<xref ref-type="bibr" rid="CR8">8</xref>], mlr [<xref ref-type="bibr" rid="CR9">9</xref>], MLLib [<xref ref-type="bibr" rid="CR6">6</xref>] and h20 [<xref ref-type="bibr" rid="CR12">12</xref>]. Furthermore, many other machine learning tools, such as PyTorch [<xref ref-type="bibr" rid="CR13">13</xref>], Keras [<xref ref-type="bibr" rid="CR14">14</xref>] or Edward [<xref ref-type="bibr" rid="CR15">15</xref>] that are comparable in functionality to the previous frameworks exist. For the sake of completeness, we also mention tools for probabilistic modelling, such as PyMC3 [<xref ref-type="bibr" rid="CR16">16</xref>], GPFlow [<xref ref-type="bibr" rid="CR17">17</xref>] or greta [<xref ref-type="bibr" rid="CR18">18</xref>] which, of course, are primarily designed for statistical modelling and probabilistic programming and not for big data analysis.</p>
      <p>We compare the different tools using the following criteria (Table <xref rid="Tab1" ref-type="table">1</xref>): (1) how easily can the tool be used, especially w.r.t. programming knowledge (<italic>usability</italic>), (2) how much time does it take to implement a method/model once the API has been learned (<italic>time to implement</italic>), (3) how much knowledge of machine learning (ML), optimization, modelling and statistics is needed to use the tool (<italic>ML knowledge</italic>), (4) is it possible to use big data with the tool, i.e., does it scale well to big and high-dimensional data sets (<italic>big data</italic>), (5) how many methods are supported from scratch without the need to implement them (<italic>supported methods</italic>), and (6) is the tool easily extended with new methods, e.g., using the provided API (<italic>extensibility</italic>).
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Common statistical analysis and machine learning tools</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left"><inline-graphic xlink:href="12859_2019_3087_Figa_HTML.gif" id="d29e543"/></td><td align="left" colspan="6"/></tr></tbody></table><table-wrap-foot><p>We compare every tool by trichotomized criteria to evaluate if it places above average (green mark), on average (orange mark), or below average (red marks)</p></table-wrap-foot></table-wrap>
</p>
      <p>In comparison to PyBDA, the other methods we considered here are either complex to learn, take some time to get used to, or are not able to cope with big data sets. For instance, TensorFlow scales well to big, high-dimensional data sets and allows for the implementation of basically any numerical method. However, while being the most advanced of the compared tools, it has a huge, complex API and needs extensive knowledge of machine learning to be usable, for instance to implement the evidence lower bound of a variational autoencoder or to choose an optimizer for minimizing a custom loss function. On the other hand, tools such as scikit-learn and mlr are easy to use and have a large range of supported methods, but do not scale well, because some of their functionality is not distributable on HPC clusters and consequently not suitable for big data. The two tools that are specifically designed for big data, namely MLLib and h20, are very similar to each other. A drawback of both is the fact that the range of models and algorithms is rather limited in comparison to tools such as scikit-learn and mlr. In comparison to h20’s H20Frame API, we think Spark not only provides a superior DataFrame/RDD API that has more capabilities and is easier for extending a code base with new methods, but also has better integration for linear algebra. For instance, computation of basic descriptive statistics using map-reduce or matrix multiplication are easier implemented using Spark.</p>
      <p>PyBDA is the only specifically built to not require much knowledge of programming or machine learning. It can be used right away without much time to get used to an API. Furthermore, due to using Spark it scales well and can be extended easily.</p>
    </sec>
  </sec>
  <sec id="Sec4">
    <title>Implementation</title>
    <sec id="Sec5">
      <title>Supported algorithms</title>
      <p>PyBDA comes with a variety of algorithms for analysing big data from which the user can choose (Table <xref rid="Tab2" ref-type="table">2</xref>). Unless already provided by MLLib, we implemented the algorithms against Spark’s DataFrame API. Especially efficient implementations of common scalable dimension reduction methods included in PyBDA, such as kernel principal component analysis (kPCA), independent component analysis (ICA), linear discriminant analysis (LDA) and factor analysis (FA), have been missing in current open source software entirely. PyBDA primarily supports simple models that do not trade biological interpretability for mathematical complexity and performance.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Methods provided by PyBDA</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Category</th><th align="left">Method</th><th align="left">Implementation</th></tr></thead><tbody><tr><td align="left">Dimension reduction</td><td align="justify">PCA</td><td align="left">MLLib</td></tr><tr><td align="justify"/><td align="justify">Factor analysis</td><td align="left">custom implementation</td></tr><tr><td align="justify"/><td align="justify"><italic>k</italic>-PCA</td><td align="left">custom implementation</td></tr><tr><td align="justify"/><td align="justify">Linear discriminant analysis</td><td align="left">custom implementation</td></tr><tr><td align="justify"/><td align="justify">Independent component analysis</td><td align="left">custom implementation</td></tr><tr><td align="left">Clustering</td><td align="justify"><italic>k</italic>-means</td><td align="left">MLLib</td></tr><tr><td align="justify"/><td align="justify">Gaussian mixture models</td><td align="left">MLLib</td></tr><tr><td align="left">Supervised learning</td><td align="justify">Random forests</td><td align="left">MLLib</td></tr><tr><td align="justify"/><td align="justify">Gradient boosting</td><td align="left">MLLib</td></tr><tr><td align="justify"/><td align="justify">Generalized linear models</td><td align="left">MLLib</td></tr></tbody></table></table-wrap>
</p>
    </sec>
    <sec id="Sec6">
      <title>Running pyBDA</title>
      <p>In order to run PyBDA on a Spark cluster, the user needs to provide an IP address to which Spark sends its jobs. Consequently, users need to either setup a cluster (standalone, Kubernetes, etc.) or submit jobs to the local host, where the strength of PyBDA is computation on a distributed HPC environment. Given the IP of the Spark cluster, the user needs to provide a config file with methods, data files, and parameterization. For instance, the config file provided in Fig. <xref rid="Fig2" ref-type="fig">2</xref>a will first trigger dimension reductions using principal component analysis (PCA) and ICA to 5 dimensions on a data set called single_cell_samples.tsv and feature names provided in feature_columns.tsv. PyBDA then uses the outputs of both methods and fits Gaussian mixture models (GMM) and runs <italic>k</italic>-means to each output with 50, or 100, cluster centers, respectively (resulting in four different results). In addition, a generalized linear model (GLM) and a random forest (RF) with binomial response variable (named is_infected) will be fitted on the same features. Thus, PyBDA automatically parses all combinations of methods and automatically executes each combination (Fig. <xref rid="Fig2" ref-type="fig">2</xref>b shows the corresponding Petri net of files and operations). The results of all methods are written to a folder called results. For each job, PyBDA allows Spark to use 15Gb of driver memory (for the master) and 50Gb memory for each executor (the main process run by a worker node).
<fig id="Fig2"><label>Fig. 2</label><caption><p>A PyBDA config file and the corresponding Petri net. Executing a config file (<bold>a</bold>) generates a corresponding Petri net (<bold>b</bold>). Here, PyBDA uses a file of single cells as input and then executes dimension reductions (PCA, ICA) and regression models (GLM, RF). The outputs from the dimension reductions are further used for clustering (GMM, <italic>k</italic>-means) resulting in a total of six sets of files</p></caption><graphic xlink:href="12859_2019_3087_Fig2_HTML" id="MO2"/></fig>
</p>
    </sec>
  </sec>
  <sec id="Sec7" sec-type="results">
    <title>Results</title>
    <p>In order to demonstrate PyBDA’s capability to deal with big biological data, we preprocessed the features extracted from microscopy imaging data of a large-scale RNA interference screen of the pathogen <italic>B. henselae</italic> and used them for big data analysis. In summary, HeLa cells have first been seeded on 384-well plates. In every well, a single gene has been knocked down and subsequently infected with <italic>B. henselae</italic>. After infection, images of cells have been taken for every plate and well, and for each cell, 43 image features have been extracted (Fig. <xref rid="Fig3" ref-type="fig">3</xref>). Features consist either of spatial/geometrical cell and nucleus properties (cells stained yellow, nuclei stained blue) or information about local cell neighborhood (Additional file 1 – features). Assuming that image features impact the cell’s infection we regressed the binary response of infection status on these features. Specifically, we hypothesized that cells in densely populated regions, or with comparatively little cell area, should on average be less vulnerable to infection in comparison to larger cells in sparsely populated regions (<italic>B. henselae</italic> stained green). Inference of the parameters for the infection status is of particular interest, because it could make using dedicated flourescence markers for pathogens obsolete. Since the data set consists of roughly 150 million single cells, a conventional analysis on desktop computers is not possible. However, it becomes feasible on a distributed environment using our command line tool PyBDA. Using a config file similar to the one in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, we fit a generalized linear model with a binomial response, a random forest, and gradient boosting machines (GBM) to the data set. In order to avoid bias, PyBDA automatically balances the data set to contain equal fractions of each class by downsampling. We found that all three methods are capable of predicting the infection state of a cell from the image features well. Overall, the GLM performed slightly poorer (precision 0.70, recall 0.68) than the GBM (precision 0.73, recall 0.74; trained with 20 decision trees; subsampling rate of data 0.25) or the RF (precision 0.71, recall 0.71; same parameters). Since we are in an almost asymptotic regime of sample size <italic>n</italic>, splitting the data into train and test sets yields the same errors on both sets. Thus we are reporting the performance measures and parameters on the full data set here. While the RF and GBM improve performance, their biological interpretation is more challenging, because they do not establish simple, parametric dependencies as the GLM. For the GLM we found that features such as the cell area (<italic>β</italic>=0.21) or cell perimeter (<italic>β</italic>=0.18) contribute to enhanced infection, while features such as the number of cell neighbors (<italic>β</italic>=−0.11) decrease infectivity. Fitting the GLM required 2:30h runtime on an HPC platform, using a rather small cluster with two nodes and five cores each and 15 Gb of memory per core. Fitting the RF and the GBM took roughly 8h each, and required increasing the resources to five worker nodes with 10 cores and 20Gb each. The amount of parallelization and available computing resources is pivotal for runtime and insofar independent of PyBDA, as all computations are run by Spark. Runtime benchmarks of big data tools including Spark have, for instance, already been conducted by others [<xref ref-type="bibr" rid="CR19">19</xref>, <xref ref-type="bibr" rid="CR20">20</xref>].
<fig id="Fig3"><label>Fig. 3</label><caption><p>An example of a single-cell image used for segmentation and extraction of image features. We extracted image features of 150 million individual cells, their nuclei and their perinuclei. Cell bodies are stained in yellow, nuclei in blue, pathogens in green (left image). Features consist of cell, nucleus and perinuclei properties and information about local cell neighborhood, and if a cell is infected with a pathogen or not (right image)</p></caption><graphic xlink:href="12859_2019_3087_Fig3_HTML" id="MO3"/></fig>
</p>
  </sec>
  <sec id="Sec8" sec-type="conclusion">
    <title>Conclusion</title>
    <p>PyBDA is a command line tool for machine learning of big biological data sets scaling up to hundreds of millions of data points. PyBDA automatically parses a user defined pipeline of multiple machine learning and data analysis tasks from a config file and distributes jobs to compute nodes using Snakemake and Apache Spark. We believe PyBDA will be a valuable and user-friendly tool supporting big data analytics and continued community-driven development of new algorithms.</p>
  </sec>
  <sec id="Sec9">
    <title>Availability and requirements</title>
    <p><bold>Project name:</bold> PyBDA</p>
    <p>
      <bold>Project home page:</bold>
      <ext-link ext-link-type="uri" xlink:href="https://github.com/cbg-ethz/pybda">https://github.com/cbg-ethz/pybda</ext-link>
    </p>
    <p><bold>Operating system(s):</bold> Linux and MacOS X</p>
    <p><bold>Programming language:</bold> Python</p>
    <p><bold>Other requirements:</bold> Python 3.6, Java JDK 8, Apache Spark 2.4.0</p>
    <p><bold>License:</bold> GNU GPLv3</p>
    <p><bold>Any restrictions to use by non-academics:</bold> License needed</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec10">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="12859_2019_3087_MOESM1_ESM.tsv">
            <caption>
              <p><bold>Additional file 1</bold> Features used for the regression tasks.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>FA</term>
        <def>
          <p>Factor analysis</p>
        </def>
      </def-item>
      <def-item>
        <term>GBM</term>
        <def>
          <p>Gradient boosting machines</p>
        </def>
      </def-item>
      <def-item>
        <term>GLM</term>
        <def>
          <p>Generalized linear model</p>
        </def>
      </def-item>
      <def-item>
        <term>GMM</term>
        <def>
          <p>Gaussian mixture model</p>
        </def>
      </def-item>
      <def-item>
        <term>HPC</term>
        <def>
          <p>High performance computing</p>
        </def>
      </def-item>
      <def-item>
        <term>ICA</term>
        <def>
          <p>Independent component analysis</p>
        </def>
      </def-item>
      <def-item>
        <term>LDA</term>
        <def>
          <p>Linear discriminant analysis</p>
        </def>
      </def-item>
      <def-item>
        <term>ML</term>
        <def>
          <p>Machine learning</p>
        </def>
      </def-item>
      <def-item>
        <term>PCA</term>
        <def>
          <p>Principal component analysis</p>
        </def>
      </def-item>
      <def-item>
        <term>RF</term>
        <def>
          <p>Random forest</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p><bold>Supplementary information</bold> accompanies this paper at 10.1186/s12859-019-3087-8.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>The authors want to thank David Seifert and Martin Pirkl for fruitful discussions and Kim Jablonski for reviewing of and commenting on code and implementation details.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>SD developed PyBDA and its documentation. SD conducted the application. SD and NB wrote the manuscript. ME and CD extracted features from the cells and provided the data. CD and NB lead and conducted the research. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work has been funded by SystemsX.ch, the Swiss Initiative in Systems Biology, under grant RTD 2013/152 (TargetInfectX), and by ERASysAPP, the ERA-Net for Applied Systems Biology, under grant ERASysAPP-30 (SysVirDrug). The funding agencies were not involved in the design of the study, nor collection, analysis, and interpretation of data, nor in writing the manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>PyBDA is available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/cbg-ethz/pybda">https://github.com/cbg-ethz/pybda</ext-link>), the Python Package Index (<ext-link ext-link-type="uri" xlink:href="https://pypi.org/project/pybda/">https://pypi.org/project/pybda/</ext-link>), or Bioconda (<ext-link ext-link-type="uri" xlink:href="https://bioconda.github.io/recipes/pybda/README.html">https://bioconda.github.io/recipes/pybda/README.html</ext-link>). Documentation is available at <ext-link ext-link-type="uri" xlink:href="https://pybda.readthedocs.io/en/latest/">https://pybda.readthedocs.io/en/latest/</ext-link>. The datasets used for the example are available from 10.3929/ethz-b-000360833.</p>
  </notes>
  <notes>
    <title>Ethics approval and consent to participate</title>
    <p>Not applicable.</p>
  </notes>
  <notes>
    <title>Consent for publication</title>
    <p>Not applicable.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Competing interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bühlmann</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>van de Geer</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Statistics for big data: A perspective</article-title>
        <source>Stat Probab Lett</source>
        <year>2018</year>
        <volume>136</volume>
        <fpage>37</fpage>
        <lpage>41</lpage>
        <pub-id pub-id-type="doi">10.1016/j.spl.2018.02.016</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2</label>
      <mixed-citation publication-type="other">Katal A, Wazid M, Goudar RH. Big Data: Issues, Challenges, Tools and Good Practices. In: 2013 Sixth International Conference on Contemporary Computing (IC3). IEEE: 2013. p. 404–9. 10.1109/IC3.2013.661222.</mixed-citation>
    </ref>
    <ref id="CR3">
      <label>3</label>
      <mixed-citation publication-type="other">Marx V. The big challenges of big data. Nature 498. 2013.</mixed-citation>
    </ref>
    <ref id="CR4">
      <label>4</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wolf</surname>
            <given-names>FA</given-names>
          </name>
          <name>
            <surname>Angerer</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Theis</surname>
            <given-names>FJ</given-names>
          </name>
        </person-group>
        <article-title>Scanpy: large-scale single-cell gene expression data analysis</article-title>
        <source>Genome Biol</source>
        <year>2018</year>
        <volume>19</volume>
        <issue>1</issue>
        <fpage>15</fpage>
        <pub-id pub-id-type="doi">10.1186/s13059-017-1382-0</pub-id>
        <pub-id pub-id-type="pmid">29409532</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Guo</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zou</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Fang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Bioinformatics applications on Apache Spark</article-title>
        <source>GigaScience</source>
        <year>2018</year>
        <volume>7</volume>
        <issue>8</issue>
        <fpage>098</fpage>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Meng</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Bradley</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Yavuz</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Sparks</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Venkataraman</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Freeman</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Tsai</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Amde</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Owen</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Mllib: Machine learning in Apache Spark</article-title>
        <source>J Mach Learn Res</source>
        <year>2016</year>
        <volume>17</volume>
        <issue>1</issue>
        <fpage>1235</fpage>
        <lpage>41</lpage>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zaharia</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Xin</surname>
            <given-names>RS</given-names>
          </name>
          <name>
            <surname>Wendell</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Das</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Armbrust</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Dave</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Meng</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Rosen</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Venkataraman</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Franklin</surname>
            <given-names>MJ</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Apache Spark: A unified engine for big data processing</article-title>
        <source>Commun ACM</source>
        <year>2016</year>
        <volume>59</volume>
        <issue>11</issue>
        <fpage>56</fpage>
        <lpage>65</lpage>
        <pub-id pub-id-type="doi">10.1145/2934664</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pedregosa</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Varoquaux</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Gramfort</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Michel</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Thirion</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Grisel</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Blondel</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Prettenhofer</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Weiss</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Dubourg</surname>
            <given-names>V</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Scikit-learn: Machine learning in Python</article-title>
        <source>J Mach Learn Res</source>
        <year>2011</year>
        <volume>12</volume>
        <issue>Oct</issue>
        <fpage>2825</fpage>
        <lpage>30</lpage>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bischl</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Lang</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kotthoff</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Schiffner</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Richter</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Studerus</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Casalicchio</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Jones</surname>
            <given-names>ZM</given-names>
          </name>
        </person-group>
        <article-title>mlr: Machine Learning in R</article-title>
        <source>J Mach Learn Res</source>
        <year>2016</year>
        <volume>17</volume>
        <issue>1</issue>
        <fpage>5938</fpage>
        <lpage>42</lpage>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Köster</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Rahmann</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Snakemake—a scalable bioinformatics workflow engine</article-title>
        <source>Bioinformatics</source>
        <year>2012</year>
        <volume>28</volume>
        <issue>19</issue>
        <fpage>2520</fpage>
        <lpage>2</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bts480</pub-id>
        <pub-id pub-id-type="pmid">22908215</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11</label>
      <mixed-citation publication-type="other">Abadi M, Agarwal A, Barham P, Brevdo E, Citro C, Corrado GS, Davis A, Dean J, Devin M, et al.TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems. arXiv preprint arXiv:1603.04467. 2016.</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12</label>
      <mixed-citation publication-type="other">H, 2O.ai. Python Interface for H2O. 2019. Python module version 3.26.0.2. <ext-link ext-link-type="uri" xlink:href="https://github.com/h2oai/h2o-3">https://github.com/h2oai/h2o-3</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13</label>
      <mixed-citation publication-type="other">Paszke A, Gross S, Chintala S, Chanan G, Yang E, DeVito Z, Lin Z, Desmaison A, Antiga L, Lerer A. Automatic Differentiation in PyTorch. In: NIPS Autodiff Workshop: 2017.</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14</label>
      <mixed-citation publication-type="other">Chollet F, et al.Keras. 2015. <ext-link ext-link-type="uri" xlink:href="https://keras.io">https://keras.io</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15</label>
      <mixed-citation publication-type="other">Tran D, Kucukelbir A, Dieng AB, Rudolph M, Liang D, Blei DM. Edward: A library for probabilistic modeling, inference, and criticism. arXiv preprint arXiv:1610.09787. 2016.</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Salvatier</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wiecki</surname>
            <given-names>TV</given-names>
          </name>
          <name>
            <surname>Fonnesbeck</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Probabilistic programming in Python using PyMC3</article-title>
        <source>PeerJ Comput Sci</source>
        <year>2016</year>
        <volume>2</volume>
        <fpage>55</fpage>
        <pub-id pub-id-type="doi">10.7717/peerj-cs.55</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Matthews</surname>
            <given-names>DG</given-names>
          </name>
          <name>
            <surname>Alexander</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Van Der Wilk</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Nickson</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Fujii</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Boukouvalas</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>León-Villagrá</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Ghahramani</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Hensman</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>GPflow: A Gaussian Process Library using TensorFlow</article-title>
        <source>J Mach Learn Res</source>
        <year>2017</year>
        <volume>18</volume>
        <issue>1</issue>
        <fpage>1299</fpage>
        <lpage>304</lpage>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18</label>
      <mixed-citation publication-type="other">Golding N. Greta: Simple and Scalable Statistical Modelling in R. 2018. R package version 0.3.0. <ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/package=greta">https://CRAN.R-project.org/package=greta</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19</label>
      <mixed-citation publication-type="other">Pafka S. benchm-ml. GitHub. 2019. <ext-link ext-link-type="uri" xlink:href="https://github.com/szilard/benchm-ml/tree/941dfd4ebab3854b3a49fd70c192ecf21e483267">https://github.com/szilard/benchm-ml/tree/941dfd4ebab3854b3a49fd70c192ecf21e483267</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>García-Gil</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Ramírez-Gallego</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>García</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Herrera</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>A comparison on scalability for batch big data processing on Apache Spark and Apache Flink</article-title>
        <source>Big Data Analytics</source>
        <year>2017</year>
        <volume>2</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <pub-id pub-id-type="doi">10.1186/s41044-016-0020-2</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
