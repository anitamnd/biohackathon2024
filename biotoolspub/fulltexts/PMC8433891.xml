<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id>
    <journal-id journal-id-type="publisher-id">sensors</journal-id>
    <journal-title-group>
      <journal-title>Sensors (Basel, Switzerland)</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1424-8220</issn>
    <publisher>
      <publisher-name>MDPI</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8433891</article-id>
    <article-id pub-id-type="pmid">34502629</article-id>
    <article-id pub-id-type="doi">10.3390/s21175740</article-id>
    <article-id pub-id-type="publisher-id">sensors-21-05740</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>BioPyC, an Open-Source Python Toolbox for Offline Electroencephalographic and Physiological Signals Classification</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-0272-3726</contrib-id>
        <name>
          <surname>Appriou</surname>
          <given-names>Aurélien</given-names>
        </name>
        <xref ref-type="aff" rid="af1-sensors-21-05740">1</xref>
        <xref ref-type="aff" rid="af2-sensors-21-05740">2</xref>
        <xref ref-type="aff" rid="af3-sensors-21-05740">3</xref>
        <xref ref-type="aff" rid="af4-sensors-21-05740">4</xref>
        <xref rid="c1-sensors-21-05740" ref-type="corresp">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Pillette</surname>
          <given-names>Léa</given-names>
        </name>
        <xref ref-type="aff" rid="af1-sensors-21-05740">1</xref>
        <xref ref-type="aff" rid="af2-sensors-21-05740">2</xref>
        <xref ref-type="aff" rid="af3-sensors-21-05740">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Trocellier</surname>
          <given-names>David</given-names>
        </name>
        <xref ref-type="aff" rid="af1-sensors-21-05740">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Dutartre</surname>
          <given-names>Dan</given-names>
        </name>
        <xref ref-type="aff" rid="af1-sensors-21-05740">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Cichocki</surname>
          <given-names>Andrzej</given-names>
        </name>
        <xref ref-type="aff" rid="af3-sensors-21-05740">3</xref>
        <xref ref-type="aff" rid="af5-sensors-21-05740">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-6888-9198</contrib-id>
        <name>
          <surname>Lotte</surname>
          <given-names>Fabien</given-names>
        </name>
        <xref ref-type="aff" rid="af1-sensors-21-05740">1</xref>
        <xref ref-type="aff" rid="af2-sensors-21-05740">2</xref>
        <xref ref-type="aff" rid="af3-sensors-21-05740">3</xref>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Munoz-Organero</surname>
          <given-names>Mario</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <aff id="af1-sensors-21-05740"><label>1</label>Inria Bordeaux Sud-Ouest, 33405 Talence, France; <email>lea.pillette@inria.fr</email> (L.P.); <email>david.trocellier@inria.fr</email> (D.T.); <email>dan.dutartre@inria.fr</email> (D.D.); <email>fabien.lotte@inria.fr</email> (F.L.)</aff>
    <aff id="af2-sensors-21-05740"><label>2</label>LaBRI (University of Bordeaux, CNRS, Bordeaux-INP), 33400 Talence, France</aff>
    <aff id="af3-sensors-21-05740"><label>3</label>RIKEN, Wako 351-0106, Japan; <email>A.Cichocki@skoltech.ru</email></aff>
    <aff id="af4-sensors-21-05740"><label>4</label>Flit Sport, 33000 Bordeaux, France</aff>
    <aff id="af5-sensors-21-05740"><label>5</label>Skoltech, 143026 Moscow, Russia</aff>
    <author-notes>
      <corresp id="c1-sensors-21-05740"><label>*</label>Correspondence: <email>aurelien.appriou@flit-sport.fr</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>26</day>
      <month>8</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <month>9</month>
      <year>2021</year>
    </pub-date>
    <volume>21</volume>
    <issue>17</issue>
    <elocation-id>5740</elocation-id>
    <history>
      <date date-type="received">
        <day>30</day>
        <month>6</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>20</day>
        <month>8</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2021 by the authors.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Research on brain–computer interfaces (BCIs) has become more democratic in recent decades, and experiments using electroencephalography (EEG)-based BCIs has dramatically increased. The variety of protocol designs and the growing interest in physiological computing require parallel improvements in processing and classification of both EEG signals and bio signals, such as electrodermal activity (EDA), heart rate (HR) or breathing. If some EEG-based analysis tools are already available for online BCIs with a number of online BCI platforms (e.g., BCI2000 or OpenViBE), it remains crucial to perform offline analyses in order to design, select, tune, validate and test algorithms before using them online. Moreover, studying and comparing those algorithms usually requires expertise in programming, signal processing and machine learning, whereas numerous BCI researchers come from other backgrounds with limited or no training in such skills. Finally, existing BCI toolboxes are focused on EEG and other brain signals but usually do not include processing tools for other bio signals. Therefore, in this paper, we describe BioPyC, a free, open-source and easy-to-use Python platform for offline EEG and biosignal processing and classification. Based on an intuitive and well-guided graphical interface, four main modules allow the user to follow the standard steps of the BCI process without any programming skills: (1) reading different neurophysiological signal data formats, (2) filtering and representing EEG and bio signals, (3) classifying them, and (4) visualizing and performing statistical tests on the results. We illustrate BioPyC use on four studies, namely classifying mental tasks, the cognitive workload, emotions and attention states from EEG signals.</p>
    </abstract>
    <kwd-group>
      <kwd>brain–computer interfaces (BCI)</kwd>
      <kwd>electroencephalography (EEG)</kwd>
      <kwd>Python platform</kwd>
      <kwd>signal processing</kwd>
      <kwd>machine learning</kwd>
      <kwd>physiological signals</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="sec1-sensors-21-05740">
    <title>1. Introduction</title>
    <p>Research on brain–computer interfaces (BCIs) started in 1973 with Jacques Vidal and his concept of direct brain–computer communication [<xref rid="B1-sensors-21-05740" ref-type="bibr">1</xref>], enabling the transfer of information from the human brain to a machine via brain signals, usually measured using electroencephalography (EEG) [<xref rid="B2-sensors-21-05740" ref-type="bibr">2</xref>]. The main objective of BCIs is to enable people with severe motor impairments to send commands to a wheelchair, e.g., by imagining left or right hand movements to make the wheelchair turn left or right [<xref rid="B3-sensors-21-05740" ref-type="bibr">3</xref>]. Such BCIs are called active BCIs since users are actively sending commands to the system, here a wheelchair, by performing mental imagery tasks [<xref rid="B4-sensors-21-05740" ref-type="bibr">4</xref>]. Another type of BCIs proved particularly promising for human–computer interaction (HCI): passive BCIs [<xref rid="B4-sensors-21-05740" ref-type="bibr">4</xref>]. Such BCIs are not used to directly control some devices, but to monitor in real time users’ mental states, e.g., mental workload or attention, in order to adapt an application accordingly. Some other BCIs do not use brain signals only. They can use a different type of input in addition, and are thus called hybrid BCIs [<xref rid="B5-sensors-21-05740" ref-type="bibr">5</xref>]. This additional input could be physiological signals, such as heart rate (HR), electrodermal activity (EDA), breathing or signals from other external device, such as an eye-tracking system.</p>
    <sec id="sec1dot1-sensors-21-05740">
      <title>1.1. Motivation</title>
      <p>Although promising, non-invasive BCIs are still barely used outside laboratories, due to their poor robustness with respect to noise and environmental conditions. In other words, they are sensitive to noise, outliers and the non-stationarity of electroencephalographic (EEG) signals [<xref rid="B6-sensors-21-05740" ref-type="bibr">6</xref>,<xref rid="B7-sensors-21-05740" ref-type="bibr">7</xref>]. While considering and optimizing user training is essential for improving such systems [<xref rid="B8-sensors-21-05740" ref-type="bibr">8</xref>,<xref rid="B9-sensors-21-05740" ref-type="bibr">9</xref>], the computing machinery considerably evolved in the last decades, and numerous signal processing and machine learning algorithms for brain signals classification have also been developed [<xref rid="B10-sensors-21-05740" ref-type="bibr">10</xref>]. It is, therefore, important to be able to test many promising new algorithms, resulting from both signal processing and machine learning research, on different data sets related to different paradigms in order to identify the most efficient tools for their future uses. This type of study can be complicated to set up online because of the costs in terms of time, e.g., equipping a subject with different sensors to measure the activity in order to analyze it online, or in terms of calculation, given that some algorithms, such as deep learning, may require a lot of computing resources and would be difficult to run online. However, such studies are possible offline, by applying recent and promising classification algorithms on existing data sets, and are actually widely used in the BCI community, notably to compare various algorithms on the exact same data [<xref rid="B11-sensors-21-05740" ref-type="bibr">11</xref>]. While offline studies are simpler to set up than online analyses, they still require specific tools and skills to be run effectively and efficiently. Indeed, using such algorithms requires expertise in programming (e.g., in MATLAB or Python), signal processing, machine learning, as well as statistics for analyzing the resulting performances of the different algorithms; however, many BCI researchers come from diverse backgrounds, such as cognitive science, neuroscience or psychology, and might not master all those skills. If multiple BCI toolboxes are already available, they all require such skills, and most of them are focused on EEG and other brain signals, but usually do not include processing tools for other biosignals. It therefore highlights the need for convenient toolboxes that would be free, open source and equipped with a graphical interface that would allow users to process and classify EEG and other biological signals offline without any programming skills.</p>
    </sec>
    <sec id="sec1dot2-sensors-21-05740">
      <title>1.2. Approach</title>
      <p>We developed and tested BioPyC, an open-source and easy-to-use software for offline EEG and physiological signal processing and classification. BioPyC is free of charge, permissively licensed AGPL ( <uri xlink:href="https://choosealicense.com/licenses/agpl-3.0/">https://choosealicense.com/licenses/agpl-3.0/</uri>; accessed on 25 August 2021) and written in Python [<xref rid="B12-sensors-21-05740" ref-type="bibr">12</xref>], an open-source programming language that is not only backed by an extensive standard library, but also by vast additional scientific computing libraries. This toolbox allows users to make offline EEG and biosignals analyses, i.e., to apply signal processing and classification algorithms to neurophysiological signals, such as EEG, electrocardiographic (ECG) signals, EDA or respiratory signal. See <uri xlink:href="https://gitlab.inria.fr/biopyc/BioPyC/">https://gitlab.inria.fr/biopyc/BioPyC/</uri>; accessed on 25 August 2021. In order to facilitate those analyses, BioPyC offers a graphical user interface (GUI) based on Jupyter [<xref rid="B13-sensors-21-05740" ref-type="bibr">13</xref>] that allows users to handle the toolbox without any prior knowledge in computer science or machine learning. Finally, with BioPyC, users can apply and study algorithms for the main steps of biosignals analysis, i.e., pre-processing, signal processing, classification, statistical analysis and data visualization, as <xref ref-type="fig" rid="sensors-21-05740-f001">Figure 1</xref> summarizes.</p>
      <p>BioPyC enables users to work on two types of data sets: either raw data sets, which require the subsequent use of the pre-processing module (described below), or pre-processed data sets, which allow users to directly apply signal processing and machine learning algorithms on them. The pre-processing module of BioPyC offers basic features, i.e., band-pass filtering, cleaning and epoching raw EEG signals. Next, for the processing step, BioPyC offers two modules that enable users to use signal processing tools, such as spatial filters and feature extraction, but also machine learning algorithms for the classification of neurophysiological signals. Finally, another module enables users to automatically apply appropriate statistical tests on the obtained classification performances, to compare algorithms, and to obtain visualization plots describing those performances.</p>
      <p>While some of the popular existing toolboxes, such as MOABB [<xref rid="B11-sensors-21-05740" ref-type="bibr">11</xref>], pyRiemann [<xref rid="B14-sensors-21-05740" ref-type="bibr">14</xref>] or MNE [<xref rid="B15-sensors-21-05740" ref-type="bibr">15</xref>], listed in the “state-of-the-art” section below, have features in common with BioPyC, e.g., programming language or supported operating systems, as well as modules of the above-mentioned BCI process, e.g., signal processing or data visualization, none of them allow users to apply and study signal and classification algorithms for both EEG and physiological signals. BioPyC is indeed the first one which allows to study and compare classification algorithms for neurophysiological signals (at the moment EEG, EDA, respiration and ECG), offering modules for all the steps of this offline (hybrid) BCI and/or physiological computing process. Concerning the statistics, both tests and visualization for comparing classification algorithms performances are done automatically by BioPyC, again facilitating and speeding up users’ analyses: to the best of our knowledge, no other toolbox offers automatic statistical testing.</p>
      <p>This paper is organized as follows: in <xref ref-type="sec" rid="sec2-sensors-21-05740">Section 2</xref>, we present the state of the art of BCI platforms, the different features they offer, and conclude on the distinctive features that make BioPyC unique. In <xref ref-type="sec" rid="sec3-sensors-21-05740">Section 3</xref>, we describe BioPyC modules and the data flow in more details, including the different algorithms that were implemented for the processing and classification of both EEG and biosignals, as well as the statistical tests available. Then, in <xref ref-type="sec" rid="sec4-sensors-21-05740">Section 4</xref>, we present results of multiple research studies that were conducted using BioPyC so far, to illustrate its use and usefulness. On the one hand, BioPyC was applied to a widely used mental task data set, i.e., the “BCI competition IV data set 2a” [<xref rid="B16-sensors-21-05740" ref-type="bibr">16</xref>]. On the other hand, we used it for studying different users’ mental states, e.g., cognitive workload, emotions and attention states. Finally, the discussion and conclusion come in <xref ref-type="sec" rid="sec5-sensors-21-05740">Section 5</xref>, followed by the future works and improvements from which BioPyC could benefit in <xref ref-type="sec" rid="sec6-sensors-21-05740">Section 6</xref>.</p>
    </sec>
  </sec>
  <sec id="sec2-sensors-21-05740">
    <title>2. State-of-the-Art BCIs Platforms</title>
    <p>So far, several platforms for online experiments (i.e., BioSig [<xref rid="B17-sensors-21-05740" ref-type="bibr">17</xref>], Timeflux [<xref rid="B18-sensors-21-05740" ref-type="bibr">18</xref>], BCI2000 [<xref rid="B19-sensors-21-05740" ref-type="bibr">19</xref>], OpenViBE [<xref rid="B20-sensors-21-05740" ref-type="bibr">20</xref>], TOBI [<xref rid="B21-sensors-21-05740" ref-type="bibr">21</xref>], BCILab [<xref rid="B22-sensors-21-05740" ref-type="bibr">22</xref>] or BCI++ [<xref rid="B23-sensors-21-05740" ref-type="bibr">23</xref>]) and also for offline studies (i.e., MOABB, MNE, EEGLAB [<xref rid="B24-sensors-21-05740" ref-type="bibr">24</xref>], PyEEG) and finally for both online and offline studies (i.e., FieldTrip [<xref rid="B25-sensors-21-05740" ref-type="bibr">25</xref>], Gumpy [<xref rid="B26-sensors-21-05740" ref-type="bibr">26</xref>]) have been developed for researchers in order to build setups that would best suit their needs. They all have modules dedicated to the various BCI processing steps: data acquisition, signal processing, classification, statistical hypothesis testing and visualization [<xref rid="B27-sensors-21-05740" ref-type="bibr">27</xref>]. We synthesized in <xref ref-type="fig" rid="sensors-21-05740-f002">Figure 2</xref> the features of each of these existing platforms, i.e., online vs. offline studies, the availability of a graphical user interface (GUI), the existence of modules for statistical testing or data visualization, the programming language, and the supported systems as well as the type of license, in order to list the strengths and weaknesses of each one. Note that we presented both EEGLAB, which is made for EEG signals analysis, and the BCILAB toolbox, which is a plugin of EEGLAB made for online and offline classification and BCI, in this table. In the following, we compare the different platforms based on each of these features.</p>
    <sec id="sec2dot1-sensors-21-05740">
      <title>2.1. Graphical User Interface (GUI)</title>
      <p>As explained above, it is useful to develop toolboxes with GUIs for the BCI community, as many BCI researchers do not come from a computer science background, notably cognitive scientists, neuroscientists or psychologists. In <xref ref-type="fig" rid="sensors-21-05740-f002">Figure 2</xref>, all Python-based toolboxes, i.e., MOABB [<xref rid="B11-sensors-21-05740" ref-type="bibr">11</xref>], MNE [<xref rid="B15-sensors-21-05740" ref-type="bibr">15</xref>], PyEEG [<xref rid="B28-sensors-21-05740" ref-type="bibr">28</xref>], pyRiemann [<xref rid="B14-sensors-21-05740" ref-type="bibr">14</xref>], gumpy [<xref rid="B26-sensors-21-05740" ref-type="bibr">26</xref>] and Wyrm [<xref rid="B29-sensors-21-05740" ref-type="bibr">29</xref>], suffer from the lack of such interfaces.</p>
    </sec>
    <sec id="sec2dot2-sensors-21-05740">
      <title>2.2. EEG Signal Processing</title>
      <p>All platforms on <xref ref-type="fig" rid="sensors-21-05740-f002">Figure 2</xref> naturally have an EEG signal processing system for classification, more or less elaborated, based on three classical steps, i.e., pre-processing, signal processing and classification.</p>
      <sec id="sec2dot2dot1-sensors-21-05740">
        <title>2.2.1. Pre-Processing</title>
        <p>This step consists, for example, of band-pass filtering raw data into specific frequency bands, epoching raw data into trials or removing artifacts, among others. While all toolboxes propose some forms of pre-processing, the ones with the most advanced pre-processing tools, including plenty of methods dedicated to BCI, are MNE and BCILAB.</p>
      </sec>
      <sec id="sec2dot2dot2-sensors-21-05740">
        <title>2.2.2. Signal Processing</title>
        <p>This step allows users to apply spatial or temporal filters on the signals and to extract features from them. Most existing platforms offer such filters and basic features, e.g., the common spatial pattern (CSP) filter or band power features, which are widely used for EEG signal analysis [<xref rid="B30-sensors-21-05740" ref-type="bibr">30</xref>].</p>
      </sec>
      <sec id="sec2dot2dot3-sensors-21-05740">
        <title>2.2.3. Classification</title>
        <p>All platforms also offer machine learning algorithms, from the simplest ones, such as linear discriminant analysis (LDA) [<xref rid="B10-sensors-21-05740" ref-type="bibr">10</xref>] for most of them, to more complex ones, such as Riemannian geometry classifiers [<xref rid="B31-sensors-21-05740" ref-type="bibr">31</xref>] for pyRiemann [<xref rid="B14-sensors-21-05740" ref-type="bibr">14</xref>] and MOABB [<xref rid="B11-sensors-21-05740" ref-type="bibr">11</xref>], deep Learning for gumpy [<xref rid="B26-sensors-21-05740" ref-type="bibr">26</xref>] and various feature extraction methods in PyEEG [<xref rid="B28-sensors-21-05740" ref-type="bibr">28</xref>].</p>
      </sec>
    </sec>
    <sec id="sec2dot3-sensors-21-05740">
      <title>2.3. Statistical Modeling and Data Visualization</title>
      <p>The last step is divided between the statistical analysis and the visualization of performance results obtained by the machine learning algorithms. First, the visualization allows users to obtain graphs in order to have an overview of the classification results. Statistical modeling, on the other hand, consists of using statistical tests to compare the classification performances of the machine learning algorithms. This step is of primary importance when comparing classification algorithms since statistical tests allow to define, for a given study, which algorithm is the most likely to recognize patterns in neurophysiological signals. To the best of our knowledge, none of the platforms for offline signals analysis listed in <xref ref-type="fig" rid="sensors-21-05740-f002">Figure 2</xref> have such dedicated features: they all require external toolboxes to do so but BioPyC does not: it provides embedded and automatic statistical analysis of the classification results.</p>
    </sec>
    <sec id="sec2dot4-sensors-21-05740">
      <title>2.4. Programming Languages</title>
      <p>Another important criteria for defining a software is the programming language used, which can possibly make it easier or harder to develop new modules for it. Concerning the main BCI platforms, the programming languages that are used are MATLAB [<xref rid="B32-sensors-21-05740" ref-type="bibr">32</xref>], C++ [<xref rid="B33-sensors-21-05740" ref-type="bibr">33</xref>] and Python [<xref rid="B12-sensors-21-05740" ref-type="bibr">12</xref>]. First, the proprietary programming language MATLAB is well known by the research community and widely used in laboratories, due to its popular rapid prototyping environment. However, the license is not free of charge nor always distributed to universities and laboratories. Second, C++ is free, very efficient, but difficult to use and, therefore, generally used by computer scientists and engineers only. Finally, Python is free, simple and extendable by non-computer scientists, making the prototyping and implementation of new modules for Python-based BCI platforms easier. Moreover, Python is widely used by the scientist community, i.e., machine learning experts, engineers and neuroscientists, and many machine learning libraries were implemented using this language, e.g., Scikit-learn [<xref rid="B34-sensors-21-05740" ref-type="bibr">34</xref>], TensorFlow [<xref rid="B35-sensors-21-05740" ref-type="bibr">35</xref>] or PyTorch [<xref rid="B36-sensors-21-05740" ref-type="bibr">36</xref>].</p>
    </sec>
    <sec id="sec2dot5-sensors-21-05740">
      <title>2.5. Supported Systems</title>
      <p>If BCI2000 [<xref rid="B19-sensors-21-05740" ref-type="bibr">19</xref>], OpenViBE [<xref rid="B20-sensors-21-05740" ref-type="bibr">20</xref>], TOBI [<xref rid="B21-sensors-21-05740" ref-type="bibr">21</xref>] and BCI++ [<xref rid="B23-sensors-21-05740" ref-type="bibr">23</xref>] do not support all operating systems, i.e., Windows, Mac OS X and Linux, all other platforms, i.e., BCILAB [<xref rid="B22-sensors-21-05740" ref-type="bibr">22</xref>], pyriemann [<xref rid="B14-sensors-21-05740" ref-type="bibr">14</xref>], Wyrm [<xref rid="B29-sensors-21-05740" ref-type="bibr">29</xref>], gumpy [<xref rid="B26-sensors-21-05740" ref-type="bibr">26</xref>], FieldTrip [<xref rid="B25-sensors-21-05740" ref-type="bibr">25</xref>], pyEEG [<xref rid="B28-sensors-21-05740" ref-type="bibr">28</xref>], BioSig [<xref rid="B17-sensors-21-05740" ref-type="bibr">17</xref>], MNE [<xref rid="B15-sensors-21-05740" ref-type="bibr">15</xref>] and MOABB [<xref rid="B11-sensors-21-05740" ref-type="bibr">11</xref>] do.</p>
    </sec>
    <sec id="sec2dot6-sensors-21-05740">
      <title>2.6. Licenses</title>
      <p>In the case of the main platforms presented in <xref ref-type="fig" rid="sensors-21-05740-f002">Figure 2</xref>, all of them are open source and have adopted either the general public license (GPL), lesser general public license (LGPL), Affero general public license (AGPL), Berkeley software distribution (BSD) or MIT license as their license.</p>
    </sec>
    <sec id="sec2dot7-sensors-21-05740">
      <title>2.7. Distinctive Features of BioPyC</title>
      <p>BioPyC differs from several existing BCI platforms, due to the use of Python as its programming language. This difference should be highlighted, as Python is free of charge—compared to MATLAB, which is not—and simple and extendable by non-computer scientists, whereas C++ requires deep engineering skills. However, several BCI platforms, such as MOABB, MNE, PyEEG, pyRiemann, gumpy and Wyrm, are also implemented in Python but only MOABB offers algorithms for the full offline EEG signal classification process, i.e., pre-processing, signal processing and classification. This last feature makes MOABB closely resemble BioPyC. However, those two platforms differ in three main ways: (1) BioPyC comes with a GUI based on Jupyter notebook [<xref rid="B13-sensors-21-05740" ref-type="bibr">13</xref>], which acts as a tutorial and allows users to interact with the toolbox in a guided way without requiring any programming nor programming skills, whereas MOABB and other Python-based platforms do not offer any GUI, and thus require programming by the user. Those existing platforms are thus most likely not usable by many BCI researchers coming from diverse backgrounds, such as cognitive science, neuroscience or psychology, with possibly no programming background; (2) MOABB allows users to perform an offline analysis only after having shared their data sets in open source, whereas BioPyC users can analyze their data sets on their own. It is an important feature to point out because most BCI researchers would want to analyze their own data before sharing them in open source; and (3) BioPyC offers modules for both statistical testing and visualization for comparing classification algorithms performance, and enables convenient analysis since tests and plots are chosen and applied automatically by the software, based on the data characteristics.</p>
      <p>More generally and more importantly, BioPyC enables its users to classify physiological signals—i.e., HR, breathing, and EDA—in addition to EEG signals, whereas, with the exception of Biosig, no other BCI platform offers an algorithm to process and classify this type of physiological signal.</p>
      <p>In conclusion, BioPyC distinguishes itself from other platforms through features that make it easy to use and more versatile. Indeed, it is based on Python and uses a Jupyter-based GUI. It also offers automatic statistical testing and visualization of classification results, as well as tools for classification of physiological signals such as as HR or breathing.</p>
    </sec>
  </sec>
  <sec id="sec3-sensors-21-05740">
    <title>3. Materials and Methods</title>
    <p>BioPyC comprises four main modules, allowing users to follow the standard BCI process for offline EEG and biosignals classification: (1) reading multiple neurophysiological data formats, (2) pre-processing, filtering and representing EEG and biosignals, (3) classifying those signals, and (4) performing visualization and statistical testing on the classification performance results. Users can follow these steps through a GUI based on Jupyter [<xref rid="B13-sensors-21-05740" ref-type="bibr">13</xref>] and Voilà (<uri xlink:href="https://blog.jupyter.org/and-voil%C3%A0-f6a2c08a4a93">https://blog.jupyter.org/and-voil%C3%A0-f6a2c08a4a93</uri>; accessed on 25 August 2021) that acts as a tutorial, explaining in a detailed way the actions to make at each step, highlighting the modularity of the platform. In this section, we detail the functionality of the platform Jupyter-based GUI, i.e., which tools we used to design it and how users are guided to interact with it. Then, we describe the modularity of BioPyC, i.e., how users can add any new module that may be necessary for their study. Finally, we present the different modules offered by BioPyC, corresponding to the major steps of the offline EEG and biosignals classification process.</p>
    <sec id="sec3dot1-sensors-21-05740">
      <title>3.1. Jupyter Notebook and Voilà as a GUI</title>
      <p>Jupyter notebook is a scientific notebook application that allows the user to write and execute code, as well as to view and save the results. With this tool, users can also write rich-text documentation, using Markdown formatting, and can display different widgets, such as textbox, checkbox or “select multiple” as we can see on <xref ref-type="fig" rid="sensors-21-05740-f003">Figure 3</xref>, to make options selections easier. Moreover, all these features are available in a single file that is accessed via a web browser. We then use voilà that turns Jupyter notebooks into standalone web applications in order to hide the code, and propose a seamless interface to users.</p>
      <p>We designed this Jupyter interface in order to give users an intuitive path through the BCI process. Each step requires a choice from the user, and the options displayed in the following steps are presented according to past choices. For example, if a user chooses to work on pre-processed data, only data sets in which data were previously pre-processed will be displayed.</p>
    </sec>
    <sec id="sec3dot2-sensors-21-05740">
      <title>3.2. BioPyC Modularity</title>
      <p>A strength of BioPyC is its modularity. Whereas the platform already comes with multiple existing modules that users can select with simple clicks, it is also possible to extend it by integrating new scripts as new modules. The kernel of the platform is designed to make such extensions easy: (1) store the new script in the appropriate folder, e.g., “BioPyC/src/classifiers/” for a new classifier or “eeg_contest/src/data_readers/” for a new data format reader, corresponding to a specific format (e.g., “.gdf” or “.mat”); (2) name the Python script after the classifier/data reader name with the “.py” extension; and (3) follow the class and method formalism that was used for other files of these modules.</p>
    </sec>
    <sec id="sec3dot3-sensors-21-05740">
      <title>3.3. Reading Data Sets</title>
      <p>BioPyC offers users various data formats that they can work with: (1) starting with raw data, directly obtained with a data acquisition software, such as GDF [<xref rid="B37-sensors-21-05740" ref-type="bibr">37</xref>]: this will lead to the optional pre-processing step; and (2) starting with pre-processed data, where trials from various runs and sessions have already been concatenated, where the data may have been cleaned with artifact removal, band-pass filtered and epoched. Users can also choose the type of signals they want to work on, i.e., EEG signals, physiological signals or a combination of EEG and physiological signals. This step is presented on <xref ref-type="fig" rid="sensors-21-05740-f003">Figure 3</xref>.</p>
      <sec id="sec3dot3dot1-sensors-21-05740">
        <title>3.3.1. Raw Data</title>
        <p>The raw data are read and pre-processed using the MNE Python library [<xref rid="B15-sensors-21-05740" ref-type="bibr">15</xref>]. So far, the supported raw data format is “.gdf” (GDF—general data format [<xref rid="B37-sensors-21-05740" ref-type="bibr">37</xref>]), a standard format for EEG and biological signals. However, due to the modularity of BioPyC, Python users can easily add a new data reader as explained in <xref ref-type="sec" rid="sec3dot2-sensors-21-05740">Section 3.2</xref> above.</p>
      </sec>
      <sec id="sec3dot3dot2-sensors-21-05740">
        <title>3.3.2. Preprocessing</title>
        <p>The pre-processing is an optional step, performed using the MNE Python library as well, with multiple parameters that have to be defined through the Jupyter interface. First, users have the freedom to choose the runs and sessions they want to use for each subject. Data can be cleaned from blink artifacts using electrooculographic (EOG) channels, using MNE [<xref rid="B15-sensors-21-05740" ref-type="bibr">15</xref>], then band-pass filtered and finally epoched based on triggers that users want to study.</p>
      </sec>
      <sec id="sec3dot3dot3-sensors-21-05740">
        <title>3.3.3. Pre-Processed Data</title>
        <p>In this configuration, BioPyC uses data that are pre-processed and formatted, either coming from the pre-processing module presented above, or by reading a pre-processed data set using data readers. So far, our toolbox can read one type of pre-processed data format using the Python library MNE: MATLAB format, i.e., “.mat” [<xref rid="B32-sensors-21-05740" ref-type="bibr">32</xref>]. This format was chosen since it is popular and widely used by the BCI community.</p>
      </sec>
    </sec>
    <sec id="sec3dot4-sensors-21-05740">
      <title>3.4. Applying Spatial Filters and Machine Learning Algorithms</title>
      <p>So far, BioPyC offers algorithms that proved effective either in BCI classification competitions, notably the filter bank common spatial pattern (FBCSP) [<xref rid="B16-sensors-21-05740" ref-type="bibr">16</xref>], standard linear discriminant analysis (LDA) and Riemannian geometry classifiers [<xref rid="B31-sensors-21-05740" ref-type="bibr">31</xref>,<xref rid="B38-sensors-21-05740" ref-type="bibr">38</xref>,<xref rid="B39-sensors-21-05740" ref-type="bibr">39</xref>], or in other rapidly increasing independent fields, such of artificial intelligence, such as deep learning [<xref rid="B40-sensors-21-05740" ref-type="bibr">40</xref>,<xref rid="B41-sensors-21-05740" ref-type="bibr">41</xref>]. We describe them below. Finally, algorithms for physiological signals classification are presented in this section. Users can select those algorithms through the Jupyter-based GUI.</p>
      <sec id="sec3dot4dot1-sensors-21-05740">
        <title>3.4.1. EEG Spatial Filters</title>
        <p>BioPyC proposes two types of spatial filters: (a) the common spatial pattern (CSP), which is widely used for binary EEG classification in BCI studies, particularly for BCIs exploiting changes in brain oscillations (also known as the frequency band power) [<xref rid="B10-sensors-21-05740" ref-type="bibr">10</xref>]; and (b) the filter bank common spatial pattern (FBCSP) which is an improved variant of the CSP that won numerous active BCI competitions [<xref rid="B16-sensors-21-05740" ref-type="bibr">16</xref>]. Instead of using a single frequency band, the FBCSP explores features based on spatial filters from numerous frequency bands.</p>
      </sec>
      <sec id="sec3dot4dot2-sensors-21-05740">
        <title>3.4.2. Physiological Signal Features</title>
        <p>BioPyC offers numerous algorithms to process and extract features from each type of physiological signal, currently for heart rate (HR), breathing and electrodermal activity (EDA).</p>
        <p>For cardiac signals, the entire electrocardiogram is first reduced to the R-R intervals (RRI) signals, where RRI corresponds to the interval between two successive heartbeats, or more precisely, the interval between two R peaks in the ECG. Note that the algorithms provided by BioPyc can be used both for ECG and for heart rate signals. Using BioPyC, one can extract the following features from cardiac signals:<list list-type="bullet"><list-item><p><italic>sdRR</italic> represents the standard deviation of the RRIs [<xref rid="B42-sensors-21-05740" ref-type="bibr">42</xref>,<xref rid="B43-sensors-21-05740" ref-type="bibr">43</xref>].</p></list-item><list-item><p><italic>meanRR</italic> represents the mean of the RRI [<xref rid="B42-sensors-21-05740" ref-type="bibr">42</xref>,<xref rid="B44-sensors-21-05740" ref-type="bibr">44</xref>].</p></list-item><list-item><p><italic>RMSSD</italic> is the root mean square of the RRIs [<xref rid="B42-sensors-21-05740" ref-type="bibr">42</xref>,<xref rid="B43-sensors-21-05740" ref-type="bibr">43</xref>,<xref rid="B44-sensors-21-05740" ref-type="bibr">44</xref>].</p></list-item><list-item><p><italic>CVSD</italic> is the coefficient of variation of successive differences. This corresponds to the RMSSD divided by meanRR [<xref rid="B42-sensors-21-05740" ref-type="bibr">42</xref>].</p></list-item><list-item><p><italic>cvRR</italic> is the RR coefficient of variation. This corresponds to the sdRR divided by the meanRR [<xref rid="B42-sensors-21-05740" ref-type="bibr">42</xref>].</p></list-item><list-item><p><italic>medianRR</italic> is the median of the absolute values of the RRIs’ successive differences [<xref rid="B44-sensors-21-05740" ref-type="bibr">44</xref>].</p></list-item><list-item><p><italic>madRR</italic> RRIs’ median absolute deviation (MAD) [<xref rid="B42-sensors-21-05740" ref-type="bibr">42</xref>].</p></list-item><list-item><p><italic>mcvRR</italic> is the RRIs’ median-based coefficient of variation. This corresponds to the ratio of madRR divided by medianRR [<xref rid="B44-sensors-21-05740" ref-type="bibr">44</xref>].</p></list-item><list-item><p><italic>RR50 or RR20</italic> is the successive RRIs’ number of interval differences greater than 50 ms or 20 ms, respectively [<xref rid="B42-sensors-21-05740" ref-type="bibr">42</xref>].</p></list-item><list-item><p><italic>pRR50 or pRR20</italic> is the proportion derived by dividing RR50 (or RR20) by the number of RRIs [<xref rid="B44-sensors-21-05740" ref-type="bibr">44</xref>].</p></list-item><list-item><p><italic>triang</italic> is the heart rate variability (HRV) triangular index measurement, i.e., plotting the integral of the ratio of the RRI density histogram by its height [<xref rid="B43-sensors-21-05740" ref-type="bibr">43</xref>,<xref rid="B45-sensors-21-05740" ref-type="bibr">45</xref>].</p></list-item><list-item><p><italic>Shannon_h</italic> is the Shannon entropy calculated on the basis of the class probabilities of the RRI density distribution [<xref rid="B44-sensors-21-05740" ref-type="bibr">44</xref>].</p></list-item><list-item><p><italic>VLF</italic> is the HRV variance in the very low frequency (0.003 to 0.04 Hz) [<xref rid="B42-sensors-21-05740" ref-type="bibr">42</xref>].</p></list-item><list-item><p><italic>LF</italic> is the HRV variance in the low frequency (0.04 to 0.15 Hz) [<xref rid="B42-sensors-21-05740" ref-type="bibr">42</xref>,<xref rid="B44-sensors-21-05740" ref-type="bibr">44</xref>].</p></list-item><list-item><p><italic>HF</italic> is the HRV variance in the high frequency (0.15 to 0.40 Hz) [<xref rid="B42-sensors-21-05740" ref-type="bibr">42</xref>,<xref rid="B44-sensors-21-05740" ref-type="bibr">44</xref>].</p></list-item><list-item><p><italic>Total_Power</italic> is the total power of the full density spectra [<xref rid="B44-sensors-21-05740" ref-type="bibr">44</xref>].</p></list-item><list-item><p><italic>LFHF</italic> is the LF/HF ratio [<xref rid="B42-sensors-21-05740" ref-type="bibr">42</xref>,<xref rid="B44-sensors-21-05740" ref-type="bibr">44</xref>].</p></list-item><list-item><p><italic>LFn</italic> is the normalized LF power. It can be calculated using the equation “LFn = LF/(LF+HF)” [<xref rid="B42-sensors-21-05740" ref-type="bibr">42</xref>].</p></list-item><list-item><p><italic>HFn</italic> is the normalized HF power. It can be calculated using the equation “HFn = HF/(LF+HF)” [<xref rid="B42-sensors-21-05740" ref-type="bibr">42</xref>].</p></list-item><list-item><p><italic>LFp</italic> is the LF/Total_Power ratio [<xref rid="B44-sensors-21-05740" ref-type="bibr">44</xref>].</p></list-item><list-item><p><italic>HFp</italic> is the HF/Total_Power ratio [<xref rid="B44-sensors-21-05740" ref-type="bibr">44</xref>].</p></list-item><list-item><p><italic>DFA</italic> is the detrended fluctuation analysis (DFA) [<xref rid="B46-sensors-21-05740" ref-type="bibr">46</xref>] of the heart rate raw signals.</p></list-item><list-item><p><italic>Shannon</italic> is the RRIs’ Shannon entropy [<xref rid="B44-sensors-21-05740" ref-type="bibr">44</xref>].</p></list-item><list-item><p><italic>sample_entropy</italic> is the RRIs’ sample entropy [<xref rid="B47-sensors-21-05740" ref-type="bibr">47</xref>].</p></list-item><list-item><p><italic>correlation_Dimension</italic> represents the RRIs’ correlation dimension [<xref rid="B44-sensors-21-05740" ref-type="bibr">44</xref>].</p></list-item><list-item><p><italic>entropy_Multiscale</italic> is the RRIs’ entropy multiscale [<xref rid="B47-sensors-21-05740" ref-type="bibr">47</xref>].</p></list-item><list-item><p><italic>entropy_SVD</italic> is the RRIs’ singular value decomposition (SVD) entropy [<xref rid="B44-sensors-21-05740" ref-type="bibr">44</xref>].</p></list-item><list-item><p><italic>entropy_Spectral_VLF</italic> represents the RRIs’ spectral entropy over the VLF [<xref rid="B44-sensors-21-05740" ref-type="bibr">44</xref>].</p></list-item><list-item><p><italic>entropy_Spectral_LF</italic> is the RRIs’ spectral entropy over the LF [<xref rid="B44-sensors-21-05740" ref-type="bibr">44</xref>].</p></list-item><list-item><p><italic>entropy_Spectral_HF</italic> is the RRIs’ spectral entropy over the HF [<xref rid="B44-sensors-21-05740" ref-type="bibr">44</xref>].</p></list-item><list-item><p><italic>Fisher_Info</italic> is the RRIs’ Fisher information [<xref rid="B48-sensors-21-05740" ref-type="bibr">48</xref>].</p></list-item><list-item><p><italic>Lyapunov</italic> is the RRIs’ Lyapunov exponent [<xref rid="B49-sensors-21-05740" ref-type="bibr">49</xref>].</p></list-item><list-item><p><italic>FD_Petrosian</italic> is the RRIs’ Petrosian’s fractal dimension [<xref rid="B50-sensors-21-05740" ref-type="bibr">50</xref>].</p></list-item><list-item><p><italic>FD_Higushi</italic> is the Higushi’s fractal dimension of RRIs [<xref rid="B51-sensors-21-05740" ref-type="bibr">51</xref>].</p></list-item></list></p>
        <p>The study of the breathing signals mainly focuses on R-R intervals (RRI) for the heart rate signals and is known as the breathing rate variability (BRV). RRI corresponds to the interval between two successive breathing signals, or more precisely, the interval between two R peaks in the breathing signals. Based on these RRIs, multiple characteristics can be extracted from the signals, and are listed as follows:<list list-type="bullet"><list-item><p><italic>peak_length</italic> is the interval of successive peaks in the breathing pattern signal [<xref rid="B52-sensors-21-05740" ref-type="bibr">52</xref>].</p></list-item><list-item><p><italic>trough_length</italic> is the interval of successive troughs in the breathing pattern signal [<xref rid="B52-sensors-21-05740" ref-type="bibr">52</xref>].</p></list-item><list-item><p><italic>peak_amplitude</italic> is the amplitude calculated for each peak of the trial [<xref rid="B52-sensors-21-05740" ref-type="bibr">52</xref>].</p></list-item><list-item><p><italic>trough_amplitude</italic> is the amplitude calculated for each trough of the trial [<xref rid="B52-sensors-21-05740" ref-type="bibr">52</xref>].</p></list-item><list-item><p><italic>resp_rate</italic> corresponds to the breathing rate, obtained from the frequency domain analysis of the breathing signals [<xref rid="B52-sensors-21-05740" ref-type="bibr">52</xref>].</p></list-item></list></p>
        <p>Descriptive statistics can also be calculated on these characteristics, i.e., mean, standard deviation, min, max, first quartile, median and the third quartile, and be used as features to feed machine learning algorithms.</p>
        <p>Another type of features is also recommended when manipulating breathing signals: the frequency domain-based features. To do so, BioPyC can also extract as features the power spectral density (PSD) calculated from 0.1 to 0.5 Hz, in 0.01 Hz steps.</p>
        <p>The electrodermal activity (EDA) can be described as the superposition of two distinct skin conductance responses (SCRs): on the one hand, we have the tonic activity and on the other hand, the phasic activity [<xref rid="B53-sensors-21-05740" ref-type="bibr">53</xref>]. In the time domain, features can be extracted from these two components through descriptive statistics, and characteristics based on deeper information can be obtained from the phasic component. These characteristics are described as follows:<list list-type="bullet"><list-item><p><italic>phasic_peak_amplitude</italic> represents the amplitude of phasic peaks [<xref rid="B54-sensors-21-05740" ref-type="bibr">54</xref>].</p></list-item><list-item><p><italic>phasic_peak_longitude</italic> is the rise time/duration of the peaks [<xref rid="B54-sensors-21-05740" ref-type="bibr">54</xref>].</p></list-item><list-item><p><italic>phasic_peak_slope</italic> represents the slope of the peaks [<xref rid="B55-sensors-21-05740" ref-type="bibr">55</xref>].</p></list-item><list-item><p><italic>ordinate_slope</italic> is the ordinate of the slope of the peaks, i.e., the starting point [<xref rid="B55-sensors-21-05740" ref-type="bibr">55</xref>].</p></list-item><list-item><p><italic>peak_peak_interval</italic> corresponds to the inter-peaks time [<xref rid="B55-sensors-21-05740" ref-type="bibr">55</xref>].</p></list-item></list></p>
        <p>Descriptive statistics can then be calculated on all these extracted characteristics, as well as on both tonic and phasic components, in order to define features representing the EDA signals. First, we have the basics statistics—i.e., mean, standard deviation, min, max, first quartile, median and the third quartile—that can be calculated for each of these characteristics. Second, two statistics, i.e., skewness and kurtosis, are calculated for both the tonic and the phasic components [<xref rid="B56-sensors-21-05740" ref-type="bibr">56</xref>]. Finally, the “nb_peak_per_min” corresponds to the frequency of the phasic peaks [<xref rid="B55-sensors-21-05740" ref-type="bibr">55</xref>] and can be defined as a feature in itself (no descriptive statistics are needed).</p>
        <p>Concerning the frequency domain, the EDA signal spectral dynamics is largely contained in frequencies below 0.4 Hz [<xref rid="B57-sensors-21-05740" ref-type="bibr">57</xref>]. Two types of frequency information can thus be also computed, i.e., the power spectral density (PSD) from 0.0 to 0.1 Hz, in 0.01 Hz steps [<xref rid="B55-sensors-21-05740" ref-type="bibr">55</xref>] and the PSD from 0.045 to 0.25 Hz [<xref rid="B58-sensors-21-05740" ref-type="bibr">58</xref>]. Altogether, BioPyC thus offers numerous features to represent, and then classify, these physiological signals.</p>
      </sec>
      <sec id="sec3dot4dot3-sensors-21-05740">
        <title>3.4.3. Machine Learning Algorithms</title>
        <p>A user can select one or multiple classifier(s) in order to compare the classification performance on a single data set. For EEG and physiological signal classification, we chose to integrate a linear discriminant algorithm (LDA) into BioPyC, both classic and with shrinkage, since it is the most commonly used classifier in BCI studies [<xref rid="B10-sensors-21-05740" ref-type="bibr">10</xref>,<xref rid="B30-sensors-21-05740" ref-type="bibr">30</xref>]. The support vector machine (SVM) [<xref rid="B59-sensors-21-05740" ref-type="bibr">59</xref>] algorithm is available as well, and can be used for EEG or physiological features classification. Then, concerning EEG signals only, we included four Riemannian classifiers [<xref rid="B31-sensors-21-05740" ref-type="bibr">31</xref>,<xref rid="B38-sensors-21-05740" ref-type="bibr">38</xref>]. The first ones are the minimum distance to mean with geodesic filtering classifier (FgMDM) and tangent space classifier (TSC). Such methods represent EEG signals as covariance matrices and classify them according to their (Riemannian) distances to prototypes of covariance matrices for each class (for FgMDM), or project the covariance matrices in the manifold tangent space before using Euclidean classifiers (e.g., logistic regression) in this tangent space (for TSC). Such methods recently won six international brain signals competitions [<xref rid="B31-sensors-21-05740" ref-type="bibr">31</xref>]. In addition to those two Riemannian approaches, two new ones—filter bank FgMDM (FBFgMDM) and filter bank TSC (FBTSC)—were introduced in a recent study [<xref rid="B39-sensors-21-05740" ref-type="bibr">39</xref>], and are also available in BioPyC. They use a bank of band-pass filters, such as the ones used for FBCSP, instead of using a unique band-pass filter, and combine Riemannian classifiers from each band. They were shown to outperform FgMDM and TSC, respectively [<xref rid="B39-sensors-21-05740" ref-type="bibr">39</xref>]. Finally, BioPyC offers to use deep learning, which recently showed promising results for many machine learning problems, with a method from [<xref rid="B40-sensors-21-05740" ref-type="bibr">40</xref>] called ShallowConvNet, using convolutional neural networks (CNN) dedicated to EEG classification. Moreover, due to the modularity of the toolbox, BioPyC users can easily add new classifiers (see <xref ref-type="sec" rid="sec3dot2-sensors-21-05740">Section 3.2</xref>), e.g., logistic regression (which is available from scikit-learn [<xref rid="B34-sensors-21-05740" ref-type="bibr">34</xref>]) to classify data previously filtered with the CSP or FBCSP.</p>
      </sec>
    </sec>
    <sec id="sec3dot5-sensors-21-05740">
      <title>3.5. Calibration Types</title>
      <p>BioPyC offers users to run different types of calibration approaches for studying their data, i.e., a subject-specific calibration or a subject-independent one, as we can see on <xref ref-type="fig" rid="sensors-21-05740-f004">Figure 4</xref>, depending on the motivation of their experiments.</p>
      <sec id="sec3dot5dot1-sensors-21-05740">
        <title>3.5.1. Subject-Specific Study</title>
        <p>So far, due to the large between-subject variability, most of the BCI studies are subject-specific, i.e., a classifier needs to be built for each individual subject [<xref rid="B30-sensors-21-05740" ref-type="bibr">30</xref>]. First, data specific to each subject are split into two parts: the training and testing sets. Then, machine learning algorithms are trained on the first set and evaluated on the second one. To do so with BioPyC, users have to set the “split ratio” (see <xref ref-type="sec" rid="sec3dot6dot1-sensors-21-05740">Section 3.6.1</xref>) through a textbox displayed on the Jupyter notebook GUI.</p>
      </sec>
      <sec id="sec3dot5dot2-sensors-21-05740">
        <title>3.5.2. Subject-Independent Study</title>
        <p>One of the major steps for using BCIs outside the laboratories would be for BCI users to be able to instantaneously use the BCI without any calibration phase. To do so, we can evaluate machine learning algorithms through offline subject-independent studies, i.e., with a classifier built on multiple subjects and used as such on a new subject, without the need for data from this new subject. In BioPyC, the evaluation method for this type of calibration is a leave-one-subject-out cross validation, i.e., the training phase uses all subjects except the target subject data to train the classifier, and the testing phase applies this classifier on the target subject data only. This process is repeated with each subject used once as the target (test) subject.</p>
      </sec>
    </sec>
    <sec id="sec3dot6-sensors-21-05740">
      <title>3.6. Evaluation</title>
      <sec id="sec3dot6dot1-sensors-21-05740">
        <title>3.6.1. Split Ratio</title>
        <p>The split ratio method for evaluation is the classic machine learning method for evaluating an algorithm. It consists in separating the data set in two parts: the first one for the training of the algorithm, and the second one for the evaluation of this algorithm. The split ratio defines the ratio of data that has to be kept for the training set. The rest of the data are used for the evaluation (test set).</p>
      </sec>
      <sec id="sec3dot6dot2-sensors-21-05740">
        <title>3.6.2. Cross-Validation</title>
        <p>Finally, due to the usually relatively small number of trials recorded during BCIs experiments, BioPyC proposes a “leave-one-out” cross-validation method based on scikit-learn [<xref rid="B34-sensors-21-05740" ref-type="bibr">34</xref>] for the evaluation if the number of trials is rather low. Each trial is used as a testing set where algorithms are trained on all other trials. The number of k-folds is equal to the number of trials. BioPyC proposes the “k-fold” cross-validation method as well, which allows users to choose the number of equal segments into which the data should be split. For each data segment, trials composing this segment are used as testing set when the rest of the data are used as the training set.</p>
      </sec>
    </sec>
    <sec id="sec3dot7-sensors-21-05740">
      <title>3.7. Statistics and Visualization</title>
      <p>As explained in the introduction, BioPyC offers its users some basic statistical tests and visualizations about the classification performances obtained.</p>
      <sec id="sec3dot7dot1-sensors-21-05740">
        <title>3.7.1. Performances</title>
        <p>Once algorithms are applied on the pre-processed data, classification performances scores—either the accuracy or the F1-score, depending on whether the classes are balanced or not—are automatically calculated for each subject and for each algorithm that is selected for the study. The accuracy score is calculated with scikit-learn “accuracy_score” method if the data set classes are balanced, or with scikit-learn “f1_score” method if they are unbalanced. Those classification performances are framed into a table using the Python library pandas [<xref rid="B60-sensors-21-05740" ref-type="bibr">60</xref>], and can be directly stored into a directory that was previously indicated by the user through a Jupyter textbox, and/or used by the module “statistical analysis”, in order to make statistical testing and plotting.</p>
      </sec>
      <sec id="sec3dot7dot2-sensors-21-05740">
        <title>3.7.2. Statistics</title>
        <p>For statistical testing, BioPyC enables users to choose to make automatic appropriate tests for comparing classification performances between machine learning algorithms with the Python libraries pandas [<xref rid="B60-sensors-21-05740" ref-type="bibr">60</xref>] and pingouin [<xref rid="B61-sensors-21-05740" ref-type="bibr">61</xref>]:<list list-type="order"><list-item><p>Test the data normality with the Shapiro–Wilk test from the Python library Scipy [<xref rid="B62-sensors-21-05740" ref-type="bibr">62</xref>].</p></list-item><list-item><p>Test the data sphericity with Mauchly’s test from Scipy.</p></list-item><list-item><p>Analyze and compare means of classification performances between machine learning algorithms, using, in the case that data are normalized, the following:
<list list-type="bullet"><list-item><p>The <italic>t</italic>-test: comparing the performance of two algorithms along all the subjects; comparing performances of an algorithm depending on the study type (subject-specific vs. subject-independent) using pingouin.</p></list-item><list-item><p>One-way ANOVA with repeated measures: comparing performances of multiple algorithms (more than two) or multiple study types using pingouin.</p></list-item><list-item><p>Two-way ANOVA with repeated measures: comparing performance with both factors (type of algorithms, type of study), using pingouin.</p></list-item></list></p></list-item></list></p>
      </sec>
      <sec id="sec3dot7dot3-sensors-21-05740">
        <title>3.7.3. Chance Level</title>
        <p>When measuring classification accuracy for a BCI task, given the usually small number of samples, the actual chance level should be carefully considered [<xref rid="B63-sensors-21-05740" ref-type="bibr">63</xref>]. For example, the chance level for a two-class paradigm will not be necessarily 50%, as it will depend on the number of testing trials and the confidence interval we want to work with. To solve this problem, BioPyC proposes an option for the calculation of the chance level based on [<xref rid="B63-sensors-21-05740" ref-type="bibr">63</xref>]. Moreover, users can test the difference between subjects performances and the chance level with a one-sample <italic>t</italic>-test from [<xref rid="B62-sensors-21-05740" ref-type="bibr">62</xref>].</p>
      </sec>
      <sec id="sec3dot7dot4-sensors-21-05740">
        <title>3.7.4. Visualization</title>
        <p>Data visualization can naturally be useful in BCI studies since it provides an informative and explicit feedback about the obtained classification performances. BioPyC proposes both boxplots and barplots from seaborn [<xref rid="B64-sensors-21-05740" ref-type="bibr">64</xref>] and pairwise <italic>t</italic>-test visualization using scikit-learn [<xref rid="B34-sensors-21-05740" ref-type="bibr">34</xref>]. The barplots can be used for visualizing the detailed performance results of each algorithm/calibration on each subject (see <xref ref-type="fig" rid="sensors-21-05740-f005">Figure 5</xref>). For the boxplots, the number of boxes on the plot will vary depending on the number of algorithms and the number of calibration types that were tested, as we can see in <xref ref-type="fig" rid="sensors-21-05740-f006">Figure 6</xref>. BioPyC also proposes to display confusion matrices using scikit-learn [<xref rid="B34-sensors-21-05740" ref-type="bibr">34</xref>], as presented in <xref ref-type="fig" rid="sensors-21-05740-f007">Figure 7</xref>.</p>
      </sec>
    </sec>
    <sec id="sec3dot8-sensors-21-05740">
      <title>3.8. Demonstrating BioPyC Use Cases</title>
      <p>BioPyC was already used to analyze four types of BCI data, for motor imagery BCIs and mental state decoding through passive BCIs, such as workload, emotions and attention. All data sets were of different sizes (number of subjects and trials), collected in different laboratories using different EEG devices, with data stored in different formats: this showed the versatility and robustness of BioPyC. In this section, we present the four data sets we analyzed using BioPyC.</p>
      <sec id="sec3dot8dot1-sensors-21-05740">
        <title>3.8.1. Motor Imagery</title>
        <p>First, we used the modern machine learning algorithms from BioPyC to classify motor imagery EEG signals using the data set coming from [<xref rid="B65-sensors-21-05740" ref-type="bibr">65</xref>] called “BCI competition IV data set 2a”. In this data set, EEG signals were recorded from 22 Ag/AgCl electrodes (with inter-electrode distances of 3.5 cm), from 9 subjects, when executing four different motor imagery tasks (left hand, right hand, both feet, and tongue). We chose to keep only two classes, namely the imagination of movement of the left hand (class 1) and right hand (class 2). Participants participated in two sessions of 6 runs, where a run consisted of 24 trials (12 for each class), yielding a total of 144 trials per session.</p>
        <p>After band-pass filtering the signals in both a single band (in 8–12 Hz) single band-based algorithms, and in 94 Hz-wide bands for filter bank-based algorithms (in 4–8 Hz, 8–12 Hz, …, 36–40 Hz), we used six methods for classifying those two mental tasks, i.e., CSP coupled with a LDA [<xref rid="B30-sensors-21-05740" ref-type="bibr">30</xref>], FBCSP coupled with a LDA [<xref rid="B16-sensors-21-05740" ref-type="bibr">16</xref>] and 4 Riemannian approaches (FgMDM, TSC, FBFgMDM and FBTSC) [<xref rid="B31-sensors-21-05740" ref-type="bibr">31</xref>,<xref rid="B39-sensors-21-05740" ref-type="bibr">39</xref>] and compared them across two types of calibration, i.e., subject-specific and subject-independent. For the subject-specific calibration, classifiers were trained on the data from the first session of a subject, and tested on the data from the second session of the same subject, as done in the original study [<xref rid="B65-sensors-21-05740" ref-type="bibr">65</xref>]. Regarding the subject-independent calibration, the training set comprised all trials of all subjects, except those of the current subject used for testing. The testing set was the second session of the current test subject.</p>
      </sec>
      <sec id="sec3dot8dot2-sensors-21-05740">
        <title>3.8.2. Workload</title>
        <p>The second study aimed at comparing modern machine learning algorithms to classify two levels of mental workload (high versus low) based on EEG signals [<xref rid="B39-sensors-21-05740" ref-type="bibr">39</xref>]. We used the data set coming from [<xref rid="B66-sensors-21-05740" ref-type="bibr">66</xref>], where signals from 28 electrodes (active electrodes in a 10/20 system: F3, F7, FC3, FT7, C3, A1, CP3, TP7, P3, P7, O1, Oz, Pz, CPz, Cz, FCz, Fz, F4, F8, FC4, FT8, C4, A2, CP4, TP8, P4, P8, O2) were recorded on 22 subjects. To induce variations of the mental workload, this study used the N-back task: letters were successively displayed on the screen, and subjects had to indicate whether the current letter was the same one as the letter that displayed N letters before. The labeling was done as follows: the “low” workload corresponded to the 2 s trials from a 0-back task, while the “high” workload corresponded to the ones from a 2-back task. In total, 720 trials were recorded for each workload level and subject. The alpha rhythm (8–12 Hz) being known to vary according to workload [<xref rid="B66-sensors-21-05740" ref-type="bibr">66</xref>], we first band-pass filtered the signals into a single band (in 8–12 Hz), before band-pass filtering them into 94 Hz-wide bands for filter bank-based algorithms (in 4–8 Hz, 8–12 Hz, …, 36–40 Hz). We then used seven methods for classifying such workload levels, i.e., the CSP coupled with a LDA [<xref rid="B30-sensors-21-05740" ref-type="bibr">30</xref>], FBCSP coupled with a LDA [<xref rid="B16-sensors-21-05740" ref-type="bibr">16</xref>], 4 Riemannian approaches (FgMDM, TSC, FBFgMDM and FBTSC) [<xref rid="B31-sensors-21-05740" ref-type="bibr">31</xref>,<xref rid="B39-sensors-21-05740" ref-type="bibr">39</xref>], as well as the ShallowConvNet from [<xref rid="B40-sensors-21-05740" ref-type="bibr">40</xref>] and compared them across two types of calibration, i.e., subject-specific and subject independent. As described in <xref ref-type="sec" rid="sec3dot5-sensors-21-05740">Section 3.5</xref>, the classifiers were trained on the first half of the trials of a subject and tested on the other half for the subject-specific calibration. For the subject-independent calibration, the training set comprised all trials of all subjects, except the current subject used for testing. The testing set was the second half of the trials of the current subject.</p>
      </sec>
      <sec id="sec3dot8dot3-sensors-21-05740">
        <title>3.8.3. Affective States</title>
        <p>The third study aimed at applying BioPyC classification algorithms to EEG signals in order to classify two types of affective states, i.e., valence (high versus low) and arousal (high versus low) [<xref rid="B39-sensors-21-05740" ref-type="bibr">39</xref>]. Emotions are known to be difficult to detect through EEG signals in the passive BCIs field [<xref rid="B67-sensors-21-05740" ref-type="bibr">67</xref>]. We thus challenged the recent and promising machine learning algorithms from BioPyC with a reference data set on emotions called DEAP [<xref rid="B68-sensors-21-05740" ref-type="bibr">68</xref>]. Signals from 32 EEG electrodes (placed according to the international 10–20 system: FP1, F3, F7, FC3, FT7, C3, T7 A1, CP3, TP7, P3, P7, O1, Oz, Pz, CPz, Cz, FCz, Fz, Fp2, F4, F8, FC4, FT8, C4, T8, A2, CP4, TP8, P4, P8, O2) were recorded from 32 subjects. The data set contained 40 trials, corresponding to signals recorded when two emotion dimensions, i.e., valence and arousal, were influenced by music–video clips. Valence and arousal levels were measured using Russell’s valence–arousal scale [<xref rid="B69-sensors-21-05740" ref-type="bibr">69</xref>] directly after each video, by clicking on a 1–9 continuous scale. This self-assessment system on a continuous scale makes the classes definition more complex: we kept the number five (corresponding to the median of the one to nine grading system) as a threshold to split trials into two unbalanced classes—low and high—for both the “emotion-arousal” and “emotion-valence” data sets.</p>
        <p>After band-pass filtering the signals in both a single band (in 8–12 Hz, corresponding to the alpha rhythm that was proven to vary according to both valence and arousal in [<xref rid="B67-sensors-21-05740" ref-type="bibr">67</xref>]) and in 94 Hz-wide bands for filter bank-based algorithms (in 4–8 Hz, 8–12 Hz, …, 36–40 Hz), we used seven methods for classifying both valence (high vs. low) and arousal (high vs. low) states, i.e., the CSP coupled with a LDA [<xref rid="B30-sensors-21-05740" ref-type="bibr">30</xref>], FBCSP coupled with a LDA [<xref rid="B16-sensors-21-05740" ref-type="bibr">16</xref>], four Riemannian approaches (FgMDM, TSC, FBFgMDM and FBTSC) [<xref rid="B31-sensors-21-05740" ref-type="bibr">31</xref>,<xref rid="B39-sensors-21-05740" ref-type="bibr">39</xref>], as well as the ShallowConvNet from [<xref rid="B40-sensors-21-05740" ref-type="bibr">40</xref>] and compared them across two types of calibration, i.e., subject-specific and subject independent. For the subject-specific study, given the low number of trials, we performed a “leave-one-out” cross-validation. Thus, we used 40 models for each subject, each model being trained on 39 trials and tested on 1 trial. For the subject-independent study, we kept all trials of all subjects to compose the training set, except the current subject used for testing, as explained in <xref ref-type="sec" rid="sec3dot5-sensors-21-05740">Section 3.5</xref>. Note that most of the classifiers we used are able to deal with unbalanced classes, except for the CNN for which we obtained balanced classes by up-sampling the minority class by randomly duplicating trials.</p>
      </sec>
      <sec id="sec3dot8dot4-sensors-21-05740">
        <title>3.8.4. Attention</title>
        <p>The fourth study aimed at developing a first comprehensive understanding of the different attentional states described in the model of van Zomeren and Brouwer using EEG data [<xref rid="B70-sensors-21-05740" ref-type="bibr">70</xref>,<xref rid="B71-sensors-21-05740" ref-type="bibr">71</xref>]. The term “Attention” encompasses several different attentional states. Given the model of van Zomeren and Brouwer it encompasses four attentional states, i.e., alertness and sustained attentions, referring to the intensity of attention (i.e., its strength), as well as selective and divided attentions, referring to its selectivity (i.e., the amount of monitored information) [<xref rid="B72-sensors-21-05740" ref-type="bibr">72</xref>]. No study provided yet a comprehensive comparison of these different attentional states in the EEG signals.</p>
        <p>Hence, the brain activity was recorded using BioSemi 64 active scalp electrodes (10–20 system)—AF7, AF3, F1, F3, F5, F7, FT7, FC5, FC3, FC1, C1, C3, C5, T7, TP7, CP5, CP3, CP1, P1, P3, P5, P7, P9, PO7, PO3, O1, Iz, Oz, POz, Pz, CPz, Fpz, Fp2, AF8, AF4, AFz, Fz, F2, F4, F6, F8, FT8, FC6, FC4, FC2, FCz, Cz, C2, C4, C6, T8, TP8, CP6, CP4, CP2, P2, P4, P6, P8, P10, PO8, PO4, O2—and we included 16 subjects into an experiment during which they were asked to perform different tasks. Each task assessed a type of attentional state, while we recorded the subjects’ EEG. During each task, the subjects had to react as quickly as possible to the appearance of target stimuli by pressing a keyboard space bar as quickly as possible. In accordance with the literature, the tasks and types of attention were differentiated by the type of sensory modality of the stimuli, the number of distractors, the presence of a warning tone before the stimuli and the length of the task [<xref rid="B73-sensors-21-05740" ref-type="bibr">73</xref>,<xref rid="B74-sensors-21-05740" ref-type="bibr">74</xref>,<xref rid="B75-sensors-21-05740" ref-type="bibr">75</xref>,<xref rid="B76-sensors-21-05740" ref-type="bibr">76</xref>]. For each task, 80 target stimuli were presented. We used one second prior to target presentation as the analysis window. Only data from targets that were at least one second apart from a motor response were analyzed to prevent motor-related artifacts.</p>
        <p>First, we used BioPyC to know if we could differentiate the different attentional states from one another. We used common spatial pattern filtering in the alpha range (8–12 Hz)—-which was associated with attention in various studies [<xref rid="B77-sensors-21-05740" ref-type="bibr">77</xref>,<xref rid="B78-sensors-21-05740" ref-type="bibr">78</xref>]—and a linear discriminant analysis classifier, with 5-fold cross-validation.</p>
        <p>Second, we used BioPyC to determine whether we could classify the five types of attentional states at once using only the EEG data. The subject-specific discriminability (one classifier per subject) of the EEG patterns between each of the five attention tasks was assessed, using the tangent-space classifier described in [<xref rid="B31-sensors-21-05740" ref-type="bibr">31</xref>], with 5-fold cross-validation. We used the method from [<xref rid="B79-sensors-21-05740" ref-type="bibr">79</xref>] to classify EEG signals into five classes: a linear discriminant analysis (LDA) was performed between each pair of class, i.e., each pair of attention tasks, and then all the resulting classifiers were combined to obtain the classification results. The 5-classes classification was performed twice with EEG data either filtered in the theta or alpha band. The confusion matrix, representing for each class the ratio of trials that were accurately or wrongfully associated with it over the total number of trials, was then computed.</p>
      </sec>
    </sec>
  </sec>
  <sec sec-type="results" id="sec4-sensors-21-05740">
    <title>4. Results</title>
    <p>In this section, we present the results obtained by the different signal processing and machine learning algorithms currently offered by BioPyC.</p>
    <sec id="sec4dot1-sensors-21-05740">
      <title>4.1. Motor Imagery</title>
      <p>The detailed results, i.e., classification accuracy scores obtained by each algorithm, for each subject, with both subject-specific and subject-independent calibrations, are represented in <xref ref-type="fig" rid="sensors-21-05740-f005">Figure 5</xref>. The classification accuracy distributions across subjects, for each algorithm and calibration type, are plotted on <xref ref-type="fig" rid="sensors-21-05740-f006">Figure 6</xref>. They revealed that FBTSC and FBFgMDM obtained the highest mean accuracy, although not significantly so, with both subject-specific (mean accuracy FBTSC = 79.6%; mean accuracy FBFgMDM = 79.7%) and subject-independent calibrations (mean accuracy FBTSC = 70.1%; mean accuracy FBFgMDM = 69.1%).</p>
    </sec>
    <sec id="sec4dot2-sensors-21-05740">
      <title>4.2. Workload</title>
      <p>The results of the study are plotted in <xref ref-type="fig" rid="sensors-21-05740-f008">Figure 8</xref> and reveal that the ShallowConvNet [<xref rid="B40-sensors-21-05740" ref-type="bibr">40</xref>] obtained the highest mean accuracy, although not significantly so, with both subject-specific (mean = 72.7%) and subject-independent (mean = 63.7%) calibrations [<xref rid="B39-sensors-21-05740" ref-type="bibr">39</xref>].</p>
    </sec>
    <sec id="sec4dot3-sensors-21-05740">
      <title>4.3. Affective States</title>
      <p>The F1-score obtained for valence is reported in <xref ref-type="fig" rid="sensors-21-05740-f009">Figure 9</xref>, and the one for arousal in <xref ref-type="fig" rid="sensors-21-05740-f010">Figure 10</xref>.</p>
      <p>Concerning the subject-specific calibration, results showed better performances for the Riemannian geometry classifiers (RGC) with the FBTSC (mean for valence = 61.0%, arousal = 60.6%) compared to state-of-the-art classification algorithms such as the CSP+LDA (valence = 57.9%, arousal = 58.1%).</p>
      <p>In contrast, the CNN from [<xref rid="B40-sensors-21-05740" ref-type="bibr">40</xref>] underperformed on both valence and arousal data sets (mean for valence = 46.3%, arousal = 40.1).</p>
      <p>Regarding the subject-independent calibration, the FBCSP+LDA obtained the best performance when applied to the valence data set (mean accuracy valence = 55.23%), and the FgMDM the best one when applied to the arousal data set (mean accuracy arousal = 56.25%).</p>
    </sec>
    <sec id="sec4dot4-sensors-21-05740">
      <title>4.4. Attention</title>
      <p>The results regarding the discrimination of attentional states from one another are promising and range from 83% accuracy (SD = 0.09) to discriminate alertness (tonic) from sustained attention to 74% accuracy (SD = 0.13) to discriminate selective and divided attention.</p>
      <p>We then classified the five types of attentional states at once. The average confusion matrices over all subjects for the classification in the theta and alpha bands are displayed in <xref ref-type="fig" rid="sensors-21-05740-f007">Figure 7</xref>.</p>
      <p>Overall, these promising results tend to validate the model of van Zomeren and Brouwer, as the different attentional state that they describe seem to have distinct electroencephalographic patterns of activation. We believe that future research assessing the learners’ attentional states during BCI user training might represent real opportunities to improve such training.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="sec5-sensors-21-05740">
    <title>5. Discussion</title>
    <p>In this article, we first introduced BioPyC and the different features that this software offers, before testing it through four studies, i.e., one on motor imagery, and the three others on mental states estimation. We first studied new and promising algorithms that proved efficient in recent active BCI classification competitions [<xref rid="B16-sensors-21-05740" ref-type="bibr">16</xref>,<xref rid="B31-sensors-21-05740" ref-type="bibr">31</xref>], such as Riemannian geometry classifiers on a motor imagery data set from [<xref rid="B65-sensors-21-05740" ref-type="bibr">65</xref>], a cognitive workload data set [<xref rid="B66-sensors-21-05740" ref-type="bibr">66</xref>] and an emotion data set (Valence/Arousal) [<xref rid="B68-sensors-21-05740" ref-type="bibr">68</xref>]. Note that we also studied an algorithm that proved efficient in other fields of artificial intelligence, i.e., deep learning [<xref rid="B40-sensors-21-05740" ref-type="bibr">40</xref>,<xref rid="B41-sensors-21-05740" ref-type="bibr">41</xref>] on both workload and emotion data sets. We finally applied a Riemannian geometry-based method to classify five different types of attention states.</p>
    <p>About the motor imagery data set, the filter-bank RGCs (FBFgMDM and FBTSC) obtained the best performance, although not significantly so, with both subject-specific (respectively, 79.7% and 79.6%) and subject-independent calibrations (respectively, 69.1% and 70.1%) when compared to other algorithms (FgMDMD, TSC, CSP coupled with a LDA, as well as FBCSP coupled with a LDA). Concerning the cognitive workload data set, our results suggested that a CNN obtained the highest mean accuracy, although not significantly so, in both conditions for the mental workload study—72.73% in the subject-specific study, 63.74% in the subject-independent study—outperforming state-of-the-art methods on this data set, followed by filter bank RGCs. This same CNN underperformed in both conditions for the emotion data set, a data set with small training data. On the contrary, RGCs proved to have the highest mean accuracy with the FBTSC for the subject-specific condition on the valence data-set (61.09%) and for the subject-specific condition on the arousal data set (60.60%). The classification of attentional states using single frequency bands-based RGCs showed promising results, ranging from 74% accuracy (SD = 0.13) to discriminate selective and divided attention to 83% accuracy (SD = 0.09) to discriminate alertness (Tonic) from sustained attention.</p>
    <p>Overall, results from these four different studies highlighted the potential of RGCs methods, i.e., the filter bank RGCs—FBFgMDM and FBTSC—that proved to be efficient for motor imagery, cognitive workload and emotion classification problems, and TSC that showed interesting results when discriminating different types of attention. Indeed, FBTSC and FBFgMDM outperformed the results from other algorithms in most conditions/data sets, showing a performance gap between RGC methods using a filter bank and the ones using the single 8–12 Hz frequency band. Indeed, covering more frequency bands brings more information: it could therefore be interesting to investigate which frequency bands were chosen by the feature selection algorithms in the studies that used filter bank-based classification methods. It should be noted that a tool for analyzing and reporting which frequency bands were selected by the feature selection algorithms is already implemented in BioPyC, although it was not used for the studies presented here.</p>
    <p>Regarding the CNN, we applied it on two data sets—workload and emotion data sets—and obtained opposite results. Indeed, the CNN obtained better performance for the workload data set, although non-significantly so, than other machine learning algorithms but underperformed on the emotion data set (for both valence and arousal). Multiple factors could explain the observed algorithm performances, such as the number of trials that are used for training models, e.g., 720 training trials for the workload data set and 39 training trials only (with cross validation calibration) for both valence and arousal data sets. This might suggest that the CNN could be useful for mental state classification, but only when large amounts of training trials are available (around 700 in our study), which is not always possible. However, other factors also differ between both data sets studied and could also explain differences in CNN performances, including the EEG epochs length (2 s epochs for workload and 60 s epochs for emotions), and the nature of the mental states studied (workload vs. emotions).</p>
    <p>Our various experiments from the different analyses performed for BioPyC revealed several positive aspects of the software, i.e., the modularity, the comparison of classification algorithms, the statistical analyses, as well as the data visualization. First, the modularity of the software is highlighted with the different data sets formats that were used for the offline analysis presented above. Data sets for workload, emotions and attention were in a Matlab format (“.mat”) and contained pre-processed data. The data set with the motor imagery tasks was in a GDF (“.gdf”) format and required a pre-processing step before performing the signal processing and classification steps. Second, the data sets with a two-class paradigm, namely motor imagery, workload, and emotions (both valence and arousal) were used for comparing the classification algorithms, which is one of the advantages of BioPyC. Those results indicated interesting information about the compared algorithms, such as the ineffectiveness of the CNN on data sets with a small number of trials, e.g., emotions, when the same algorithms proved to be efficient on data sets with a large number of trials, e.g., workload. Moreover, BioPyC also proved to have efficient machine learning algorithms for multi-class classification. This is the case of the study on attention, where the data were divided into five classes of attention (tonic, phasic, sustained, selective and divided). Third, the automatic statistical analyses were used in two ways: (1) the two-way ANOVA with repeated measures was used for analyzing performance results of the machine learning algorithms (factor 1) with both subject-specific and subject-independent calibrations (factor 2) in the motor imagery, workload and emotions studies; and (2) post-hoc <italic>t</italic>-tests were performed to check significant differences in the performance results between algorithms. Fourth, concerning the BioPyC data visualization module, all plots that were shown in <xref ref-type="sec" rid="sec4-sensors-21-05740">Section 4</xref> were automatically generated by BioPyC: (1) boxplots represented on <xref ref-type="fig" rid="sensors-21-05740-f006">Figure 6</xref>, <xref ref-type="fig" rid="sensors-21-05740-f007">Figure 7</xref>, <xref ref-type="fig" rid="sensors-21-05740-f008">Figure 8</xref>, <xref ref-type="fig" rid="sensors-21-05740-f009">Figure 9</xref> and <xref ref-type="fig" rid="sensors-21-05740-f010">Figure 10</xref>; (2) barplots represented on <xref ref-type="fig" rid="sensors-21-05740-f005">Figure 5</xref>; and (3) confusion matrix plots on <xref ref-type="fig" rid="sensors-21-05740-f007">Figure 7</xref>.</p>
    <p>On the other hand, the studies that were run with BioPyC have pointed some limits regarding the pre-processing, e.g., EOG artifacts removal for EEG-based studies, but also regarding the processing, e.g., the lack of information about the features that were selected by the feature selection algorithms. First, concerning the pre-processing, the main missing feature is indeed an EOG artifacts removal system: EEG signals are known to be sensitive to noise, and multiple methods are available to reduce this noise. For example, tools such as artifact rejection based on ICA are available in Python libraries, such as MNE [<xref rid="B15-sensors-21-05740" ref-type="bibr">15</xref>]. About the processing, machine learning algorithms obtained interesting classification performances among these four studies. However, filter-bank RGCs, i.e., FBTSC and FBFgMDM, proved to be robust by obtaining the best classification performance on the motor imagery and the emotion (valence/arousal) data sets, as well as the second best classification performances on the workload data set, but we did not check which frequency bands were used for each data set to obtain such results. We indeed chose to apply single band-based algorithms such as the CSP coupled with a LDA on the alpha band (8–12Hz) for most of the studies, but more information has probably been given by other frequency bands (theta or beta) when using Filter bank RGCs. Thus, we implemented a first module to report the frequency bands selected by the feature selection algorithm (not shown in this paper). However, in general, it would be interesting to implement, in the future, machine learning introspection algorithms to understand and interpret what the machine learning algorithms have learned from the data [<xref rid="B80-sensors-21-05740" ref-type="bibr">80</xref>]. Still regarding the processing step of BioPyC, a module for physiological signals classification, e.g., EDA, respiration and ECG, was implemented in BioPyC. While we did not report on their use in this paper, they were tested in a recent study for estimating the curiosity levels in brain and physiological signals [<xref rid="B81-sensors-21-05740" ref-type="bibr">81</xref>]. It should therefore be relevant to run several other studies in order to further validate the methods that are proposed for each of these physiological signals. The classification of such physiological signals usually requires extracting many features from them, and a feature selection algorithm is generally applied in order to keep the features that bring the most interesting information to the machine learning algorithms. As for the filter bank methods that were used for classifying EEG signals in our studies, it would therefore be useful to study the features that are extracted and selected from the physiological signals. Finally, another limit of BioPyC is the absence of modules, which would study the electrodes that bring the most information to the machine learning algorithms and, therefore, the brain areas that are mostly involved in the various studies conducted.</p>
  </sec>
  <sec id="sec6-sensors-21-05740">
    <title>6. Current Status and Future Work</title>
    <p>BioPyC is currently publicly available on GitHub at <uri xlink:href="https://gitlab.inria.fr/biopyc/BioPyC/">https://gitlab.inria.fr/biopyc/BioPyC/</uri>; accessed on 25 August 2021. Users have to clone the repository and install all dependencies (Jupyter, voilà, numpy, pandas, pingouin, pyRiemann, scikit learn, scikit_posthocs, MNE 0.17 and braindecode) using pip. Then, users have to find the file BioPyC.ipynb and run “voila BioPyC.ipynb” in order to display the interface in a web browser and initialize the application. All instructions will then be given by the application that is made as an intuitive tutorial.</p>
    <p>In its current stage, BioPyC offers the different modules that allow users to follow the standard steps of the BCI process, i.e., reading different EEG data format, filtering and cleaning EEG signals, classifying EEG signals and finally visualizing and performing statistical tests on the classification performance results.</p>
    <p>Regarding the reading of different EEG formats, two modules are available in the current stage of the platform, namely GDF (“.gdf”) and Matlab (“.mat”), and one is still in progress, i.e., MNE (“.fiff”). Future versions on BioPyC will offer more modules for reading data, starting with those for Python, i.e., “.pkl” and “.dat”.</p>
    <p>BioPyC currently offers tools for pre-processing the signals as well, all based on MNE [<xref rid="B15-sensors-21-05740" ref-type="bibr">15</xref>]: band-pass filtering and epoching. More pre-processing features are available in MNE, such as EOG-based artifacts removal. It would, therefore, be easy to add new modules for pre-processing data in the future versions of BioPyC. Indeed, EEG signals being noisy, artifact correction is an important part of the pre-processing step of the BCI process. This feature will, therefore, be the next future addition to BioPyC.</p>
    <p>Regarding the third step of the BCI process—i.e., signal processing and machine learning for EEG signals classification—so far, BioPyc proposes several efficient algorithms for decoding oscillatory activity: the CSP [<xref rid="B30-sensors-21-05740" ref-type="bibr">30</xref>] and FBCSP [<xref rid="B16-sensors-21-05740" ref-type="bibr">16</xref>] for spatial filtering, and the LDA, Riemannian geometry methods [<xref rid="B31-sensors-21-05740" ref-type="bibr">31</xref>,<xref rid="B39-sensors-21-05740" ref-type="bibr">39</xref>], as well as the CNN [<xref rid="B40-sensors-21-05740" ref-type="bibr">40</xref>] as machine learning algorithms implemented in BioPyC. However, the ongoing works aim to integrate new signal processing and machine learning algorithms for the classification of event related potentials (ERPs) into BioPyC. Among them, xDAWN [<xref rid="B82-sensors-21-05740" ref-type="bibr">82</xref>], which is widely used for spatial filtering and has proved to be efficient for EEG-based classification of workload levels [<xref rid="B83-sensors-21-05740" ref-type="bibr">83</xref>]. It would also be interesting to integrate other ERP spatial filtering methods into the future versions of BioPyC, e.g., principal component analysis (PCA) or canonical correlation approaches (CCA) [<xref rid="B84-sensors-21-05740" ref-type="bibr">84</xref>] that proved efficient for EEG classification of mental workload levels as well [<xref rid="B85-sensors-21-05740" ref-type="bibr">85</xref>]. Moreover, current works also aim at integrating machine learning methods for the classification of both ERPs and oscillatory activity with BioPyC, e.g., EEGNet [<xref rid="B86-sensors-21-05740" ref-type="bibr">86</xref>]. Finally, the fourth and last step of the BCI process, i.e., performing visualization and statistical tests on classification performance results, also benefits from current improvements: a new module for visualizing the percentage of use of the different frequency bands selected by the filter bank-based algorithms, i.e., FBCSP, FBFgMDM and FBTSC, is now integrated but has not been used so far. Moreover, a module for analyzing which brain areas are used by the classifiers (i.e., which electrodes brought the most information) will be soon integrated into BioPyC.</p>
  </sec>
  <sec sec-type="conclusions" id="sec7-sensors-21-05740">
    <title>7. Conclusions</title>
    <p>We presented BioPyC, an open-source and easy-to-use BCI Python software for offline EEG and biosignal analysis. This platform allows BCI and physiological computing researchers to quickly analyze offline their data by following the classical steps of pre-processing (optional), signal processing and classification, statistical analysis and data visualization. It is important to note that users do not need any programming skills to be able to process their data since BioPyC is built in the form of a Jupyter notebook with a voilà GUI that acts as a tutorial: each step is described with instructions that guide users in their analyses and choices for parameters and algorithms to use. BioPyC is already proved to be a comprehensive tool since it was used for four extensive studies so far, with quite different aims and requirements. Moreover, since Python is free of charge, any researcher can use it for his/her experiments. Moreover, BioPyC is open source and allows users to build new modules. For example, new signal processing or classification algorithms can be easily added to the platform, as well as data readers for new data sets formats. So far, BioPyC has still a modest number of tools but can easily be extended in the future and is still growing. For example, currently, the toolbox can only support one main BCI paradigm, i.e., oscillatory-based BCI, but will soon be extended to support the evoked potentials-based BCIs paradigm.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p><bold>Publisher’s Note:</bold> MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <notes>
    <title>Author Contributions</title>
    <p>Conceptualization, A.A.; Methodology, A.A. and L.P.; Software, A.A., D.T. and D.D.; Supervision, A.C. and F.L. All authors have read and agreed to the published version of the manuscript.</p>
  </notes>
  <notes>
    <title>Funding</title>
    <p>This work received support from the European Research Council (grant ERC-2016-STG-714567), the Japanese Society for the Promotion of Science (JSPS) and the Ministry of Education and Science of the RF (grant 14.796.31.0001).</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Conflicts of Interest</title>
    <p>The authors declare no conflict of interest.</p>
  </notes>
  <ref-list>
    <title>References</title>
    <ref id="B1-sensors-21-05740">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vidal</surname>
            <given-names>J.J.</given-names>
          </name>
        </person-group>
        <article-title>Toward direct brain-computer communication</article-title>
        <source>Annu. Rev. Biophys. Bioeng.</source>
        <year>1973</year>
        <volume>2</volume>
        <fpage>157</fpage>
        <lpage>180</lpage>
        <pub-id pub-id-type="doi">10.1146/annurev.bb.02.060173.001105</pub-id>
        <?supplied-pmid 4583653?>
        <pub-id pub-id-type="pmid">4583653</pub-id>
      </element-citation>
    </ref>
    <ref id="B2-sensors-21-05740">
      <label>2.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Clerc</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Bougrain</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Lotte</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <source>Brain-Computer Interfaces 1</source>
        <publisher-name>Wiley-ISTE</publisher-name>
        <publisher-loc>London, UK</publisher-loc>
        <year>2016</year>
      </element-citation>
    </ref>
    <ref id="B3-sensors-21-05740">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Millán</surname>
            <given-names>J.R.</given-names>
          </name>
          <name>
            <surname>Rupp</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Müller-Putz</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Murray-Smith</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Giugliemma</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Tangermann</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Vidaurre</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Cincotti</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Kübler</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Leeb</surname>
            <given-names>R.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Combining Brain-Computer Interfaces and Assistive Technologies: State-of-the-Art and Challenges</article-title>
        <source>Front. Neurosci.</source>
        <year>2010</year>
        <volume>4</volume>
        <fpage>161</fpage>
        <pub-id pub-id-type="doi">10.3389/fnins.2010.00161</pub-id>
        <?supplied-pmid 20877434?>
        <pub-id pub-id-type="pmid">20877434</pub-id>
      </element-citation>
    </ref>
    <ref id="B4-sensors-21-05740">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zander</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Kothe</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>Towards passive brain-computer interfaces: Applying brain-computer interface technology to human-machine systems in general</article-title>
        <source>J. Neural Eng.</source>
        <year>2011</year>
        <volume>8</volume>
        <fpage>025005</fpage>
        <pub-id pub-id-type="doi">10.1088/1741-2560/8/2/025005</pub-id>
        <?supplied-pmid 21436512?>
        <pub-id pub-id-type="pmid">21436512</pub-id>
      </element-citation>
    </ref>
    <ref id="B5-sensors-21-05740">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pfurtscheller</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>The hybrid BCI</article-title>
        <source>Front. Neurosci.</source>
        <year>2010</year>
        <volume>4</volume>
        <fpage>1</fpage>
        <lpage>11</lpage>
        <pub-id pub-id-type="doi">10.3389/fnpro.2010.00003</pub-id>
        <?supplied-pmid 20582271?>
        <pub-id pub-id-type="pmid">20582256</pub-id>
      </element-citation>
    </ref>
    <ref id="B6-sensors-21-05740">
      <label>6.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Wolpaw</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Wolpaw</surname>
            <given-names>E.</given-names>
          </name>
        </person-group>
        <source>Brain-Computer Interfaces: Principles and Practice</source>
        <publisher-name>OSO</publisher-name>
        <publisher-loc>Oxford, MS, USA</publisher-loc>
        <year>2012</year>
        <fpage>1</fpage>
        <lpage>424</lpage>
        <pub-id pub-id-type="doi">10.1093/acprof:oso/9780195388855.001.0001</pub-id>
      </element-citation>
    </ref>
    <ref id="B7-sensors-21-05740">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Erp</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Lotte</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Tangermann</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Brain-Computer Interfaces: Beyond Medical Applications</article-title>
        <source>Computer</source>
        <year>2012</year>
        <volume>45</volume>
        <fpage>26</fpage>
        <lpage>34</lpage>
        <pub-id pub-id-type="doi">10.1109/MC.2012.107</pub-id>
      </element-citation>
    </ref>
    <ref id="B8-sensors-21-05740">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jeunet</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>N’Kaoua</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Lotte</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>Advances in user-training for mental-imagery-based BCI control: Psychological and cognitive factors and their neural correlates</article-title>
        <source>Prog. Brain Res.</source>
        <year>2016</year>
        <volume>228</volume>
        <fpage>3</fpage>
        <lpage>35</lpage>
        <pub-id pub-id-type="doi">10.1016/bs.pbr.2016.04.002</pub-id>
        <pub-id pub-id-type="pmid">27590964</pub-id>
      </element-citation>
    </ref>
    <ref id="B9-sensors-21-05740">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Roc</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Pillette</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Mladenovic</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Benaroch</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>N’Kaoua</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Jeunet</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Lotte</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>A review of user training methods in brain computer interfaces based on mental tasks</article-title>
        <source>J. Neural Eng.</source>
        <year>2020</year>
        <volume>18</volume>
        <fpage>011002</fpage>
        <pub-id pub-id-type="doi">10.1088/1741-2552/abca17</pub-id>
        <?supplied-pmid 33181488?>
        <pub-id pub-id-type="pmid">33181488</pub-id>
      </element-citation>
    </ref>
    <ref id="B10-sensors-21-05740">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lotte</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Bougrain</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Cichocki</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Clerc</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Congedo</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Rakotomamonjy</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Yger</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>A Review of Classification Algorithms for EEG-based Brain-Computer Interfaces: A 10-year Update</article-title>
        <source>J. Neural Eng.</source>
        <year>2018</year>
        <volume>15</volume>
        <fpage>031005</fpage>
        <pub-id pub-id-type="doi">10.1088/1741-2552/aab2f2</pub-id>
        <pub-id pub-id-type="pmid">29488902</pub-id>
      </element-citation>
    </ref>
    <ref id="B11-sensors-21-05740">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jayaram</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Barachant</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>MOABB: Trustworthy algorithm benchmarking for BCIs</article-title>
        <source>J. Neural Eng.</source>
        <year>2018</year>
        <volume>15</volume>
        <fpage>066011</fpage>
        <pub-id pub-id-type="doi">10.1088/1741-2552/aadea0</pub-id>
        <?supplied-pmid 30177583?>
        <pub-id pub-id-type="pmid">30177583</pub-id>
      </element-citation>
    </ref>
    <ref id="B12-sensors-21-05740">
      <label>12.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Rossum</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <source>Python Reference Manual</source>
        <comment>Technical Report</comment>
        <publisher-name>Network Theory Ltd.</publisher-name>
        <publisher-loc>Amsterdam, The Netherlands</publisher-loc>
        <year>1995</year>
      </element-citation>
    </ref>
    <ref id="B13-sensors-21-05740">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pérez</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Granger</surname>
            <given-names>B.E.</given-names>
          </name>
        </person-group>
        <article-title>{IP}ython: A System for Interactive Scientific Computing</article-title>
        <source>Comput. Sci. Eng.</source>
        <year>2007</year>
        <volume>9</volume>
        <fpage>21</fpage>
        <lpage>29</lpage>
        <comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://ipython.org">http://ipython.org</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2014-05-10">(accessed on 10 May 2014)</date-in-citation>
        <pub-id pub-id-type="doi">10.1109/MCSE.2007.53</pub-id>
      </element-citation>
    </ref>
    <ref id="B14-sensors-21-05740">
      <label>14.</label>
      <element-citation publication-type="web">
        <person-group person-group-type="author">
          <name>
            <surname>Barachant</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>King</surname>
            <given-names>J.R.</given-names>
          </name>
        </person-group>
        <article-title>pyRiemann 0.2.2</article-title>
        <year>2015</year>
        <comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://pyriemann.readthedocs.io/en/latest/">https://pyriemann.readthedocs.io/en/latest/</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2021-08-25">(accessed on 25 August 2021)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B15-sensors-21-05740">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gramfort</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Luessi</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Larson</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Engemann</surname>
            <given-names>D.A.</given-names>
          </name>
          <name>
            <surname>Strohmeier</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Brodbeck</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Goj</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Jas</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Brooks</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Parkkonen</surname>
            <given-names>L.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>MEG and EEG data analysis with MNE-Python</article-title>
        <source>Front. Neurosci.</source>
        <year>2013</year>
        <volume>7</volume>
        <fpage>267</fpage>
        <pub-id pub-id-type="doi">10.3389/fnins.2013.00267</pub-id>
        <?supplied-pmid 24431986?>
        <pub-id pub-id-type="pmid">24431986</pub-id>
      </element-citation>
    </ref>
    <ref id="B16-sensors-21-05740">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ang</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Chin</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Guan</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Filter bank common spatial pattern algorithm on BCI competition IV datasets 2a and 2b</article-title>
        <source>Front. Neurosci.</source>
        <year>2012</year>
        <volume>6</volume>
        <fpage>1</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.3389/fnins.2012.00039</pub-id>
        <pub-id pub-id-type="pmid">22294978</pub-id>
      </element-citation>
    </ref>
    <ref id="B17-sensors-21-05740">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schölgl</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Vidaurre</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>T.H.</given-names>
          </name>
        </person-group>
        <article-title>BioSig: The free and open source software library for biomedical signal processing</article-title>
        <source>Comput. Intell. Neurosci.</source>
        <year>2011</year>
        <volume>2011</volume>
        <fpage>935364</fpage>
        <pub-id pub-id-type="doi">10.1155/2011/935364</pub-id>
        <pub-id pub-id-type="pmid">21437227</pub-id>
      </element-citation>
    </ref>
    <ref id="B18-sensors-21-05740">
      <label>18.</label>
      <element-citation publication-type="web">
        <person-group person-group-type="author">
          <name>
            <surname>Clisson</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Bertrand-Lalo</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Congedo</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Victor-Thomas</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Chatel-Goldman</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Timeflux: An Open-Source Framework for the Acquisition and Near Real-Time Processing of Signal Streams</article-title>
        <year>2019</year>
        <fpage>6</fpage>
        <lpage>11</lpage>
        <comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://hal.archives-ouvertes.fr/hal-02315098/document">https://hal.archives-ouvertes.fr/hal-02315098/document</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2021-08-25">(accessed on 25 August 2021)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B19-sensors-21-05740">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schalk</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Mcfarland</surname>
            <given-names>D.J.</given-names>
          </name>
          <name>
            <surname>Hinterberger</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Birbaumer</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Wolpaw</surname>
            <given-names>J.R.</given-names>
          </name>
        </person-group>
        <article-title>BCI2000: A General-Purpose Brain-Computer Interface (BCI) System</article-title>
        <source>IEEE Trans. Biomed. Eng.</source>
        <year>2004</year>
        <volume>51</volume>
        <fpage>1034</fpage>
        <lpage>1043</lpage>
        <pub-id pub-id-type="doi">10.1109/TBME.2004.827072</pub-id>
        <pub-id pub-id-type="pmid">15188875</pub-id>
      </element-citation>
    </ref>
    <ref id="B20-sensors-21-05740">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Renard</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Lotte</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Gibert</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Congedo</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Maby</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Delannoy</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Bertrand</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Lécuyer</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>OpenViBE: An open-source software platform to design, test, and use brain-computer interfaces in real and virtual environments</article-title>
        <source>Presence Teleoper. Virtual Environ.</source>
        <year>2010</year>
        <volume>19</volume>
        <fpage>35</fpage>
        <lpage>53</lpage>
        <pub-id pub-id-type="doi">10.1162/pres.19.1.35</pub-id>
      </element-citation>
    </ref>
    <ref id="B21-sensors-21-05740">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Müller-Putz</surname>
            <given-names>G.R.</given-names>
          </name>
          <name>
            <surname>Breitwieser</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Cincotti</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Leeb</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Schreuder</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Leotta</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Tavella</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Bianchi</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Kreilinger</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Ramsay</surname>
            <given-names>A.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Tools for brain-computer interaction: A general concept for a hybrid BCI</article-title>
        <source>Front. Neuroinform.</source>
        <year>2011</year>
        <volume>5</volume>
        <fpage>30</fpage>
        <pub-id pub-id-type="doi">10.3389/fninf.2011.00030</pub-id>
        <pub-id pub-id-type="pmid">22131973</pub-id>
      </element-citation>
    </ref>
    <ref id="B22-sensors-21-05740">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kothe</surname>
            <given-names>C.A.</given-names>
          </name>
          <name>
            <surname>Makeig</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>BCILAB: A platform for brain-computer interface development</article-title>
        <source>J. Neural Eng.</source>
        <year>2013</year>
        <volume>10</volume>
        <fpage>056014</fpage>
        <pub-id pub-id-type="doi">10.1088/1741-2560/10/5/056014</pub-id>
        <?supplied-pmid 23985960?>
        <pub-id pub-id-type="pmid">23985960</pub-id>
      </element-citation>
    </ref>
    <ref id="B23-sensors-21-05740">
      <label>23.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Perego</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Maggi</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Parini</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Andreoni</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>BCI++: A new framework for brain computer interface application</article-title>
        <source>18th International Conference on Software Engineering and Data Engineering 2009, SEDE 2009</source>
        <publisher-name>International Society for Computers and Their Applications (ISCA)</publisher-name>
        <publisher-loc>Las Vegas, NV, USA</publisher-loc>
        <year>2009</year>
        <fpage>37</fpage>
        <lpage>41</lpage>
      </element-citation>
    </ref>
    <ref id="B24-sensors-21-05740">
      <label>24.</label>
      <element-citation publication-type="web">
        <person-group person-group-type="author">
          <name>
            <surname>Delorme</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Makeig</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Eeglab_Jnm03.Pdf</article-title>
        <comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://sccn.ucsd.edu/eeglab/index.php">https://sccn.ucsd.edu/eeglab/index.php</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2021-08-25">(accessed on 25 August 2021)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B25-sensors-21-05740">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Oostenveld</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Fries</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Maris</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Schoffelen</surname>
            <given-names>J.M.</given-names>
          </name>
        </person-group>
        <article-title>FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title>
        <source>Comput. Intell. Neurosci.</source>
        <year>2011</year>
        <volume>2011</volume>
        <fpage>156869</fpage>
        <pub-id pub-id-type="doi">10.1155/2011/156869</pub-id>
        <pub-id pub-id-type="pmid">21253357</pub-id>
      </element-citation>
    </ref>
    <ref id="B26-sensors-21-05740">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tayeb</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Waniek</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Fedjaev</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Ghaboosi</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Rychly</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Widderich</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Richter</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Braun</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Saveriano</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>G.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Gumpy: A Python toolbox suitable for hybrid brain-computer interfaces</article-title>
        <source>J. Neural Eng.</source>
        <year>2018</year>
        <volume>15</volume>
        <fpage>065003</fpage>
        <pub-id pub-id-type="doi">10.1088/1741-2552/aae186</pub-id>
        <?supplied-pmid 30215610?>
        <pub-id pub-id-type="pmid">30215610</pub-id>
      </element-citation>
    </ref>
    <ref id="B27-sensors-21-05740">
      <label>27.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Brunner</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Andreoni</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Bianchi</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Blankertz</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Breitwieser</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Kanoh</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Susila</surname>
            <given-names>I.P.</given-names>
          </name>
          <name>
            <surname>Venthur</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>BCI Software Platforms</article-title>
        <source>Towards Practical Brain-Computer Interfaces</source>
        <publisher-name>Springer</publisher-name>
        <publisher-loc>Berlin/Heidelberg, Germany</publisher-loc>
        <year>2013</year>
        <fpage>303</fpage>
        <lpage>331</lpage>
      </element-citation>
    </ref>
    <ref id="B28-sensors-21-05740">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bao</surname>
            <given-names>F.S.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>PyEEG: An open source python module for EEG/MEG feature extraction</article-title>
        <source>Comput. Intell. Neurosci.</source>
        <year>2011</year>
        <volume>2011</volume>
        <fpage>406391</fpage>
        <pub-id pub-id-type="doi">10.1155/2011/406391</pub-id>
        <?supplied-pmid 21512582?>
        <pub-id pub-id-type="pmid">21512582</pub-id>
      </element-citation>
    </ref>
    <ref id="B29-sensors-21-05740">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Venthur</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Dähne</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Höhne</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Heller</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Blankertz</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>Wyrm: A Brain-Computer Interface Toolbox in Python</article-title>
        <source>Neuroinformatics</source>
        <year>2015</year>
        <volume>13</volume>
        <fpage>471</fpage>
        <lpage>486</lpage>
        <pub-id pub-id-type="doi">10.1007/s12021-015-9271-8</pub-id>
        <pub-id pub-id-type="pmid">26001643</pub-id>
      </element-citation>
    </ref>
    <ref id="B30-sensors-21-05740">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Blankertz</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Tomioka</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Lemm</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Kawanabe</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Müller</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Optimizing spatial filters for robust EEG single-trial analysis</article-title>
        <source>IEEE Signal Process. Mag.</source>
        <year>2008</year>
        <volume>25</volume>
        <fpage>41</fpage>
        <lpage>56</lpage>
        <pub-id pub-id-type="doi">10.1109/MSP.2008.4408441</pub-id>
      </element-citation>
    </ref>
    <ref id="B31-sensors-21-05740">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yger</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Berar</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Lotte</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>Riemannian approaches in Brain-Computer Interfaces: A review</article-title>
        <source>IEEE TNSRE</source>
        <year>2016</year>
        <volume>25</volume>
        <fpage>1753</fpage>
        <lpage>1762</lpage>
        <pub-id pub-id-type="doi">10.1109/TNSRE.2016.2627016</pub-id>
      </element-citation>
    </ref>
    <ref id="B32-sensors-21-05740">
      <label>32.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <collab>MATLAB</collab>
        </person-group>
        <source>Version 7.10.0 (R2010a)</source>
        <publisher-name>The MathWorks Inc.</publisher-name>
        <publisher-loc>Natick, MA, USA</publisher-loc>
        <year>2010</year>
      </element-citation>
    </ref>
    <ref id="B33-sensors-21-05740">
      <label>33.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <collab>ISO</collab>
        </person-group>
        <source>ISO/IEC 14882:1998: Programming Languages—C++</source>
        <publisher-name>American National Standards Institute</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>1998</year>
        <fpage>732</fpage>
      </element-citation>
    </ref>
    <ref id="B34-sensors-21-05740">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pedregosa</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Varoquaux</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Gramfort</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Michel</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Thirion</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Grisel</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Blondel</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Prettenhofer</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Weiss</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Dubourg</surname>
            <given-names>V.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Scikit-learn: Machine Learning in Python</article-title>
        <source>J. Mach. Learn. Res.</source>
        <year>2011</year>
        <volume>12</volume>
        <fpage>2825</fpage>
        <lpage>2830</lpage>
      </element-citation>
    </ref>
    <ref id="B35-sensors-21-05740">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dignam</surname>
            <given-names>J.D.</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>P.L.</given-names>
          </name>
          <name>
            <surname>Shastry</surname>
            <given-names>B.S.</given-names>
          </name>
          <name>
            <surname>Roeder</surname>
            <given-names>R.G.</given-names>
          </name>
        </person-group>
        <article-title>Eukaryotic gene transcription with purified components</article-title>
        <source>Methods Enzymol.</source>
        <year>1983</year>
        <volume>101</volume>
        <fpage>582</fpage>
        <lpage>598</lpage>
        <pub-id pub-id-type="doi">10.1016/0076-6879(83)01039-3</pub-id>
        <pub-id pub-id-type="pmid">6888276</pub-id>
      </element-citation>
    </ref>
    <ref id="B36-sensors-21-05740">
      <label>36.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Paszke</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Gross</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Chintala</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Chanan</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>DeVito</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Desmaison</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Antiga</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Lerer</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Automatic Differentiation in PyTorch</article-title>
        <source>Proceedings of the NIPS Autodiff Workshop</source>
        <conf-loc>Long Beach, CA, USA</conf-loc>
        <conf-date>9 December 2017</conf-date>
      </element-citation>
    </ref>
    <ref id="B37-sensors-21-05740">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Alois</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>GDF—A General Dataformat for BIOSIGNALS</article-title>
        <source>arXiv</source>
        <year>2006</year>
        <pub-id pub-id-type="arxiv">0608052v10</pub-id>
      </element-citation>
    </ref>
    <ref id="B38-sensors-21-05740">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Congedo</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Barachant</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Bhatia</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Riemannian geometry for EEG-based brain-computer interfaces; a primer and a review</article-title>
        <source>Brain-Comput. Interfaces</source>
        <year>2017</year>
        <volume>4</volume>
        <fpage>155</fpage>
        <lpage>174</lpage>
        <pub-id pub-id-type="doi">10.1080/2326263X.2017.1297192</pub-id>
      </element-citation>
    </ref>
    <ref id="B39-sensors-21-05740">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Appriou</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Cichocki</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Lotte</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>Modern machine learning algorithms to classify cognitive and affective states from electroencephalography signals</article-title>
        <source>IEEE Syst. Man Cybern. Mag.</source>
        <year>2020</year>
        <volume>6</volume>
        <fpage>29</fpage>
        <lpage>38</lpage>
        <pub-id pub-id-type="doi">10.1109/MSMC.2020.2968638</pub-id>
      </element-citation>
    </ref>
    <ref id="B40-sensors-21-05740">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schirrmeister</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Springenberg</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Fiederer</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Glasstetter</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Eggensperger</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Tangermann</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Hutter</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Burgard</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Ball</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Deep learning with convolutional neural networks for EEG decoding and visualization</article-title>
        <source>Hum. Brain Mapp.</source>
        <year>2017</year>
        <volume>38</volume>
        <fpage>5391</fpage>
        <lpage>5420</lpage>
        <pub-id pub-id-type="doi">10.1002/hbm.23730</pub-id>
        <?supplied-pmid 28782865?>
        <pub-id pub-id-type="pmid">28782865</pub-id>
      </element-citation>
    </ref>
    <ref id="B41-sensors-21-05740">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lecun</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Deep learning</article-title>
        <source>Nature</source>
        <year>2015</year>
        <volume>521</volume>
        <fpage>436</fpage>
        <lpage>444</lpage>
        <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>
        <?supplied-pmid 26017442?>
        <pub-id pub-id-type="pmid">26017442</pub-id>
      </element-citation>
    </ref>
    <ref id="B42-sensors-21-05740">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Malik</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Heart rate variability: Standards of measurement, physiological interpretation, and clinical use</article-title>
        <source>Circulation</source>
        <year>1996</year>
        <volume>93</volume>
        <fpage>1043</fpage>
        <lpage>1065</lpage>
        <pub-id pub-id-type="doi">10.1093/oxfordjournals.eurheartj.a014868</pub-id>
        <pub-id pub-id-type="pmid">8598068</pub-id>
      </element-citation>
    </ref>
    <ref id="B43-sensors-21-05740">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Smith</surname>
            <given-names>A.L.</given-names>
          </name>
          <name>
            <surname>Owen</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Reynolds</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Heart rate variability indices for very short-term (30 beat) analysis. Part 1: Survey and toolbox</article-title>
        <source>J. Clin. Monit. Comput.</source>
        <year>2013</year>
        <volume>27</volume>
        <fpage>569</fpage>
        <lpage>576</lpage>
        <pub-id pub-id-type="doi">10.1007/s10877-013-9471-4</pub-id>
        <?supplied-pmid 23674071?>
        <pub-id pub-id-type="pmid">23674071</pub-id>
      </element-citation>
    </ref>
    <ref id="B44-sensors-21-05740">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Voss</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Schroeder</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Heitmann</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Peters</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Perz</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Short-term heart rate variability—Influence of gender and age in healthy subjects</article-title>
        <source>PLoS ONE</source>
        <year>2015</year>
        <volume>10</volume>
        <elocation-id>e0118308</elocation-id>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0118308</pub-id>
        <?supplied-pmid 25822720?>
        <pub-id pub-id-type="pmid">25822720</pub-id>
      </element-citation>
    </ref>
    <ref id="B45-sensors-21-05740">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shaffer</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Ginsberg</surname>
            <given-names>J.P.</given-names>
          </name>
        </person-group>
        <article-title>An Overview of Heart Rate Variability Metrics and Norms</article-title>
        <source>Front. Public Health</source>
        <year>2017</year>
        <volume>5</volume>
        <fpage>258</fpage>
        <pub-id pub-id-type="doi">10.3389/fpubh.2017.00258</pub-id>
        <?supplied-pmid 29034226?>
        <pub-id pub-id-type="pmid">29034226</pub-id>
      </element-citation>
    </ref>
    <ref id="B46-sensors-21-05740">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Peng</surname>
            <given-names>C.K.</given-names>
          </name>
          <name>
            <surname>Havlin</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Stanley</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Goldberger</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Quantification of scaling exponents and crossover phenomena in nonstationary heartbeat time series</article-title>
        <source>Chaos</source>
        <year>1995</year>
        <volume>5</volume>
        <fpage>82</fpage>
        <lpage>87</lpage>
        <pub-id pub-id-type="doi">10.1063/1.166141</pub-id>
        <pub-id pub-id-type="pmid">11538314</pub-id>
      </element-citation>
    </ref>
    <ref id="B47-sensors-21-05740">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tiwari</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Albuquerque</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Parent</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Gagnon</surname>
            <given-names>J.F.</given-names>
          </name>
          <name>
            <surname>Lafond</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Tremblay</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Falk</surname>
            <given-names>T.H.</given-names>
          </name>
        </person-group>
        <article-title>Multi-scale heart beat entropy measures for mental workload assessment of ambulant users</article-title>
        <source>Entropy</source>
        <year>2019</year>
        <volume>21</volume>
        <elocation-id>783</elocation-id>
        <pub-id pub-id-type="doi">10.3390/e21080783</pub-id>
      </element-citation>
    </ref>
    <ref id="B48-sensors-21-05740">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>de Geus</surname>
            <given-names>E.J.</given-names>
          </name>
          <name>
            <surname>Gianaros</surname>
            <given-names>P.J.</given-names>
          </name>
          <name>
            <surname>Brindle</surname>
            <given-names>R.C.</given-names>
          </name>
          <name>
            <surname>Jennings</surname>
            <given-names>J.R.</given-names>
          </name>
          <name>
            <surname>Berntson</surname>
            <given-names>G.G.</given-names>
          </name>
        </person-group>
        <article-title>Should heart rate variability be “corrected” for heart rate? Biological, quantitative, and interpretive considerations</article-title>
        <source>Psychophysiology</source>
        <year>2019</year>
        <volume>56</volume>
        <fpage>1</fpage>
        <lpage>26</lpage>
        <pub-id pub-id-type="doi">10.1111/psyp.13287</pub-id>
        <?supplied-pmid 30357862?>
        <pub-id pub-id-type="pmid">30357862</pub-id>
      </element-citation>
    </ref>
    <ref id="B49-sensors-21-05740">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Goshvarpour</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Goshvarpour</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Classification of Heart Rate Signals during Meditation using Lyapunov Exponents and Entropy</article-title>
        <source>Int. J. Intell. Syst. Appl.</source>
        <year>2012</year>
        <volume>2</volume>
        <fpage>35</fpage>
        <lpage>41</lpage>
        <pub-id pub-id-type="doi">10.5815/ijisa.2012.02.04</pub-id>
      </element-citation>
    </ref>
    <ref id="B50-sensors-21-05740">
      <label>50.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Petrosian</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Kolmogorov complexity of finite sequences and recognition of different preictal EEG patterns</article-title>
        <source>Proceedings of the IEEE Symposium on Computer-Based Medical Systems</source>
        <conf-loc>Lubbock, TX, USA</conf-loc>
        <conf-date>9–10 June 1995</conf-date>
        <fpage>212</fpage>
        <lpage>217</lpage>
        <pub-id pub-id-type="doi">10.1109/CBMS.1995.465426</pub-id>
      </element-citation>
    </ref>
    <ref id="B51-sensors-21-05740">
      <label>51.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gomes</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Vanderlei</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Garner</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Vanderlei</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Valenti</surname>
            <given-names>V.</given-names>
          </name>
        </person-group>
        <article-title>Higuchi Fractal Analysis of Heart Rate Variability is Sensitive during Recovery from Exercise in Physically Active Men</article-title>
        <source>Med. Express</source>
        <year>2017</year>
        <volume>4</volume>
        <pub-id pub-id-type="doi">10.5935/MedicalExpress.2017.03.02</pub-id>
      </element-citation>
    </ref>
    <ref id="B52-sensors-21-05740">
      <label>52.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jaiswal</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Chowdhury</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Banerjee</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Chatterjee</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>Effect of Mental Workload on Breathing Pattern and Heart Rate for a Working Memory Task: A Pilot Study</article-title>
        <source>Annu. Int. Conf. IEEE Eng. Med. Biol. Soc.</source>
        <year>2019</year>
        <volume>2019</volume>
        <fpage>2202</fpage>
        <lpage>2206</lpage>
        <pub-id pub-id-type="doi">10.1109/EMBC.2019.8856458</pub-id>
        <?supplied-pmid 31946338?>
        <pub-id pub-id-type="pmid">31946338</pub-id>
      </element-citation>
    </ref>
    <ref id="B53-sensors-21-05740">
      <label>53.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Fritz</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Begel</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Müller</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Yigit-Elliott</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Züger</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Using psycho-physiological measures to assess task difficulty in software development</article-title>
        <source>Proceedings of the 36th International Conference on Software Engineering</source>
        <publisher-name>ACM</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2014</year>
        <pub-id pub-id-type="doi">10.1145/2568225.2568266</pub-id>
      </element-citation>
    </ref>
    <ref id="B54-sensors-21-05740">
      <label>54.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schmidt</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Walach</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Electrodermal Activity (EDA)—State-of-the-art measurement and techniques for parapsychological purposes</article-title>
        <source>J. Parapsychol.</source>
        <year>2000</year>
        <volume>64</volume>
        <fpage>139</fpage>
        <lpage>163</lpage>
      </element-citation>
    </ref>
    <ref id="B55-sensors-21-05740">
      <label>55.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Parent</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Diagnosticité des Mesures Physiologiques PériphéRiques de la Charge Mentale</article-title>
        <source>Ph.D. Thesis</source>
        <publisher-name>Université du Québec à Montréal</publisher-name>
        <publisher-loc>Montréal, QC, Canada</publisher-loc>
        <year>2019</year>
        <volume>Volume 5</volume>
        <fpage>82</fpage>
        <lpage>87</lpage>
        <pub-id pub-id-type="doi">10.1063/1.166141</pub-id>
      </element-citation>
    </ref>
    <ref id="B56-sensors-21-05740">
      <label>56.</label>
      <element-citation publication-type="web">
        <person-group person-group-type="author">
          <name>
            <surname>Braithwaite</surname>
            <given-names>J.J.</given-names>
          </name>
          <name>
            <surname>Derrick</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Watson</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Jones</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Rowe</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Watson</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Robert</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>A Guide for Analysing Electrodermal Activity (EDA) &amp; Skin Conductance Responses (SCRs) for Psychological Experiments</article-title>
        <year>2013</year>
        <fpage>1</fpage>
        <lpage>42</lpage>
        <comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://www.lancaster.ac.uk/media/lancaster-university/content-assets/documents/psychology/ABriefGuideforAnalysingElectrodermalActivityv4.pdf">https://www.lancaster.ac.uk/media/lancaster-university/content-assets/documents/psychology/ABriefGuideforAnalysingElectrodermalActivityv4.pdf</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2020-08-25">(accessed on 25 August 2020)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B57-sensors-21-05740">
      <label>57.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shimomura</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Yoda</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Sugiura</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Horiguchi</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Iwanaga</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Katsuura</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Use of Frequency Domain Analysis of Skin Conductance for Evaluation of Mental Workload</article-title>
        <source>J. Physiol. Anthropol.</source>
        <year>2008</year>
        <volume>27</volume>
        <fpage>173</fpage>
        <lpage>177</lpage>
        <pub-id pub-id-type="doi">10.2114/jpa2.27.173</pub-id>
        <?supplied-pmid 18832780?>
        <pub-id pub-id-type="pmid">18832780</pub-id>
      </element-citation>
    </ref>
    <ref id="B58-sensors-21-05740">
      <label>58.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Posada-Quintero</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Florian</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Orjuela-Cañón</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Corrales</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Charleston-Villalobos</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Chon</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Power Spectral Density Analysis of Electrodermal Activity for Sympathetic Function Assessment</article-title>
        <source>Ann. Biomed. Eng.</source>
        <year>2016</year>
        <volume>44</volume>
        <fpage>3124</fpage>
        <lpage>3135</lpage>
        <pub-id pub-id-type="doi">10.1007/s10439-016-1606-6</pub-id>
        <?supplied-pmid 27059225?>
        <pub-id pub-id-type="pmid">27059225</pub-id>
      </element-citation>
    </ref>
    <ref id="B59-sensors-21-05740">
      <label>59.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cortes</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Vapnik</surname>
            <given-names>V.</given-names>
          </name>
        </person-group>
        <article-title>Support-vector networks</article-title>
        <source>Mach. Learn.</source>
        <year>1995</year>
        <volume>20</volume>
        <fpage>273</fpage>
        <lpage>297</lpage>
        <pub-id pub-id-type="doi">10.1007/BF00994018</pub-id>
      </element-citation>
    </ref>
    <ref id="B60-sensors-21-05740">
      <label>60.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>McKinney</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <article-title>Data Structures for Statistical Computing in Python</article-title>
        <source>Proceedings of the 9th Python in Science Conference</source>
        <conf-loc>Austin, TX, USA</conf-loc>
        <conf-date>28 June–3 July 2010</conf-date>
      </element-citation>
    </ref>
    <ref id="B61-sensors-21-05740">
      <label>61.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vallat</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Pingouin: Statistics in Python</article-title>
        <source>J. Open Source Softw.</source>
        <year>2018</year>
        <volume>3</volume>
        <fpage>1026</fpage>
        <pub-id pub-id-type="doi">10.21105/joss.01026</pub-id>
      </element-citation>
    </ref>
    <ref id="B62-sensors-21-05740">
      <label>62.</label>
      <element-citation publication-type="web">
        <person-group person-group-type="author">
          <name>
            <surname>Jones</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Oliphant</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Peterson</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>A Guide to NumPy, 2001–Today</article-title>
        <comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://web.mit.edu/dvp/Public/numpybook.pdf">https://web.mit.edu/dvp/Public/numpybook.pdf</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2020-08-25">(accessed on 25 August 2020)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B63-sensors-21-05740">
      <label>63.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Müller-Putz</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Scherer</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Brunner</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Leeb</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Pfurtscheller</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Better than random? A closer look on BCI results</article-title>
        <source>Int. J. Bioelectromagn.</source>
        <year>2008</year>
        <volume>10</volume>
        <fpage>52</fpage>
        <lpage>55</lpage>
      </element-citation>
    </ref>
    <ref id="B64-sensors-21-05740">
      <label>64.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Waskom</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>seaborn: Statistical data visualization</article-title>
        <source>J. Open Source Softw.</source>
        <year>2021</year>
        <volume>6</volume>
        <fpage>3021</fpage>
        <pub-id pub-id-type="doi">10.21105/joss.03021</pub-id>
      </element-citation>
    </ref>
    <ref id="B65-sensors-21-05740">
      <label>65.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Brunner</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Leeb</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Muller-Putz</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <source>BCI Competition IV Dataset 2a: 4-Class Motor Imagery</source>
        <publisher-name>Graz University of Technology</publisher-name>
        <publisher-loc>Graz, Austria</publisher-loc>
        <year>2008</year>
      </element-citation>
    </ref>
    <ref id="B66-sensors-21-05740">
      <label>66.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mühl</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Jeunet</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Lotte</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>EEG-based workload estimation across affective contexts</article-title>
        <source>Front. Neurosci.</source>
        <year>2014</year>
        <volume>8</volume>
        <fpage>1</fpage>
        <lpage>15</lpage>
        <pub-id pub-id-type="pmid">24478622</pub-id>
      </element-citation>
    </ref>
    <ref id="B67-sensors-21-05740">
      <label>67.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Muhl</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Allison</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Nijholt</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Chanel</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>A survey of affective brain computer interfaces: Principles, state-of-the-art, and challenges</article-title>
        <source>Brain-Comput. Interfaces</source>
        <year>2014</year>
        <volume>1</volume>
        <fpage>66</fpage>
        <lpage>84</lpage>
        <pub-id pub-id-type="doi">10.1080/2326263X.2014.912881</pub-id>
      </element-citation>
    </ref>
    <ref id="B68-sensors-21-05740">
      <label>68.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Koelstra</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Yazdani</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Soleymani</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Mühl</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>J.S.</given-names>
          </name>
          <name>
            <surname>Nijholt</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Pun</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Ebrahimi</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Patras</surname>
            <given-names>I.</given-names>
          </name>
        </person-group>
        <article-title>Single trial classification of EEG and peripheral physiological signals for recognition of emotions induced by music videos</article-title>
        <source>Lect. Notes Comput. Sci.</source>
        <year>2010</year>
        <volume>6334</volume>
        <fpage>89</fpage>
        <lpage>100</lpage>
      </element-citation>
    </ref>
    <ref id="B69-sensors-21-05740">
      <label>69.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Russell</surname>
            <given-names>J.A.</given-names>
          </name>
        </person-group>
        <article-title>A circumplex model of affect</article-title>
        <source>J. Personal. Soc. Psychol.</source>
        <year>1980</year>
        <volume>39</volume>
        <fpage>1161</fpage>
        <lpage>1178</lpage>
        <pub-id pub-id-type="doi">10.1037/h0077714</pub-id>
      </element-citation>
    </ref>
    <ref id="B70-sensors-21-05740">
      <label>70.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Pillette</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Appriou</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Cichocki</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>N’Kaoua</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Lotte</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>Classification of attention types in EEG signals</article-title>
        <source>Proceedings of the International BCI Meeting</source>
        <conf-loc>Pacific Grove, CA, USA</conf-loc>
        <conf-date>21–25 May 2018</conf-date>
      </element-citation>
    </ref>
    <ref id="B71-sensors-21-05740">
      <label>71.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Pillette</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Redefining and Adapting Feedback for Mental-Imagery Based Brain-Computer Interface User Training to the Learners’ Traits and States</article-title>
        <source>Ph.D. Thesis</source>
        <publisher-name>Université de Bordeaux</publisher-name>
        <publisher-loc>Bordeaux, France</publisher-loc>
        <year>2019</year>
      </element-citation>
    </ref>
    <ref id="B72-sensors-21-05740">
      <label>72.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Zomeren</surname>
            <given-names>A.H.</given-names>
          </name>
          <name>
            <surname>Brouwer</surname>
            <given-names>W.H.</given-names>
          </name>
        </person-group>
        <source>Clinical Neuropsychology of Attention</source>
        <publisher-name>Oxford University Press</publisher-name>
        <publisher-loc>Oxford, MS, USA</publisher-loc>
        <year>1994</year>
      </element-citation>
    </ref>
    <ref id="B73-sensors-21-05740">
      <label>73.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schmidt</surname>
            <given-names>R.A.</given-names>
          </name>
        </person-group>
        <article-title>Anticipation and timing in human motor performance</article-title>
        <source>Psychol. Bull.</source>
        <year>1968</year>
        <volume>70</volume>
        <fpage>631</fpage>
        <pub-id pub-id-type="doi">10.1037/h0026740</pub-id>
      </element-citation>
    </ref>
    <ref id="B74-sensors-21-05740">
      <label>74.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sturm</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Willmes</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>On the functional neuroanatomy of intrinsic and phasic alertness</article-title>
        <source>Neuroimage</source>
        <year>2001</year>
        <volume>14</volume>
        <fpage>S76</fpage>
        <lpage>S84</lpage>
        <pub-id pub-id-type="doi">10.1006/nimg.2001.0839</pub-id>
        <pub-id pub-id-type="pmid">11373136</pub-id>
      </element-citation>
    </ref>
    <ref id="B75-sensors-21-05740">
      <label>75.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Van Leeuwen</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Lachmann</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Negative and positive congruence effects in letters and shapes</article-title>
        <source>Atten. Percept. Psychophys.</source>
        <year>2004</year>
        <volume>66</volume>
        <fpage>908</fpage>
        <lpage>925</lpage>
        <pub-id pub-id-type="doi">10.3758/BF03194984</pub-id>
        <?supplied-pmid 15675640?>
        <pub-id pub-id-type="pmid">15675640</pub-id>
      </element-citation>
    </ref>
    <ref id="B76-sensors-21-05740">
      <label>76.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Francis</surname>
            <given-names>A.L.</given-names>
          </name>
        </person-group>
        <article-title>Improved segregation of simultaneous talkers differentially affects perceptual and cognitive capacity demands for recognizing speech in competing speech</article-title>
        <source>Atten. Percept. Psychophys.</source>
        <year>2010</year>
        <volume>72</volume>
        <fpage>501</fpage>
        <lpage>516</lpage>
        <pub-id pub-id-type="doi">10.3758/APP.72.2.501</pub-id>
        <?supplied-pmid 20139463?>
        <pub-id pub-id-type="pmid">20139463</pub-id>
      </element-citation>
    </ref>
    <ref id="B77-sensors-21-05740">
      <label>77.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mulholland</surname>
            <given-names>T.B.</given-names>
          </name>
        </person-group>
        <article-title>The concept of attention and the electroencephalographic alpha rhythm</article-title>
        <source>Atten. Neurophysiol.</source>
        <year>1969</year>
        <volume>24</volume>
        <fpage>100</fpage>
        <lpage>127</lpage>
      </element-citation>
    </ref>
    <ref id="B78-sensors-21-05740">
      <label>78.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ray</surname>
            <given-names>W.J.</given-names>
          </name>
          <name>
            <surname>Cole</surname>
            <given-names>H.W.</given-names>
          </name>
        </person-group>
        <article-title>EEG alpha activity reflects attentional demands, and beta activity reflects emotional and cognitive processes</article-title>
        <source>Science</source>
        <year>1985</year>
        <volume>228</volume>
        <fpage>750</fpage>
        <lpage>752</lpage>
        <pub-id pub-id-type="doi">10.1126/science.3992243</pub-id>
        <pub-id pub-id-type="pmid">3992243</pub-id>
      </element-citation>
    </ref>
    <ref id="B79-sensors-21-05740">
      <label>79.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Barachant</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Congedo</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Jutten</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>Multiclass Brain-Computer Interface Classification by Riemannian Geometry to Cite This Version: Multi-class Brain Computer Interface Classification by Riemannian Geometry</article-title>
        <source>IEEE Trans. Biomed. Eng.</source>
        <year>2012</year>
        <volume>59</volume>
        <fpage>920</fpage>
        <lpage>928</lpage>
        <pub-id pub-id-type="doi">10.1109/TBME.2011.2172210</pub-id>
        <pub-id pub-id-type="pmid">22010143</pub-id>
      </element-citation>
    </ref>
    <ref id="B80-sensors-21-05740">
      <label>80.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Samek</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Müller</surname>
            <given-names>K.R.</given-names>
          </name>
        </person-group>
        <article-title>Towards explainable artificial intelligence</article-title>
        <source>Explainable AI: Interpreting, Explaining and Visualizing Deep Learning</source>
        <publisher-name>Springer</publisher-name>
        <publisher-loc>Berlin/Heidelberg, Germany</publisher-loc>
        <year>2019</year>
        <fpage>5</fpage>
        <lpage>22</lpage>
      </element-citation>
    </ref>
    <ref id="B81-sensors-21-05740">
      <label>81.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Appriou</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Lotte</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>Tools for affective, cognitive and conative states estimation from both EEG and physiological signals</article-title>
        <source>Proceedings of the Third International Neuroergonomics Conference</source>
        <conf-loc>Munich, Germany</conf-loc>
        <conf-date>11–16 September 2021</conf-date>
      </element-citation>
    </ref>
    <ref id="B82-sensors-21-05740">
      <label>82.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rivet</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Souloumiac</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Attina</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Gibert</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>xDAWN Algorithm to Enhance Evoked Potentials: Application to Brain-Computer Interface</article-title>
        <source>IEEE Trans. Biomed. Eng.</source>
        <year>2009</year>
        <volume>56</volume>
        <fpage>2035</fpage>
        <lpage>2043</lpage>
        <pub-id pub-id-type="doi">10.1109/TBME.2009.2012869</pub-id>
        <pub-id pub-id-type="pmid">19174332</pub-id>
      </element-citation>
    </ref>
    <ref id="B83-sensors-21-05740">
      <label>83.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Roy</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Bonnet</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Charbonnier</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Campagne</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Enhancing single-trial mental workload estimation through xDAWN spatial filtering</article-title>
        <source>Proceedings of the International IEEE/EMBS Conference on Neural Engineering, NER 2015</source>
        <conf-loc>Montpellier, France</conf-loc>
        <conf-date>22–24 April 2015</conf-date>
        <fpage>360</fpage>
        <lpage>363</lpage>
        <pub-id pub-id-type="doi">10.1109/NER.2015.7146634</pub-id>
      </element-citation>
    </ref>
    <ref id="B84-sensors-21-05740">
      <label>84.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Noh</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>De Sa</surname>
            <given-names>V.</given-names>
          </name>
        </person-group>
        <article-title>Canonical correlation approach to common spatial patterns</article-title>
        <source>Proceedings of the International IEEE/EMBS Conference on Neural Engineering, NER 2013</source>
        <conf-loc>San Diego, CA, USA</conf-loc>
        <conf-date>6–8 November 2013</conf-date>
        <fpage>669</fpage>
        <lpage>672</lpage>
        <pub-id pub-id-type="doi">10.1109/NER.2013.6696023</pub-id>
      </element-citation>
    </ref>
    <ref id="B85-sensors-21-05740">
      <label>85.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Roy</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Charbonnier</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Jallon</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>A Comparison of ERP Spatial Filtering Methods for Optimal Mental Workload Estimation</article-title>
        <source>Proceedings of the 2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</source>
        <conf-loc>Milan, Italy</conf-loc>
        <conf-date>25–29 August 2015</conf-date>
        <fpage>7254</fpage>
        <lpage>7257</lpage>
      </element-citation>
    </ref>
    <ref id="B86-sensors-21-05740">
      <label>86.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vernon</surname>
            <given-names>J.L.</given-names>
          </name>
          <name>
            <surname>Amelia</surname>
            <given-names>J.S.</given-names>
          </name>
          <name>
            <surname>Nicholas</surname>
            <given-names>R.W.</given-names>
          </name>
          <name>
            <surname>Stephen</surname>
            <given-names>M.G.</given-names>
          </name>
          <name>
            <surname>Chou</surname>
            <given-names>P.H.</given-names>
          </name>
          <name>
            <surname>Brent</surname>
            <given-names>J.L.</given-names>
          </name>
        </person-group>
        <article-title>EEGNet: A compact convolutional neural network for EEG-based brain–computer interfaces</article-title>
        <source>J. Neural Eng.</source>
        <year>2018</year>
        <volume>15</volume>
        <fpage>056013</fpage>
        <pub-id pub-id-type="doi">10.1088/1741-2552/aace8c</pub-id>
        <pub-id pub-id-type="pmid">29932424</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig id="sensors-21-05740-f001" orientation="portrait" position="float">
    <label>Figure 1</label>
    <caption>
      <p>BioPyC data flow: the 4 main modules allow users to follow the standard BCI process for offline EEG and biosignal processing and classification.</p>
    </caption>
    <graphic xlink:href="sensors-21-05740-g001"/>
  </fig>
  <fig id="sensors-21-05740-f002" orientation="portrait" position="float">
    <label>Figure 2</label>
    <caption>
      <p>Comparison of main features of existing toolboxes having modules for EEG signals processing and classification. BioPyC values for each feature are written in black; values of features that are similar to those of BioPyC are written in green; and finally, values of features that differ from those of BioPyC are written in grey. “opt” stands for “optional” in the figure.</p>
    </caption>
    <graphic xlink:href="sensors-21-05740-g002"/>
  </fig>
  <fig id="sensors-21-05740-f003" orientation="portrait" position="float">
    <label>Figure 3</label>
    <caption>
      <p>Screenshot of BioPyC’s widgets, i.e., “select multiples” and buttons at the step of selecting the type of data/signals to work on. In BioPyC, a blue button stands for the action to make, when the disabled orange ones stand for future actions to make: orange buttons turn blue when the previous action is done.</p>
    </caption>
    <graphic xlink:href="sensors-21-05740-g003"/>
  </fig>
  <fig id="sensors-21-05740-f004" orientation="portrait" position="float">
    <label>Figure 4</label>
    <caption>
      <p>Screenshot of BioPyC’s choice of both calibration and evaluation types.</p>
    </caption>
    <graphic xlink:href="sensors-21-05740-g004"/>
  </fig>
  <fig id="sensors-21-05740-f005" orientation="portrait" position="float">
    <label>Figure 5</label>
    <caption>
      <p>Classification accuracy of each algorithm, for each subject, on the “BCI competition IV data set 2a”, in both subject-specific and subject-independent calibrations.</p>
    </caption>
    <graphic xlink:href="sensors-21-05740-g005"/>
  </fig>
  <fig id="sensors-21-05740-f006" orientation="portrait" position="float">
    <label>Figure 6</label>
    <caption>
      <p>Classification accuracy of each algorithm on the “BCI competition IV data set 2a”, in both subject-specific and subject-independent calibrations.</p>
    </caption>
    <graphic xlink:href="sensors-21-05740-g006"/>
  </fig>
  <fig id="sensors-21-05740-f007" orientation="portrait" position="float">
    <label>Figure 7</label>
    <caption>
      <p>Average confusion matrices over all subjects for classification of attention in theta (4–8 Hz) and alpha (8–12 Hz) frequency bands of 5 attentional states, i.e., alertness (tonic), alertness (phasic), sustained, selective, and divided.</p>
    </caption>
    <graphic xlink:href="sensors-21-05740-g007"/>
  </fig>
  <fig id="sensors-21-05740-f008" orientation="portrait" position="float">
    <label>Figure 8</label>
    <caption>
      <p>Classification accuracy of each algorithm on the workload data, in both subject-specific and subject-independent calibrations.</p>
    </caption>
    <graphic xlink:href="sensors-21-05740-g008"/>
  </fig>
  <fig id="sensors-21-05740-f009" orientation="portrait" position="float">
    <label>Figure 9</label>
    <caption>
      <p>Classification accuracy of each algorithm on the valence data, in both subject-specific and subject-independent calibrations.</p>
    </caption>
    <graphic xlink:href="sensors-21-05740-g009"/>
  </fig>
  <fig id="sensors-21-05740-f010" orientation="portrait" position="float">
    <label>Figure 10</label>
    <caption>
      <p>Classification accuracy of each algorithm on the arousal data, in both subject-specific and subject-independent calibrations.</p>
    </caption>
    <graphic xlink:href="sensors-21-05740-g010"/>
  </fig>
</floats-group>
