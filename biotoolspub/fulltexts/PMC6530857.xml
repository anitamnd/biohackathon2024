<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-id journal-id-type="pmc">plosone</journal-id>
    <journal-title-group>
      <journal-title>PLoS ONE</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1932-6203</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6530857</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pone.0217146</article-id>
    <article-id pub-id-type="publisher-id">PONE-D-18-29452</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Electronics</subject>
          <subj-group>
            <subject>Comparators</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Diagnostic Medicine</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Diagnostic Medicine</subject>
          <subj-group>
            <subject>Signs and Symptoms</subject>
            <subj-group>
              <subject>Sepsis</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Pathology and Laboratory Medicine</subject>
          <subj-group>
            <subject>Signs and Symptoms</subject>
            <subj-group>
              <subject>Sepsis</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Pulmonology</subject>
          <subj-group>
            <subject>Pneumonia</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Diagnostic Medicine</subject>
          <subj-group>
            <subject>Signs and Symptoms</subject>
            <subj-group>
              <subject>Sepsis</subject>
              <subj-group>
                <subject>Systemic Inflammatory Response Syndrome</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Pathology and Laboratory Medicine</subject>
          <subj-group>
            <subject>Signs and Symptoms</subject>
            <subj-group>
              <subject>Sepsis</subject>
              <subj-group>
                <subject>Systemic Inflammatory Response Syndrome</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Simulation and Modeling</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Clinical Medicine</subject>
          <subj-group>
            <subject>Clinical Trials</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Pharmacology</subject>
          <subj-group>
            <subject>Drug Research and Development</subject>
            <subj-group>
              <subject>Clinical Trials</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Clinical Trials</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Source Code</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Source Code</subject>
          </subj-group>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>The effect of uncertainty in patient classification on diagnostic performance estimations</article-title>
      <alt-title alt-title-type="running-head">The effect of uncertainty in patient classification on diagnostic performance estimations</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>McHugh</surname>
          <given-names>Leo C.</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Conceptualization</role>
        <role content-type="http://credit.casrai.org/">Data curation</role>
        <role content-type="http://credit.casrai.org/">Formal analysis</role>
        <role content-type="http://credit.casrai.org/">Investigation</role>
        <role content-type="http://credit.casrai.org/">Methodology</role>
        <role content-type="http://credit.casrai.org/">Software</role>
        <role content-type="http://credit.casrai.org/">Visualization</role>
        <role content-type="http://credit.casrai.org/">Writing – original draft</role>
        <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff001">
          <sup>1</sup>
        </xref>
        <xref ref-type="corresp" rid="cor001">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Snyder</surname>
          <given-names>Kevin</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Conceptualization</role>
        <role content-type="http://credit.casrai.org/">Formal analysis</role>
        <role content-type="http://credit.casrai.org/">Investigation</role>
        <role content-type="http://credit.casrai.org/">Methodology</role>
        <role content-type="http://credit.casrai.org/">Software</role>
        <role content-type="http://credit.casrai.org/">Visualization</role>
        <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff002">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-2341-2353</contrib-id>
        <name>
          <surname>Yager</surname>
          <given-names>Thomas D.</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Investigation</role>
        <role content-type="http://credit.casrai.org/">Visualization</role>
        <role content-type="http://credit.casrai.org/">Writing – original draft</role>
        <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff001">
          <sup>1</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <label>1</label>
      <addr-line>Immunexpress, Inc., Seattle, Washington, United States of America</addr-line>
    </aff>
    <aff id="aff002">
      <label>2</label>
      <addr-line>Center for Drug Evaluation and Research, United States Food and Drug Administration, Silver Spring, Maryland, United States of America</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Sartori</surname>
          <given-names>Giuseppe</given-names>
        </name>
        <role>Editor</role>
        <xref ref-type="aff" rid="edit1"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>University of Padova, ITALY</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p><bold>Competing Interests: </bold>Leo C. McHugh and Thomas D. Yager declare that they are present or past employees and shareholders of Immunexpress. Leo C. McHugh declares that he is an inventor on the following patents and patent applications filed by Immunexpress: PCT/AU2017/050894 (“Systemic Inflammatory and Pathogen Biomarkers and Uses Therefor”), PCT/AU2016/050927 (“Pathogen Biomarkers and Uses Therefor”), PCT/AU2016/051269 (“Triage Biomarkers and Uses Therefor”), US 15/160,749 (“Validating Biomarker Measurement”), PCT/AU2016/250388 (“Validating Biomarker Measurement”), US Patent 10,167,511 (“Biomarker Identification”), US Patent 10,190,169 (“Biomarker Identification”), US 14/714,188 (“Biomarker signature method, and apparatus and kits therefor”), US 14/616,565 (“Biomarker signature method, and apparatus and kits therefor”), PCT/AU2015/050043 (“Biomarker signature method, and apparatus and kits therefor”). This does not alter our adherence to PLOS ONE policies on sharing data and materials.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>leo@leomchugh.com</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>22</day>
      <month>5</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>14</volume>
    <issue>5</issue>
    <elocation-id>e0217146</elocation-id>
    <history>
      <date date-type="received">
        <day>10</day>
        <month>10</month>
        <year>2018</year>
      </date>
      <date date-type="accepted">
        <day>6</day>
        <month>5</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <license xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">
        <license-p>This is an open access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">Creative Commons CC0</ext-link> public domain dedication.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pone.0217146.pdf"/>
    <abstract>
      <sec id="sec001">
        <title>Background</title>
        <p>The performance of a new diagnostic test is typically evaluated against a comparator which is assumed to correspond closely to some true state of interest. Judgments about the new test’s performance are based on the differences between the outputs of the test and comparator. It is commonly assumed that a small amount of uncertainty in the comparator’s classifications will negligibly affect the measured performance of a diagnostic test.</p>
      </sec>
      <sec id="sec002">
        <title>Methods</title>
        <p>Simulated datasets were generated to represent typical diagnostic scenarios. Comparator noise was introduced in the form of random misclassifications, and the effect on the apparent performance of the diagnostic test was determined. An actual dataset from a clinical trial on a new diagnostic test for sepsis was also analyzed.</p>
      </sec>
      <sec id="sec003">
        <title>Results</title>
        <p>We demonstrate that as little as 5% misclassification of patients by the comparator can be enough to statistically invalidate performance estimates such as sensitivity, specificity and area under the receiver operating characteristic curve, if this uncertainty is not measured and taken into account. This distortion effect is found to increase non-linearly with comparator uncertainty, under some common diagnostic scenarios. For clinical populations exhibiting high degrees of classification uncertainty, failure to measure and account for this effect will introduce significant risks of drawing false conclusions. The effect of classification uncertainty is magnified further for high performing tests that would otherwise reach near-perfection in diagnostic evaluation trials. A requirement of very high diagnostic performance for clinical adoption, such as a 99% sensitivity, can be rendered nearly unachievable even for a perfect test, if the comparator diagnosis contains even small amounts of uncertainty. This paper and an accompanying online simulation tool demonstrate the effect of classification uncertainty on the apparent performance of tests across a range of typical diagnostic scenarios. Both simulated and real datasets are used to show the degradation of apparent test performance as comparator uncertainty increases.</p>
      </sec>
      <sec id="sec004">
        <title>Conclusions</title>
        <p>Overall, a 5% or greater misclassification rate by the comparator can lead to significant underestimation of true test performance. An online simulation tool allows researchers to explore this effect using their own trial parameters (<ext-link ext-link-type="uri" xlink:href="https://imperfect-gold-standard.shinyapps.io/classification-noise/">https://imperfect-gold-standard.shinyapps.io/classification-noise/</ext-link>) and the source code is freely available (<ext-link ext-link-type="uri" xlink:href="https://github.com/ksny/Imperfect-Gold-Standard">https://github.com/ksny/Imperfect-Gold-Standard</ext-link>).</p>
      </sec>
    </abstract>
    <funding-group>
      <funding-statement>Immunexpress provided salary support for Leo C. McHugh and Thomas D. Yager over the course of this work. The funding organization did not play a role in the study design, data collection and analysis, or choice of content in the manuscript, and only provided financial support in the form of these authors' salaries. The specific roles of these authors are articulated in the ‘author contributions’ section of the manuscript.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="8"/>
      <table-count count="4"/>
      <page-count count="19"/>
    </counts>
    <custom-meta-group>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>All relevant data are within the manuscript and its Supporting Information files. The simulation tool is freely available online at the following website: <ext-link ext-link-type="uri" xlink:href="https://imperfect-gold-standard.shinyapps.io/classification-noise/">https://imperfect-gold-standard.shinyapps.io/classification-noise/</ext-link>. The source code for the simulation tool has been placed in GitHub and is publicly available at this location: <ext-link ext-link-type="uri" xlink:href="https://github.com/ksny/Imperfect-Gold-Standard">https://github.com/ksny/Imperfect-Gold-Standard</ext-link>.</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>All relevant data are within the manuscript and its Supporting Information files. The simulation tool is freely available online at the following website: <ext-link ext-link-type="uri" xlink:href="https://imperfect-gold-standard.shinyapps.io/classification-noise/">https://imperfect-gold-standard.shinyapps.io/classification-noise/</ext-link>. The source code for the simulation tool has been placed in GitHub and is publicly available at this location: <ext-link ext-link-type="uri" xlink:href="https://github.com/ksny/Imperfect-Gold-Standard">https://github.com/ksny/Imperfect-Gold-Standard</ext-link>.</p>
  </notes>
</front>
<body>
  <sec sec-type="intro" id="sec005">
    <title>Introduction</title>
    <p>The performance of a new diagnostic test is typically evaluated against a comparator or ‘gold standard’ which is assumed to correspond closely to some true state of interest (‘Ground Truth’). Judgments about the performance of the new test are based on the differences between the outputs of the test and its comparator. In many contexts, however, the comparator itself may be imperfect. Examples of imperfect comparators are common in medical diagnostics. These include comparators for which the measured value is precise but imperfectly represents the state of the underlying condition, such as serum creatinine for diagnosing kidney injury [<xref rid="pone.0217146.ref001" ref-type="bibr">1</xref>]; comparators for which the measurements are variable but interpretation of the result is not variable such as the diagnosis of hypertension [<xref rid="pone.0217146.ref002" ref-type="bibr">2</xref>]; and comparators for which accurate measurements are taken that truly represent the true state of the patient, but for which inconsistency exists in the interpretation of results, such as the diagnosis of pneumonia by chest X-ray [<xref rid="pone.0217146.ref003" ref-type="bibr">3</xref>]. These sources of variability in the comparator introduce a problem in the interpretation of data generated by a new diagnostic test. It may not be possible to know whether discrepancies between the results produced by the test and the comparator are due to inaccuracy of the test, inaccuracy of the comparator, or both. In such cases the measured performance of the new test relative to that of the comparator will not be an accurate indicator of the true test performance.</p>
    <p>It is commonly assumed that a small amount of uncertainty in classification by the comparator will be of negligible consequence, when measuring the performance of a diagnostic test. The present work critically examines this assumption in a variety of contexts, and shows it to be generally false, especially for tests that are required to have very high performance. Non-statistician medical experts may underappreciate the magnitude of the effect of even small amounts of comparator uncertainty on apparent test performance. Consequently, in diagnostic evaluation studies, comparator uncertainty may not always be identified or accounted for in the analysis or interpretation of results, thus risking erroneous or biased conclusions (see, for example, reference [<xref rid="pone.0217146.ref004" ref-type="bibr">4</xref>] and references 18–25 therein). The purpose of the present study is not to develop theory to allow calculation of this effect, as theory is already well researched and established [<xref rid="pone.0217146.ref005" ref-type="bibr">5</xref>,<xref rid="pone.0217146.ref006" ref-type="bibr">6</xref>]. Rather we seek to further develop two practical aspects: 1) to explore, by way of specific examples, the consequence and magnitude of the effect of comparator uncertainty on the apparent performance of binary tests, in various diagnostic settings; and 2) to present a simulation tool that will be useful for clinical trial stakeholders who might not have the specialized statistical training or tools needed to estimate the effect of comparator classification uncertainty, yet who nonetheless need to understand this effect for the correct interpretation of trials. The accompanying simulation tool (<ext-link ext-link-type="uri" xlink:href="https://imperfect-gold-standard.shinyapps.io/classification-noise/">https://imperfect-gold-standard.shinyapps.io/classification-noise/</ext-link>) will allow the non-statistician medical expert to easily conduct simulations to further explore the effects of comparator uncertainty, without the need for advanced statistical training. Some of the results of our study have been previously reported in the form of an abstract [<xref rid="pone.0217146.ref007" ref-type="bibr">7</xref>].</p>
  </sec>
  <sec id="sec006">
    <title>Definitions</title>
    <p><italic>Call</italic>: A positive or negative classification or designation, derived or provided by any method, algorithm, test or device. For example, a test result falling above a given threshold could be considered a positive call, and a clinician’s opinion that a patient is disease-free could be considered a negative call.</p>
    <p><italic>Comparator</italic>: a previously established test method, against which the results from a new test will be compared. ‘Comparator’ is used in preference to ‘gold standard’ or ‘reference method’ to signify that the results from the comparator may diverge significantly from the Ground Truth.</p>
    <p><italic>Ground Truth</italic>: the true positive or negative state of a subject, in a binary classification scheme.</p>
    <p><italic>Negative Percent Agreement (NPA)</italic>: The percentage of comparator negative calls that are called as negative by the test under evaluation. This value is calculated identically to specificity. However, NPA is used in place of specificity to recognize the fact that due to the uncertain comparator, this measure should not be construed to accurately reflect the measurement that specificity presumes.</p>
    <p><italic>Positive Percent Agreement (PPA)</italic>: The percentage of comparator positive calls that are called as positive by the test under evaluation. This value is calculated identically to sensitivity. However, PPA is used in place of sensitivity to recognize the fact that due to the uncertain comparator, this measure should not be construed to accurately reflect the measurement that sensitivity presumes.</p>
  </sec>
  <sec id="sec007">
    <title>Theory</title>
    <p>The effect of classification uncertainty on apparent test performance is known variously as ‘information bias’, ‘misclassification bias’ or ‘non-differential bias’ in medicine and epidemiology and goes by other names in other fields [<xref rid="pone.0217146.ref008" ref-type="bibr">8</xref>–<xref rid="pone.0217146.ref010" ref-type="bibr">10</xref>]. These terms refer to the fact that as classification uncertainty increases, an increasingly large gap will appear between the true performance of the test and empirical measures of test performance such as sensitivity, specificity, negative predictive value (NPV), positive predictive value (PPV), or area under the receiver operating characteristic curve (ROC AUC). It has been recognized for many years that the imperfection of available comparators constitutes a source of difficulty in the evaluation of new diagnostic tests [<xref rid="pone.0217146.ref011" ref-type="bibr">11</xref>–<xref rid="pone.0217146.ref016" ref-type="bibr">16</xref>]. The more recent literature describes a number of examples in which the use of imperfect comparators has led to complications in evaluating the performance of new diagnostic tests for conditions as varied as carpal tunnel syndrome [<xref rid="pone.0217146.ref017" ref-type="bibr">17</xref>], kidney injury [<xref rid="pone.0217146.ref001" ref-type="bibr">1</xref>,<xref rid="pone.0217146.ref018" ref-type="bibr">18</xref>] and leptospirosis [<xref rid="pone.0217146.ref019" ref-type="bibr">19</xref>].</p>
    <sec id="sec008">
      <title>Reference bias and classification noise</title>
      <p>In general, discrepancies between a comparator and the true state it purports to measure may arise from two sources: reference bias and classification noise.</p>
      <p><italic>Reference bias</italic> is the tendency of a comparator to produce values that fall systematically to one side of the true state being evaluated, resulting in consistent misclassification of patients on the basis of known or unknown patient characteristics. For example, multiple clinicians may agree on a diagnosis based upon reference to a single comparator, but if the comparator is biased or deficient in some way, then we would expect the agreed-upon cases to have a common tendency to be incorrect. Similarly, if one of the variables used to place subjects into categories is overly weighted, then the comparator will be biased away from the true state and towards the over-weighted variable. As a third example, reference bias may also occur when there is incomplete representation (i.e. missing data) for some of the variables used for classification. This will lead to reference bias away from the incompletely specified variables, and towards those variables that have greatest representation in the dataset. A fourth example of reference bias may occur when multiple clinicians are consulted for a medical diagnosis. If, for a patient, each clinician can see the diagnoses made by the previous clinicians, this could lead to a reference bias in the direction of the earlier diagnoses. A fifth example of reference bias is automation bias, where a software algorithm consistently makes the same mistakes in the same kinds of patients, such as in the automated diagnosis of electrocardiograms [<xref rid="pone.0217146.ref020" ref-type="bibr">20</xref>]. Reference bias is particularly difficult to detect because multiple independent comparators (for example independent clinicians’ diagnoses) may be consistent with each other, giving the appearance of being correct, yet may be incorrect nonetheless. Furthermore, because the Ground Truth cannot be known directly, estimating the magnitude of reference bias is difficult. Approaches such as discrepant analysis [<xref rid="pone.0217146.ref021" ref-type="bibr">21</xref>] have been proposed, for estimating the magnitude of reference bias in particular situations.</p>
      <p>It is also worth noting that in rare cases, reference bias can lead to inflation of the apparent performance of a test, as described in <xref ref-type="supplementary-material" rid="pone.0217146.s001">S1 Supporting Information</xref> (“Example of reference bias”). This can occur when the same manner of classification bias is present in both the comparator and the new diagnostic test under evaluation, leading to correlated misclassifications of the same patients. The risk of reference bias-induced performance inflation is only relevant under the conditions of low noise in both the new test and the comparator. This is because noise will decouple the agreements produced by the correlated biases in the new test and comparator. In this paper we acknowledge but do not further explore reference bias because is it not usually possible to measure it. However, reference bias can be seen as an additional potential contributor to under-estimation of diagnostic performance in datasets such as those considered in this paper.</p>
      <p><italic>Classification noise</italic> is the other fundamental cause of differences between a comparator and the Ground Truth it is supposed to represent. It can be viewed as the amount of uncertainty inherent in the classifications produced by the comparator. Classification noise can be intuitively understood by considering that if a comparator diagnosis is uncertain, and if a number of similar cases are presented and similarly classified, we would expect some of them to be wrong (but would not know which ones). We define classification noise as instability (random variation) in the comparator, which produces randomly scattered values on either side of the true state. Classification noise is not attributed to systematic causes, but rather to stochastic processes or to the absence of information that would close the gap between measured values and Ground Truth.</p>
      <p>Noise in a comparator can be estimated by applying the comparator multiple times to the same patient or sample, or to replicate samples, and then observing the variation between results. (The replicate results would be identical if the comparator did not contain noise.) For example, in a situation where a comparator consists of the consensus of expert opinions, different clinicians making diagnoses may interpret the same information differently, leading to different diagnoses for the same patient. In defining a comparator, attempts should be made to minimize the amount of classification noise. However, because the comparator is based ultimately on experimental measurements or empirical assessments, it will not be possible to remove all classification noise.</p>
    </sec>
    <sec id="sec009">
      <title>Uncertainty and misclassification events</title>
      <p>Uncertainty in patient classification can be measured in a number of ways, most commonly by an inter-observer agreement statistic such as Cohen’s Kappa, or by the correlation terms in a Multitrait-Multimethod Matrix. These and related statistics estimate the extent of agreement in classifying the same patients or samples by different tests or reviewers, relative to the extent of agreement that would be expected at random. Cohen’s Kappa ranges from 0 to 1. A value of 1 indicates perfect agreement, and values of less than 0.65 are generally interpreted to mean that there is a high degree of variability in classifying the same patients or samples. Kappa values are often used to describe inter-rater reliability (i.e. the same patient between clinicians) and intra-rater reliability (i.e. the same patient with the same clinician on different days). Kappa values can also be used to estimate the variability in test measurements, such as between commercially available at-home pregnancy tests. Variability in patient classification can also be captured directly as a probability, as in standard Bayesian analysis. Irrespective of which metric is used to capture the variability in classification, there is a direct correspondence between the measured variability in a test or comparator, the uncertainty reflected in that measurement, and the misclassifications that occur as a result of this uncertainty.</p>
      <p>Generally speaking, a known amount of uncertainty will correspond to an exact expected misclassification rate. However, for diagnosis in any specific patient cohort, an observer will not know for sure which patients have been misdiagnosed, or even how many have been misclassified. For example, if a test is used for binary classification and we know that a negative call for a test is 95% accurate, then for each patient classified as negative there will be a 5% chance that the patient is actually positive. The random nature of the uncertainty means that for 100 patients in a trial who have been called negative by the test, we expect 5% to be misclassified, but it could be that actually in the trial 10 are misclassified, or that none are (although both of these alternatives are relatively unlikely). The misclassification rate for a trial can be estimated in a number of ways, such as repeat testing of samples, comparison to another test of presumed greater accuracy, or inferring an expected error rate from other sources of information.</p>
    </sec>
    <sec id="sec010">
      <title>Example: Smeared distribution of output values</title>
      <p>A comparator might have an inherent property or limitation that causes it to return a broadened distribution of values, as compared to the Ground Truth that is being measured. An example is shown in <xref ref-type="fig" rid="pone.0217146.g001">Fig 1</xref>. Suppose that a particular condition is characterized by a variable having a continuous normal distribution at the level of Ground Truth, and that cutoffs have been defined to identify rare events (positive or negative calls) at the tails of the distribution. Suppose also that a comparator used to repeatedly measure this condition returns a Cauchy distribution of values. Then the distribution of measured values will have extended tails, not present at the Ground Truth level, which could lead to either false positive or false negative calls being made by the comparator. See <xref ref-type="fig" rid="pone.0217146.g001">Fig 1</xref> and also references [<xref rid="pone.0217146.ref022" ref-type="bibr">22</xref>,<xref rid="pone.0217146.ref023" ref-type="bibr">23</xref>].</p>
      <fig id="pone.0217146.g001" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0217146.g001</object-id>
        <label>Fig 1</label>
        <caption>
          <title>Example illustrating the problem of noise in a comparator.</title>
        </caption>
        <graphic xlink:href="pone.0217146.g001"/>
      </fig>
      <p>With increasing noise in a comparator, the totality of observed differences between a diagnostic test under evaluation and the comparator will increase. The consequence is that any new diagnostic test will appear to perform better relative to a comparator containing less noise, and worse relative to a comparator containing more noise. Consequently, the new test may appear to exhibit different levels of performance in different populations or settings where the amount of comparator noise can vary [<xref rid="pone.0217146.ref024" ref-type="bibr">24</xref>].</p>
    </sec>
  </sec>
  <sec sec-type="materials|methods" id="sec011">
    <title>Methods</title>
    <sec id="sec012">
      <title>Simulated data</title>
      <p>Simulated datasets were generated to represent typical diagnostic scenarios, and are presented as a ‘starting point’ assuming no classification error. Comparator noise was then introduced in the form of random misclassifications. The effect on the apparent performance of the diagnostic test was determined, as the amount of comparator noise was increased. To present a statistically valid representation of the randomness of noise injection, each amount of noise was randomly introduced in 100 iterations and the aggregate results are shown. The same simulation methodology used to generate the figures discussed below has also been implemented in an online simulation tool allows researchers to explore the comparator noise effect using their own trial parameters: <ext-link ext-link-type="uri" xlink:href="https://imperfect-gold-standard.shinyapps.io/classification-noise/">https://imperfect-gold-standard.shinyapps.io/classification-noise/</ext-link>. The source code for the simulation tool has been made publicly available: <ext-link ext-link-type="uri" xlink:href="https://github.com/ksny/Imperfect-Gold-Standard">https://github.com/ksny/Imperfect-Gold-Standard</ext-link>.</p>
    </sec>
    <sec id="sec013">
      <title>Actual data</title>
      <p>We also consider data from a study conducted in the USA and Netherlands on a new sepsis diagnostic test [<xref rid="pone.0217146.ref025" ref-type="bibr">25</xref>]. Three independent diagnoses per patient were rendered by expert panelists on the basis of information contained in case report forms, and the combination of diagnoses was used to determine the overall confidence of classification for each patient as detailed in <xref ref-type="supplementary-material" rid="pone.0217146.s002">S2 Supporting Information</xref> (“Method to estimate the confidence of patient classifications by an expert panel comparator”). Misclassifications were introduced randomly, weighted by the uncertainty distribution observed in patient classification as described in <xref ref-type="supplementary-material" rid="pone.0217146.s003">S3 Supporting Information</xref> (“Weighting for introduced misclassification events”). To present a statistically valid representation of the randomness of selection, each injection of classification noise was randomly drawn from the uncertainty distribution observed in the trial and introduced in 100 iterations, and the aggregate results are shown. Four different selections of patients from the total trial enrollment (N = 447) were made, and were analyzed separately: (1) the subset of patients (N = 290; 64.9% of total) who received unanimous concordant diagnoses by the external expert panelists, and who were also assigned the same diagnosis by the study investigators at the clinical sites where the patients originated. We deemed this the “super-unanimous” group, and assumed that when the external expert panelists and the study investigators at the clinical sites agreed, the diagnoses were more likely to be correct. These patients represent the stratum of the trial cohort with the lowest probability of error in the comparator; (2) the subset of patients (N = 410; 91.7% of total) who received a consensus (majority) diagnosis by the external panel. This patient subset excluded 37 patients who were classified as ‘indeterminate’ because a consensus diagnosis could not be reached by the expert panelists; (3) the set of all patients (N = 447) with a forced diagnosis of either positive or negative, regardless of the degree of uncertainty associated with each patient; (4) the subset of patients with clinical notes indicating respiratory related illness (N = 93; 20.8% of total) for whom a relatively high level of classification uncertainty was expected and observed. Each of these four selections of patients had an expected misclassification rate determined by the mean of the residual uncertainty averaged over the three external panelists’s assessments, as detailed in <xref ref-type="supplementary-material" rid="pone.0217146.s004">S4 Supporting Information</xref> (“Calculating misclassification rates, based on patient confidence values”).</p>
    </sec>
  </sec>
  <sec sec-type="results" id="sec014">
    <title>Results</title>
    <sec id="sec015">
      <title>Simulated data</title>
      <p><xref ref-type="fig" rid="pone.0217146.g002">Fig 2</xref> shows graphically the effect of misclassification by the comparator on the interpretation of diagnostic test performance. This figure was generated from a simulation with 100 Ground Truth negative samples and 100 Ground Truth positive samples. Panel A shows the true test performance (0% comparator misclassification), while Panel B shows the effect of randomly injecting 5% misclassification into the comparator calls. The quantitative results from this particular simulation are compiled in <xref rid="pone.0217146.t001" ref-type="table">Table 1</xref>. To assess the significance of the apparent differences in misclassification rates suggested by this table, we conducted a further investigation in which the number of simulated samples (trial size) was varied (<xref ref-type="supplementary-material" rid="pone.0217146.s005">S5 Supporting Information</xref>, “Decrease in apparent performance of index test, with 5% noise injected into comparator”). As expected, we found all the confidence intervals to shrink with increasing trial size. These results demonstrate the generality that for AUC, sensitivity/PPA, specificity/NPA, PPV and NPV, any degree of misclassification will lead to underestimates of true performance which can be detected if the trial is large enough and if the Ground Truth is known.</p>
      <fig id="pone.0217146.g002" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0217146.g002</object-id>
        <label>Fig 2</label>
        <caption>
          <title>Example of the effect of misclassification by a comparator, on the apparent performance of a diagnostic test.</title>
          <p>A total of 100 Ground Truth negative patients and 100 Ground Truth positive patients were considered. In Panel A, there is no error in patient classification (i.e. the comparator is perfectly concordant with the Ground Truth). In Panel B, a random 5% of the comparator’s classifications are assumed to diverge incorrectly from the Ground Truth. The difference in the distribution of test scores (y-axis) between the panels of this figure results in significant underestimates of diagnostic performance as shown in <xref rid="pone.0217146.t001" ref-type="table">Table 1</xref>.</p>
        </caption>
        <graphic xlink:href="pone.0217146.g002"/>
      </fig>
      <table-wrap id="pone.0217146.t001" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0217146.t001</object-id>
        <label>Table 1</label>
        <caption>
          <title>Effect of uncertainty in the comparator on estimates of test performance.</title>
        </caption>
        <alternatives>
          <graphic id="pone.0217146.t001g" xlink:href="pone.0217146.t001"/>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="left" rowspan="1" colspan="1">Parameter</th>
                <th align="left" rowspan="1" colspan="1">0% Misclassification<break/>Rate in the Comparator</th>
                <th align="left" rowspan="1" colspan="1">5% Misclassification<break/>Rate in the Comparator</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <bold>AUC</bold>
                </td>
                <td align="left" rowspan="1" colspan="1">0.977 (0.960–0.993)</td>
                <td align="left" rowspan="1" colspan="1">0.940 (0.910–0.971)</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <bold>Sensitivity/PPA</bold>
                </td>
                <td align="left" rowspan="1" colspan="1">0.970 (0.915–0.994)</td>
                <td align="left" rowspan="1" colspan="1">0.929 (0.858–0.971)</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <bold>Specificity/NPA</bold>
                </td>
                <td align="left" rowspan="1" colspan="1">0.850 (0.765–0.914)</td>
                <td align="left" rowspan="1" colspan="1">0.794 (0.703–0.868)</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <bold>PPV</bold>
                </td>
                <td align="left" rowspan="1" colspan="1">0.866 (0.789–0.923)</td>
                <td align="left" rowspan="1" colspan="1">0.812 (0.728–0.880)</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <bold>NPV</bold>
                </td>
                <td align="left" rowspan="1" colspan="1">0.966 (0.904–0.993)</td>
                <td align="left" rowspan="1" colspan="1">0.920 (0.843–0.967)</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>This table corresponds to <xref ref-type="fig" rid="pone.0217146.g002">Fig 2</xref>. A total of 100 Ground Truth negative patients and 100 Ground Truth positive patients were considered. The 95% confidence intervals about the medians were computed by resampling and are given within the parentheses.</p>
      <p><xref ref-type="fig" rid="pone.0217146.g003">Fig 3</xref> shows the effect of false positive and false negative comparator misclassifications on the apparent performance of a perfect test. In this simulation, there is no overlap between Ground Truth negative and Ground Truth positive patients. The test is assumed 100% accurate, so the reduced test performance values shown under various comparator misclassification rates are purely a result of uncertainty in the comparator. Varying the comparator misclassification rate between 0% and 20% results in a monotonic drop in AUC and other performance measures. <xref ref-type="fig" rid="pone.0217146.g003">Fig 3</xref> also illustrates the point that the observed decrease in apparent test performance due to comparator noise can be expressed relative to the maximum possible test performance in the absence of comparator noise.</p>
      <fig id="pone.0217146.g003" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0217146.g003</object-id>
        <label>Fig 3</label>
        <caption>
          <title>Degradation of apparent performance of a perfect diagnostic test, as a function of error in the comparator.</title>
          <p>In this scenario, Ground Truth positive patients and Ground Truth negative patients are equally likely to be misclassified by the comparator. (A) Comparator with no classification error, perfectly representing the Ground Truth for 100 negative patients and 100 positive patients. (B) Apparent performance of diagnostic test, as a function of the misclassification rate of the comparator. The error bars describe 95% empirical confidence intervals about medians, computed over 100 simulation cycles. True test performance is indicated when the FP and FN rates are each 0%. The terms Sensitivity and Specificity are appropriate when there is no misclassification in the comparator (FP rate = FN rate = 0%). The terms Positive Percent Agreement (PPA) and Negative Percent Agreement (NPA) should be used in place of Sensitivity and Specificity, respectively, when the comparator is known to contain uncertainty.</p>
        </caption>
        <graphic xlink:href="pone.0217146.g003"/>
      </fig>
      <p>It appears from these simulations that once the comparator misclassification rates are in excess of about 5%, all test performance measures are significantly under-estimated and therefore should not be reported without acknowledging this effect. This figure also shows that when classification uncertainty is present in the comparator, any calculated performance measure will fall randomly within a range represented in <xref ref-type="fig" rid="pone.0217146.g003">Fig 3</xref> by confidence intervals. As the comparator uncertainty increases, in addition to the general downward trend on the median apparent test performance value, the apparent test performance values will vary randomly within increasingly large ranges, as represented by increasingly wide confidence intervals.</p>
      <p>Fig S6.1, presented in <xref ref-type="supplementary-material" rid="pone.0217146.s006">S6 Supporting Information</xref> (“Unequal FP and FN rates”), displays a modification of this effect, in which the false positive rate is twice as high as the false negative rate (Panel B of this figure). This situation could occur, for example, in the diagnosis of a serious infectious disease for which a treatment exists, where a clinician typically will ‘err on the side of caution’ in classifying patients, under the assumption that it is better to over-treat with side effects than under-treat with serious consequences (false positives being considered less risky than false negatives). Fig S6.2, presented in <xref ref-type="supplementary-material" rid="pone.0217146.s006">S6 Supporting Information</xref> (“Unequal FP and FN rates”), displays the complementary scenario in which the false negative rate is twice as high as the false positive rate. Certain types of tests, for example home pregnancy tests, are known to suffer from high false negative rates [<xref rid="pone.0217146.ref026" ref-type="bibr">26</xref>,<xref rid="pone.0217146.ref027" ref-type="bibr">27</xref>].</p>
      <p><xref ref-type="fig" rid="pone.0217146.g004">Fig 4</xref> displays a less idealized diagnostic scenario, in which there is some small degree of overlap between Ground Truth negative and Ground Truth positive patients. We consider such a typical high-performing test, and estimate the degradation of apparent test performance under conditions of increasing comparator uncertainty. Panel A shows the distribution of test results against the Ground Truth. Panel B shows the expected decrease in all test performance parameters, as a monotonic function of increasing comparator uncertainty. Note the generally worse apparent test performance in <xref ref-type="fig" rid="pone.0217146.g004">Fig 4</xref> at all levels of comparator misclassification, as compared to <xref ref-type="fig" rid="pone.0217146.g003">Fig 3</xref> in which Ground Truth negative and Ground Truth positive patients display no overlap in diagnostic test scores.</p>
      <fig id="pone.0217146.g004" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0217146.g004</object-id>
        <label>Fig 4</label>
        <caption>
          <title>Degradation of apparent performance of a perfect diagnostic test, as a function of error in the comparator.</title>
          <p>(A) Representation of Ground Truth for 100 negative patients (grey points) and 100 positive patients (blue points). A slight overlap between Ground Truth negative and Ground Truth positive distributions is assumed, leading to AUC 0.98 with the Ground Truth as reference. (B) Apparent performance of a perfect diagnostic test, as a function of the misclassification rate of the comparator. The error bars describe 95% empirical confidence intervals about medians, computed over 100 simulation cycles. True test performance is indicated when the FP and FN rates are each 0%. The terms Sensitivity and Specificity are appropriate when there is no misclassification in the comparator (FP rate = FN rate = 0%). The terms Positive Percent Agreement (PPA) and Negative Percent Agreement (NPA) should be used in place of Sensitivity and Specificity, respectively, when the comparator is known to contain uncertainty.</p>
        </caption>
        <graphic xlink:href="pone.0217146.g004"/>
      </fig>
      <p><xref ref-type="fig" rid="pone.0217146.g005">Fig 5</xref> simulates a screening test in a low prevalence setting, in which Ground Truth negatives are significantly more prevalent than Ground Truth positives. An example of such a scenario is found in screening for cervical cancer by Pap smear cytology, in which significant false positive rates (cellular abnormalities of unknown significance) may be anticipated, and in which positive test results do not necessarily confer high confidence regarding the presence of high level disease [<xref rid="pone.0217146.ref028" ref-type="bibr">28</xref>,<xref rid="pone.0217146.ref029" ref-type="bibr">29</xref>].</p>
      <fig id="pone.0217146.g005" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0217146.g005</object-id>
        <label>Fig 5</label>
        <caption>
          <title>A simulated inaccurate screening test in a moderately low prevalence setting.</title>
          <p>In this scenario, ground truth positive patients are equally likely to be misclassified as ground truth negative patients. (A) Representation of Ground Truth for 250 negative patients, and 50 positive patients, with significant overlap between the positive and negative ground truth distributions. (B) Apparent performance of diagnostic test, as a function of the misclassification rate of the comparator. The error bars describe 95% empirical confidence intervals about medians, computed over 100 simulation cycles. True test performance is indicated when the FP and FN rates are each 0%. The terms Sensitivity and Specificity are appropriate when there is no misclassification in the comparator (FP rate = FN rate = 0%). The terms Positive Percent Agreement (PPA) and Negative Percent Agreement (NPA) should be used in place of Sensitivity and Specificity, respectively, when the comparator is known to contain uncertainty.</p>
        </caption>
        <graphic xlink:href="pone.0217146.g005"/>
      </fig>
      <p><xref ref-type="fig" rid="pone.0217146.g006">Fig 6</xref> simulates the effect of comparator uncertainty in a screening test scenario of even lower disease prevalence, for example an epidemiological setting [<xref rid="pone.0217146.ref030" ref-type="bibr">30</xref>,<xref rid="pone.0217146.ref031" ref-type="bibr">31</xref>].</p>
      <fig id="pone.0217146.g006" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0217146.g006</object-id>
        <label>Fig 6</label>
        <caption>
          <title>A simulated screening test in a low prevalence setting, for example for a relatively uncommon infectious disease.</title>
          <p>In this scenario ground truth positive patients are equally likely to be misclassified as negative patients (A) Representation of Ground Truth for 1950 negative patients, and 50 positive patients. with some overlap between the positive and negative ground truth distributions. (B) Apparent performance of diagnostic test, as a function of the misclassification rate of the comparator. The error bars describe 95% empirical confidence intervals about medians, computed over 100 simulation cycles. True test performance is indicated when the FP and FN rates are each 0%. The terms Sensitivity and Specificity are appropriate when there is no misclassification in the comparator (FP rate = FN rate = 0%). The terms Positive Percent Agreement (PPA) and Negative Percent Agreement (NPA) should be used in place of Sensitivity and Specificity, respectively, when the comparator is known to contain uncertainty.</p>
        </caption>
        <graphic xlink:href="pone.0217146.g006"/>
      </fig>
    </sec>
    <sec id="sec016">
      <title>Actual data</title>
      <p>We next turn to the analysis of a real dataset, collected during a clinical validation study of a novel diagnostic test for sepsis [<xref rid="pone.0217146.ref025" ref-type="bibr">25</xref>]. The test was designed to discriminate infection-induced sepsis from non-infectious systemic inflammatory response syndrome (SIRS) in adult critical care patients, and was validated using a cohort of 447 patients from seven sites in the USA and one site in the Netherlands. In the validation study, the comparator consisted of retrospective physician diagnosis (RPD) by a panel of three independent expert clinicians, leading to either the unanimous, consensus, or forced classification of patients as having either sepsis or SIRS.</p>
      <p><xref ref-type="fig" rid="pone.0217146.g007">Fig 7</xref>, Panel A shows the distribution of test scores for the Super-Unanimous subset of 290 patients (119 sepsis, 171 SIRS), defined as those patients who were classified as either sepsis or SIRS by all three of the external expert panelists and also by the study investigators at the clinical sites where the patients were recruited. As stated previously, these patients represent the stratum of the trial cohort with the lowest expected probability of error in the comparator. Panel B shows the calculated estimates of performance (AUC, sensitivity/PPA, specificity/NPA, PPV, NPV) as an increasing amount of uncertainty is introduced into the comparator. The false positive (FP) and false negative (FN) rates for the Super-Unanimous subset are assumed to be zero, as shown by the leftmost vertical dotted line of panel B. For the Consensus subset, the observed misclassification rates were 4.9% FP and 4.7% FN, which correspond to an injection of about 4.84% random misclassifications into the Super-Unanimous subset. For the Forced subset, the observed misclassification rates were 6.1% FP and 9.0% FN, which correspond to an injection of about 7.46% random misclassifications into the Super-Unanimous subset. In panel B, the triangles indicate the calculated values of the performance parameters (AUC, sensitivity/PPA, specificity/NPA, PPV, NPV), after injection of the stated amounts of uncertainty (random misclassification noise) into the comparator.</p>
      <fig id="pone.0217146.g007" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0217146.g007</object-id>
        <label>Fig 7</label>
        <caption>
          <p>(A) Real data from a clinical trial for a new sepsis diagnostic test, conducted over 8 sites in the USA and Netherlands [<xref rid="pone.0217146.ref025" ref-type="bibr">25</xref>]. (B) The apparent performance of the test (y axis) decreases as uncertainty is introduced into the comparator (x axis). 95% confidence intervals are shown. The difference between the apparent test performance at a given comparator misclassification rate and at a comparator misclassification rate of zero indicates the degree of underestimation of true test performance due to uncertainty in the comparator. The vertical lines mark the observed misclassification rates for various patient subsets within the same trial, as described in the text. Misclassification rates are based on quantifying the discordance between independent expert opinions. Solid triangles show the observed measurements for the trial for each of these groups without correction for comparator uncertainty. Sensitivity/PPA and Specificity/NPA are each marked with an asterisk (*) to emphasize that these measures assume no misclassification in the comparator. Positive Percent Agreement (PPA) and Negative Percent Agreement (NPA) are the correct terms to use, when the comparator is known to contain uncertainty as in this case.</p>
        </caption>
        <graphic xlink:href="pone.0217146.g007"/>
      </fig>
      <p>We simulated the effects of comparator uncertainty by starting with the Super-Unanimous classifications (assumed to be error free), injecting noise (uncertainty) into the underlying comparator (randomly sampled from the empirical noise distribution), and calculating the resultant increase in apparent FP and FN rates. As the comparator misclassification rate increased, the apparent performance of the new diagnostic test declined, consistent with the earlier simulation studies shown in Figs <xref ref-type="fig" rid="pone.0217146.g002">2</xref>–<xref ref-type="fig" rid="pone.0217146.g006">6</xref>. Specifically, as the comparator noise level was increased, there was a corresponding decrease in AUC. When expressed in terms of relative error (relative error = (1-AUC)/(1-AUC<sub>0</sub>) where AUC<sub>0</sub> = AUC at zero misclassification rate), we found that each 1% increase in comparator noise produced an approximately 9% increase in relative error. With the injection of 4.8% misclassifications into the Super-Unanimous comparator, the simulation contained as much noise as was observed in the Consensus subset of the actual clinical trial data. Similarly, with the injection of 7.5% misclassifications, the simulation contained as much noise as was observed for the Forced group, as shown in <xref rid="pone.0217146.t002" ref-type="table">Table 2</xref>. The comparison between simulated (predicted) and observed diagnostic test performance is shown in <xref rid="pone.0217146.t002" ref-type="table">Table 2</xref>.</p>
      <table-wrap id="pone.0217146.t002" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0217146.t002</object-id>
        <label>Table 2</label>
        <caption>
          <title>Testing the models: Simulated vs. observed effect of comparator noise on test performance.</title>
        </caption>
        <alternatives>
          <graphic id="pone.0217146.t002g" xlink:href="pone.0217146.t002"/>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="justify" rowspan="2" style="background-color:#FFFFFF" colspan="1">Parameter<break/></th>
                <th align="justify" colspan="2" style="background-color:#FFFFFF" rowspan="1">Super-Unanimous (N = 290)<break/>FP Rate = 0.0%<break/>FN Rate = 0.0%</th>
                <th align="justify" colspan="2" style="background-color:#FFFFFF" rowspan="1">Consensus (N = 410)<break/>FP rate = 4.9%<break/>FN rate = 4.7%</th>
                <th align="justify" colspan="2" style="background-color:#FFFFFF" rowspan="1">Forced (N = 447)<break/>FP rate = 6.1%<break/>FN rate = 9.0%</th>
              </tr>
              <tr>
                <th align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">No<break/>misclassifications<break/>injected into<break/>Super-Unanimous</th>
                <th align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">Observed<break/></th>
                <th align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">4.84%<break/>misclassifications<break/>injected into<break/>Super-Unanimous</th>
                <th align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">Observed<break/></th>
                <th align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">7.46%<break/>misclassifications<break/>injected into<break/>Super-Unanimous</th>
                <th align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">Observed<break/></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                  <bold>AUC</bold>
                </td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.887<break/>(0.848–0.926)</td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.887<break/>(0.848–0.926)</td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.839<break/>(0.805–0.863)</td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.852<break/>(0.814–0.890)</td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.815<break/>(0.777–0.846)</td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.818<break/>(0.778–0.858)</td>
              </tr>
              <tr>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                  <bold>Sensitivity</bold>
                  <break/>
                  <bold>/ PPA</bold>
                </td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.967<break/>(0.918–0.991)</td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.967<break/>(0.918–0.991)</td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.950<break/>(0.932–0.966)</td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.944<break/>(0.900–0.973)</td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.946<break/>(0.926–0.965)</td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.921<break/>(0.875–0.954)</td>
              </tr>
              <tr>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                  <bold>Specificity</bold>
                  <break/>
                  <bold>/ NPA</bold>
                </td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.339<break/>(0.269–0.415)</td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.339<break/>(0.269–0.415)</td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.322<break/>(0.307–0.337)</td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.352<break/>(0.291–0.418)</td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.313<break/>(0.297–0.329)</td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.347<break/>(0.287–0.410)</td>
              </tr>
              <tr>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                  <bold>PPV</bold>
                </td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.509<break/>(0.442–0.575)</td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.509<break/>(0.442–0.575)</td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.491<break/>(0.461–0.513)</td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.533<break/>(0.477–0.589)</td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.465<break/>(0.435–0.500)</td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.538<break/>(0.483–0.591)</td>
              </tr>
              <tr>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                  <bold>NPV</bold>
                </td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.936<break/>(0.843–0.982)</td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.936<break/>(0.843–0.982)</td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.903<break/>(0.871–0.935)</td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.890<break/>(0.807–0.946)</td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.903<break/>(0.871–0.935)</td>
                <td align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.842<break/>(0.756–0.907)</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
        <table-wrap-foot>
          <fn id="t002fn001">
            <p>Clinical trial data from Miller et al. [<xref rid="pone.0217146.ref025" ref-type="bibr">25</xref>] were used. The 95% confidence intervals are given within the parentheses. The following tradeoff for a binary test is reflected in the data: a high value of sensitivity/PPA will imply a low value of specificity/NPA.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>Of the 447 patients in the trial, 93 were diagnosed by consensus RPD as having pneumonia or lower respiratory tract infections (LRTI). With respect to the secondary diagnosis of sepsis vs. SIRS, very high levels of disagreement (uncertainty) by the expert panelists were found for this subset of patients. In only 45/93 (48%) of these cases did all three external panelists agree on the diagnosis of sepsis or SIRS. A further indication of the difficulty of diagnosing pneumonia/LRTI patients as having sepsis or SIRS came from an examination of the 37/447 patients classified as indeterminate by the consensus RPD of the three external panelists. Of these 37 patients, 20 (54%) were diagnosed with pneumonia/LRTI (<xref rid="pone.0217146.t003" ref-type="table">Table 3</xref>). The misclassification rates for this sub-population were calculated to be 17.5% FP, 13.7% FN, 14.4% overall.</p>
      <table-wrap id="pone.0217146.t003" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0217146.t003</object-id>
        <label>Table 3</label>
        <caption>
          <title>Classification of patients in a trial on a new sepsis diagnostic test.</title>
        </caption>
        <alternatives>
          <graphic id="pone.0217146.t003g" xlink:href="pone.0217146.t003"/>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="left" rowspan="1" colspan="1"> Comparator</th>
                <th align="left" rowspan="1" colspan="1">Condition</th>
                <th align="left" rowspan="1" colspan="1">SIRS</th>
                <th align="left" rowspan="1" colspan="1">Indeterminate</th>
                <th align="left" rowspan="1" colspan="1">Sepsis</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="2" colspan="1">
                  <bold>A. Unanimous RPD</bold>
                  <break/>
                  <bold>(N = 315)</bold>
                </td>
                <td align="left" rowspan="1" colspan="1">All other conditions </td>
                <td align="left" rowspan="1" colspan="1">169 (53.6%)</td>
                <td align="left" rowspan="1" colspan="1">0 (0.0%)</td>
                <td align="left" rowspan="1" colspan="1">100 (31.7%)</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Pneumonia / LRTI</td>
                <td align="left" rowspan="1" colspan="1">4 (1.3%)</td>
                <td align="left" rowspan="1" colspan="1">1 (0.3%)</td>
                <td align="left" rowspan="1" colspan="1">41 (13.0%)</td>
              </tr>
              <tr>
                <td align="left" rowspan="2" colspan="1">
                  <bold>B. Super-Unanimous RPD (N = 290)</bold>
                </td>
                <td align="left" rowspan="1" colspan="1">All other conditions </td>
                <td align="left" rowspan="1" colspan="1">168 (57.9%)</td>
                <td align="left" rowspan="1" colspan="1">0 (0.0%)</td>
                <td align="left" rowspan="1" colspan="1">90 (31.0%)</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Pneumonia / LRTI</td>
                <td align="left" rowspan="1" colspan="1">3 (1.0%)</td>
                <td align="left" rowspan="1" colspan="1">0 (0.0%)</td>
                <td align="left" rowspan="1" colspan="1">29 (10.0%)</td>
              </tr>
              <tr>
                <td align="left" rowspan="2" colspan="1">
                  <bold>C. Consensus RPD</bold>
                  <break/>
                  <bold>(N = 447)</bold>
                </td>
                <td align="left" rowspan="1" colspan="1">All other conditions </td>
                <td align="left" rowspan="1" colspan="1">221 (49.4%)</td>
                <td align="left" rowspan="1" colspan="1">17 (3.8%)</td>
                <td align="left" rowspan="1" colspan="1">116 (26.0%)</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Pneumonia / LRTI</td>
                <td align="left" rowspan="1" colspan="1">9 (2.0%)</td>
                <td align="left" rowspan="1" colspan="1">20 (4.5%)</td>
                <td align="left" rowspan="1" colspan="1">64 (14.3%)</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
        <table-wrap-foot>
          <fn id="t003fn001">
            <p>The trial is described in Miller et al. [<xref rid="pone.0217146.ref025" ref-type="bibr">25</xref>]. (A) Unanimous RPD, in which all three external panelists agreed on diagnosis of sepsis, SIRS, or indeterminate. (B) Super-Unanimous RPD, in which all three external panelists, and also the study investigators at the clinical site of origin, agreed on the diagnosis of sepsis, SIRS, or indeterminate. (C) Consensus RPD, in which two or all three external panelists agreed on diagnosis of sepsis, SIRS, or indeterminate.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>For the patients with pneumonia or LRTI, the level of uncertainty in the comparator was roughly double that of the non-pneumonia patients. The two groups also differed in the prevalence of sepsis (see <xref rid="pone.0217146.t003" ref-type="table">Table 3</xref>, p &lt;0.001), the pattern of physician discordance in classification, and the distribution of SeptiCyte LAB test scores. <xref ref-type="fig" rid="pone.0217146.g008">Fig 8</xref>, Panel A shows a subset of super-unanimously classified sepsis and SIRS patients from the complete trial population, selected to match the sepsis prevalence and test score distribution of pneumonia/LRTI patients. <xref ref-type="fig" rid="pone.0217146.g008">Fig 8</xref>, Panel B shows the effects of increasing the comparator misclassification rate on the performance parameters for this patient subset. The triangles in this panel signify the parameter values observed for the pneumonia/LRTI patients. The triangles have been placed at a position along the x-axis (17.5% FPR, 13.7% FNR) that is appropriate for the pneumonia/LRTI patient group, as inferred from the measured discordance in the comparator diagnoses for this group (see <xref ref-type="supplementary-material" rid="pone.0217146.s002">S2</xref>–<xref ref-type="supplementary-material" rid="pone.0217146.s004">S4</xref> Supporting Information).</p>
      <fig id="pone.0217146.g008" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0217146.g008</object-id>
        <label>Fig 8</label>
        <caption>
          <p>(A) Subset of pneumonia/LRTI-specific data (N = 93) from a clinical trial for a new sepsis diagnostic test, conducted over 8 sites in the USA and Netherlands [<xref rid="pone.0217146.ref025" ref-type="bibr">25</xref>]. (B) The apparent performance of the test (y axis) decreases as uncertainty is introduced into the comparator (x axis). 95% confidence intervals are shown. The difference between the apparent test performance at a given comparator misclassification rate and at a comparator misclassification rate of zero indicates the degree of underestimation of true test performance due to uncertainty in the comparator. Solid triangles show the observed measurements for the trial for each of these groups without correction for comparator uncertainty. Misclassification rates are based on quantifying the discordance between independent expert opinions. Sensitivity/PPA and Specificity/NPA are each marked with an asterisk (*) to emphasize that these measures assume no misclassification in the comparator. Positive Percent Agreement (PPA) and Negative Percent Agreement (NPA) are the correct terms to use, when the comparator is known to contain uncertainty as in this case.</p>
        </caption>
        <graphic xlink:href="pone.0217146.g008"/>
      </fig>
      <p>Regarding the 45 pneumonia/LRTI patients who were diagnosed by all three expert panelists to be either positive or negative for sepsis, we observed no significant difference in test performance between this patient subset and the subset of all patients with Super-Unanimous diagnoses across the trial as a whole (<xref rid="pone.0217146.t003" ref-type="table">Table 3</xref>). Simulations in which a weighted average 14.4% misclassification rate for pneumonia/LRTI patients was introduced into the Super-Unanimous RPD comparator led to predicted underestimates of performance that were well aligned with the observed measurements of test performance (<xref ref-type="fig" rid="pone.0217146.g008">Fig 8</xref>, Panel B, triangles and <xref rid="pone.0217146.t004" ref-type="table">Table 4</xref>).</p>
      <table-wrap id="pone.0217146.t004" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0217146.t004</object-id>
        <label>Table 4</label>
        <caption>
          <title>Demonstration of the effect of comparator uncertainty on estimates of test performance, for the pneumonia/LRTI patient subset.</title>
        </caption>
        <alternatives>
          <graphic id="pone.0217146.t004g" xlink:href="pone.0217146.t004"/>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="justify" rowspan="2" colspan="1"> Parameter</th>
                <th align="justify" colspan="2" rowspan="1">Minimal uncertainty in<break/>comparator classification</th>
                <th align="justify" colspan="2" rowspan="1">17.5% False Positive rate<break/>13.7% False Negative rate<break/>(due to comparator uncertainty)</th>
              </tr>
              <tr>
                <th align="left" rowspan="1" colspan="1">A. All conditions<break/>(N = 290)</th>
                <th align="left" rowspan="1" colspan="1">B. Pneumonia /LRTI<break/>(N = 45)</th>
                <th align="left" rowspan="1" colspan="1">C. Introduced to<break/>Col A. (N = 290)</th>
                <th align="left" rowspan="1" colspan="1">D. Observed in<break/>pneumonia/LRTI<break/>(N = 45)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="justify" rowspan="1" colspan="1">
                  <bold>AUC</bold>
                </td>
                <td align="justify" rowspan="1" colspan="1">0.887<break/>(0.848–0.926)</td>
                <td align="justify" rowspan="1" colspan="1">0.87<break/>(0.71–1.00)</td>
                <td align="justify" rowspan="1" colspan="1">0.679<break/>(0.576–0.723)</td>
                <td align="justify" rowspan="1" colspan="1">0.67<break/>(0.64–0.72)</td>
              </tr>
              <tr>
                <td align="justify" rowspan="1" colspan="1">
                  <bold>Sensitivity/PPA</bold>
                </td>
                <td align="justify" rowspan="1" colspan="1">0.967<break/>(0.918–0.991)</td>
                <td align="justify" rowspan="1" colspan="1">0.97<break/>(0.84–1.00)</td>
                <td align="justify" rowspan="1" colspan="1">0.909<break/>(0.783–0.975)</td>
                <td align="justify" rowspan="1" colspan="1">0.89<break/>(0.85–0.92)</td>
              </tr>
              <tr>
                <td align="justify" rowspan="1" colspan="1">
                  <bold>Specificity/NPA</bold>
                </td>
                <td align="justify" rowspan="1" colspan="1">0.339<break/>(0.269–0.415)</td>
                <td align="justify" rowspan="1" colspan="1">0<break/>(0.00–0.52)</td>
                <td align="justify" rowspan="1" colspan="1">0.274<break/>(0.169–0.402)</td>
                <td align="justify" rowspan="1" colspan="1">0.28<break/>(0.25–0.30)</td>
              </tr>
              <tr>
                <td align="justify" rowspan="1" colspan="1">
                  <bold>PPV</bold>
                </td>
                <td align="justify" rowspan="1" colspan="1">0.509<break/>(0.442–0.575)</td>
                <td align="justify" rowspan="1" colspan="1">0.86<break/>(0.71–0.96)</td>
                <td align="justify" rowspan="1" colspan="1">0.471<break/>(0.361–0.582)</td>
                <td align="justify" rowspan="1" colspan="1">0.46<break/>(0.42–0.50)</td>
              </tr>
              <tr>
                <td align="justify" rowspan="1" colspan="1">
                  <bold>NPV</bold>
                </td>
                <td align="justify" rowspan="1" colspan="1">0.935<break/>(0.843–0.982)</td>
                <td align="justify" rowspan="1" colspan="1">0<break/>(0.00–0.98)</td>
                <td align="justify" rowspan="1" colspan="1">0.810<break/>(0.581–0.946)</td>
                <td align="justify" rowspan="1" colspan="1">0.79<break/>(0.71–0.85)</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
        <table-wrap-foot>
          <fn id="t004fn001">
            <p>The clinical trial of a new sepsis test is described in the publication of Miller et al. [<xref rid="pone.0217146.ref025" ref-type="bibr">25</xref>]. Column A: Test performance for the Super-Unanimous subset of patients (N = 290) who received unanimous diagnoses of sepsis or SIRS by three external expert panelists, and who were also assigned the same diagnosis by the study investigators at the clinical sites where the patients originated. Column B: Test performance for the subset of pneumonia/LRTI patients from column A. Column C: Based on the uncertainty distribution observed in the trial, a comparator misclassification rate of 17.54% FP, 13.73% FN, 14.4% overall was estimated for the pneumonia/LRTI patient subset, and introduced into the simulation. The simulation was repeated 100 times with median and 95% CIs indicated. Column D: Observed performance metrics for the pneumonia / LRTI subset of the Super-Unanimous population (Column A). The following tradeoff for a binary test is reflected in the data: a high value of sensitivity/PPA will imply a low value of specificity/NPA.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec id="sec017">
      <title>The unattainability of a perfect test</title>
      <p>Additional simulations showed that it is extremely unlikely for any test, even a perfect test, to achieve very high performance in a diagnostic evaluation trial, when even a small amount of uncertainty is present in the comparator against which the test is being evaluated. For example, as shown in <xref ref-type="supplementary-material" rid="pone.0217146.s007">S7 Supporting Information</xref> (“Very high performance tests”), if 99% PPA (sensitivity) or NPA (specificity) is required in a diagnostic evaluation trial, then a modest 5% patient misclassification rate in the comparator will lead to rejection of the perfect diagnostic test with a probability greater than 99.999%. Thus a specific numerical requirement for test performance, especially a very high performance requirement such as 99% PPA (sensitivity), can only be meaningfully discussed if all classification uncertainty in a trial is either ruled out or characterized, and the measured test performance interpreted with respect to the theoretical limits imposed by comparator uncertainty.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec018">
    <title>Discussion</title>
    <p>The performance of any diagnostic test must be evaluated in reference to a comparator. The presence of noise (classification uncertainty) in the comparator is therefore an important confounding factor to consider in the interpretation of the performance of diagnostic tests. As increasing amounts of noise are introduced into the comparator, the apparent performance of the diagnostic test (relative to the comparator) will decline in a concomitant fashion. This is consistent with the general expectation that the addition of randomness into a method for analyzing a test’s performance should drag any performance indicator down towards a limiting minimum value (for example 0.50 for AUC). While this confounding factor was first identified over 30 years ago [<xref rid="pone.0217146.ref012" ref-type="bibr">12</xref>], it has received renewed attention in the 2015 STARD standard for diagnostic reporting [<xref rid="pone.0217146.ref032" ref-type="bibr">32</xref>,<xref rid="pone.0217146.ref033" ref-type="bibr">33</xref>] under checklist item #15 (“How indeterminate index test or comparator results were handled”).</p>
    <p>In the analysis of real world data from a clinical trial on a new sepsis test [<xref rid="pone.0217146.ref025" ref-type="bibr">25</xref>], significant differences in apparent test performance were measured in different patient subsets. One interpretation is that the actual performance of the test varied, depending on the patient subset being considered. Although genuine differences in test performance may indeed exist between patient subsets, an alternative explanation is suggested by two key results: 1) the correspondence between the observed performance and the predicted performance as a function of comparator noise level; 2) the lack of a significant difference in test performance between patient subsets, when comparator noise was removed by excluding patients with uncertainty in the comparator diagnosis. These results support the hypothesis that the diagnostic test performed equally well across all patient subsets, but varying levels of comparator noise led to the appearance of diagnostic test performance differences.</p>
    <p>Our simulations show, perhaps counter-intuitively, that increasing uncertainty in the comparator can sometimes have a non-linear effect on performance (as measured by different performance indicators such as AUC, PPA, NPA). The non-linearity is most pronounced when disease prevalence is either high or low (see for example <xref ref-type="fig" rid="pone.0217146.g006">Fig 6</xref>). This non-linearity increases the difficulty of intuiting the effect of uncertainty on evaluating diagnostic performance, because linear extrapolation under these conditions may lead to erroneous conclusions. The use of a simulation tool, such as the one associated with this paper, may be the only realistic way for non-statistical experts to verify that the level of uncertainty measured in their trial does not invalidate their conclusions.</p>
    <p>An upper bound on the apparent performance of a test will be imposed by the presence of noise in the comparator. For example, consider a misclassification rate of 10% in a comparator. This would impose a maximum theoretical AUC of 0.90 (95% CI 0.86–0.94) for any test, including a perfect test, that is measured against this comparator, as shown in <xref ref-type="fig" rid="pone.0217146.g003">Fig 3</xref>. If using this comparator a new diagnostic test measures an apparent AUC of 0.89 (95% CI 0.87–0.91), then the test would be statistically indistinguishable from perfect, and should be reported as such. To report an AUC of 0.89 for this new test without this important qualifier would be misleading. The simulation tool associated with this paper can be used to generate theoretical maximum limits on performance estimates under conditions of comparator uncertainty, against which trial results can be assessed in the context of their proximity to theoretical perfection under these conditions.</p>
    <p>Our simulations also reveal that for tests requiring high accuracy, the presence of uncertainty in the comparator will be highly detrimental. For example, degradation of apparent test performance (AUC) from a true performance level of 0.97 AUC to a measured performance level of 0.93 AUC, while only -0.04 AUC units in absolute terms, in fact represents more than a doubling of the apparent error rate of the test (from 3% to 7%), due solely to the comparator noise effect. This could lead to a decision not to adopt a test in the clinic because the test performance is (erroneously) assumed to be inadequate.</p>
    <p>For tests requiring even higher accuracy, for example 99% sensitivity or negative predictive value, extreme caution must be exercised in trial interpretation if even small amounts of uncertainty may be present in the comparator. In these cases, an apparently reasonable requirement for robust test performance will result in the rejection of even a perfect test, in almost all cases, due to failure to account for the effects demonstrated in this paper. Stakeholders interested in ensuring very high performance (i.e. 99% sensitivity or NPV) must bear in mind that such high performance characteristics can only be practically demonstrated with respect to a nearly flawless comparator method. In the absence of a nearly flawless comparator method, it will not be possible to validate such high test performance characteristics, and attempts to do so will likely result in underestimation of candidate test performance. The impact of an imperfect comparator on very high performance tests is analyzed quantitatively in <xref ref-type="supplementary-material" rid="pone.0217146.s007">S7 Supporting Information</xref> (“Very high performance tests”).</p>
    <p>Generally, it can be seen from both the simulations and our actual data that a 5% or greater misclassification rate in the comparator may result in significant underestimates of test performance, which could in turn have significant consequences, e.g. in a clinical trial. If there is reason to suspect a misclassification rate above this limit, it is advisable (in accord with STARD criterion #15) to report the comparator uncertainty together with the estimated performance of the new test. The estimated test performance should be reported relative to the expected performance of a perfect test under the prevailing conditions of uncertainty, which can be estimated with the simulation tool that accompanies this paper. At minimum, the amount of comparator uncertainty should be measured or described so that its effect can be bounded or incorporated into the interpretation of the data. Without taking classification uncertainty into account, researchers risk arriving at false or biased conclusions.</p>
  </sec>
  <sec sec-type="conclusions" id="sec019">
    <title>Conclusions</title>
    <p>This study has shown, with both simulated and real data, that noise (classification uncertainty) in a comparator will exert a significant downward effect on the apparent performance of any new test under evaluation. The common condition of uncertainty in clinical trial classifications, combined with the knowledge that very few such trials measure and account for this uncertainty, could potentially undermine the conclusions drawn from such trials, if the effect is not corrected for. If there is reason to suspect a significant amount of classification uncertainty in the comparator, it is advisable to report the comparator uncertainty together with the estimated performance of the new test. We provide an online simulation tool to allow researchers to explore the effect of comparator noise, using their own trial parameters: <ext-link ext-link-type="uri" xlink:href="https://imperfect-gold-standard.shinyapps.io/classification-noise/">https://imperfect-gold-standard.shinyapps.io/classification-noise/</ext-link>. The source code for the simulation tool has been made publicly available: <ext-link ext-link-type="uri" xlink:href="https://github.com/ksny/Imperfect-Gold-Standard">https://github.com/ksny/Imperfect-Gold-Standard</ext-link>.</p>
  </sec>
  <sec sec-type="supplementary-material" id="sec020">
    <title>Supporting information</title>
    <supplementary-material content-type="local-data" id="pone.0217146.s001">
      <label>S1 Supporting Information</label>
      <caption>
        <title>Example of reference bias.</title>
        <p>The example shown here leads to over-estimation of test performance.</p>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0217146.s001.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pone.0217146.s002">
      <label>S2 Supporting Information</label>
      <caption>
        <title>Method to estimate the confidence of patient classifications by an expert panel comparator.</title>
        <p>In the clinical trial described by Miller et al. [<xref rid="pone.0217146.ref025" ref-type="bibr">25</xref>], three clinicians provided independent patient diagnoses while blinded to the diagnoses of each other. For an individual clinician’s diagnosis regarding the presence of systemic infection in an individual patient, a classification of ‘No’ carried a probability of systemic infection of zero, ‘Yes’ carried a probability of one, and ‘Indeterminate’ carried a probability of one half. The overall infection probability was calculated as a simple average of the three input values.</p>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0217146.s002.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pone.0217146.s003">
      <label>S3 Supporting Information</label>
      <caption>
        <title>Weighting for introduced misclassification events.</title>
        <p>Using the dataset of Miller et al. [<xref rid="pone.0217146.ref025" ref-type="bibr">25</xref>] and the method described in S2 Supporting information, each patient receives a probabilistic assessment of his or her systemic infection status. Samples are then selected, on the basis of the observed uncertainty distribution. This selection process reflects the expectation that patients with more certainty in classification are less likely to be misclassified. The selected samples are then relabeled with the opposite status indicator.</p>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0217146.s003.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pone.0217146.s004">
      <label>S4 Supporting Information</label>
      <caption>
        <title>Calculating misclassification rates, based on patient confidence values.</title>
        <p>The overall expected total misclassification rate is the FP rate applied to the negative patients plus the FN rate applied to the positive patients.</p>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0217146.s004.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pone.0217146.s005">
      <label>S5 Supporting Information</label>
      <caption>
        <title>Decrease in apparent performance of index test, with 5% noise injected into comparator.</title>
        <p>A set of simulation runs was conducted to further explore the effect, on apparent performance of an index test, of 5% noise injected into the comparator.</p>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0217146.s005.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pone.0217146.s006">
      <label>S6 Supporting Information</label>
      <caption>
        <title>Unequal FP and FN rates.</title>
        <p>The examples show degradation of apparent performance of a diagnostic test as a function of noise in the comparator, when FP and FN rates are not equal.</p>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0217146.s006.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pone.0217146.s007">
      <label>S7 Supporting Information</label>
      <caption>
        <title>Very high performance tests.</title>
        <p>The analysis shows that the goal of achieving a very high diagnostic test performance, such as a 99% sensitivity, can be rendered nearly unreachable, if the comparator diagnosis contains even small amounts of uncertainty.</p>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0217146.s007.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack>
    <p>The authors acknowledge helpful comments from Marina Kondratovich, Associate Director for Clinical Studies, Personalized Medicine, Center for Devices and Radiological Health, United States Food and Drug Administration. We also thank Dr. Rollie Carlson for a critical review of the manuscript.</p>
  </ack>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>ROC</term>
        <def>
          <p>receiver operating characteristic</p>
        </def>
      </def-item>
      <def-item>
        <term>AUC</term>
        <def>
          <p>area under ROC curve</p>
        </def>
      </def-item>
      <def-item>
        <term>CI</term>
        <def>
          <p>confidence interval</p>
        </def>
      </def-item>
      <def-item>
        <term>LRTI</term>
        <def>
          <p>lower respiratory tract infection</p>
        </def>
      </def-item>
      <def-item>
        <term>NPA</term>
        <def>
          <p>negative percent agreement</p>
        </def>
      </def-item>
      <def-item>
        <term>NPV</term>
        <def>
          <p>negative predictive value</p>
        </def>
      </def-item>
      <def-item>
        <term>PPA</term>
        <def>
          <p>positive percent agreement</p>
        </def>
      </def-item>
      <def-item>
        <term>PPV</term>
        <def>
          <p>positive predictive value</p>
        </def>
      </def-item>
      <def-item>
        <term>ROC</term>
        <def>
          <p>receiver operating characteristic</p>
        </def>
      </def-item>
      <def-item>
        <term>RPD</term>
        <def>
          <p>retrospective physician diagnosis</p>
        </def>
      </def-item>
      <def-item>
        <term>SIRS</term>
        <def>
          <p>systemic inflammatory response syndrome</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <ref-list>
    <title>References</title>
    <ref id="pone.0217146.ref001">
      <label>1</label>
      <mixed-citation publication-type="journal"><name><surname>Waikar</surname><given-names>SS</given-names></name>, <name><surname>Betensky</surname><given-names>RA</given-names></name>, <name><surname>Emerson</surname><given-names>SC</given-names></name>, <name><surname>Bonventre</surname><given-names>JV</given-names></name>. <article-title>Imperfect gold standards for kidney injury biomarker evaluation.</article-title><source>J Am Soc Nephrol</source>. <year>2012</year>;<volume>23</volume>:<fpage>13</fpage>–<lpage>21</lpage>. <pub-id pub-id-type="doi">10.1681/ASN.2010111124</pub-id><?supplied-pmid 22021710?><pub-id pub-id-type="pmid">22021710</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref002">
      <label>2</label>
      <mixed-citation publication-type="journal"><name><surname>Rothwell</surname><given-names>PM</given-names></name>. <article-title>Limitations of the usual blood-pressure hypothesis and importance of variability, instability, and episodic hypertension</article-title>. <source>Lancet</source><year>2010</year>;<volume>375</volume>:<fpage>938</fpage>–<lpage>948</lpage>. <pub-id pub-id-type="doi">10.1016/S0140-6736(10)60309-1</pub-id><?supplied-pmid 20226991?><pub-id pub-id-type="pmid">20226991</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref003">
      <label>3</label>
      <mixed-citation publication-type="journal"><name><surname>Cherian</surname><given-names>T</given-names></name>, <name><surname>Mulholland</surname><given-names>EK</given-names></name>, <name><surname>Carlin</surname><given-names>JB</given-names></name>, <name><surname>Ostensen</surname><given-names>H</given-names></name>, <name><surname>Amin</surname><given-names>R</given-names></name>, <name><surname>de Campo</surname><given-names>M</given-names></name>, <etal>et al</etal><article-title>Standardized interpretation of paediatric chest radiographs for the diagnosis of pneumonia in epidemiological studies.</article-title><source>Bull World Health Organ</source>. <year>2005</year>;<volume>83</volume>:<fpage>353</fpage>–<lpage>359</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/S0042-96862005000500011">/S0042-96862005000500011</ext-link></comment><?supplied-pmid 15976876?><pub-id pub-id-type="pmid">15976876</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref004">
      <label>4</label>
      <mixed-citation publication-type="journal"><name><surname>Omurtag</surname><given-names>A</given-names></name>, <name><surname>Fenton</surname><given-names>AA</given-names></name>. <article-title>Assessing diagnostic tests: how to correct for the combined effects of interpretation and reference standard</article-title>. <source>PLOS One</source><year>2012</year>;<volume>7</volume>:<fpage>e52221</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0052221</pub-id><?supplied-pmid 23300619?><pub-id pub-id-type="pmid">23300619</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref005">
      <label>5</label>
      <mixed-citation publication-type="journal"><name><surname>Toft</surname><given-names>N</given-names></name>, <name><surname>Jørgensen</surname><given-names>E</given-names></name>, <name><surname>Højsgaard</surname><given-names>S</given-names></name>. <article-title>Diagnosing diagnostic tests: evaluating the assumptions underlying the estimation of sensitivity and specificity in the absence of a gold standard.</article-title><source>Prev Vet Med</source>. <year>2005</year>;<volume>68</volume>:<fpage>19</fpage>–<lpage>33</lpage>. <pub-id pub-id-type="doi">10.1016/j.prevetmed.2005.01.006</pub-id><?supplied-pmid 15795013?><pub-id pub-id-type="pmid">15795013</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref006">
      <label>6</label>
      <mixed-citation publication-type="journal"><name><surname>Hui</surname><given-names>SL</given-names></name>, <name><surname>Zhou</surname><given-names>XH</given-names></name>. <article-title>Evaluation of diagnostic tests without gold standards</article-title>. <source>Stat Methods Med Res</source>. <year>1998</year>;<volume>7</volume>:<fpage>354</fpage>–<lpage>370</lpage>. <pub-id pub-id-type="doi">10.1177/096228029800700404</pub-id><?supplied-pmid 9871952?><pub-id pub-id-type="pmid">9871952</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref007">
      <label>7</label>
      <mixed-citation publication-type="journal"><name><surname>McHugh</surname><given-names>LC</given-names></name>, <name><surname>Yager</surname><given-names>TD</given-names></name>. <article-title>Don’t blame the messenger: correcting for uncertainty in diagnosis when reporting sepsis diagnostic results</article-title>. <source>Intensive Care Med Exp</source>. <year>2017</year>;<volume>5</volume>(<issue>Suppl 1</issue>):<fpage>P34</fpage><pub-id pub-id-type="doi">10.1186/s40635-017-0149-y</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref008">
      <label>8</label>
      <mixed-citation publication-type="journal"><name><surname>Fox</surname><given-names>MP</given-names></name>, <name><surname>Lash</surname><given-names>TL</given-names></name>, <name><surname>Greenland</surname><given-names>S</given-names></name>. <article-title>A method to automate probabilistic sensitivity analyses of misclassified binary variables</article-title>. <source>Int J Epidemiol</source>. <year>2005</year>;<volume>34</volume>:<fpage>1370</fpage>–<lpage>1376</lpage>. <pub-id pub-id-type="doi">10.1093/ije/dyi184</pub-id><?supplied-pmid 16172102?><pub-id pub-id-type="pmid">16172102</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref009">
      <label>9</label>
      <mixed-citation publication-type="journal"><name><surname>Betebenner</surname><given-names>DW</given-names></name>, <name><surname>Shang</surname><given-names>Y</given-names></name>, <name><surname>Xiang</surname><given-names>Y</given-names></name>, <name><surname>Zhao</surname><given-names>Y</given-names></name>, <name><surname>Yue</surname><given-names>X</given-names></name>. <article-title>The impact of performance level misclassification on the accuracy and precision of percent at performance level measures</article-title>. <source>J Educ Meas</source>. <year>2008</year>; <volume>45</volume>:<fpage>119</fpage>–<lpage>137</lpage>. <pub-id pub-id-type="doi">10.1111/j.1745-3984.2007.00056.x</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref010">
      <label>10</label>
      <mixed-citation publication-type="book"><name><surname>Porta</surname><given-names>M.</given-names></name><source>A Dictionary of Epidemiology</source>, <edition>6th Edn</edition><publisher-name>Oxford University Press</publisher-name>, <year>2014</year>.</mixed-citation>
    </ref>
    <ref id="pone.0217146.ref011">
      <label>11</label>
      <mixed-citation publication-type="journal"><name><surname>Bross</surname><given-names>I.</given-names></name><article-title>Misclassification in 2 X 2 tables</article-title>. <source>Biometrics</source><year>1954</year>;<volume>10</volume>:<fpage>478</fpage>–<lpage>486</lpage>. <pub-id pub-id-type="doi">10.2307/3001619</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref012">
      <label>12</label>
      <mixed-citation publication-type="journal"><name><surname>Begg</surname><given-names>CG</given-names></name>. <article-title>Biases in the assessment of diagnostic tests</article-title>. <source>Stat Med</source>. <year>1987</year>;<volume>6</volume>: <fpage>411</fpage>–<lpage>423</lpage>. <pub-id pub-id-type="doi">10.1002/sim.4780060402</pub-id><?supplied-pmid 3114858?><pub-id pub-id-type="pmid">3114858</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref013">
      <label>13</label>
      <mixed-citation publication-type="journal"><name><surname>Boyko</surname><given-names>EJ</given-names></name>, <name><surname>Alderman</surname><given-names>BW</given-names></name>, <name><surname>Baron</surname><given-names>AE</given-names></name>. <article-title>Comparator test errors bias the evaluation of diagnostic tests for ischemic heart disease</article-title>. <source>J Gen Intern Med</source>. <year>1988</year>;<volume>3</volume>:<fpage>476</fpage>–<lpage>481</lpage>. <pub-id pub-id-type="doi">10.1007/BF02595925</pub-id><?supplied-pmid 3049969?><pub-id pub-id-type="pmid">3049969</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref014">
      <label>14</label>
      <mixed-citation publication-type="journal"><name><surname>Valenstein</surname><given-names>PN</given-names></name>. <article-title>Evaluating diagnostic tests with imperfect standards</article-title>. <source>Am J Clin Pathol</source>. <year>1990</year>;<volume>93</volume>:<fpage>252</fpage>–<lpage>258</lpage>. <pub-id pub-id-type="doi">10.1093/ajcp/93.2.252</pub-id><?supplied-pmid 2405632?><pub-id pub-id-type="pmid">2405632</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref015">
      <label>15</label>
      <mixed-citation publication-type="journal"><name><surname>Hawkins</surname><given-names>DM</given-names></name>, <name><surname>Garrett</surname><given-names>JA</given-names></name>, <name><surname>Stephenson</surname><given-names>B</given-names></name>. <article-title>Some issues in resolution of diagnostic tests using an imperfect gold standard</article-title>. <source>Stat Med</source>. <year>2001</year>;<volume>20</volume>:<fpage>1987</fpage>–<lpage>2001</lpage>. <pub-id pub-id-type="doi">10.1002/sim.819</pub-id><?supplied-pmid 11427955?><pub-id pub-id-type="pmid">11427955</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref016">
      <label>16</label>
      <mixed-citation publication-type="journal"><name><surname>Reitsma</surname><given-names>JB</given-names></name>, <name><surname>Rutjes</surname><given-names>AW</given-names></name>, <name><surname>Khan</surname><given-names>KS</given-names></name>, <name><surname>Coomarasamy</surname><given-names>A</given-names></name>, <name><surname>Bossuyt</surname><given-names>PM</given-names></name>. <article-title>A review of solutions for diagnostic accuracy studies with an imperfect or missing comparator standard</article-title>. <source>J Clin Epidemiol</source>. <year>2009</year>;<volume>62</volume>:<fpage>797</fpage>–<lpage>806</lpage>. <pub-id pub-id-type="doi">10.1016/j.jclinepi.2009.02.005</pub-id><?supplied-pmid 19447581?><pub-id pub-id-type="pmid">19447581</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref017">
      <label>17</label>
      <mixed-citation publication-type="journal"><name><surname>Bachmann</surname><given-names>LM</given-names></name>, <name><surname>Jüni</surname><given-names>P</given-names></name>, <name><surname>Reichenbach</surname><given-names>S</given-names></name>, <name><surname>Ziswiler</surname><given-names>HR</given-names></name>, <name><surname>Kessels</surname><given-names>AG</given-names></name>, <name><surname>Vögelin</surname><given-names>E</given-names></name>. <article-title>Consequences of different diagnostic "gold standards" in test accuracy research: Carpal Tunnel Syndrome as an example</article-title>. <source>Int J Epidemiol</source>. <year>2005</year>;<volume>34</volume>:<fpage>953</fpage>–<lpage>955</lpage>. <pub-id pub-id-type="doi">10.1093/ije/dyi105</pub-id><?supplied-pmid 15911545?><pub-id pub-id-type="pmid">15911545</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref018">
      <label>18</label>
      <mixed-citation publication-type="journal"><name><surname>Waikar</surname><given-names>SS</given-names></name>, <name><surname>Betensky</surname><given-names>RA</given-names></name>, <name><surname>Emerson</surname><given-names>SC</given-names></name>, <name><surname>Bonventre</surname><given-names>JV</given-names></name>. <article-title>Imperfect Gold Standards for Biomarker Evaluation</article-title>. <source>Clin Trials</source>. <year>2013</year>;<volume>10</volume>:<fpage>696</fpage>–<lpage>700</lpage>. <pub-id pub-id-type="doi">10.1177/1740774513497540</pub-id><?supplied-pmid 24006246?><pub-id pub-id-type="pmid">24006246</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref019">
      <label>19</label>
      <mixed-citation publication-type="journal"><name><surname>Limmathurotsakul</surname><given-names>D</given-names></name>, <name><surname>Turner</surname><given-names>EL</given-names></name>, <name><surname>Wuthiekanun</surname><given-names>V</given-names></name>, <name><surname>Thaipadungpanit</surname><given-names>J</given-names></name>, <name><surname>Suputtamongkol</surname><given-names>Y</given-names></name>, <name><surname>Chierakul</surname><given-names>W</given-names></name>, <etal>et al</etal><article-title>Fool's gold: Why imperfect comparator tests are undermining the evaluation of novel diagnostics: a reevaluation of 5 diagnostic tests for leptospirosis</article-title>. <source>Clin Infect Dis</source>. <year>2012</year>;<volume>55</volume>:<fpage>322</fpage>–<lpage>331</lpage>. <pub-id pub-id-type="doi">10.1093/cid/cis403</pub-id><?supplied-pmid 22523263?><pub-id pub-id-type="pmid">22523263</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref020">
      <label>20</label>
      <mixed-citation publication-type="journal"><name><surname>Bond</surname><given-names>RR</given-names></name>, <name><surname>Novotny</surname><given-names>T</given-names></name>, <name><surname>Andrsova</surname><given-names>I</given-names></name>, <name><surname>Koc</surname><given-names>L</given-names></name>, <name><surname>Sisakova</surname><given-names>M</given-names></name>, <name><surname>Finlay</surname><given-names>D</given-names></name>, <etal>et al</etal><article-title>Automation bias in medicine: The influence of automated diagnoses on interpreter accuracy and uncertainty when reading electrocardiograms</article-title>. <source>J Electrocardiol</source>. <year>2018</year>;<volume>51</volume>(<issue>6S</issue>):<fpage>S6</fpage>–<lpage>S11</lpage>. <pub-id pub-id-type="doi">10.1016/j.jelectrocard.2018.08.007</pub-id><?supplied-pmid 30122457?><pub-id pub-id-type="pmid">30122457</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref021">
      <label>21</label>
      <mixed-citation publication-type="journal"><name><surname>Green</surname><given-names>TA</given-names></name>, <name><surname>Black</surname><given-names>CM</given-names></name>, <name><surname>Johnson</surname><given-names>RE</given-names></name>. <article-title>Evaluation of bias in diagnostic-test sensitivity and specificity estimates computed by discrepant analysis</article-title>. <source>J Clin Microbiol</source>. <year>1998</year>;<volume>36</volume>:<fpage>375</fpage>–<lpage>381</lpage>. <?supplied-pmid 9466744?><pub-id pub-id-type="pmid">9466744</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref022">
      <label>22</label>
      <mixed-citation publication-type="journal"><name><surname>Brody</surname><given-names>JP</given-names></name>, <name><surname>Williams</surname><given-names>BA</given-names></name>, <name><surname>Wold</surname><given-names>BJ</given-names></name>, <name><surname>Quake</surname><given-names>SR</given-names></name>. <article-title>Significance and statistical errors in the analysis of DNA microarray data</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2002</year>;<volume>99</volume>:<fpage>12975</fpage>–<lpage>12978</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.162468199</pub-id><?supplied-pmid 12235357?><pub-id pub-id-type="pmid">12235357</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref023">
      <label>23</label>
      <mixed-citation publication-type="journal"><name><surname>Bailey</surname><given-names>DC</given-names></name>. <article-title>Not Normal: the uncertainties of scientific measurements</article-title>. <source>R Soc Open Sci</source>. <year>2017</year>;<volume>4</volume>:<fpage>160600</fpage><pub-id pub-id-type="doi">10.1098/rsos.160600</pub-id><?supplied-pmid 28280557?><pub-id pub-id-type="pmid">28280557</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref024">
      <label>24</label>
      <mixed-citation publication-type="journal"><name><surname>Popovici</surname><given-names>V</given-names></name>, <name><surname>Chen</surname><given-names>W</given-names></name>, <name><surname>Gallas</surname><given-names>BG</given-names></name>, <name><surname>Hatzis</surname><given-names>C</given-names></name>, <name><surname>Shi</surname><given-names>W</given-names></name>, <name><surname>Samuelson</surname><given-names>FW</given-names></name>, <etal>et al</etal><article-title>Effect of training-sample size and classification difficulty on the accuracy of genomic predictors</article-title>. <source>Breast Cancer Res</source>. <year>2010</year>;<volume>12</volume>(<issue>1</issue>):<fpage>R5</fpage><pub-id pub-id-type="doi">10.1186/bcr2468</pub-id><?supplied-pmid 20064235?><pub-id pub-id-type="pmid">20064235</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref025">
      <label>25</label>
      <mixed-citation publication-type="journal"><name><surname>Miller</surname><given-names>RR</given-names><suffix>III</suffix></name>, <name><surname>Lopansri</surname><given-names>BK</given-names></name>, <name><surname>Burke</surname><given-names>JP</given-names></name>, <name><surname>Levy</surname><given-names>M</given-names></name>, <name><surname>Opal</surname><given-names>S</given-names></name>, <name><surname>Rothman</surname><given-names>RE</given-names></name>, <etal>et al</etal><article-title>Validation of a Host Response Assay, Septicyte LAB, for Discriminating Sepsis from SIRS in the ICU</article-title>. <source>Am J Respir Crit Care Med</source>. <year>2018</year>;<volume>198</volume>:<fpage>903</fpage>–<lpage>913</lpage>. <pub-id pub-id-type="doi">10.1164/rccm.201712-2472OC</pub-id><?supplied-pmid 29624409?><pub-id pub-id-type="pmid">29624409</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref026">
      <label>26</label>
      <mixed-citation publication-type="journal"><name><surname>Daviaud</surname><given-names>J</given-names></name>, <name><surname>Fournet</surname><given-names>D</given-names></name>, <name><surname>Ballongue</surname><given-names>C</given-names></name>, <name><surname>Guillem</surname><given-names>GP</given-names></name>, <name><surname>Leblanc</surname><given-names>A</given-names></name>, <name><surname>Casellas</surname><given-names>C</given-names></name>, <etal>et al</etal><article-title>Reliability and feasibility of pregnancy home-use tests: laboratory validation and diagnostic evaluation by 638 volunteers</article-title>. <source>Clin Chem</source>. <year>1993</year>;<volume>39</volume>:<fpage>53</fpage>–<lpage>59</lpage>. <?supplied-pmid 8419058?><pub-id pub-id-type="pmid">8419058</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref027">
      <label>27</label>
      <mixed-citation publication-type="journal"><name><surname>Yadav</surname><given-names>YK</given-names></name>, <name><surname>Fatima</surname><given-names>U</given-names></name>, <name><surname>Dogra</surname><given-names>S</given-names></name>, <name><surname>Kaushik</surname><given-names>A</given-names></name>. <article-title>Beware of "hook effect" giving false negative pregnancy test on point-of-care kits</article-title>. <source>J Postgrad Med</source>. <year>2013</year>;<volume>59</volume>:<fpage>153</fpage>–<lpage>154</lpage>. <pub-id pub-id-type="doi">10.4103/0022-3859.113838</pub-id><?supplied-pmid 23793322?><pub-id pub-id-type="pmid">23793322</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref028">
      <label>28</label>
      <mixed-citation publication-type="journal"><name><surname>Apgar</surname><given-names>BS</given-names></name>, <name><surname>Brotzman</surname><given-names>G</given-names></name>. <article-title>Management of cervical cytologic abnormalities</article-title>. <source>Am Fam Physician</source>. <year>2004</year>;<volume>70</volume>:<fpage>1905</fpage>–<lpage>1916</lpage>. <?supplied-pmid 15571057?><pub-id pub-id-type="pmid">15571057</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref029">
      <label>29</label>
      <mixed-citation publication-type="journal"><name><surname>Boardman</surname><given-names>LA</given-names></name>, <name><surname>Kennedy</surname><given-names>CM</given-names></name>. <article-title>Management of atypical squamous cells, low-grade squamous intraepithelial lesions, and cervical intraepithelial neoplasia 1</article-title>. <source>Obstet Gynecol Clin North Am.</source><year>2008</year>;<volume>35</volume>:<fpage>599</fpage>–<lpage>614</lpage>. <pub-id pub-id-type="doi">10.1016/j.ogc.2008.09.001</pub-id><?supplied-pmid 19061819?><pub-id pub-id-type="pmid">19061819</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref030">
      <label>30</label>
      <mixed-citation publication-type="journal"><name><surname>Boffetta</surname><given-names>P</given-names></name>, <name><surname>McLaughlin</surname><given-names>JK</given-names></name>, <name><surname>La Vecchia</surname><given-names>C</given-names></name>, <name><surname>Tarone</surname><given-names>RE</given-names></name>, <name><surname>Lipworth</surname><given-names>L</given-names></name>, <name><surname>Blot</surname><given-names>WJ</given-names></name>. <article-title>False-positive results in cancer epidemiology: a plea for epistemological modesty</article-title>. <source>J Natl Cancer Inst</source>. <year>2008</year>;<volume>100</volume>:<fpage>988</fpage>–<lpage>995</lpage>. <pub-id pub-id-type="doi">10.1093/jnci/djn191</pub-id><?supplied-pmid 18612135?><pub-id pub-id-type="pmid">18612135</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref031">
      <label>31</label>
      <mixed-citation publication-type="journal"><name><surname>McLaughlin</surname><given-names>JK</given-names></name>, <name><surname>Tarone</surname><given-names>RE</given-names></name>. <article-title>False positives in cancer epidemiology</article-title>. <source>Cancer Epidemiol Biomarkers Prev</source>. <year>2013</year>;<volume>22</volume>:<fpage>11</fpage>–<lpage>15</lpage>. <pub-id pub-id-type="doi">10.1158/1055-9965.EPI-12-0995</pub-id><?supplied-pmid 23118145?><pub-id pub-id-type="pmid">23118145</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref032">
      <label>32</label>
      <mixed-citation publication-type="journal"><name><surname>Bossuyt</surname><given-names>PM</given-names></name>, <name><surname>Reitsma</surname><given-names>JB</given-names></name>, <name><surname>Bruns</surname><given-names>DE</given-names></name>, <name><surname>Gatsonis</surname><given-names>CA</given-names></name>, <name><surname>Glasziou</surname><given-names>PP</given-names></name>, <name><surname>Irwig</surname><given-names>L</given-names></name>, <etal>et al</etal><article-title>STARD 2015: An Updated List of Essential Items for Reporting Diagnostic Accuracy Studies</article-title>. <source>Clin Chem</source>. <year>2015</year>;<volume>61</volume>:<fpage>1446</fpage>–<lpage>1452</lpage>. <pub-id pub-id-type="doi">10.1373/clinchem.2015.246280</pub-id><?supplied-pmid 26510957?><pub-id pub-id-type="pmid">26510957</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0217146.ref033">
      <label>33</label>
      <mixed-citation publication-type="journal"><name><surname>Cohen</surname><given-names>JF</given-names></name>, <name><surname>Korevaar</surname><given-names>DA</given-names></name>, <name><surname>Altman</surname><given-names>DG</given-names></name>, <name><surname>Bruns</surname><given-names>DE</given-names></name>, <name><surname>Gatsonis</surname><given-names>CA</given-names></name>, <name><surname>Hooft</surname><given-names>L</given-names></name>, <etal>et al</etal><article-title>STARD 2015 guidelines for reporting diagnostic accuracy studies: explanation and elaboration</article-title>. <source>BMJ Open</source>. <year>2016</year>;<volume>6</volume>:<fpage>e012799</fpage><pub-id pub-id-type="doi">10.1136/bmjopen-2016-012799</pub-id><?supplied-pmid 28137831?><pub-id pub-id-type="pmid">28137831</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
