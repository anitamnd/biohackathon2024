<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sci Rep</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id>
    <journal-title-group>
      <journal-title>Scientific Reports</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2045-2322</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7138789</article-id>
    <article-id pub-id-type="publisher-id">63159</article-id>
    <article-id pub-id-type="doi">10.1038/s41598-020-63159-5</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DeepMicro: deep representation learning for disease prediction based on microbiome data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Oh</surname>
          <given-names>Min</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Zhang</surname>
          <given-names>Liqing</given-names>
        </name>
        <address>
          <email>lqzhang@cs.vt.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0694 4940</institution-id><institution-id institution-id-type="GRID">grid.438526.e</institution-id><institution>Department of Computer Science, Virginia Tech, </institution></institution-wrap>Blacksburg, VA USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>7</day>
      <month>4</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>7</day>
      <month>4</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2020</year>
    </pub-date>
    <volume>10</volume>
    <elocation-id>6026</elocation-id>
    <history>
      <date date-type="received">
        <day>20</day>
        <month>10</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>9</day>
        <month>3</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Human microbiota plays a key role in human health and growing evidence supports the potential use of microbiome as a predictor of various diseases. However, the high-dimensionality of microbiome data, often in the order of hundreds of thousands, yet low sample sizes, poses great challenge for machine learning-based prediction algorithms. This imbalance induces the data to be highly sparse, preventing from learning a better prediction model. Also, there has been little work on deep learning applications to microbiome data with a rigorous evaluation scheme. To address these challenges, we propose DeepMicro, a deep representation learning framework allowing for an effective representation of microbiome profiles. DeepMicro successfully transforms high-dimensional microbiome data into a robust low-dimensional representation using various autoencoders and applies machine learning classification algorithms on the learned representation. In disease prediction, DeepMicro outperforms the current best approaches based on the strain-level marker profile in five different datasets. In addition, by significantly reducing the dimensionality of the marker profile, DeepMicro accelerates the model training and hyperparameter optimization procedure with 8X–30X speedup over the basic approach. DeepMicro is freely available at <ext-link ext-link-type="uri" xlink:href="https://github.com/minoh0201/DeepMicro">https://github.com/minoh0201/DeepMicro</ext-link>.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Computational biology and bioinformatics</kwd>
      <kwd>Machine learning</kwd>
      <kwd>Predictive markers</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2020</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1" sec-type="introduction">
    <title>Introduction</title>
    <p id="Par2">As our knowledge of microbiota grows, it becomes increasingly clear that the human microbiota plays a key role in human health and diseases<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. The microbial community, composed of trillions of microbes, is a complex and diverse ecosystem living on and inside a human. These commensal microorganisms benefit humans by allowing them to harvest inaccessible nutrients and maintain the integrity of mucosal barriers and homeostasis. Especially, the human microbiota contributes to the host immune system development, affecting multiple cellular processes such as metabolism and immune-related functions<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>. They have been shown to be responsible for carcinogenesis of certain cancers and substantially affect therapeutic response<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. All these emerging evidences substantiate the potential use of microbiota as a predictor for various diseases<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>.</p>
    <p id="Par3">The development of high-throughput sequencing technologies has enabled researchers to capture a comprehensive snapshot of the microbial community of interest. The most common components of the human microbiome can be profiled with 16 S rRNA gene sequencing technology in a cost-effective way<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. Comparatively, shotgun metagenomic sequencing technology can provide a deeper resolution profile of the microbial community at the strain level<sup><xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR7">7</xref></sup>. As the cost of shotgun metagenomic sequencing keeps decreasing and the resolution increasing, it is likely that a growing role of the microbiome in human health will be uncovered from the mounting metagenomic datasets.</p>
    <p id="Par4">Although novel technologies have dramatically increased our ability to characterize human microbiome and there is evidence suggesting the potential use of the human microbiome for predicting disease state, how to effectively utilize the human microbiome data faces several key challenges. Firstly, effective dimensionality reduction that preserves the intrinsic structure of the microbiome data is required to handle the high dimensional data with low sample sizes, especially the microbiome data with strain-level information that often contain hundreds of thousands of gene markers but for only some hundred or fewer samples. With a low number of samples, large number of features can cause the curse of dimensionality, usually inducing sparsity of the data in the feature space. Along with traditional dimensionality reduction algorithms, autoencoder that learns a low-dimensional representation by reconstructing the input<sup><xref ref-type="bibr" rid="CR8">8</xref></sup> can be applied to exploit microbiome data. Secondly, given the fast amounting metagenomic data, there is an inadequate effort in adapting machine learning algorithms for predicting disease state based on microbiome data. In particular, deep learning is a class of machine learning algorithms that builds on large multi-layer neural networks, and that can potentially make effective use of metagenomic data. With the rapidly growing attention from both academia and industry, deep learning has produced unprecedented performance in various fields, including not only image and speech recognition, natural language processing, and language translation but also biological and healthcare research<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>. A few studies have applied deep learning approaches to abundance profiles of the human gut microbiome for disease prediction<sup><xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR11">11</xref></sup>. However, there has been no research utilizing strain-level profiles for the purpose. Comparatively, strain level profiles, often containing hundreds of thousands of gene markers’ information, should be more informative for accurately classifying the samples into patient and healthy control groups across different types of diseases than abundance profiles that usually contain only a few hundred bacteria’s abundance information<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. Lastly, to evaluate and compare the performance of machine learning models, it is necessary to introduce a rigorous validation framework to estimate their performance over unseen data. Pasolli <italic>et al</italic>., a study that built classification models based on microbiome data, utilized a 10-fold cross-validation scheme that tunes the hyper-parameters on the test set without using a validation set<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. This approach may overestimate model performance as it exposes the test set to the model in the training procedure<sup><xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR14">14</xref></sup>.</p>
    <p id="Par5">To address these issues, we propose DeepMicro, a deep representation learning framework that deploys various autoencoders to learn robust low-dimensional representations from high-dimensional microbiome profiles and trains classification models based on the learned representation. We applied a thorough validation scheme that excludes the test set from hyper-parameter optimization to ensure fairness of model comparison. Our model surpasses the current best methods in terms of disease state prediction of inflammatory bowel disease, type 2 diabetes in the Chinese cohort as well as European women cohort, liver cirrhosis, and obesity. DeepMicro is open-sourced and publicly available software to benefit future research, allowing researchers to obtain a robust low-dimensional representation of microbiome profiles with user-defined deep architecture and hyper-parameters.</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <sec id="Sec3">
      <title>Dataset and extracting microbiome profiles</title>
      <p id="Par6">We considered publicly available human gut metagenomic samples of six different disease cohorts: inflammatory bowel disease (IBD), type 2 diabetes in European women (EW-T2D), type 2 diabetes in Chinese (C-T2D) cohortobesity, (Obesity), liver cirrhosis (Cirrhosis), and colorectal cancer (Colorectal). All these samples were derived from whole-genome shotgun metagenomic studies that used Illumina paired-end sequencing technology. Each cohort consists of healthy control and patient samples as shown in Table <xref rid="Tab1" ref-type="table">1</xref>. IBD cohort has 25 individuals with inflammatory bowel disease and 85 healthy controls<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>. EW-T2D cohort has 53 European women with type 2 diabetes and 43 healthy European women<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>. C-T2D cohort has 170 Chinese individuals with type 2 diabetes and 174 healthy Chinese controls<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. Obesity cohort has 164 obese patients and 89 non-obese controls<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>. Cirrhosis cohort has 118 patients with liver cirrhosis and 114 healthy controls<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>. Colorectal cohort has 48 colorectal cancer patients and 73 healthy controls<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. In total, 1,156 human gut metagenomic samples, obtained from MetAML repository<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>, were used in our experiments.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Human gut microbiome datasets used for disease state prediction.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Disease</th><th>Dataset name</th><th># total samples</th><th># of healthy controls</th><th># of patient samples</th><th>Data source references</th></tr></thead><tbody><tr><td>Inflammatory Bowel Disease</td><td>IBD</td><td>110</td><td>85</td><td>25</td><td><sup><xref ref-type="bibr" rid="CR15">15</xref></sup></td></tr><tr><td rowspan="2">Type 2 Diabetes</td><td>EW-T2D</td><td>96</td><td>43</td><td>53</td><td><sup><xref ref-type="bibr" rid="CR16">16</xref></sup></td></tr><tr><td>C-T2D</td><td>344</td><td>174</td><td>170</td><td><sup><xref ref-type="bibr" rid="CR17">17</xref></sup></td></tr><tr><td>Obesity</td><td>Obesity</td><td>253</td><td>89</td><td>164</td><td><sup><xref ref-type="bibr" rid="CR18">18</xref></sup></td></tr><tr><td>Liver Cirrhosis</td><td>Cirrhosis</td><td>232</td><td>114</td><td>118</td><td><sup><xref ref-type="bibr" rid="CR19">19</xref></sup></td></tr><tr><td>Colorectal Cancer</td><td>Colorectal</td><td>121</td><td>73</td><td>48</td><td><sup><xref ref-type="bibr" rid="CR20">20</xref></sup></td></tr></tbody></table></table-wrap></p>
      <p id="Par7">Two types of microbiome profiles were extracted from the metagenomic samples: 1) strain-level marker profile and 2) species-level relative abundance profile. MetaPhlAn2 was utilized to extract these profiles with default parameters<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. We utilized MetAML to preprocess the abundance profile by selecting species-level features and excluding sub-species-level features<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. The strain-level marker profile consists of binary values indicating the presence (1) or absence (0) of a certain strain. The species-level relative abundance profile consists of real values in [0,1] indicating the percentages of the species in the total observed species. The abundance profile has a few hundred dimensions, whereas the marker profile has a much larger number of dimensions, up to over a hundred thousand in the current data (Table <xref rid="Tab2" ref-type="table">2</xref>).<table-wrap id="Tab2"><label>Table 2</label><caption><p>The number of dimensions of the preprocessed microbiome profiles.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Profile type</th><th>IBD</th><th>EW-T2D</th><th>C-T2D</th><th>Obesity</th><th>Cirrhosis</th><th>Colorectal</th></tr></thead><tbody><tr><td>marker profile</td><td>91,756</td><td>83,456</td><td>119,792</td><td>99,568</td><td>120,553</td><td>108,034</td></tr><tr><td>abundance profile</td><td>443</td><td>381</td><td>572</td><td>465</td><td>542</td><td>503</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec4">
      <title>Deep representation learning</title>
      <p id="Par8">An autoencoder is a neural network reconstructing its input <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x$$\end{document}</tex-math><mml:math id="M2"><mml:mi>x</mml:mi></mml:math><inline-graphic xlink:href="41598_2020_63159_Article_IEq1.gif"/></alternatives></inline-formula>. Internally, its general form consists of an encoder function <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${f}_{\phi }(\,\cdot \,)$$\end{document}</tex-math><mml:math id="M4"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mspace width="-.25em"/><mml:mo>⋅</mml:mo><mml:mspace width="-.25em"/><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="41598_2020_63159_Article_IEq2.gif"/></alternatives></inline-formula> and a decoder function <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${f}_{\theta }^{{\prime} }(\,\cdot \,)$$\end{document}</tex-math><mml:math id="M6"><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mspace width="-.25em"/><mml:mo>⋅</mml:mo><mml:mspace width="-.25em"/><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="41598_2020_63159_Article_IEq3.gif"/></alternatives></inline-formula> where <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\phi $$\end{document}</tex-math><mml:math id="M8"><mml:mi>ϕ</mml:mi></mml:math><inline-graphic xlink:href="41598_2020_63159_Article_IEq4.gif"/></alternatives></inline-formula> and <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\theta $$\end{document}</tex-math><mml:math id="M10"><mml:mi>θ</mml:mi></mml:math><inline-graphic xlink:href="41598_2020_63159_Article_IEq5.gif"/></alternatives></inline-formula> are parameters of encoder and decoder functions, respectively. An autoencoder is trained to minimize the difference between an input <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x$$\end{document}</tex-math><mml:math id="M12"><mml:mi>x</mml:mi></mml:math><inline-graphic xlink:href="41598_2020_63159_Article_IEq6.gif"/></alternatives></inline-formula> and a reconstructed input <inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x{\prime} $$\end{document}</tex-math><mml:math id="M14"><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:math><inline-graphic xlink:href="41598_2020_63159_Article_IEq7.gif"/></alternatives></inline-formula>, the reconstruction loss (e.g., squared error) that can be written as follows:<disp-formula id="Equa"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L(x,x\text{'})=||x-x\text{'}|{|}^{2}=||x-f{\text{'}}_{\theta }({f}_{\phi }(x))|{|}^{2}.$$\end{document}</tex-math><mml:math id="M16" display="block"><mml:mi>L</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>'</mml:mo></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo>'</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:msub><mml:mrow><mml:mo>'</mml:mo></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:msup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:math><graphic xlink:href="41598_2020_63159_Article_Equa.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par9">After training an autoencoder, we are interested in obtaining a latent representation <inline-formula id="IEq8"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z={f}_{\phi }(x)$$\end{document}</tex-math><mml:math id="M18"><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_63159_Article_IEq8.gif"/></alternatives></inline-formula> of the input using the trained encoder. The latent representation, usually in a much lower-dimensional space than the original input, contains sufficient information for reconstructing the original input as close as possible. We utilized this representation to train classifiers for disease prediction.</p>
      <p id="Par10">For the DeepMicro framework, we incorporated various deep representation learning techniques, including shallow autoencoder (SAE), deep autoencoder (DAE), variational autoencoder (VAE), and convolutional autoencoder (CAE), to learn a low-dimensional embedding for microbiome profiles. Note that the diverse combinations of hyper-parameters defining the structure of autoencoders (e.g., the number of units and layers) have been explored in a grid fashion as described below, however, users are not limited to the tested hyper-parameters and can use their own hyper-parameter grid fitted to their data.</p>
      <p id="Par11">Firstly, we utilized SAE, the simplest autoencoder structure composed of the encoder part where the input layer is fully connected with the latent layer, and the decoder part where the output layer produces reconstructed input <inline-formula id="IEq9"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x}^{\text{'}}$$\end{document}</tex-math><mml:math id="M20"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>'</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="41598_2020_63159_Article_IEq9.gif"/></alternatives></inline-formula> by taking weighted sums of outputs of the latent layer. We introduced a linear activation function for the latent and output layer. Other options for the loss and activation functions are available for users (such as binary cross-entropy and sigmoid function). Initial values of the weights and bias were initialized with Glorot uniform initializer<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. We examined five different sizes of dimensions for the latent representation (32, 64, 128, 256, and 512).</p>
      <p id="Par12">In addition to the SAE model, we implemented the DAE model by introducing hidden layers between the input and latent layers as well as between the latent and output layers. All of the additional hidden layers were equipped with Rectified Linear Unit (ReLu) activation function and Glorot uniform initializer. The same number of hidden layers (one layer or two layers) were inserted into both encoder and decoder parts. Also, we gradually increased the number of hidden units. The number of hidden units in the added layers was set to the double of the successive layer in the encoder part and to the double of the preceding layer in the decoder part. With this setting, model complexity is controlled by both the number of hidden units and the number of hidden layers, maintaining structural symmetry of the model. For example, if the latent layer has 512 hidden units and if two layers are inserted to the encoder and decoder parts, then the resulting autoencoder has 5 hidden layers with 2048, 1024, 512, 1024, and 2048 hidden units, respectively. Similar to SAE, we varied the number of hidden units in the latent layer as follows: 32, 64, 128, 256, 512, thus, in total, we tested 10 different DAE architectures (Table <xref rid="MOESM1" ref-type="media">S2</xref>).</p>
      <p id="Par13">A variational autoencoder (VAE) learns probabilistic representations <inline-formula id="IEq10"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z$$\end{document}</tex-math><mml:math id="M22"><mml:mi>z</mml:mi></mml:math><inline-graphic xlink:href="41598_2020_63159_Article_IEq10.gif"/></alternatives></inline-formula> given input <inline-formula id="IEq11"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x$$\end{document}</tex-math><mml:math id="M24"><mml:mi>x</mml:mi></mml:math><inline-graphic xlink:href="41598_2020_63159_Article_IEq11.gif"/></alternatives></inline-formula> and then use these representations to reconstruct input <inline-formula id="IEq12"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x\text{'}$$\end{document}</tex-math><mml:math id="M26"><mml:mi>x</mml:mi><mml:mo>'</mml:mo></mml:math><inline-graphic xlink:href="41598_2020_63159_Article_IEq12.gif"/></alternatives></inline-formula><sup><xref ref-type="bibr" rid="CR22">22</xref></sup>. Using variational inference, the true posterior distribution of latent embeddings (i.e., <inline-formula id="IEq13"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p(z|x)$$\end{document}</tex-math><mml:math id="M28"><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>|</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_63159_Article_IEq13.gif"/></alternatives></inline-formula>) can be approximated by the introduced posterior <inline-formula id="IEq14"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${q}_{\phi }(z|x)$$\end{document}</tex-math><mml:math id="M30"><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>|</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_63159_Article_IEq14.gif"/></alternatives></inline-formula> where <inline-formula id="IEq15"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\phi $$\end{document}</tex-math><mml:math id="M32"><mml:mi>ϕ</mml:mi></mml:math><inline-graphic xlink:href="41598_2020_63159_Article_IEq15.gif"/></alternatives></inline-formula> are parameters of an encoder network. Unlike the previous autoencoders learning an unconstrained representation VAE, learns a generalized latent representation under the assumption that the posterior approximation follows Gaussian distribution. The encoder network encodes the means and variances of the multivariate Gaussian distribution. The latent representation <inline-formula id="IEq16"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z$$\end{document}</tex-math><mml:math id="M34"><mml:mi>z</mml:mi></mml:math><inline-graphic xlink:href="41598_2020_63159_Article_IEq16.gif"/></alternatives></inline-formula> can be sampled from the learned posterior distribution <inline-formula id="IEq17"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${q}_{\phi }(z|x)\sim {\rm{{\rm N}}}(\mu ,\Sigma )$$\end{document}</tex-math><mml:math id="M36"><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>z</mml:mi><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>∼</mml:mo><mml:mi mathvariant="normal">Ν</mml:mi><mml:mo>(</mml:mo><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Σ</mml:mi><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="41598_2020_63159_Article_IEq17.gif"/></alternatives></inline-formula>. Then the sampled latent representation is passed into the decoder network to generate the reconstructed input <inline-formula id="IEq18"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x{\prime} \sim {g}_{\theta }(x|z)$$\end{document}</tex-math><mml:math id="M38"><mml:mrow><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:mrow><mml:mo>∼</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>|</mml:mo><mml:mi>z</mml:mi><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="41598_2020_63159_Article_IEq18.gif"/></alternatives></inline-formula> where <inline-formula id="IEq19"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\theta $$\end{document}</tex-math><mml:math id="M40"><mml:mi>θ</mml:mi></mml:math><inline-graphic xlink:href="41598_2020_63159_Article_IEq19.gif"/></alternatives></inline-formula> are the parameters of the decoder.</p>
      <p id="Par14">To approximate the true posterior, we need to minimize the Kullback-Leibler (KL) divergence between the introduced posterior and the true posterior,<disp-formula id="Equb"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$KL({q}_{\phi }(z|x)||p(z|x))=-ELBO(\phi ,\theta ;x)+\,\log (p(x)),$$\end{document}</tex-math><mml:math id="M42" display="block"><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>|</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>|</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mi>L</mml:mi><mml:mi>B</mml:mi><mml:mi>O</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mspace width=".25em"/><mml:mi>log</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41598_2020_63159_Article_Equb.gif" position="anchor"/></alternatives></disp-formula>rewritten as<disp-formula id="Equc"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\log (p(x))=ELBO(\phi ,\theta ;x)+KL({q}_{\phi }(z|x)||p(z|x)),$$\end{document}</tex-math><mml:math id="M44" display="block"><mml:mi>log</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mi>L</mml:mi><mml:mi>B</mml:mi><mml:mi>O</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>|</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>|</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41598_2020_63159_Article_Equc.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq20"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ELBO(\phi ,\theta ;x)$$\end{document}</tex-math><mml:math id="M46"><mml:mi>E</mml:mi><mml:mi>L</mml:mi><mml:mi>B</mml:mi><mml:mi>O</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2020_63159_Article_IEq20.gif"/></alternatives></inline-formula> is an evidence lower bound on the log probability of the data because the KL term must be greater than or equal to zero. It is intractable to compute the KL term directly but minimizing the KL divergence is equivalent to maximizing the lower bound, decomposed as follows:<disp-formula id="Equd"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ELBO(\phi ,\theta ;x)={{\mathbb{E}}}_{{q}_{\phi }(z|x)}[\log (\,{g}_{\theta }(x|z))]-KL({q}_{\phi }(z|x)||p(z)).$$\end{document}</tex-math><mml:math id="M48" display="block"><mml:mi>E</mml:mi><mml:mi>L</mml:mi><mml:mi>B</mml:mi><mml:mi>O</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>|</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mspace width=".10em"/><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>|</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>|</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math><graphic xlink:href="41598_2020_63159_Article_Equd.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par15">The final objective function can be induced by converting the maximization problem to the minimization problem.<disp-formula id="Eque"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L(\phi ,\theta ;x)=-\,{{\mathbb{E}}}_{{q}_{\phi }(z|x)}[\log (\,{g}_{\theta }(x|z))]+KL({q}_{\phi }(z|x)||p(z))$$\end{document}</tex-math><mml:math id="M50" display="block"><mml:mi>L</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mspace width="-.25em"/><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>|</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mspace width=".10em"/><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>|</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>|</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2020_63159_Article_Eque.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par16">The first term can be viewed as a reconstruction term as it forces the inferred latent representation to recover its corresponding input and the second KL term can be considered as a regularization term to modulate the posterior of the learned representation to be Gaussian distribution. We used ReLu activation and Glorot uniform initializer for intermediate hidden layers in encoder and decoder. One intermediate hidden layer was used and the number of hidden units in it varied from 32, 64, 128, 256, to 512. The latent layer was set to 4, 8, or 16 units. Thus, altogether we tested 15 different model structures.</p>
      <p id="Par17">Instead of fully connected layers, a convolutional autoencoder (CAE) is equipped with convolutional layers in which each unit is connected to only local regions of the previous layer<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>. A convolutional layer consists of multiple filters (kernels) and each filter has a set of weights used to perform convolution operation that computes dot products between a filter and a local region<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. We used ReLu activation and Glorot uniform initializer for convolutional layers. We did not use any pooling layer as it may generalize too much to reconstruct an input. The <inline-formula id="IEq21"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n$$\end{document}</tex-math><mml:math id="M52"><mml:mi>n</mml:mi></mml:math><inline-graphic xlink:href="41598_2020_63159_Article_IEq21.gif"/></alternatives></inline-formula>-dimensional input vector was reshaped like a squared image with a size of <inline-formula id="IEq22"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d\times d\times 1$$\end{document}</tex-math><mml:math id="M54"><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:math><inline-graphic xlink:href="41598_2020_63159_Article_IEq22.gif"/></alternatives></inline-formula> where <inline-formula id="IEq23"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d=\lfloor \sqrt{n}\rfloor +1$$\end{document}</tex-math><mml:math id="M56"><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⌊</mml:mo><mml:mrow><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:mrow><mml:mo fence="false" stretchy="false">⌋</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:math><inline-graphic xlink:href="41598_2020_63159_Article_IEq23.gif"/></alternatives></inline-formula>. As <inline-formula id="IEq24"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${d}^{2}\ge n$$\end{document}</tex-math><mml:math id="M58"><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>≥</mml:mo><mml:mi>n</mml:mi></mml:math><inline-graphic xlink:href="41598_2020_63159_Article_IEq24.gif"/></alternatives></inline-formula>, we padded the rest part of the reshaped input with zeros. To be flexible to an input size, the filter size of the first convolutional layer was set to 10% of the input width and height, respectively (i.e. <inline-formula id="IEq25"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lfloor 0.1d\rfloor \times \lfloor 0.1d\rfloor $$\end{document}</tex-math><mml:math id="M60"><mml:mo fence="false" stretchy="false">⌊</mml:mo><mml:mrow><mml:mn>0.1</mml:mn><mml:mi>d</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">⌋</mml:mo><mml:mo>×</mml:mo><mml:mo fence="false" stretchy="false">⌊</mml:mo><mml:mrow><mml:mn>0.1</mml:mn><mml:mi>d</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">⌋</mml:mo></mml:math><inline-graphic xlink:href="41598_2020_63159_Article_IEq25.gif"/></alternatives></inline-formula>). For the first convolutional layer, we used 25% of the filter size as the size of stride which configures how much we slide the filter. For the following convolutional layers in the encoder part, we used 10% of the output size of the preceding layer as the filter size and 50% of this filter size as the stride size. All units in the last convolutional layer of the encoder part have been flattened in the following flatten layer which is designated as a latent layer. We utilized convolutional transpose layers (deconvolutional layers) to make the decoder symmetry to the encoder. In our experiment, the number of filters in a convolutional layer was set to half of that of the preceding layer for the encoder part. For example, if the first convolutional layer has 64 filters and there are three convolutional layers in the encoder, then the following two convolutional layers have 32 and 16 filters, respectively. We varied the number of convolutional layers from 2 to 3 and tried five different numbers of filters in the first convolutional layer (4, 8, 16, 32, and 64). In total, we tested 10 different CAE model structures.</p>
      <p id="Par18">To train deep representation models, we split each dataset into a training set, a validation set, and a test set (64% training set, 16% validation set, and 20% test set; Fig. <xref rid="MOESM1" ref-type="media">S1</xref>). Note that the test set was withheld from training the model. We used the early-stopping strategy, that is, trained the models on the training set, computed the reconstruction loss for the validation set after each epoch, stopped the training if there was no improvement in validation loss during 20 epochs, and then selected the model with the least validation loss as the best model. We used mean squared error for reconstruction loss and applied adaptive moment estimation (Adam) optimizer for gradient descent with default parameters (learning rate: 0.001, epsilon: 1e-07) as provided in the original paper<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. We utilized the encoder part of the best model to produce a low-dimensional representation of the microbiome data for downstream disease prediction.</p>
    </sec>
    <sec id="Sec5">
      <title>Prediction of disease states based on the learned representation</title>
      <p id="Par19">We built classification models based on the encoded low-dimensional representations of microbiome profiles (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). Three machine learning algorithms, support vector machine (SVM), random forest (RF), and Multi-Layer Perceptron (MLP), were used. We explored hyper-parameter space with grid search SVM. maximizes the margin between the supporting hyperplanes to optimize a decision boundary separating data points of different classes<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. In this study, we utilized both radial basis function (RBF) kernel and a linear kernel function to compute decision margins in the transformed space to which the original data was mapped. We varied penalty parameter <italic>C</italic> (2<sup>−5</sup>, 2<sup>−3</sup>, …, 25) for both kernels as well as kernel coefficient <italic>gamma</italic> (2<sup>−15</sup>, 2<sup>−13</sup>, …, 23) for RBF kernel. In total, 60 different combinations of hyper-parameters were examined to optimize SVM (Table <xref rid="MOESM1" ref-type="media">S2</xref>).<fig id="Fig1"><label>Figure 1</label><caption><p>DeepMicro framework. An autoencoder is trained to map the input X to the low-dimensional latent space with the encoder and to reconstruct X with the decoder. The encoder part is reused to produce a latent representation of any new input X that is in turn fed into a classification algorithm to determine whether the input is the positive or negative class.</p></caption><graphic xlink:href="41598_2020_63159_Fig1_HTML" id="d29e1686"/></fig></p>
      <p id="Par20">RF builds multiple decision trees based on various sub-samples of the training data and merges them to improve the prediction accuracy. The size of sub-samples is the same as that of training data but the samples are drawn randomly with replacement from the training data. For the hyper-parameter grid of RF classifier, the number of trees (estimators) was set to 100, 300, 500, 700, and 900, and the minimum number of samples in a leaf node was altered from 1 to 5. Also, we tested two criteria, Gini impurity and information gain, for selecting features to split a node in a decision tree. For the maximum number of features considered to find the best split at each split, we used a square root of <inline-formula id="IEq26"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n$$\end{document}</tex-math><mml:math id="M62"><mml:mi>n</mml:mi></mml:math><inline-graphic xlink:href="41598_2020_63159_Article_IEq26.gif"/></alternatives></inline-formula> and a logarithm to base 2 of <inline-formula id="IEq27"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n$$\end{document}</tex-math><mml:math id="M64"><mml:mi>n</mml:mi></mml:math><inline-graphic xlink:href="41598_2020_63159_Article_IEq27.gif"/></alternatives></inline-formula> (<inline-formula id="IEq28"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n$$\end{document}</tex-math><mml:math id="M66"><mml:mi>n</mml:mi></mml:math><inline-graphic xlink:href="41598_2020_63159_Article_IEq28.gif"/></alternatives></inline-formula> is the sample size). In total, we tested 100 combinations of hyper-parameters of RF.</p>
      <p id="Par21">MLP is an artificial neural network classifier that consists of an input layer, hidden layers, and an output layer. All of the layers are fully connected to their successive layer. We used ReLu activations for all hidden layers and sigmoid activation for the output layer that has a single unit. The number of units in the hidden layers was set to half of that of the preceding layer except the first hidden layer. We varied the number of hidden layers (1, 2, and 3), the number of epochs (30, 50, 100, 200, and 300), the number of units in the first hidden layer (10, 30, 50, 100), and dropout rate (0.1 and 0.3). In total, 120 hyper-parameter combinations were tested in our experiment.</p>
      <p id="Par22">We implemented DeepMicro in Python 3.5.2 using machine learning and data analytics libraries, including Numpy 1.16.2, Pandas 0.24.2, Scipy 1.2.1, Scikt-learn 0.20.3, Keras 2.2.4, and Tensorflow 1.13.1. Source code is publicly available at the git repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/minoh0201/DeepMicro">https://github.com/minoh0201/DeepMicro</ext-link>).</p>
    </sec>
    <sec id="Sec6">
      <title>Performance evaluation</title>
      <p id="Par23">To avoid an overestimation of prediction performance, we designed a thorough performance evaluation scheme (Fig. <xref rid="MOESM1" ref-type="media">S1</xref>). For a given dataset (e.g. Cirrhosis), we split it into training and test set in the ratio of 8:2 with a given random partition seed, keeping a ratio between classes in both training and test set to be the same as that of the given dataset. Using only the training set, a representation learning model was trained. Then, the learned representation model was applied to the training set and test set to obtain dimensionality-reduced training and test set. After the dimensionality has been reduced, we conducted 5-fold cross-validation on the training set by varying hyper-parameters of classifiers. The best hyper-parameter combination for each classifier was selected by averaging an accuracy metric of the five different results. The area under the receiver operating characteristics curve (AUC) was used for performance evaluation. We trained a final classification model using the whole training set with the best combination of hyper-parameters and tested it on the test set. This procedure was repeated five times by changing the random partition seed at the beginning of the procedure. The resulting AUC scores were averaged and the average was used to compare model performance.</p>
    </sec>
  </sec>
  <sec id="Sec7" sec-type="results">
    <title>Results</title>
    <p id="Par24">We developed DeepMicro, a deep representation learning framework for predicting individual phenotype based on microbiome profiles. Various autoencoders (SAE, DAE, VAE, and CAE) have been utilized to learn a low-dimensional representation of the microbiome profiles. Then three classification models including SVM, RF, and MLP were trained on the learned representation to discriminate between disease and control sample groups. We tested our framework on six disease datasets (Table <xref rid="Tab1" ref-type="table">1</xref>), including inflammatory bowel disease (IBD), type 2 diabetes in European women (EW-T2D), type 2 diabetes in Chinese (C-T2D), obesity (Obesity), liver cirrhosis (Cirrhosis), and colorectal cancer (Colorectal). For all the datasets, two types of microbiome profiles, strain-level marker profile and species-level relative abundance profile, have been extracted and tested (Table <xref rid="Tab2" ref-type="table">2</xref>). Also, we devised a thorough performance evaluation scheme that isolates the test set from the training and validation sets in the hyper-parameter optimization phase to compare various models (See Methods and Fig. <xref rid="MOESM1" ref-type="media">S1</xref>).</p>
    <p id="Par25">We compared our method to the current best approach (MetAML) that directly trained classifiers, such as SVM and RF, on the original microbiome profile<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. We utilized the same hyper-parameters grid used in MetAML for each classification algorithm. In addition, we tested Principal Component Analysis (PCA) and Gaussian Random Projection (RP), using them as the replacement of the representation learning to observe how traditional dimensionality reduction algorithms behave. For PCA, we selected the principal components explaining 99% of the variance in the data<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. For RP, we set the number of components to be automatically adjusted according to Johnson-Lindenstrauss lemma (eps parameter was set to 0.5)<sup><xref ref-type="bibr" rid="CR28">28</xref>–<xref ref-type="bibr" rid="CR30">30</xref></sup>.</p>
    <p id="Par26">We picked the best model for each approach in terms of prediction performance and compared the approaches across the datasets. Figure <xref rid="Fig2" ref-type="fig">2</xref> shows the results of DeepMicro and the other approaches for the strain-level marker profile. DeepMicro outperforms the other approaches for five datasets, including IBD (AUC = 0.955)EW-T2D, (AUC = 0.899)C-T2D, (AUC = 0.763) Obesity, (AUC = 0.659), and Cirrhosis (AUC = 0.940). For Colorectal dataset, DeepMicro has slightly lower performance than the best approach (DeepMicro’s AUC = 0.803 vs. MetAML’s AUC = 0.811). The marker profile-based models generally perform better than the abundance profile-based models (Figs. <xref rid="MOESM1" ref-type="media">S8</xref> and <xref rid="MOESM1" ref-type="media">S2</xref>). The only exception is Obesity dataset for which the abundance-based DeepMicro model shows better performance (AUC = 0.674). Note that as AUC could be misleading in an imbalanced classification scenario<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>, we also evaluated the area under the precision-recall curve (AUPRC) for the imbalanced data set IBD and observed the same trend between AUC and AUPRC (Table <xref rid="MOESM1" ref-type="media">S3</xref>).<fig id="Fig2"><label>Figure 2</label><caption><p>Disease prediction performance for marker profile-based models. Prediction performance of various methods built on marker profile has been assessed with AUC. MetAML utilizes support vector machine (SVM) and random forest (RF), and the superior model is presented (green). Principal component analysis (PCA; blue) and gaussian random projection (RP; yellow) have been applied to reduce dimensions of datasets before classification. DeepMicro (red) applies shallow autoencoder (SAE), deep autoencoder (DAE), variational autoencoder (VAE), and convolutional autoencoder (CAE) for dimensionality reduction. Then SVM, RF, and multi-layer perceptron (MLP) classification algorithms have been used.</p></caption><graphic xlink:href="41598_2020_63159_Fig2_HTML" id="d29e1794"/></fig></p>
    <p id="Par27">For marker profile, none of the autoencoders dominate across the datasets in terms of getting the best representation for classification. Also, the best classification algorithm varied according to the learned representation and to the dataset (Fig. <xref rid="Fig3" ref-type="fig">3</xref>). For abundance profile CAE,dominates over the other autoencoders with RF classifier across all the datasets (Fig. <xref rid="MOESM1" ref-type="media">S3</xref>).<fig id="Fig3"><label>Figure 3</label><caption><p>Disease prediction performance for different autoencoders based on marker profile (assessed with AUC). Classifiers used: support vector machine (SVM), random forest (RF), and multi-layer perceptron (MLP); Autoencoders used: shallow autoencoder (SAE), deep autoencoder (DAE), variational autoencoder (VAE), and convolutional autoencoder (CAE).</p></caption><graphic xlink:href="41598_2020_63159_Fig3_HTML" id="d29e1811"/></fig></p>
    <p id="Par28">We also directly trained MLP on the dataset without representation learning and compared the prediction performance with that of the traditional approach (the best between SVM and RF). It is shown that MLP performs better than MetAML in three datasetsEW-T2D, C-T2D, and Obesity, when marker profile is used (Fig. <xref rid="MOESM1" ref-type="media">S4</xref>). However, when abundance profile is used, the performance of MLP was worse than that of the traditional approach across all the datasets (Fig. <xref rid="MOESM1" ref-type="media">S5</xref>).</p>
    <p id="Par29">Furthermore, we compared running time of DeepMicro on marker profiles with a basic approach not using representation learning. For comparison, we tracked both training time and representation learning time. For each dataset, we tested the best performing representation learning model producing the highest AUC score (i.eSAE.for IBD and EW-T2D, DAE for Obesity and Colorectal, and CAE for C-T2D and Cirrhosis; Table <xref rid="MOESM1" ref-type="media">S1</xref>). We fixed the seed for random partitioning of the data, and applied the formerly used performance evaluation procedure where 5-fold cross-validation is conducted on the training set to obtain the best hyper-parameter with which the best model is trained on the whole training set and is evaluated on the test set (See Methods). The computing machine we used for timestamping is running on Ubuntu 18.04 and equipped with an Intel Core i9-9820X CPU (10 cores), 64 GB Memory, and a GPU of NVIDIA GTX 1080 Ti. We note that our implementation utilizes GPU when it learns representations and switches to CPU mode to exhaustively use multiple cores in a parallel way to find best hyper-parameters of the classifiers. Table <xref rid="Tab3" ref-type="table">3</xref> shows the benchmarking result on marker profile. It is worth noting that DeepMicro is 8X to 30X times faster than the basic approach (17X times faster on average). Even if MLP is excluded from the benchmarking because it requires heavy computation, DeepMicro is up to 5X times faster than the basic (2X times faster on average).<table-wrap id="Tab3"><label>Table 3</label><caption><p>Time benchmark for DeepMicro and basic approaches without representation learning (in sec).</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2">Method</th><th>IBD</th><th>EW-T2D</th><th>C-T2D</th><th>Obesity</th><th>Cirrhosis</th><th>Colorectal</th></tr></thead><tbody><tr><td rowspan="3">Basic approach</td><td><bold>SVM</bold><sup><bold>*</bold></sup></td><td>126</td><td>85</td><td>1705</td><td>711</td><td>777</td><td>187</td></tr><tr><td><bold>RF</bold></td><td>42</td><td>41</td><td>99</td><td>79</td><td>72</td><td>50</td></tr><tr><td><bold>MLP</bold></td><td>3,776</td><td>2,449</td><td>12,057</td><td>8,186</td><td>8,593</td><td>4,508</td></tr><tr><td colspan="2"><bold>Total elapsed</bold></td><td>3,943</td><td>2,575</td><td>13,861</td><td>8,976</td><td>9,442</td><td>4,745</td></tr><tr><td rowspan="4">DeepMicro</td><td><bold>RL</bold></td><td>74</td><td>194</td><td>554</td><td>113</td><td>521</td><td>215</td></tr><tr><td><bold>SVM</bold></td><td>2</td><td>2</td><td>8</td><td>8</td><td>17</td><td>2</td></tr><tr><td><bold>RF</bold></td><td>28</td><td>28</td><td>47</td><td>33</td><td>40</td><td>30</td></tr><tr><td><bold>MLP</bold></td><td>103</td><td>93</td><td>188</td><td>137</td><td>287</td><td>105</td></tr><tr><td colspan="2"><bold>Total elapsed</bold></td><td>207</td><td>317</td><td>798</td><td>291</td><td>864</td><td>352</td></tr></tbody></table><table-wrap-foot><p>*RL: Representation Learning; SVM: Support Vector Machine; RF: Random Forest; MLP: Multi-layer Perceptron.</p></table-wrap-foot></table-wrap></p>
  </sec>
  <sec id="Sec8" sec-type="discussion">
    <title>Discussion</title>
    <p id="Par30">We developed a deep learning framework transforming a high-dimensional microbiome profile into a low-dimensional representation and building classification models based on the learned representation. At the beginning of this study, the main goal was to reduce dimensions as strain-level marker profile has too many dimensions to handle, expecting that noisy and unnecessary information fades out and the refined representation becomes tractable for downstream prediction. Firstly, we tested PCA on marker profile and it showed a slight improvement in prediction performance for C-T2D and Obesity but not for the others. The preliminary result indicates that either some of the meaningful information was dropped or noisy information still remains. To learn meaningful feature representations, we trained various autoencoders on microbiome profiles. Our intuition behind the autoencoders was that the learned representation should keep essential information in a condensed way because autoencoders are forced to prioritize which properties of the input should be encoded during the learning process. We found that although the most appropriate autoencoder usually allows for better representation that in turn results in better prediction performance, what kind of autoencoder is appropriate highly depends on problem complexity and intrinsic properties of the data.</p>
    <p id="Par31">In the previous study, it has been shown that adding healthy controls of the other datasets could improve prediction performance assessed by AUC<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. To check if this finding can be reproduced, for each dataset, we added control samples of the other datasets only into the training set and kept the test set the same as before. Figure <xref rid="MOESM1" ref-type="media">S6</xref> shows the difference between the best performing models built with and without additional controls. In general, prediction performance dropped (on average by 0.037) once negative (control) samples are introduced to the training set across the datasets in almost all approaches except only a few cases (Fig. <xref rid="MOESM1" ref-type="media">S6</xref>). In contrast to the previous study, the result indicates that the insertion of only negative samples into the training set may not help to improve the classification models, and a possible explanation might be that changes in the models rarely contribute to improving the classification of positive samples<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>. Interestingly, if we added negative samples into the whole data set before split it into training and test set, we usually observed improvements in prediction performance. However, we found that these improvements are trivial because introducing negative samples into the test set easily reduces false positive rate (as the denominator of false positive rate formula is increased), resulting in higher AUC scores.</p>
    <p id="Par32">Even though adding negative samples might not be helpful for a better model, it does not mean that additional samples are meaningless. We argue that more samples can improve prediction performance, especially when a well-balanced set of samples is augmented. To test this argument, we gradually increased the proportion of the training set and observed how prediction performance changed over the training sets of different sizes. Generally, improved prediction performance has been observed as more data of both positive and negative samples are included (Fig. <xref rid="MOESM1" ref-type="media">S7</xref>). With the continued availability of large samples of microbiome data, the deep representation learning framework is expected to become increasingly effective for both condensed representation of the original data and also downstream prediction based on the deep representation.</p>
    <p id="Par33">DeepMicro is publicly available software which offers cutting-edge deep learning techniques for learning meaningful representations from the given data. Researchers can apply DeepMicro to their high-dimensional microbiome data to obtain a robust low-dimensional representation for the subsequent supervised or unsupervised learning. For predictive problems increasingly studied with microbiome data such as drug response prediction, forensic human identification, and food allergy prediction, deep representation learning might be useful in terms of boosting the model performance. Moreover, it might be worthwhile to use the learned representation for clustering analysis. The distance between data points in the latent space can be a basis for clustering microbiome samples and it could help capture the shared characteristics within a group which are difficult to be identified in the original data space. DeepMicro has been used to deal with microbiome data but it is not limited to a specific type of data and its application can be extended to various omics data, such as genome and proteome data.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec9">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="41598_2020_63159_MOESM1_ESM.pdf">
            <caption>
              <p>Supplementary Information.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p>is available for this paper at 10.1038/s41598-020-63159-5.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>This work is partially supported by the funding from Data and Decisions Destination Area at Virginia Tech. Also, this publication is supported by Virginia Tech’s Open Access Subvention Fund. Lastly, we thank Dr. Bert Huang for the constructive discussion.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>M.O. designed the study, collected data, implemented the software, and performed experiments. M.O. and L.Z. interpreted the results and wrote the manuscript. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>All data and codes are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/minoh0201/DeepMicro">https://github.com/minoh0201/DeepMicro</ext-link>.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par34">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cho</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Blaser</surname>
            <given-names>MJ</given-names>
          </name>
        </person-group>
        <article-title>The human microbiome: at the interface of health and disease</article-title>
        <source>Nature Reviews Genetics</source>
        <year>2012</year>
        <volume>13</volume>
        <fpage>260</fpage>
        <pub-id pub-id-type="doi">10.1038/nrg3182</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Huttenhower</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Structure, function and diversity of the healthy human microbiome</article-title>
        <source>nature</source>
        <year>2012</year>
        <volume>486</volume>
        <fpage>207</fpage>
        <pub-id pub-id-type="doi">10.1038/nature11234</pub-id>
        <pub-id pub-id-type="pmid">22699609</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>McQuade</surname>
            <given-names>JL</given-names>
          </name>
          <name>
            <surname>Daniel</surname>
            <given-names>CR</given-names>
          </name>
          <name>
            <surname>Helmink</surname>
            <given-names>BA</given-names>
          </name>
          <name>
            <surname>Wargo</surname>
            <given-names>JA</given-names>
          </name>
        </person-group>
        <article-title>Modulating the microbiome to improve therapeutic response in cancer</article-title>
        <source>The Lancet Oncology</source>
        <year>2019</year>
        <volume>20</volume>
        <fpage>e77</fpage>
        <lpage>e91</lpage>
        <pub-id pub-id-type="doi">10.1016/S1470-2045(18)30952-5</pub-id>
        <pub-id pub-id-type="pmid">30712808</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Eloe-Fadrosh</surname>
            <given-names>EA</given-names>
          </name>
          <name>
            <surname>Rasko</surname>
            <given-names>DA</given-names>
          </name>
        </person-group>
        <article-title>The human microbiome: from symbiosis to pathogenesis</article-title>
        <source>Annual review of medicine</source>
        <year>2013</year>
        <volume>64</volume>
        <fpage>145</fpage>
        <lpage>163</lpage>
        <pub-id pub-id-type="doi">10.1146/annurev-med-010312-133513</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hamady</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Knight</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Microbial community profiling for human microbiome projects: tools, techniques, and challenges</article-title>
        <source>Genome research</source>
        <year>2009</year>
        <volume>19</volume>
        <fpage>1141</fpage>
        <lpage>1152</lpage>
        <pub-id pub-id-type="doi">10.1101/gr.085464.108</pub-id>
        <pub-id pub-id-type="pmid">19383763</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Scholz</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Strain-level microbial epidemiology and population genomics from shotgun metagenomics</article-title>
        <source>Nature methods</source>
        <year>2016</year>
        <volume>13</volume>
        <fpage>435</fpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.3802</pub-id>
        <pub-id pub-id-type="pmid">26999001</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Truong</surname>
            <given-names>DT</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>MetaPhlAn2 for enhanced metagenomic taxonomic profiling</article-title>
        <source>Nature methods</source>
        <year>2015</year>
        <volume>12</volume>
        <fpage>902</fpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.3589</pub-id>
        <pub-id pub-id-type="pmid">26418763</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kramer</surname>
            <given-names>MA</given-names>
          </name>
        </person-group>
        <article-title>Nonlinear principal component analysis using autoassociative neural networks</article-title>
        <source>AIChE journal</source>
        <year>1991</year>
        <volume>37</volume>
        <fpage>233</fpage>
        <lpage>243</lpage>
        <pub-id pub-id-type="doi">10.1002/aic.690370209</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Min</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Yoon</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Deep learning in bioinformatics</article-title>
        <source>Briefings in bioinformatics</source>
        <year>2017</year>
        <volume>18</volume>
        <fpage>851</fpage>
        <lpage>869</lpage>
        <?supplied-pmid 27473064?>
        <pub-id pub-id-type="pmid">27473064</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">Nguyen, T. H., Chevaleyre, Y., Prifti, E., Sokolovska, N. &amp; Zucker, J.-D. Deep learning for metagenomic data: using 2d embeddings and convolutional neural networks. <italic>arXiv preprint arXiv:1712.00244</italic> (2017).</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Nguyen, T. H., Prifti, E., Chevaleyre, Y., Sokolovska, N. &amp; Zucker, J.-D. Disease classification in metagenomics with 2d embeddings and deep learning. <italic>arXiv preprint arXiv:1806.09046</italic> (2018).</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pasolli</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Truong</surname>
            <given-names>DT</given-names>
          </name>
          <name>
            <surname>Malik</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Waldron</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Segata</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Machine learning meta-analysis of large metagenomic datasets: tools and biological insights</article-title>
        <source>PLoS computational biology</source>
        <year>2016</year>
        <volume>12</volume>
        <fpage>e1004977</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1004977</pub-id>
        <pub-id pub-id-type="pmid">27400279</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cawley</surname>
            <given-names>GC</given-names>
          </name>
          <name>
            <surname>Talbot</surname>
            <given-names>NL</given-names>
          </name>
        </person-group>
        <article-title>On over-fitting in model selection and subsequent selection bias in performance evaluation</article-title>
        <source>Journal of Machine Learning Research</source>
        <year>2010</year>
        <volume>11</volume>
        <fpage>2079</fpage>
        <lpage>2107</lpage>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Varma</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Simon</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Bias in error estimation when using cross-validation for model selection</article-title>
        <source>BMC bioinformatics</source>
        <year>2006</year>
        <volume>7</volume>
        <fpage>91</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-7-91</pub-id>
        <pub-id pub-id-type="pmid">16504092</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Qin</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A human gut microbial gene catalogue established by metagenomic sequencing</article-title>
        <source>nature</source>
        <year>2010</year>
        <volume>464</volume>
        <fpage>59</fpage>
        <pub-id pub-id-type="doi">10.1038/nature08821</pub-id>
        <pub-id pub-id-type="pmid">20203603</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Karlsson</surname>
            <given-names>FH</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Gut metagenome in European women with normal, impaired and diabetic glucose control</article-title>
        <source>Nature</source>
        <year>2013</year>
        <volume>498</volume>
        <fpage>99</fpage>
        <pub-id pub-id-type="doi">10.1038/nature12198</pub-id>
        <pub-id pub-id-type="pmid">23719380</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Qin</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A metagenome-wide association study of gut microbiota in type 2 diabetes</article-title>
        <source>Nature</source>
        <year>2012</year>
        <volume>490</volume>
        <fpage>55</fpage>
        <pub-id pub-id-type="doi">10.1038/nature11450</pub-id>
        <pub-id pub-id-type="pmid">23023125</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Le Chatelier</surname>
            <given-names>E</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Richness of human gut microbiome correlates with metabolic markers</article-title>
        <source>Nature</source>
        <year>2013</year>
        <volume>500</volume>
        <fpage>541</fpage>
        <pub-id pub-id-type="doi">10.1038/nature12506</pub-id>
        <pub-id pub-id-type="pmid">23985870</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Qin</surname>
            <given-names>N</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Alterations of the human gut microbiome in liver cirrhosis</article-title>
        <source>Nature</source>
        <year>2014</year>
        <volume>513</volume>
        <fpage>59</fpage>
        <pub-id pub-id-type="doi">10.1038/nature13568</pub-id>
        <pub-id pub-id-type="pmid">25079328</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zeller</surname>
            <given-names>G</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Potential of fecal microbiota for early‐stage detection of colorectal cancer</article-title>
        <source>Molecular systems biology</source>
        <year>2014</year>
        <volume>10</volume>
        <fpage>766</fpage>
        <pub-id pub-id-type="doi">10.15252/msb.20145645</pub-id>
        <pub-id pub-id-type="pmid">25432777</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Glorot, X. &amp; Bengio, Y. in <italic>Proceedings of the thirteenth international conference on artificial intelligence and statistics</italic>. 249-256.</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Kingma, D. P. &amp; Welling, M. Auto-encoding variational bayes. <italic>arXiv preprint arXiv:1312.6114</italic> (2013).</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Qiao</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Discriminatively boosted image clustering with fully convolutional auto-encoders</article-title>
        <source>Pattern Recognition</source>
        <year>2018</year>
        <volume>83</volume>
        <fpage>161</fpage>
        <lpage>173</lpage>
        <pub-id pub-id-type="doi">10.1016/j.patcog.2018.05.019</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Krizhevsky, A., Sutskever, I. &amp; Hinton, G. E. in <italic>Advances in neural information processing systems</italic>. 1097-1105.</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Kingma, D. P. &amp; Ba, J. Adam: A method for stochastic optimization. <italic>arXiv preprint arXiv:1412.6980</italic> (2014).</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cortes</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Vapnik</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Support-vector networks</article-title>
        <source>Machine learning</source>
        <year>1995</year>
        <volume>20</volume>
        <fpage>273</fpage>
        <lpage>297</lpage>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pearson</surname>
            <given-names>KL</given-names>
            <suffix>III</suffix>
          </name>
        </person-group>
        <article-title>On lines and planes of closest fit to systems of points in space</article-title>
        <source>The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</source>
        <year>1901</year>
        <volume>2</volume>
        <fpage>559</fpage>
        <lpage>572</lpage>
        <pub-id pub-id-type="doi">10.1080/14786440109462720</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Bingham, E. &amp; Mannila, H. in <italic>Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining</italic>. 245-250 (ACM).</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Dasgupta, S. Experiments with random projection. <italic>arXiv preprint arXiv:1301.3849</italic> (2013).</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dasgupta</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gupta</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>An elementary proof of the Johnson-Lindenstrauss lemma</article-title>
        <source>International Computer Science Institute, Technical Report</source>
        <year>1999</year>
        <volume>22</volume>
        <fpage>1</fpage>
        <lpage>5</lpage>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Saito</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Rehmsmeier</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets</article-title>
        <source>PloS one</source>
        <year>2015</year>
        <volume>10</volume>
        <fpage>e0118432</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0118432</pub-id>
        <pub-id pub-id-type="pmid">25738806</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mazurowski</surname>
            <given-names>MA</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Training neural network classifiers for medical decision making: The effects of imbalanced datasets on classification performance</article-title>
        <source>Neural networks</source>
        <year>2008</year>
        <volume>21</volume>
        <fpage>427</fpage>
        <lpage>436</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neunet.2007.12.031</pub-id>
        <pub-id pub-id-type="pmid">18272329</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
