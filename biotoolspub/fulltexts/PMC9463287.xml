<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Int J Comput Assist Radiol Surg</journal-id>
    <journal-id journal-id-type="iso-abbrev">Int J Comput Assist Radiol Surg</journal-id>
    <journal-title-group>
      <journal-title>International Journal of Computer Assisted Radiology and Surgery</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1861-6410</issn>
    <issn pub-type="epub">1861-6429</issn>
    <publisher>
      <publisher-name>Springer International Publishing</publisher-name>
      <publisher-loc>Cham</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9463287</article-id>
    <article-id pub-id-type="pmid">35460019</article-id>
    <article-id pub-id-type="publisher-id">2619</article-id>
    <article-id pub-id-type="doi">10.1007/s11548-022-02619-x</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Explainability of deep neural networks for MRI analysis of brain tumors</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-8630-9046</contrib-id>
        <name>
          <surname>Zeineldin</surname>
          <given-names>Ramy A.</given-names>
        </name>
        <address>
          <email>Ramy.Zeineldin@Reutlingen-University.DE</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-0474-4723</contrib-id>
        <name>
          <surname>Karar</surname>
          <given-names>Mohamed E.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-2643-5646</contrib-id>
        <name>
          <surname>Elshaer</surname>
          <given-names>Ziad</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-3677-8258</contrib-id>
        <name>
          <surname>Coburger</surname>
          <given-names>·Jan</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-8358-1813</contrib-id>
        <name>
          <surname>Wirtz</surname>
          <given-names>Christian R.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7118-4730</contrib-id>
        <name>
          <surname>Burgert</surname>
          <given-names>Oliver</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5239-5305</contrib-id>
        <name>
          <surname>Mathis-Ullrich</surname>
          <given-names>Franziska</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.7892.4</institution-id><institution-id institution-id-type="ISNI">0000 0001 0075 5874</institution-id><institution>Institute for Anthropomatics and Robotics, </institution><institution>Karlsruhe Institute of Technology (KIT), </institution></institution-wrap>76131 Karlsruhe, Germany </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.434088.3</institution-id><institution-id institution-id-type="ISNI">0000 0001 0666 4420</institution-id><institution>Research Group Computer Assisted Medicine (CaMed), </institution><institution>Reutlingen University, </institution></institution-wrap>72762 Reutlingen, Germany </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.411775.1</institution-id><institution-id institution-id-type="ISNI">0000 0004 0621 4712</institution-id><institution>Faculty of Electronic Engineering (FEE), </institution><institution>Menoufia University, </institution></institution-wrap>Menouf, 32952 Egypt </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.6582.9</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 9748</institution-id><institution>Department of Neurosurgery, </institution><institution>University of Ulm, </institution></institution-wrap>89312 Günzburg, Germany </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>23</day>
      <month>4</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>23</day>
      <month>4</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2022</year>
    </pub-date>
    <volume>17</volume>
    <issue>9</issue>
    <fpage>1673</fpage>
    <lpage>1683</lpage>
    <history>
      <date date-type="received">
        <day>10</day>
        <month>1</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>23</day>
        <month>3</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Purpose</title>
        <p id="Par1">Artificial intelligence (AI), in particular deep neural networks, has achieved remarkable results for medical image analysis in several applications. Yet the lack of explainability of deep neural models is considered the principal restriction before applying these methods in clinical practice.
</p>
      </sec>
      <sec>
        <title>Methods</title>
        <p id="Par2">In this study, we propose a NeuroXAI framework for explainable AI of deep learning networks to increase the trust of medical experts. NeuroXAI implements seven state-of-the-art explanation methods providing visualization maps to help make deep learning models transparent.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par3">NeuroXAI has been applied to two applications of the most widely investigated problems in brain imaging analysis, i.e., image classification and segmentation using magnetic resonance (MR) modality. Visual attention maps of multiple XAI methods have been generated and compared for both applications. Another experiment demonstrated that NeuroXAI can provide information flow visualization on internal layers of a segmentation CNN.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p id="Par4">Due to its open architecture, ease of implementation, and scalability to new XAI methods, NeuroXAI could be utilized to assist radiologists and medical professionals in the detection and diagnosis of brain tumors in the clinical routine of cancer patients. The code of NeuroXAI is publicly accessible at <ext-link ext-link-type="uri" xlink:href="https://github.com/razeineldin/NeuroXAI">https://github.com/razeineldin/NeuroXAI</ext-link>.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keyword</title>
      <kwd>Brain glioma</kwd>
      <kwd>Computer-aided diagnosis</kwd>
      <kwd>Convolutional neural networks</kwd>
      <kwd>Explainable AI</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100007948</institution-id>
            <institution>Deutscher Akademischer Austausch Dienst Kairo</institution>
          </institution-wrap>
        </funding-source>
        <award-id>91705803</award-id>
        <principal-award-recipient>
          <name>
            <surname>Zeineldin</surname>
            <given-names>Ramy A.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Hochschule Reutlingen / Reutlingen University (3399)</institution>
        </funding-source>
      </award-group>
      <open-access>
        <p>Open Access funding enabled and organized by Projekt DEAL.</p>
      </open-access>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© CARS 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par5">Brain and other nervous system tumors (ONS), including the glioblastoma (GBM), are among the leading cause of cancer death in adults [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR2">2</xref>]. Brain cancer, explicitly malignant and benign, represents the second major source of cancer-related deaths in young adults and children [<xref ref-type="bibr" rid="CR1">1</xref>]. Common treatment options for brain cancer include surgical intervention, radiotherapy, and chemotherapy [<xref ref-type="bibr" rid="CR3">3</xref>]. Nevertheless, physically localizing and resecting pathological targets by surgery is almost impossible, owing to the difficulty in visually distinguishing brain tumors from surrounding brain parenchyma [<xref ref-type="bibr" rid="CR4">4</xref>].</p>
    <p id="Par6">In practice, magnetic resonance imaging (MRI) can help physicians detect brain tumors by providing soft tissue imaging allowing improved tumor localization and boundary definition [<xref ref-type="bibr" rid="CR5">5</xref>]. By varying the weightage of image contrast, the anatomy of the human brain, blood–brain barrier, and brain tumor boundaries could be detected and visualized. Multi-parametric MRI includes native T1-weighted (T1W), post-contrast T1-weighted (T1Gd), T2-weighted (T2W), and T2 fluid-attenuated inversion recovery (FLAIR). However, interpreting these multi-modal images can be highly challenging for physicians to analyze and provide diagnosis, make intraoperative decisions in a short time as wrong remedy procedures could lead to patient discomfort physically and financially [<xref ref-type="bibr" rid="CR3">3</xref>].</p>
    <p id="Par7">Computer-aided diagnosis systems (CADs) aid in these cases to detect brain tumors using multimodal MRI scans, minimizing these inconveniences [<xref ref-type="bibr" rid="CR6">6</xref>]. CADs are computer systems that assist radiologists and physicians in the interpretation, analysis, and evaluation of MRI data comprehensively in a short time, e.g., brain tumor segmentation and predicting histological grades of intracranial neoplasms [<xref ref-type="bibr" rid="CR7">7</xref>, <xref ref-type="bibr" rid="CR8">8</xref>].</p>
    <p id="Par8">Recent developments in the field of artificial intelligence (AI), especially deep learning (DL), have led to a renewed interest in analyzing brain cancer, its causes, and its various development phases [<xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR10">10</xref>]. In medical applications, there are typically fewer data samples with higher complexity compared with other applications. Among the numerous segmentation techniques, convolutional neural networks (CNN) have attracted much attention for medical image understanding tasks like image classification or multimodal tumor segmentation. For instance, U-Net variants [<xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR12">12</xref>], which use symmetric encoder–decoder architecture, have performed state-of-the-art results for medical image segmentation. Similarly, several publications have appeared in recent years for accurate medical classification including [<xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR13">13</xref>–<xref ref-type="bibr" rid="CR16">16</xref>]. Hence, employing DL technologies in CADs could potentially expand physicians’ capabilities assisting in perioperative evaluation of intracranial pathologies and enhancing the efficiency of postoperative follow-up [<xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR10">10</xref>].</p>
    <p id="Par9">Nevertheless, the introduction of DL techniques in the clinical environment is still limited due to some restrictions [<xref ref-type="bibr" rid="CR17">17</xref>]. The most significant one is that DL strategies consider only the input images and the output results, without any transparency of the underlying information flow in the network internal layers. In sensitive applications such as brain imaging applications, it is crucial to understand the reason behind the network prediction to ensure that the model provides the correct estimation. Accordingly, explainable AI (XAI) has gained a substantial interest to explore the “black box” DL networks in the medical field [<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR18">18</xref>]. XAI methods allow researchers, developers, and end-users to obtain transparent DL models that can describe their decisions to humans in an understandable manner. For medical end-users, the demand for explainability is increasing to create their trust in DL techniques and to encourage them to utilize these systems for assisting the clinical procedures. Moreover, the European Union data protection law, titled General Data Protection Regulation (GDPR), imposes the explanation as a requirement for automated learning systems before being used with patients clinically [<xref ref-type="bibr" rid="CR19">19</xref>].</p>
  </sec>
  <sec id="Sec2">
    <title>Related work</title>
    <p id="Par10">Generally, XAI techniques in medical imaging can be grouped into perturbation-based or gradient-based approaches. Perturbation-based methods investigate the network by changing the input features and measuring the impact on the output estimations by a forward training of the model. Some examples include LIME [<xref ref-type="bibr" rid="CR20">20</xref>], SHAP [<xref ref-type="bibr" rid="CR20">20</xref>], deconvolution [<xref ref-type="bibr" rid="CR21">21</xref>], and occlusion [<xref ref-type="bibr" rid="CR21">21</xref>]. Gradient-based XAI methods have been widely adopted to provide feature attribution maps by calculating the partial derivative of the output predictions through every layer of the neural network with respect to (w.r.t) the input images. These techniques have the advantage of being post hoc, meaning that they are applied after the training phase of the DL model avoiding the accuracy vs explainability trade-off. In addition, they are usually fast compared with perturbation approaches since their runtime does not depend on the number of input features. A number of publications have been reported for back-propagating approaches such as Vanilla gradient [<xref ref-type="bibr" rid="CR22">22</xref>], guided backpropagation [<xref ref-type="bibr" rid="CR23">23</xref>], integrated gradients [<xref ref-type="bibr" rid="CR24">24</xref>], guided integrated gradients [<xref ref-type="bibr" rid="CR25">25</xref>], SmoothGrad [<xref ref-type="bibr" rid="CR26">26</xref>], Grad-CAM [<xref ref-type="bibr" rid="CR27">27</xref>], and guided Grad-CAM [<xref ref-type="bibr" rid="CR27">27</xref>]. Several XAI methods have been previously proposed for natural image tasks, while little attention has been paid to explain brain imaging applications [<xref ref-type="bibr" rid="CR18">18</xref>]. For brain cancer classification, Windisch et al. [<xref ref-type="bibr" rid="CR28">28</xref>] applied 2D Grad-CAM to generate heatmaps indicating which areas of the input MRI made the classifier decide on the category of the existence of a brain tumor. Similarly, 2D Grad-CAM was used in [<xref ref-type="bibr" rid="CR29">29</xref>] to evaluate the performance of three DL models in brain tumor classification. The key limitation of these studies is that experiments were concluded on 2D MRI slices without investigating the model on 3D medical applications.</p>
    <p id="Par11">Explainable learning has been applied as well for brain glioma segmentation [<xref ref-type="bibr" rid="CR30">30</xref>, <xref ref-type="bibr" rid="CR31">31</xref>]. In [<xref ref-type="bibr" rid="CR30">30</xref>], 2D Grad-CAM was applied to extract explanations for the deep neural networks for brain tumors identification. It suffers from the same limitations associated with the previous classification explanation methods of being 2D only. Another approach was introduced in [<xref ref-type="bibr" rid="CR31">31</xref>] that extends class activation mapping (CAM) [<xref ref-type="bibr" rid="CR32">32</xref>] by generating 3D heatmaps to visualize the importance of segmentation output. Despite being highly class-discriminative, it made a trade-off between the model complexity and the performance to make CNNs transparent.</p>
    <p id="Par12">In this paper, our main goal is to develop a new NeuroXAI framework for obtaining 2D and 3D explainable sensitivity maps to assist clinicians to understand and trust the performance of DL algorithms in clinical procedures. Hence, the contribution of this study has threefold:<list list-type="order"><list-item><p id="Par13">A new explainability framework, namely NeuroXAI, is proposed to make the current DL models for brain imaging research interpretable without any architecture modification or performance degradation.</p></list-item><list-item><p id="Par14">NeuroXAI included seven state-of-the-art backpropagating XAI techniques for generating 2D and 3D visual interpretations of CNN output.</p></list-item><list-item><p id="Par15">A comprehensive evaluation of the proposed framework demonstrated promising explanation results for two showcases of MRI classification and segmentation of brain tumors.</p></list-item></list></p>
  </sec>
  <sec id="Sec3">
    <title>Methods</title>
    <sec id="Sec4">
      <title>NeuroXAI</title>
      <p id="Par16">The overall pipeline of NeuroXAI is shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. It consists of two main parts, which are a deep neural network to achieve processing tasks of the brain images and an explanation generator. Given brain MRI volumes as input, the images are forward propagated through the CNN generating convolutional feature maps and then through task-specific computations to obtain the desired output (e.g., category prediction in case of classification and/or tumor segmentation). Afterward, the network output is presented to medical professionals to assess the findings and request an explanation if necessary. Finally, visual explanation maps are provided by the explainability part to interpret the results of applied deep neural networks. This can be achieved using state-of-the-art XAI methods.<fig id="Fig1"><label>Fig. 1</label><caption><p>Pipeline of the proposed NeuroXAI framework</p></caption><graphic xlink:href="11548_2022_2619_Fig1_HTML" id="MO1"/></fig></p>
      <p id="Par17">While the utilized explanation methods were primarily proposed for interpreting deep image classification, our proposed framework provides an adaption approach to medical image segmentation as well. Further, NeuroXAI converts the segmentation task into a multi-label classification task. This is achieved through global average pooling for each class on the output prediction layer. Therefore, our NeuroXAI offers state-of-the-art XAI methods for classification and segmentation for both 2D and 3D medical image data.</p>
    </sec>
    <sec id="Sec5">
      <title>Vanilla gradient</title>
      <p id="Par18">Vanilla gradient (VG) [<xref ref-type="bibr" rid="CR22">22</xref>] is the simplest form of visualizing regions of the image that contributes most to the classification output of the neural network. This computes the saliency map by making a single backward pass of the activation of the output class after a forward pass over the network, which can be defined as computing the VG of the output activation w.r.t the input image. Let <italic>P</italic><sub><italic>c</italic></sub><italic>(Im)</italic> be the prediction of class <italic>c</italic>, computed by the classification layer of the CNN for an input image <italic>X</italic><sup><italic>I</italic></sup>. The objective of Vanilla gradient is to find the L<sub>2</sub>-regularized image, which has the maximum <italic>P</italic><sub><italic>c</italic></sub>, while <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\uplambda $$\end{document}</tex-math><mml:math id="M2"><mml:mi mathvariant="normal">λ</mml:mi></mml:math><inline-graphic xlink:href="11548_2022_2619_Article_IEq1.gif"/></alternatives></inline-formula> is the regularization term:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$VG={argmax}_{c}{P}_{c}\left({X}^{I}\right)-\lambda {\Vert {X}^{I}\Vert }_{2}^{2}$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:mrow><mml:mi>V</mml:mi><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="italic">argmax</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msub><mml:msub><mml:mi>P</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mfenced close=")" open="("><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>I</mml:mi></mml:msup></mml:mfenced><mml:mo>-</mml:mo><mml:mi>λ</mml:mi><mml:msubsup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>I</mml:mi></mml:msup><mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math><graphic xlink:href="11548_2022_2619_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p>
    </sec>
    <sec id="Sec6">
      <title>Guided backpropagation</title>
      <p id="Par19">An alternative way of calculating the gradient of a particular output w.r.t the input is by using guided backpropagation (GBP) [<xref ref-type="bibr" rid="CR23">23</xref>]. The GBP is a new variant of the deconvolution approach [<xref ref-type="bibr" rid="CR21">21</xref>] for visualizing the region of interest of an image that most activates a given class. Suppose <italic>F</italic> be the output of a convolutional layer <italic>l</italic> from <italic>L</italic> layers in a multi-layer CNN, and <italic>B</italic> denotes the resultant image from backpropagation:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${B}_{i}^{l}=\left({F}_{i}^{l}&gt;0\right).\left({B}_{i}^{l+1}&gt;0\right).{B}_{i}^{l+1}$$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mrow><mml:msubsup><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mfenced close=")" open="("><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mfenced><mml:mo>.</mml:mo><mml:mfenced close=")" open="("><mml:msubsup><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mfenced><mml:mo>.</mml:mo><mml:msubsup><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math><graphic xlink:href="11548_2022_2619_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${B}_{i}^{l+1}=\frac{\partial {F}_{i}^{L}}{\partial {F}_{i}^{l+1}}$$\end{document}</tex-math><mml:math id="M8" display="block"><mml:mrow><mml:msubsup><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>L</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="11548_2022_2619_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
    </sec>
    <sec id="Sec7">
      <title>Integrated gradients</title>
      <p id="Par20">Sundararajan et al. [<xref ref-type="bibr" rid="CR24">24</xref>] introduced integrated gradients (IG) to mitigate the saturation problem of gradient-based methods. Let a function <italic>F</italic>: <italic>R</italic><sup>n</sup> → [0, 1] denote a deep neural network which has <italic>X</italic><sup><italic>I</italic></sup> = <italic>γ (α</italic> = <italic>1)</italic> ∈ <italic>R</italic><sup><italic>n</italic></sup> as the input image, while <italic>X</italic><sup><italic>B</italic></sup> = <italic>γ (α</italic> = <italic>0)</italic> ∈ <italic>R</italic><sup><italic>n</italic></sup> represents the baseline. The baseline is simply a black image with all values set to zeros. The IG can be computed by accumulating the gradients at all points on the straight-line path from the baseline <italic>X</italic><sup><italic>B</italic></sup> to the input image <italic>X</italic><sup><italic>I</italic></sup>:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${IG}_{i}(x)={\int }_{\alpha =0}^{1}\frac{\partial F(\gamma (\alpha ))}{\partial {\gamma }_{i}(\alpha )}\frac{\partial {\gamma }_{i}(\alpha )}{\partial \alpha }d\alpha $$\end{document}</tex-math><mml:math id="M10" display="block"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="italic">IG</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mi>γ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mi>γ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:mi>α</mml:mi></mml:mrow></mml:mfrac><mml:mi>d</mml:mi><mml:mi>α</mml:mi></mml:mrow></mml:math><graphic xlink:href="11548_2022_2619_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par21">Here, <italic>i</italic> is the feature for the input image, whereas <italic>α</italic> represents interpolation constant to perturb image features.</p>
    </sec>
    <sec id="Sec8">
      <title>Guided integrated gradients</title>
      <p id="Par22">Kapishnikov et al. [<xref ref-type="bibr" rid="CR25">25</xref>] proposed guided integrated gradients (GIG) as an adaption of the attribution path based on the input image, baseline, and the deep model to be explained. Similar to IG, the GIG calculates the gradients on the path (<italic>c</italic>) which starts at the baseline (<italic>X</italic><sup><italic>B</italic></sup>) and ends at the input being explained (<italic>X</italic><sup><italic>I</italic></sup>). However, the GIG path (<italic>c</italic>) is determined at every step as opposed to the fixed direction of the IG. This means that GIG finds a subset of features (<italic>S</italic>) that have the least importance among all features toward the input image. Mathematically,<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${GIG}_{i}\left({X}^{B},{X}^{I},F\right)=\frac{\partial {\gamma }_{i}^{F}\left(\alpha \right)}{\partial \alpha }=\left\{\begin{array}{c}{x}_{i}^{I}-{x}_{i}^{B}, if i \epsilon S,\\ 0 , otherwise.\end{array}\right.$$\end{document}</tex-math><mml:math id="M12" display="block"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="italic">GIG</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mfenced close=")" open="("><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>B</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>I</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mi>F</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:msubsup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>F</mml:mi></mml:msubsup><mml:mfenced close=")" open="("><mml:mi>α</mml:mi></mml:mfenced></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:mi>α</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>I</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>B</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>ϵ</mml:mi><mml:mi>S</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow/><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="11548_2022_2619_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S={argmin}_{i}\left(Y\right)$$\end{document}</tex-math><mml:math id="M14" display="block"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="italic">argmin</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mfenced close=")" open="("><mml:mi>Y</mml:mi></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="11548_2022_2619_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${y}_{i}=\left\{\begin{array}{c}\left|\frac{\partial F(x)}{\partial {x}_{i}}\right|, if i \epsilon \left\{j|{x}_{j}\ne {x}_{j}^{I}\right\}\\ \infty , otherwise.\end{array}\right.$$\end{document}</tex-math><mml:math id="M16" display="block"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfenced close="|" open="|"><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mfenced><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>ϵ</mml:mi><mml:mfenced close="}" open="{"><mml:mrow><mml:mi>j</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>≠</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mi>I</mml:mi></mml:msubsup></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow/><mml:mi>∞</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="11548_2022_2619_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula></p>
    </sec>
    <sec id="Sec9">
      <title>SmoothGrad</title>
      <p id="Par23">Smilkov et al. [<xref ref-type="bibr" rid="CR26">26</xref>] presented an improvement for the common problem of gradient-based methods. SmoothGrad [<xref ref-type="bibr" rid="CR26">26</xref>] solved this problem by providing visually sharpened sensitivity maps. It computes the gradient over multiple samples surrounding the input <italic>X</italic><sup><italic>I</italic></sup>, and the average is calculated after adding Gaussian noise. More formally,<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overline{{M }_{c}}({X}^{I})=\frac{1}{n}\sum_{1}^{n}{M}_{c}({X}^{I}+\fancyscript{g}(0,{\sigma }^{2}))$$\end{document}</tex-math><mml:math id="M18" display="block"><mml:mrow><mml:mover><mml:msub><mml:mi>M</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>I</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mi>M</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>I</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="script">g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic xlink:href="11548_2022_2619_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq2"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{M}}_{{c}}({{X}}^{{I}})$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>I</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="11548_2022_2619_Article_IEq2.gif"/></alternatives></inline-formula> is the original sensitivity map, n is the number of samples, and <inline-formula id="IEq3"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\fancyscript{g}(0,{\sigma }^{2})$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:mi mathvariant="script">g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="11548_2022_2619_Article_IEq3.gif"/></alternatives></inline-formula> denotes Gaussian noise with variance <inline-formula id="IEq4"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\sigma }^{2}$$\end{document}</tex-math><mml:math id="M24"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="11548_2022_2619_Article_IEq4.gif"/></alternatives></inline-formula>. In general, <inline-formula id="IEq5"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{M}}_{{c}}({{X}}^{{I}})$$\end{document}</tex-math><mml:math id="M26"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>I</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="11548_2022_2619_Article_IEq5.gif"/></alternatives></inline-formula> can be any gradient-based visualization method, such as explanation methods in the previous sub-sections.</p>
    </sec>
    <sec id="Sec10">
      <title>Grad-CAM</title>
      <p id="Par24">The authors in [<xref ref-type="bibr" rid="CR27">27</xref>] extended the class activation mapping (CAM) visualization technique to a wide variety of CNNs. The proposed gradient CAM (GCAM) produces visual explanations without re-training or modifications in the model architecture. The gradient of any target class <italic>c</italic> is first computed, and the activation feature map <italic>M</italic> of a specific layer <italic>l</italic> is globally averaged over the width, height, and depth dimensions. Then, the class-discriminative heatmap of GCAM is obtained using a weighted combination of these activation maps, followed by the ReLU function. Here, <inline-formula id="IEq6"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\alpha }_{l}^{c}$$\end{document}</tex-math><mml:math id="M28"><mml:msubsup><mml:mi>α</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="11548_2022_2619_Article_IEq6.gif"/></alternatives></inline-formula> denotes the neuron importance weights.<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${GCAM}_{l}^{c}=ReLU(\sum_{l}{\alpha }_{l}^{c}{M}^{l})$$\end{document}</tex-math><mml:math id="M30" display="block"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="italic">GCAM</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>l</mml:mi></mml:munder><mml:msubsup><mml:mi>α</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic xlink:href="11548_2022_2619_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\alpha }_{l}^{c}=\frac{1}{N}\sum_{x}\sum_{y}\sum_{z}\frac{\partial {y}^{c}}{\partial {A}_{x,y,z}^{l}}$$\end{document}</tex-math><mml:math id="M32" display="block"><mml:mrow><mml:msubsup><mml:mi>α</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mi>x</mml:mi></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mi>y</mml:mi></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mi>z</mml:mi></mml:munder><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="11548_2022_2619_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula></p>
    </sec>
    <sec id="Sec11">
      <title>Guided Grad-CAM</title>
      <p id="Par25">Guided GCAM (GGCAM) was introduced to provide higher-resolution visualizations capturing fine-grained details of the object of interest [<xref ref-type="bibr" rid="CR27">27</xref>]. GGCAM fuses the point-space gradient visualization method GBP [<xref ref-type="bibr" rid="CR23">23</xref>] and the class-discriminative coarse heatmaps of GCAM through element-wise multiplication. The estimated saliency map of GCAM is first upsampled to the input <italic>X</italic><sup><italic>I</italic></sup> spatial resolution using bilinear interpolation before applying the point-wise multiplication with GBP.</p>
    </sec>
  </sec>
  <sec id="Sec12">
    <title>Experiments</title>
    <sec id="Sec13">
      <title>Data</title>
      <p id="Par26">MRI data from the BraTS challenges 2019 and 2021 [<xref ref-type="bibr" rid="CR33">33</xref>–<xref ref-type="bibr" rid="CR36">36</xref>] have been used in this study for accomplishing the classification and segmentation tasks. Each subject has four MRI sequences including preoperative multimodal MRI scans of native T1W, Gadolinium T1Gd, T2W, and FLAIR, acquired from multiple different institutions. Although the main aim of the challenge is to compare the best algorithms for segmenting the enhancing tumor (ET), the tumor core (TC), and the whole tumor (WT) regions, the BraTS 2019 dataset also provides classification labels for gliomas. BraTS 2019 database comprises 259 cases of high-grade gliomas (HGG) and 76 cases of low-grade gliomas (LGG), which were used for the first showcase. The second showcase applies the BraTS 2021 database, which contains 1251 MRI images with ground truth annotations without any explicit glioma classification.</p>
      <p id="Par27">Since MRI sequences were acquired using multi-parametric instruments in multi-location centers, input images are needed to be standardized. A preprocessing stage has been applied to all MRI scans, specifically min–max scaling of each MRI modality using z-score normalization, and image cropping to a spatial resolution of 192 × 224 × 160. During the training, data augmentation was applied random flipping, random rotations, intensity transformation, as well as dynamic patch augmentation cropping size of 128 × 128 × 128 to avoid overfitting problems.</p>
    </sec>
    <sec id="Sec14">
      <title>Implementation</title>
      <p id="Par28">For the classification task, we employed a simple classifier based on a pretrained ResNet [<xref ref-type="bibr" rid="CR37">37</xref>] because of its accurate classification results. Deep transfer learning was then adopted to make the model capable of extracting features from brain MR images. Table <xref rid="Tab1" ref-type="table">1</xref> summarizes the added top layers to the ResNet-50 in our experiment. For the segmentation task, an encoder–decoder neural network was utilized, named 3D DeepSeg [<xref ref-type="bibr" rid="CR38">38</xref>]. The structure of our network is shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>List of the added top layers to the standard ResNet-50</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Type</th><th align="left">Output</th><th align="left">Feature maps</th></tr></thead><tbody><tr><td align="left">Average Pooling 2D</td><td align="left">2 × 2</td><td char="." align="char">512</td></tr><tr><td align="left">Flatten</td><td align="left">2048</td><td char="." align="char">1</td></tr><tr><td align="left">Dense</td><td align="left">256</td><td char="." align="char">1</td></tr><tr><td align="left">Dropout</td><td align="left">256</td><td char="." align="char">1</td></tr><tr><td align="left">Dense</td><td align="left">2</td><td char="." align="char">1</td></tr></tbody></table></table-wrap><fig id="Fig2"><label>Fig. 2</label><caption><p>Overview of the architecture details of 3D CNN for glioma segmentation [<xref ref-type="bibr" rid="CR38">38</xref>]</p></caption><graphic xlink:href="11548_2022_2619_Fig2_HTML" id="MO2"/></fig></p>
      <p id="Par29">Both DL models were implemented using the TensorFlow library [<xref ref-type="bibr" rid="CR39">39</xref>] version 2.4. Adam optimizer [<xref ref-type="bibr" rid="CR40">40</xref>] was used to update the weights of the network, with an initial learning rate of 1e <sup>– 3</sup> and 1e <sup>– 4</sup> at the very beginning, and the maximum number of training epochs is set to 150 and 1000, and batch size of 64 and 5 for the classification and segmentation networks, respectively. Training the networks was performed on a single NVIDIA graphic card (RTX 2080Ti with 11 GB RAM or RTX 3060 with 12 GB RAM). Explainability experiments were carried out after the training of the original neural network because of using post hoc XAI methods without network re-training or architecture modifications. The final sensitivity maps were generated by our proposed NeuroXAI framework with the pretrained saved weights for both DL models.</p>
    </sec>
  </sec>
  <sec id="Sec15">
    <title>Results</title>
    <sec id="Sec16">
      <title>Showcase I: application to classification</title>
      <p id="Par30">Here, we introduce the application of NeuroXAI to generate visual explanations for automatic brain glioma grading using DL. The main objective of this study is to illustrate the explainability capabilities of our proposed NeuroXAI framework for assisting clinicians, not to obtain the best classification results only. However, the applied classifier achieved a superior accuracy of 98.62%, comparing to the state-of-the-art methods [<xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR13">13</xref>–<xref ref-type="bibr" rid="CR16">16</xref>] as given in Table <xref rid="Tab2" ref-type="table">2</xref>.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Comparison of our proposed classifier and other deep models in previous studies</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model/ Year</th><th align="left">Preprocessing</th><th align="left">Method</th><th align="left">Accuracy</th></tr></thead><tbody><tr><td align="left">Ge et al. 2018 [<xref ref-type="bibr" rid="CR8">8</xref>]</td><td align="left">Class balancing and tumor masks</td><td align="left">2D CNN</td><td align="left">90.87%</td></tr><tr><td align="left">Ge et al. 2020 [<xref ref-type="bibr" rid="CR13">13</xref>]</td><td align="left">Tumor mask enhancement</td><td align="left">GAN*</td><td align="left">88.82%</td></tr><tr><td align="left">Mzoughi et al. 2020 [<xref ref-type="bibr" rid="CR14">14</xref>]</td><td align="left">Intensity normalization, contrast enhancement, and cubic B-spline resizing</td><td align="left">3D CNN</td><td align="left">96.49%</td></tr><tr><td align="left">Ahuja et al. 2020 [<xref ref-type="bibr" rid="CR15">15</xref>]</td><td align="left">Data normalization</td><td align="left">VGG</td><td align="left">99.30%</td></tr><tr><td align="left">Dixit and Nanda 2021 [<xref ref-type="bibr" rid="CR16">16</xref>]</td><td align="left">Grayscale conversion and tumor segmentation</td><td align="left">IWOA-RBNN**</td><td align="left">96%</td></tr><tr><td align="left">Our classifier</td><td align="left">Z-score normalization, image cropping, and transfer learning</td><td align="left">ResNet-50</td><td align="left">98.62%</td></tr></tbody></table><table-wrap-foot><p>*<bold><italic>GAN</italic></bold> Generative adversarial network</p><p>**<italic>IWOA-RBNN</italic> Improved whale optimization algorithm for radial basis neural network</p></table-wrap-foot></table-wrap></p>
      <p id="Par31">To better understand the deep model’s prediction, we used the DL model to visualize various sensitivity maps using NeuroXAI as shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. These 3D feature visualizations were generated from our model once the training is complete. Explanation maps by methods in (b-f) highlight all contributing features. In contrast, CAM heatmaps (g and h) highlight which regions of the input image are important for discriminating targeted classes.<fig id="Fig3"><label>Fig. 3</label><caption><p>Comparing different XAI visualization methods for brain glioma classification. Sensitivity maps are presented for HGG cases in the first four rows, while for LGG cases in the last three rows. Left to right: original MRI image, Vanilla gradient, guided backpropagation, integrated gradients, guided integrated gradients, SmoothGrad, Grad-CAM, and guided Grad-CAM visualizations. Note that in (b, c, d, e, f), all contributing features are highlighted in white, while in (g, h), red regions correspond to a high score for the predicted class</p></caption><graphic xlink:href="11548_2022_2619_Fig3_HTML" id="MO3"/></fig></p>
      <p id="Par32">Moreover, the visualization maps by pixel-space XAI methods, such as GBP, IG, and GIG, underlined fine-grained details in the input MRI image, but not being class-distinctive. In contrast, localization approaches like GCAM, are highly class-distinctive providing a smooth activation map. Notably, combining GBP with GCAM yielded better-localized visualizations with high resolution. SmoothGrad provided the best overall feature maps highlighting the main discriminative parts of the input FLAIR image so as to make the glioma grading. In contrast, VG provided noisy visualization maps compared with other methods due to the gradient saturation as reported in [<xref ref-type="bibr" rid="CR41">41</xref>], making it less reliable for this application.</p>
    </sec>
    <sec id="Sec17">
      <title>Showcase II: application to segmentation</title>
      <p id="Par33">In this subsection, a feasible application of NeuroXAI is provided to interpret deep brain glioma sub-region segmentation using multimodal MRIs. Table <xref rid="Tab3" ref-type="table">3</xref> presents the comparison of the proposed segmentation model with the existing techniques on the BraTS validation dataset. Remarkably, our DL model has achieved the best dice score coefficient (DSC) of 84.10, 87.33, and 92 for the enhancing tumor, tumor core, and whole tumor regions, respectively.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Comparison of our segmentation model and existing methods on the validation set</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Model</th><th align="left" rowspan="2">Preprocessing</th><th align="left" rowspan="2">Method</th><th align="left" colspan="3">DSC</th></tr><tr><th align="left">ET*</th><th align="left">TC*</th><th align="left">WT*</th></tr></thead><tbody><tr><td align="left">DeepSeg (2D) [<xref ref-type="bibr" rid="CR7">7</xref>]</td><td align="left">FLAIR MRI, bias correction, data normalization, and transfer learning</td><td align="left">2D U-Net</td><td char="." align="char">–</td><td char="." align="char">–</td><td align="left">84.10</td></tr><tr><td align="left">DeepSeg (3D) [<xref ref-type="bibr" rid="CR38">38</xref>]</td><td align="left"><italic>Z</italic>-score normalization and image cropping</td><td align="left">3D U-Net</td><td char="." align="char">82.50</td><td char="." align="char">84.73</td><td align="left">90.05</td></tr><tr><td align="left">Ilhan et al. 2022 [<xref ref-type="bibr" rid="CR42">42</xref>]</td><td align="left">FLAIR MRI, tumor localization, and histogram equalization</td><td align="left">U-net</td><td char="." align="char">–</td><td char="." align="char">–</td><td align="left">0.88</td></tr><tr><td align="left">nnU-Net [<xref ref-type="bibr" rid="CR43">43</xref>]</td><td align="left">Image cropping, data normalization, image resampling</td><td align="left">U-Net</td><td char="." align="char">79.45</td><td char="." align="char">85.24</td><td align="left">91.19</td></tr><tr><td align="left">CASPIANET +  + [<xref ref-type="bibr" rid="CR44">44</xref>]</td><td align="left"><italic>Z</italic>-score normalization</td><td align="left">Attention U-Net</td><td char="." align="char">81.08</td><td char="." align="char">87.60</td><td align="left">91.20</td></tr><tr><td align="left">Our model</td><td align="left"><italic>Z</italic>-score normalization, image cropping, on-the-fly data augmentation</td><td align="left">ResNet-50</td><td char="." align="char">84.10</td><td char="." align="char">87.33</td><td align="left">92</td></tr></tbody></table><table-wrap-foot><p>*<italic>ET, TC, and WT</italic> Enhancing tumor, tumor core, and whole tumor regions</p></table-wrap-foot></table-wrap></p>
      <p id="Par34">Figure <xref rid="Fig4" ref-type="fig">4</xref> shows the qualitative results from different XAI methods for explaining our glioma segmentation network. It can be seen that the employed visualization methods generally clustered their attributions around the segmented brain tumor. In particular, GCAM, GGCAM, and SmoothGrad provided the least noisy visualization maps with the advantage of GCAM of being class discriminative. GBP generated high-resolution saliency maps in which the edges of the tumor sub-regions are highlighted instead of the tumor itself.<fig id="Fig4"><label>Fig. 4</label><caption><p>Comparing different XAI visualization methods for brain glioma segmentation. Left to right: original MRI image, Vanilla gradient, guided backpropagation, integrated gradients, guided integrated gradients, SmoothGrad, Grad-CAM, guided Grad-CAM, and the manual truth annotations. Note that in (<bold>b, c, d, e, f</bold>), all contributing features are highlighted in white, while in (<bold>g, h</bold>), red regions correspond to a high score for the tumor region</p></caption><graphic xlink:href="11548_2022_2619_Fig4_HTML" id="MO4"/></fig></p>
      <p id="Par35">Besides, we analyzed each layer output toward the transparency of the black-box segmentation model. This experiment, explicitly network inspection, aims to clarify the flow of internal information in the neural network and whether this is in line with human-level concepts. For network inspection, GCAM was utilized since it allows visualizing activations in any layer of the deep network with respect to the network’s final output for a particular decision of interest. Figure <xref rid="Fig5" ref-type="fig">5</xref> provides these explanation maps following the layers from the input MRI scans to the predicted segmentation map. These layer-wise importance maps show that the deep network follows a hierarchical nature similar to the human brain. For instance, layer 17 shows a neuron learning the initial brain boundaries, while the fine-grained brain localization was achieved later in layer 21. Similarly, the tumor was initially detected in layer 8, but the final precise segmentation was provided by the output layer.<fig id="Fig5"><label>Fig. 5</label><caption><p>Visualization of the information flow in the segmentation CNN internal layers. The input MRI sequences are shown in <bold>(a)</bold>. <bold>b</bold>–<bold>d</bold> show implicit concepts for which no ground truth labels are available in addition to explicit concepts <bold>e</bold>–<bold>g</bold> with trained labels. L stands for convolutional layer</p></caption><graphic xlink:href="11548_2022_2619_Fig5_HTML" id="MO5"/></fig></p>
      <p id="Par36">Moreover, this deep neural network can learn some explicit concepts, which the CNN was not originally trained on, as well as implicit concepts from the underlying dataset. For instance, layer 22 in Fig. <xref rid="Fig5" ref-type="fig">5</xref>g seems to be learning the whole tumor region, as an explicit concept from the ground truth labeling data. Another example is shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>c for layer 3 learning the gray and white matter as an implicit concept which is not included in the training annotations.</p>
    </sec>
  </sec>
  <sec id="Sec18">
    <title>Discussion</title>
    <p id="Par37">DL has achieved the state-of-the-art in a wide range of medical tasks including medical image processing and analysis. By employing these AI advances in CADs, medical experts such as radiologists and surgeons become capable of detecting and diagnosing brain gliomas with great accuracy and shorter intervals. A deep neural network consists of numerous input, hidden, and output layers containing a large number of parameters (within millions). In applications increasingly vital to human healthcare, applying these models has been limited due to the lack of explainability.</p>
    <p id="Par38">NeuroXAI implements seven different gradient-based explanation methods, namely VG, GBP, IG, GIG, SmoothGrad, GCAM, and GGCAM, helping to make deep neural networks transparent. Each XAI method is unique and can be helpful in a different scenario with its inherent advantages and limitations. For example, VG is simple with the advantage of being supported by conventional machine learning frameworks such as TensorFlow [<xref ref-type="bibr" rid="CR39">39</xref>] and PyTorch [<xref ref-type="bibr" rid="CR45">45</xref>]. This makes VG applicable to any deep neural network without architectural modifications. On the other hand, the saliency maps generated by VG are noisy as well as they suffer from declining influences of features due to gradient saturation as reported in previous work [<xref ref-type="bibr" rid="CR41">41</xref>]. GBP is efficient in terms of implementation; however, it is limited to CNN models with ReLU activations and does not provide class-distinctive visualization maps.</p>
    <p id="Par39">Recently, IG has become popular thanks to the ease of implementation, no requirement for instrumentation of the network, and fixed number of calls to the gradient. GIG is an enhancement to eliminate the false perturbations problem of IG, but a choice has to be made at every step at the path from baseline to input, and thus the direction of the path is not fixed. Although SmoothGrad can help improve visualizations of the overall true signal with the major drawback of being non-class discriminative, conversely, GCAM allows interpreting any convolutional layer of the CNN by highlighting the discriminative region and thus can help in understanding the internal functionality. To eliminate the lower-resolution heatmaps problem of GCAM, GGCAM was implemented as the combination of GBP and GCAM advantages.</p>
    <p id="Par40">These explanation methods and their application to two common applications for brain imaging analysis tasks, namely brain glioma grading and glioma localization, have been examined in detail. For both applications, high-resolution gradient-based saliency maps, including VG, GBP, IG, GIG, and SmoothGrad, highlight all contributing features, regardless of the selected class, as shown in Figs. <xref rid="Fig3" ref-type="fig">3</xref> and <xref rid="Fig4" ref-type="fig">4</xref>. On the other hand, GCAM and GGCAM localize the most important regions for the network decision. This is consistent with findings in [<xref ref-type="bibr" rid="CR27">27</xref>] showing that humans can better understand regions instead of pixels. Besides, network dissection, shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>, demonstrates that CNN follows a systematic approach for detecting the brain gliomas coherent with experts’ knowledge. First, the network learns the abstract features, such as the brain boundaries in Fig. <xref rid="Fig5" ref-type="fig">5</xref>a, and afterward identifies finely detailed tumor boundaries shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>c.</p>
  </sec>
  <sec id="Sec19">
    <title>Conclusions and outlook</title>
    <p id="Par41">This study presented a new explainability framework, named NeuroXAI, for assisting the interpretation of the behavior of DL networks using state-of-the-art visualization attention maps. NeuroXAI is post hoc and can therefore be applied to any deep neural models gaining insight into the behavior of these already trained models. Additionally, our two showcases have demonstrated the significance of incorporating XAI methods in medical image analysis tasks. NeuroXAI can also support the analysis of CNNs by providing an individual activation map for every internal filter. Moreover, our NeuroXAI results showed the importance of XAI for medical imaging tasks to understand DL models to accelerate their clinical acceptance by medical staff in the field.</p>
    <p id="Par42">Future work will be focused on the quantitative evaluation of XAI methods to assess the quality of the generated sensitivity maps and study their relationship with the DL accuracy metrics with additional experiments on multi-modal MRI-guided neurosurgery. Another main prospect of this research work is to investigate the possibility of extracting quantitative features from the explanation methods such as tumor volume and centroid.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>The corresponding author is funded by the German Academic Exchange Service (DAAD) under scholarship No. 91705803.</p>
  </ack>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>Open Access funding enabled and organized by Projekt DEAL.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar1" notes-type="COI-statement">
      <title>Conflict of interest</title>
      <p id="Par43">The authors have no conflict of interest to disclose.</p>
    </notes>
    <notes id="FPar2">
      <title>Ethical approval</title>
      <p id="Par44">This article does not contain any studies with human participants or animals performed by any of the authors.</p>
    </notes>
    <notes id="FPar3">
      <title>Informed consent</title>
      <p id="Par45">This article does not contain patient data collected by any of the authors.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Siegel</surname>
            <given-names>RL</given-names>
          </name>
          <name>
            <surname>Miller</surname>
            <given-names>KD</given-names>
          </name>
          <name>
            <surname>Fuchs</surname>
            <given-names>HE</given-names>
          </name>
          <name>
            <surname>Jemal</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Cancer statistics</article-title>
        <source>CA: A Cancer J Clinicians</source>
        <year>2021</year>
        <volume>71</volume>
        <issue>1</issue>
        <fpage>7</fpage>
        <lpage>33</lpage>
        <pub-id pub-id-type="doi">10.3322/caac.21654</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sonali</surname>
            <given-names>VMK</given-names>
          </name>
          <name>
            <surname>Singh</surname>
            <given-names>RP</given-names>
          </name>
          <name>
            <surname>Agrawal</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Mehata</surname>
            <given-names>AK</given-names>
          </name>
          <name>
            <surname>Pawde</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>Narendra</surname>
            <given-names>SR</given-names>
          </name>
          <name>
            <surname>Muthu</surname>
            <given-names>MS</given-names>
          </name>
        </person-group>
        <article-title>Nanotheranostics: emerging strategies for early diagnosis and therapy of brain cancer</article-title>
        <source>Nanotheranostics</source>
        <year>2018</year>
        <volume>2</volume>
        <issue>1</issue>
        <fpage>70</fpage>
        <lpage>86</lpage>
        <pub-id pub-id-type="doi">10.7150/ntno.21638</pub-id>
        <?supplied-pmid 29291164?>
        <pub-id pub-id-type="pmid">29291164</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <mixed-citation publication-type="other">Dandıl E, Çakıroğlu M, Ekşi Z (2015) Computer-aided diagnosis of malign and benign brain tumors on MR images. In: ICT innovations 2014. Advances in intelligent systems and computing. pp 157–166. doi:10.1007/978-3-319-09879-1_16</mixed-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tu</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>Y-L</given-names>
          </name>
          <name>
            <surname>Huo</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>X-J</given-names>
          </name>
        </person-group>
        <article-title>Gold-based nanomaterials for the treatment of brain cancer</article-title>
        <source>Cancer Biol Med</source>
        <year>2021</year>
        <volume>18</volume>
        <issue>2</issue>
        <fpage>372</fpage>
        <lpage>387</lpage>
        <pub-id pub-id-type="doi">10.20892/j.issn.2095-3941.2020.0524</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Miner</surname>
            <given-names>RC</given-names>
          </name>
        </person-group>
        <article-title>Image-guided neurosurgery</article-title>
        <source>J Med Imag Radiation Sci</source>
        <year>2017</year>
        <volume>48</volume>
        <issue>4</issue>
        <fpage>328</fpage>
        <lpage>335</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jmir.2017.06.005</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Paul</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Sivarani</surname>
            <given-names>TS</given-names>
          </name>
        </person-group>
        <article-title>Computer aided diagnosis of brain tumor using novel classification techniques</article-title>
        <source>J Ambient Intell Humaniz Comput</source>
        <year>2020</year>
        <volume>12</volume>
        <issue>7</issue>
        <fpage>7499</fpage>
        <lpage>7509</lpage>
        <pub-id pub-id-type="doi">10.1007/s12652-020-02429-6</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zeineldin</surname>
            <given-names>RA</given-names>
          </name>
          <name>
            <surname>Karar</surname>
            <given-names>ME</given-names>
          </name>
          <name>
            <surname>Coburger</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wirtz</surname>
            <given-names>CR</given-names>
          </name>
          <name>
            <surname>Burgert</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>DeepSeg: deep neural network framework for automatic brain tumor segmentation using magnetic resonance FLAIR images</article-title>
        <source>Int J Comput Assist Radiol Surg</source>
        <year>2020</year>
        <volume>15</volume>
        <issue>6</issue>
        <fpage>909</fpage>
        <lpage>920</lpage>
        <pub-id pub-id-type="doi">10.1007/s11548-020-02186-z</pub-id>
        <?supplied-pmid 32372386?>
        <pub-id pub-id-type="pmid">32372386</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">Ge C, Gu IY-H, Jakola AS, Yang J (2018) Deep learning and multi-sensor fusion for glioma classification using multistream 2D convolutional networks. In: Paper presented at the 2018 40th annual international conference of the IEEE engineering in medicine and biology society (EMBC),</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chan</surname>
            <given-names>HP</given-names>
          </name>
          <name>
            <surname>Hadjiiski</surname>
            <given-names>LM</given-names>
          </name>
          <name>
            <surname>Samala</surname>
            <given-names>RK</given-names>
          </name>
        </person-group>
        <article-title>Computer-aided diagnosis in the era of deep learning</article-title>
        <source>Med Phys</source>
        <year>2020</year>
        <pub-id pub-id-type="doi">10.1002/mp.13764</pub-id>
        <?supplied-pmid 32990328?>
        <pub-id pub-id-type="pmid">32990328</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lynch</surname>
            <given-names>CJ</given-names>
          </name>
          <name>
            <surname>Liston</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>New machine-learning technologies for computer-aided diagnosis</article-title>
        <source>Nat Med</source>
        <year>2018</year>
        <volume>24</volume>
        <issue>9</issue>
        <fpage>1304</fpage>
        <lpage>1305</lpage>
        <pub-id pub-id-type="doi">10.1038/s41591-018-0178-4</pub-id>
        <?supplied-pmid 30177823?>
        <pub-id pub-id-type="pmid">30177823</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Ronneberger O, Fischer P, Brox T (2015) U-Net: convolutional networks for biomedical image segmentation. In: Medical image computing and computer-assisted intervention – MICCAI 2015. Lecture notes in computer science. pp 234–241. doi:10.1007/978-3-319-24574-4_28</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Isensee</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Jaeger</surname>
            <given-names>PF</given-names>
          </name>
          <name>
            <surname>Kohl</surname>
            <given-names>SAA</given-names>
          </name>
          <name>
            <surname>Petersen</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Maier-Hein</surname>
            <given-names>KH</given-names>
          </name>
        </person-group>
        <article-title>nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</article-title>
        <source>Nat Methods</source>
        <year>2020</year>
        <volume>18</volume>
        <issue>2</issue>
        <fpage>203</fpage>
        <lpage>211</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-020-01008-z</pub-id>
        <?supplied-pmid 33288961?>
        <pub-id pub-id-type="pmid">33288961</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ge</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Gu</surname>
            <given-names>IY-H</given-names>
          </name>
          <name>
            <surname>Jakola</surname>
            <given-names>AS</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Enlarged training dataset by pairwise GANs for molecular-based brain tumor classification</article-title>
        <source>IEEE Access</source>
        <year>2020</year>
        <volume>8</volume>
        <fpage>22560</fpage>
        <lpage>22570</lpage>
        <pub-id pub-id-type="doi">10.1109/access.2020.2969805</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mzoughi</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Njeh</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Wali</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Slima</surname>
            <given-names>MB</given-names>
          </name>
          <name>
            <surname>BenHamida</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Mhiri</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Mahfoudhe</surname>
            <given-names>KB</given-names>
          </name>
        </person-group>
        <article-title>Deep multi-scale 3D convolutional neural network (CNN) for MRI Gliomas brain tumor classification</article-title>
        <source>J Digit Imag</source>
        <year>2020</year>
        <volume>33</volume>
        <issue>4</issue>
        <fpage>903</fpage>
        <lpage>915</lpage>
        <pub-id pub-id-type="doi">10.1007/s10278-020-00347-9</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Ahuja S, Panigrahi BK, Gandhi T (2020) Transfer learning based brain tumor detection and segmentation using superpixel technique. In: Paper presented at the 2020 international conference on contemporary computing and applications (IC3A)</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dixit</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Nanda</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>An improved whale optimization algorithm-based radial neural network for multi-grade brain tumor classification</article-title>
        <source>Vis Comput</source>
        <year>2021</year>
        <pub-id pub-id-type="doi">10.1007/s00371-021-02176-5</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Ye</surname>
            <given-names>QH</given-names>
          </name>
          <name>
            <surname>Xia</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Unbox the black-box for the medical explainable AI via multi-modal and multi-centre data fusion: a mini-review, two showcases and beyond</article-title>
        <source>Inform Fusion</source>
        <year>2022</year>
        <volume>77</volume>
        <fpage>29</fpage>
        <lpage>52</lpage>
        <pub-id pub-id-type="doi">10.1016/j.inffus.2021.07.016</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gulum</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Trombley</surname>
            <given-names>CM</given-names>
          </name>
          <name>
            <surname>Kantardzic</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>A review of explainable deep learning cancer detection models in medical imaging</article-title>
        <source>Appl Sci-Basel</source>
        <year>2021</year>
        <pub-id pub-id-type="doi">10.3390/app11104573</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Temme</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Algorithms and transparency in view of the new general data protection regulation</article-title>
        <source>Eur Data Protect Law Rev</source>
        <year>2017</year>
        <volume>3</volume>
        <issue>4</issue>
        <fpage>473</fpage>
        <lpage>485</lpage>
        <pub-id pub-id-type="doi">10.21552/edpl/2017/4/9</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Ribeiro MT, Singh S, Guestrin C (2016) "Why should i trust you?" Explaining the predictions of any classifier. In: Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp 1135–1144</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Zeiler MD, Fergus R (2014) Visualizing and understanding convolutional networks. In: Computer vision – ECCV 2014. Lecture Notes in Computer Science. pp 818–833. doi:10.1007/978-3-319-10590-1_53</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Simonyan K, Vedaldi A, Zisserman A (2014) Deep inside convolutional networks: visualising image classification models and saliency maps. In: In workshop at international conference on learning representations. Citeseer,</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Springenberg J, Dosovitskiy A, Brox T, Riedmiller M (2015) Striving for simplicity: the all convolutional net. In: ICLR (workshop track)</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Sundararajan M, Taly A, Yan Q (2017) Axiomatic attribution for deep networks. In: International conference on machine learning, PMLR, pp 3319–3328</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Kapishnikov A, Venugopalan S, Avci B, Wedin B, Terry M, Bolukbasi T (2021) Guided integrated gradients: an adaptive path method for removing noise. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp 5050–5058</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Smilkov D, Thorat N, Kim B, Viégas F, Wattenberg M (2017) Smoothgrad: removing noise by adding noise. In: Proceedings of the ICML workshop on visualization for deep learning, Sydney, Australia</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Selvaraju RR, Cogswell M, Das A, Vedantam R, Parikh D, Batra D (2017) Grad-cam: visual explanations from deep networks via gradient-based localization. In: Proceedings of the IEEE international conference on computer vision. pp 618–626</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Windisch</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Weber</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Fürweger</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Ehret</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Kufeld</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Zwahlen</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Muacevic</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Implementation of model explainability for a basic brain tumor detection using convolutional neural networks on MRI slices</article-title>
        <source>Neuroradiology</source>
        <year>2020</year>
        <volume>62</volume>
        <issue>11</issue>
        <fpage>1515</fpage>
        <lpage>1518</lpage>
        <pub-id pub-id-type="doi">10.1007/s00234-020-02465-1</pub-id>
        <?supplied-pmid 32500277?>
        <pub-id pub-id-type="pmid">32500277</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Esmaeili</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Vettukattil</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Banitalebi</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Krogh</surname>
            <given-names>NR</given-names>
          </name>
          <name>
            <surname>Geitung</surname>
            <given-names>JT</given-names>
          </name>
        </person-group>
        <article-title>Explainable artificial intelligence for human-machine interaction in brain tumor localization</article-title>
        <source>J Person Med</source>
        <year>2021</year>
        <pub-id pub-id-type="doi">10.3390/jpm11111213</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Natekar</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Kori</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Krishnamurthi</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Demystifying brain tumor segmentation networks: interpretability and uncertainty analysis</article-title>
        <source>Front Comput Neurosci</source>
        <year>2020</year>
        <volume>14</volume>
        <fpage>6</fpage>
        <pub-id pub-id-type="doi">10.3389/fncom.2020.00006</pub-id>
        <?supplied-pmid 32116620?>
        <pub-id pub-id-type="pmid">32116620</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Saleem</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Shahid</surname>
            <given-names>AR</given-names>
          </name>
          <name>
            <surname>Raza</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Visual interpretability in 3D brain tumor segmentation network</article-title>
        <source>Comput Biol Med</source>
        <year>2021</year>
        <volume>133</volume>
        <fpage>104410</fpage>
        <pub-id pub-id-type="doi">10.1016/j.compbiomed.2021.104410</pub-id>
        <?supplied-pmid 33894501?>
        <pub-id pub-id-type="pmid">33894501</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Zhou B, Khosla A, Lapedriza A, Oliva A, Torralba A (2016) Learning deep features for discriminative localization. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp 2921–2929</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Menze</surname>
            <given-names>BH</given-names>
          </name>
          <name>
            <surname>Jakab</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bauer</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kalpathy-Cramer</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Farahani</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Kirby</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Burren</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Porz</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Slotboom</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wiest</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lanczi</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Gerstner</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Weber</surname>
            <given-names>M-A</given-names>
          </name>
          <name>
            <surname>Arbel</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Avants</surname>
            <given-names>BB</given-names>
          </name>
          <name>
            <surname>Ayache</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Buendia</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Collins</surname>
            <given-names>DL</given-names>
          </name>
          <name>
            <surname>Cordier</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Corso</surname>
            <given-names>JJ</given-names>
          </name>
          <name>
            <surname>Criminisi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Das</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Delingette</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Demiralp</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Durst</surname>
            <given-names>CR</given-names>
          </name>
          <name>
            <surname>Dojat</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Doyle</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Festa</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Forbes</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Geremia</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Glocker</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Golland</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Hamamci</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Iftekharuddin</surname>
            <given-names>KM</given-names>
          </name>
          <name>
            <surname>Jena</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>John</surname>
            <given-names>NM</given-names>
          </name>
          <name>
            <surname>Konukoglu</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Lashkari</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Mariz</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Meier</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Pereira</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Precup</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Price</surname>
            <given-names>SJ</given-names>
          </name>
          <name>
            <surname>Raviv</surname>
            <given-names>TR</given-names>
          </name>
          <name>
            <surname>Reza</surname>
            <given-names>SMS</given-names>
          </name>
          <name>
            <surname>Ryan</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Sarikaya</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Schwartz</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Shin</surname>
            <given-names>H-C</given-names>
          </name>
          <name>
            <surname>Shotton</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Silva</surname>
            <given-names>CA</given-names>
          </name>
          <name>
            <surname>Sousa</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Subbanna</surname>
            <given-names>NK</given-names>
          </name>
          <name>
            <surname>Szekely</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Taylor</surname>
            <given-names>TJ</given-names>
          </name>
          <name>
            <surname>Thomas</surname>
            <given-names>OM</given-names>
          </name>
          <name>
            <surname>Tustison</surname>
            <given-names>NJ</given-names>
          </name>
          <name>
            <surname>Unal</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Vasseur</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Wintermark</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ye</surname>
            <given-names>DH</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Zikic</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Prastawa</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Reyes</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Van Leemput</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>The multimodal brain tumor image segmentation benchmark (BRATS)</article-title>
        <source>IEEE Trans Med Imaging</source>
        <year>2015</year>
        <volume>34</volume>
        <issue>10</issue>
        <fpage>1993</fpage>
        <lpage>2024</lpage>
        <pub-id pub-id-type="doi">10.1109/tmi.2014.2377694</pub-id>
        <?supplied-pmid 25494501?>
        <pub-id pub-id-type="pmid">25494501</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bakas</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Akbari</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Sotiras</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bilello</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Rozycki</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kirby</surname>
            <given-names>JS</given-names>
          </name>
          <name>
            <surname>Freymann</surname>
            <given-names>JB</given-names>
          </name>
          <name>
            <surname>Farahani</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Davatzikos</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Advancing the cancer genome atlas glioma MRI collections with expert segmentation labels and radiomic features</article-title>
        <source>Sci Data</source>
        <year>2017</year>
        <pub-id pub-id-type="doi">10.1038/sdata.2017.117</pub-id>
        <?supplied-pmid 28872634?>
        <pub-id pub-id-type="pmid">28872634</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Bakas S, Reyes M, Jakab A, Bauer S, Rempfler M, Crimi A, Shinohara RT, Berger C, Ha SM, Rozycki M (2018) Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the BRATS challenge. arXiv preprint arXiv:181102629</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Baid U, Ghodasara S, Bilello M, Mohan S, Calabrese E, Colak E, Farahani K, Kalpathy-Cramer J, Kitamura FC, Pati S, Prevedello LM, Rudie JD, Sako C, Shinohara RT, Bergquist T, Chai R, Eddy J, Elliott J, Reade W, Schaffter T, Yu T, Zheng J, Annotators B, Davatzikos C, Mongan J, Hess C, Cha S, Villanueva-Meyer J, Freymann JB, Kirby JS, Wiestler B, Crivellaro P, Colen RR, Kotrotsou A, Marcus D, Milchenko M, Nazeri A, Fathallah-Shaykh H, Wiest R, Jakab A, Weber M-A, Mahajan A, Menze B, Flanders AE, Bakas S (2021) The RSNA-ASNR-MICCAI BraTS 2021 Benchmark on Brain Tumor Segmentation and Radiogenomic Classification.arXiv:2107.02314</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">He K, Zhang X, Ren S, Sun J (2016) Deep residual learning for image recognition. In: Proceedings of the IEEE computer society conference on computer vision and pattern recognition, vol 2016-Decem. doi:10.1109/CVPR.2016.90</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Zeineldin RA, Karar ME, Mathis-Ullrich F, Burgert O (2021) Ensemble CNN networks for GBM tumors segmentation using multi-parametric MRI. arXiv preprint arXiv:211206554</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Abadi M, Barham P, Chen J, Chen Z, Davis A, Dean J, Devin M, Ghemawat S, Irving G, Isard M (2016) {TensorFlow}: A System for {Large-Scale} machine learning. In: 12th USENIX symposium on operating systems design and implementation (OSDI 16), pp 265–283</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">Kingma DP, Ba J (2015) Adam: a method for stochastic optimization. CoRR abs/1412.6980</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Shrikumar A, Greenside P, Kundaje A (2017) Learning important features through propagating activation differences. In: International conference on machine learning. PMLR, pp 3145–3153</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ilhan</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sekeroglu</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Abiyev</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Brain tumor segmentation in MRI images using nonparametric localization and enhancement methods with U-net</article-title>
        <source>Int J Comput Assist Radiol Surg</source>
        <year>2022</year>
        <volume>17</volume>
        <issue>3</issue>
        <fpage>589</fpage>
        <lpage>600</lpage>
        <pub-id pub-id-type="doi">10.1007/s11548-022-02566-7</pub-id>
        <?supplied-pmid 35092598?>
        <pub-id pub-id-type="pmid">35092598</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Isensee F, Jäger PF, Full PM, Vollmuth P, Maier-Hein KH (2021) nnU-net for brain tumor segmentation. In: Brainlesion: glioma, multiple sclerosis, stroke and traumatic brain injuries. Lecture Notes in Computer Science. pp 118–132. doi:10.1007/978-3-030-72087-2_11</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liew</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>CC</given-names>
          </name>
          <name>
            <surname>Lan</surname>
            <given-names>BL</given-names>
          </name>
          <name>
            <surname>Tan</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>CASPIANET++: a multidimensional channel-spatial asymmetric attention network with noisy student curriculum learning paradigm for brain tumor segmentation</article-title>
        <source>Comput Biol Med</source>
        <year>2021</year>
        <pub-id pub-id-type="doi">10.1016/j.compbiomed.2021.104690</pub-id>
        <?supplied-pmid 34352452?>
        <pub-id pub-id-type="pmid">34352452</pub-id>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Paszke</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gross</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Massa</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Lerer</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bradbury</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Chanan</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Killeen</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Gimelshein</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Antiga</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Pytorch: an imperative style, high-performance deep learning library</article-title>
        <source>Adv Neural Inf Process Syst</source>
        <year>2019</year>
        <volume>32</volume>
        <fpage>8026</fpage>
        <lpage>8037</lpage>
      </element-citation>
    </ref>
  </ref-list>
</back>
