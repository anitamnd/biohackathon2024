<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_PATTER100228 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEgr4 jpg ?>
<?FILEgr5 jpg ?>
<?FILEgr6 jpg ?>
<?FILEgr7 jpg ?>
<?FILEmmc1 pdf ?>
<?FILEmmc2 pdf ?>
<?FILEsi1 gif ?>
<?FILEsi2 gif ?>
<?FILEsi3 gif ?>
<?FILEsi4 gif ?>
<?FILEsi5 gif ?>
<?FILEsi6 gif ?>
<?FILEsi7 gif ?>
<?FILEsi8 gif ?>
<?FILEsi9 gif ?>
<?FILEsi10 gif ?>
<?FILEsi11 gif ?>
<?FILEsi12 gif ?>
<?FILEsi13 gif ?>
<?FILEsi14 gif ?>
<?FILEsi15 gif ?>
<?FILEsi16 gif ?>
<?FILEsi17 gif ?>
<?FILEsi18 gif ?>
<?FILEsi19 gif ?>
<?FILEsi20 gif ?>
<?FILEsi21 gif ?>
<?FILEsi22 gif ?>
<?FILEsi23 gif ?>
<?FILEsi24 gif ?>
<?FILEsi25 gif ?>
<?FILEsi26 gif ?>
<?FILEsi27 gif ?>
<?FILEsi28 gif ?>
<?FILEsi29 gif ?>
<?FILEsi30 gif ?>
<?FILEsi31 gif ?>
<?FILEsi32 gif ?>
<?FILEsi33 gif ?>
<?FILEsi34 gif ?>
<?FILEsi35 gif ?>
<?FILEsi36 gif ?>
<?FILEsi37 gif ?>
<?FILEsi38 gif ?>
<?FILEsi39 gif ?>
<?FILEsi40 gif ?>
<?FILEsi41 gif ?>
<?FILEsi42 gif ?>
<?FILEsi43 gif ?>
<?FILEsi44 gif ?>
<?FILEsi45 gif ?>
<?FILEsi46 gif ?>
<?FILEsi47 gif ?>
<?FILEsi48 gif ?>
<?FILEsi49 gif ?>
<?FILEsi50 gif ?>
<?FILEsi51 gif ?>
<?FILEsi52 gif ?>
<?FILEsi53 gif ?>
<?FILEsi54 gif ?>
<?FILEsi55 gif ?>
<?FILEsi56 gif ?>
<?FILEsi57 gif ?>
<?FILEsi58 gif ?>
<?FILEsi59 gif ?>
<?FILEsi60 gif ?>
<?FILEsi61 gif ?>
<?FILEsi62 gif ?>
<?FILEsi63 gif ?>
<?FILEsi64 gif ?>
<?FILEsi65 gif ?>
<?FILEsi66 gif ?>
<?FILEsi67 gif ?>
<?FILEsi68 gif ?>
<?FILEsi69 gif ?>
<?FILEsi70 gif ?>
<?FILEsi71 gif ?>
<?FILEsi72 gif ?>
<?FILEsi73 gif ?>
<?FILEsi74 gif ?>
<?FILEsi75 gif ?>
<?FILEsi76 gif ?>
<?FILEsi77 gif ?>
<?FILEsi78 gif ?>
<?FILEsi79 gif ?>
<?FILEsi80 gif ?>
<?FILEsi81 gif ?>
<?FILEsi82 gif ?>
<?FILEsi83 gif ?>
<?FILEsi84 gif ?>
<?FILEsi85 gif ?>
<?FILEsi86 gif ?>
<?FILEsi87 gif ?>
<?FILEsi88 gif ?>
<?FILEsi89 gif ?>
<?FILEsi90 gif ?>
<?FILEsi91 gif ?>
<?FILEsi92 gif ?>
<?FILEsi93 gif ?>
<?FILEsi94 gif ?>
<?FILEsi95 gif ?>
<?FILEsi96 gif ?>
<?FILEsi97 gif ?>
<?FILEsi98 gif ?>
<?FILEsi99 gif ?>
<?FILEsi100 gif ?>
<?FILEsi101 gif ?>
<?FILEsi102 gif ?>
<?FILEsi103 gif ?>
<?FILEsi104 gif ?>
<?FILEsi105 gif ?>
<?FILEsi106 gif ?>
<?FILEsi107 gif ?>
<?FILEsi108 gif ?>
<?FILEsi109 gif ?>
<?FILEsi110 gif ?>
<?FILEsi111 gif ?>
<?FILEsi112 gif ?>
<?FILEsi113 gif ?>
<?FILEsi114 gif ?>
<?FILEsi115 gif ?>
<?FILEsi116 gif ?>
<?FILEsi117 gif ?>
<?FILEsi118 gif ?>
<?FILEsi119 gif ?>
<?FILEsi120 gif ?>
<?FILEsi121 gif ?>
<?FILEsi122 gif ?>
<?properties open_access?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Patterns (N Y)</journal-id>
    <journal-id journal-id-type="iso-abbrev">Patterns (N Y)</journal-id>
    <journal-title-group>
      <journal-title>Patterns</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2666-3899</issn>
    <publisher>
      <publisher-name>Elsevier</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8085609</article-id>
    <article-id pub-id-type="pii">S2666-3899(21)00042-8</article-id>
    <article-id pub-id-type="doi">10.1016/j.patter.2021.100228</article-id>
    <article-id pub-id-type="publisher-id">100228</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Descriptor</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>reval: A Python package to determine best clustering solutions with stability-based relative clustering validation</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au1">
        <name>
          <surname>Landi</surname>
          <given-names>Isotta</given-names>
        </name>
        <email>landi.isotta@gmail.com</email>
        <xref rid="aff1" ref-type="aff">1</xref>
        <xref rid="fn1" ref-type="fn">4</xref>
        <xref rid="fn2" ref-type="fn">5</xref>
        <xref rid="cor1" ref-type="corresp">∗</xref>
      </contrib>
      <contrib contrib-type="author" id="au2">
        <name>
          <surname>Mandelli</surname>
          <given-names>Veronica</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">1</xref>
        <xref rid="aff2" ref-type="aff">2</xref>
      </contrib>
      <contrib contrib-type="author" id="au3">
        <name>
          <surname>Lombardo</surname>
          <given-names>Michael V.</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">1</xref>
        <xref rid="aff3" ref-type="aff">3</xref>
      </contrib>
      <aff id="aff1"><label>1</label>Laboratory for Autism and Neurodevelopmental Disorders, Center for Neuroscience and Cognitive Systems @UniTn, Istituto Italiano di Tecnologia, Rovereto, Italy</aff>
      <aff id="aff2"><label>2</label>Center for Mind/Brain Sciences, University of Trento, Rovereto, Italy</aff>
      <aff id="aff3"><label>3</label>Autism Research Centre, Department of Psychiatry, University of Cambridge, Cambridge, UK</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>∗</label>Corresponding author <email>landi.isotta@gmail.com</email></corresp>
      <fn id="fn1">
        <label>4</label>
        <p id="ntpara0010">Lead contact</p>
      </fn>
      <fn id="fn2">
        <label>5</label>
        <p id="ntpara0015">Twitter: <ext-link ext-link-type="uri" xlink:href="https://twitter.com/IsottaLandi" id="intref0010">@IsottaLandi</ext-link></p>
      </fn>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>02</day>
      <month>4</month>
      <year>2021</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="collection">
      <day>09</day>
      <month>4</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>02</day>
      <month>4</month>
      <year>2021</year>
    </pub-date>
    <volume>2</volume>
    <issue>4</issue>
    <elocation-id>100228</elocation-id>
    <history>
      <date date-type="received">
        <day>28</day>
        <month>8</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>18</day>
        <month>1</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>2</day>
        <month>3</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2021 The Authors</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).</license-p>
      </license>
    </permissions>
    <abstract id="abs0010">
      <title>Summary</title>
      <p>Determining the best partition for a dataset can be a challenging task because of the lack of <italic>a priori</italic> information within an unsupervised learning framework and the absence of a unique clustering validation approach to evaluate clustering solutions. Here we present reval: a Python package that leverages stability-based relative clustering validation methods to select best clustering solutions as the ones that replicate, via supervised learning, on unseen subsets of data. The implementation of relative validation methods can contribute to the theory of clustering by fostering new approaches for the investigation of clustering results in different situations and for different data distributions. This work aims at contributing to this effort by implementing a package that works with multiple clustering and classification algorithms, hence allowing both the automation of the labeling process and the assessment of the stability of different clustering mechanisms.</p>
    </abstract>
    <abstract abstract-type="author-highlights" id="abs0020">
      <title>Highlights</title>
      <p>
        <list list-type="simple" id="ulist0010">
          <list-item id="u0010">
            <label>•</label>
            <p id="p0010">Implementation of a stability-based relative validation technique for clustering</p>
          </list-item>
          <list-item id="u0015">
            <label>•</label>
            <p id="p0015">Best number of clusters selection for multiple tasks</p>
          </list-item>
          <list-item id="u0020">
            <label>•</label>
            <p id="p0020">Automation of stable clustering labels on new data via supervised learning</p>
          </list-item>
          <list-item id="u0025">
            <label>•</label>
            <p id="p0025">Complement or alternative to internal validation measures for clustering</p>
          </list-item>
        </list>
      </p>
    </abstract>
    <abstract abstract-type="editor-highlights" id="abs0025">
      <title>The bigger picture</title>
      <p>reval is a Python package for stability-based relative clustering validation. It works with multiple clustering and classification algorithms, and as such, it enables the selection of best clustering solutions as the ones that replicate, via supervised learning, on unseen subsets of data. It is a tool that provides measures to evaluate clustering replicability and implements the automation of the labeling process. reval can be used as a complement or an alternative to internal validation measures, which highly rely on features inherent to a specific grouping solution, hindering the validation of replicable clusters.</p>
    </abstract>
    <abstract abstract-type="teaser" id="abs0030">
      <p>In the absence of labels that help categorize data, researchers rely on clustering algorithms to find meaningful partitions. Validating such partitions is a difficult task and widespread validation methods rely highly on the structure of the data at hand. This work presents a software implementation of a stability-based relative validation method for the selection of replicable stable solutions. It can be used with multiple clustering algorithms for different tasks, and it enables the automation of cluster labeling via classification methods.</p>
    </abstract>
    <kwd-group id="kwrds0010">
      <title>Keywords</title>
      <kwd>stability-based relative validation</kwd>
      <kwd>clustering</kwd>
      <kwd>unsupervised learning</kwd>
      <kwd>clustering replicability</kwd>
    </kwd-group>
  </article-meta>
  <notes>
    <p id="misc0010">Published: April 2, 2021</p>
  </notes>
</front>
<body>
  <sec id="sec1">
    <title>Introduction</title>
    <p id="p0030">Clustering algorithms identify intrinsic subgroups in a dataset by arranging together elements that show higher pairwise similarities relative to other subgroups.<xref rid="bib1" ref-type="bibr"><sup>1</sup></xref> While their usage is relatively widespread, the lack of <italic>a priori</italic> information complicates the evaluation of clustering solutions. Attempts to address this challenge usually rely on the implementation of internal validation approaches, which focus on quantities and features inherent to a grouping solution.<xref rid="bib2" ref-type="bibr"><sup>2</sup></xref> Here we present the reval Python package (pronounced ˈrϵvɘl), which implements an approach for stability-based validation of clustering solutions described by Lange et al.<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref> that allows for the identification and evaluation of partitions that best generalize to unseen data and the automation of the labeling process. In contrast to internal validation, relative validation methods<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref><sup>,</sup><xref rid="bib4" ref-type="bibr"><sup>4</sup></xref> have the potential to transform cluster analysis into a model selection problem and help evaluate the best clustering solution (i.e., best number of clusters). The way these methods are conceived also offers the possibility to determine the extent to which a clustering solution generalizes to unseen data and hence to enable the replication of the data partition chosen. While a variety of software packages contain internal cluster validation methods and measures, open-source software to easily implement the full potential of relative validation techniques are lacking.</p>
    <p id="p0035">Many methods are available to compute internal validation measures that help in determining the best number of clusters.<xref rid="bib5" ref-type="bibr">5</xref>, <xref rid="bib6" ref-type="bibr">6</xref>, <xref rid="bib7" ref-type="bibr">7</xref> For example, the elbow method<xref rid="bib5" ref-type="bibr"><sup>5</sup></xref> selects the number of clusters for which the within-cluster variability decrement is minimal. Another popular method using internal criteria is the silhouette-based approach.<xref rid="bib7" ref-type="bibr"><sup>7</sup></xref> This method maximizes cluster cohesion and separation; that is, how similar an object is to other elements of the same cluster compared with elements of other clusters. Libraries and methods for the automated selection of the best number of clusters are available in both Python and R. The yellowbrick Python visual analysis and diagnostic tool suite<xref rid="bib8" ref-type="bibr"><sup>8</sup></xref> includes the implementation of the elbow method to determine the best number of clusters. In R, NbClust<xref rid="bib9" ref-type="bibr"><sup>9</sup></xref> is a popular library that compiles 30 different internal metrics and allows for users to compute all or a subset of metrics to be used in combination with a majority vote rule to select the optimal number of clusters. For relative validation approaches there are the clValid<xref rid="bib10" ref-type="bibr"><sup>10</sup></xref> and cstab<xref rid="bib11" ref-type="bibr"><sup>11</sup></xref> libraries, which apply stability-based relative validation models. clValid was designed to work with highly correlated high-throughput genomic data and computes stability measures comparing clustering solutions based on full data and data with a single column removed. cstab implements the selection of the best number of clusters via model-based and model-free clustering instability<xref rid="bib12" ref-type="bibr"><sup>12</sup></xref> using a bootstrap approach.</p>
    <p id="p0040">The reval package contributes to this landscape by implementing a stability-based approach that can be easily applied to different datasets using multiple clustering and classification algorithms. Built on top of the stability-based algorithm<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref>, reval applies a classifier trained on the best clustering solution to a test set, returning classification metrics that help interpret the generalization performance, guide the clustering process, and enable labeling replication. This tool can be used with internal measures to assess the underlying structure of a dataset to help avoid the risk of overfitting. With respect to clustering errors, internal and relative indices can exhibit similar behavior, with the advantage of the former being less computationally expensive.<xref rid="bib13" ref-type="bibr"><sup>13</sup></xref> However, in the case of complex models and clusters, an approach based on the minimization of prediction error may be particularly advantageous because internal indices tend to fail to correlate well with errors.<xref rid="bib13" ref-type="bibr"><sup>13</sup></xref></p>
    <sec id="sec1.1">
      <title>Methods</title>
      <p id="p0045">Stability-based methods return the number of clusters that minimizes the expected distance between clustering solutions obtained for different datasets. Several options are available<xref rid="bib14" ref-type="bibr"><sup>14</sup></xref> to (1) generate the datasets (e.g., random subsampling of the original dataset<xref rid="bib15" ref-type="bibr"><sup>15</sup></xref>, or adding random noise<xref rid="bib16" ref-type="bibr"><sup>16</sup></xref>); (2) compare clustering solutions (e.g., overlapping subsamples<xref rid="bib15" ref-type="bibr"><sup>15</sup></xref>); and (3) compute clustering distances (e.g., the consensus index by Vinh and Epps<xref rid="bib17" ref-type="bibr"><sup>17</sup></xref>). The method proposed by Lange et al.<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref> has the advantage of transforming the unsupervised setting into a classification problem and guiding selection through the minimization of prediction error. First, a dataset is split into training and validation sets and then independently partitioned into clusters. Second, training set labels are used within supervised classification methods to learn how to best predict the labels. Applying the classification model to the validation set, the model's predicted labels are then compared with the actual clustering labels derived from the validation set. This procedure is repeated using cross-validation and the optimal number of clusters is identified corresponding to the maximum number of clusters that minimizes prediction error. Prediction performance is defined by the authors as the 0-1 loss in supervised classification<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref><sup>,</sup><xref rid="bib14" ref-type="bibr"><sup>14</sup></xref>, namely, the normalized Hamming distance. Nevertheless, other choices are possible; for example, Tibshirani and Walther<xref rid="bib4" ref-type="bibr"><sup>4</sup></xref> used prediction strength; that is, the proportion of observation pairs in the validation set that are assigned to the same cluster by both the clustering algorithm and the partition based on the training set centers.</p>
      <sec id="sec1.1.1">
        <title>Stability measure</title>
        <p id="p0050">The notion of stability by Lange et al.<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref> is used to assess solutions of clustering algorithms based on the rationale that true clusters are those that can always be identified by a clustering algorithm when applied to different datasets from the same generating process. Formally, let <inline-formula><mml:math id="M1" altimg="si1.gif"><mml:mrow><mml:msub><mml:mi mathvariant="script">A</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> be a clustering algorithm with <italic>k</italic> the number of clusters, <italic>ϕ</italic> a classifier, and <inline-formula><mml:math id="M2" altimg="si2.gif"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> the training set and clustering labels, i.e., <inline-formula><mml:math id="M3" altimg="si3.gif"><mml:mrow><mml:msub><mml:mi mathvariant="script">A</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow></mml:math></inline-formula>. After training <italic>ϕ</italic> on <inline-formula><mml:math id="M4" altimg="si2.gif"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, both the clustering algorithm and trained classifier are applied to a separate dataset <inline-formula><mml:math id="M5" altimg="si4.gif"><mml:mrow><mml:msup><mml:mi mathvariant="bold">X</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>. The distance between the two solutions is the normalized Hamming distance:<disp-formula id="fd1"><label>(Equation 1)</label><mml:math id="M6" altimg="si5.gif"><mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub><mml:mfenced><mml:mrow><mml:mi>ϕ</mml:mi><mml:mfenced><mml:msup><mml:mi mathvariant="bold">X</mml:mi><mml:mtext>′</mml:mtext></mml:msup></mml:mfenced><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">Y</mml:mi><mml:mtext>′</mml:mtext></mml:msup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:munder><mml:mtext>min</mml:mtext><mml:mrow><mml:mi>σ</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mn>1</mml:mn><mml:mfenced close="}" open="{"><mml:mrow><mml:mi>σ</mml:mi><mml:mfenced><mml:mrow><mml:mi>ϕ</mml:mi><mml:mfenced><mml:msubsup><mml:mi>X</mml:mi><mml:mi>i</mml:mi><mml:mtext>′</mml:mtext></mml:msubsup></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>≠</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>i</mml:mi><mml:mtext>′</mml:mtext></mml:msubsup></mml:mrow></mml:mfenced></mml:msub></mml:math></disp-formula>with <inline-formula><mml:math id="M7" altimg="si6.gif"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> the set of all possible permutations of <italic>k</italic> elements. Supervised labels are permuted to overcome the non-uniqueness of clustering labeling and <italic>σ</italic> is the permutation that minimizes the solutions dissimilarity. Averaging out the distance between any pair of partitions <inline-formula><mml:math id="M8" altimg="si7.gif"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msup><mml:mi mathvariant="bold">X</mml:mi><mml:mtext>′</mml:mtext></mml:msup></mml:mrow></mml:math></inline-formula> from <xref rid="fd1" ref-type="disp-formula">Equation 1</xref> we define the stability index of the clustering algorithm as:<disp-formula id="fd2"><label>(Equation 2)</label><mml:math id="M9" altimg="si8.gif"><mml:mrow><mml:mi mathvariant="script">S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">A</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:msub><mml:mi mathvariant="sans-serif">E</mml:mi><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">X</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold">X</mml:mi><mml:mtext>′</mml:mtext></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">Y</mml:mi><mml:mtext>′</mml:mtext></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p>
        <p id="p0055">The stability index ranges in <inline-formula><mml:math id="M10" altimg="si9.gif"><mml:mrow><mml:mo>[</mml:mo><mml:mn>0,1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula>, with lower values indicating more stable and reproducible solutions.<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref> Because this measure scales with the number of clusters, the measure suggested by the authors is the normalized stability <inline-formula><mml:math id="M11" altimg="si10.gif"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="script">S</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, i.e., the stability from <xref rid="fd2" ref-type="disp-formula">Equation 2</xref> normalized for the stability of random labeling <inline-formula><mml:math id="M12" altimg="si11.gif"><mml:mrow><mml:msub><mml:mi mathvariant="script">R</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
      </sec>
      <sec id="sec1.1.2">
        <title>Reval algorithm</title>
        <p id="p0060">The algorithm implemented in reval allows the user to (1) automatically select the number of clusters for a dataset by minimizing validation stability, via repeated cross-validation (see <xref rid="tbox1" ref-type="boxed-text">Algorithm 1</xref>); and (2) compute classification performance obtained when generalizing the solution to a held-out dataset (see <xref rid="tbox2" ref-type="boxed-text">Algorithm 2</xref>). An overview of the framework is reported in <xref rid="fig1" ref-type="fig">Figure 1</xref>.<boxed-text id="tbox1"><label>Algorithm 1</label><caption><title>Return number of clusters that minimizes normalized stability</title></caption><p id="p0280"><list list-type="simple" id="ulist0020"><list-item id="u0050"><p id="p0285">Input: <inline-formula><mml:math id="M13" altimg="si13.gif"><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M14" altimg="si15.gif"><mml:mrow><mml:mi mathvariant="script">A</mml:mi></mml:mrow></mml:math></inline-formula>, <italic>ϕ</italic>, <italic>K</italic>, <inline-formula><mml:math id="M15" altimg="si16.gif"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mtext>fold</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M16" altimg="si17.gif"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M17" altimg="si20.gif"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></p></list-item><list-item id="u0055"><p id="p0290">Result: <inline-formula><mml:math id="M18" altimg="si64.gif"><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mtext>∗</mml:mtext></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M19" altimg="si100.gif"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="script">S</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:msup><mml:mi>k</mml:mi><mml:mtext>∗</mml:mtext></mml:msup></mml:msub></mml:mrow></mml:math></inline-formula></p></list-item><list-item id="u0060"><p id="p0295">for <inline-formula><mml:math id="M20" altimg="si101.gif"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mtext>fold</mml:mtext></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo linebreak="badbreak">×</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> do<list list-type="simple" id="ulist0025"><list-item id="u0065"><p id="p0300">Find clustering solution <inline-formula><mml:math id="M21" altimg="si102.gif"><mml:mrow><mml:msub><mml:mi mathvariant="script">A</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:msubsup><mml:mi mathvariant="bold">Y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msubsup></mml:mrow></mml:math></inline-formula> and train <italic>ϕ</italic> on (<inline-formula><mml:math id="M22" altimg="si103.gif"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold">Y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msubsup></mml:mrow></mml:math></inline-formula>);</p></list-item><list-item id="u0070"><p id="p0305">Compute <inline-formula><mml:math id="M23" altimg="si104.gif"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:msub><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M24" altimg="si105.gif"><mml:mrow><mml:msub><mml:mi mathvariant="script">A</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:msubsup><mml:mi mathvariant="bold">Y</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msubsup></mml:mrow></mml:math></inline-formula>;</p></list-item><list-item id="u0075"><p id="p0310">Select permutation <inline-formula><mml:math id="M25" altimg="si106.gif"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:msub><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> that yields to minimum dissimilarity <inline-formula><mml:math id="M26" altimg="si107.gif"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:msub><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:msub><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold">Y</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>;</p></list-item><list-item id="u0080"><p id="p0315">for <inline-formula><mml:math id="M27" altimg="si108.gif"><mml:mrow><mml:mi>r</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> do<list list-type="simple" id="ulist0030"><list-item id="u0085"><p id="p0320">Train <italic>ϕ</italic> on (<inline-formula><mml:math id="M28" altimg="si109.gif"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="script">R</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">Y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>);</p></list-item><list-item id="u0090"><p id="p0325">Compute <inline-formula><mml:math id="M29" altimg="si110.gif"><mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mover><mml:mi>σ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mfenced><mml:msubsup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msubsup></mml:mfenced><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold">Y</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msubsup></mml:mrow></mml:mfenced></mml:math></inline-formula> as before;</p></list-item></list></p></list-item><list-item id="u0095"><p id="p0330">end</p></list-item><list-item id="u0100"><p id="p0335">Compute <inline-formula><mml:math id="M30" altimg="si111.gif"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:msub><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub></mml:msub><mml:mo>/</mml:mo><mml:mrow><mml:msubsup><mml:mtext>Avg</mml:mtext><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item id="u0105"><p id="p0340">Compute normalized stability <inline-formula><mml:math id="M31" altimg="si112.gif"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="script">S</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:msubsup><mml:mtext>Avg</mml:mtext><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:msubsup><mml:msubsup><mml:mtext>Avg</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mtext>fold</mml:mtext></mml:msub></mml:msubsup><mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula>;</p></list-item></list></p></list-item><list-item id="u0110"><p id="p0345">end</p></list-item><list-item id="u0115"><p id="p0350">Return <inline-formula><mml:math id="M32" altimg="si64.gif"><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mtext>∗</mml:mtext></mml:msup></mml:mrow></mml:math></inline-formula> s.t. <inline-formula><mml:math id="M33" altimg="si113.gif"><mml:msub><mml:mtext>max argmin</mml:mtext><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mover><mml:mi mathvariant="script">S</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>.</p></list-item></list></p><p id="p0355">Input parameters: training dataset (<inline-formula><mml:math id="M34" altimg="si13.gif"><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>), clustering algorithm (<inline-formula><mml:math id="M35" altimg="si15.gif"><mml:mrow><mml:mi mathvariant="script">A</mml:mi></mml:mrow></mml:math></inline-formula>), classification algorithm (<italic>ϕ</italic>), set of number of clusters to evaluate (<italic>K</italic>), number of cross-validation folds (<inline-formula><mml:math id="M36" altimg="si16.gif"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mtext>fold</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula>), number of cross-validation repetitions (<inline-formula><mml:math id="M37" altimg="si17.gif"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>), and number of random labeling repetitions (<inline-formula><mml:math id="M38" altimg="si20.gif"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>). With <inline-formula><mml:math id="M39" altimg="si114.gif"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo></mml:mrow></mml:math></inline-formula> we indicate the Cartesian product of sets, i.e., the set of all possible ordered combinations of elements in the sets, which is equivalent to a nested for-loop.</p></boxed-text><boxed-text id="tbox2"><label>Algorithm 2</label><caption><title>Test best solution on unseen data</title></caption><p id="p0360"><list list-type="simple" id="ulist0035"><list-item id="u0120"><p id="p0365">Input: <inline-formula><mml:math id="M40" altimg="si13.gif"><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M41" altimg="si14.gif"><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M42" altimg="si64.gif"><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mtext>∗</mml:mtext></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M43" altimg="si115.gif"><mml:mrow><mml:msub><mml:mi mathvariant="script">A</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>∗</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <italic>ϕ</italic></p></list-item><list-item id="u0125"><p id="p0370">Result: classification accuracy</p></list-item><list-item id="u0130"><p id="p0375">Find clustering solution <inline-formula><mml:math id="M44" altimg="si116.gif"><mml:mrow><mml:msub><mml:mi mathvariant="script">A</mml:mi><mml:msup><mml:mi>k</mml:mi><mml:mtext>∗</mml:mtext></mml:msup></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:msub><mml:mi mathvariant="bold">Y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and train <italic>ϕ</italic> on (<inline-formula><mml:math id="M45" altimg="si117.gif"><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">Y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>);</p></list-item><list-item id="u0135"><p id="p0380">Compute <inline-formula><mml:math id="M46" altimg="si118.gif"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:msup><mml:mi>k</mml:mi><mml:mtext>∗</mml:mtext></mml:msup></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M47" altimg="si119.gif"><mml:mrow><mml:msub><mml:mi mathvariant="script">A</mml:mi><mml:msup><mml:mi>k</mml:mi><mml:mtext>∗</mml:mtext></mml:msup></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:msub><mml:mi mathvariant="bold">Y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>;</p></list-item><list-item id="u0140"><p id="p0385">Compute the accuracy (ACC) with permuted clustering labels for consistency between training and test sets:</p></list-item></list><disp-formula id="ufd2"><mml:math id="M48" altimg="si120.gif"><mml:mtext>ACC</mml:mtext><mml:mo>=</mml:mo><mml:munder><mml:mtext>max</mml:mtext><mml:mrow><mml:mi>σ</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:msup><mml:mi>k</mml:mi><mml:mtext>∗</mml:mtext></mml:msup></mml:msub></mml:mrow></mml:munder><mml:msubsup><mml:mtext>Avg</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mn>1</mml:mn><mml:mfenced close="}" open="{"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mfenced><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mfenced><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:msub><mml:mfenced><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mfenced><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:msub></mml:math></disp-formula><list list-type="simple" id="ulist0040"><list-item id="u0145"><p id="p0390">Return <inline-formula><mml:math id="M49" altimg="si121.gif"><mml:mrow><mml:mtext>ACC</mml:mtext><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>0,1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item></list></p><p id="p0395">Input parameters: training dataset (<inline-formula><mml:math id="M50" altimg="si13.gif"><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>), testing dataset (<inline-formula><mml:math id="M51" altimg="si14.gif"><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>), best number of clusters selected with cross-validation (<inline-formula><mml:math id="M52" altimg="si64.gif"><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mtext>∗</mml:mtext></mml:msup></mml:mrow></mml:math></inline-formula>), clustering algorithm with best number of clusters fixed (<inline-formula><mml:math id="M53" altimg="si122.gif"><mml:mrow><mml:msub><mml:mi mathvariant="script">A</mml:mi><mml:msup><mml:mi>k</mml:mi><mml:mtext>∗</mml:mtext></mml:msup></mml:msub></mml:mrow></mml:math></inline-formula>), and classifier (<italic>ϕ</italic>).</p></boxed-text><fig id="fig1"><label>Figure 1</label><caption><p>reval implementation overview</p><p>Repeated cross-validation procedure is included within the dashed circle and it is repeated for different numbers of clusters <italic>k</italic> as indicated by the orange arrows (<xref rid="tbox1" ref-type="boxed-text">Algorithm 1</xref>). The clustering algorithm with number of clusters <inline-formula><mml:math id="M54" altimg="si64.gif"><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mtext>∗</mml:mtext></mml:msup></mml:mrow></mml:math></inline-formula>, i.e., the maximum value that minimizes normalized stability, is evaluated on a held-out dataset (<xref rid="tbox2" ref-type="boxed-text">Algorithm 2</xref>).</p></caption><graphic xlink:href="gr1"/></fig></p>
        <p id="p0065">A dataset <inline-formula><mml:math id="M55" altimg="si12.gif"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:math></inline-formula> is first split into training <inline-formula><mml:math id="M56" altimg="si13.gif"><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and test <inline-formula><mml:math id="M57" altimg="si14.gif"><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> sets and a clustering <inline-formula><mml:math id="M58" altimg="si15.gif"><mml:mrow><mml:mi mathvariant="script">A</mml:mi></mml:mrow></mml:math></inline-formula> and classifier <italic>ϕ</italic> are selected. Let <inline-formula><mml:math id="M59" altimg="si16.gif"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mtext>fold</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> be the number of folds for cross-validation and <inline-formula><mml:math id="M60" altimg="si17.gif"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> the number of repetitions. In <xref rid="tbox1" ref-type="boxed-text">Algorithm 1</xref>, we refer to <inline-formula><mml:math id="M61" altimg="si18.gif"><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mtext>fold</mml:mtext></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M62" altimg="si19.gif"><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula> as the sets of indices corresponding to fold splits. Moreover, let <inline-formula><mml:math id="M63" altimg="si20.gif"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> be the number of random labeling iterations, and <italic>k</italic> the number of clusters in set <italic>K</italic>. In <xref rid="tbox1" ref-type="boxed-text">Algorithm 1</xref> we indicate with <inline-formula><mml:math id="M64" altimg="si21.gif"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M65" altimg="si22.gif"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msubsup></mml:mrow></mml:math></inline-formula> the internal training and validation splits of training set <inline-formula><mml:math id="M66" altimg="si13.gif"><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, respectively, for cross-validation <italic>i</italic>th fold split at the <italic>j</italic>th shuffled repetition. These correspond to <inline-formula><mml:math id="M67" altimg="si12.gif"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M68" altimg="si4.gif"><mml:mrow><mml:msup><mml:mi mathvariant="bold">X</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> sets introduced in the stability measure section. With <inline-formula><mml:math id="M69" altimg="si23.gif"><mml:mfenced><mml:mrow><mml:mi>K</mml:mi><mml:mo>×</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mtext>fold</mml:mtext></mml:msub><mml:mo>]</mml:mo><mml:mo>×</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula> we indicate the Cartesian product of the sets of number of clusters and repeated cross-validation splits. The fitted model becomes the one trained on <inline-formula><mml:math id="M70" altimg="si24.gif"><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> that returns the maximum number of clusters with minimum stability. That model can then be used within <xref rid="tbox2" ref-type="boxed-text">Algorithm 2</xref> for generalization on the test set.</p>
        <p id="p0070">Among clustering methods that work within reval, hierarchical density-based spatial clustering of applications with noise (HDBSCAN)<xref rid="bib18" ref-type="bibr"><sup>18</sup></xref> does not need any assumption on the number of clusters. Hence, we do not need to iterate over different number of clusters to select the best solution. Instead, normalized stability is computed within the repeated cross-validation loops that return the same number of clusters.</p>
      </sec>
      <sec id="sec1.1.3">
        <title>Technical details</title>
        <p id="p0075">The reval package has 4 core modules and additional functions that can be found in the <italic>utils</italic> file.<list list-type="simple" id="ulist0015"><list-item id="u0030"><label>•</label><p id="p0080">relative_validation. This module includes training and test methods that return misclassification errors obtained by comparing classification labels and clustering labels. It also includes the random labeling method, which allows users to compute the asymptotic misclassification rate.</p></list-item><list-item id="u0035"><label>•</label><p id="p0085">best_nclust_cv. This module implements repeated cross-validation and returns the best clustering solution together with normalized stability scores, obtained from the average of the misclassification scores divided by the asymptotic misclassification rate. Repeated cross-validation leads to unbiased stability estimates and it can also be performed stratifying the repeated randomized splits according to a desired variable. To control for the size imbalance that derives from cross-validation, we initialized the repeated cross-validation loop to a <inline-formula><mml:math id="M71" altimg="si25.gif"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> schema as default. Users can change this configuration according to dataset size and available stratifiers, which can be useful to overcome imbalance issues. The evaluation method applies the fitted model with the returned number of clusters to the held-out dataset and returns accuracy (ACC). Other metrics, such as Matthews correlation coefficient (MCC)<xref rid="bib19" ref-type="bibr"><sup>19</sup></xref>, F1 score, precision, and recall scores, can also be computed (see <italic>utils</italic> file).</p></list-item><list-item id="u0040"><label>•</label><p id="p0090">param_selection. This module enables hyperparameter tuning to select the best configuration of classifier/clustering (SCParamSelection class) and the parameters within clustering and classifier themselves (ParamSelection class). Best parameters are those that report minimum normalized stability. If the number of true classes is available, this module also returns the best solution that correctly identifies the true number of clusters, if it exists.</p></list-item><list-item id="u0045"><label>•</label><p id="p0095">visualization. This module includes the function to plot cross-validation performance metrics with 95% confidence intervals for a varying number of clustering solutions. The threshold of random labeling stability can be displayed to visually investigate model performance.</p></list-item></list></p>
        <p id="p0100">As suggested by Lange et al.<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref>, we used the Kuhn-Munkres algorithm<xref rid="bib20" ref-type="bibr"><sup>20</sup></xref><sup>,</sup><xref rid="bib21" ref-type="bibr"><sup>21</sup></xref> to obtain the label permutation that minimizes misclassification error. However, differently from the work of Lange et al.<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref>, reval permutes the clustering labels instead of the classification labels, i.e., the normalized Hamming distance in <xref rid="fd1" ref-type="disp-formula">Equation 1</xref> becomes:<disp-formula id="ufd1"><mml:math id="M72" altimg="si26.gif"><mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub><mml:mfenced><mml:mrow><mml:mi>ϕ</mml:mi><mml:mfenced><mml:msup><mml:mi mathvariant="bold">X</mml:mi><mml:mtext>′</mml:mtext></mml:msup></mml:mfenced><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">Y</mml:mi><mml:mtext>′</mml:mtext></mml:msup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:munder><mml:mtext>min</mml:mtext><mml:mrow><mml:mi>σ</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mn>1</mml:mn><mml:mfenced close="}" open="{"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mfenced><mml:msubsup><mml:mi>X</mml:mi><mml:mi>i</mml:mi><mml:mtext>′</mml:mtext></mml:msubsup></mml:mfenced><mml:mo>≠</mml:mo><mml:mi>σ</mml:mi><mml:mfenced><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>i</mml:mi><mml:mtext>′</mml:mtext></mml:msubsup></mml:mfenced></mml:mrow></mml:mfenced></mml:msub></mml:math></disp-formula></p>
        <p id="p0105">This approach allows the test set to preserve the partition structure of the training set, to better investigate results replicability and to aid visual comparison. Figure 2 shows the rationale behind the need to permute clustering labels when training a classifier within reval. We simulated 3 Gaussian blobs and divided them into training (<inline-formula><mml:math id="M73" altimg="si27.gif"><mml:mrow><mml:mi>N</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula>) and test (<inline-formula><mml:math id="M74" altimg="si28.gif"><mml:mrow><mml:mi>N</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>) sets. <xref rid="fig2" ref-type="fig">Figure 2</xref>A shows clustering labels for the training set and <xref rid="fig2" ref-type="fig">Figure 2</xref>B the clustering labels for the test set. <xref rid="fig2" ref-type="fig">Figure 2</xref>C shows what might happen when training a classifier on the training set labels and then predicts labels for the test set. Because 2 out of 3 classes show label discordance, the trained classifier fails to correctly predict the classes. Nevertheless, if we permute class labels 0 and 1 in the example, the trained classifier will correctly identify all 3 classes on the test set returning minimum prediction error and consistent label ordering is preserved. The kuhn_munkres_algorithm() function can be found in <italic>utils</italic> file.<fig id="fig2"><label>Figure 2</label><caption><p>Relabeling practice</p><p>Clustering labels on the training (A) and test (B) sets. Differences in labeling between training and test sets are displayed in (C). The labeled points on which the classifier is trained are shown in blue, the labeled points whose classes we want to predict are in black.</p></caption><graphic xlink:href="gr2"/></fig></p>
        <p id="p0110">A more thorough description of the code and its usage can be found at <ext-link ext-link-type="uri" xlink:href="https://reval.readthedocs.io/en/latest/code_description.html" id="intref0015">https://reval.readthedocs.io/en/latest/code_description.html</ext-link>. reval mainly works with the scikit-learn Python library for machine learning.<xref rid="bib22" ref-type="bibr"><sup>22</sup></xref> In particular, among clustering methods, users can select those with number of clusters parameter, i.e., k-means, hierarchical clustering, and spectral clustering, but also density-based clustering HDBSCAN<xref rid="bib18" ref-type="bibr"><sup>18</sup></xref> from the hdbscan library. Moreover, any classifier from scikit-learn can be selected.</p>
      </sec>
      <sec id="sec1.1.4">
        <title>Algorithm complexity</title>
        <p id="p0115">We report here the complexity analysis of the 2 core methods included in best_nclust_cv module. In particular, we focus on best_nclustcv(), which enables the selection of the best number of clusters via repeated cross-validation, and evaluate(), which implements the testing of the best solution on held-out datasets. The best_nclustcv() method includes sequential calls to train (train()), test (test()), and random labeling (rndlabels_traineval()) methods from the relative_validation module, whereas evaluate() sequentially calls the train and test functions.</p>
        <p id="p0120">The complexity of the training method is led by the sum of the costs of the chosen classification (i.e., <inline-formula><mml:math id="M75" altimg="si29.gif"><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtext>C</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) and clustering (i.e., <inline-formula><mml:math id="M76" altimg="si30.gif"><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtext>G</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) algorithms, which depend on the number of data samples and features (see <xref rid="tbl1" ref-type="table">Table 1</xref>). The complexity of the test() method mainly depends on <inline-formula><mml:math id="M77" altimg="si30.gif"><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtext>G</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in that only prediction is performed for the classifier. The random labeling algorithm increases <inline-formula><mml:math id="M78" altimg="si29.gif"><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtext>C</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> by a factor of <inline-formula><mml:math id="M79" altimg="si20.gif"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (i.e., the number of random labeling iterations). The overall complexity to perform cross-validation and evaluation depends on the number of calls to the relative validation module functions and their intrinsic cost. To perform cross-validation and compute normalized stability, we need to set the following parameters: <inline-formula><mml:math id="M80" altimg="si31.gif"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>K</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (i.e., the cardinality of the set with the different number of clusters to try); <inline-formula><mml:math id="M81" altimg="si16.gif"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mtext>fold</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M82" altimg="si17.gif"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, which correspond to the number of cross-validation folds and repetitions, respectively; and <inline-formula><mml:math id="M83" altimg="si20.gif"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. In conclusion, the cost of best_nclustcv() is equal to <inline-formula><mml:math id="M84" altimg="si32.gif"><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo linebreak="badbreak">⋅</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mtext>fold</mml:mtext></mml:msub><mml:mo linebreak="badbreak">⋅</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo linebreak="badbreak">⋅</mml:mo><mml:mo>(</mml:mo><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:mtext>G</mml:mtext><mml:mo>)</mml:mo><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">⋅</mml:mo><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:mtext>C</mml:mtext><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> and the complexity of evaluate() is <inline-formula><mml:math id="M85" altimg="si33.gif"><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtext>C</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">+</mml:mo><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtext>G</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.<table-wrap position="float" id="tbl1"><label>Table 1</label><caption><p>Algorithm complexity</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Algorithm</th><th>Complexity</th><th>Problem</th></tr></thead><tbody><tr><td>HDBSCAN</td><td><inline-formula><mml:math id="M86" altimg="si66.gif"><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mtext>log</mml:mtext><mml:mi>N</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td>clustering</td></tr><tr><td>K-means</td><td><inline-formula><mml:math id="M87" altimg="si67.gif"><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mi>N</mml:mi><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td>clustering</td></tr><tr><td>Agglomerative</td><td><inline-formula><mml:math id="M88" altimg="si68.gif"><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td>clustering</td></tr><tr><td>Spectral</td><td><inline-formula><mml:math id="M89" altimg="si69.gif"><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td>clustering</td></tr><tr><td>LR</td><td><inline-formula><mml:math id="M90" altimg="si70.gif"><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td>classification</td></tr><tr><td>KNN</td><td><inline-formula><mml:math id="M91" altimg="si70.gif"><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td>classification</td></tr><tr><td>Support vector machine</td><td><inline-formula><mml:math id="M92" altimg="si71.gif"><mml:mrow><mml:mo>[</mml:mo><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>;</mml:mo><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:msup><mml:mi>N</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula></td><td>classification</td></tr><tr><td>RF</td><td><inline-formula><mml:math id="M93" altimg="si72.gif"><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td>classification</td></tr></tbody></table><table-wrap-foot><fn id="tspara0015"><p><inline-formula><mml:math id="M94" altimg="si73.gif"><mml:mrow><mml:mi>N</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mtext>number of samples</mml:mtext></mml:mrow></mml:math></inline-formula>; <inline-formula><mml:math id="M95" altimg="si74.gif"><mml:mrow><mml:mi>p</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mtext>number of features</mml:mtext></mml:mrow></mml:math></inline-formula></p></fn><fn id="tspara0020"><p>K-means: <inline-formula><mml:math id="M96" altimg="si75.gif"><mml:mrow><mml:mi>k</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mtext>number of clusters</mml:mtext></mml:mrow></mml:math></inline-formula>; <inline-formula><mml:math id="M97" altimg="si76.gif"><mml:mrow><mml:mi>T</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mtext>number of iterations</mml:mtext></mml:mrow></mml:math></inline-formula></p></fn><fn id="tspara0025"><p>Agglomerative: <inline-formula><mml:math id="M98" altimg="si75.gif"><mml:mrow><mml:mi>k</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mtext>number of clusters</mml:mtext></mml:mrow></mml:math></inline-formula></p></fn><fn id="tspara0030"><p>RF: <inline-formula><mml:math id="M99" altimg="si77.gif"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mtext>number of trees in the forest</mml:mtext></mml:mrow></mml:math></inline-formula></p></fn><fn id="tspara0035"><p>Clustering and classification algorithms available within reval package from the scikit-learn and hdbscan libraries.</p></fn></table-wrap-foot></table-wrap></p>
        <p id="p0125">We run the methods considered on simulated datasets to empirically investigate the execution times. Simulations were run on a MacBook Pro 2020 with a 2.3 GHz Quad-Core Intel Core i7 processor and 32 GB of RAM. The complexity of clustering algorithms and state-of-the-art classifiers that can be used with reval can be found in <xref rid="tbl1" ref-type="table">Table 1</xref>. <xref rid="fig3" ref-type="fig">Figures 3</xref>A and 3B show the execution times of best_nclustcv() and evaluate(), respectively, on a 2-blob dataset of 100 samples and 10 features with different combinations of classifiers and clustering algorithms. <inline-formula><mml:math id="M100" altimg="si34.gif"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is set at 5, <inline-formula><mml:math id="M101" altimg="si16.gif"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mtext>fold</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M102" altimg="si17.gif"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are 2 and 10 respectively, and <inline-formula><mml:math id="M103" altimg="si20.gif"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is equal to 10. Default parameters were used for all classifiers and clustering algorithms. It is straightforward to observe that execution times largely depend on the chosen algorithms, with HDBSCAN the least expensive and spectral clustering the most expensive choice among clustering techniques, irrespective of classifier, and random forest the most expensive choice among classifiers. Overall, the execution time of the evaluate() method is reduced compared with repeated cross-validation. The package implementation also includes a multiprocessing feature that speeds up computations for repeated cross-validation but not for the evaluation method (see <xref rid="mmc1" ref-type="supplementary-material">Figure S1</xref>A), where 7 jobs are simultaneously run. <xref rid="fig3" ref-type="fig">Figure 3</xref>C shows the execution times with varying number of samples and features when sequentially running best_nclustcv() and evaluate() with KNN classifier and k-means clustering. We can observe a moderate increase for low number of samples (<inline-formula><mml:math id="M104" altimg="si35.gif"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>) with varying number of features and a steep growth for <inline-formula><mml:math id="M105" altimg="si36.gif"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> samples. Corresponding execution times with multiprocessing can be observed in <xref rid="mmc1" ref-type="supplementary-material">Figure S1</xref>C.<fig id="fig3"><label>Figure 3</label><caption><p>Execution times</p><p>Different combinations of classification and clustering algorithms applied to blobs dataset with 100 samples and 10 features. best_nclustcv() (A); evaluate() (B); sequential calls to best_nclustcv() and evaluate() (C) with KNN and k-means for blobs dataset with varying combinations of samples and features.</p></caption><graphic xlink:href="gr3"/></fig></p>
      </sec>
    </sec>
  </sec>
  <sec id="sec2.4">
    <title>Results</title>
    <sec id="sec2">
      <title>Technical validation</title>
      <p id="p0130">To investigate reval performance, first we investigate group detection in simulated non-overlapping Gaussian blobs, and the handwritten digits dataset from the Modified National Institute of Standards and Technology (MNIST) digit recognition database (<ext-link ext-link-type="uri" xlink:href="http://yann.lecun.com/exdb/mnist/" id="intref0020">http://yann.lecun.com/exdb/mnist/</ext-link>). Then, we leverage the SCParamSelection class to determine the combination of classifier and clustering algorithms that best identifies the number of classes for 19 different datasets from the University of California, Irvine (UCI) Machine Learning repository.<xref rid="bib23" ref-type="bibr"><sup>23</sup></xref></p>
    </sec>
    <sec id="sec2.1">
      <title>Blobs dataset</title>
      <p id="p0135">To provide a simple example of how to use reval, we generated 5 isotropic Gaussian blobs for a total of 1,000 samples with 2 features. To run the example, refer to Code S1-S6 in the blobs dataset section of the supplemental material. The dataset was first split into training and test sets (<inline-formula><mml:math id="M106" altimg="si37.gif"><mml:mrow><mml:mn>70</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>30</mml:mn><mml:mtext>%</mml:mtext></mml:mrow></mml:math></inline-formula>). Then we selected KNN with the number of neighbors equal to 15 and k-means with Euclidean distance, as suggested by the experiment reported in the algorithm selection section. We then run the stability-based algorithm with a <inline-formula><mml:math id="M107" altimg="si38.gif"><mml:mrow><mml:mn>10</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> repeated cross-validation framework, with 10 random labeling repetitions, and the number of clusters varying from 2 to 6. We evaluated the solution found on the test set and report ACC and MCC scores as metrics. Last, we report the adjusted mutual information (AMI) score to compare true labels and predicted labels on the test set. AMI external score measures the similarity of 2 labelings of the same data, irrespective of label order and ranges from [0, 1], with a perfect match equal to 1. We also determined the best number of clusters maximizing and minimizing silhouette and Davies-Bouldin internal measures, respectively. Davies-Bouldin index<sup>6</sup> measures clusters separation and “tightness” with Euclidean distance. Lower indices correspond to better solutions. We computed the metrics independently on the training and test sets and return the best number of clusters found with k-means.</p>
      <p id="p0140">The normalized stability for varying number of clusters in <xref rid="fig4" ref-type="fig">Figure 4</xref> shows that the model perfectly identified 5 clusters with 0.0 normalized stability. The comparison between true and clustering labels can be observed in <xref rid="mmc1" ref-type="supplementary-material">Figure S3</xref>. The ACC and MCC scores on the test set are equal to 1.0 and the AMI is equal to 1.0. The maximum silhouette score is 0.83 on both training and test sets with the correct number of clusters and AMI scores equal to 1.0. The Davies-Bouldin index results do not replicate between training and test sets. The index is equal to 0.23 with 4 clusters on the training set and to 0.23 with 5 clusters on the test set. AMI scores are 0.91 for training and 1.0 for testing.<fig id="fig4"><label>Figure 4</label><caption><p>reval performance for blobs dataset</p><p>Solid line represents the validation normalized stability with <inline-formula><mml:math id="M108" altimg="si65.gif"><mml:mrow><mml:mn>95</mml:mn><mml:mtext>%</mml:mtext></mml:mrow></mml:math></inline-formula> confidence intervals. Dashed line shows stability during training.</p></caption><graphic xlink:href="gr4"/></fig></p>
    </sec>
    <sec id="sec2.2">
      <title>MNIST dataset</title>
      <p id="p0145">For the real-world dataset example, we considered the handwritten digits MNIST dataset (<ext-link ext-link-type="uri" xlink:href="http://yann.lecun.com/exdb/mnist/" id="intref0025">http://yann.lecun.com/exdb/mnist/</ext-link>), which includes <inline-formula><mml:math id="M109" altimg="si39.gif"><mml:mrow><mml:mn>70,000</mml:mn></mml:mrow></mml:math></inline-formula> samples corresponding to <inline-formula><mml:math id="M110" altimg="si40.gif"><mml:mrow><mml:mn>28</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>28</mml:mn></mml:mrow></mml:math></inline-formula> images of digits from 0 to 9. When flattened, each sample has 784 features. The dataset has 10 classes, with ~7,000 samples for each class.</p>
      <p id="p0150">First, we split the dataset into training and test sets of <inline-formula><mml:math id="M111" altimg="si41.gif"><mml:mrow><mml:mn>60,000</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M112" altimg="si42.gif"><mml:mrow><mml:mn>10,000</mml:mn></mml:mrow></mml:math></inline-formula> samples, respectively. Then, we preprocessed the dataset with uniform manifold approximation and projection (UMAP)<xref rid="bib24" ref-type="bibr"><sup>24</sup></xref> for dimensionality reduction with 2 components. We run HDBSCAN clustering, as suggested in <ext-link ext-link-type="uri" xlink:href="https://umap-learn.readthedocs.io/en/latest/clustering.html" id="intref0030">https://umap-learn.readthedocs.io/en/latest/clustering.html</ext-link>, with different classifiers and selected the best configuration. In particular, we considered k-nearest neighbors (KNN) with the number of neighbors equal to 30, and support vector machines (SVMs), random forest (RF), logistic regression (LR) with default parameters from scikit-learn library. The HDBSCAN algorithm was initialized with minimum samples equal to 10 and minimum cluster size equal to 200. The relative clustering validation procedure was run with 10 repetitions of 2-fold cross-validation, and number of random labeling iterations equal to 10. Because HDBSCAN does not need the number of clusters specified <italic>a priori</italic>, the normalized stability is computed averaging the misclassification error over the solutions that return the same number of clusters. We selected the one that minimizes the normalized stability and the trained classifier applied to test set is the one trained on the corresponding cross-validation fold. Because we are not preselecting the number of clusters, results might differ between training and test sets.</p>
      <p id="p0155">The best classifier-clustering combination is RF and HDBSCAN, which correctly identifies 10 clusters on the training set, with a misclassification error equal to <inline-formula><mml:math id="M113" altimg="si43.gif"><mml:mrow><mml:mn>0.06</mml:mn><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>0.06</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and AMI score equal to 0.91 (see <xref rid="fig5" ref-type="fig">Figure 5</xref>). On the test set, it identifies 9 clusters with AMI equal to 0.87. When the experiment was run with internal validation measures, we obtained 10 clusters in training and 9 clusters during testing, with AMI scores equal to 0.88 and 0.87, respectively, for both silhouette and Davies-Bouldin measures. In <xref rid="fig6" ref-type="fig">Figure 6</xref> and by the comparison of training set AMI scores between reval and internal measures, we observe that internal measures fail to detect the actual digit classes during training, whereas reval with HDBSCAN and RF successfully identifies them (see <xref rid="fig6" ref-type="fig">Figures 6</xref>C and 6D), with AMI score of 0.91. On test set the result is the same among all methods, as demonstrated by equal AMI scores (see <xref rid="mmc1" ref-type="supplementary-material">Figure S4)</xref>. Based on these results, we can speculate that clustering on a subset of data (<inline-formula><mml:math id="M114" altimg="si44.gif"><mml:mrow><mml:mi>N</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>35,000</mml:mn></mml:mrow></mml:math></inline-formula>) better detects the digits classes.<fig id="fig5"><label>Figure 5</label><caption><p>reval performance for MNIST dataset with RF and HDBSCAN algorithms</p><p>Solid line represents the validation normalized stability with <inline-formula><mml:math id="M115" altimg="si65.gif"><mml:mrow><mml:mn>95</mml:mn><mml:mtext>%</mml:mtext></mml:mrow></mml:math></inline-formula> confidence intervals. Dashed line shows training stability.</p></caption><graphic xlink:href="gr5"/></fig><fig id="fig6"><label>Figure 6</label><caption><p>MNIST cluster visualization</p><p>Training set UMAP visualization for true labels (A); relative validation labels (B); silhouette score labels (C); Davies-Bouldin index labels (D). For internal measures we circled the erroneous cluster identified.</p></caption><graphic xlink:href="gr6"/></fig></p>
      <p id="p0160">It is worth noting that the classifier performance highly depends on the clustering solution in that it tends to overfit to the training dataset. To guide the choice of a classifier, users should first select an appropriate clustering algorithm, then select the classifier that minimizes stability and that reports the best performance on the held-out dataset. In case of equal or very similar outcomes, algorithm complexity should guide the classifier selection.</p>
      <sec id="sec2.2.1">
        <title>Algorithm selection</title>
        <p id="p0165">We considered 19 datasets from the UCI Machine Learning repository,<xref rid="bib23" ref-type="bibr"><sup>23</sup></xref> including the test set of the handwritten digits dataset that can be found in scikit-learn toy datasets (<ext-link ext-link-type="uri" xlink:href="https://scikit-learn.org/stable/datasets/index.html" id="intref0035">https://scikit-learn.org/stable/datasets/index.html</ext-link>). In <xref rid="tbl2" ref-type="table">Table 2</xref> we report the dataset names along with the number of samples, features, and inherent classes. We applied the relative validation algorithm with different combinations of classifier and clustering algorithms. In particular, for clustering we selected hierarchical clustering with Ward's method and Euclidean distance, k-means clustering with Euclidean distance, and HDBSCAN with minimum cluster size equal to 5, and Euclidean distance. Among classifiers, we opted for KNN with 1 neighbor and Euclidean distance, RF classifier with 100 estimators, SVM with <inline-formula><mml:math id="M116" altimg="si45.gif"><mml:mrow><mml:mi>C</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M117" altimg="si46.gif"><mml:mrow><mml:mi>γ</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mtext>N samples</mml:mtext></mml:mfrac></mml:mrow></mml:math></inline-formula>, and LR. We did not consider spectral clustering because of its computational cost (see <xref rid="tbl1" ref-type="table">Table 1</xref>). To improve the performance, in addition to raw datasets, we repeatedly run the experiments after preprocessing with (1) standard scaler, which removes the mean and scales to unit variance; (2) UMAP algorithm<xref rid="bib24" ref-type="bibr"><sup>24</sup></xref> for dimensionality reduction; and (3) standard scaler and UMAP. UMAP parameters are chosen according to those suggested in the documentation (<ext-link ext-link-type="uri" xlink:href="https://umap-learn.readthedocs.io/en/latest/clustering.html" id="intref0040">https://umap-learn.readthedocs.io/en/latest/clustering.html</ext-link>). We ran the algorithm with 10 replications of 2-fold cross-validation and 10 random labeling iterations. The number of clusters ranges from 2 to <inline-formula><mml:math id="M118" altimg="si47.gif"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M119" altimg="si48.gif"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mtext>classes</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> is the number of classes for each dataset.<table-wrap position="float" id="tbl2"><label>Table 2</label><caption><p>Benchmark datasets from the UCI Machine Learning repository</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Dataset</th><th>Samples</th><th>Features</th><th>Classes</th></tr></thead><tbody><tr><td>Handwritten digits</td><td align="char">1,797</td><td align="char">64</td><td align="char">10</td></tr><tr><td>Yeast</td><td align="char">1,484</td><td align="char">8</td><td align="char">10</td></tr><tr><td>Banknote</td><td align="char">1,372</td><td align="char">4</td><td align="char">2</td></tr><tr><td>Biodegradation</td><td align="char">1,055</td><td align="char">41</td><td align="char">2</td></tr><tr><td>Transfusion</td><td align="char">748</td><td align="char">4</td><td align="char">2</td></tr><tr><td>Breast cancer Wisconsin (WI)</td><td align="char">683</td><td align="char">9</td><td align="char">2</td></tr><tr><td>Urban land cover</td><td align="char">675</td><td align="char">147</td><td align="char">9</td></tr><tr><td>Climate</td><td align="char">540</td><td align="char">18</td><td align="char">2</td></tr><tr><td>Forest type</td><td align="char">523</td><td align="char">27</td><td align="char">4</td></tr><tr><td>Wholesale</td><td align="char">440</td><td align="char">7</td><td align="char">3</td></tr><tr><td>Movement</td><td align="char">360</td><td align="char">90</td><td align="char">15</td></tr><tr><td>Ionosphere</td><td align="char">351</td><td align="char">34</td><td align="char">2</td></tr><tr><td>Liver disorders</td><td align="char">345</td><td align="char">5</td><td align="char">2</td></tr><tr><td>Leaf</td><td align="char">340</td><td align="char">14</td><td align="char">30</td></tr><tr><td>Ecoli</td><td align="char">336</td><td align="char">7</td><td align="char">8</td></tr><tr><td>Glass</td><td align="char">214</td><td align="char">9</td><td align="char">6</td></tr><tr><td>Seeds</td><td align="char">210</td><td align="char">7</td><td align="char">3</td></tr><tr><td>Parkinsons</td><td align="char">195</td><td align="char">22</td><td align="char">2</td></tr><tr><td>Iris</td><td align="char">150</td><td align="char">4</td><td align="char">3</td></tr></tbody></table></table-wrap></p>
        <p id="p0170"><xref rid="tbl3" ref-type="table">Table 3</xref> reports the best solutions selected as those reporting minimum stability along with the correct number of clusters. If no experiment identified the correct number of classes, we chose, among the solutions with minimum stability for each preprocessed dataset, the one with maximum AMI on test set. For comparison, we computed the silhouette score and Davies-Bouldin index<xref rid="bib6" ref-type="bibr"><sup>6</sup></xref> internal measures independently on both training and test sets with the clustering algorithm selected by the stability-based approach and with the same varying number of clusters. We report in <xref rid="tbl3" ref-type="table">Table 3</xref> the best solutions as those that maximize and minimize those measures, respectively, and AMI scores to evaluate the similarity of true and cluster labels on test sets.<table-wrap position="float" id="tbl3"><label>Table 3</label><caption><p>Best combinations of classifiers and clustering methods for reval with grid search applied to benchmark datasets from the UCI Machine Learning repository</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Dataset</th><th rowspan="2">Classes</th><th colspan="5">reval<hr/></th><th colspan="2">Silhouette<hr/></th><th colspan="2">Davies-Bouldin<hr/></th></tr><tr><th>Clusters</th><th>Best model</th><th>Preprocessing</th><th>Stability (error)</th><th>AMI</th><th>Clusters</th><th>AMI</th><th>Clusters</th><th>AMI</th></tr></thead><tbody><tr><td>Handwritten digits</td><td align="char">10</td><td>10</td><td>KNN/k-means</td><td>UMAP</td><td align="char">0.0</td><td align="char">0.76</td><td><inline-formula><mml:math id="M120" altimg="si78.gif"><mml:mrow><mml:mn>10</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.76</td><td><inline-formula><mml:math id="M121" altimg="si78.gif"><mml:mrow><mml:mn>10</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.76</td></tr><tr><td>Yeast</td><td align="char">10</td><td>7</td><td>RF/k-means∗</td><td>scaled + UMAP</td><td><inline-formula><mml:math id="M122" altimg="si79.gif"><mml:mrow><mml:mn>0.05</mml:mn><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>0.01</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="char">0.25</td><td><inline-formula><mml:math id="M123" altimg="si80.gif"><mml:mrow><mml:mn>4</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.25</td><td><inline-formula><mml:math id="M124" altimg="si80.gif"><mml:mrow><mml:mn>4</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.25</td></tr><tr><td>Banknote</td><td align="char">2</td><td>2</td><td>SVM/HC</td><td>scaled + UMAP</td><td><inline-formula><mml:math id="M125" altimg="si81.gif"><mml:mrow><mml:mn>0.003</mml:mn><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>0.006</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="char">0.93</td><td><inline-formula><mml:math id="M126" altimg="si82.gif"><mml:mrow><mml:mn>2</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.93</td><td><inline-formula><mml:math id="M127" altimg="si82.gif"><mml:mrow><mml:mn>2</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.93</td></tr><tr><td>Biodegradation</td><td align="char">2</td><td>2</td><td>KNN/k-means</td><td>Raw</td><td><inline-formula><mml:math id="M128" altimg="si83.gif"><mml:mrow><mml:mn>0.03</mml:mn><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>0.006</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="M129" altimg="si84.gif"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>0.002</mml:mn></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="M130" altimg="si82.gif"><mml:mrow><mml:mn>2</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="M131" altimg="si84.gif"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>0.002</mml:mn></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="M132" altimg="si82.gif"><mml:mrow><mml:mn>2</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="M133" altimg="si84.gif"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>0.002</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td>Transfusion</td><td align="char">2</td><td>2</td><td>KNN/k-means</td><td>UMAP</td><td align="char">0.0</td><td align="char">0.005</td><td><inline-formula><mml:math id="M134" altimg="si82.gif"><mml:mrow><mml:mn>2</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.005</td><td><inline-formula><mml:math id="M135" altimg="si82.gif"><mml:mrow><mml:mn>2</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.005</td></tr><tr><td>Breast cancer (WI)</td><td align="char">2</td><td>2</td><td>SVM/k-means</td><td>raw</td><td><inline-formula><mml:math id="M136" altimg="si85.gif"><mml:mrow><mml:mn>0.03</mml:mn><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>0.01</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="char">0.76</td><td><inline-formula><mml:math id="M137" altimg="si82.gif"><mml:mrow><mml:mn>2</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.76</td><td><inline-formula><mml:math id="M138" altimg="si82.gif"><mml:mrow><mml:mn>2</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.76</td></tr><tr><td>Urban land cover</td><td align="char">9</td><td>3</td><td>KNN/k-means∗</td><td>scaled + UMAP</td><td><inline-formula><mml:math id="M139" altimg="si86.gif"><mml:mrow><mml:mn>0.006</mml:mn><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>0.003</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="char">0.45</td><td><inline-formula><mml:math id="M140" altimg="si87.gif"><mml:mrow><mml:mn>3</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.45</td><td><inline-formula><mml:math id="M141" altimg="si87.gif"><mml:mrow><mml:mn>3</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.45</td></tr><tr><td>Climate</td><td align="char">2</td><td>2</td><td>KNN/k-means</td><td>scaled + UMAP</td><td><inline-formula><mml:math id="M142" altimg="si53.gif"><mml:mrow><mml:mn>0.20</mml:mn><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>0.04</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="M143" altimg="si54.gif"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>0.005</mml:mn></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="M144" altimg="si82.gif"><mml:mrow><mml:mn>2</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="M145" altimg="si54.gif"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>0.005</mml:mn></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="M146" altimg="si82.gif"><mml:mrow><mml:mn>2</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="M147" altimg="si54.gif"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>0.005</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td>Forest type</td><td align="char">4</td><td>4</td><td>KNN/HC</td><td>raw</td><td><inline-formula><mml:math id="M148" altimg="si88.gif"><mml:mrow><mml:mn>0.35</mml:mn><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>0.11</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="char">0.55</td><td><inline-formula><mml:math id="M149" altimg="si80.gif"><mml:mrow><mml:mn>4</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.55</td><td><inline-formula><mml:math id="M150" altimg="si80.gif"><mml:mrow><mml:mn>4</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.55</td></tr><tr><td>wholesale</td><td align="char">3</td><td>3</td><td>SVM/k-means</td><td>UMAP</td><td><inline-formula><mml:math id="M151" altimg="si89.gif"><mml:mrow><mml:mn>0.07</mml:mn><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>0.03</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="char">0.002</td><td><inline-formula><mml:math id="M152" altimg="si87.gif"><mml:mrow><mml:mn>3</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.002</td><td><inline-formula><mml:math id="M153" altimg="si87.gif"><mml:mrow><mml:mn>3</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.002</td></tr><tr><td>Movement</td><td align="char">15</td><td>6</td><td>KNN/HC∗</td><td>UMAP</td><td><inline-formula><mml:math id="M154" altimg="si90.gif"><mml:mrow><mml:mn>0.05</mml:mn><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>0.02</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="char">0.42</td><td><inline-formula><mml:math id="M155" altimg="si91.gif"><mml:mrow><mml:mn>6</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.42</td><td><inline-formula><mml:math id="M156" altimg="si91.gif"><mml:mrow><mml:mn>6</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.42</td></tr><tr><td>Ionosphere</td><td align="char">2</td><td>2</td><td>SVM/k-means</td><td>raw</td><td><inline-formula><mml:math id="M157" altimg="si92.gif"><mml:mrow><mml:mn>0.01</mml:mn><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>0.009</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="char">0.21</td><td><inline-formula><mml:math id="M158" altimg="si82.gif"><mml:mrow><mml:mn>2</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.21</td><td><inline-formula><mml:math id="M159" altimg="si82.gif"><mml:mrow><mml:mn>2</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.21</td></tr><tr><td>Liver disorders</td><td align="char">2</td><td>2</td><td>KNN/k-means</td><td>UMAP</td><td><inline-formula><mml:math id="M160" altimg="si93.gif"><mml:mrow><mml:mn>0.11</mml:mn><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>0.04</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="char">0.03</td><td><inline-formula><mml:math id="M161" altimg="si82.gif"><mml:mrow><mml:mn>2</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.03</td><td><inline-formula><mml:math id="M162" altimg="si82.gif"><mml:mrow><mml:mn>2</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.03</td></tr><tr><td>Leaf</td><td align="char">30</td><td>3</td><td>RF/k-means∗</td><td>scaled</td><td><inline-formula><mml:math id="M163" altimg="si94.gif"><mml:mrow><mml:mn>0.19</mml:mn><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>0.03</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="char">0.22</td><td><inline-formula><mml:math id="M164" altimg="si87.gif"><mml:mrow><mml:mn>3</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.22</td><td><inline-formula><mml:math id="M165" altimg="si87.gif"><mml:mrow><mml:mn>3</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.22</td></tr><tr><td>Ecoli</td><td align="char">8</td><td>2</td><td>KNN/k-means∗</td><td>UMAP</td><td align="char">0.0</td><td align="char">0.48</td><td><inline-formula><mml:math id="M166" altimg="si82.gif"><mml:mrow><mml:mn>2</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.48</td><td><inline-formula><mml:math id="M167" altimg="si82.gif"><mml:mrow><mml:mn>2</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.48</td></tr><tr><td>Glass</td><td align="char">6</td><td>3</td><td>KNN/k-means∗</td><td>scaled</td><td><inline-formula><mml:math id="M168" altimg="si95.gif"><mml:mrow><mml:mn>0.39</mml:mn><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>0.06</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="char">0.35</td><td><inline-formula><mml:math id="M169" altimg="si87.gif"><mml:mrow><mml:mn>3</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.35</td><td><inline-formula><mml:math id="M170" altimg="si87.gif"><mml:mrow><mml:mn>3</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.35</td></tr><tr><td>Seeds</td><td align="char">3</td><td>3</td><td>SVM/k-means</td><td>Raw</td><td><inline-formula><mml:math id="M171" altimg="si96.gif"><mml:mrow><mml:mn>0.05</mml:mn><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>0.03</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="char">0.66</td><td><inline-formula><mml:math id="M172" altimg="si87.gif"><mml:mrow><mml:mn>3</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.66</td><td><inline-formula><mml:math id="M173" altimg="si87.gif"><mml:mrow><mml:mn>3</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.66</td></tr><tr><td>Parkinsons</td><td align="char">2</td><td>2</td><td>KNN/k-means</td><td>scaled + UMAP</td><td><inline-formula><mml:math id="M174" altimg="si97.gif"><mml:mrow><mml:mn>0.02</mml:mn><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>0.01</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="char">0.09</td><td><inline-formula><mml:math id="M175" altimg="si82.gif"><mml:mrow><mml:mn>2</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.09</td><td><inline-formula><mml:math id="M176" altimg="si82.gif"><mml:mrow><mml:mn>2</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.09</td></tr><tr><td>Iris</td><td align="char">3</td><td><inline-formula><mml:math id="M177" altimg="si98.gif"><mml:mrow><mml:mn>3</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> (tr/ts)</td><td>RF/HDBSCAN</td><td>UMAP</td><td><inline-formula><mml:math id="M178" altimg="si52.gif"><mml:mrow><mml:mn>0.33</mml:mn><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>0.19</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="char">0.73</td><td><inline-formula><mml:math id="M179" altimg="si99.gif"><mml:mrow><mml:mn>4</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.73</td><td><inline-formula><mml:math id="M180" altimg="si99.gif"><mml:mrow><mml:mn>4</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="char">0.73</td></tr></tbody></table><table-wrap-foot><fn id="tspara0050"><p>HC, hierarchical clustering; KNN, k-nearest neighbors; RF, random forest; SVM, support vector machine; AMI, adjusted mutual information score; ACC, accuracy.</p></fn><fn id="tspara0055"><p>Marked with ∗ results that failed in identifying the correct number of clusters. Number of clusters, and stability measures are reported. For comparison, the number of clusters identified based on internal measures is reported. The best clustering algorithms selected are independently applied to train and test sets, with best solution defined as the one that maximizes or minimizes silhouette and Davies-Bouldin measures, respectively. AMI on test set is reported for performance evaluation in all cases.</p></fn></table-wrap-foot></table-wrap></p>
        <p id="p0175">We observe that the stability-based approach identified the correct number of classes in <inline-formula><mml:math id="M181" altimg="si49.gif"><mml:mrow><mml:mn>68</mml:mn><mml:mtext>%</mml:mtext></mml:mrow></mml:math></inline-formula> of cases (<inline-formula><mml:math id="M182" altimg="si50.gif"><mml:mrow><mml:mi>N</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>13</mml:mn></mml:mrow></mml:math></inline-formula>). Moreover, 6 out 13 experiments selected k-means clustering and KNN classifier as the best choice. Of these, only 1 utilized raw data with no preprocessing, whereas the rest required either UMAP-preprocessed and/or scaled datasets. Because k-means works well with center-based spherical clusters and usually cannot find a good representation if clusters are very elongated or have complicated shapes, data preprocessing can relax this issue. From this experiment, it emerged that UMAP can be used as a preprocessing tool in this sense, in that it unwraps manifolds to find manifold boundaries. Nevertheless, because each dataset has its own intrinsic characteristics, preprocessing steps must be chosen with care to avoid breaking clusters into several erroneous small spherical clusters (see example with k-means at <ext-link ext-link-type="uri" xlink:href="https://umap-learn.readthedocs.io/en/latest/clustering.html" id="intref0045">https://umap-learn.readthedocs.io/en/latest/clustering.html</ext-link>). It has been observed<xref rid="bib14" ref-type="bibr"><sup>14</sup></xref> that, if the number of clusters <italic>k</italic> is too large with respect to the true clusters, the k-means algorithm tends to be unstable. On the contrary, if <italic>k</italic> is smaller or equal to the true number of clusters, the algorithm tends to be stable. Therefore, it is argued that k-means stability depends on the number of true clusters in the dataset, which should be on the order of 10 to provide stable solutions. This seems to hold when we look at the UCI datasets with <inline-formula><mml:math id="M183" altimg="si51.gif"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">≥</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> classes for which k-means is often selected as the best clustering algorithm, although it returns a smaller number of clusters with respect to true classes. For this reason, despite KNN/k-means providing the best algorithm configuration in more than half the experiments, the choice of a classifier/clustering should be done carefully, taking into account the dataset dimension and the computational cost of the algorithms.</p>
        <p id="p0180">The number of clusters selected with the silhouette score and Davies-Bouldin index (see <xref rid="tbl3" ref-type="table">Table 3</xref>) is equal to that reported by reval. The exception here was the <italic>iris</italic> dataset,<xref rid="bib25" ref-type="bibr"><sup>25</sup></xref> whereby reval selects 3 clusters during training and 2 during testing with RF and HDBSCAN, whereas internal measures result in 4- and 2-cluster solutions. Nevertheless, the validation stability of the relative approach is equal to <inline-formula><mml:math id="M184" altimg="si52.gif"><mml:mrow><mml:mn>0.33</mml:mn><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>0.19</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, suggesting that the partition does not generalize well because the solution is not stable. If we could only rely on internal measures, we would have failed to acknowledge the quality of the solution found. Generally, because 2 out of 3 classes are not linearly separable and the data are displayed in 2 separate groups<xref rid="bib25" ref-type="bibr"><sup>25</sup></xref> the <italic>iris</italic> dataset is not a good candidate for clustering and only the relative validation approach can clearly show that.</p>
        <p id="p0185">More generally, when comparing the AMI scores, we have no ability to say how well the results are actually doing. On the contrary, we have a sense of how good the clustering is with relative validation through the generalization process. See the example of the climate dataset, which has a validation normalized stability of <inline-formula><mml:math id="M185" altimg="si53.gif"><mml:mrow><mml:mn>0.20</mml:mn><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>0.04</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, indicating poor generalization, and a silhouette score on test set of 0.49, although both methods provide the same clustering solution on test set given that AMI score is equal to <inline-formula><mml:math id="M186" altimg="si54.gif"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>0.005</mml:mn></mml:mrow></mml:math></inline-formula> in both cases.</p>
        <p id="p0190">In conclusion, grid search for classifiers and clustering methods is easily and effectively implemented with reval and can be handy to avoid <italic>a priori</italic> selection of a classifier. Furthermore, the stability-based approach helps evaluate the goodness of a clustering solution by means of its generalization process. This information about generalization is absent with internal measures.</p>
      </sec>
    </sec>
    <sec id="sec2.3">
      <title>Stability regime to guide cluster selection</title>
      <p id="p0195">Ben-Hur et al.<xref rid="bib15" ref-type="bibr"><sup>15</sup></xref> presented a stability-based method with data subsampling and reported on the risk of underestimating/overestimating the true number of clusters. Introducing prediction strength computed via repeated cross-validation, Tibshirani et al.<xref rid="bib4" ref-type="bibr"><sup>4</sup></xref> linked unsupervised to supervised learning in an attempt to overcome the best cluster estimation issue. The reval implementation moves forward adding the evaluation of the best solution on unseen data, an approach that is particularly suitable for datasets with large sample size. While large sample sizes were less common in the past, dataset size has increased substantially over time and reval can be particularly important and well suited for modern data science contexts. Solution generalizability can be leveraged to select the best number of clusters not only based on validation metrics (e.g., prediction strength greater than <inline-formula><mml:math id="M187" altimg="si55.gif"><mml:mrow><mml:mn>0.8</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula><xref rid="bib4" ref-type="bibr"><sup>4</sup></xref>) but also on the performance of the solution applied to a new set of data. In this way, we are able to compare test set distribution with the one on which the result was based, hence reinforcing the decision. Selecting the number of clusters that best generalizes to new data (i.e., investigation of the stability regime) holds promise for overcoming the underestimation issue. To give a better sense of how this works in practice, we present a case study in the context of autism research, where clustering solutions within a stability regime could be further investigated.</p>
      <p id="p0200">Autism spectrum conditions (ASCs) are characterized by difficulties in social communication alongside heightened restricted and repetitive behaviors.<xref rid="bib26" ref-type="bibr"><sup>26</sup></xref> The spectrum of affected individuals with ASC is highly heterogeneous and this heterogeneity is present at multiple levels, from genome to phenome, and can co-exist with differing levels of severity and comorbidities.<xref rid="bib27" ref-type="bibr"><sup>27</sup></xref><sup>,</sup><xref rid="bib28" ref-type="bibr"><sup>28</sup></xref> Given the high level of heterogeneity, data-driven clustering could be a promising approach to isolating different types of autisms. To split ASC into data-driven subtypes, we applied reval to clinical data obtained from the National Database for Autism Research (NDAR; <ext-link ext-link-type="uri" xlink:href="https://nda.nih.gov/" id="intref0050">https://nda.nih.gov/</ext-link>). NDAR is a database that includes a heterogeneous collection of de-identified human subjects’ data for autism research. We focus here on clinical behavioral data from the Vineland Adaptive Behavior Scales (VABS).<xref rid="bib29" ref-type="bibr"><sup>29</sup></xref><sup>,</sup><xref rid="bib30" ref-type="bibr"><sup>30</sup></xref> Within the VABS, there are 3 domain total scores for communication, living skills, and socialization skills. Using these 3 domain total scores with UMAP preprocessing, we trained the stability-based model on 420 subjects (mean age <inline-formula><mml:math id="M188" altimg="si56.gif"><mml:mrow><mml:mn>41.65</mml:mn><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>17.91</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> months, female/male counts <inline-formula><mml:math id="M189" altimg="si57.gif"><mml:mrow><mml:mn>109</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>311</mml:mn></mml:mrow></mml:math></inline-formula>). As for analysis choices, we ran this analysis with <inline-formula><mml:math id="M190" altimg="si58.gif"><mml:mrow><mml:mn>100</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> repeated cross-validation and 100 iterations of random labeling, with number of clusters ranging from 2 to 10, using k-means clustering, and a KNN classifier with number of neighbors equal to 15.</p>
      <p id="p0205">From the performance plot in <xref rid="fig7" ref-type="fig">Figure 7</xref>, we find that both a 2-cluster and 3-cluster solution results in small stabilities (2-cluster solution (error): <inline-formula><mml:math id="M191" altimg="si59.gif"><mml:mrow><mml:mn>0.027</mml:mn><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>0.004</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>; 3-cluster solution [error]: <inline-formula><mml:math id="M192" altimg="si60.gif"><mml:mrow><mml:mn>0.036</mml:mn><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>0.003</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) and thus form a stability regime whereby either solution might be a promising solution to follow up with future work. Based on the minimization of the normalized stability measure, the default behavior when using reval would be to select 2 as the best number of clusters. However, upon evaluation of these solutions on the unseen test set (n = 344 subjects; mean age <inline-formula><mml:math id="M193" altimg="si61.gif"><mml:mrow><mml:mn>43.12</mml:mn><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>17.04</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> months, female/male counts <inline-formula><mml:math id="M194" altimg="si62.gif"><mml:mrow><mml:mn>90</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>254</mml:mn></mml:mrow></mml:math></inline-formula>), we find that both solutions reach <inline-formula><mml:math id="M195" altimg="si63.gif"><mml:mrow><mml:mn>94</mml:mn><mml:mtext>%</mml:mtext></mml:mrow></mml:math></inline-formula> accuracy, further confirming the presence of a stability regime whereby more than 1 solution might be a plausibly good model for follow-up work. The generalization performance alongside visual inspection (see <xref rid="mmc1" ref-type="supplementary-material">Figure S5</xref>) indicates that the default selection of a 2-cluster solution would possibly underestimate the true number of clusters.<xref rid="bib4" ref-type="bibr"><sup>4</sup></xref> Thus, based on the stability regime present here, we could select 3 as the true number of clusters since the stability differences are likely negligible and because both 2- and 3-cluster solutions generalize equally well. In practice, we would ultimately follow up with examination of both 2- and 3-cluster solutions and utilize other datasets to better understand which solution might be most illuminating for decomposing clinical and biological heterogeneity of importance for autism research.<fig id="fig7"><label>Figure 7</label><caption><p>Stability regime for the NDAR dataset</p><p>Random label stability is displayed for performance evaluation. Dashed line shows training stability. Solid line represents validation normalized stability with <inline-formula><mml:math id="M196" altimg="si65.gif"><mml:mrow><mml:mn>95</mml:mn><mml:mtext>%</mml:mtext></mml:mrow></mml:math></inline-formula> confidence intervals. Arrows point to the stability-regime solutions.</p></caption><graphic xlink:href="gr7"/></fig></p>
      <p id="p0210">To contrast the reval stability-regime results here with internal measures, we would have obtained 2-cluster solutions for both silhouette (scores of 0.41 and 0.42 on training and test respectively) and Davies-Bouldin measures (0.86 on both training and test sets). If we force the number of clusters to 3, we obtain lower silhouette scores (i.e., 0.35 in both training and test sets) and higher Davies-Bouldin indices (i.e., 0.93 and 0.95, respectively). In this example, internal measures do not reveal a regime of possible cluster solutions. Given the additional lack of information about generalization from internal measures, such a regime might be easily missed. This example illustrates a real-world example in data science for how relative validation implemented with reval may reveal insights regarding regimes of clustering solutions that may be missed with internal validation approaches.</p>
    </sec>
  </sec>
  <sec id="sec3">
    <title>Discussion</title>
    <p id="p0215">In this work, we introduce the reval package for relative clustering validation and describe how it can be utilized, as well as providing examples for how it performs in simulations and several real datasets. In many cases, reval successfully identifies the correct number of clusters and confers several other advantages over and above other internal validation approaches. In particular, from the examples reported, it is straightforward to observe that the numbers of clusters identified through reval and internal measures usually do not differ, and that the clustering solutions report the same AMI scores compared with true labels. Internal measures have the advantage of being less computationally expensive; nevertheless, they do not inform on the generalization process, nor do they grant the possibility to generate labels on unseen data. This happens because cluster labels are obtained from in-sample measures. On the contrary, reval relies on out-of-sample stability of the solutions and it allows estimation of whether the clustering used is successful in determining partitions. In conclusion, compared with internal validation measures, reval is able to report the extent to which different clustering solutions fit to the data at hand and how well those solutions may generalize or replicate on unseen data.</p>
    <p id="p0220">Moreover, because reval works with multiple clustering algorithms, it can facilitate a more thorough investigation of clustering mechanisms. In fact, although a thorough theoretical and experimental analysis of stability-based model selection with k-means clustering has been done<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref><sup>,</sup><xref rid="bib14" ref-type="bibr"><sup>14</sup></xref><sup>,</sup><xref rid="bib15" ref-type="bibr"><sup>15</sup></xref>, the effectiveness of such an approach needs to be further investigated with different clustering algorithms. Furthermore, reval can be included in ensemble learning pipelines<xref rid="bib31" ref-type="bibr"><sup>31</sup></xref> or integrated in ensemble clustering frameworks for the selection of the best clustering solution.<xref rid="bib32" ref-type="bibr"><sup>32</sup></xref> Last, the ability to identify stability regimes and evaluate such regimes based on generalization to unseen data can inform the underestimation issue of the best number of clusters identified in real-world datasets.</p>
    <sec id="sec3.1">
      <title>Limitations of the study</title>
      <p id="p0225">A primary caveat or limitation to the approach reval takes is primarily one of data size. reval identifies the best clustering solution within a cross-validation framework, and hence needs large sample sizes to preserve cluster distribution between training and validation sets. Moreover, a separate held-out dataset is also needed to generalize the solution found. Smaller datasets may not allow for sufficient splitting within a cross-validation framework to allow for robust clustering solutions to generalize in unseen datasets. Finally, reval does not address the possibility of finding unrealistic data partitions. Because classifiers can overfit to their training set, a stable solution does not necessarily imply the true presence of subgroups in the data. Future work will focus on the implementation of other relative validation methods, e.g., based on prediction strength,<xref rid="bib4" ref-type="bibr"><sup>4</sup></xref> with the aim to create a comprehensive library.</p>
    </sec>
  </sec>
  <sec id="sec4">
    <title>Experimental procedures</title>
    <sec id="sec4.1">
      <title>Resource availability</title>
      <sec id="sec4.1.1">
        <title>Lead contact</title>
        <p id="p0230">For further information, suggestions, or to contribute to the package, please reach out to the lead contact, Isotta Landi (<ext-link ext-link-type="uri" xlink:href="mailto:landi.isotta@gmail.com" id="intref0055">landi.isotta@gmail.com</ext-link>, @IsottaLandi).</p>
      </sec>
      <sec id="sec4.1.2">
        <title>Materials availability</title>
        <p id="p0235">This study did not generate any new materials.</p>
      </sec>
      <sec sec-type="data-availability" id="sec4.1.3">
        <title>Data and code availability</title>
        <p id="p0240">Code and package installation instructions can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/IIT-LAND/reval_clustering" id="PC_linksI3WhFcAEz">https://github.com/IIT-LAND/reval_clustering</ext-link>. Documentation with working examples is at <ext-link ext-link-type="uri" xlink:href="https://reval.readthedocs.io/en/latest/" id="intref0065">https://reval.readthedocs.io/en/latest/</ext-link>. Code with manuscript experiments and simulations can be found in the script ./working_examples/manuscript_examples.py at <ext-link ext-link-type="uri" xlink:href="https://github.com/IIT-LAND/reval_clustering" id="PC_linkV8pJN1ennD">https://github.com/IIT-LAND/reval_clustering</ext-link>.</p>
        <p id="p0245">Data used for the experiments and simulations are publicly available. In particular, the MNIST handwritten digits dataset can be downloaded from <ext-link ext-link-type="uri" xlink:href="http://yann.lecun.com/exdb/mnist/" id="intref0075">http://yann.lecun.com/exdb/mnist/</ext-link>, whereas the handwritten digits toy dataset can be found in the scikit-learn library. Datasets from the UCI Machine Learning repository can be found at <ext-link ext-link-type="uri" xlink:href="https://archive.ics.uci.edu/ml/index.php" id="intref0080">https://archive.ics.uci.edu/ml/index.php</ext-link>, and those used in the algorithm selection section can be created by running the ./working_examples/datasets/manuscript_builddatasets.py script at <ext-link ext-link-type="uri" xlink:href="https://github.com/IIT-LAND/reval_clustering" id="PC_linkeEQRvbIrWK">https://github.com/IIT-LAND/reval_clustering</ext-link>. The VABS dataset used to present the stability-regime-based clusters selection was extracted from the NDAR database <ext-link ext-link-type="uri" xlink:href="https://nda.nih.gov/" id="intref0090">https://nda.nih.gov/</ext-link>, which can be accessed upon approval by the NIH Data Access Committee.</p>
      </sec>
    </sec>
    <sec id="sec4.2">
      <title>Computational details</title>
      <p id="p0250">The package was developed in Python 3.8 and simulations were run on a MacBook Pro 2020 with a 2.3 GHz Quad-Core Intel Core i7 processor and 32 GB RAM. All simulations presented in the technical validation section were run with reval v0.1.0.</p>
    </sec>
  </sec>
</body>
<back>
  <ref-list id="cebib0010">
    <title>References</title>
    <ref id="bib1">
      <label>1</label>
      <element-citation publication-type="book" id="sref1">
        <person-group person-group-type="author">
          <name>
            <surname>Friedman</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Hastie</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Tibshirani</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <chapter-title>The Elements of Statistical Learning 10</chapter-title>
        <year>2001</year>
        <publisher-name>Springer series in statistics</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib2">
      <label>2</label>
      <element-citation publication-type="book" id="sref2">
        <person-group person-group-type="author">
          <name>
            <surname>Vazirgiannis</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <chapter-title>Clustering validity</chapter-title>
        <person-group person-group-type="editor">
          <name>
            <surname>Liu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Ozsu</surname>
            <given-names>M.T.</given-names>
          </name>
        </person-group>
        <source>Encyclopedia of Database Systems</source>
        <year>2009</year>
        <publisher-name>Springer US</publisher-name>
        <fpage>388</fpage>
        <lpage>393</lpage>
      </element-citation>
    </ref>
    <ref id="bib3">
      <label>3</label>
      <element-citation publication-type="journal" id="sref3">
        <person-group person-group-type="author">
          <name>
            <surname>Lange</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Roth</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Braun</surname>
            <given-names>M.L.</given-names>
          </name>
          <name>
            <surname>Buhmann</surname>
            <given-names>J.M.</given-names>
          </name>
        </person-group>
        <article-title>Stability-based validation of clustering solutions</article-title>
        <source>Neural Comput.</source>
        <volume>16</volume>
        <year>2004</year>
        <fpage>1299</fpage>
        <lpage>1323</lpage>
        <pub-id pub-id-type="pmid">15130251</pub-id>
      </element-citation>
    </ref>
    <ref id="bib4">
      <label>4</label>
      <element-citation publication-type="journal" id="sref4">
        <person-group person-group-type="author">
          <name>
            <surname>Tibshirani</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Walther</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Cluster validation by prediction strength</article-title>
        <source>J. Comput. Graph Stat.</source>
        <volume>14</volume>
        <year>2005</year>
        <fpage>511</fpage>
        <lpage>528</lpage>
      </element-citation>
    </ref>
    <ref id="bib5">
      <label>5</label>
      <element-citation publication-type="journal" id="sref5">
        <person-group person-group-type="author">
          <name>
            <surname>Thorndike</surname>
            <given-names>R.L.</given-names>
          </name>
        </person-group>
        <article-title>Who belongs in the family? Psychometrika</article-title>
        <volume>18</volume>
        <year>1953</year>
        <fpage>267</fpage>
        <lpage>276</lpage>
      </element-citation>
    </ref>
    <ref id="bib6">
      <label>6</label>
      <element-citation publication-type="journal" id="sref6">
        <person-group person-group-type="author">
          <name>
            <surname>Davies</surname>
            <given-names>D.L.</given-names>
          </name>
          <name>
            <surname>Bouldin</surname>
            <given-names>D.W.</given-names>
          </name>
        </person-group>
        <article-title>A cluster separation measure</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell</source>
        <year>1979</year>
        <fpage>224</fpage>
        <lpage>227</lpage>
        <pub-id pub-id-type="pmid">21868852</pub-id>
      </element-citation>
    </ref>
    <ref id="bib7">
      <label>7</label>
      <element-citation publication-type="journal" id="sref7">
        <person-group person-group-type="author">
          <name>
            <surname>Rousseeuw</surname>
            <given-names>P.J.</given-names>
          </name>
        </person-group>
        <article-title>Silhouettes: a graphical aid to the interpretation and validation of cluster analysis</article-title>
        <source>J. Comput. Appl. Math.</source>
        <volume>20</volume>
        <year>1987</year>
        <fpage>53</fpage>
        <lpage>65</lpage>
      </element-citation>
    </ref>
    <ref id="bib8">
      <label>8</label>
      <element-citation publication-type="journal" id="sref8">
        <person-group person-group-type="author">
          <name>
            <surname>Bengfort</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Bilbro</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Yellowbrick: Visualizing the scikit-learn model selection process</article-title>
        <source>J. Open Source Softw.</source>
        <volume>4</volume>
        <year>2019</year>
        <fpage>1075</fpage>
      </element-citation>
    </ref>
    <ref id="bib9">
      <label>9</label>
      <element-citation publication-type="journal" id="sref9">
        <person-group person-group-type="author">
          <name>
            <surname>Charrad</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Ghazzali</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Boiteau</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Niknafs</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>NbClust: an R package for determining the relevant number of clusters in a data set</article-title>
        <source>J. Stat. Softw.</source>
        <volume>61</volume>
        <year>2014</year>
        <fpage>1</fpage>
        <lpage>36</lpage>
      </element-citation>
    </ref>
    <ref id="bib10">
      <label>10</label>
      <element-citation publication-type="journal" id="sref10">
        <person-group person-group-type="author">
          <name>
            <surname>Brock</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Pihur</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Datta</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Datta</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>clValid: an R package for cluster validation</article-title>
        <source>J. Stat. Softw.</source>
        <volume>25</volume>
        <year>2008</year>
        <fpage>1</fpage>
        <lpage>22</lpage>
      </element-citation>
    </ref>
    <ref id="bib11">
      <label>11</label>
      <element-citation publication-type="other" id="sref11">
        <person-group person-group-type="author">
          <name>
            <surname>Haslbeck</surname>
            <given-names>J.M.B.</given-names>
          </name>
          <name>
            <surname>Wulff</surname>
            <given-names>D.U.</given-names>
          </name>
        </person-group>
        <article-title>Cstab: selection of number of clusters via normalized clustering instability R package version 0.2-2</article-title>
        <ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/package=cstab" id="intref0100">https://CRAN.R-project.org/package=cstab</ext-link>
        <year>2018</year>
      </element-citation>
    </ref>
    <ref id="bib12">
      <label>12</label>
      <element-citation publication-type="other" id="sref12">
        <person-group person-group-type="author">
          <name>
            <surname>Haslbeck</surname>
            <given-names>J.M.B.</given-names>
          </name>
          <name>
            <surname>Wulff</surname>
            <given-names>D.U.</given-names>
          </name>
        </person-group>
        <article-title>Estimating the number of clusters via normalized cluster instability</article-title>
        <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1608.07494.%202016" id="intref0105">https://arxiv.org/abs/1608.07494. 2016</ext-link>
      </element-citation>
    </ref>
    <ref id="bib13">
      <label>13</label>
      <element-citation publication-type="journal" id="sref13">
        <person-group person-group-type="author">
          <name>
            <surname>Brun</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Model-based evaluation of clustering validation measures</article-title>
        <source>Pattern Recogn.</source>
        <volume>40</volume>
        <year>2007</year>
        <fpage>807824</fpage>
      </element-citation>
    </ref>
    <ref id="bib14">
      <label>14</label>
      <element-citation publication-type="book" id="sref14">
        <person-group person-group-type="author">
          <name>
            <surname>Von Luxburg</surname>
            <given-names>U.</given-names>
          </name>
        </person-group>
        <series>Clustering stability: an overview</series>
        <volume>vol 2</volume>
        <year>2010</year>
        <publisher-name>Foundations Trends® in Machine Learning</publisher-name>
        <fpage>235</fpage>
        <lpage>274</lpage>
        <comment>ISSN: 1935-8237</comment>
      </element-citation>
    </ref>
    <ref id="bib15">
      <label>15</label>
      <element-citation publication-type="book" id="sref15">
        <person-group person-group-type="author">
          <name>
            <surname>Ben-Hur</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Elisseeff</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Guyon</surname>
            <given-names>I.</given-names>
          </name>
        </person-group>
        <chapter-title>Biocomputing 2002 6-17</chapter-title>
        <year>2001</year>
        <publisher-name>World Scientific</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib16">
      <label>16</label>
      <mixed-citation publication-type="other" id="sref16">Moller, U. &amp; Radke, D. A cluster validity approach based on nearest-neighbor resampling. 18th International Conference on Pattern Recognition (ICPR 06) 1 (2006). 10.1109/ICPR.2006.42.</mixed-citation>
    </ref>
    <ref id="bib17">
      <label>17</label>
      <mixed-citation publication-type="other" id="sref17">Vinh, N.X. and Epps, J. A novel approach for automatic number of clusters detection in microarray data based on consensus clustering. 2009 Ninth IEEE International Conference on Bioinformatics and BioEngineering (2009). 10.1109/BIBE.2009.19.</mixed-citation>
    </ref>
    <ref id="bib18">
      <label>18</label>
      <mixed-citation publication-type="other" id="sref18">Campello, R.J.G.B., Moulavi, D., and Sander, J. Density-based clustering based on hierarchical density estimates in Pacific-Asia conference on knowledge discovery and data mining (2013), 160-172.</mixed-citation>
    </ref>
    <ref id="bib19">
      <label>19</label>
      <element-citation publication-type="journal" id="sref19">
        <person-group person-group-type="author">
          <name>
            <surname>Matthews</surname>
            <given-names>B.W.</given-names>
          </name>
        </person-group>
        <article-title>Comparison of the predicted and observed secondary structure of T4 phage lysozyme</article-title>
        <source>Biochim. Biophys. Acta</source>
        <volume>405</volume>
        <year>1975</year>
        <fpage>442</fpage>
        <lpage>451</lpage>
        <pub-id pub-id-type="pmid">1180967</pub-id>
      </element-citation>
    </ref>
    <ref id="bib20">
      <label>20</label>
      <element-citation publication-type="journal" id="sref20">
        <person-group person-group-type="author">
          <name>
            <surname>Kuhn</surname>
            <given-names>H.W.</given-names>
          </name>
        </person-group>
        <article-title>The Hungarian method for the assignment problem</article-title>
        <source>Naval Res. Logist. Q.</source>
        <volume>2</volume>
        <year>1955</year>
        <fpage>83</fpage>
        <lpage>97</lpage>
      </element-citation>
    </ref>
    <ref id="bib21">
      <label>21</label>
      <element-citation publication-type="journal" id="sref21">
        <person-group person-group-type="author">
          <name>
            <surname>Munkres</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Algorithms for the assignment and transportation problems</article-title>
        <source>J. Soc. Ind. Appl. Math.</source>
        <volume>5</volume>
        <year>1957</year>
        <fpage>32</fpage>
        <lpage>38</lpage>
      </element-citation>
    </ref>
    <ref id="bib22">
      <label>22</label>
      <element-citation publication-type="journal" id="sref22">
        <person-group person-group-type="author">
          <name>
            <surname>Pedregosa</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Varoquaux</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Gramfort</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Michel</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Thirion</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Grisel</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Blondel</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Prettenhofer</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Weiss</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Dubourg</surname>
            <given-names>V.</given-names>
          </name>
        </person-group>
        <article-title>Scikit-learn: machine learning in Python</article-title>
        <source>J. Machine Learn. Res.</source>
        <volume>12</volume>
        <year>2011</year>
        <fpage>2825</fpage>
        <lpage>2830</lpage>
      </element-citation>
    </ref>
    <ref id="bib23">
      <label>23</label>
      <element-citation publication-type="other" id="sref23">
        <person-group person-group-type="author">
          <name>
            <surname>Dua</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Graff</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>UCI machine learning repository</article-title>
        <ext-link ext-link-type="uri" xlink:href="http://archive.ics.uci.edu/ml" id="intref0110">http://archive.ics.uci.edu/ml</ext-link>
        <year>2017</year>
      </element-citation>
    </ref>
    <ref id="bib24">
      <label>24</label>
      <element-citation publication-type="journal" id="sref24">
        <person-group person-group-type="author">
          <name>
            <surname>McInnes</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Healy</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Saul</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Großberger</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>UMAP: uniform manifold approximation and projection</article-title>
        <source>J. Open Source Softw.</source>
        <volume>3</volume>
        <year>2018</year>
        <fpage>861</fpage>
      </element-citation>
    </ref>
    <ref id="bib25">
      <label>25</label>
      <element-citation publication-type="journal" id="sref25">
        <person-group person-group-type="author">
          <name>
            <surname>Fisher</surname>
            <given-names>R.A.</given-names>
          </name>
        </person-group>
        <article-title>The use of multiple measurements in taxonomic problems</article-title>
        <source>Ann. Eugen.</source>
        <volume>7</volume>
        <year>1936</year>
        <fpage>179</fpage>
        <lpage>188</lpage>
      </element-citation>
    </ref>
    <ref id="bib26">
      <label>26</label>
      <element-citation publication-type="book" id="sref26">
        <person-group person-group-type="author">
          <collab>American Psychiatric Association</collab>
        </person-group>
        <source>Diagnostic and Statistical Manual of Mental Disorders</source>
        <edition>5th edition</edition>
        <year>2013</year>
        <publisher-name>American Psychiatric Association</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib27">
      <label>27</label>
      <element-citation publication-type="journal" id="sref27">
        <person-group person-group-type="author">
          <name>
            <surname>Lai</surname>
            <given-names>M.-C.</given-names>
          </name>
          <name>
            <surname>Lombardo</surname>
            <given-names>M.V.</given-names>
          </name>
          <name>
            <surname>Baron-Cohen</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <source>Autism. Lancet</source>
        <volume>383</volume>
        <year>2014</year>
        <fpage>896</fpage>
        <lpage>910</lpage>
        <comment>ISSN: 1474-547X</comment>
        <pub-id pub-id-type="pmid">24074734</pub-id>
      </element-citation>
    </ref>
    <ref id="bib28">
      <label>28</label>
      <element-citation publication-type="journal" id="sref28">
        <person-group person-group-type="author">
          <name>
            <surname>Lombardo</surname>
            <given-names>M.V.</given-names>
          </name>
          <name>
            <surname>Lai</surname>
            <given-names>M.-C.</given-names>
          </name>
          <name>
            <surname>Baron-Cohen</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Big data approaches to decomposing heterogeneity across the autism spectrum</article-title>
        <source>Mol. Psychiatry</source>
        <volume>24</volume>
        <year>2019</year>
        <fpage>1435</fpage>
        <lpage>1450</lpage>
        <pub-id pub-id-type="pmid">30617272</pub-id>
      </element-citation>
    </ref>
    <ref id="bib29">
      <label>29</label>
      <element-citation publication-type="book" id="sref29">
        <person-group person-group-type="author">
          <name>
            <surname>Sparrow</surname>
            <given-names>S.S.</given-names>
          </name>
          <name>
            <surname>Balla</surname>
            <given-names>D.A.</given-names>
          </name>
          <name>
            <surname>Cicchetti</surname>
            <given-names>D.V.</given-names>
          </name>
        </person-group>
        <chapter-title>Vineland II: Vineland Adaptive Behavior Scales</chapter-title>
        <year>2005</year>
        <publisher-name>AGS Publishing</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib30">
      <label>30</label>
      <element-citation publication-type="book" id="sref30">
        <person-group person-group-type="author">
          <name>
            <surname>Sparrow</surname>
            <given-names>S.S.</given-names>
          </name>
          <name>
            <surname>Cicchetti</surname>
            <given-names>D.V.</given-names>
          </name>
          <name>
            <surname>Saulnier</surname>
            <given-names>C.A.</given-names>
          </name>
        </person-group>
        <chapter-title>Vineland-3: Vineland Adaptive Behavior Scales</chapter-title>
        <year>2016</year>
        <publisher-name>PsychCorp</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib31">
      <label>31</label>
      <element-citation publication-type="journal" id="sref31">
        <person-group person-group-type="author">
          <name>
            <surname>Rodriguez</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Medina-Perez</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Gutierrez-Rodriguez</surname>
            <given-names>A.E.</given-names>
          </name>
          <name>
            <surname>Monroy</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Terashima-Marin</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Cluster validation using an ensemble of supervised classifiers</article-title>
        <source>Knowl. Based Syst.</source>
        <volume>145</volume>
        <year>2018</year>
        <fpage>134</fpage>
        <lpage>144</lpage>
      </element-citation>
    </ref>
    <ref id="bib32">
      <label>32</label>
      <element-citation publication-type="journal" id="sref32">
        <person-group person-group-type="author">
          <name>
            <surname>Strehl</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Ghosh</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Cluster ensembles—a knowledge reuse framework for combining multiple partitions</article-title>
        <source>J. Machine Learn. Res.</source>
        <volume>3</volume>
        <year>2002</year>
        <fpage>583</fpage>
        <lpage>617</lpage>
      </element-citation>
    </ref>
  </ref-list>
  <sec id="appsec2" sec-type="supplementary-material">
    <title>Supplemental information</title>
    <p id="p0275">
      <supplementary-material content-type="local-data" id="mmc1">
        <caption>
          <title>Document S1. Supplemental experimental procedures, Figures S1–S5, and Code S1–S6</title>
        </caption>
        <media xlink:href="mmc1.pdf"/>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="mmc2">
        <caption>
          <title>Document S2. Article plus supplemental information</title>
        </caption>
        <media xlink:href="mmc2.pdf"/>
      </supplementary-material>
    </p>
  </sec>
  <ack id="ack0010">
    <title>Acknowledgments</title>
    <p id="p0255">This project was supported by funding from the <funding-source id="gs1"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source> (ERC) under the European Union's Horizon 2020 research and innovation program under grant agreement no. 755816 (ERC Starting Grant to M.V.L.).</p>
    <sec id="sec5">
      <title>Author contributions</title>
      <p id="p0260">Conceptualization, I.L. and M.V.L; methodology, I.L.; software, I.L. and V.M.; investigation, I.L. and V.M.; writing – original draft, I.L. and M.V.L.; writing – review &amp; editing, I.L., V.M., and M.V.L.; funding acquisition, M.V.L.; supervision, M.V.L.</p>
    </sec>
    <sec sec-type="COI-statement" id="sec6">
      <title>Declaration of interests</title>
      <p id="p0265">The authors declare no competing interests.</p>
    </sec>
  </ack>
  <fn-group>
    <fn id="appsec1" fn-type="supplementary-material">
      <p id="p0270">Supplemental information can be found online at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.patter.2021.100228" id="intref0095">https://doi.org/10.1016/j.patter.2021.100228</ext-link>.</p>
    </fn>
  </fn-group>
</back>
