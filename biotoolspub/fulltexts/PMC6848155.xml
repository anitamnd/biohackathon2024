<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sci Rep</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id>
    <journal-title-group>
      <journal-title>Scientific Reports</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2045-2322</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6848155</article-id>
    <article-id pub-id-type="publisher-id">52937</article-id>
    <article-id pub-id-type="doi">10.1038/s41598-019-52937-5</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DeePathology: Deep Multi-Task Learning for Inferring Molecular Pathology from Cancer Transcriptome</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Azarkhalili</surname>
          <given-names>Behrooz</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Saberi</surname>
          <given-names>Ali</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Chitsaz</surname>
          <given-names>Hamidreza</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0087-7596</contrib-id>
        <name>
          <surname>Sharifi-Zarchi</surname>
          <given-names>Ali</given-names>
        </name>
        <address>
          <email>asharifi@sharif.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 0612 4397</institution-id><institution-id institution-id-type="GRID">grid.419336.a</institution-id><institution>Department of Stem Cell Biology and Technology, </institution><institution>Royan Institute, </institution></institution-wrap>Tehran, Iran </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0740 9747</institution-id><institution-id institution-id-type="GRID">grid.412553.4</institution-id><institution>Department of Computer Engineering, </institution><institution>Sharif University of Technology, </institution></institution-wrap>Tehran, Iran </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1936 8083</institution-id><institution-id institution-id-type="GRID">grid.47894.36</institution-id><institution>Department of Computer Science, </institution><institution>Colorado State University, </institution></institution-wrap>Fort Collins, CO USA </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0740 9747</institution-id><institution-id institution-id-type="GRID">grid.412553.4</institution-id><institution>Department of Mathematics and Computer Science, </institution><institution>Sharif University of Technology, </institution></institution-wrap>Tehran, Iran </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>11</day>
      <month>11</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>11</day>
      <month>11</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>9</volume>
    <elocation-id>16526</elocation-id>
    <history>
      <date date-type="received">
        <day>13</day>
        <month>12</month>
        <year>2018</year>
      </date>
      <date date-type="accepted">
        <day>23</day>
        <month>10</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Despite great advances, molecular cancer pathology is often limited to the use of a small number of biomarkers rather than the whole transcriptome, partly due to computational challenges. Here, we introduce a novel architecture of Deep Neural Networks (DNNs) that is capable of simultaneous inference of various properties of biological samples, through multi-task and transfer learning. It encodes the whole transcription profile into a strikingly low-dimensional latent vector of size 8, and then recovers mRNA and miRNA expression profiles, tissue and disease type from this vector. This latent space is significantly better than the original gene expression profiles for discriminating samples based on their tissue and disease. We employed this architecture on mRNA transcription profiles of 10750 clinical samples from 34 classes (one healthy and 33 different types of cancer) from 27 tissues. Our method significantly outperforms prior works and classical machine learning approaches in predicting tissue-of-origin, normal or disease state and cancer type of each sample. For tissues with more than one type of cancer, it reaches 99.4% accuracy in identifying the correct cancer subtype. We also show this system is very robust against noise and missing values. Collectively, our results highlight applications of artificial intelligence in molecular cancer pathology and oncological research. DeePathology is freely available at <ext-link ext-link-type="uri" xlink:href="https://github.com/SharifBioinf/DeePathology">https://github.com/SharifBioinf/DeePathology</ext-link>.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Cancer</kwd>
      <kwd>Computational biology and bioinformatics</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1" sec-type="introduction">
    <title>Introduction</title>
    <p id="Par2">Improving the accuracy of cancer diagnosis is extremely important for millions of patients and for far more non-patients who are tested worldwide every year. Despite great advances in oncologic pathology, there has been a significant ratio of errors that potentially affect the diagnostic results and/or treatment strategies<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. Our systematic search through COREMINE (<ext-link ext-link-type="uri" xlink:href="http://www.coremine.com">www.coremine.com</ext-link>) revealed 7652 article abstracts containing both <italic>neoplasms</italic> and <italic>diagnostic errors</italic> (or synonymous terms), with an increasing trend over time. An M.D. Anderson Cancer Center study of 500 brain or spinal cord biopsies that were submitted to their neuropathology consultation service for a second opinion revealed 42.8% disagreement between the original and the review diagnoses, including 8.8% serious cases<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. A study of 340 breast cancer patients identified differences between the first and the second pathology opinions in 80% of the cases, including major changes that altered surgical therapy occurred in 7.8% of cases<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. A review of 66 thyroid cancer patients revealed a different pathological diagnosis of 18% of the cases<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. A recent study verified the accuracy and reproducibility of pathologists’ diagnoses of melanocytic skin lesions for 240 skin biopsy cases from 10 US states and revealed 8–75% error rates in different interpretation classes and an estimated 17.8% whole-population error rate<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. Another recent study of 263 Australian Lichenoid keratosis patients revealed a diagnosis failure rate higher than 70%, including 47% of the cases misdiagnosed as basal cell carcinoma<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. This situation is even worse in rare types of cancer. A study of 26 patients revealed 30.8% misdiagnosis ratio in discriminating common gastric adenocarcinoma from hepatoid adenocarcinoma of the stomach, a rare subtype of gastric cancer<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. Accurate diagnosis has been also challenging for a number of cancer types, including soft tissue sarcomas that are often misdiagnosed as other types of cancer<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>.</p>
    <p id="Par3">One limitation of the current molecular pathology methods such as Immunohistochemistry (IHC) is the limited number of genes or proteins monitored for diagnosis. Staining biopsies using antibodies against one or two proteins cannot discriminate between different cancer types if they have similar expression patterns of the target proteins. One possible solution is to use the whole-transcriptome of tissue biopsies<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>. But this approach is computationally challenging and different algorithmic and machine learning approaches have been employed so far to address this problem. A subset of research is focused on <italic>binary classification</italic>, e.g. discriminating between normal vs. tumor samples<sup><xref ref-type="bibr" rid="CR10">10</xref>–<xref ref-type="bibr" rid="CR13">13</xref></sup>. Stacked autoencoders are used for binary classification between glioma grades III vs. IV, and evaluated on 185 samples<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. These methods, however, can have limited clinical applications since most of the molecular pathology problems are <italic>multiclass</italic>, e.g. assigning each sample to one of the different cancer types. Reaching high accuracy in classification problems usually becomes harder as the number of classes increases. Even a random assignment of samples to two classes will achieve 50% accuracy if the classes are <italic>balanced</italic> (i.e. there are an equal number of samples in each class), but a random classification will be around 3% accurate if there are 33 balanced classes. Hence, it is important to consider the number of classes for comparing the accuracy of different techniques.</p>
    <p id="Par4">Optimal Feature Weighting (OFW) is one of the earliest multiclass algorithms employed for cancer sample classification based on Microarray transcriptomes. This algorithm selects an optimal discriminative subset of genes and uses Support Vector Machines (SVM) or Classification And Regression Trees (CART). In previous work, it has been applied to five different problems, each consisting of 3 to 11 classes, without explicitly mentioning the obtained accuracy<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>. A combination of SVM with Recursive Feature Elimination (RFE) is used to classify Microarray data of three cancer-related problems consisting of 3 to 8 classes, with accuracy between 95% (for 8-class) to 100% (for 3-class)<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>. Greedy search over top-scoring gene-sets has achieved an average 88% accuracy, ranging from 48% to 100%, on seven different cancer datasets, each consisting of 3 or 4 classes with 40 to 96 samples per dataset<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>.</p>
    <p id="Par5">One of the largest databases of cancer transcriptome, genome and epigenome profiles is Genomic Data Commons (GDC) that includes The Cancer Genome Atlas (TCGA) and Therapeutically Applicable Research to Generate Effective Treatments (TARGET) programs<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>. There have been comprehensive works to analyze GDC data from different perspectives including identification of cancer driver somatic and pathogenic germline variations<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>, oncogenic signaling pathways<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>, the role of cell-of-origin<sup><xref ref-type="bibr" rid="CR21">21</xref></sup> and cancer stem cells<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>, relationships between tumor genomes, epigenomes and microenvironments<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>. However, there has been little effort directed towards developing a molecular cancer pathology framework out of this valuable data. Semi-supervised stacked sparse autoencoders are used for binary classification of three types of cancer, and evaluated on 1311 samples<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>. DeepGene, a feed-forward artificial neural network (ANN) based classifier, uses somatic point mutations profiles in order to assign each sample to one of 12 different cancer types. It achieved a mean 58% and maximum 64% accuracy over 3122 TCGA samples from 12 cancer types<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. Another study used genetic algorithms and k-Nearest Neighbors (KNN) to classify TCGA samples based on RNA-seq transcriptome profiles. It reached about 90% accuracy for classification of 602 normal samples and 9096 samples from 31 tumor types<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>.</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <p id="Par6">Here we used Deep Neural Networks (DNNs) in a multi-task learning approach to infer different biological and clinical information from transcriptome profiles. We designed four different architectures as shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref> based on two different methods, including Contractive Autoencoder (CAE) and Variational Autoencoder (VAE). Each of our DNNs consists of two parts that are serially connected: the <italic>encoder</italic> part, that learns to convert a given mRNA expression profile (mRNA EP) of a clinical sample at the input layer to a latent representation, which we call Cell Identity Code (CIC), and the <italic>decoder</italic> part that infers multiple outputs from the CIC. While the CIC is a simple vector of numerical values in the CAEs, the VAEs encode the input into two equal-sized vectors, which represent means and standard deviations of multiple Gaussian distributions. For each method, we designed two architectures, one simpler and another consisting of dropout layer after each layer of the encoder. To make our architectures resistant to missing values and noisy data, we added a <italic>Gaussian Dropout</italic> layer after the input, which perturbs the data with Gaussian noise and randomly sets some input values to zero. More details are provided in the online methods.<fig id="Fig1"><label>Figure 1</label><caption><p>Different architectures of DNN used for this study. (<bold>a</bold>) Contractive Autoencoder (CAE), (<bold>b</bold>) Dropout Contractive Autoencoder (Dropout-CAE), (<bold>c</bold>) Variational Autoencoder (VAE), (<bold>d</bold>) Dropout Variational Autoencoder (Droput-VAE). For each network, the layers are shown as boxes and the connections between them as arrows. The evaluated hyperparameters of each layer are shown next to each layer. The hyperparameter values in red show the optimal parameters identified through hyperparameter optimization. See extended methods for more details.</p></caption><graphic xlink:href="41598_2019_52937_Fig1_HTML" id="d29e413"/></fig></p>
    <p id="Par7">What makes our architectures different from conventional autoencoders is its particular design to learn <italic>useful</italic> latent representations of the input data. Altered weights of the encoder part result in different latent representations of the same input, which can be decoded to the same output by different decoder weights. Hence, there is an infinitely large number of latent representation sets for a given set of data, however, the question is whether all of these representations are useful. Some latent representations might be extremely cryptic, while the others might be very useful for classification of biological samples, or obtaining other information. For instance, classification would be much easier if all cells of each type cluster together, distantly from the other clusters, in the latent representation space. The challenge is how to train the network in order to learn useful latent representations of the mRNA EPs.</p>
    <p id="Par8">To address this challenge, we designed the decoder part of each network to simultaneously learn four different classification and regression problems using the CIC, in a multi-task learning scheme: (i) reproducing mRNA EP as one of the outputs that is as close to the original mRNA EP in the input as possible, (ii) predicting a miRNA expression profile (miRNA EP) that is as close as possible to the experimentally measured miRNA EP of the same sample, (iii) predicting the sample tissue of origin, among 27 different tissues, and (iv) predicting the sample disease state, which can be either normal or one of 33 different cancer types. All parts of each network, including encoder, decoder and classifiers were trained simultaneously.</p>
    <p id="Par9">From the above tasks, (i) and (ii) can be viewed as non-linear regression, and (iii) and (iv) are classification. Importantly, the multi-task learning part of the DNN is aimed to accomplish all of these tasks only by getting the CIC as the input. Task (i) is to ensure the CIC stores much of the information in the original mRNA EP. Due to task (ii), we selected from GDC a subset of samples having both mRNA and miRNA EPs available. Furthermore, we removed 11 miRNA-encoding genes from the mRNA profiles, to make this task non-trivial.</p>
    <p id="Par10">The other key advantage of our model is <italic>hyperparameter optimization</italic>. In addition to the internal network parameters (i.e. synapse weights and neuron bias values), each network has a set of hyperparameters, including the number of neurons in each layer, activation functions, size of mini-batches, standard deviations of the Gaussian noises, ratios of dropout layers, and the number of training epochs. Altered values of hyperparameters greatly affect the network results. Due to their cryptic inter-dependencies, all hyperparameters are required to be optimized simultaneously. For this purpose, we performed a comprehensive hyperparameter optimization through a Bayesian approach that runs in a number of iterations. In each iteration, it tries to find a set of hyperparameters that has the maximum likelihood of optimizing the network training outcome by integrating the results of all previous iterations in a Bayesian model. This process has superior advantages over grid search or random search of the hyperparameters in reducing the search space and more direct approaching the optimal hyperparameters using much fewer iterations.</p>
    <sec id="Sec3">
      <title>Data pre-processing</title>
      <p id="Par11">Transcriptome profiles of 11,500 samples were obtained from the Genomic Data Commons (GDC) consortium<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, including The Cancer Genome Atlas (TCGA)<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> and the Therapeutically Applicable Research to Generate Effective Treatments (TARGET) databases. For each mRNA profile, its corresponding miRNA profile was needed to train the model. So we kept the samples that both mRNA and miRNA expression profiles were available. This resulted in a total number of 10787 samples. For training a VAE, the total number of samples should be a multiple of batch size. Since a batch size of 250 was chosen for the model training process, total number of samples should be a multiple of 250. Thus we kept 10750 samples for the analysis, including 10124 tumor and 626 normal samples. The mRNA and miRNA profiles of each sample were matched by the ID values of the patients, using the TCGAbiolinks R/Bioconductor package<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. We randomly selected about 10% of the whole data as the test dataset, and the remaining samples were assigned to the training dataset.</p>
      <p id="Par12">One of our tasks was to predict expression level of miRNAs from mRNA profile. Hence we removed all miRNA genes from mRNA profiles, to ensure the answer to this task is not provided to the network as input. As the result, each mRNA and miRNA expression profile was of sizes 19671 and 2588, respectively. The precise number of samples from each type of tissue and tumor is provided in Tables S<xref rid="MOESM1" ref-type="media">1</xref> and S<xref rid="MOESM1" ref-type="media">2</xref>.</p>
    </sec>
    <sec id="Sec4">
      <title>Deep autoencoder</title>
      <p id="Par13">An interesting Neural Network (NN) architecture is autoencoder (AE), which compresses the input into a latent space representation, and then reconstructs the input back from this representation. In other words, an AE seeks to learn an identity-like mapping function <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f$$\end{document}</tex-math><mml:math id="M2"><mml:mi>f</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq1.gif"/></alternatives></inline-formula> such that <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f(x)\approx x$$\end{document}</tex-math><mml:math id="M4"><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:mi>x</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq2.gif"/></alternatives></inline-formula>. AEs can reduce the dimensionality of data without losing significant information and can be trained using unlabeled data, hence they are widely used in different problems including data compression, dimensionality reduction, manifold learning, and feature learning<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>.</p>
      <p id="Par14">Each autoencoder consists of two parts, the <italic>encoder</italic> and the <italic>decoder</italic>, which can be defined as transition functions <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f$$\end{document}</tex-math><mml:math id="M6"><mml:mi>f</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq3.gif"/></alternatives></inline-formula> and <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$g$$\end{document}</tex-math><mml:math id="M8"><mml:mi>g</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq4.gif"/></alternatives></inline-formula> such that:</p>
      <p id="Par15"><disp-formula id="Equa"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{rcl}f &amp; : &amp; X\to {\mathscr{F}}\\ g &amp; : &amp; {\mathscr{F}}\to X\\ f,g &amp; = &amp; \arg \,{\min }_{\varphi ,\psi }{\mathscr{J}}(x,\widetilde{x}(\varphi ,\psi ,x))\end{array}$$\end{document}</tex-math><mml:math id="M10" display="block"><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mi>f</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:mo>:</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mstyle mathvariant="normal"><mml:mi>X</mml:mi></mml:mstyle><mml:mo>→</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">F</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mi>g</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:mo>:</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">F</mml:mi><mml:mo>→</mml:mo><mml:mstyle mathvariant="normal"><mml:mi>X</mml:mi></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi>arg</mml:mi><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mi>φ</mml:mi><mml:mo>,</mml:mo><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mi class="MJX-tex-caligraphic" mathvariant="script">J</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>φ</mml:mi><mml:mo>,</mml:mo><mml:mi>ψ</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="41598_2019_52937_Article_Equa.gif" position="anchor"/></alternatives></disp-formula> where <inline-formula id="IEq5"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\psi $$\end{document}</tex-math><mml:math id="M12"><mml:mi>ψ</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq5.gif"/></alternatives></inline-formula> and <inline-formula id="IEq6"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varphi $$\end{document}</tex-math><mml:math id="M14"><mml:mi>φ</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq6.gif"/></alternatives></inline-formula> are the encoder and the decoder functions, respectively, <inline-formula id="IEq7"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\widetilde{x}(\varphi ,\psi ,x)=\varphi \,\circ \,\psi (x)$$\end{document}</tex-math><mml:math id="M16"><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>φ</mml:mi><mml:mo>,</mml:mo><mml:mi>ψ</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>φ</mml:mi><mml:mspace width="0.25em"/><mml:mo>∘</mml:mo><mml:mspace width="0.25em"/><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq7.gif"/></alternatives></inline-formula> is the reconstruction of input vector <inline-formula id="IEq8"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x$$\end{document}</tex-math><mml:math id="M18"><mml:mi>x</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq8.gif"/></alternatives></inline-formula> and <inline-formula id="IEq9"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr{J}}={\sum }_{x\in D}L(x,\widetilde{x}(\varphi ,\psi ,x))$$\end{document}</tex-math><mml:math id="M20"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">J</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo> ∑</mml:mo></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>φ</mml:mi><mml:mo>,</mml:mo><mml:mi>ψ</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq9.gif"/></alternatives></inline-formula> is the total loss, which is evaluated as the summation of reconstruction error <inline-formula id="IEq10"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L$$\end{document}</tex-math><mml:math id="M22"><mml:mi>L</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq10.gif"/></alternatives></inline-formula> on training dataset <inline-formula id="IEq11"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$D$$\end{document}</tex-math><mml:math id="M24"><mml:mi>D</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq11.gif"/></alternatives></inline-formula>.</p>
    </sec>
    <sec id="Sec5">
      <title>Autoencoders variants</title>
      <p id="Par16">While training an AE, we aim not only to reconstruct the input from the latent representation, but also to extract beneficial features in this representation. Therefore, it is vital to utilize some forms of regularization techniques to avoid overfitting or useless representations, even if the AE can reconstruct the input with minimal loss<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR28">28</xref></sup>. Different regularization techniques can veritably produce different variations of objective functions, and subsequently different features extracted from the data. The next sub-section explains the autoencoder variants of this study.</p>
      <sec id="Sec6">
        <title>Denoising autoencoder</title>
        <p id="Par17">A beneficial form of regularization is used in <italic>denoising autoencoders</italic> (DAE), where the input vector <inline-formula id="IEq12"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x$$\end{document}</tex-math><mml:math id="M26"><mml:mi>x</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq12.gif"/></alternatives></inline-formula> is slightly corrupted and the autoencoder is expected to reconstruct the clean data from the latent representation<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. As a result, the DAE learns to resist against input noise and overfitting. The following objective function is used for training DAEs: <disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr{J}}=\sum _{x\in D}{{\mathbb{E}}}_{\widehat{x} \sim q(\widehat{x}| x)}L(x,\widetilde{x}(\varphi ,\psi ,\widehat{x}))$$\end{document}</tex-math><mml:math id="M28" display="block"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">J</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo mathsize="big"> ∑</mml:mo></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>~</mml:mo><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>∣</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>φ</mml:mi><mml:mo>,</mml:mo><mml:mi>ψ</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2019_52937_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula> where the expectation is evaluated over the corrupted versions <inline-formula id="IEq13"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\widehat{x}$$\end{document}</tex-math><mml:math id="M30"><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq13.gif"/></alternatives></inline-formula> of the original data <inline-formula id="IEq14"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x$$\end{document}</tex-math><mml:math id="M32"><mml:mi>x</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq14.gif"/></alternatives></inline-formula>, obtained from a corruption function <inline-formula id="IEq15"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$q(\widehat{x}| x)$$\end{document}</tex-math><mml:math id="M34"><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>∣</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq15.gif"/></alternatives></inline-formula>. This objective is optimized by stochastic gradient descent (SGD) or another iterative optimization algorithm. Additive isotropic Gaussian noise and binary masking noise are among the most frequently used corruption processes.</p>
        <p id="Par18">Dropout<sup><xref ref-type="bibr" rid="CR30">30</xref>,<xref ref-type="bibr" rid="CR31">31</xref></sup> is another regularization technique that introduces some noise to the nodes of any hidden layer, in contrast to the DAE that adds noise only to the input layer. The foremost dropout techniques utilized in deep learning are Bernoulli and Gaussian. In the former case, the output values of individual nodes are either dropped to zero with a probability <inline-formula id="IEq16"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1-p$$\end{document}</tex-math><mml:math id="M36"><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq16.gif"/></alternatives></inline-formula>, or kept unchanged with a probability <inline-formula id="IEq17"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p$$\end{document}</tex-math><mml:math id="M38"><mml:mi>p</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq17.gif"/></alternatives></inline-formula>. In the latter case, a multiplicative one-centered Gaussian noise <inline-formula id="IEq18"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr{N}}(1,{\sigma }^{2})$$\end{document}</tex-math><mml:math id="M40"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">N</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq18.gif"/></alternatives></inline-formula> is applied to the output values of the nodes.</p>
      </sec>
      <sec id="Sec7">
        <title>Contractive autoencoder</title>
        <p id="Par19">An alternative regularization technique is used in contractive autoencoder (CAE). If <inline-formula id="IEq19"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h=\psi (x)$$\end{document}</tex-math><mml:math id="M42"><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq19.gif"/></alternatives></inline-formula> is the latent representation of the input data <inline-formula id="IEq20"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x$$\end{document}</tex-math><mml:math id="M44"><mml:mi>x</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq20.gif"/></alternatives></inline-formula>, the regularization term in CAE is the total squares of all partial derivatives of <inline-formula id="IEq21"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h$$\end{document}</tex-math><mml:math id="M46"><mml:mi>h</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq21.gif"/></alternatives></inline-formula> with respect to each dimension of the previous layer; therefore, the objective function of a CAE is expressed as: <disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr{J}}=\sum _{x\in D}L(x,\widetilde{x}(\varphi ,\psi ,x))+\lambda \parallel {J}_{\psi }(x){\parallel }_{F}^{2}$$\end{document}</tex-math><mml:math id="M48" display="block"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">J</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo mathsize="big"> ∑</mml:mo></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:munder><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>φ</mml:mi><mml:mo>,</mml:mo><mml:mi>ψ</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><graphic xlink:href="41598_2019_52937_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula> where the penalty term <inline-formula id="IEq22"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\parallel {J}_{\psi }(x){\parallel }_{F}^{2}$$\end{document}</tex-math><mml:math id="M50"><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq22.gif"/></alternatives></inline-formula> is the Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input, and <inline-formula id="IEq23"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda $$\end{document}</tex-math><mml:math id="M52"><mml:mi>λ</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq23.gif"/></alternatives></inline-formula> is a balancing factor. The main goal of this term is to enforce the learned representation to be robust against small variations in the input data.</p>
      </sec>
      <sec id="Sec8">
        <title>Variational autoencoder</title>
        <p id="Par20">Variational autoencoder (VAE) is a special type of autoencoder with additional constraints on the encoded representations. It assumes that a latent, unobserved random variable <inline-formula id="IEq24"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z$$\end{document}</tex-math><mml:math id="M54"><mml:mi>z</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq24.gif"/></alternatives></inline-formula> exists, which can leads to the observations <inline-formula id="IEq25"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x$$\end{document}</tex-math><mml:math id="M56"><mml:mi>x</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq25.gif"/></alternatives></inline-formula> by some stochastic mapping. As a result, its objective is to approximate the distribution of the latent variable <inline-formula id="IEq26"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z$$\end{document}</tex-math><mml:math id="M58"><mml:mi>z</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq26.gif"/></alternatives></inline-formula> given the observations <inline-formula id="IEq27"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x$$\end{document}</tex-math><mml:math id="M60"><mml:mi>x</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq27.gif"/></alternatives></inline-formula>.</p>
        <p id="Par21">VAEs replace deterministic functions in the encoder and decoder by stochastic mappings; and compute the objective function in virtue of the density functions of the random variables: <disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr{J}}(\varphi ,\theta ,x)={D}_{KL}({q}_{\varphi }(z| x)\parallel {p}_{\theta }(z))-{{\mathbb{E}}}_{{q}_{\varphi }((z| x))}(log({p}_{\theta }(x| z)))$$\end{document}</tex-math><mml:math id="M62" display="block"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">J</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>φ</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>φ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>∣</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>φ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>∣</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mstyle mathvariant="normal"><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi></mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>∣</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2019_52937_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
        <p id="Par22">where <inline-formula id="IEq28"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${D}_{KL}$$\end{document}</tex-math><mml:math id="M64"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq28.gif"/></alternatives></inline-formula> stands for the KullbackâĂŞLeibler divergence, and <inline-formula id="IEq29"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$q$$\end{document}</tex-math><mml:math id="M66"><mml:mi>q</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq29.gif"/></alternatives></inline-formula> is the distribution approximating the true latent distribution of <inline-formula id="IEq30"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z$$\end{document}</tex-math><mml:math id="M68"><mml:mi>z</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq30.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq31"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\theta $$\end{document}</tex-math><mml:math id="M70"><mml:mi>θ</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq31.gif"/></alternatives></inline-formula>, <inline-formula id="IEq32"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varphi $$\end{document}</tex-math><mml:math id="M72"><mml:mi>φ</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq32.gif"/></alternatives></inline-formula> are the parameters of each distribution. The prior distribution over the latent variables is generally set to standard multivariate Gaussian <inline-formula id="IEq33"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${p}_{\theta }(z)={\mathscr{N}}(0,{\sigma }^{2}I)$$\end{document}</tex-math><mml:math id="M74"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">N</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>I</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq33.gif"/></alternatives></inline-formula>; however, alternative distribution have also been recently considered.</p>
        <p id="Par23">Although AEs can not generally be able to construct meaningful outputs from arbitrary encodings, VAE can learn a model of the data that can generate new samples from scratch by random sampling from the latent distribution. Therefore, VAEs are among the generative models.</p>
      </sec>
    </sec>
    <sec id="Sec9">
      <title>Models architecture</title>
      <p id="Par24">Our problem consists of two regression, namely reproducing mRNA and miRNA profiles from the latent variables, and two classification tasks to determine the tissue and disease state of each sample. All mRNA and miRNA expression profiles were normalized in <inline-formula id="IEq34"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[0,1]$$\end{document}</tex-math><mml:math id="M76"><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq34.gif"/></alternatives></inline-formula> by max-norm method of Scikit-Learn package<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>. We subsequently constructed the multi-input and multi-output models in Keras<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. Keras is a high-level Python NN library that runs on top of either TensorFlow<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> or Theano<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>.</p>
      <p id="Par25">We constructed four different NN architectures:</p>
      <p id="Par26">
        <list list-type="bullet">
          <list-item>
            <p id="Par27">Variational autoencoder (VAE)</p>
          </list-item>
          <list-item>
            <p id="Par28">Dropout-VAE, an extension of VAE with Bernoulli and Gaussian dropout layers being utilized for denoising.</p>
          </list-item>
          <list-item>
            <p id="Par29">Contractive autoencoder (CAE)</p>
          </list-item>
          <list-item>
            <p id="Par30">Dropout-CAE, the extended CAE with Bernoulli and Gaussian dropout layers.</p>
          </list-item>
        </list>
      </p>
      <p id="Par31">To make each network tolerate input noise, an optional Gaussian noise <inline-formula id="IEq35"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr{N}}(0,\,{\sigma }^{2})$$\end{document}</tex-math><mml:math id="M78"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">N</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq35.gif"/></alternatives></inline-formula> can be added to the input.</p>
    </sec>
    <sec id="Sec10">
      <title>Loss functions</title>
      <p id="Par32">We utilized <italic>cosine similarity</italic> as the loss for the classification tasks. Another choice was <italic>categorical cross entropy</italic>, but we preferred to use the former function since it provides bounded results, which can be easily used in hyperparameter optimization. We also used <italic>mean squared error (MSE)</italic> as the loss function for the regression tasks. The total loss in our multi-task learning problem was be the weighted sum of individual losses, where the weights were <inline-formula id="IEq36"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$5\times 1{0}^{-1}$$\end{document}</tex-math><mml:math id="M80"><mml:mn>5</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq36.gif"/></alternatives></inline-formula> for each classification task, and <inline-formula id="IEq37"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1\times 1{0}^{-3}$$\end{document}</tex-math><mml:math id="M82"><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq37.gif"/></alternatives></inline-formula> for each regression task. To evaluate the model performance in the validation dataset, we used used accuracy as the classification metric, and MSE and mean absolute error (MAE) for the regression tasks.</p>
      <p id="Par33">For presentation of the results, we preferred to use balanced accuracy. For imbalanced datasets, the conventional accuracy can be misleading. For instance if 95% of the data are normal, classifying all samples as normal gives 95% accuracy. Balanced accuracy solves this problem by normalizing the number of correctly predicted samples of each class by the size of the same class. More specifically, we used the python package scikit-learn implementation of balanced accuracy. If <inline-formula id="IEq38"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${y}_{i}$$\end{document}</tex-math><mml:math id="M84"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq38.gif"/></alternatives></inline-formula>, <inline-formula id="IEq39"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{y}}_{i}$$\end{document}</tex-math><mml:math id="M86"><mml:msub><mml:mrow><mml:mi>ŷ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq39.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq40"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${w}_{i}$$\end{document}</tex-math><mml:math id="M88"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq40.gif"/></alternatives></inline-formula> are the true label, predicted label, and weight of sample <inline-formula id="IEq41"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i$$\end{document}</tex-math><mml:math id="M90"><mml:mi>i</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq41.gif"/></alternatives></inline-formula>, respectively, then the adjusted sample weights <inline-formula id="IEq42"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{w}}_{i}$$\end{document}</tex-math><mml:math id="M92"><mml:msub><mml:mrow><mml:mi>ŵ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq42.gif"/></alternatives></inline-formula> and balanced accuracy are computed as follows: <disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{w}}_{i}=\frac{{w}_{i}}{{\sum }_{j}1({y}_{i}={y}_{j}){w}_{j}}$$\end{document}</tex-math><mml:math id="M94" display="block"><mml:msub><mml:mrow><mml:mi>ŵ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="41598_2019_52937_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\,Balanced\,accuracy\,(y,\hat{y},w)=\frac{1}{\sum {\hat{w}}_{i}}{\sum }_{i}1({\hat{y}}_{i}={y}_{i}){\hat{w}}_{i}$$\end{document}</tex-math><mml:math id="M96" display="block"><mml:mspace width="0.25em"/><mml:mstyle mathvariant="normal"><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mspace width="0.25em"/><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi></mml:mstyle><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>ŷ</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo mathsize="big">∑</mml:mo><mml:msub><mml:mrow><mml:mi>ŵ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>ŷ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>ŵ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><graphic xlink:href="41598_2019_52937_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula> where <inline-formula id="IEq43"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1(x)$$\end{document}</tex-math><mml:math id="M98"><mml:mn>1</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq43.gif"/></alternatives></inline-formula> is the indicator function.</p>
    </sec>
    <sec id="Sec11">
      <title>Batch normalization</title>
      <p id="Par34">A major issue in training the DNNs is altered distribution of each layer inputs, as the weights and other parameters of the previous layers change. As a result of this <italic>internal covariate shift</italic>, the learning rate should be lowered that causes reduced training speed rate, and also the initialization parameters should be assigned carefully<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. A well-known approach to address these challenges is <italic>batch normalization</italic>.</p>
      <p id="Par35">Let’s <inline-formula id="IEq44"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x}_{1\cdots m}$$\end{document}</tex-math><mml:math id="M100"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>⋯</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq44.gif"/></alternatives></inline-formula> be the values of an activation <inline-formula id="IEq45"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x$$\end{document}</tex-math><mml:math id="M102"><mml:mi>x</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq45.gif"/></alternatives></inline-formula> during a mini-batch. Then the batch normalized values <inline-formula id="IEq46"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${y}_{1\cdots m}$$\end{document}</tex-math><mml:math id="M104"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>⋯</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq46.gif"/></alternatives></inline-formula> are computed as follows: <disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${y}_{i}=\gamma \frac{{x}_{i}-\mu }{\sqrt{{\sigma }^{2}+\varepsilon }}+\beta $$\end{document}</tex-math><mml:math id="M106" display="block"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>γ</mml:mi><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:msqrt><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>ε</mml:mi></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi>β</mml:mi></mml:math><graphic xlink:href="41598_2019_52937_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par36">where <inline-formula id="IEq47"><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mu $$\end{document}</tex-math><mml:math id="M108"><mml:mi>μ</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq47.gif"/></alternatives></inline-formula> and <inline-formula id="IEq48"><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\sigma }^{2}$$\end{document}</tex-math><mml:math id="M110"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq48.gif"/></alternatives></inline-formula> are the mean and variance of the <inline-formula id="IEq49"><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x}_{1\cdots m}$$\end{document}</tex-math><mml:math id="M112"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>⋯</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq49.gif"/></alternatives></inline-formula> values, <inline-formula id="IEq50"><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\gamma $$\end{document}</tex-math><mml:math id="M114"><mml:mi>γ</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq50.gif"/></alternatives></inline-formula> and <inline-formula id="IEq51"><alternatives><tex-math id="M115">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\beta $$\end{document}</tex-math><mml:math id="M116"><mml:mi>β</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq51.gif"/></alternatives></inline-formula> are the scaling parameters to be learned, and <inline-formula id="IEq52"><alternatives><tex-math id="M117">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varepsilon $$\end{document}</tex-math><mml:math id="M118"><mml:mi>ε</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq52.gif"/></alternatives></inline-formula> is a small constant value added to the mini-batch variance for numerical stability.</p>
      <p id="Par37">This method basically standardizes the inputs of each layer in such a way that they have a mean and standard deviation of zero and one, respectively. Then scales the standardized values by a linear function, with parameters that are learned. Batch normalization is analogous to how the inputs to the networks are standardized, but it can be performed for each internal layer of a DNN. It turns out that, extending this technique to hidden layers can significantly improve the training speed.</p>
      <p id="Par38">As depicted in Fig. <xref rid="Fig1" ref-type="fig">1</xref>, each <italic>building layer</italic> of our network is consisted of a batch normalization layer, followed by a dense layer. Our experience showed the batch normalization layers significantly improve the training speed of the networks.</p>
    </sec>
    <sec id="Sec12">
      <title>Hyperparameters tuning</title>
      <p id="Par39">Many machine learning methods have a set of architectural parameters, called <italic>hyperparameters</italic>, which are determined prior to training the model. For example, the number of layers, the number of neurons per layer, the type of activation functions, and the type of dropout or noises are among the DNN hyperparameters. Since the values of hyperparameters affect the model architecture, hyperparameter optimization can be considered as model selection technique.</p>
      <p id="Par40">A key advantage of our work is optimizing the network hyperparameters in order to achieve the best classification and regression objectives. Besides two simple methods of hyperparameter tuning including grid search and random search, there exists a more advanced Bayesian algorithm<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>.</p>
      <p id="Par41">Bayesian optimization, in contrast to random or grid search, keeps track of the past evaluations and utilizes them to define a probabilistic model mapping hyperparameters to a probability distribution of the outcome of the objective function <inline-formula id="IEq53"><alternatives><tex-math id="M119">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P(s| h)$$\end{document}</tex-math><mml:math id="M120"><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>∣</mml:mo><mml:mi>h</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq53.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq54"><alternatives><tex-math id="M121">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$s$$\end{document}</tex-math><mml:math id="M122"><mml:mi>s</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq54.gif"/></alternatives></inline-formula> and <inline-formula id="IEq55"><alternatives><tex-math id="M123">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h$$\end{document}</tex-math><mml:math id="M124"><mml:mi>h</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq55.gif"/></alternatives></inline-formula> are the objective function score and hyperparameters. This is called a <italic>surrogate</italic> model for the objective function, and is easier to be optimized in comparison with the objective function itself. In each iteration, the Bayesian optimization selects optimal hyperparameters based on the surrogate function as the next set of hyperparameters to be evaluated by the actual objective function. Briefly, it works as the following algorithm:</p>
      <p id="Par42">
        <list list-type="bullet">
          <list-item>
            <p id="Par43">Build a surrogate probability model of the objective function.</p>
          </list-item>
          <list-item>
            <p id="Par44">Find the hyperparameters that perform best on the surrogate.</p>
          </list-item>
          <list-item>
            <p id="Par45">Evaluate these hyperparameters by the actual objective function.</p>
          </list-item>
          <list-item>
            <p id="Par46">Update the surrogate model by incorporating the new results.</p>
          </list-item>
          <list-item>
            <p id="Par47">Repeat steps 2âĂŞ4 for the specified number of iterations or running time.</p>
          </list-item>
        </list>
      </p>
      <p id="Par48">The main advantage of the Bayesian optimization is becoming more intelligent by continuously updating the surrogate probability model after each evaluation of the objective function. By intelligently selecting hyperparameters that are more likely to optimize the objective function in each iteration, they can find better set of hyperparameters in a fewer iterations, in comparison with random and grid search<sup><xref ref-type="bibr" rid="CR38">38</xref>,<xref ref-type="bibr" rid="CR39">39</xref></sup>.</p>
      <p id="Par49">To exploit Bayesian optimization in our model selection procedure, we utilized a Python module called Hyperopt<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>. Hyperopt provides efficacious algorithms and parallel infrastructure. We needed to define a search space and an objective function for the hyperparameter optimization. The next sub-section explains the search space. As the objective function, we used an average of the total losses of all data, which was split to 80% training and 20% test dataset.</p>
      <p id="Par50">While we benefited from all data in hyperparameter optimization, we performed 5-fold cross-validation while measuring classification accuracy to ensure the hyperparameters are not overfitted to some specific portion of the data.</p>
    </sec>
    <sec id="Sec13">
      <title>Hyperparameters search space</title>
      <p id="Par51">For each architecture, we selected a set of hyperparameters; and for each hyperparameter, we defined a set of discrete values as the search space. The Cartesian product of all of these search spaces was used as the total search space of the hyperparameter optimization process.</p>
      <p id="Par52">Below is a description of the hyperparameters: <list list-type="bullet"><list-item><p id="Par53"><bold>Units</bold>: A critical feature of each layer is the number of its neurons. Based on our experiences in a prior work, we selected a wide range of different values as the search space of each layer. These values were selected in a decreasing order for the encoder part, to gradually narrow it down from the input to the latent (code) layer, followed by an increasing order in the decoder part.</p></list-item><list-item><p id="Par54"><bold>Activation Functions</bold>: A widely used activation function is Rectified Linear Unit (ReLU), which is defined as <inline-formula id="IEq56"><alternatives><tex-math id="M125">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f(x)=\max (x,0)$$\end{document}</tex-math><mml:math id="M126"><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>max</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq56.gif"/></alternatives></inline-formula><sup><xref ref-type="bibr" rid="CR41">41</xref></sup>. Since gradients can readily flow whenever the input to the ReLU function is positive, gradient descent optimization is much simpler for ReLU than sigmoid activation functions. Although a few efficacious activation functions such as Swish<sup><xref ref-type="bibr" rid="CR42">42</xref></sup> and Selu<sup><xref ref-type="bibr" rid="CR43">43</xref></sup> have been recently introduced; However, there has been no significant and universal enhancement to utilize them instead of ReLu function. Despite promising features, ReLU can cause some difficulties in autoencoders; including gradient explosion and saturation. Hence we additionally used linear activation functions <inline-formula id="IEq57"><alternatives><tex-math id="M127">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f(x)=x$$\end{document}</tex-math><mml:math id="M128"><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>x</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq57.gif"/></alternatives></inline-formula> and SoftPlus activation function <inline-formula id="IEq58"><alternatives><tex-math id="M129">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f(x)=ln(1+\exp (x))$$\end{document}</tex-math><mml:math id="M130"><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle mathvariant="normal"><mml:mi>l</mml:mi><mml:mi>n</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq58.gif"/></alternatives></inline-formula>, as comprehensively discussed<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>.</p></list-item><list-item><p id="Par55"><bold>Dropout Rate</bold>: Dropout layer plays an important role by acting as regularizer to prevent overfitting. It has one main tunable parameter, which is the noise rate. We chose <inline-formula id="IEq59"><alternatives><tex-math id="M131">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\{0,0.25,0.5\}$$\end{document}</tex-math><mml:math id="M132"><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.25</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq59.gif"/></alternatives></inline-formula> as the set of noise rates. By putting 0 in the options, we provided Dropout-CAE and Dropout-VAE models. By this way, these models become more flexible to either exploit or not to exploit dropout layers into their own structures to achieve the maximum performance.</p></list-item></list></p>
      <p>For each NN architecture, we set Hyperopt to iterate over 200 different networks and select the best one among them.</p>
    </sec>
    <sec id="Sec14">
      <title>Dimension reduction analysis and comparison with other algorithms</title>
      <p id="Par56">We observed the performance of our architecture in reducing the initial dimension of the data by visualizing Principal Components Analysis (PCA) and t-distributed Stochastic Neighbor Embedding (t-SNE) of the mRNA profiles as well as CICs. This was done by performing 5-fold cross-validation and using the CIC of each sample when it was used in the test-dataset.</p>
      <p id="Par57">We compared the classification accuracy of the proposed deep learning method against other classification methods including <italic>KNN</italic>, <italic>Extra Tree Classifier</italic>, <italic>Random Forest Classifier</italic>, <italic>Stochastic Gradient Descent (SGD) Classifier</italic> and <italic>Support Vector Machine (SVM)</italic>. We selected these algorithms because they were available for parallel execution in Scikit-Learn package<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>. Moreover, they are among the most efficient and versatile classification algorithms. The inputs of all models were the same mRNA expression profiles. Each other machine learning algorithm was trained to solve one of the classification problems (i.e. tissue or disease). To ensure the hyperparameters of the other classifiers are selected properly, we performed <inline-formula id="IEq60"><alternatives><tex-math id="M133">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$100$$\end{document}</tex-math><mml:math id="M134"><mml:mn>100</mml:mn></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq60.gif"/></alternatives></inline-formula> iterations of hyperparameter optimization on each classification algorithm.</p>
    </sec>
  </sec>
  <sec id="Sec15" sec-type="results">
    <title>Results and Discussion</title>
    <sec id="Sec16">
      <title>Performance of the four different DNN architectures on different tasks</title>
      <p id="Par58">We trained the hyperparameter optimized networks for 200 epochs using the training dataset. As shown in Fig. S<xref rid="MOESM1" ref-type="media">1</xref>, network architecture and hyperparameters have a big impact on the regression and classification results. Figure S<xref rid="MOESM1" ref-type="media">2</xref> shows the performance of these networks on the training dataset during the training, and Fig. S<xref rid="MOESM1" ref-type="media">3</xref> shows their performance on the test dataset. The CAE and Dropout-CAE architectures depicted the best performance in all four tasks. For the regression tasks (i.e. reproducing the mRNA and predicting the miRNA EP) the CAE architecture had a slightly better performance over the Dropout-CAE, but for the classification tasks, both networks performed very similarly. Furthermore, there was no sign of overfitting for these architectures as the test dataset errors did not increase during the training. Also, all accuracy and MSEs reached stable levels, that showed 200 training epochs was sufficient. The red values in Fig. <xref rid="Fig1" ref-type="fig">1</xref> depict the optimal hyperparameter values. A striking observation was the size of CIC, determined by hyperparameter optimization. Although there were higher values up to 32 available as options for the size of code layer, all architectures were optimized by using smaller sizes between 8 to 20 (<xref rid="Fig1" ref-type="fig">1</xref>). Particularly, the Dropout-CAE that outperformed other architectures was optimized by a CIC of size 8 (<xref rid="Fig1" ref-type="fig">1b</xref>). This means that the original mRNA profile of size 19671 is encoded into an 8-dimensional latent space, and then all of the mRNA and miRNA expression profiles, tissue and disease state of the samples are obtained from this CIC space.</p>
    </sec>
    <sec id="Sec17">
      <title>DNN outperforms classical algorithms in tissue and cancer type classification</title>
      <p id="Par59">We compared the balanced accuracy of tissue and cancer classification using our DNN in comparison with five widely-used other classification algorithms including Extra Tree, k-Nearest Neighbors, Random Forest, Stochastic Gradient Descent and SVM methods. The input of each algorithm was the whole mRNA EP of one sample, and the tissue or disease label of the sample was expected as the output. To make fair comparisons, we also performed hyper-parameter optimization for all of classical classifiers (Fig. S<xref rid="MOESM1" ref-type="media">4</xref>). The results are provided in Fig. <xref rid="Fig2" ref-type="fig">2a,b</xref>. While our DNN reaches balanced accuracy of 98.1% for tissue and 95.2% for disease classification, the balanced accuracy of other methods is at most 95.1% and 90.9% for tissue and disease classification, respectively.<fig id="Fig2"><label>Figure 2</label><caption><p>Accuracy of the DNN, in comparison with classical dimension reduction and classification algorithms. (Left) Each shows the distribution of tissue (<bold>a</bold>) or disease (<bold>b</bold>) classification accuracy obtained by 200 iterations of hyperparameter optimization for each algorithm. The leftmost violin shows our DNN. The input to each algorithm is the whole mRNA EP, and the tissue or disease type is expected as the output. (Middle) Accuracy of the same algorithms for tissue (<bold>c</bold>) or disease (<bold>d</bold>) classification, when the input to each algorithm is the CICs obtained by DNN. The bar height and error bar length show the mean and standard deviation of 5-fold cross-validation results, respectively. (Right) Four different dimensionality reduction algorithms are compared with the CICs obtained by the DNN. For each algorithm, the dimension of mRNA EP is reduced to 8 (the same dimension of CIC). Given the 8-dimensional vector of each algorithm as the input, an Ensemble learning classifier is trained to predict the tissue (<bold>e</bold>) or disease (<bold>f</bold>) type. Bar heights and error bar lengths represent mean and standard deviation of accuracy in 5-fold cross validation. PCA = Principal Components Analysis; Kernel PCA = Non-linear version of PCA using kernels; Sparse PCA = implementation of PCA that finds a sparse set of components that can optimally reconstruct the data; Incremental PCA = PCA using Singular Value Decomposition (SVD) of the data, keeping only the most significant singular vectors, KNN = k-nearest neighbors, SGD = Stochastic Gradient Descent, SVM = Support Vector Machine. See scikit-learn manual for more details about these algorithms.</p></caption><graphic xlink:href="41598_2019_52937_Fig2_HTML" id="d29e2495"/></fig></p>
      <p id="Par60">We questioned whether classical algorithms could provide better results by having our CIC rather than the original mRNA EP as the input. For this purpose, we performed a new experiment in which the 8-dimensional CIC was given to each algorithm, including our DNN, as the input and the tissue or disease state of the sample was expected as the output (Fig. <xref rid="Fig2" ref-type="fig">2c,d</xref>). We used 5-fold cross validation in this experiment. Again, the accuracy of our deep classifier (that was stated earlier) was significantly better than the best accuracy obtained by other algorithms (McNemar <inline-formula id="IEq61"><alternatives><tex-math id="M135">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p$$\end{document}</tex-math><mml:math id="M136"><mml:mi>p</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq61.gif"/></alternatives></inline-formula>-value &lt; <inline-formula id="IEq62"><alternatives><tex-math id="M137">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1e-15$$\end{document}</tex-math><mml:math id="M138"><mml:mn>1</mml:mn><mml:mi>e</mml:mi><mml:mo>−</mml:mo><mml:mn>15</mml:mn></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq62.gif"/></alternatives></inline-formula>).</p>
    </sec>
    <sec id="Sec18">
      <title>CIC is a better dimensionality reduction than those obtained by classical algorithms</title>
      <p id="Par61">An important question is whether the CIC can provide more discriminative dimensionality reduction, in comparison with classical algorithms such as Principal Components Analysis (PCA), or its derivatives. To address this question, we reduced the dimensionality of each sample to 8 using the top principal components of PCA as well as its three other derivatives (Incremental, Kernel and Sparse PCA, see scikit-learn documentation for more details). Then we compared the 8-dimensional CICs with the 8-dimensional vectors obtained by the other algorithms. To make a fair comparison, we used trained an independent Ensemble-learning algorithm consisting of one SVM and another Random Forest classifier for each dimensionality reduction algorithm. The input of the Ensemble learning model was an 8-dimensional vector of one algorithm, and the output was either tissue or disease state.</p>
      <p id="Par62">As shown in Fig. <xref rid="Fig2" ref-type="fig">2e,f</xref>, the maximum classification accuracy for tissue and disease states from the PCA derivatives were 38.7% and 33.2%, respectively. Ensemble-learning model, however, could be trained to predict the tissue and disease state with 93.7% and 89.3% using CICs. These results were significantly higher (McNemar <inline-formula id="IEq63"><alternatives><tex-math id="M139">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p$$\end{document}</tex-math><mml:math id="M140"><mml:mi>p</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq63.gif"/></alternatives></inline-formula>-value &lt; <inline-formula id="IEq64"><alternatives><tex-math id="M141">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1e-15$$\end{document}</tex-math><mml:math id="M142"><mml:mn>1</mml:mn><mml:mi>e</mml:mi><mml:mo>−</mml:mo><mml:mn>15</mml:mn></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq64.gif"/></alternatives></inline-formula>) than those obtained by PCA derivatives.</p>
    </sec>
    <sec id="Sec19">
      <title>Samples are better discriminated by CICs rather than the original mRNA expression profiles</title>
      <p id="Par63">The TCGA data is integrated from different studies, hence one might ask whether the original mRNA expression profiles of each study are discriminated from the other studies due to batch effects. In that case, classification of different cancer types might only learn artificial patterns of batch effects that discriminate studies, rather than the real biological features of transcriptomes that discriminate tissues and cancer types.</p>
      <p id="Par64">To address this question, we compared the original 19671-dimensional mRNA expression profiles against the 8-dimensional CIC feature space that we learn using Dropout-CAE. It is noteworthy that each sample in the dataset will be used exactly once as a test sample in our cross-validation procedure; hence, all the datasets utilized for visualization and statistical analysis have been resulted in combining all corresponding samples generated by each fold of cross-validation. For visualization of the samples in each space, we used Principal Components Analysis (PCA) as a linear dimension reduction algorithm, and t-distributed Stochastic Neighbor Embedding (t-SNE)<sup><xref ref-type="bibr" rid="CR45">45</xref></sup> as a non-linear manifold-learning algorithm.</p>
      <p id="Par65">As shown in Fig. <xref rid="Fig3" ref-type="fig">3a</xref>, the original mRNA expression profiles from most of the studies overlap and it’s impossible to discriminate disease states of the samples from 2D PCA. The same PCA projection, however, has a significantly better discrimination of disease states if applied to the 8-dimensional latent space of the CICs (Fig. <xref rid="Fig3" ref-type="fig">3b</xref>).<fig id="Fig3"><label>Figure 3</label><caption><p>Discrimination of disease state of samples using the original mRNA expression profiles vs. Cell Identity Codes (CIC). (<bold>a</bold>) PCA plot of the original mRNA expression profiles of all samples. Each dot and its color show a sample and its disease state, respectively. (<bold>b</bold>) PCA plot of the 8-dimensional CIC space. (<bold>c</bold>) t-SNE plot of the original mRNA expression profiles. (<bold>d</bold>) t-SNE plot of the 8-dimensional CIC space.</p></caption><graphic xlink:href="41598_2019_52937_Fig3_HTML" id="d29e2602"/></fig></p>
      <p id="Par66">A more recent manifold learning algorithm called t-SNE is known to provide better representation of biological samples such as transcriptomes or single-cell expression profiles in low-dimensional spaces than linear methods such as PCA. We compared t-SNE visualization of original mRNA expression profiles (Fig. <xref rid="Fig3" ref-type="fig">3c</xref>) against latent CIC representations (Fig. <xref rid="Fig3" ref-type="fig">3d</xref>). Evidently, the 8-dimensional CIC space is significantly better discriminative of the disease state of the samples than the original gene expression profiles. Doing the same analyses for studying tissue-type discrimination provided similar results (Fig. S<xref rid="MOESM1" ref-type="media">5</xref>). Taken together, these results have two important conclusions: (I) There is no significant batch effect in the original mRNA expression profiles that can discriminate different studies. (II) The 8-dimensional CIC space is learning features from the transcription profiles that can very well discriminate different tissues and diseases.</p>
    </sec>
    <sec id="Sec20">
      <title>DNN accurately discriminates different cancer subtypes</title>
      <p id="Par67">Different cancer types or subtypes might arise from the same tissue (e.g. lung squamous cell carcinoma vs. lung adenocarcinoma). Each cancer type/subtype might have a specific therapeutic strategy. Hence, correct discrimination of cancer types/subtypes that arise from the same tissue is a critically important and sometimes challenging task of cancer pathology. To scrutinize this, we first visualized the confusion matrix of the 5-fold cross-validation classification results for all tissues (Fig. <xref rid="Fig4" ref-type="fig">4a</xref>) and disease states (Fig. <xref rid="Fig4" ref-type="fig">4b</xref>). Extended results are provided in Supplementing Tables S<xref rid="MOESM1" ref-type="media">3</xref> and S<xref rid="MOESM1" ref-type="media">4</xref>. It is also crucial to restate that all the datasets utilized for statistical analysis of Tables S<xref rid="MOESM1" ref-type="media">3</xref> and S<xref rid="MOESM1" ref-type="media">4</xref> have been resulted in combining all corresponding samples generated by each fold of cross-validation.<fig id="Fig4"><label>Figure 4</label><caption><p>Confusion matrices of tissue, disease, and disease sub-type classification. (<bold>a</bold>) Confusion matrix of tissue classification, in which the columns and rows represent the real and predicted tissue types, respectively. Each cell contains zero or some positive number of samples, and the diagonal and other cells represent correct and incorrect classifications, respectively. (<bold>b</bold>) Confusion matrix for classification of the disease type. (<bold>c</bold>) Confusion matrices for classification of cancer types/subtypes that originate from the same tissue and should be distinguished for appropriate clinical management. All tissues of the dataset having more than one cancer type are presented.</p></caption><graphic xlink:href="41598_2019_52937_Fig4_HTML" id="d29e2655"/></fig></p>
      <p id="Par68">Then, we focused on all 4 tissue types that had more than one type of cancer in this dataset: Colorectal, Lung, Uterus and Kidney. As shown in Fig. <xref rid="Fig4" ref-type="fig">4c</xref>, the confusion matrices for these cancer types confirm there are only 17 misclassifications out of 3120 total samples, which provides us with 99.4% accuracy for cancer subtype classification. This accuracy ranges from 98.7% (Colorectal cancer subtypes) to 100% (Uterus). Discriminating cancer subtypes is clinically very important for deciding a correct therapeutic strategy, and is often challenging due to histological similarity of cancer subtypes and their heterogeneity. This finding paves the road of using high-throughput molecular data to address challenging pathology problems.</p>
    </sec>
    <sec id="Sec21">
      <title>Stable classification of different tissues and cancer types</title>
      <p id="Par69">To ensure the results are not overfitted to a particular subset of the data, we performed a 5-fold cross-validation and measured different accuracy criteria for tissue and disease classification. The data was shuffled and each sample was randomly assigned to one of 5 groups. In <inline-formula id="IEq65"><alternatives><tex-math id="M143">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i$$\end{document}</tex-math><mml:math id="M144"><mml:mi>i</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq65.gif"/></alternatives></inline-formula>-th round of cross-validation (<inline-formula id="IEq66"><alternatives><tex-math id="M145">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1\le i\le 5$$\end{document}</tex-math><mml:math id="M146"><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:mn>5</mml:mn></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq66.gif"/></alternatives></inline-formula>), group <inline-formula id="IEq67"><alternatives><tex-math id="M147">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i$$\end{document}</tex-math><mml:math id="M148"><mml:mi>i</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq67.gif"/></alternatives></inline-formula> was used as the test set and the remaining four groups were used for training. The results are depicted in Fig. <xref rid="Fig5" ref-type="fig">5</xref>.<fig id="Fig5"><label>Figure 5</label><caption><p>Performance of DNN in the classification of different tissues and diseases. (<bold>a</bold>) Balanced accuracy of tissue classification. (<bold>b</bold>) Balanced accuracy of disease classification. Healthy samples are depicted as “Healthy”. (<bold>c</bold>,<bold>d</bold>) Sensitivity and specificity of disease classification. All analyses are performed using 5-fold cross-validation. Error bars indicate standard error (SE).</p></caption><graphic xlink:href="41598_2019_52937_Fig5_HTML" id="d29e2727"/></fig></p>
      <p id="Par70">The balanced accuracy of tissue classification was <inline-formula id="IEq68"><alternatives><tex-math id="M149">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\ge $$\end{document}</tex-math><mml:math id="M150"><mml:mo>≥</mml:mo></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq68.gif"/></alternatives></inline-formula>99% for 14 tissues, and <inline-formula id="IEq69"><alternatives><tex-math id="M151">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\ge $$\end{document}</tex-math><mml:math id="M152"><mml:mo>≥</mml:mo></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq69.gif"/></alternatives></inline-formula>95% for 25 out of 27 tissues. This showed the DNN results are stable across different tissue types. Disease classification was <inline-formula id="IEq70"><alternatives><tex-math id="M153">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\ge $$\end{document}</tex-math><mml:math id="M154"><mml:mo>≥</mml:mo></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq70.gif"/></alternatives></inline-formula>99% accurate for 9 cancer types, and <inline-formula id="IEq71"><alternatives><tex-math id="M155">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\ge $$\end{document}</tex-math><mml:math id="M156"><mml:mo>≥</mml:mo></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq71.gif"/></alternatives></inline-formula>95% for 25 cancer types and normal tissues. Only 3 out of 33 cancer types had a balanced accuracy less than 90%. The standard error among 5 rounds of cross-validation was negligible for most of the tissues and disease types. These analyses confirmed our method is not overfitted towards a particular tissue or disease type or some subset of the data.</p>
      <p id="Par71">Figure <xref rid="Fig5" ref-type="fig">5c,d</xref> show sensitivity and specificity of DNN in disease classification. We observed an average sensitivity <inline-formula id="IEq72"><alternatives><tex-math id="M157">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\ge $$\end{document}</tex-math><mml:math id="M158"><mml:mo>≥</mml:mo></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq72.gif"/></alternatives></inline-formula>99% and <inline-formula id="IEq73"><alternatives><tex-math id="M159">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\ge $$\end{document}</tex-math><mml:math id="M160"><mml:mo>≥</mml:mo></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq73.gif"/></alternatives></inline-formula>95% for 6 and 18 cancer types, respectively. Seven cancer types had a sensitivity lower than 90%. The specificity was <inline-formula id="IEq74"><alternatives><tex-math id="M161">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\ge $$\end{document}</tex-math><mml:math id="M162"><mml:mo>≥</mml:mo></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq74.gif"/></alternatives></inline-formula>99% for all 34 classes, including 33 cancer types and 1 normal tissues.</p>
      <p id="Par72">It is important to mention that the samples considered as “Normal” in TCGA are not purely normal because most of them are obtained from the tissues adjacent to cancer tumors. As a result, we can consider some samples that are labeled “Normal” to be predicted as non-normal in our model. This can be seen in Fig. <xref rid="Fig4" ref-type="fig">4b</xref>, as the highest misclassfications have occurred in predicting “Normal” samples to be from one cancer type. It’s hard to say that our model has made errors in these cases, and we expect the true accuracy of our model is slightly higher than the reported value.</p>
    </sec>
    <sec id="Sec22">
      <title>DNN can resist noise and missing values</title>
      <p id="Par73">A potential issue with classifiers is their reduced accuracy when the input data are noisy or have missing values, due to sample quality or measurement errors. To check the effect of missing values, we added a dropout layer after the input that randomly dropped the expression values of a random set of genes by setting them to zero. The fraction of dropout genes was increased from 0 to 50% with 1% steps. The results are presented in Fig. <xref rid="Fig6" ref-type="fig">6a</xref>. In each plot, the <inline-formula id="IEq75"><alternatives><tex-math id="M163">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x$$\end{document}</tex-math><mml:math id="M164"><mml:mi>x</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq75.gif"/></alternatives></inline-formula>-axis shows the fraction of dropout genes, and the <inline-formula id="IEq76"><alternatives><tex-math id="M165">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y$$\end{document}</tex-math><mml:math id="M166"><mml:mi>y</mml:mi></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq76.gif"/></alternatives></inline-formula>-axis shows either regression MSE or classification accuracy for the test dataset. As shown, dropping the values of 50% of the genes has a negligible effect on reproducing mRNA expression profiles, with almost no MSE change for all architectures excepting Dropout-VAE. Interestingly, the increased dropout rate caused an improved MSE for Dropout-VAE. Also, increasing the dropout rate elevated the MSE of predicting miRNA EP for all DNN architectures. But even at 50% dropout, the MSE of all architectures was around 0.004, which is quite small.<fig id="Fig6"><label>Figure 6</label><caption><p>The resistance of different DNN architectures against missing values and noise. (<bold>a</bold>) In each plot, the x-axis shows the fragment of randomly-selected input values that are set to zero (dropout), and y-axis shows either regression MSE or classification accuracy for the test dataset. (<bold>b</bold>) The x-axis shows the standard error (SD) of a zero-centered Gaussian noise which was added to the input values, and y-axis shows MSE or accuracy for the test dataset. Colors indicate different DNN architectures (see the figure legend).</p></caption><graphic xlink:href="41598_2019_52937_Fig6_HTML" id="d29e2852"/></fig></p>
      <p id="Par74">The classification accuracy of all DNNs, particularly the Dropout-CAE and Dropout-VAE had small changes by increasing the missing values from 0 to 20%. Even at 30% dropout rate, Dropout-CAE was 95% accurate. Disease classification accuracy of both Dropout-CAE and Dropout-VAE had also small changes by a dropout rate up to 20%. These experiments showed that the DNN architectures that contained Dropout layer had the highest resistance to missing values.</p>
      <p id="Par75">We also measured the resistance of DNNs to a noisy input (Fig. <xref rid="Fig6" ref-type="fig">6b</xref>). A layer just after the input added a zero-mean Gaussian noise to the input GEPs. The magnitude of the noise was controlled by increasing its standard deviation (SD) from 0 to 0.25 with 0.01 steps. Both CAE and Dropout-CAE architectures were quite resistant to noise in reproducing the mRNA EP, and their MSEs were almost unchanged at SD = 0.25. All networks could predict miRNA EP with MSE <inline-formula id="IEq77"><alternatives><tex-math id="M167">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\le $$\end{document}</tex-math><mml:math id="M168"><mml:mo>≤</mml:mo></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq77.gif"/></alternatives></inline-formula>0.01 when the SD of Gaussian noise was at most 0.08. But their errors were increased by increasing the noise magnitude. The tissue and disease classification accuracy of Dropout-CAE were almost unchanged for a noise with SD <inline-formula id="IEq78"><alternatives><tex-math id="M169">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\le 0.05$$\end{document}</tex-math><mml:math id="M170"><mml:mo>≤</mml:mo><mml:mn>0.05</mml:mn></mml:math><inline-graphic xlink:href="41598_2019_52937_Article_IEq78.gif"/></alternatives></inline-formula>. Dropout-CAE and Dropout-VAE outperformed the other architectures in resisting against Gaussian noise.</p>
      <p id="Par76">Collectively, our results indicate the power of DNNs in obtaining biologically and clinically important information from transcriptome profiles. Our networks compress the transcriptome profile into a thumbnail CIC, and obtain tissue and disease type and miRNA expression profiles out of it. This process is greatly robust against noisy and missing data and outperforms baseline algorithms in accuracy. We suggest employing DNNs in inferring the outcome of molecular cancer pathology.</p>
    </sec>
    <sec id="Sec23">
      <title>Future works</title>
      <p id="Par77">Due to the black-box nature of deep neural networks, it is difficult to extract easy to understand patterns from the model and discover causality relationship between the original dataset and outputs<sup><xref ref-type="bibr" rid="CR46">46</xref>,<xref ref-type="bibr" rid="CR47">47</xref></sup>. The authors, however, have been exerting efforts to utilize statistical methods to extract such information from the trained models. These methods are usually based on gradient variation and are categorized as sample-based and model-based approaches, including Shap<sup><xref ref-type="bibr" rid="CR48">48</xref>,<xref ref-type="bibr" rid="CR49">49</xref></sup>, DeepLift<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>, LIME<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>, and Interpret<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>. This can pave the way for finding novel biomarkers and regulatory interactions of different tissues and disease states.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec24">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="41598_2019_52937_MOESM1_ESM.pdf">
            <caption>
              <p>Supplementary Information</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p>is available for this paper at 10.1038/s41598-019-52937-5.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>Authors would like to acknowledge creative comments and ideas by Dr. S. M. Ali Eslami and Dr. Mahmoud Ghandi. We are also grateful to Hamid Malki for his artworks in Figures 1 and S4.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>A.S.Z. and H.C. developed the idea of using DNNs for processing mRNA profiles. AS developed a pipeline for integration and preprocessing of the data. B.A. developed the deep learning framework and hyperparameter optimization, generated the results and created the figures. All authors contributed in discussing the results and writing the manuscript. H.C. provided the computational resources.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>All data can be obtained from Genomic Data Commons (https://gdc.cancer.gov). All data pre-processing and analysis source codes are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/SharifBioinf/DeePathology">https://github.com/SharifBioinf/DeePathology</ext-link>.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par78">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Raab</surname>
            <given-names>SS</given-names>
          </name>
          <name>
            <surname>Grzybicki</surname>
            <given-names>DM</given-names>
          </name>
        </person-group>
        <article-title>Quality in Cancer Diagnosis</article-title>
        <source>CA: A Cancer Journal for Clinicians</source>
        <year>2010</year>
        <volume>60</volume>
        <fpage>139</fpage>
        <lpage>165</lpage>
        <pub-id pub-id-type="pmid">20444999</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bruner</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Inouye</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Fuller</surname>
            <given-names>GN</given-names>
          </name>
          <name>
            <surname>Langford</surname>
            <given-names>LA</given-names>
          </name>
        </person-group>
        <article-title>Diagnostic discrepancies and their clinical impact in a neuropathology referral practice</article-title>
        <source>Cancer</source>
        <year>1997</year>
        <volume>79</volume>
        <fpage>796</fpage>
        <lpage>803</lpage>
        <pub-id pub-id-type="doi">10.1002/(SICI)1097-0142(19970215)79:4&lt;796::AID-CNCR17&gt;3.0.CO;2-V</pub-id>
        <pub-id pub-id-type="pmid">9024718</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Staradub</surname>
            <given-names>VL</given-names>
          </name>
          <name>
            <surname>Messenger</surname>
            <given-names>KA</given-names>
          </name>
          <name>
            <surname>Hao</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Wiley</surname>
            <given-names>EL</given-names>
          </name>
          <name>
            <surname>Morrow</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Changes in breast cancer therapy because of pathology second opinions</article-title>
        <source>Annals of surgical oncology</source>
        <year>2002</year>
        <volume>9</volume>
        <fpage>982</fpage>
        <lpage>987</lpage>
        <pub-id pub-id-type="doi">10.1007/BF02574516</pub-id>
        <pub-id pub-id-type="pmid">12464590</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hamady</surname>
            <given-names>ZZR</given-names>
          </name>
          <name>
            <surname>Mather</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Lansdown</surname>
            <given-names>MR</given-names>
          </name>
          <name>
            <surname>Davidson</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Maclennan</surname>
            <given-names>KA</given-names>
          </name>
        </person-group>
        <article-title>Surgical pathological second opinion in thyroid malignancy: impact on patients’ management and prognosis. - PubMed - NCBI</article-title>
        <source>European Journal of Surgical Oncology (EJSO)</source>
        <year>2005</year>
        <volume>31</volume>
        <fpage>74</fpage>
        <lpage>77</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ejso.2004.08.010</pub-id>
        <pub-id pub-id-type="pmid">15642429</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Elmore</surname>
            <given-names>JG</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Pathologists’ diagnosis of invasive melanoma and melanocytic proliferations: observer accuracy and reproducibility study. - PubMed - NCBI</article-title>
        <source>BMJ</source>
        <year>2017</year>
        <volume>357</volume>
        <fpage>j2813</fpage>
        <pub-id pub-id-type="doi">10.1136/bmj.j2813</pub-id>
        <pub-id pub-id-type="pmid">28659278</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Maor</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Ondhia</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>LL</given-names>
          </name>
          <name>
            <surname>Chan</surname>
            <given-names>JJ</given-names>
          </name>
        </person-group>
        <article-title>Lichenoid keratosis is frequently misdiagnosed as basal cell carcinoma</article-title>
        <source>Clinical and Experimental Dermatology</source>
        <year>2017</year>
        <volume>42</volume>
        <fpage>663</fpage>
        <lpage>666</lpage>
        <pub-id pub-id-type="doi">10.1111/ced.13178</pub-id>
        <pub-id pub-id-type="pmid">28636260</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xie</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Hepatoid adenocarcinoma of the stomach is a special and easily misdiagnosed or missed diagnosed subtype of gastric cancer with poor prognosis but curative for patients of pN0/1: the experience of a single center</article-title>
        <source>International Journal of Clinical and Experimental Medicine</source>
        <year>2015</year>
        <volume>8</volume>
        <fpage>6762</fpage>
        <lpage>6772</lpage>
        <?supplied-pmid 26221214?>
        <pub-id pub-id-type="pmid">26221214</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Emori</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A typical presentation of primary pulmonary epithelioid sarcoma misdiagnosed as non-small cell lung cancer</article-title>
        <source>Pathology International</source>
        <year>2017</year>
        <volume>67</volume>
        <fpage>222</fpage>
        <lpage>224</lpage>
        <pub-id pub-id-type="doi">10.1111/pin.12503</pub-id>
        <pub-id pub-id-type="pmid">28078765</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lapuk</surname>
            <given-names>AV</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>From sequence to molecular pathology, and a mechanism driving the neuroendocrine phenotype in prostate cancer</article-title>
        <source>The Journal of pathology</source>
        <year>2012</year>
        <volume>227</volume>
        <fpage>286</fpage>
        <lpage>297</lpage>
        <pub-id pub-id-type="doi">10.1002/path.4047</pub-id>
        <pub-id pub-id-type="pmid">22553170</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Ogihara</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>A comparative study of feature selection and multiclass classification methods for tissue classification based on gene expression</article-title>
        <source>Bioinformatics</source>
        <year>2004</year>
        <volume>20</volume>
        <fpage>2429</fpage>
        <lpage>2437</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bth267</pub-id>
        <pub-id pub-id-type="pmid">15087314</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other"> Bhat, R. R., Viswanath, V. &amp; Li, X. DeepCancer: Detecting Cancer through Gene Expressions via Deep Generative Learning. <italic>arXiv.org</italic> 1612.03211v2 (2016).</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Khalili</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Majd</surname>
            <given-names>HA</given-names>
          </name>
          <name>
            <surname>Khodakarim</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Prediction of the thromboembolic syndrome: an application of artificial neural networks in gene expression data analysis</article-title>
        <source>Journal of Paramedical Sciences</source>
        <year>2016</year>
        <volume>7</volume>
        <fpage>15</fpage>
        <lpage>22</lpage>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Yuan</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Random Subspace Aggregation for Cancer Prediction with Gene Expression Profiles</article-title>
        <source>BioMed Research International</source>
        <year>2016</year>
        <volume>2016</volume>
        <fpage>1</fpage>
        <lpage>10</lpage>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Patil</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Naik</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Pai</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Gad</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Stacked autoencoder for classification of glioma grade iii and grade iv</article-title>
        <source>Biomedical Signal Processing and Control</source>
        <year>2018</year>
        <volume>46</volume>
        <fpage>67</fpage>
        <lpage>75</lpage>
        <pub-id pub-id-type="doi">10.1016/j.bspc.2018.07.002</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lê Cao</surname>
            <given-names>K-A</given-names>
          </name>
          <name>
            <surname>Bonnet</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gadat</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Multiclass classification and gene selection with a stochastic algorithm</article-title>
        <source>Computational Statistics &amp; Data Analysis</source>
        <year>2009</year>
        <volume>53</volume>
        <fpage>3601</fpage>
        <lpage>3615</lpage>
        <pub-id pub-id-type="doi">10.1016/j.csda.2009.02.028</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other"> Chai, H. &amp; Domeniconi, C. An evaluation of gene selection methods for multi-class microarray data classification. In <italic>Proceedings of the Second European Workshop on Data Mining and Text Mining for Bioinformatics (in conjunction with ECML/PKDD)</italic> (2004).</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Naiman</surname>
            <given-names>DQ</given-names>
          </name>
        </person-group>
        <article-title>Multiclass cancer classification based on gene expression comparison. - PubMed - NCBI</article-title>
        <source>Statistical applications in genetics and molecular biology</source>
        <year>2014</year>
        <volume>0</volume>
        <fpage>477</fpage>
        <lpage>496</lpage>
        <pub-id pub-id-type="doi">10.1515/sagmb-2013-0053</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Grossman</surname>
            <given-names>Robert L.</given-names>
          </name>
          <name>
            <surname>Heath</surname>
            <given-names>Allison P.</given-names>
          </name>
          <name>
            <surname>Ferretti</surname>
            <given-names>Vincent</given-names>
          </name>
          <name>
            <surname>Varmus</surname>
            <given-names>Harold E.</given-names>
          </name>
          <name>
            <surname>Lowy</surname>
            <given-names>Douglas R.</given-names>
          </name>
          <name>
            <surname>Kibbe</surname>
            <given-names>Warren A.</given-names>
          </name>
          <name>
            <surname>Staudt</surname>
            <given-names>Louis M.</given-names>
          </name>
        </person-group>
        <article-title>Toward a Shared Vision for Cancer Genomic Data</article-title>
        <source>New England Journal of Medicine</source>
        <year>2016</year>
        <volume>375</volume>
        <issue>12</issue>
        <fpage>1109</fpage>
        <lpage>1112</lpage>
        <pub-id pub-id-type="doi">10.1056/NEJMp1607591</pub-id>
        <pub-id pub-id-type="pmid">27653561</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ding</surname>
            <given-names>L</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Perspective on Oncogenic Processes at the End of the Beginning of Cancer Genomics</article-title>
        <source>Cell</source>
        <year>2018</year>
        <volume>173</volume>
        <fpage>305</fpage>
        <lpage>320.e10</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2018.03.033</pub-id>
        <pub-id pub-id-type="pmid">29625049</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sanchez-Vega</surname>
            <given-names>F</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Oncogenic Signaling Pathways in The Cancer Genome Atlas</article-title>
        <source>Cell</source>
        <year>2018</year>
        <volume>173</volume>
        <fpage>321</fpage>
        <lpage>337.e10</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2018.03.035</pub-id>
        <pub-id pub-id-type="pmid">29625050</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hoadley</surname>
            <given-names>KA</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Cell-of-Origin Patterns Dominate the Molecular Classification of 10,000 Tumors from 33 Types of Cancer</article-title>
        <source>Cell</source>
        <year>2018</year>
        <volume>173</volume>
        <fpage>291</fpage>
        <lpage>304.e6</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2018.03.022</pub-id>
        <pub-id pub-id-type="pmid">29625048</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Malta</surname>
            <given-names>TM</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Machine Learning Identifies Stemness Features Associated with Oncogenic Dedifferentiation</article-title>
        <source>Cell</source>
        <year>2018</year>
        <volume>173</volume>
        <fpage>338</fpage>
        <lpage>354.e15</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2018.03.034</pub-id>
        <pub-id pub-id-type="pmid">29625051</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xiao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>A semi-supervised deep learning method based on stacked sparse auto-encoder for cancer prediction using rna-seq data</article-title>
        <source>Computer methods and programs in biomedicine</source>
        <year>2018</year>
        <volume>166</volume>
        <fpage>99</fpage>
        <lpage>105</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cmpb.2018.10.004</pub-id>
        <pub-id pub-id-type="pmid">30415723</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yuan</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>DeepGene: an advanced cancer type classifier based on deep learning and somatic point mutations</article-title>
        <source>BMC Bioinformatics</source>
        <year>2016</year>
        <volume>17</volume>
        <fpage>476</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-016-1334-9</pub-id>
        <pub-id pub-id-type="pmid">28155641</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A comprehensive genomic pan-cancer classification using The Cancer Genome Atlas gene expression data</article-title>
        <source>BMC Genomics</source>
        <year>2017</year>
        <volume>18</volume>
        <fpage>508</fpage>
        <pub-id pub-id-type="doi">10.1186/s12864-017-3906-0</pub-id>
        <pub-id pub-id-type="pmid">28673244</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Colaprico</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>TCGAbiolinks: an R/Bioconductor package for integrative analysis of TCGA data</article-title>
        <source>Nucleic acids research</source>
        <year>2016</year>
        <volume>44</volume>
        <fpage>e71</fpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkv1507</pub-id>
        <pub-id pub-id-type="pmid">26704973</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Charte</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Charte</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Garcia</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>del Jesus</surname>
            <given-names>MIAJ</given-names>
          </name>
          <name>
            <surname>Herrera</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>A practical tutorial on autoencoders for nonlinear feature fusion: Taxonomy, models, software and guidelines</article-title>
        <source>Information Fusion</source>
        <year>2018</year>
        <volume>44</volume>
        <fpage>78</fpage>
        <lpage>96</lpage>
        <pub-id pub-id-type="doi">10.1016/j.inffus.2017.12.007</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other"> Rifai, S., Glorot, X., Bengio, Y. &amp; Vincent, P. Adding noise to the input of a model trained with a regularized objective. <italic>arXiv.org</italic> 1–13 (2011).</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other"> Vincent, P., Larochelle, H., Bengio, Y. &amp; Manzagol, P.-A. Extracting and composing robust features with denoising autoencoders. <italic>Proceedings of the 25th international conference on Machine learning - ICML ’08</italic> 1096–1103 (2008).</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other"> Goodfellow, I., Bengio, Y. &amp; Courville, A. Regularization for Deep Learning. <italic>Deep Learning</italic> 216–261 (2016).</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Srivastava</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Krizhevsky</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Salakhutdinov</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</article-title>
        <source>Journal of Machine Learning Research</source>
        <year>2014</year>
        <volume>15</volume>
        <fpage>1929</fpage>
        <lpage>1958</lpage>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pedregosa</surname>
            <given-names>F</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Scikit-learn: Machine Learning in Python</article-title>
        <source>Journal of Machine Learning Research</source>
        <year>2012</year>
        <volume>12</volume>
        <fpage>2825</fpage>
        <lpage>2830</lpage>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other"> Keras deep-learning python package., <ext-link ext-link-type="uri" xlink:href="https://github.com/keras-team/keras">https://github.com/keras-team/keras</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other"> Abadi, M. I. N. <italic>et al</italic>. TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems. <italic>arXiv.org</italic> (2016).</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other"> The Theano Development Team <italic>et al</italic>. Theano: A Python framework for fast computation of mathematical expressions. <italic>arXiv.org</italic> 1–19 (2016).</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other"> Ioffe, S. &amp; Szegedy, C. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1502.03167">http://arxiv.org/abs/1502.03167</ext-link>. 1502.03167, (2015).</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shahriari</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Swersky</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Adams</surname>
            <given-names>RP</given-names>
          </name>
          <name>
            <surname>De Freitas</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Taking the human out of the loop: A review of Bayesian optimization</article-title>
        <source>Proceedings of the IEEE</source>
        <year>2016</year>
        <volume>104</volume>
        <fpage>148</fpage>
        <lpage>175</lpage>
        <pub-id pub-id-type="doi">10.1109/JPROC.2015.2494218</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other"> Dewancker, I., Clark, S., Com, I. A. N. S., Com, M. S. &amp; Com, S. S. Bayesian Optimization Primer 2–5 (2015).</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other"> Snoek, J., Larochelle, H. &amp; Adams, R. Practical Bayesian Optimization of Machine Learning Algorithms. <italic>Nips</italic> 1–9 (2012).</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other"> Bergstra, J., Yamins, D. &amp; Cox, D. D. Hyperopt: A python library for optimizing the hyperparameters of machine learning algorithms 13–20 (2013).</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other"> Nair, V. &amp; Hinton, G. E. Rectified Linear Units Improve Restricted Boltzmann Machines. <italic>Proceedings of the 27th International Conference on Machine Learning</italic> 807–814 (2010).</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <mixed-citation publication-type="other"> Ramachandran, P., Zoph, B. &amp; Le, Q. V. Searching for Activation Functions. <italic>arXiv.org</italic> 1–12 (2017).</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other"> Klambauer, G., Unterthiner, T., Mayr, A. &amp; Hochreiter, S. Self-Normalizing Neural Networks. <italic>arXiv.org</italic> (2017).</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Glorot</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Bordes</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Deep sparse rectifier neural networks</article-title>
        <source>Journal of Machine Learning Research</source>
        <year>2011</year>
        <volume>15</volume>
        <fpage>315</fpage>
        <lpage>323</lpage>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Maaten</surname>
            <given-names>LVD</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Visualizing Data using t-SNE</article-title>
        <source>Journal of Machine Learning Research</source>
        <year>2008</year>
        <volume>9</volume>
        <fpage>2579</fpage>
        <lpage>2605</lpage>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <mixed-citation publication-type="other"> Ghorbani, A., Abid, A. &amp; Zou, J. Y. Interpretation of neural networks is fragile. <italic>CoRR</italic> abs/1710.10547 (2018).</mixed-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Z</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Opening the black box of neural networks: methods for interpreting neural network models in clinical applications</article-title>
        <source>Annals of translational medicine</source>
        <year>2018</year>
        <volume>6</volume>
        <issue>11</issue>
        <fpage>216</fpage>
        <pub-id pub-id-type="doi">10.21037/atm.2018.05.32</pub-id>
        <pub-id pub-id-type="pmid">30023379</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <mixed-citation publication-type="other"> Lundberg, S. M. &amp; Lee, S.-I. A unified approach to interpreting model predictions. In Guyon, I. <italic>et al</italic>. (eds) <italic>Advances in Neural Information Processing Systems 30</italic>, 4765–4774, <ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf">http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf</ext-link> (Curran Associates, Inc., 2017).</mixed-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lundberg</surname>
            <given-names>SM</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Explainable machine-learning predictions for the prevention of hypoxaemia during surgery</article-title>
        <source>Nature Biomedical Engineering</source>
        <year>2018</year>
        <volume>2</volume>
        <fpage>749</fpage>
        <pub-id pub-id-type="doi">10.1038/s41551-018-0304-0</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <mixed-citation publication-type="other"> DeepLIFT: Deep Learning Important FeaTures. <ext-link ext-link-type="uri" xlink:href="https://github.com/kundajelab/deeplift">https://github.com/kundajelab/deeplift</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <mixed-citation publication-type="other"> Ribeiro, M. T., Singh, S. &amp; Guestrin, C. “why should I trust you?”: Explaining the predictions of any classifier. In <italic>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13–17, 2016</italic>, 1135–1144 (2016).</mixed-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <mixed-citation publication-type="other"> InterpretML: python package for training interpretable models and explaining blackbox systems., <ext-link ext-link-type="uri" xlink:href="https://github.com/microsoft/interpret">https://github.com/microsoft/interpret</ext-link>.</mixed-citation>
    </ref>
  </ref-list>
</back>
