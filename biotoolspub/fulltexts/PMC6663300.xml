<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with OASIS Tables with MathML3 v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-archive-oasis-article1-mathml3.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jpoasis-nisons2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Netw Neurosci</journal-id>
    <journal-id journal-id-type="iso-abbrev">Netw Neurosci</journal-id>
    <journal-id journal-id-type="publisher-id">netn</journal-id>
    <journal-title-group>
      <journal-title>Network Neuroscience</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2472-1751</issn>
    <publisher>
      <publisher-name>MIT Press</publisher-name>
      <publisher-loc>One Rogers Street, Cambridge, MA 02142-1209USAjournals-info@mit.edu</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6663300</article-id>
    <article-id pub-id-type="publisher-id">netn_a_00092</article-id>
    <article-id pub-id-type="doi">10.1162/netn_a_00092</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Methods</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Large-scale directed network inference with multivariate transfer entropy and hierarchical statistical testing</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-6081-3367</contrib-id>
        <name>
          <surname>Novelli</surname>
          <given-names>Leonardo</given-names>
        </name>
        <xref rid="cor1" ref-type="corresp">*</xref>
        <aff id="aff1">Centre for Complex Systems, Faculty of Engineering, The University of Sydney, Sydney, Australia</aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-7105-5207</contrib-id>
        <name>
          <surname>Wollstadt</surname>
          <given-names>Patricia</given-names>
        </name>
        <xref rid="fn1" ref-type="author-notes">†</xref>
        <aff id="aff2">Honda Research Institute Europe, Offenbach am Main, Germany</aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1789-5894</contrib-id>
        <name>
          <surname>Mediano</surname>
          <given-names>Pedro</given-names>
        </name>
        <aff id="aff3">Computational Neurodynamics Group, Department of Computing, Imperial College London, London, United Kingdom</aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-8010-5862</contrib-id>
        <name>
          <surname>Wibral</surname>
          <given-names>Michael</given-names>
        </name>
        <aff id="aff4">Campus Institute for Dynamics of Biological Networks, Georg-August University, Göttingen, Germany</aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-9910-8972</contrib-id>
        <name>
          <surname>Lizier</surname>
          <given-names>Joseph T.</given-names>
        </name>
        <aff id="aff5">Centre for Complex Systems, Faculty of Engineering, The University of Sydney, Sydney, Australia</aff>
      </contrib>
    </contrib-group>
    <author-notes>
      <fn fn-type="COI-statement">
        <p>Competing Interests: The authors have declared that no competing interests exist.</p>
      </fn>
      <corresp id="cor1">* Corresponding Author: <email xlink:href="mailto:leonardo.novelli@sydney.edu.au">leonardo.novelli@sydney.edu.au</email></corresp>
      <fn id="fn1">
        <label>†</label>
        <p>First authors contributed equally to this work.</p>
      </fn>
      <fn>
        <p>Handling Editor: Olaf Sporns</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>01</day>
      <month>7</month>
      <year>2019</year>
      <string-date>2019</string-date>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>3</volume>
    <issue>3</issue>
    <issue-title>Focus Feature: Topological Neuroscience</issue-title>
    <fpage>827</fpage>
    <lpage>847</lpage>
    <history>
      <date date-type="received">
        <day>24</day>
        <month>1</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>24</day>
        <month>4</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2019 Massachusetts Institute of Technology</copyright-statement>
      <copyright-year>2019</copyright-year>
      <copyright-holder>Massachusetts Institute of Technology</copyright-holder>
      <license license-type="open-access">
        <license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. For a full description of the license, please visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/legalcode">https://creativecommons.org/licenses/by/4.0/legalcode</ext-link>.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="netn-03-827.pdf"/>
    <abstract>
      <p>Network inference algorithms are valuable tools for the study of large-scale neuroimaging datasets. Multivariate transfer entropy is well suited for this task, being a model-free measure that captures nonlinear and lagged dependencies between time series to infer a minimal directed network model. Greedy algorithms have been proposed to efficiently deal with high-dimensional datasets while avoiding redundant inferences and capturing synergistic effects. However, multiple statistical comparisons may inflate the false positive rate and are computationally demanding, which limited the size of previous validation studies. The algorithm we present—as implemented in the IDTxl open-source software—addresses these challenges by employing hierarchical statistical tests to control the family-wise error rate and to allow for efficient parallelization. The method was validated on synthetic datasets involving random networks of increasing size (up to 100 nodes), for both linear and nonlinear dynamics. The performance increased with the length of the time series, reaching consistently high precision, recall, and specificity (&gt;98% on average) for 10,000 time samples. Varying the statistical significance threshold showed a more favorable precision-recall trade-off for longer time series. Both the network size and the sample size are one order of magnitude larger than previously demonstrated, showing feasibility for typical EEG and magnetoencephalography experiments.</p>
    </abstract>
    <kwd-group kwd-group-type="text">
      <title>Keywords</title>
      <kwd>Neuroimaging</kwd>
      <x xml:space="preserve">, </x>
      <kwd>Directed connectivity</kwd>
      <x xml:space="preserve">, </x>
      <kwd>Effective network</kwd>
      <x xml:space="preserve">, </x>
      <kwd>Multivariate transfer entropy</kwd>
      <x xml:space="preserve">, </x>
      <kwd>Information theory</kwd>
      <x xml:space="preserve">, </x>
      <kwd>Nonlinear dynamics</kwd>
      <x xml:space="preserve">, </x>
      <kwd>Statistical inference</kwd>
      <x xml:space="preserve">, </x>
      <kwd>Nonparametric tests</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Universities Australia/German Academic Exchange Service (DAAD) Australia-Germany Joint Research Cooperation Scheme grant: “Measuring neural information synthesis and its impairment”</institution>
          </institution-wrap>
        </funding-source>
        <award-id>57216857</award-id>
      </award-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Universities Australia/German Academic Exchange Service (DAAD) Australia-Germany Joint Research Cooperation Scheme grant: “Measuring neural information synthesis and its impairment”</institution>
          </institution-wrap>
        </funding-source>
        <award-id>57216857</award-id>
      </award-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Australian Research Council DECRA Grant</institution>
          </institution-wrap>
        </funding-source>
        <award-id>DE160100630</award-id>
      </award-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Deutsches Krebsforschungszentrum (DE)</institution>
          </institution-wrap>
        </funding-source>
        <award-id>CRC 1193 C04</award-id>
      </award-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Australian Research Council Discovery Grant</institution>
          </institution-wrap>
        </funding-source>
        <award-id>DP160102742</award-id>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="7"/>
      <table-count count="1"/>
      <equation-count count="8"/>
      <ref-count count="66"/>
      <page-count count="21"/>
    </counts>
    <custom-meta-group>
      <custom-meta>
        <meta-name>citation</meta-name>
        <meta-value>Novelli, L., Wollstadt, P., Mediano, P., Wibral, M., &amp; Lizier, J. T. (2019). Large-scale directed network inference with multivariate transfer entropy and hierarchical statistical testing. <italic>Network Neuroscience</italic>, <italic>3</italic>(3), 827–847. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/netn_a_00092">https://doi.org/10.1162/netn_a_00092</ext-link></meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec>
    <title>INTRODUCTION</title>
    <p>The increasing availability of large-scale, fine-grained datasets provides an unprecedented opportunity for quantitative studies of complex systems. Nonetheless, a shift toward data-driven modeling of these systems requires efficient algorithms for analyzing multivariate time series, which are obtained from observation of the activity of a large number of elements.</p>
    <p>In the field of neuroscience, the multivariate time series typically obtained from brain recordings serve to infer minimal (effective) network models which can explain the dynamics of the nodes in a neural system. The motivation for such models can be, for instance, to describe a causal network (Ay &amp; Polani, <xref rid="bib2" ref-type="bibr">2008</xref>; Friston, <xref rid="bib17" ref-type="bibr">1994</xref>) or to model the directed information flow in the system (Vicente et al., <xref rid="bib59" ref-type="bibr">2011</xref>) in order to produce a minimal computationally equivalent network (Lizier &amp; Rubinov, <xref rid="bib32" ref-type="bibr">2012</xref>).</p>
    <p>Information theory (Cover &amp; Thomas, <xref rid="bib12" ref-type="bibr">2005</xref>; Shannon, <xref rid="bib46" ref-type="bibr">1948</xref>) is well suited for the latter motivation of inferring networks that describe information flow as it provides model-free measures that can be applied at different scales and to different types of recordings. These measures, including conditional mutual information (Cover &amp; Thomas, <xref rid="bib12" ref-type="bibr">2005</xref>) and transfer entropy (Schreiber, <xref rid="bib44" ref-type="bibr">2000</xref>), are based purely on probability distributions and are able to identify nonlinear relationships (Paluš et al., <xref rid="bib36" ref-type="bibr">1993</xref>). Most importantly, information-theoretic measures allow the interpretation of the results from a distributed computation or information processing perspective, by modeling the information storage, transfer, and modification within the system (Lizier, <xref rid="bib26" ref-type="bibr">2013</xref>). Therefore, information theory simultaneously provides the tools for building the network model and the mathematical framework for its interpretation.</p>
    <p>The general approach to network model construction can be outlined as follows: for any <italic>target</italic> process (element) in the system, the inference algorithm selects the <italic>minimal set</italic> of processes that collectively contribute to the computation of the target’s next state. Every process can be separately studied as a target, and the results can be combined into a directed network describing the information flows in the system. This task presents several challenges:<list list-type="simple"><list-item><label>• </label><p>The state space of the possible network models grows faster than exponentially with respect to the size of the network;</p></list-item><list-item><label>• </label><p>Information-theoretic estimators suffer from the “curse of dimensionality” for large sets of variables (Paninski, <xref rid="bib37" ref-type="bibr">2003</xref>; Roulston, <xref rid="bib39" ref-type="bibr">1999</xref>);</p></list-item><list-item><label>• </label><p>In a network setting, statistical significance testing requires multiple comparisons. This results in a high false positive rate (type I errors) without adequate family-wise error rate controls (Dickhaus, <xref rid="bib13" ref-type="bibr">2014</xref>) or a high false negative rate (type II errors) with naive control procedures;</p></list-item><list-item><label>• </label><p>Nonparametric statistical testing based on shuffled surrogate time series is computationally demanding but currently necessary when using general information-theoretic estimators (Bossomaier et al., <xref rid="bib6" ref-type="bibr">2016</xref>; Lindner et al., <xref rid="bib25" ref-type="bibr">2011</xref>).</p></list-item></list></p>
    <p>Several previous studies (Faes et al., <xref rid="bib15" ref-type="bibr">2011</xref>; Lizier &amp; Rubinov, <xref rid="bib32" ref-type="bibr">2012</xref>; Sun et al., <xref rid="bib53" ref-type="bibr">2015</xref>; Vlachos &amp; Kugiumtzis, <xref rid="bib60" ref-type="bibr">2010</xref>) proposed greedy algorithms to tackle the first two challenges outlined above (see a summary by Bossomaier et al., <xref rid="bib6" ref-type="bibr">2016</xref>, sec 7.2). These algorithms mitigate the curse of dimensionality by greedily selecting the random variables that iteratively reduce the uncertainty about the present state of the target. The reduction of uncertainty is rigorously quantified by the information-theoretic measure of conditional mutual information (CMI), which can also be interpreted as a measure of conditional independence (Cover &amp; Thomas, <xref rid="bib12" ref-type="bibr">2005</xref>). In particular, these previous studies employed multivariate forms of the transfer entropy, that is, conditional and collective forms (Lizier et al., <xref rid="bib29" ref-type="bibr">2008</xref>, <xref rid="bib30" ref-type="bibr">2010</xref>). In general, such greedy optimization algorithms provide a locally optimal solution to the NP-hard problem of selecting the most informative set of random variables. An alternative optimization strategy—also based on conditional independence—employs a preliminary step to prune the set of sources (Runge et al., <xref rid="bib42" ref-type="bibr">2012</xref>, <xref rid="bib43" ref-type="bibr">2018</xref>). Despite this progress, the computational challenges posed by the estimation of multivariate transfer entropy have severely limited the size of problems investigated in previous validation studies in the general case of nonlinear estimators, for example, Montalto et al. (<xref rid="bib34" ref-type="bibr">2014</xref>) used 5 nodes and 512 samples; Kim et al. (<xref rid="bib22" ref-type="bibr">2016</xref>) used 6 nodes and 100 samples; Runge et al. (<xref rid="bib43" ref-type="bibr">2018</xref>) used 10 nodes and 500 samples. However, modern neural recordings often provide hundreds of nodes and tens of thousands of samples.</p>
    <p>These computational challenges, as well as the multiple testing challenges described above, are addressed here by the implementation of rigorous statistical tests, which represent the main theoretical contribution of this paper. These tests are used to control the family-wise error rate and are compatible with parallel processing, allowing the simultaneous analysis of the targets. This is a crucial feature, which enabled an improvement on the previous greedy algorithms. Exploiting the parallel computing capabilities of high-performance computing clusters and graphics processing units (GPUs) enabled the analysis of networks at a relevant scale for brain recordings—up to 100 nodes and 10,000 samples. Our algorithm has been implemented in the recently released <xref rid="def1" ref-type="def">IDTxl</xref> Python package (the “Information Dynamics Toolkit xl”; Wollstadt et al., <xref rid="bib62" ref-type="bibr">2019</xref>).</p>
    <p>We validated our method on synthetic datasets involving random structural networks of increasing size (also referred to as <italic>ground truth</italic>) and different types of dynamics (vector autoregressive processes and coupled logistic maps). In general, effective networks are able to reflect dynamic changes in the regime of the system and do not reflect an underlying structural network. Nonetheless, in the absence of hidden nodes (and other assumptions, including stationarity and the causal Markov condition), the inferred information network was proven to reflect the underlying structure for a sufficiently large sample size (Sun et al., <xref rid="bib53" ref-type="bibr">2015</xref>). Experiments under these conditions provide arguably the most important validation that the algorithm performs as expected, and here we perform the first large-scale empirical validation for non-Gaussian variables. As shown in the Results, the performance of our algorithm increased with the length of the time series, reaching consistently high precision, recall, and specificity ( &gt;98% on average) for 10,000 time samples. Varying the statistical significance threshold showed a more favorable precision-recall trade-off for longer time series.</p>
  </sec>
  <sec id="sec1">
    <title>METHODS</title>
    <sec>
      <title>Definitions and assumptions</title>
      <p>Let us consider a system of <italic>N</italic> discrete-time stochastic processes for which a finite number of samples have been recorded (over time and/or in different replications of the same experiment). In general, let us assume that the stochastic processes are stationary in each experimental time-window and <xref rid="def2" ref-type="def">Markovian with finite memory <italic>l</italic><sub>M</sub></xref>. Further assumptions will be made for the validation study. The following quantities are needed for the setup and formal treatment of the algorithm and are visualized in <xref ref-type="fig" rid="F1">Figure 1</xref> and <xref ref-type="fig" rid="F2">Figure 2</xref>:</p>
      <fig id="F1" orientation="portrait" position="float">
        <label><bold>Figure 1.</bold> </label>
        <caption>
          <p>Example of a possible definition of the candidate sets. The bottom row represents the time series of the target process <bold><italic>Y</italic></bold>, with the present state <italic>Y</italic><sub><italic>t</italic></sub> highlighted in green and the candidate target past set <inline-formula><mml:math id="m1"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> highlighted in red (up to a lag <italic>l</italic><sub>target</sub>). The remaining rows represent the time series of the source processes <bold><italic>X</italic></bold><sub><italic>i</italic></sub>, with the candidate sources past set <inline-formula><mml:math id="m2"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> highlighted in blue (up to a lag <italic>l</italic><sub>sources</sub>). For simplicity, only a single trial of the experiment is represented.</p>
        </caption>
        <graphic xlink:href="netn-03-827-g001"/>
      </fig>
      <fig id="F2" orientation="portrait" position="float">
        <label><bold>Figure 2.</bold> </label>
        <caption>
          <p>Example of a resulting nonuniform embedding of the time series relevant to <italic>Y</italic><sub><italic>t</italic></sub>. The bottom row represents the time series of the target process <bold><italic>Y</italic></bold>, with the present state <italic>Y</italic><sub><italic>t</italic></sub> highlighted in green and the selected target past set <inline-formula><mml:math id="m3"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> highlighted in red (as a subset of the candidate target past set shown in light red). The remaining rows represent the time series of the source processes <bold><italic>X</italic></bold><sub><italic>i</italic></sub>, with the selected sources past set <inline-formula><mml:math id="m4"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> highlighted in blue (as a subset of the candidate sources past set shown in light blue). The embedding only specifies the <italic>relative</italic> lags between the variables. For simplicity, only a single trial of the experiment is shown.</p>
        </caption>
        <graphic xlink:href="netn-03-827-g002"/>
      </fig>
      <p>
        <def-list>
          <def-item>
            <term><bold>Target process</bold><bold><italic>Y</italic></bold>:</term>
            <def>
              <p>a process of interest within the system (where <bold><italic>Y</italic></bold> = {<italic>Y</italic><sub><italic>t</italic></sub> ∣ <italic>t</italic> ∈ ℕ}); the choice of the target process is arbitrary and all the processes in the system can separately be studied as targets.</p>
            </def>
          </def-item>
          <def-item>
            <term><bold>Source processes</bold><bold><italic>X</italic></bold><sub><italic>i</italic></sub>:</term>
            <def>
              <p>the remaining processes within the system (where <italic>i</italic> = 1, …, <italic>N</italic> − 1 and <bold><italic>X</italic></bold><sub><italic>i</italic></sub> = {<italic>X</italic><sub><italic>i</italic>,<italic>t</italic></sub> ∣ <italic>t</italic> ∈ ℕ}).</p>
            </def>
          </def-item>
          <def-item>
            <term><bold>Sample number (or size)</bold><italic>T</italic>:</term>
            <def>
              <p>the number of samples recorded over time.</p>
            </def>
          </def-item>
          <def-item>
            <term><bold>Replication number</bold><italic>R</italic>:</term>
            <def>
              <p>the number of replications of the same experiment (e.g., trials).</p>
            </def>
          </def-item>
          <def-item>
            <term><bold>Target present state</bold><italic>Y</italic><sub><italic>t</italic></sub>:</term>
            <def>
              <p>the random variable (RV) representing the state of the target at time <italic>t</italic> (where <italic>t</italic> ≤ <italic>T</italic>), whose information contributors will be inferred.</p>
            </def>
          </def-item>
          <def-item>
            <term><bold>Candidate target past</bold><inline-formula><mml:math id="m5"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>:</term>
            <def>
              <p>an arbitrary finite set of RVs in the past of the target, up to a maximum lag <italic>l</italic><sub>target</sub>, i.e., <inline-formula><mml:math id="m6"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> = {<italic>Y</italic><sub><italic>t</italic>−1</sub>, …, <italic>Y</italic><sub><italic>t</italic>−<italic>l</italic><sub>target</sub></sub>}.</p>
            </def>
          </def-item>
          <def-item>
            <term><bold>Candidate sources past</bold><inline-formula><mml:math id="m7"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>:</term>
            <def>
              <p>an arbitrary finite set of RVs in the past of the sources, up to a maximum lag <italic>l</italic><sub>sources</sub>, i.e., <inline-formula><mml:math id="m8"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> = {<italic>X</italic><sub><italic>i</italic>,<italic>t</italic>−1</sub>, …, <italic>X</italic><sub><italic>i</italic>,<italic>t</italic>−<italic>l</italic><sub>sources</sub></sub> ∣ <italic>i</italic> = 1, …, <italic>N</italic> − 1}.</p>
            </def>
          </def-item>
          <def-item>
            <term><bold>Selected target past</bold><inline-formula><mml:math id="m9"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>:</term>
            <def>
              <p>the subset of RVs within the candidate target past set <inline-formula><mml:math id="m10"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> that maximally reduces the uncertainty about the present state of the target.</p>
            </def>
          </def-item>
          <def-item>
            <term><bold>Selected sources past</bold><inline-formula><mml:math id="m11"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>:</term>
            <def>
              <p>the subset of RVs within the candidate sources past set <inline-formula><mml:math id="m12"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> that maximally further reduces the uncertainty about the present state of the target, in the context of the selected target past (explained in detail in the following section).</p>
            </def>
          </def-item>
        </def-list>
      </p>
    </sec>
    <sec>
      <title>Inference Algorithm</title>
      <p>For a given target process <bold><italic>Y</italic></bold>, the goal of the algorithm is to infer the minimal set of information contributors to <italic>Y</italic><sub><italic>t</italic></sub>—defined as the selected sources past <inline-formula><mml:math id="m13"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>—in the context of the relevant information contributors from the candidate target past set, defined as the selected target past <inline-formula><mml:math id="m14"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>.</p>
      <p>The algorithm operates in four steps:<list list-type="simple"><list-item><label>1. </label><p>Select variables in the candidate target past set <inline-formula><mml:math id="m15"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> to obtain <inline-formula><mml:math id="m16"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>.</p></list-item><list-item><label>2. </label><p>Select variables in the candidate sources past set <inline-formula><mml:math id="m17"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> to obtain <inline-formula><mml:math id="m18"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>.</p></list-item><list-item><label>3. </label><p>Prune the selected sources past variables.</p></list-item><list-item><label>4. </label><p>Test relevant variables collectively for statistical significance.</p></list-item></list></p>
      <p>The operations performed in the four steps are described in detail hereafter; the result is a <xref rid="def3" ref-type="def"><italic>nonuniform embedding</italic></xref> of the target and sources time series (Faes et al., <xref rid="bib15" ref-type="bibr">2011</xref>; Takens, <xref rid="bib54" ref-type="bibr">1981</xref>; Vlachos and Kugiumtzis, <xref rid="bib60" ref-type="bibr">2010</xref>), as illustrated in <xref ref-type="fig" rid="F2">Figure 2</xref>.</p>
      <sec>
        <title>Step 1: Select variables in the candidate target past set.</title>
        <p>The goal of the first step is to find the subset of RVs within the candidate target past set <inline-formula><mml:math id="m19"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> that maximally reduces the uncertainty about the present state of the target while meeting statistical significance requirements. Let <inline-formula><mml:math id="m20"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> be the <italic>selected target past</italic> set found via optimization under these criteria.</p>
        <p>Finding the globally optimal embedding is an NP-hard problem and requires testing all the subsets of the candidate target past set. Since the number of subsets grow exponentially with the size of the candidate set, this is computationally unfeasible; therefore, a greedy approximation algorithm is employed to find a locally optimal solution in the search space of possible embeddings. This approach tackles the challenge of computational complexity by aiming at identifying a minimal conditioning set; in doing so, it also tackles the curse of dimensionality in the estimation of information-theoretic functionals.</p>
        <p>The set <inline-formula><mml:math id="m21"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is initialized as an empty set and it is iteratively built up via the following algorithm:<list list-type="simple"><list-item><label>a. </label><p>For each candidate variable <italic>C</italic> ∈ <inline-formula><mml:math id="m22"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, estimate the CMI contribution <italic>I</italic>(<italic>C</italic>; <italic>Y</italic><sub><italic>t</italic></sub>|<inline-formula><mml:math id="m23"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>);</p></list-item><list-item><label>b. </label><p>Find the candidate <italic>C</italic>* which maximizes the CMI contribution (reduction of uncertainty) and perform a statistical significance test against the null hypothesis of conditional independence, that is, that the new variable does not further reduce the uncertainty in the context of the previously included variables. If significant, add <italic>C</italic>* to <inline-formula><mml:math id="m24"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> and remove it from <inline-formula><mml:math id="m25"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>. The <italic>maximum statistic</italic> is employed to control the family-wise error rate (explained in detail in the <xref rid="sec2" ref-type="section"><italic>Statistical Tests</italic></xref> section);</p></list-item><list-item><label>c. </label><p>Repeat the previous steps until the maximum CMI contribution is not significant or <inline-formula><mml:math id="m26"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is empty.</p></list-item></list></p>
        <p>From a distributed, intrinsic computation perspective, the goal can be interpreted as finding the embedding of the target’s past states that maximizes the <xref rid="def4" ref-type="def"><italic>active information storage</italic></xref> (Lizier et al., <xref rid="bib31" ref-type="bibr">2012</xref>) to ensure self-prediction optimality as suggested by Wibral et al. (<xref rid="bib61" ref-type="bibr">2013</xref>). This approach is similar to the one proposed by Garland et al. (<xref rid="bib19" ref-type="bibr">2016</xref>) but uses nonuniform embedding and additional statistical controls.</p>
        <p>The nonuniform embedding of the time series was introduced by Vlachos and Kugiumtzis (<xref rid="bib60" ref-type="bibr">2010</xref>) and Faes et al. (<xref rid="bib15" ref-type="bibr">2011</xref>), who used an arbitrary threshold for the conditional mutual information. Lizier and Rubinov (<xref rid="bib32" ref-type="bibr">2012</xref>) introduced a statistical significance test to select the candidates, which this study builds on in proposing the maximum statistic. In addition, they embedded the target time series before embedding the sources, that is, the active information storage is modeled first and the information transfer is then examined in that context, thereby taking a specific modeling perspective on the information processing carried out by the system.</p>
      </sec>
      <sec>
        <title>Step 2: Select variables in the candidate sources past set.</title>
        <p>The goal of the second step is to find the subset of RVs within the candidate sources past set <inline-formula><mml:math id="m27"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> that maximally further reduces the uncertainty about the present state of the target, in the context of the selected target past found in the first step. Let <inline-formula><mml:math id="m28"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> be the <italic>selected sources past</italic> set found via optimization under these criteria.</p>
        <p>As for step 1, a greedy approximation algorithm is employed, and the statistical significance is tested throughout the selection process. <inline-formula><mml:math id="m29"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is initialized as an empty set and it is iteratively built up via the following algorithm:<list list-type="simple"><list-item><label>a. </label><p>For each candidate variable <italic>C</italic> ∈ <inline-formula><mml:math id="m30"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, estimate the conditional transfer entropy contribution <italic>I</italic>(<italic>C</italic>; <italic>Y</italic><sub><italic>t</italic></sub>|<inline-formula><mml:math id="m31"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="m32"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>) (Lizier et al., <xref rid="bib29" ref-type="bibr">2008</xref>, <xref rid="bib30" ref-type="bibr">2010</xref>; Vakorin et al., <xref rid="bib55" ref-type="bibr">2009</xref>; Verdes, <xref rid="bib58" ref-type="bibr">2005</xref>). When <inline-formula><mml:math id="m33"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is empty, this is simply a pairwise or bivariate transfer entropy (Schreiber, <xref rid="bib44" ref-type="bibr">2000</xref>); using the conditional form serves to prevent candidates carrying only redundant information (due to, e.g., common driver or pathway effects) from being selected, as well as to capture synergistic interactions between <italic>C</italic> and <inline-formula><mml:math id="m34"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>.</p></list-item><list-item><label>b. </label><p>Find the candidate <italic>C</italic>* which maximizes the conditional transfer entropy contribution (reduction of uncertainty) and perform a statistical significance test against the null hypothesis of conditional independence: if significant, add <italic>C</italic>* to <inline-formula><mml:math id="m35"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> and remove it from <inline-formula><mml:math id="m36"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>. The <italic>maximum statistic</italic> is employed to control the family-wise error rate.</p></list-item><list-item><label>c. </label><p>Repeat the previous steps until the maximum conditional transfer entropy contribution is not significant or <inline-formula><mml:math id="m37"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is empty.</p></list-item></list></p>
        <p>From a distributed computation perspective, the goal can be interpreted as finding the nonuniform embedding of the source processes’ past that maximizes the <italic>collective transfer entropy</italic> to the target, defined as <italic>I</italic>(<inline-formula><mml:math id="m38"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>; <italic>Y</italic><sub><italic>t</italic></sub>|<inline-formula><mml:math id="m39"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>) (Lizier et al., <xref rid="bib30" ref-type="bibr">2010</xref>). As above, the rationale for embedding the past of the sources as a second step is to achieve optimal separation of the storage and transfer contributions (Lizier &amp; Rubinov, <xref rid="bib32" ref-type="bibr">2012</xref>).</p>
      </sec>
      <sec>
        <title>Step 3: Prune the selected sources past variables.</title>
        <p>The third step of the algorithm is a pruning procedure performed to ensure that the variables included in the early iterations of the second step still provide a statistically significant information contribution in the context of the final selected sources past set <inline-formula><mml:math id="m40"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>. The pruning step involves the following operations:<list list-type="simple"><list-item><label>a. </label><p>For each variable <italic>C</italic> ∈ <inline-formula><mml:math id="m41"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, estimate the conditional mutual information contribution <italic>I</italic>(<italic>C</italic>; <italic>Y</italic><sub><italic>t</italic></sub>|<inline-formula><mml:math id="m42"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="m43"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> ∖ {<italic>C</italic>}), where the set difference operation is performed to exclude the variable <italic>C</italic> from the conditioning set;</p></list-item><list-item><label>b. </label><p>Find the variable <italic>C</italic>* which minimizes the CMI contribution and perform a statistical significance test: if <italic>not</italic> significant, remove <italic>C</italic> from <inline-formula><mml:math id="m44"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>. The <italic>minimum statistic</italic> is employed to test for significance against the null hypothesis of conditional independence while controlling the family-wise error rate;</p></list-item><list-item><label>c. </label><p>Repeat the previous steps until the minimum CMI contribution is not significant or <inline-formula><mml:math id="m45"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is empty.</p></list-item></list></p>
        <p>The pruning step was introduced by Lizier &amp; Rubinov (<xref rid="bib32" ref-type="bibr">2012</xref>); remarkably, Sun et al. (<xref rid="bib53" ref-type="bibr">2015</xref>) proved that this step is essential for the theoretical convergence of the inferred network to the causal network in the Granger-Wiener framework; they also rigorously laid out the mathematical assumptions needed for such convergence (see <xref rid="sec4" ref-type="section"><italic>Validation Tasks</italic></xref> section).</p>
      </sec>
      <sec>
        <title>Step 4: Test relevant variables collectively for statistical significance.</title>
        <p>The fourth and final step of the algorithm is the computation of the collective transfer entropy from the selected sources past set <inline-formula><mml:math id="m46"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> to the target and the performance of an <italic>omnibus test</italic> to ensure statistical significance against the null hypothesis of conditional independence. The resulting omnibus <italic>p</italic> value can further be used for correction of the family-wise error rate if the inference is carried out for multiple targets. The set <inline-formula><mml:math id="m47"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is only accepted as a result if all the statistical tests are passed. Importantly, the selected sources set <inline-formula><mml:math id="m48"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, inferred in the context of <inline-formula><mml:math id="m49"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, is the final result of the algorithm for a given target process <bold><italic>Y</italic></bold>. The order in which variables were inferred is not relevant.</p>
        <p>The statistical tests play a fundamental role in the inference and provide the stopping conditions for the iterations involved in the first and second steps of the algorithm. These stopping conditions are adaptive and change according to the amount of data available (the length of the time series). Given their importance, the statistical tests are described in detail in the following section.</p>
      </sec>
    </sec>
    <sec id="sec2">
      <title>Statistical Tests</title>
      <p>The crucial steps in the inference algorithm rely on determining whether the CMI is positive. However, due to the finite sample size, the CMI estimators may produce nonzero estimates in the case of zero CMI, and it may even return negative estimates if the estimator bias is larger than the true CMI (Kraskov et al., <xref rid="bib23" ref-type="bibr">2004</xref>; Roulston, <xref rid="bib39" ref-type="bibr">1999</xref>). For this reason, statistical tests are required to assess the significance of the CMI estimates against the null hypothesis of no CMI (i.e., conditional independence) (Chávez et al., <xref rid="bib9" ref-type="bibr">2003</xref>; Lindner et al., <xref rid="bib25" ref-type="bibr">2011</xref>; Lizier et al., <xref rid="bib28" ref-type="bibr">2011</xref>; Vicente et al., <xref rid="bib59" ref-type="bibr">2011</xref>).</p>
      <p>For certain estimators, analytic solutions exist for the finite-sample distribution under this null hypothesis (see Lizier, <xref rid="bib27" ref-type="bibr">2014</xref>); in the absence of an analytic solution, the null distributions are computed in a nonparametric way by using surrogate time series (Schreiber &amp; Schmitz, <xref rid="bib45" ref-type="bibr">2000</xref>). The surrogates are generated to satisfy the null hypothesis by destroying the temporal relationship between the source and the target while preserving the temporal dependencies within the sources.</p>
      <p>Finally, the inference algorithm is based on multiple comparisons and requires an appropriate calibration of the statistical tests to achieve the desired family-wise error rate (i.e., the probability of making one or more false discoveries, or <italic>type I errors</italic>, when performing multiple hypotheses tests). The maximum statistic and minimum statistic tests employed in this study were specifically conceived to tackle these challenges.</p>
      <sec id="sec3">
        <title>Maximum statistic test.</title>
        <p>The maximum statistic test is a <xref rid="def5" ref-type="def">step-down statistical test</xref> used to control the family-wise error rate when selecting the past variables for the target and source embeddings, which involves multiple comparisons.</p>
        <p>Let us first consider the first step of the main algorithm and assume that we have picked the single candidate variable <italic>C</italic>* (from the candidate target past set <inline-formula><mml:math id="m50"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>), which maximizes the CMI contribution. The maximum statistic test mirrors this selection process by picking the maximum value among the surrogates. Specifically, let <italic>I</italic>* := <italic>I</italic>(<italic>C</italic>*; <italic>Y</italic><sub><italic>t</italic></sub>|<inline-formula><mml:math id="m51"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>) be the maximum contribution (i.e., the maximum statistic); the following algorithm is used to test <italic>I</italic>* for statistical significance:<list list-type="simple"><list-item><label>1. </label><p>For each <italic>C</italic><sub><italic>j</italic></sub> ∈ <inline-formula><mml:math id="m52"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, generate <italic>S</italic> surrogates time series <inline-formula><mml:math id="m53"><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>, …, <inline-formula><mml:math id="m54"><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> and compute the corresponding surrogate CMI values <inline-formula><mml:math id="m55"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> = <italic>I</italic>(<inline-formula><mml:math id="m56"><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>; <italic>Y</italic><sub><italic>t</italic></sub>|<inline-formula><mml:math id="m57"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>), …, <inline-formula><mml:math id="m58"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> = <italic>I</italic>(<inline-formula><mml:math id="m59"><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>; <italic>Y</italic><sub><italic>t</italic></sub>|<inline-formula><mml:math id="m60"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>). More details about the surrogate generation are provided at the end of this section. The number of surrogates <italic>S</italic> must be chosen according to the desired significance level <italic>α</italic><sub>max</sub>, i.e., such that <italic>S</italic> &gt; 1/<italic>α</italic><sub>max</sub>.</p></list-item><list-item><label>2. </label><p>Compute the maximum CMI value over candidates <inline-formula><mml:math id="m61"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> := <italic>max</italic>(<inline-formula><mml:math id="m62"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>, …, <inline-formula><mml:math id="m63"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>) for each surrogate <italic>s</italic> = 1, …, <italic>S</italic>. Here, <italic>n</italic> denotes the number of candidates and hence the number of comparisons. The obtained values <inline-formula><mml:math id="m64"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>, …, <inline-formula><mml:math id="m65"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> provide the (empirical) null distribution of the maximum statistic (see <xref rid="T1" ref-type="table">Table 1</xref>).</p></list-item><list-item><label>3. </label><p>Calculate the <italic>p</italic> value for <italic>I</italic>* as the fraction of surrogate maximum statistic values that are larger than <italic>I</italic>*.</p></list-item><list-item><label>4. </label><p><italic>I</italic>* is deemed <italic>significant</italic> if the <italic>p</italic> value is smaller than <italic>α</italic><sub>max</sub> (i.e., the null hypothesis of conditional independence for the candidate variable with the maximum CMI contribution is rejected at level <italic>α</italic><sub>max</sub>).</p></list-item></list></p>
        <table-wrap id="T1" orientation="portrait" position="float">
          <label><bold>Table 1.</bold> </label>
          <caption>
            <p>Computing the null distribution of the maximum statistic. The null distribution is empirically described by the values <inline-formula><mml:math id="m66"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>, …, <inline-formula><mml:math id="m67"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>, obtained as <inline-formula><mml:math id="m68"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> := <italic>max</italic>(<inline-formula><mml:math id="m69"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>, …, <inline-formula><mml:math id="m70"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>), for each surrogate <italic>s</italic> = 1, …, <italic>S</italic>. Here, <italic>n</italic> denotes the number of candidates and hence the number of comparisons. The null distribution is used to test the significance of <italic>I</italic>* against the null hypothesis of zero CMI.</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr valign="bottom">
                <th rowspan="2" colspan="1"> </th>
                <th align="center" rowspan="2" colspan="1">Variable <italic>C</italic><sub><italic>j</italic></sub> ∈ <inline-formula><mml:math id="m71"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula></th>
                <th align="center" rowspan="2" colspan="1">CMI <italic>I</italic><sub><italic>j</italic></sub> = <italic>I</italic>(<italic>C</italic><sub><italic>j</italic></sub>; <italic>Y</italic><sub><italic>t</italic></sub>|<inline-formula><mml:math id="m72"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>)</th>
                <th align="center" colspan="4" rowspan="1">Surrogate variables</th>
                <th align="center" colspan="4" rowspan="1">Surrogate CMI</th>
              </tr>
              <tr valign="bottom">
                <th align="center" rowspan="1" colspan="1">1</th>
                <th align="center" rowspan="1" colspan="1">2</th>
                <th align="center" rowspan="1" colspan="1">⋯</th>
                <th align="center" rowspan="1" colspan="1">
                  <italic>S</italic>
                </th>
                <th align="center" rowspan="1" colspan="1">1</th>
                <th align="center" rowspan="1" colspan="1">2</th>
                <th align="center" rowspan="1" colspan="1">⋯</th>
                <th align="center" rowspan="1" colspan="1">
                  <italic>S</italic>
                </th>
              </tr>
            </thead>
            <tbody>
              <tr valign="top">
                <td rowspan="1" colspan="1"> </td>
                <td align="center" rowspan="1" colspan="1">
                  <italic>C</italic>
                  <sub>1</sub>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <italic>I</italic>
                  <sub>1</sub>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <inline-formula>
                    <mml:math id="m73">
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>C</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>1</mml:mn>
                          <mml:mo>,</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>′</mml:mo>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:math>
                  </inline-formula>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <inline-formula>
                    <mml:math id="m74">
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>C</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>1</mml:mn>
                          <mml:mo>,</mml:mo>
                          <mml:mn>2</mml:mn>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>′</mml:mo>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:math>
                  </inline-formula>
                </td>
                <td align="center" rowspan="1" colspan="1">⋯</td>
                <td align="center" rowspan="1" colspan="1">
                  <inline-formula>
                    <mml:math id="m75">
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>C</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>1</mml:mn>
                          <mml:mo>,</mml:mo>
                          <mml:mi>S</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>′</mml:mo>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:math>
                  </inline-formula>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <inline-formula>
                    <mml:math id="m76">
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>I</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>1</mml:mn>
                          <mml:mo>,</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>′</mml:mo>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:math>
                  </inline-formula>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <inline-formula>
                    <mml:math id="m77">
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>I</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>1</mml:mn>
                          <mml:mo>,</mml:mo>
                          <mml:mn>2</mml:mn>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>′</mml:mo>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:math>
                  </inline-formula>
                </td>
                <td align="center" rowspan="1" colspan="1">⋯</td>
                <td align="center" rowspan="1" colspan="1">
                  <inline-formula>
                    <mml:math id="m78">
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>I</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>1</mml:mn>
                          <mml:mo>,</mml:mo>
                          <mml:mi>S</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>′</mml:mo>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:math>
                  </inline-formula>
                </td>
              </tr>
              <tr valign="top">
                <td rowspan="1" colspan="1"> </td>
                <td align="center" rowspan="1" colspan="1">
                  <italic>C</italic>
                  <sub>2</sub>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <italic>I</italic>
                  <sub>2</sub>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <inline-formula>
                    <mml:math id="m79">
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>C</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>2</mml:mn>
                          <mml:mo>,</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>′</mml:mo>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:math>
                  </inline-formula>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <inline-formula>
                    <mml:math id="m80">
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>C</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>2</mml:mn>
                          <mml:mo>,</mml:mo>
                          <mml:mn>2</mml:mn>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>′</mml:mo>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:math>
                  </inline-formula>
                </td>
                <td align="center" rowspan="1" colspan="1">⋯</td>
                <td align="center" rowspan="1" colspan="1">
                  <inline-formula>
                    <mml:math id="m81">
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>C</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>2</mml:mn>
                          <mml:mo>,</mml:mo>
                          <mml:mi>S</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>′</mml:mo>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:math>
                  </inline-formula>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <inline-formula>
                    <mml:math id="m82">
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>I</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>2</mml:mn>
                          <mml:mo>,</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>′</mml:mo>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:math>
                  </inline-formula>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <inline-formula>
                    <mml:math id="m83">
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>I</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>2</mml:mn>
                          <mml:mo>,</mml:mo>
                          <mml:mn>2</mml:mn>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>′</mml:mo>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:math>
                  </inline-formula>
                </td>
                <td align="center" rowspan="1" colspan="1">⋯</td>
                <td align="center" rowspan="1" colspan="1">
                  <inline-formula>
                    <mml:math id="m84">
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>I</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>2</mml:mn>
                          <mml:mo>,</mml:mo>
                          <mml:mi>S</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>′</mml:mo>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:math>
                  </inline-formula>
                </td>
              </tr>
              <tr valign="top">
                <td rowspan="1" colspan="1"> </td>
                <td align="center" rowspan="1" colspan="1">⋮</td>
                <td align="center" rowspan="1" colspan="1">⋮</td>
                <td align="center" rowspan="1" colspan="1">⋮</td>
                <td align="center" rowspan="1" colspan="1">⋮</td>
                <td rowspan="1" colspan="1"> </td>
                <td align="center" rowspan="1" colspan="1">⋮</td>
                <td align="center" rowspan="1" colspan="1">⋮</td>
                <td align="center" rowspan="1" colspan="1">⋮</td>
                <td rowspan="1" colspan="1"> </td>
                <td align="center" rowspan="1" colspan="1">⋮</td>
              </tr>
              <tr valign="top">
                <td rowspan="1" colspan="1"> </td>
                <td align="center" rowspan="1" colspan="1">
                  <italic>C</italic>
                  <sub>
                    <italic>n</italic>
                  </sub>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <italic>I</italic>
                  <sub>
                    <italic>n</italic>
                  </sub>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <inline-formula>
                    <mml:math id="m85">
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>C</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>n</mml:mi>
                          <mml:mo>,</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>′</mml:mo>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:math>
                  </inline-formula>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <inline-formula>
                    <mml:math id="m86">
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>C</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>n</mml:mi>
                          <mml:mo>,</mml:mo>
                          <mml:mn>2</mml:mn>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>′</mml:mo>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:math>
                  </inline-formula>
                </td>
                <td align="center" rowspan="1" colspan="1">⋯</td>
                <td align="center" rowspan="1" colspan="1">
                  <inline-formula>
                    <mml:math id="m87">
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>C</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>n</mml:mi>
                          <mml:mo>,</mml:mo>
                          <mml:mi>S</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>′</mml:mo>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:math>
                  </inline-formula>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <inline-formula>
                    <mml:math id="m88">
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>I</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>n</mml:mi>
                          <mml:mo>,</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>′</mml:mo>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:math>
                  </inline-formula>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <inline-formula>
                    <mml:math id="m89">
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>I</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>n</mml:mi>
                          <mml:mo>,</mml:mo>
                          <mml:mn>2</mml:mn>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>′</mml:mo>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:math>
                  </inline-formula>
                </td>
                <td align="center" rowspan="1" colspan="1">⋯</td>
                <td align="center" rowspan="1" colspan="1">
                  <inline-formula>
                    <mml:math id="m90">
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>I</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>n</mml:mi>
                          <mml:mo>,</mml:mo>
                          <mml:mi>S</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>′</mml:mo>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:math>
                  </inline-formula>
                </td>
              </tr>
              <tr valign="bottom">
                <th align="left" rowspan="1" colspan="1">max CMI</th>
                <th rowspan="1" colspan="1"> </th>
                <th align="center" rowspan="1" colspan="1"><italic>I</italic>*</th>
                <th rowspan="1" colspan="1"> </th>
                <th rowspan="1" colspan="1"> </th>
                <th rowspan="1" colspan="1"> </th>
                <th rowspan="1" colspan="1"> </th>
                <th align="center" rowspan="1" colspan="1">
                  <inline-formula>
                    <mml:math id="m91">
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>I</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>*</mml:mo>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:math>
                  </inline-formula>
                </th>
                <th align="center" rowspan="1" colspan="1">
                  <inline-formula>
                    <mml:math id="m92">
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>I</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>2</mml:mn>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>*</mml:mo>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:math>
                  </inline-formula>
                </th>
                <th align="center" rowspan="1" colspan="1">⋯</th>
                <th align="center" rowspan="1" colspan="1">
                  <inline-formula>
                    <mml:math id="m93">
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>I</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>S</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>*</mml:mo>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:math>
                  </inline-formula>
                </th>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <p>The variables and quantities used in the above algorithm are presented in <xref rid="T1" ref-type="table">Table 1</xref>. The key goal in the surrogate generation is to preserve the temporal order of samples in the target time series <italic>Y</italic><sub><italic>t</italic></sub> (which is not shuffled) and preserve the distribution of the sources <italic>C</italic><sub><italic>j</italic></sub> while destroying any potential relationships between the sources and the target (Vicente et al., <xref rid="bib59" ref-type="bibr">2011</xref>). This can be achieved in multiple ways. If multiple replications (e.g., trials) are available, surrogate data is generated by shuffling the order of replications for the candidate <italic>C</italic><sub><italic>j</italic></sub> while keeping the order of replications for the remaining variables intact. When the number of replications is not sufficient to guarantee enough permutations, the embedded source samples within individual trials are shuffled instead (see Chávez et al., <xref rid="bib9" ref-type="bibr">2003</xref>; Lizier et al., <xref rid="bib28" ref-type="bibr">2011</xref>; Verdes, <xref rid="bib58" ref-type="bibr">2005</xref>; Vicente et al., <xref rid="bib59" ref-type="bibr">2011</xref>; and the summary by Lizier, <xref rid="bib27" ref-type="bibr">2014</xref>, Appendix A.5). Note that the generation of surrogates (steps 1-3) can be avoided when the null distributions can be derived analytically, for example, with Gaussian estimators (Barnett &amp; Bossomaier, <xref rid="bib4" ref-type="bibr">2012</xref>).</p>
        <p>The same test is performed during the selection of the variables in the candidate sources past set (step 2 of the main algorithm), with the only difference that <italic>C</italic><sub><italic>j</italic></sub> ∈ <inline-formula><mml:math id="m94"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> and that <inline-formula><mml:math id="m95"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is added to the conditioning set, that is, <inline-formula><mml:math id="m96"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> = <italic>I</italic>(<inline-formula><mml:math id="m97"><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>; <italic>Y</italic><sub><italic>t</italic></sub>|<inline-formula><mml:math id="m98"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="m99"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>) for each surrogate <italic>s</italic> = 1, …, <italic>S</italic>.</p>
      </sec>
      <sec>
        <title>Family-wise error rate correction.</title>
        <p>How does the maximum statistic test control the family-wise error rate? Intuitively, one or more statistics will exceed a given threshold if and only if the maximum exceeds it. This relationship can be used to obtain an adjusted threshold from the distribution of the maximum statistic under the null hypothesis, which can be used to control the family-wise error rate both in the weak and strong sense (Nichols &amp; Hayasaka, <xref rid="bib35" ref-type="bibr">2003</xref>).</p>
        <p>Let us quantify the false positive rate <italic>v</italic><sub>FPR</sub> for a single variable when the maximum statistic at the significance level <italic>α</italic><sub>max</sub> is employed. For simplicity, the derivation is performed under the hypothesis that the information contributors to the target have been selected in the first iterations of the greedy algorithm and removed from the candidate sources past set <inline-formula><mml:math id="m100"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>. Under this hypothesis, the target is conditionally independent of the remaining <italic>n</italic> variables in <inline-formula><mml:math id="m101"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> given the selected source and target variables. Let <italic>I</italic><sub>1</sub>, …, <italic>I</italic><sub><italic>n</italic></sub> be the corresponding CMI estimates and let <italic>I</italic><sub>max</sub> := max(<italic>I</italic><sub>1</sub>, …, <italic>I</italic><sub><italic>n</italic></sub>) be the maximum statistic. As discussed above, the estimates might be positive even under the conditional independence hypothesis, due to finite-sample effects. Since the estimates are independently obtained from shuffled time series, they are treated as i.i.d. RVs.</p>
        <p>Let <italic>i</italic><sub>threshold</sub> be the critical threshold corresponding to the given significance value <italic>α</italic><sub>max</sub>, that is, <italic>i</italic><sub>threshold</sub> := sup{<italic>x</italic> ∈ ℝ|<italic>P</italic>(<italic>I</italic><sub>max</sub> ≥ <italic>x</italic>) = <italic>α</italic><sub>max</sub>}. Then<disp-formula id="E1"><mml:math id="m102"><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi>P</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mtext>threshold</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mtext>threshold</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mtext>threshold</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mrow><mml:mo>∏</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:mi>P</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mtext>threshold</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:msup><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mtext>threshold</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mtext>FPR</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:math><label>(1)</label></disp-formula>Therefore,<disp-formula id="E2"><mml:math id="m103"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mtext>FPR</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math><label>(2)</label></disp-formula></p>
        <p>Interestingly, <xref rid="E2" ref-type="disp-formula">Equation 2</xref> shows that the maximum statistic correction is equivalent to the Dunn-Šidák correction (Šidák, <xref rid="bib47" ref-type="bibr">1967</xref>). Performing a Taylor expansion of <xref rid="E2" ref-type="disp-formula">Equation 2</xref> around <italic>α</italic><sub>max</sub> = 0 yields:<disp-formula id="E3"><mml:math id="m104"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mtext>FPR</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>∞</mml:mo></mml:mrow></mml:munderover></mml:mstyle><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mrow><mml:mo>∏</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>!</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:msup><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mtext>target</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:math><label>(3)</label></disp-formula>Truncating the Taylor series at <italic>j</italic> = 1 yields the first-order approximation<disp-formula id="E4"><mml:math id="m105"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mtext>FPR</mml:mtext></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:math><label>(4)</label></disp-formula>which coincides with the false positive rate resulting from the Bonferroni correction (Dickhaus, <xref rid="bib13" ref-type="bibr">2014</xref>). Moreover, since the summands in <xref rid="E3" ref-type="disp-formula">Equation 3</xref> are positive for every <italic>j</italic>, the Taylor series is lower bounded by any truncated series. In particular, the false positive rate resulting from the Bonferroni correction is a lower bound for the <italic>v</italic><sub>FPR</sub> (the false positive rate for a single variable resulting from the maximum statistic test), that is, the maximum statistic correction is less stringent than the Bonferroni correction.</p>
        <p>Let us now study the effect of the maximum statistic test on the family-wise error rate <italic>t</italic><sub>FPR</sub> for a single target while accounting for all the iterations performed during the step-down test, (i.e., <italic>t</italic><sub>FPR</sub> is the probability that at least one of the selected sources is a false positive). We have:<disp-formula id="E5"><mml:math id="m106"><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mtext>FPR</mml:mtext></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mstyle displaystyle="true"><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>“the source selected on step j is false positive”</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mstyle displaystyle="true"><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:msubsup><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math><label>(5)</label></disp-formula>Therefore,<disp-formula id="E6"><mml:math id="m107"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mtext>FPR</mml:mtext></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msub></mml:math><label>(6)</label></disp-formula>for the typical small values of <italic>α</italic><sub>max</sub> used in statistical testing (even in the limit of large <italic>n</italic>), which shows that <italic>α</italic><sub>max</sub> effectively controls the family-wise error rate for a single target.</p>
      </sec>
      <sec>
        <title>Minimum statistic test.</title>
        <p>The minimum statistic test is employed during the third main step of the algorithm (pruning step) to remove the selected variables that have become redundant in the context of the final set of selected source past variables <inline-formula><mml:math id="m108"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, while controlling the family-wise error rate. This is necessary because of the multiple comparisons involved in the pruning procedure. The minimum statistic test works identically to the maximum statistic test (replacing “maximum” with “minimum” in the algorithm presented above).</p>
      </sec>
      <sec>
        <title>Omnibus test.</title>
        <p>Let <italic>T</italic>* := <italic>I</italic>(<inline-formula><mml:math id="m109"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>; <italic>Y</italic><sub><italic>t</italic></sub>|<inline-formula><mml:math id="m110"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>) be the collective transfer entropy from all the selected sources past variables <inline-formula><mml:math id="m111"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> to the target <bold><italic>Y</italic></bold>. The value <italic>T</italic>* is tested for statistical significance against the null hypothesis of zero transfer entropy (this test is referred to as the omnibus test). The null distribution is built using surrogates time series obtained via shuffling of the realizations of the selected sources (see Chávez et al. (<xref rid="bib9" ref-type="bibr">2003</xref>); Lizier et al. (<xref rid="bib28" ref-type="bibr">2011</xref>); Verdes (<xref rid="bib58" ref-type="bibr">2005</xref>); Vicente et al. (<xref rid="bib59" ref-type="bibr">2011</xref>) and the summary by Lizier (<xref rid="bib27" ref-type="bibr">2014</xref>, Appendix A.5)), i.e., using a similar procedure to the one described in the <xref rid="sec3" ref-type="section"><italic>Maximum statistic test</italic></xref> section above. Testing all the selected sources collectively is in line with the perspective that the goal of the network inference is to find the <italic>set</italic> of relevant sources for each node.</p>
      </sec>
      <sec>
        <title>Combining across multiple targets.</title>
        <p>When the inference is performed on multiple targets, the omnibus <italic>p</italic> values can be employed in further statistical tests to control the family-wise error rate for the overall network (e.g., via FDR-correction; Benjamini &amp; Hochberg, <xref rid="bib5" ref-type="bibr">1995</xref>; Dickhaus, <xref rid="bib13" ref-type="bibr">2014</xref>; which is implemented in the IDTxl toolbox).</p>
        <p>It is important to fully understand the statistical questions and validation procedure implied by this approach. Combining the results across multiple targets by reusing the omnibus test <italic>p</italic> values for the FDR-correction yields a <italic>hierarchical</italic> test. The test answers two nested questions: (1) “<italic>which nodes receive any significant overall information transfer?</italic>” and, if any, (2) “<italic>what is the structure of the incoming information transfer to each node?</italic>.” However, the answers are computed in the reverse order, for the following reason: it would be computationally unfeasible to directly compute the collective transfer entropy from all candidate sources to the target right at the beginning of the network inference process. At this point, the candidate source set usually contains a large number of variables so that estimation will likely fall prey to the curse of dimensionality. Instead, a conservative <italic>approximation</italic> of the collective information transfer is obtained by considering only a subset of the potential sources, that is, those deemed significant by the maximum and minimum statistic tests described in the previous sections. Only if this approximation of the total information transfer is also deemed significant by the omnibus test (as well as by the FDR test at the network level), then the subset of significant sources for that target is interpreted post hoc as the local structure of the incoming information transfer. This way, the testing procedure exhibits a hierarchical structure: the omnibus test operates at the higher (global) level concerned with the collective information transfer, whereas the minimum and maximum tests operate at the lower (local) level of individual source-target connections.</p>
        <p>Compared with a nonhierarchical analysis with a correction for multiple comparisons across all links (e.g., by network-wide Bonferroni correction or the use of the maximum statistic across all potential links), the above strategy buys both statistical sensitivity (“recall”) and the possibility to trivially parallelize computations across targets. The price to be paid is that a link with a relatively strong information transfer into a node with nonsignificant overall incoming information transfer may get pruned, while a link with relatively weaker information transfer into a node with significant overall incoming information transfer will prevail. This behavior clearly differs from a correction for multiple comparisons across all links. Arguably, this difference is irrelevant in many practical cases, although it could become noticeable for networks with high average in-degree and relatively uniform information transfer across the links. The difference can be reduced by setting a conservative critical threshold for the lower-level greedy analysis.</p>
      </sec>
    </sec>
    <sec id="sec4">
      <title>Validation Tasks</title>
      <p>For the purpose of the validation study, the additional assumptions of <xref rid="def6" ref-type="def"><italic>causal sufficiency</italic></xref> and the <xref rid="def7" ref-type="def"><italic>causal Markov condition</italic></xref> were made, such that the inferred network was expected to closely reflect the structural network for a sufficiently large sample size (Sun et al., <xref rid="bib53" ref-type="bibr">2015</xref>). Although this is not always the case, experiments under these conditions allow the evaluation of the performance of the algorithm with respect to an expected ground truth. An intuitive definition of these conditions is provided here, while the technical details are discussed at length in Spirtes et al. (<xref rid="bib50" ref-type="bibr">1993</xref>). Moreover, the intrinsic stochastic nature of the processes makes purely synergistic and purely redundant interactions unlikely (and indeed vanishing for large sample size), thus satisfying the <italic>faithfulness</italic> condition (Spirtes et al., <xref rid="bib50" ref-type="bibr">1993</xref>).</p>
      <p>The complete network inference algorithm implemented in the IDTxl toolkit (release v1.0) was validated on multiple synthetic datasets, where both the structural connectivity and the dynamics were known. Given the general scope of the toolkit, two dynamical models of broad applicability were chosen: a vector autoregressive process (VAR) and a coupled logistic maps process (CLM); both models are widely used in computational neuroscience (Rubinov et al., <xref rid="bib40" ref-type="bibr">2009</xref>; Valdes-Sosa et al., <xref rid="bib56" ref-type="bibr">2011</xref>; Zalesky et al., <xref rid="bib64" ref-type="bibr">2014</xref>), macroeconomics (Lorenz, <xref rid="bib33" ref-type="bibr">1993</xref>; Sims, <xref rid="bib48" ref-type="bibr">1980</xref>), and chaos theory (Strogatz, <xref rid="bib52" ref-type="bibr">2015</xref>).</p>
      <p>The primary goal was to quantify the scaling of the performance with respect to the size of the network and the length of the time series. Sparse directed random Erdős-Rényi networks (Erdős &amp; Rényi, <xref rid="bib14" ref-type="bibr">1959</xref>) of increasing size (<italic>N</italic> = 10 to 100 nodes) were generated with a link probability <italic>p</italic> = 3/<italic>N</italic> to obtain an expected in-degree of 3 links. Both the VAR and the CLM stochastic processes were repeatedly simulated on each causal network with increasingly longer time series (<italic>T</italic> = 100 to 10000 samples), a single replication (or trial, i.e., <italic>R</italic> = 1), and with 10 random initial conditions. The performance was evaluated in terms of precision, recall, and specificity in the classification of the links. Further simulations were carried out to investigate the influence of the critical alpha level for statistical significance and the performance of different estimators of conditional mutual information.</p>
      <sec>
        <title>Vector autoregressive process.</title>
        <p>The specific VAR process used in this study is described by the following discrete-time recurrence relation:<disp-formula id="E7"><mml:math id="m112"><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder></mml:mstyle><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><label>(7)</label></disp-formula>where <bold><italic>X</italic></bold><sub><bold><italic>Y</italic></bold></sub> denotes the set of causal sources of the target process <bold><italic>Y</italic></bold> and a single random lag <italic>l</italic><sub><bold><italic>X</italic></bold></sub> ∈ {1, 2, 3, 4, 5} was used for each source <bold><italic>X</italic></bold> ∈ <bold><italic>X</italic></bold><sub><bold><italic>Y</italic></bold></sub>. A Gaussian noise term <italic>η</italic><sub><italic>t</italic></sub> with mean <italic>μ</italic> = 0 and standard deviation <italic>θ</italic> = 0.1 was added at each time step <italic>t</italic>; the noise terms added to different variables were uncorrelated. The self-coupling coefficient was set to <italic>β</italic> = 0.5 and the cross-coupling coefficients <italic>α</italic><sub><bold><italic>X</italic></bold></sub> were uniform and normalized for each target such that ∑<sub><bold><italic>X</italic></bold>∈<bold><italic>X</italic></bold><sub><bold><italic>Y</italic></bold></sub></sub>
<italic>α</italic><sub><bold><italic>X</italic></bold></sub> = 0.4. This choice of parameters guaranteed that the VAR processes were stable (the resulting spectral radii were between 0.9 and 0.95) and had stationary multivariate Gaussian distributions (Atay &amp; Karabacak, <xref rid="bib1" ref-type="bibr">2006</xref>). As such, the Gaussian estimator implemented in IDTxl was employed for transfer entropy measurements in VAR processes. Note that transfer entropy and Granger causality (Granger, <xref rid="bib21" ref-type="bibr">1969</xref>) are equivalent for Gaussian variables (Barnett et al., <xref rid="bib3" ref-type="bibr">2009</xref>); therefore, using the Gaussian estimator with our algorithm can be viewed as extending Granger causality in the same multivariate/greedy fashion.</p>
      </sec>
      <sec>
        <title>Coupled logistic maps process.</title>
        <p>The coupled logistic maps process used in this study is described by the following discrete-time recurrence relations:<disp-formula id="E8"><mml:math id="m113"><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi>β</mml:mi><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder></mml:mstyle><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mspace width="0.1em"/><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:math><label>(8)</label></disp-formula>At each time step <italic>t</italic>, each node <bold><italic>Y</italic></bold> computes the weighted input <italic>a</italic><sub><italic>t</italic></sub> as a linear combination of its past value and the past of its sources, with the same conditions used for the VAR process on the choice of the random lags <italic>l</italic><sub><bold><italic>X</italic></bold></sub> and coupling coefficients <italic>β</italic> and <italic>α</italic><sub><bold><italic>X</italic></bold></sub>. The value <italic>Y</italic><sub><italic>t</italic></sub> is then computed by applying the logistic map activation function <italic>f</italic>(<italic>x</italic>) = 4<italic>x</italic>(1 − <italic>x</italic>) to the weighted input <italic>a</italic><sub><italic>t</italic></sub> and adding the Gaussian noise <italic>η</italic><sub><italic>t</italic></sub> with the same properties used for the VAR process. Notice that the coefficient (<italic>r</italic> = 4) used in the logistic map function corresponds to the fully developed chaotic regime. The modulo-1 operation ensures that <italic>Y</italic><sub><italic>t</italic></sub> ∈ [0, 1] after the addition of noise. The <italic>nearest-neighbor</italic> estimators were employed for transfer entropy measurements in the analysis of the CLM processes (in particular, Kraskov’s estimator <italic>I</italic><sup>(1)</sup> with <italic>k</italic> = 4 nearest neighbors (Kraskov et al., <xref rid="bib23" ref-type="bibr">2004</xref>) and its extension to CMI Frenzel &amp; Pompe, <xref rid="bib16" ref-type="bibr">2007</xref>; Gómez-Herrero et al., <xref rid="bib20" ref-type="bibr">2015</xref>; Vejmelka &amp; Paluš, <xref rid="bib57" ref-type="bibr">2008</xref>). Nearest-neighbor estimators are model-free and are able to detect nonlinear dependencies in stochastic processes with non-Gaussian stationary distributions; fast CPU and GPU implementations are provided by the IDTxl package.</p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>RESULTS</title>
    <sec>
      <title>Influence of Network Size and Length of the Time Series</title>
      <p>The aim of the first analysis was to quantify the scaling of the performance with respect to the size of the network and the length of the time series.</p>
      <p>The inferred network was built by adding a directed link from a source node <bold><italic>X</italic></bold> to a target node <bold><italic>Y</italic></bold> whenever a significant transfer entropy from <bold><italic>X</italic></bold> to <bold><italic>Y</italic></bold> was measured while building the selected sources past set <inline-formula><mml:math id="m114"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> (i.e., whenever <bold><italic>X</italic></bold> ∩ <inline-formula><mml:math id="m115"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> ≠ ∅). The critical alpha level for statistical significance was set to <italic>α</italic><sub>max</sub> = 0.001 and <italic>S</italic> = 1000 surrogates were used for all experiments unless otherwise stated. The candidate sets for the target as well as the sources were initialized with a maximum lag of five (i.e., <italic>l</italic><sub>target</sub> = <italic>l</italic><sub>sources</sub> = 5, corresponding to the largest lag values used in the definition of the VAR and CLM processes).</p>
      <p>The network inference performance was evaluated in comparison to the known underlying structural network as a binary classification task, using standard statistics based on the number of <italic>true positives</italic> (TP, i.e., correctly classified existing links), <italic>false positives</italic> (FP, i.e., absent links falsely classified as existing), <italic>true negatives</italic> (TN, i.e., correctly classified absent links), and <italic>false negatives</italic> (FN, i.e., existing links falsely classified as absent). The following standard statistics were employed in the evaluation:<list list-type="simple"><list-item><p><bold>precision</bold> = <italic>TP</italic>/(<italic>TP</italic> + <italic>FP</italic>)</p></list-item><list-item><p><bold>recall</bold> = <italic>TP</italic>/(<italic>TP</italic> + <italic>FN</italic>)</p></list-item><list-item><p><bold>specificity</bold> = <italic>TN</italic>/(<italic>TN</italic> + <italic>FP</italic>)</p></list-item></list></p>
      <p>The plots in <xref ref-type="fig" rid="F3">Figure 3</xref> summarize the results in terms of precision and recall, while the specificity is additionally plotted in the <xref rid="sec5" ref-type="section">Supporting Information</xref>. For both types of dynamics, the performance increased with the number of samples and decreased with the size of the network.</p>
      <fig id="F3" orientation="portrait" position="float">
        <label><bold>Figure 3.</bold> </label>
        <caption>
          <p>Precision (top) and recall (bottom) for different network sizes, sample sizes, and dynamics. Left: Vector autoregressive process; Right: Coupled logistic maps. Each subplot shows five curves, corresponding to different time series lengths (<italic>T</italic> = 100, 300, 1,000, 3,000, 10,000). The results for 10 simulations from different initial conditions are shown (low-opacity markers) in addition to the mean values (solid markers). All the random networks have an average in-degree <italic>Np</italic> = 3.</p>
        </caption>
        <graphic xlink:href="netn-03-827-g003"/>
      </fig>
      <p>For shorter time series (<italic>T</italic> = 100 and <italic>T</italic> = 1000), the recall was the most affected performance measure as a function of <italic>N</italic> and <italic>T</italic>, while the precision and the specificity were always close to optimal (&gt;98% on average). (Note that, while <italic>S</italic> = 1,000 is minimal for <italic>α</italic><sub>max</sub> = 0.001, recall was unchanged using <italic>S</italic> = 10,000 for <italic>T</italic> = 100.) For longer time series (<italic>T</italic> = 10,000), high performance according to all measures was achieved for both the VAR and CLM processes, regardless of the size of the network. The high precision and specificity are due to the effective control of the false positives, in accordance with the strict statistical significance level <italic>α</italic><sub>max</sub> = 0.001 (the influence of <italic>α</italic><sub>max</sub> is further discussed in the following sections). The inference algorithm was therefore conservative in the classification of the links.</p>
    </sec>
    <sec>
      <title>Validation of False Positive Rate</title>
      <p>The critical alpha level for statistical significance <italic>α</italic><sub>max</sub> is a parameter of the algorithm that is designed to control the number of false positives in the network inference. As discussed in the <xref rid="sec2" ref-type="section"><italic>Statistical Tests</italic></xref> section in the <xref rid="sec1" ref-type="section">Methods</xref>, <italic>α</italic><sub>max</sub> controls the probability that a target is a false positive, that is, that at least one of its sources is a false positive. This approach is in line with the perspective that the goal of the network inference is to find the <italic>set</italic> of relevant sources for each node.</p>
      <p>A validation study was carried out to verify that the final number of <xref rid="def8" ref-type="def">false positives</xref> is consistent with the desired level <italic>α</italic><sub>max</sub> after multiple statistical tests are performed. The <italic>false positive rate</italic> was computed after performing the inference on empty networks, where every inferred link is a false positive by definition (i.e., under the complete null hypothesis). The rate was in good accordance with the critical alpha threshold <italic>α</italic><sub>max</sub> for all network sizes, as shown in <xref ref-type="fig" rid="F4">Figure 4</xref>.</p>
      <fig id="F4" orientation="portrait" position="float">
        <label><bold>Figure 4.</bold> </label>
        <caption>
          <p>Validation of false positive rate for a single target (<italic>t</italic><sub>FPR</sub>) on empty networks. The points indicate the average false positive rate over 50 simulations of a vector autoregressive process (<italic>T</italic> = 10,000). The horizontal marks indicate the corresponding 5th and 95th percentiles of the expected range. These were computed empirically from the distribution of the random variable 〈<italic>X</italic><sub><italic>j</italic></sub>/<italic>N</italic>〉, where <italic>X</italic><sub><italic>j</italic></sub> ∼ <italic>Binomial</italic>(<italic>N</italic>, <italic>α</italic><sub>max</sub>) are i.i.d. random variables, and the angular brackets indicate the finite average over 50 repetitions. The 5th percentile for <italic>N</italic> = 10 and <italic>N</italic> = 40 and <italic>α</italic><sub>max</sub> = 10<sup>−3</sup> are equal to zero and therefore omitted from the log-log plot. The identity function is plotted as a reference (dashed line).</p>
        </caption>
        <graphic xlink:href="netn-03-827-g004"/>
      </fig>
      <p>The false positive rate validation was replicated in a scenario where the null hypothesis held for real fMRI data from the Human Connectome Project resting-state dataset (see <xref rid="sec5" ref-type="section">Supporting Information</xref>). The findings are presented in the <xref rid="sec5" ref-type="section">Supporting Information</xref>, together with a note on autocorrelation. Notably, the results on fMRI data are in agreement with the results on synthetic data shown in <xref ref-type="fig" rid="F4">Figure 4</xref>.</p>
    </sec>
    <sec>
      <title>Influence of Critical Level for Statistical Significance</title>
      <p>Given the conservative results obtained for both the VAR and CLM processes (<xref ref-type="fig" rid="F3">Figure 3</xref>), a natural question is to what extent the recall could be improved by increasing the critical alpha level <italic>α</italic><sub>max</sub> and to what extent the precision would be negatively affected as a side effect.</p>
      <p>In order to elucidate this trade-off, the analysis described above (<xref ref-type="fig" rid="F3">Figure 3</xref>) was repeated for increasing values of <italic>α</italic><sub>max</sub>, with results shown in <xref ref-type="fig" rid="F5">Figure 5</xref>. For the shortest time series (<italic>T</italic> = 100), increasing <italic>α</italic><sub>max</sub> resulted in a higher recall and a lower precision, as expected; on the other hand, for the longest time series (<italic>T</italic> = 10,000), the performance measures were not significantly affected. Interestingly, for the intermediate case (<italic>T</italic> = 1,000), increasing <italic>α</italic><sub>max</sub> resulted in higher recall without negatively affecting the precision.</p>
      <fig id="F5" orientation="portrait" position="float">
        <label><bold>Figure 5.</bold> </label>
        <caption>
          <p>Influence of statistical significance threshold on network inference performance. Precision versus recall for different statistical significance levels (<italic>α</italic><sub>max</sub> = 0.05, 0.01, 0.001), corresponding to different colors. The plots summarize the results for different dynamics (Top: Vector autoregressive process; Bottom: Coupled logistic maps), different time series lengths (<italic>T</italic> = 100, 1,000, 10,000), and different network sizes (<italic>N</italic> = 10, 40, 70, 100, not distinguished). The arrows join the mean population values for the lowest and highest significance levels, illustrating the average trade-off between precision loss and recall gain.</p>
        </caption>
        <graphic xlink:href="netn-03-827-g005"/>
      </fig>
    </sec>
    <sec>
      <title>Inference of Coupling Lags</title>
      <p>So far, the performance evaluation focused on the identification the correct set of sources for each target node, regardless of the coupling lags. However, since the identification of the correct coupling lags is particularly relevant in neuroscience (see Wibral et al., <xref rid="bib61" ref-type="bibr">2013</xref>, and references therein), the performance of the algorithm in identifying the correct coupling lags was additionally investigated.</p>
      <p>By construction, a single coupling lag was imposed between each pair of processes (chosen at random between one and five discrete time steps, as described in the <xref rid="sec1" ref-type="section">Methods</xref>). The average absolute error between the real and the inferred coupling lags was computed on the correctly recalled sources and divided by the value expected at random (which is the average absolute difference between two i.i.d. random integers in the [1, 5] interval). In line with the previous results on precision, the absolute error on coupling lag is consistently much smaller than that expected at random, even for the shortest time series (<xref ref-type="fig" rid="F6">Figure 6</xref>). Furthermore, 1,000 samples were sufficient to achieve nearly optimal performance for both the VAR and the CLM processes, regardless of the size of the network. Note that as <italic>T</italic> increases and the recall increases, the lag error can increase (cf. <italic>T</italic> = 100 to 300 for the CLM process). This is perhaps because while the larger <italic>T</italic> permits more weakly contributing sources to be identified, it is not large enough to reduce the estimation error to make lag identification on these sources precise.</p>
      <fig id="F6" orientation="portrait" position="float">
        <label><bold>Figure 6.</bold> </label>
        <caption>
          <p>Average absolute error between the real and the inferred coupling lags, relative to the value expected at random. Results for different dynamics (Left: Vector autoregressive process; Right: Coupled logistic maps), different time series lengths (<italic>T</italic> = 100, 300, 1,000, 3,000, 10,000), and different network sizes (<italic>N</italic> = 10, 40, 70, 100). The error bars indicate the standard deviation over 10 simulations from different initial conditions.</p>
        </caption>
        <graphic xlink:href="netn-03-827-g006"/>
      </fig>
    </sec>
    <sec>
      <title>Estimators</title>
      <p>Given its speed, the Gaussian estimator is often used for large datasets or as a first exploratory step, even when the stationary distribution cannot be assumed to be Gaussian. The availability of the ground truth allowed us to compare the performance of the Gaussian estimator and the nearest-neighbor estimator on the nonlinear CLM process, which does not satisfy the Gaussian assumption. As expected, the performance of the Gaussian estimator was lower than the performance of the nearest-neighbor estimator for all network sizes (<xref ref-type="fig" rid="F7">Figure 7</xref>).</p>
      <fig id="F7" orientation="portrait" position="float">
        <label><bold>Figure 7.</bold> </label>
        <caption>
          <p>Gaussian versus nearest-neighbor estimator on the coupled logistic maps process. The precision (left) and recall (right) are plotted against the network size and a fixed time series length (<italic>T</italic> = 10,000 samples). The results for 10 simulations from different initial conditions are shown (low-opacity markers) in addition to the mean values (solid markers). The statistical significance level <italic>α</italic><sub>max</sub> = 0.05 was employed; an even larger gap between the recall of the estimators is obtained with <italic>α</italic><sub>max</sub> = 0.001.</p>
        </caption>
        <graphic xlink:href="netn-03-827-g007"/>
      </fig>
      <p>The hierarchical tests introduced in the <xref rid="sec1" ref-type="section">Methods</xref> section allow running the network inference algorithm in parallel on a high-performance computing cluster. Such parallelization is especially needed when employing the nearest-neighbor estimator. In particular, each target node can be analyzed in parallel on a CPU (employing one or more cores) or a GPU, which is made possible by the CPU and GPU estimators provided by the IDTxl package (custom OpenCL kernels were written for the GPU implementation). A summary of the CPU and GPU run times is provided in the <xref rid="sec5" ref-type="section">Supporting Information</xref>.</p>
    </sec>
  </sec>
  <sec>
    <title>DISCUSSION</title>
    <p>The algorithm presented in this paper provides robust statistical tests for network inference to control the false positive rate. These tests are compatible with parallel computation on high-performance computing clusters, which enabled the validation study on synthetic sparse networks of increasing size (10 to 100 nodes), using different dynamics (linear autoregressive processes and nonlinear coupled logistic maps) and increasingly longer time series (100 to 10,000 samples). Both the network size and the sample size are one order of magnitude larger than previously demonstrated, showing feasibility for typical EEG and MEG experiments. The results demonstrate that the statistical tests achieve the desired false positive rate and successfully address the multiple-comparison problems inherent in network inference tasks (<xref ref-type="fig" rid="F4">Figure 4</xref>).</p>
    <p>The ability to control the false positives while building connectomes is a crucial prerequisite for the application of complex network measures, to the extent that Zalesky et al. (<xref rid="bib65" ref-type="bibr">2016</xref>) concluded that “specificity is at least twice as important as sensitivity (i.e., recall) when estimating key properties of brain networks, including topological measures of network clustering, network efficiency and network modularity.” The reason is that false positives occur more prevalently between network modules than within them, and the spurious intermodular connections have a dramatic impact on network topology (Zalesky et al., <xref rid="bib65" ref-type="bibr">2016</xref>).</p>
    <p>The trade-off between precision and recall when relaxing the statistical significance threshold was further investigated (<xref ref-type="fig" rid="F5">Figure 5</xref>). When only 100 samples were used, the average recall gain was more than five times smaller than the average precision loss. In our opinion, this result is possibly due to the sparsity of the networks used in this study and suggests a conservative choice of the threshold for sparse networks and short time series. The trade-off was reversed for longer time series: when 1,000 samples were used, the average recall gain was more than five times larger than the average precision loss. Finally, for 10,000 samples, high precision and recall were achieved (&gt;98% on average) for both the vector autoregressive and the coupled logistic maps processes, regardless of the statistical significance threshold.</p>
    <p>For both types of dynamics, the network inference performance increased with the length of the time series and decreased with the size of the network (<xref ref-type="fig" rid="F3">Figure 3</xref>). This is to be expected since larger systems require more statistical tests and hence stricter conditions to control the family-wise error rate (false positives). Specifically, larger networks result in wider null distributions of the maximum statistic (i.e., larger variance), whereas longer time series have the opposite effect. Therefore, for large networks and short time series, controlling the false positives can have a negative impact on the ability to identify the true positives, particularly when the effect size (i.e., the transfer entropy value) is small.</p>
    <p>In addition, the superior ability of the nearest-neighbor estimator over the Gaussian estimator in detecting nonlinear dependencies was quantified. There is a critical motivation for this comparison: the general applicability of the nearest-neighbor estimators comes at the price of higher computational complexity and a significantly longer run time, so that the Gaussian estimator is often used for large datasets (or at least as a first exploratory step), even when the Gaussian hypothesis is not justified. To investigate such a scenario, the Gaussian estimator was tested on the nonlinear logistic map processes: while the resulting recall was significantly lower than the nearest-neighbor estimator for all network sizes, it was nonetheless able to identify over half of the links for a sufficiently large number (10,000) of time samples (<xref ref-type="fig" rid="F7">Figure 7</xref>).</p>
    <p>The stationarity assumption about the time series corresponds to assuming a single regime of neuronal activity in real brain recordings. If multiple regimes are recorded, which is typical in experimental settings (e.g., sequences of tasks or repeated presentation of stimuli interleaved with resting time windows), different stationary regimes can be studied by performing the analysis within each time window. The networks obtained in different time windows can either be studied separately and compared against each other or collectively interpreted as a single evolving temporal network. To obtain a sufficient amount of observations per window, multiple replications of the experiment under the same conditions are typically carried out. Replications can be assumed to be cyclo-stationary and estimation techniques exploiting this property have been proposed (Gómez-Herrero et al., <xref rid="bib20" ref-type="bibr">2015</xref>; Wollstadt et al., <xref rid="bib63" ref-type="bibr">2014</xref>); these estimators are also available in the IDTxl Python package. The convergence to the (unknown) causal network was only proven under the hypotheses of stationarity, causal sufficiency, and the causal Markov condition (Sun et al., <xref rid="bib53" ref-type="bibr">2015</xref>). However, conditional independence holds under milder assumptions (Runge, <xref rid="bib41" ref-type="bibr">2018</xref>) and the absence of links is valid under general conditions. The conditional independence relationships can, therefore, be used to exclude variables in following intervention-based causal experiments, making network inference methods valuable for exploratory studies.</p>
    <p>In fact, the directed network is only one part of the model and provides the scaffold over which the information-theoretic measures are computed. Therefore, even if the structure of a system is known and there is no need for network inference, information theory can still provide nontrivial insights on the distributed computation by modeling the information storage, transfer, and modification within the system (Lizier, <xref rid="bib26" ref-type="bibr">2013</xref>). This decomposition of the predictive information into the active information storage and transfer entropy components is one out of many alternatives within the framework proposed by Chicharro &amp; Ledberg (<xref rid="bib10" ref-type="bibr">2012</xref>). Arguably, the storage-transfer decomposition reflects the segregation-integration dichotomy that has long characterized the interpretation of brain function (Sporns, <xref rid="bib51" ref-type="bibr">2010</xref>; Zeki &amp; Shipp, <xref rid="bib66" ref-type="bibr">1988</xref>). Information theory has the potential to provide a quantitative definition of these fundamental but still unsettled concepts (Li et al., <xref rid="bib24" ref-type="bibr">2019</xref>). In addition, information theory provides a new way of testing fundamental computational theories in neuroscience, for example, predictive coding (Brodski-Guerniero et al., <xref rid="bib7" ref-type="bibr">2017</xref>).</p>
    <p>As such, information-theoretic methods should not be seen as opposed to model-based approaches, but complementary to them (Friston et al., <xref rid="bib18" ref-type="bibr">2013</xref>). If certain physically motivated parametric models are assumed, the two approaches are equivalent for network inference: maximizing the log-likelihood is asymptotically equivalent to maximizing the transfer entropy (Barnett &amp; Bossomaier, <xref rid="bib4" ref-type="bibr">2012</xref>; Cliff et al., <xref rid="bib11" ref-type="bibr">2018</xref>). Moreover, different approaches can be combined; for example, the recent large-scale application of spectral DCM was made possible by using functional connectivity models to place prior constraints on the parameter space (Razi et al., <xref rid="bib38" ref-type="bibr">2017</xref>). Networks inferred using bivariate transfer entropy have also been employed to reduce the model space prior to DCM analysis (Chan et al., <xref rid="bib8" ref-type="bibr">2017</xref>).</p>
    <p>In conclusion, the continuous evolution and combination of methods show that network inference from time series is an active field of research and there is a current trend of larger validation studies, statistical significance improvements, and reduction of computational complexity. Information-theoretic approaches require efficient tools to employ nearest-neighbor estimators on large datasets of continuous-valued time series, which are ubiquitous in large-scale brain recordings (calcium imaging, EEG, MEG, fMRI). The algorithm presented in this paper is compatible with parallel computation on high-performance computing clusters, which enabled the study of synthetic nonlinear systems of 100 nodes and 10,000 samples. Both the network size and the sample size are one order of magnitude larger than previously demonstrated, bringing typical EEG and MEG experiments into scope for future information-theoretic network inference studies. Furthermore, the statistical tests presented in the <xref rid="sec1" ref-type="section">Methods</xref> are generic and compatible with any underlying conditional mutual information or transfer entropy estimators, meaning that estimators applicable to spike trains (Spinney et al., <xref rid="bib49" ref-type="bibr">2017</xref>) can be used with this algorithm in future studies.</p>
  </sec>
  <sec>
    <title>ACKNOWLEDGMENTS</title>
    <p>The authors acknowledge the Sydney Informatics Hub and the University of Sydney’s high-performance computing cluster Artemis for providing the high-performance computing resources that have contributed to the research results reported within this paper. Furthermore, the authors thank Aaron J. Gutknecht for commenting on a draft of this paper, and Oliver Cliff for useful discussions and comments.</p>
  </sec>
  <sec id="sec5">
    <title>SUPPORTING INFORMATION</title>
    <p>The network inference algorithm described in this paper is implemented in the open-source Python software package IDTxl (Wollstadt et al., <xref rid="bib62" ref-type="bibr">2019</xref>), which is freely available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/pwollstadt/IDTxl">https://github.com/pwollstadt/IDTxl</ext-link>). In this paper, we refer to the current release (v1.0) at the time of writing (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.2554339">doi:10.5281/zenodo.2554339</ext-link>).</p>
    <p>The raw data used for the experiment presented in the Supporting Information (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/netn_a_00092">https://doi.org/10.1162/netn_a_00092</ext-link>) is openly available on the MGH-USC Human Connectome Project database (<ext-link ext-link-type="uri" xlink:href="https://ida.loni.usc.edu/login.jsp">https://ida.loni.usc.edu/login.jsp</ext-link>).</p>
  </sec>
  <sec>
    <title>ROLE INFORMATION</title>
    <p>Leonardo Novelli: Conceptualization; Data Curation; Formal Analysis; Investigation; Software; Validation; Visualization; Writing - Original Draft; Writing - Review &amp; Editing. Patricia Wollstadt: Conceptualization; Software; Writing - Review &amp; Editing. Pedro Mediano: Software; Writing - Review &amp; Editing. Michael Wibral: Conceptualization; Funding Acquisition; Methodology; Software; Supervision; Writing - Review &amp; Editing. Joseph T. Lizier: Conceptualization; Funding Acquisition; Methodology; Software; Supervision; Writing - Review &amp; Editing.</p>
  </sec>
  <sec>
    <title>FUNDING INFORMATION</title>
    <p>Joseph T. Lizier, Universities Australia/German Academic Exchange Service (DAAD) Australia-Germany Joint Research Cooperation Scheme Grant: “Measuring Neural Information Synthesis and Its Impairment,” Award Id: 57216857. Michael Wibral, Universities Australia/German Academic Exchange Service (DAAD) Australia-Germany Joint Research Cooperation Scheme Grant: “Measuring Neural Information Synthesis and Its Impairment,” Award Id: 57216857. Joseph T. Lizier, Australian Research Council DECRA Grant, Award Id: DE160100630. Michael Wibral, Deutsche Forschungsgemeinschaft (DFG) Grant, Award Id: CRC 1193 C04. Joseph T. Lizier, Australian Research Council Discovery Grant, Award Id: DP160102742.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="SMS1">
      <media xlink:href="netn-03-827-s001.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <glossary>
    <title>TECHNICAL TERMS</title>
    <def-list>
      <def-item id="def1">
        <term>IDTxl:</term>
        <def>
          <p>The “Information Dynamics Toolkit xl” is an open-source Python package available on GitHub (see <xref rid="sec5" ref-type="section">Supporting Information</xref>).</p>
        </def>
      </def-item>
      <def-item id="def2">
        <term>Markovian with finite memory:</term>
        <def>
          <p>The present state of the target does not depend on the past values of the target and the sources beyond a maximum finite lag <italic>l</italic><sub>M</sub>.</p>
        </def>
      </def-item>
      <def-item id="def3">
        <term>Nonuniform embedding:</term>
        <def>
          <p>A set of nonuniformly spaced time lags that captures the underlying state of the process, akin to a Takens’ embedding.</p>
        </def>
      </def-item>
      <def-item id="def4">
        <term>Active information storage:</term>
        <def>
          <p>The mutual information between the past and the present of the target: <italic>I</italic>(<inline-formula><mml:math id="m116"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>; <italic>Y</italic><sub><italic>t</italic></sub>).</p>
        </def>
      </def-item>
      <def-item id="def5">
        <term>Step-down statistical test:</term>
        <def>
          <p>A test which proceeds from the smallest to the largest <italic>p</italic> value. When the first non-significant <italic>p</italic> value is found, all the larger <italic>p</italic> values are also deemed not significant.</p>
        </def>
      </def-item>
      <def-item id="def6">
        <term>Causal sufficiency:</term>
        <def>
          <p>The set of observed variables includes all their common causes (or the unobserved common causes have constant values).</p>
        </def>
      </def-item>
      <def-item id="def7">
        <term>Causal Markov condition:</term>
        <def>
          <p>A variable X is independent of every other past variable conditional on all of its direct causes.</p>
        </def>
      </def-item>
      <def-item id="def8">
        <term>False positive rate:</term>
        <def>
          <p><italic>FP</italic>/(<italic>FP</italic> + <italic>TN</italic>).</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <ref-list>
    <title>REFERENCES</title>
    <ref id="bib1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Atay</surname><given-names>F. M.</given-names></name>, &amp; <name name-style="western"><surname>Karabacak</surname><given-names>Ö.</given-names></name></person-group> (<year>2006</year>). <article-title>Stability of coupled map networks with delays</article-title>. <source>SIAM Journal on Applied Dynamical Systems</source>, <volume>5</volume>(<issue>3</issue>), <fpage>508</fpage>–<lpage>527</lpage>.</mixed-citation>
    </ref>
    <ref id="bib2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ay</surname><given-names>N.</given-names></name>, &amp; <name name-style="western"><surname>Polani</surname><given-names>D.</given-names></name></person-group> (<year>2008</year>). <article-title>Information flows in causal networks</article-title>. <source>Advances in Complex Systems</source>, <volume>11</volume>(<issue>01</issue>), <fpage>17</fpage>–<lpage>41</lpage>.</mixed-citation>
    </ref>
    <ref id="bib3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Barnett</surname><given-names>L.</given-names></name>, <name name-style="western"><surname>Barrett</surname><given-names>A. B.</given-names></name>, &amp; <name name-style="western"><surname>Seth</surname><given-names>A. K.</given-names></name></person-group> (<year>2009</year>). <article-title>Granger causality and transfer entropy are equivalent for Gaussian variables</article-title>. <source>Physical Review Letters</source>, <volume>103</volume>(<issue>23</issue>), <fpage>238701</fpage>.<pub-id pub-id-type="pmid">20366183</pub-id></mixed-citation>
    </ref>
    <ref id="bib4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Barnett</surname><given-names>L.</given-names></name>, &amp; <name name-style="western"><surname>Bossomaier</surname><given-names>T.</given-names></name></person-group> (<year>2012</year>). <article-title>Transfer entropy as a log-likelihood ratio</article-title>. <source>Physical Review Letters</source>, <volume>109</volume>(<issue>13</issue>), <fpage>138105</fpage>.<pub-id pub-id-type="pmid">23030125</pub-id></mixed-citation>
    </ref>
    <ref id="bib5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Benjamini</surname><given-names>Y.</given-names></name>, &amp; <name name-style="western"><surname>Hochberg</surname><given-names>Y.</given-names></name></person-group> (<year>1995</year>). <article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title>. <source>Journal of the Royal Statistical Society. Series B (Methodological)</source>, <volume>57</volume>(<issue>1</issue>), <fpage>289</fpage>–<lpage>300</lpage>.</mixed-citation>
    </ref>
    <ref id="bib6">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Bossomaier</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Barnett</surname><given-names>L.</given-names></name>, <name name-style="western"><surname>Harré</surname><given-names>M.</given-names></name>, &amp; <name name-style="western"><surname>Lizier</surname><given-names>J. T.</given-names></name></person-group> (<year>2016</year>). <source>An Introduction to Transfer Entropy</source>. <publisher-name>Springer International Publishing</publisher-name>, <publisher-loc>Chambridge, UK</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="bib7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Brodski-Guerniero</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Paasch</surname><given-names>G.-F.</given-names></name>, <name name-style="western"><surname>Wollstadt</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Özdemir</surname><given-names>I.</given-names></name>, <name name-style="western"><surname>Lizier</surname><given-names>J. T.</given-names></name>, &amp; <name name-style="western"><surname>Wibral</surname><given-names>M.</given-names></name></person-group> (<year>2017</year>). <article-title>Information-theoretic evidence for predictive coding in the face-processing system</article-title>. <source>The Journal of Neuroscience</source>, <volume>37</volume>(<issue>34</issue>), <fpage>8273</fpage>–<lpage>8283</lpage>.<pub-id pub-id-type="pmid">28751458</pub-id></mixed-citation>
    </ref>
    <ref id="bib8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chan</surname><given-names>J. S.</given-names></name>, <name name-style="western"><surname>Wibral</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Wollstadt</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Stawowsky</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Brandl</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Helbling</surname><given-names>S.</given-names></name>, … <name name-style="western"><surname>Kaiser</surname><given-names>J.</given-names></name></person-group> (<year>2017</year>). <article-title>Predictive coding over the lifespan: Increased reliance on perceptual priors in older adults—a magnetoencephalography and dynamic causal modelling study</article-title>. <source>bioRxiv Preprint</source>, page <fpage>178095</fpage>.</mixed-citation>
    </ref>
    <ref id="bib9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chávez</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Martinerie</surname><given-names>J.</given-names></name>, &amp; <name name-style="western"><surname>Le Van Quyen</surname><given-names>M.</given-names></name></person-group> (<year>2003</year>). <article-title>Statistical assessment of nonlinear causality: application to epileptic EEG signals</article-title>. <source>Journal of Neuroscience Methods</source>, <volume>124</volume>(<issue>2</issue>), <fpage>113</fpage>–<lpage>128</lpage>.<pub-id pub-id-type="pmid">12706841</pub-id></mixed-citation>
    </ref>
    <ref id="bib10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chicharro</surname><given-names>D.</given-names></name>, &amp; <name name-style="western"><surname>Ledberg</surname><given-names>A.</given-names></name></person-group> (<year>2012</year>). <article-title>Framework to study dynamic dependencies in networks of interacting processes</article-title>. <source>Physical Review E</source>, <volume>86</volume>(<issue>4</issue>), <fpage>041901</fpage>.</mixed-citation>
    </ref>
    <ref id="bib11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cliff</surname><given-names>O.</given-names></name>, <name name-style="western"><surname>Prokopenko</surname><given-names>M.</given-names></name>, &amp; <name name-style="western"><surname>Fitch</surname><given-names>R.</given-names></name></person-group> (<year>2018</year>). <article-title>Minimising the Kullback-Leibler divergence for model selection in distributed nonlinear systems</article-title>. <source>Entropy</source>, <volume>20</volume>(<issue>2</issue>), <fpage>51</fpage>.</mixed-citation>
    </ref>
    <ref id="bib12">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Cover</surname><given-names>T. M.</given-names></name>, &amp; <name name-style="western"><surname>Thomas</surname><given-names>J. A.</given-names></name></person-group> (<year>2005</year>). <source>Elements of Information Theory</source>. <publisher-name>John Wiley &amp; Sons, Hoboken, NJ, USA, 2 edition</publisher-name>.</mixed-citation>
    </ref>
    <ref id="bib13">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Dickhaus</surname><given-names>T.</given-names></name></person-group> (<year>2014</year>). <source>Simultaneous Statistical Inference</source>. <publisher-name>Springer Berlin Heidelberg, Berlin, Heidelberg</publisher-name>.</mixed-citation>
    </ref>
    <ref id="bib14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Erdős</surname><given-names>P.</given-names></name>, &amp; <name name-style="western"><surname>Rényi</surname><given-names>A.</given-names></name></person-group> (<year>1959</year>). <article-title>On random graphs</article-title>. <source>Publicationes Mathematicae Debrecen</source>, <volume>6</volume>, <fpage>290</fpage>–<lpage>297</lpage>.</mixed-citation>
    </ref>
    <ref id="bib15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Faes</surname><given-names>L.</given-names></name>, <name name-style="western"><surname>Nollo</surname><given-names>G.</given-names></name>, &amp; <name name-style="western"><surname>Porta</surname><given-names>A.</given-names></name></person-group> (<year>2011</year>). <article-title>Information-based detection of nonlinear Granger causality in multivariate processes via a nonuniform embedding technique</article-title>. <source>Physical Review E</source>, <volume>83</volume>(<issue>5</issue>), <fpage>051112</fpage>.</mixed-citation>
    </ref>
    <ref id="bib16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Frenzel</surname><given-names>S.</given-names></name>, &amp; <name name-style="western"><surname>Pompe</surname><given-names>B.</given-names></name></person-group> (<year>2007</year>). <article-title>Partial mutual information for coupling analysis of multivariate time series</article-title>. <source>Physical Review Letters</source>, <volume>99</volume>(<issue>20</issue>), <fpage>204101</fpage>.<pub-id pub-id-type="pmid">18233144</pub-id></mixed-citation>
    </ref>
    <ref id="bib17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Friston</surname><given-names>K. J.</given-names></name></person-group> (<year>1994</year>). <article-title>Functional and effective connectivity in neuroimaging: a synthesis</article-title>. <source>Human Brain Mapping</source>, <volume>2</volume>(<issue>1–2</issue>), <fpage>56</fpage>–<lpage>78</lpage>.</mixed-citation>
    </ref>
    <ref id="bib18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Friston</surname><given-names>K. J.</given-names></name>, <name name-style="western"><surname>Moran</surname><given-names>R.</given-names></name>, &amp; <name name-style="western"><surname>Seth</surname><given-names>A. K.</given-names></name></person-group> (<year>2013</year>). <article-title>Analysing connectivity with Granger causality and dynamic causal modelling</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>23</volume>(<issue>2</issue>), <fpage>172</fpage>–<lpage>178</lpage>.<pub-id pub-id-type="pmid">23265964</pub-id></mixed-citation>
    </ref>
    <ref id="bib19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Garland</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>James</surname><given-names>R. G.</given-names></name>, &amp; <name name-style="western"><surname>Bradley</surname><given-names>E.</given-names></name></person-group> (<year>2016</year>). <article-title>Leveraging information storage to select forecast-optimal parameters for delay-coordinate reconstructions</article-title>. <source>Physical Review E</source>, <volume>93</volume>(<issue>2</issue>), <fpage>022221</fpage>.<pub-id pub-id-type="pmid">26986345</pub-id></mixed-citation>
    </ref>
    <ref id="bib20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gómez-Herrero</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>Wu</surname><given-names>W.</given-names></name>, <name name-style="western"><surname>Rutanen</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>Soriano</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Pipa</surname><given-names>G.</given-names></name>, &amp; <name name-style="western"><surname>Vicente</surname><given-names>R.</given-names></name></person-group> (<year>2015</year>). <article-title>Assessing coupling dynamics from an ensemble of time series</article-title>. <source>Entropy</source>, <volume>17</volume>(<issue>4</issue>), <fpage>1958</fpage>–<lpage>1970</lpage>.</mixed-citation>
    </ref>
    <ref id="bib21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Granger</surname><given-names>C. W. J.</given-names></name></person-group> (<year>1969</year>). <article-title>Investigating causal relations by econometric models and cross-spectral methods</article-title>. <source>Econometrica</source>, <volume>37</volume>(<issue>3</issue>), <fpage>424</fpage>–<lpage>438</lpage>.</mixed-citation>
    </ref>
    <ref id="bib22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Rogers</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name>, &amp; <name name-style="western"><surname>Bollt</surname><given-names>E. M.</given-names></name></person-group> (<year>2016</year>). <article-title>Causation entropy identifies sparsity structure for parameter estimation of dynamic systems</article-title>. <source>Journal of Computational and Nonlinear Dynamics</source>, <volume>12</volume>(<issue>1</issue>), <fpage>011008</fpage>.</mixed-citation>
    </ref>
    <ref id="bib23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kraskov</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Stögbauer</surname><given-names>H.</given-names></name>, &amp; <name name-style="western"><surname>Grassberger</surname><given-names>P.</given-names></name></person-group> (<year>2004</year>). <article-title>Estimating mutual information</article-title>. <source>Physical Review E</source>, <volume>69</volume>(<issue>6</issue>), <fpage>066138</fpage>.</mixed-citation>
    </ref>
    <ref id="bib24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Han</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Aburn</surname><given-names>M. J.</given-names></name>, <name name-style="western"><surname>Breakspear</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Poldrack</surname><given-names>R. A.</given-names></name>, <name name-style="western"><surname>Shine</surname><given-names>J. M.</given-names></name>, &amp; <name name-style="western"><surname>Lizier</surname><given-names>J. T.</given-names></name></person-group> (<year>2019</year>). <article-title>Transitions in brain-network level information processing dynamics are driven by alterations in neural gain</article-title>. <source>bioRxiv Preprint</source>, page <fpage>581538</fpage>.</mixed-citation>
    </ref>
    <ref id="bib25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lindner</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Vicente</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Priesemann</surname><given-names>V.</given-names></name>, &amp; <name name-style="western"><surname>Wibral</surname><given-names>M.</given-names></name></person-group> (<year>2011</year>). <article-title>TRENTOOL: A Matlab open source toolbox to analyse information flow in time series data with transfer entropy</article-title>. <source>BMC Neuroscience</source>, <volume>12</volume>, <fpage>119</fpage>.<pub-id pub-id-type="pmid">22098775</pub-id></mixed-citation>
    </ref>
    <ref id="bib26">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Lizier</surname><given-names>J. T.</given-names></name></person-group> (<year>2013</year>). <source>The Local Information Dynamics of Distributed Computation in Complex Systems</source>. <publisher-name>Springer Berlin, Heidelberg</publisher-name>.</mixed-citation>
    </ref>
    <ref id="bib27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lizier</surname><given-names>J. T.</given-names></name></person-group> (<year>2014</year>). <article-title>JIDT: An information-theoretic toolkit for studying the dynamics of complex systems</article-title>. <source>Frontiers in robotics and AI</source>, <volume>1</volume>, <fpage>11</fpage>.</mixed-citation>
    </ref>
    <ref id="bib28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lizier</surname><given-names>J. T.</given-names></name>, <name name-style="western"><surname>Heinzle</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Horstmann</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Haynes</surname><given-names>J.-D.</given-names></name>, &amp; <name name-style="western"><surname>Prokopenko</surname><given-names>M.</given-names></name></person-group> (<year>2011</year>). <article-title>Multivariate information-theoretic measures reveal directed information structure and task relevant changes in fMRI connectivity</article-title>. <source>Journal of Computational Neuroscience</source>, <volume>30</volume>(<issue>1</issue>), <fpage>85</fpage>–<lpage>107</lpage>.<pub-id pub-id-type="pmid">20799057</pub-id></mixed-citation>
    </ref>
    <ref id="bib29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lizier</surname><given-names>J. T.</given-names></name>, <name name-style="western"><surname>Prokopenko</surname><given-names>M.</given-names></name>, &amp; <name name-style="western"><surname>Zomaya</surname><given-names>A. Y.</given-names></name></person-group> (<year>2008</year>). <article-title>Local information transfer as a spatiotemporal filter for complex systems</article-title>. <source>Physical Review E</source>, <volume>77</volume>(<issue>2</issue>), <fpage>026110</fpage>.</mixed-citation>
    </ref>
    <ref id="bib30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lizier</surname><given-names>J. T.</given-names></name>, <name name-style="western"><surname>Prokopenko</surname><given-names>M.</given-names></name>, &amp; <name name-style="western"><surname>Zomaya</surname><given-names>A. Y.</given-names></name></person-group> (<year>2010</year>). <article-title>Information modification and particle collisions in distributed computation</article-title>. <source>Chaos</source>, <volume>20</volume>(<issue>3</issue>), <fpage>037109</fpage>.<pub-id pub-id-type="pmid">20887075</pub-id></mixed-citation>
    </ref>
    <ref id="bib31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lizier</surname><given-names>J. T.</given-names></name>, <name name-style="western"><surname>Prokopenko</surname><given-names>M.</given-names></name>, &amp; <name name-style="western"><surname>Zomaya</surname><given-names>A. Y.</given-names></name></person-group> (<year>2012</year>). <article-title>Local measures of information storage in complex distributed computation</article-title>. <source>Information Sciences</source>, <volume>208</volume>, <fpage>39</fpage>–<lpage>54</lpage>.</mixed-citation>
    </ref>
    <ref id="bib32">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Lizier</surname><given-names>J. T.</given-names></name>, &amp; <name name-style="western"><surname>Rubinov</surname><given-names>M.</given-names></name></person-group> (<year>2012</year>). <article-title>Multivariate construction of effective computational networks from observational data</article-title>. <comment>Technical Report Preprint 25/2012</comment>, <publisher-name>Max Planck Institute for Mathematics in the Sciences</publisher-name>.</mixed-citation>
    </ref>
    <ref id="bib33">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Lorenz</surname><given-names>H.-W.</given-names></name></person-group> (<year>1993</year>). <article-title>Chaotic dynamics in discrete-time economic models</article-title>. In <source>Nonlinear Dynamical Economics and Chaotic Motion</source> (pages <fpage>119</fpage>–<lpage>166</lpage>). <publisher-name>Springer</publisher-name>, <publisher-loc>Berlin</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="bib34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Montalto</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Faes</surname><given-names>L.</given-names></name>, &amp; <name name-style="western"><surname>Marinazzo</surname><given-names>D.</given-names></name></person-group> (<year>2014</year>). <article-title>MuTE: A MATLAB toolbox to compare established and novel estimators of the multivariate transfer entropy</article-title>. <source>PLoS ONE</source>, <volume>9</volume>(<issue>10</issue>), <fpage>e109462</fpage>.<pub-id pub-id-type="pmid">25314003</pub-id></mixed-citation>
    </ref>
    <ref id="bib35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nichols</surname><given-names>T.</given-names></name>, &amp; <name name-style="western"><surname>Hayasaka</surname><given-names>S.</given-names></name></person-group> (<year>2003</year>). <article-title>Controlling the familywise error rate in functional neuroimaging: a comparative review</article-title>. <source>Statistical Methods in Medical Research</source>, <volume>12</volume>(<issue>5</issue>), <fpage>419</fpage>–<lpage>446</lpage>.<pub-id pub-id-type="pmid">14599004</pub-id></mixed-citation>
    </ref>
    <ref id="bib36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Paluš</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Albrecht</surname><given-names>V.</given-names></name>, &amp; <name name-style="western"><surname>Dvoák</surname><given-names>I.</given-names></name></person-group> (<year>1993</year>). <article-title>Information theoretic test for nonlinearity in time series</article-title>. <source>Physics Letters A</source>, <volume>175</volume>(<issue>3–4</issue>), <fpage>203</fpage>–<lpage>209</lpage>.</mixed-citation>
    </ref>
    <ref id="bib37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Paninski</surname><given-names>L.</given-names></name></person-group> (<year>2003</year>). <article-title>Estimation of entropy and mutual information</article-title>. <source>Neural Computation</source>, <volume>15</volume>(<issue>6</issue>), <fpage>1191</fpage>–<lpage>1253</lpage>.</mixed-citation>
    </ref>
    <ref id="bib38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Razi</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Seghier</surname><given-names>M. L.</given-names></name>, <name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>McColgan</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Zeidman</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Park</surname><given-names>H.-J.</given-names></name>, <name name-style="western"><surname>Sporns</surname><given-names>O.</given-names></name>, <name name-style="western"><surname>Rees</surname><given-names>G.</given-names></name>, &amp; <name name-style="western"><surname>Friston</surname><given-names>K. J.</given-names></name></person-group> (<year>2017</year>). <article-title>Large-scale DCMs for resting-state fMRI</article-title>. <source>Network Neuroscience</source>, <volume>1</volume>(<issue>3</issue>), <fpage>222</fpage>–<lpage>241</lpage>.<pub-id pub-id-type="pmid">29400357</pub-id></mixed-citation>
    </ref>
    <ref id="bib39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Roulston</surname><given-names>M. S.</given-names></name></person-group> (<year>1999</year>). <article-title>Estimating the errors on measured entropy and mutual information</article-title>. <source>Physica D: Nonlinear Phenomena</source>, <volume>125</volume>(<issue>3–4</issue>), <fpage>285</fpage>–<lpage>294</lpage>.</mixed-citation>
    </ref>
    <ref id="bib40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rubinov</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Sporns</surname><given-names>O.</given-names></name>, <name name-style="western"><surname>van Leeuwen</surname><given-names>C.</given-names></name>, &amp; <name name-style="western"><surname>Breakspear</surname><given-names>M.</given-names></name></person-group> (<year>2009</year>). <article-title>Symbiotic relationship between brain structure and dynamics</article-title>. <source>BMC Neuroscience</source>, <volume>10</volume>, <fpage>55</fpage>.<pub-id pub-id-type="pmid">19486538</pub-id></mixed-citation>
    </ref>
    <ref id="bib41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Runge</surname><given-names>J.</given-names></name></person-group> (<year>2018</year>). <article-title>Causal network reconstruction from time series: from theoretical assumptions to practical estimation</article-title>. <source>Chaos</source>, <volume>28</volume>(<issue>7</issue>), <fpage>075310</fpage>.<pub-id pub-id-type="pmid">30070533</pub-id></mixed-citation>
    </ref>
    <ref id="bib42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Runge</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Heitzig</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Petoukhov</surname><given-names>V.</given-names></name>, &amp; <name name-style="western"><surname>Kurths</surname><given-names>J.</given-names></name></person-group> (<year>2012</year>). <article-title>Escaping the curse of dimensionality in estimating multivariate transfer entropy</article-title>. <source>Physical Review Letters</source>, <volume>108</volume>(<issue>25</issue>), <fpage>258701</fpage>.<pub-id pub-id-type="pmid">23004667</pub-id></mixed-citation>
    </ref>
    <ref id="bib43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Runge</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Nowack</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Kretschmer</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Flaxman</surname><given-names>S.</given-names></name>, &amp; <name name-style="western"><surname>Sejdinovic</surname><given-names>D.</given-names></name></person-group> (<year>2018</year>). <article-title>Detecting causal associations in large nonlinear time series datasets</article-title>. <source>arXiv Preprint</source>. <comment>arXiv: 1702.07007</comment>.</mixed-citation>
    </ref>
    <ref id="bib44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Schreiber</surname><given-names>T.</given-names></name></person-group> (<year>2000</year>). <article-title>Measuring information transfer</article-title>. <source>Physical Review Letters</source>, <volume>85</volume>(<issue>2</issue>), <fpage>461</fpage>–<lpage>464</lpage>.<pub-id pub-id-type="pmid">10991308</pub-id></mixed-citation>
    </ref>
    <ref id="bib45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Schreiber</surname><given-names>T.</given-names></name>, &amp; <name name-style="western"><surname>Schmitz</surname><given-names>A.</given-names></name></person-group> (<year>2000</year>). <article-title>Surrogate time series</article-title>. <source>Physica D: Nonlinear Phenomena</source>, <volume>142</volume>(<issue>3–4</issue>), <fpage>346</fpage>–<lpage>382</lpage>.</mixed-citation>
    </ref>
    <ref id="bib46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shannon</surname><given-names>C. E.</given-names></name></person-group> (<year>1948</year>). <article-title>A mathematical theory of communication</article-title>. <source>Bell System Technical Journal</source>, <volume>27</volume>(<issue>3</issue>), <fpage>379</fpage>–<lpage>423</lpage>.</mixed-citation>
    </ref>
    <ref id="bib47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Šidák</surname><given-names>Z.</given-names></name></person-group> (<year>1967</year>). <article-title>Rectangular confidence regions for the means of multivariate normal distributions</article-title>. <source>Journal of the American Statistical Association</source>, <volume>62</volume>(<issue>318</issue>), <fpage>626</fpage>–<lpage>633</lpage>.</mixed-citation>
    </ref>
    <ref id="bib48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sims</surname><given-names>C. A.</given-names></name></person-group> (<year>1980</year>). <article-title>Macroeconomics and reality</article-title>. <source>Econometrica</source>, <volume>48</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>48</lpage>.</mixed-citation>
    </ref>
    <ref id="bib49">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Spinney</surname><given-names>R. E.</given-names></name>, <name name-style="western"><surname>Prokopenko</surname><given-names>M.</given-names></name>, &amp; <name name-style="western"><surname>Lizier</surname><given-names>J. T.</given-names></name></person-group> (<year>2017</year>). <article-title>Transfer entropy in continuous time, with applications to jump and neural spiking processes</article-title>. <source>Physical Review E</source>, <volume>95</volume>(<issue>3</issue>), <fpage>032319</fpage>.<pub-id pub-id-type="pmid">28415203</pub-id></mixed-citation>
    </ref>
    <ref id="bib50">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Spirtes</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Glymour</surname><given-names>C.</given-names></name>, &amp; <name name-style="western"><surname>Scheines</surname><given-names>R.</given-names></name></person-group> (<year>1993</year>). <source>Causation, Prediction, and Search</source>, <volume>volume 81</volume> of <series><italic>Lecture Notes in Statistics</italic></series>
<publisher-name>Springer New York</publisher-name>.</mixed-citation>
    </ref>
    <ref id="bib51">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Sporns</surname><given-names>O.</given-names></name></person-group> (<year>2010</year>). <source>Networks of the Brain</source>. <publisher-name>MIT Press</publisher-name>, <publisher-loc>Cambridge, MA</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="bib52">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Strogatz</surname><given-names>S. H.</given-names></name></person-group> (<year>2015</year>). <source>Nonlinear Dynamics and Chaos</source>. <publisher-name>CRC Press</publisher-name>, <publisher-loc>Boca Raton, FL</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="bib53">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Taylor</surname><given-names>D.</given-names></name>, &amp; <name name-style="western"><surname>Bollt</surname><given-names>E. M.</given-names></name></person-group> (<year>2015</year>). <article-title>Causal network inference by optimal causation entropy</article-title>. <source>SIAM Journal on Applied Dynamical Systems</source>, <volume>14</volume>(<issue>1</issue>), <fpage>73</fpage>–<lpage>106</lpage>.</mixed-citation>
    </ref>
    <ref id="bib54">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Takens</surname><given-names>F.</given-names></name></person-group> (<year>1981</year>). <article-title>Detecting strange attractors in turbulence</article-title>. In <person-group person-group-type="editor"><name name-style="western"><surname>Rand</surname><given-names>D.</given-names></name> and <name name-style="western"><surname>Young</surname><given-names>L.</given-names></name></person-group>, editors, <source>Dynamical Systems and Turbulence</source>, pages <fpage>366</fpage>–<lpage>381</lpage>. <publisher-name>Springer</publisher-name>, <publisher-loc>Berlin</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="bib55">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vakorin</surname><given-names>V. A.</given-names></name>, <name name-style="western"><surname>Krakovska</surname><given-names>O. A.</given-names></name>, &amp; <name name-style="western"><surname>McIntosh</surname><given-names>A. R.</given-names></name></person-group> (<year>2009</year>). <article-title>Confounding effects of indirect connections on causality estimation</article-title>. <source>Journal of Neuroscience Methods</source>, <volume>184</volume>(<issue>1</issue>), <fpage>152</fpage>–<lpage>160</lpage>.<pub-id pub-id-type="pmid">19628006</pub-id></mixed-citation>
    </ref>
    <ref id="bib56">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Valdes-Sosa</surname><given-names>P. A.</given-names></name>, <name name-style="western"><surname>Roebroeck</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Daunizeau</surname><given-names>J.</given-names></name>, &amp; <name name-style="western"><surname>Friston</surname><given-names>K. J.</given-names></name></person-group> (<year>2011</year>). <article-title>Effective connectivity: influence, causality and biophysical modeling</article-title>. <source>NeuroImage</source>, <volume>58</volume>(<issue>2</issue>), <fpage>339</fpage>–<lpage>361</lpage>.<pub-id pub-id-type="pmid">21477655</pub-id></mixed-citation>
    </ref>
    <ref id="bib57">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vejmelka</surname><given-names>M.</given-names></name>, &amp; <name name-style="western"><surname>Paluš</surname><given-names>M.</given-names></name></person-group> (<year>2008</year>). <article-title>Inferring the directionality of coupling with conditional mutual information</article-title>. <source>Physical Review E</source>, <volume>77</volume>(<issue>2</issue>), <fpage>026214</fpage>.</mixed-citation>
    </ref>
    <ref id="bib58">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Verdes</surname><given-names>P. F.</given-names></name></person-group> (<year>2005</year>). <article-title>Assessing causality from multivariate time series</article-title>. <source>Physical Review E</source>, <volume>72</volume>(<issue>2</issue>), <fpage>026222</fpage>.</mixed-citation>
    </ref>
    <ref id="bib59">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vicente</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Wibral</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Lindner</surname><given-names>M.</given-names></name>, &amp; <name name-style="western"><surname>Pipa</surname><given-names>G.</given-names></name></person-group> (<year>2011</year>). <article-title>Transfer entropy—a model-free measure of effective connectivity for the neurosciences</article-title>. <source>Journal of Computational Neuroscience</source>, <volume>30</volume>(<issue>1</issue>), <fpage>45</fpage>–<lpage>67</lpage>.<pub-id pub-id-type="pmid">20706781</pub-id></mixed-citation>
    </ref>
    <ref id="bib60">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vlachos</surname><given-names>I.</given-names></name>, &amp; <name name-style="western"><surname>Kugiumtzis</surname><given-names>D.</given-names></name></person-group> (<year>2010</year>). <article-title>Nonuniform state-space reconstruction and coupling detection</article-title>. <source>Physical Review E</source>, <volume>82</volume>(<issue>1</issue>), <fpage>016207</fpage>.</mixed-citation>
    </ref>
    <ref id="bib61">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wibral</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Pampu</surname><given-names>N.</given-names></name>, <name name-style="western"><surname>Priesemann</surname><given-names>V.</given-names></name>, <name name-style="western"><surname>Siebenhühner</surname><given-names>F.</given-names></name>, <name name-style="western"><surname>Seiwert</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Lindner</surname><given-names>M.</given-names></name>, … <name name-style="western"><surname>Vicente</surname><given-names>R.</given-names></name></person-group> (<year>2013</year>). <article-title>Measuring information-transfer delays</article-title>. <source>PLoS ONE</source>, <volume>8</volume>(<issue>2</issue>), <fpage>e55809</fpage>.<pub-id pub-id-type="pmid">23468850</pub-id></mixed-citation>
    </ref>
    <ref id="bib62">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wollstadt</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Lizier</surname><given-names>J. T.</given-names></name>, <name name-style="western"><surname>Vicente</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Finn</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Martínez-Zarzuela</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Mediano</surname><given-names>P.</given-names></name>, … <name name-style="western"><surname>Wibral</surname><given-names>M.</given-names></name></person-group> (<year>2019</year>). <article-title>IDTxl: The Information Dynamics Toolkit xl: a Python package for the efficient analysis of multivariate information dynamics in networks</article-title>. <source>Journal of Open Source Software</source>, <volume>4</volume>(<issue>34</issue>), <fpage>1081</fpage>.</mixed-citation>
    </ref>
    <ref id="bib63">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wollstadt</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Martínez-Zarzuela</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Vicente</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Díaz-Pernas</surname><given-names>F. J.</given-names></name>, &amp; <name name-style="western"><surname>Wibral</surname><given-names>M.</given-names></name></person-group> (<year>2014</year>). <article-title>Efficient transfer entropy analysis of non-stationary neural time series</article-title>. <source>PLoS ONE</source>, <volume>9</volume>(<issue>7</issue>), <fpage>e102833</fpage>.<pub-id pub-id-type="pmid">25068489</pub-id></mixed-citation>
    </ref>
    <ref id="bib64">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zalesky</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Fornito</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Cocchi</surname><given-names>L.</given-names></name>, <name name-style="western"><surname>Gollo</surname><given-names>L. L.</given-names></name>, &amp; <name name-style="western"><surname>Breakspear</surname><given-names>M.</given-names></name></person-group> (<year>2014</year>). <article-title>Time-resolved resting-state brain networks</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>111</volume>(<issue>28</issue>), <fpage>10341</fpage>–<lpage>10346</lpage>.</mixed-citation>
    </ref>
    <ref id="bib65">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zalesky</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Fornito</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Cocchi</surname><given-names>L.</given-names></name>, <name name-style="western"><surname>Gollo</surname><given-names>L. L.</given-names></name>, <name name-style="western"><surname>van den Heuvel</surname><given-names>M. P.</given-names></name>, &amp; <name name-style="western"><surname>Breakspear</surname><given-names>M.</given-names></name></person-group> (<year>2016</year>). <article-title>Connectome sensitivity or specificity: which is more important?</article-title>
<source>NeuroImage</source>, <volume>142</volume>, <fpage>407</fpage>–<lpage>420</lpage>.<pub-id pub-id-type="pmid">27364472</pub-id></mixed-citation>
    </ref>
    <ref id="bib66">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zeki</surname><given-names>S.</given-names></name>, &amp; <name name-style="western"><surname>Shipp</surname><given-names>S.</given-names></name></person-group> (<year>1988</year>). <article-title>The functional logic of cortical connections</article-title>. <source>Nature</source>, <volume>335</volume>, <fpage>311</fpage>–<lpage>317</lpage>.<pub-id pub-id-type="pmid">3047584</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
