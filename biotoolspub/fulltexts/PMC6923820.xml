<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Genomics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Genomics</journal-id>
    <journal-title-group>
      <journal-title>BMC Genomics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2164</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6923820</article-id>
    <article-id pub-id-type="publisher-id">6285</article-id>
    <article-id pub-id-type="doi">10.1186/s12864-019-6285-x</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Integrate multi-omics data with biological interaction networks using Multi-view Factorization AutoEncoder (MAE)</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Ma</surname>
          <given-names>Tianle</given-names>
        </name>
        <address>
          <email>tianlema@buffalo.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Zhang</surname>
          <given-names>Aidong</given-names>
        </name>
        <address>
          <email>aidong@virginia.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1936 9887</institution-id><institution-id institution-id-type="GRID">grid.273335.3</institution-id><institution>Department of Computer Science and Engineering, University at Buffalo, </institution></institution-wrap>338 Davis Hall, Buffalo, 14260 NY USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9136 933X</institution-id><institution-id institution-id-type="GRID">grid.27755.32</institution-id><institution>Department of Computer Science, University of Virginia, </institution></institution-wrap>509 Rice Hall, Charlottesville, 22904 VA USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>20</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>20</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>20</volume>
    <issue>Suppl 11</issue>
    <issue-sponsor>Publication of this supplement has not been supported by sponsorship. Information about the source of funding for publication charges can be found in the individual articles. The articles have undergone the journal's standard peer review process for supplements. The Supplement Editors declare that they have no competing interests.</issue-sponsor>
    <elocation-id>944</elocation-id>
    <permissions>
      <copyright-statement>© The Author(s) 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Comprehensive molecular profiling of various cancers and other diseases has generated vast amounts of multi-omics data. Each type of -omics data corresponds to one feature space, such as gene expression, miRNA expression, DNA methylation, etc. Integrating multi-omics data can link different layers of molecular feature spaces and is crucial to elucidate molecular pathways underlying various diseases. Machine learning approaches to mining multi-omics data hold great promises in uncovering intricate relationships among molecular features. However, due to the “big p, small n” problem (i.e., small sample sizes with high-dimensional features), training a large-scale generalizable deep learning model with multi-omics data alone is very challenging.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">We developed a method called Multi-view Factorization AutoEncoder (MAE) with network constraints that can seamlessly integrate multi-omics data and domain knowledge such as molecular interaction networks. Our method learns feature and patient embeddings simultaneously with deep representation learning. Both feature representations and patient representations are subject to certain constraints specified as regularization terms in the training objective. By incorporating domain knowledge into the training objective, we implicitly introduced a good inductive bias into the machine learning model, which helps improve model generalizability. We performed extensive experiments on the TCGA datasets and demonstrated the power of integrating multi-omics data and biological interaction networks using our proposed method for predicting target clinical variables.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">To alleviate the overfitting problem in deep learning on multi-omics data with the “big p, small n” problem, it is helpful to incorporate biological domain knowledge into the model as inductive biases. It is very promising to design machine learning models that facilitate the seamless integration of large-scale multi-omics data and biomedical domain knowledge for uncovering intricate relationships among molecular features and clinical features.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Multi-omics data</kwd>
      <kwd>Biological interaction networks</kwd>
      <kwd>Deep learning</kwd>
      <kwd>Multi-view learning</kwd>
      <kwd>Autoencoder</kwd>
      <kwd>Data integration</kwd>
      <kwd>Graph regularization</kwd>
    </kwd-group>
    <conference xlink:href="http://orienta.ugr.es/bibm2018/">
      <conf-name>IEEE International Conference on Bioinformatics and Biomedicine 2018</conf-name>
      <conf-loc>Madrid, Spain</conf-loc>
      <conf-date>3-6 December 2018</conf-date>
    </conference>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p>With the fast adoption of Next Generation Sequencing (NGS) technologies, petabytes of genomic, transcriptomic, proteomic, and epigenomic data (collectively called multi-omics data) have been accumulated in the past decade. Notably, The Cancer Genome Atlas (TCGA) Network [<xref ref-type="bibr" rid="CR1">1</xref>] alone had generated over one petabyte of multi-omics data for comprehensive molecular profiling of over 11,000 patients from 33 cancer types. Multi-omics data includes multiple types of -omics data, each of which represents one view and has a different feature set (for instance, gene expressions, miRNA expressions, and so on). Since multiple views for the same patients can provide complementary information, integrative analysis of multi-omics data with machine learning approaches has great potentials to elucidate the molecular underpinning of disease etiology. However, due to the “big p, small n” problem, many statistical machine learning approaches that require lots of training data may fail to extract true signals from multi-omics data alone.</p>
    <p>Deep learning has achieved great success in computer vision, speech recognition, natural language processing and many other fields in the past decade [<xref ref-type="bibr" rid="CR2">2</xref>]. However, deep learning models often require large amounts of annotated training data with clearly defined structures (such as images, audio, and natural languages), and cannot be directly applied to multi-omic data with unclear structures among features and a small sample size. Novel model architectures and learning strategies need to be invented to address the challenge of learning from multi-omics data with heterogeneous features and the “big p, small n” problem.</p>
    <p>In this paper, we present a framework called Multi-view Factorization AutoEncoder (MAE) with network constraints [<xref ref-type="bibr" rid="CR3">3</xref>], combining multi-view learning [<xref ref-type="bibr" rid="CR4">4</xref>] and matrix factorization [<xref ref-type="bibr" rid="CR5">5</xref>] with deep learning for integrating multi-omics data with biological domain knowledge. The MAE model consists of multiple autoencoders as submodules (one for each data view), and a submodule that combines individual views. The model facilitates learning feature and patient embeddings simultaneously with deep representation learning. Importantly, we incorporate domain knowledge such as biological interaction networks into the model training objective to ensure the learned feature embeddings are consistent with the domain knowledge.</p>
    <p>Besides the molecular interaction networks, we can construct multiple patient similarity networks based on the learned patient embeddings from individual views. We included patient similarity network constraints to ensure these similarity networks for the same set of patients are consistent with each other. Equipped with feature interaction and patient similarity network constraints, our model achieved better performance than traditional machine learning methods and conventional deep learning models without using domain knowledge on the TCGA datasets [<xref ref-type="bibr" rid="CR1">1</xref>].</p>
    <sec id="Sec2">
      <title>Related work</title>
      <p>Many genetic disease studies focus on molecular characterization of individual disease types [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR6">6</xref>], employing mainly statistical analyses to find associations among molecular and clinical features. Machine learning has been applied to individual -omics data types [<xref ref-type="bibr" rid="CR7">7</xref>] and to integrate multi-omics data [<xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR9">9</xref>]. Because most existing deep learning models cannot handle the “big p, small n” problem effectively, many traditional machine learning methods (such as logistic regression [<xref ref-type="bibr" rid="CR7">7</xref>], random forest [<xref ref-type="bibr" rid="CR8">8</xref>], and similarity network fusion [<xref ref-type="bibr" rid="CR9">9</xref>]) have been applied to -omics data.</p>
      <p>Comprehensive multi-omics data analysis with machine learning has been a frontier in cancer genomics [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR10">10</xref>, <xref ref-type="bibr" rid="CR11">11</xref>]. Unsupervised clustering approaches (such as iCluster [<xref ref-type="bibr" rid="CR12">12</xref>], SNF [<xref ref-type="bibr" rid="CR13">13</xref>], ANF [<xref ref-type="bibr" rid="CR14">14</xref>], etc.) are popular for multi-omics data analysis as annotated labels are often lacking in biomedical data. Probabilistic models [<xref ref-type="bibr" rid="CR12">12</xref>] and network-based regularization [<xref ref-type="bibr" rid="CR15">15</xref>] have been employed to learn from multi-omics data. Recently, deep learning has been applied to sequencing data [<xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR17">17</xref>], imaging data [<xref ref-type="bibr" rid="CR18">18</xref>], medical records [<xref ref-type="bibr" rid="CR19">19</xref>], etc. However, most existing deep learning methods focused on individual data types instead of integrating multi-omics data. Multi-view learning provides a natural framework for learning from multimodal data. Typical techniques for multi-view learning include co-training, co-regularization, and margin consistency approaches [<xref ref-type="bibr" rid="CR4">4</xref>]. Combining deep learning with multi-view learning more effectively is still active research [<xref ref-type="bibr" rid="CR4">4</xref>]. There are multiple ways to incorporate biological networks as inductive biases into a deep learning model. Besides network regularization approaches, directly encoding biological networks into the model architecture is also possible [<xref ref-type="bibr" rid="CR20">20</xref>, <xref ref-type="bibr" rid="CR21">21</xref>], which usually requires subcellular hierarchical molecular networks as the prior knowledge. Because high-quality human data is lacking (human biological interaction networks such as protein-protein interaction networks are still incomplete and noisy), network regularization approaches are often preferable to directly encoding the noisy interaction network into the model architecture.</p>
      <p>Multi-modality deep learning [<xref ref-type="bibr" rid="CR22">22</xref>] has been successfully applied to integrate audio and video features [<xref ref-type="bibr" rid="CR23">23</xref>] by employing shared feature representations. However, many multi-modality deep learning models still rely on large amounts of training data and do not facilitate knowledge integration. Our method can learn feature and patient embeddings simultaneously with the integration of domain knowledge to learn robust and generalizable deep learning models.</p>
      <p>Many multi-view learning techniques have been proposed [<xref ref-type="bibr" rid="CR24">24</xref>, <xref ref-type="bibr" rid="CR25">25</xref>]. Many of these methods learn transformations that map each view to a latent space and reconstruct the original data from the latent space representation (i.e., adopting an AutoEncoder architecture). Importantly, they may add additional constraints to ensure the latent features for multiple views are highly correlated [<xref ref-type="bibr" rid="CR24">24</xref>]. Our model also adopted the Multi-view AutoEncoder architecture as the model backbone, but we chose different regularization schemes for incorporating domain knowledge as inductive biases into the model. We do not assume the latent spaces learned for each view to be “canonically correlated”. Instead, the learned feature representations should be consistent with the domain knowledge such as gene-gene and miRNA-miRNA interaction networks. As the gene-gene interaction network and the miRNA-miRNA interaction network are very different, the corresponding gene and miRNA feature interactions can be very different as well. Importantly, we are focusing on the multi-omics data, of which each feature (such as a gene) has a clear biological meaning and the feature interactions have been captured as domain knowledge, while many other proposed multi-view learning methods deal with data without “biologically meaningful” features (for example, in image data, individual pixels are not informative at all, but their arrangement and structure do contain information). While other widely used multi-view learning methods [<xref ref-type="bibr" rid="CR22">22</xref>, <xref ref-type="bibr" rid="CR24">24</xref>, <xref ref-type="bibr" rid="CR25">25</xref>] focus on how to effectively utilize feature correlation among different views to improve model performance, our main focus in this paper is to demonstrate that biological interaction networks as an "external" domain knowledge source can be effectively incorporated into deep learning models through network regularization to improve model generalizability for multi-omics data analysis.</p>
      <p>Our main contribution can be summarized as follows. We proposed a Multi-view AutoEncoder model with network constraints for the integrative analysis of multi-omics data. Our model learns good representations for both molecular entities and patients simultaneously and facilitates mining relationships among molecular features and clinical features. Most importantly, we demonstrated that “external” domain knowledge sources such as biological interaction networks can be incorporated into the model as inductive biases, which could improve model generalizability and reduce the risk of overfitting in the “big p, small n” problem. We devised novel network regularizers that will “force” the learned feature representations to be consistent with domain knowledge, effectively reducing the search space for good feature embeddings. We have performed extensive experiments and showed that the models trained with domain knowledge outperformed those without using domain knowledge. Our work provides a proof-of-concept framework for unifying data-driven and knowledge-driven approaches for mining multi-omics data with biological knowledge.</p>
    </sec>
  </sec>
  <sec id="Sec3">
    <title>Methods and implementation</title>
    <p>Our method builds upon matrix factorization [<xref ref-type="bibr" rid="CR5">5</xref>], multi-view learning, and deep learning. We will describe each component in the following section.</p>
    <p><bold>Some notations</bold>Given <italic>N</italic> samples and <italic>V</italic> types of -omics data, we can often represent the data using <italic>V</italic> sample-feature matrices: <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathbf {M}^{(i)} \in \mathbb {R}^{N \times p^{(i)}}, i=1,2,\cdots, V$\end{document}</tex-math><mml:math id="M2"><mml:msup><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mspace width="0.3em"/><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:math><inline-graphic xlink:href="12864_2019_6285_Article_IEq1.gif"/></alternatives></inline-formula>. Each matrix corresponds to one data view, and <italic>p</italic><sup>(<italic>i</italic>)</sup> is the feature dimension for view <italic>i</italic>.</p>
    <p>Before describing Multi-view Factorization AutoEncoder, we first discuss how to process individual views. For ease of description, we drop the superscript <sup>(·)</sup> when dealing with a single view. For matrix <bold>M</bold>,<italic>M</italic><sub><italic>ij</italic></sub> represents the element of <italic>i</italic>th row and <italic>j</italic>th column, <bold>M</bold><sub><italic>i</italic>,·</sub> represents the <italic>i</italic>th row vector, and <bold>M</bold><sub>·,<italic>j</italic></sub> represents <italic>j</italic>th column vector.</p>
    <p>Let <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathbf {M} \in \mathbb {R}^{N \times p}$\end{document}</tex-math><mml:math id="M4"><mml:mi mathvariant="bold">M</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12864_2019_6285_Article_IEq2.gif"/></alternatives></inline-formula> be a feature matrix, with each row corresponding to a sample and each column corresponding to a feature. The features are often not independent. We represent the interactions among these features with a network <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathbf {G} \in \mathbb {R}^{p \times p}$\end{document}</tex-math><mml:math id="M6"><mml:mi mathvariant="bold">G</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>×</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12864_2019_6285_Article_IEq3.gif"/></alternatives></inline-formula>. For instance, if these features correspond to protein expressions, then <bold>G</bold> will be a protein-protein interaction network, which is available in public databases such as STRING [<xref ref-type="bibr" rid="CR26">26</xref>] and Reactome [<xref ref-type="bibr" rid="CR27">27</xref>]. <bold>G</bold> can be an unweighted graph or a weighted graph with non-negative elements. Let <bold>D</bold> be a diagonal matrix with <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$D_{ii}=\sum _{j=1}^{p} G_{ij}$\end{document}</tex-math><mml:math id="M8"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">ii</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">ij</mml:mtext></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12864_2019_6285_Article_IEq4.gif"/></alternatives></inline-formula>, then the graph Laplacian of <bold>G</bold> is <bold>L</bold><sub><italic>G</italic></sub>=<bold>D</bold>−<bold>G</bold>.</p>
    <sec id="Sec5">
      <title>Low-rank matrix factorization</title>
      <p>Matrix factorization techniques [<xref ref-type="bibr" rid="CR5">5</xref>] are widely used for clustering and dimensionality reduction. In many real-world applications, <bold>M</bold> often has a low rank. As a result, low-rank matrix factorization can be used for dimensionality reduction and clustering:</p>
      <p><bold>M</bold>≈<bold>X</bold><bold>Y</bold>, where <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathbf {X} \in \mathbb {R}^{N \times k}, \mathbf {Y} \in \mathbb {R}^{k \times p}, k&lt; p$\end{document}</tex-math><mml:math id="M10"><mml:mi mathvariant="bold">X</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>×</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>p</mml:mi></mml:math><inline-graphic xlink:href="12864_2019_6285_Article_IEq5.gif"/></alternatives></inline-formula></p>
      <p>Some additional constraints are often added as regularizers in the objective function or enforced in the learning algorithm to find a good solution {<bold>X</bold>,<bold>Y</bold>}. For instance, when <bold>M</bold> is non-negative, Non-negative Matrix Factorization (NMF) [<xref ref-type="bibr" rid="CR28">28</xref>] is often a “natural” choice to ensure both <bold>X</bold> and <bold>Y</bold> are non-negative.</p>
      <p>Generally speaking, the objective function can be formulated as follows:
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \arg \min_{\mathbf{X}, \mathbf{Y}} \left\Vert \mathbf{M} - \mathbf{X} \mathbf{Y} \right\Vert^{2}_{F} + \lambda R(\mathbf{X}, \mathbf{Y})  $$ \end{document}</tex-math><mml:math id="M12"><mml:mo>arg</mml:mo><mml:munder><mml:mrow><mml:mo>min</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow></mml:munder><mml:munderover><mml:mrow><mml:mfenced close="∥" open="∥" separators=""><mml:mrow><mml:mi mathvariant="bold">M</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:munderover><mml:mo>+</mml:mo><mml:mi>λR</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>)</mml:mo></mml:math><graphic xlink:href="12864_2019_6285_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>In Eq. <xref rid="Equ1" ref-type="">1</xref>, <italic>R</italic>(<bold>X</bold>,<bold>Y</bold>) is a regularization term for <bold>X</bold> and <bold>Y</bold>. For instance, <italic>R</italic>(<bold>X</bold>,<bold>Y</bold>) can include <italic>L</italic><sub>1</sub> and <italic>L</italic><sub>2</sub> norms for <bold>X</bold> and <bold>Y</bold>. In addition, structural constraints based on biological interaction networks can also be incorporated into <italic>R</italic>(<bold>X</bold>,<bold>Y</bold>).</p>
      <p><bold>Interpretation</bold><inline-formula id="IEq6"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathbf {X} \in \mathbb {R}^{N \times k}$\end{document}</tex-math><mml:math id="M14"><mml:mi mathvariant="bold">X</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12864_2019_6285_Article_IEq6.gif"/></alternatives></inline-formula> can be regarded as a sample-factor matrix and the inherent non-redundant representation of <italic>N</italic> samples, with each column corresponding to an independent factor. These <italic>k</italic> factors are often latent variables. <inline-formula id="IEq7"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathbf {Y} \in \mathbb {R}^{k \times p}$\end{document}</tex-math><mml:math id="M16"><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>×</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12864_2019_6285_Article_IEq7.gif"/></alternatives></inline-formula> can be seen as a linear transformation matrix. The <italic>k</italic> rows of <bold>Y</bold> can be regarded as a basis for the underlying factor space. The observable feature matrix <bold>M</bold> is generated by a linear transformation <bold>Y</bold> from <bold>X</bold>. In a sense, this formulation can be seen as a shallow linear generative model.</p>
      <p><bold>Limitations</bold>The limitations of matrix factorization techniques often stem from their “shallow” linear structure with a limited representation power. In many real-world applications, however, we need to learn a complex nonlinear transformation. Deep neural networks are often good at approximating any complex nonlinear transformations with appropriate training on a sufficiently large dataset.</p>
    </sec>
    <sec id="Sec8">
      <title>Non-linear factorization with AutoEncoder</title>
      <p>As simple matrix factorization techniques are limited to model complex nonlinear relationships, we can use an Autoencoder to reconstruct the observable sample-feature matrix <bold>M</bold>, as it can approximate more complex nonlinear transformations well.</p>
      <p>The entire Autoencoder is a multi-layer neural network with a encoder and a decoder. We use a neural network with parameter <italic>Θ</italic><sub><italic>e</italic></sub> as the encoder:
<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ Encoder(\mathbf{M}, \mathbf{\Theta_{e}}) = \mathbf{X} \in \mathbb{R}^{N \times k}  $$ \end{document}</tex-math><mml:math id="M18"><mml:mtext mathvariant="italic">Encoder</mml:mtext><mml:mo>(</mml:mo><mml:mi mathvariant="bold">M</mml:mi><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:math><graphic xlink:href="12864_2019_6285_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>Here <bold>X</bold> can be regarded as a factor matrix containing the essential information for all <italic>N</italic> samples. The encoder network will transform the observable sample-feature matrix <bold>M</bold> to its latent representation <bold>X</bold>. The decoder reconstructs the original data from the latent representation.
<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$  Decoder(\mathbf{X}, \mathbf{\Theta_{d}}) = \mathbf{Z} \in \mathbb{R}^{N \times p}  $$ \end{document}</tex-math><mml:math id="M20"><mml:mtext mathvariant="italic">Decoder</mml:mtext><mml:mo>(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="bold">Z</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:math><graphic xlink:href="12864_2019_6285_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>In our framework, for the convenience of incorporating biological interaction networks into the framework, the encoder (Eq. <xref rid="Equ2" ref-type="">2</xref>) contains all layers but the last one, and the decoder is the last linear layer. The parameter of decoder (Eq. <xref rid="Equ3" ref-type="">3</xref>) is a linear transformation matrix same as in matrix factorization:
<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$  \mathbf{\Theta_{d}} = \mathbf{Y} \in \mathbb{R}^{k \times p}  $$ \end{document}</tex-math><mml:math id="M22"><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mo>=</mml:mo><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>×</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:math><graphic xlink:href="12864_2019_6285_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>The input sample-feature matrix can be reconstructed as
<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$  \mathbf{Z} = Encoder(\mathbf{M}, \mathbf{\Theta_{e}}) \cdot \mathbf{Y} = \mathbf{X} \mathbf{Y}  $$ \end{document}</tex-math><mml:math id="M24"><mml:mi mathvariant="bold">Z</mml:mi><mml:mo>=</mml:mo><mml:mtext mathvariant="italic">Encoder</mml:mtext><mml:mo>(</mml:mo><mml:mi mathvariant="bold">M</mml:mi><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mo>)</mml:mo><mml:mo>·</mml:mo><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mi mathvariant="bold">Y</mml:mi></mml:math><graphic xlink:href="12864_2019_6285_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>The reconstruction error can be computed as: <inline-formula id="IEq8"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\left \Vert \mathbf {M} - \mathbf {Z} \right \Vert ^{2}_{F}$\end{document}</tex-math><mml:math id="M26"><mml:msubsup><mml:mrow><mml:mfenced close="∥" open="∥" separators=""><mml:mrow><mml:mi mathvariant="bold">M</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12864_2019_6285_Article_IEq8.gif"/></alternatives></inline-formula>.</p>
      <p>Different from matrix factorization–which can be regarded as a one-layer AutoEncoder, the encoder in our framework is a multi-layer neural network that can learn complex nonlinear transformations through backpropagation. Moreover, the encoder output <bold>X</bold> can be regarded as the learned patient representations for <italic>N</italic> samples, and <bold>Y</bold> can be seen as the learned feature representations. With the learned patient and feature representations, we can calculate patient similarity networks and feature interaction networks, and add network regularizers to the objective function.</p>
    </sec>
    <sec id="Sec9">
      <title>Incorporate biological knowledge as network regularizers</title>
      <p>We aim to incorporate biological knowledge such as molecular interaction networks into our model as inductive biases to increase model generalizability. Denote <inline-formula id="IEq9"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathbf {G} \in \mathbb {R}^{p \times p}$\end{document}</tex-math><mml:math id="M28"><mml:mi mathvariant="bold">G</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>×</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12864_2019_6285_Article_IEq9.gif"/></alternatives></inline-formula> as the interaction matrix among <italic>p</italic> genomic features, which can be obtained from biological databases such as STRING [<xref ref-type="bibr" rid="CR26">26</xref>] and Reactome [<xref ref-type="bibr" rid="CR27">27</xref>].</p>
      <p>Since our model can learn a feature representation <bold>Y</bold>, this representation should ideally be “consistent” with the biological interaction network corresponding to these features. We use a graph Laplacian regularizer to minimize the inconsistency between the learned feature representation <bold>Y</bold> and the feature interaction network <bold>G</bold>:
<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$  Trace\left(\mathbf{Y} \mathbf{L}_{G} \mathbf{Y}^{T}\right) = \frac{1}{2} \sum_{i=1}^{p} \sum_{j=1}^{p} G_{ij} \lVert \mathbf{Y}_{\cdot, i} - \mathbf{Y}_{\cdot, j} \rVert^{2}  $$ \end{document}</tex-math><mml:math id="M30"><mml:mtext mathvariant="italic">Trace</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">ij</mml:mtext></mml:mrow></mml:msub><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>·</mml:mo><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>·</mml:mo><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math><graphic xlink:href="12864_2019_6285_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula></p>
      <p><bold>L</bold><sub><italic>G</italic></sub> is the graph Laplacian matrix of <bold>G</bold> in Eq. <xref rid="Equ6" ref-type="">6</xref>. <italic>G</italic><sub><italic>ij</italic></sub>≥0 captures how “similar” feature <italic>i</italic> and feature <italic>j</italic> are. Each feature <italic>i</italic> is represented as a <italic>k</italic>-dimensional vector <bold>Y</bold><sub>·,<italic>i</italic></sub>. We can calculate the Euclidean distance between feature <italic>i</italic> and <italic>j</italic> as ∥<bold>Y</bold><sub>·,<italic>i</italic></sub>−<bold>Y</bold><sub>·,<italic>j</italic></sub>∥. The term <italic>T</italic><italic>r</italic><italic>a</italic><italic>c</italic><italic>e</italic>(<bold>Y</bold><bold>L</bold><sub><italic>G</italic></sub><bold>Y</bold><sup><italic>T</italic></sup>) is a surrogate for measuring the inconsistency between the learned feature representation <bold>Y</bold> and the known feature interaction network <bold>G</bold>. When <bold>Y</bold> is highly inconsistent with <bold>G</bold>, the loss term <italic>T</italic><italic>r</italic><italic>a</italic><italic>c</italic><italic>e</italic>(<bold>Y</bold><bold>L</bold><sub><italic>G</italic></sub><bold>Y</bold><sup><italic>T</italic></sup>), which accounts for the level of inconsistency between the learned feature representation and the biological interaction network, will be large. Therefore, minimizing the loss function can effectively reduce the inconsistency between the learned feature representation and the biological interaction network.</p>
      <p>The objective function incorporating biological interaction networks through the graph Laplacian regularizer is as follows:
<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$  \arg \min_{\mathbf{\Theta_{e}}, \mathbf{Y}} \left\Vert \mathbf{M} - \mathbf{Z} \right\Vert^{2}_{F} + \alpha \ Trace\left(\mathbf{Y} \cdot \mathbf{L}_{G} \cdot \mathbf{Y}^{T}\right)  $$ \end{document}</tex-math><mml:math id="M32"><mml:mo>arg</mml:mo><mml:munder><mml:mrow><mml:mo>min</mml:mo></mml:mrow><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow></mml:munder><mml:munderover><mml:mrow><mml:mfenced close="∥" open="∥" separators=""><mml:mrow><mml:mi mathvariant="bold">M</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:munderover><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mspace width="1em"/><mml:mtext mathvariant="italic">Trace</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12864_2019_6285_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>In Eq. <xref rid="Equ7" ref-type="">7</xref>, <italic>α</italic>≥0 is a hyperparameter as the weight for the network regularization term. In practice, we normalize <bold>G</bold> and <bold>Y</bold> so that the <italic>T</italic><italic>r</italic><italic>a</italic><italic>c</italic><italic>e</italic>(<bold>Y</bold>·<bold>L</bold><sub><italic>G</italic></sub>·<bold>Y</bold><sup><italic>T</italic></sup>) is within the range of [0,1]. In the implementation of our model, we set <inline-formula id="IEq10"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\lVert \mathbf {G} \rVert _{F} = 1, \lVert \mathbf {Y}_{\cdot, i} \rVert = \frac {1}{\sqrt {p}}, i=1,2,\cdots, p$\end{document}</tex-math><mml:math id="M34"><mml:mo>∥</mml:mo><mml:mi mathvariant="bold">G</mml:mi><mml:msub><mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>·</mml:mo><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∥</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mspace width="0.3em"/><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:math><inline-graphic xlink:href="12864_2019_6285_Article_IEq10.gif"/></alternatives></inline-formula> (this also means ∥<bold>Y</bold>∥<sub><italic>F</italic></sub>=1). This facilitates easy multi-view integration since all the network regularizers from individual views are on the same scale.</p>
      <sec id="Sec10">
        <title>Measuring feature similarity with mutual information</title>
        <p>Eq. <xref rid="Equ6" ref-type="">6</xref> uses Euclidean distance to measure the dissimilarity between learned feature representations. Euclidean distance relies on the inner product operator, which is essentially linear. The fact that two molecular entities interact with each other does not imply that they should have very similar feature representations or a small Euclidean distance. Mutual information can be a better metric quantifying if two molecular entities interact with each other.</p>
        <p>Let’s briefly review the definition of mutual information between two random variables <italic>X</italic> and <italic>Y</italic>.
<disp-formula id="Equa"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} I(X, Y) &amp;= H(X) - H(X|Y) \\ &amp;= H(Y) - H(Y|X) \\ &amp;= H(X) + H(Y) - H(X,Y) \end{array} $$ \end{document}</tex-math><mml:math id="M36"><mml:mtable class="align" columnalign="left"><mml:mtr><mml:mtd class="align-1"><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:mtd><mml:mtd class="align-2"><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>|</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo><mml:mspace width="2em"/></mml:mtd><mml:mtd><mml:mspace width="2em"/></mml:mtd></mml:mtr><mml:mtr><mml:mtd class="align-1"/><mml:mtd class="align-2"><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo><mml:mspace width="2em"/></mml:mtd><mml:mtd><mml:mspace width="2em"/></mml:mtd></mml:mtr><mml:mtr><mml:mtd class="align-1"/><mml:mtd class="align-2"><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo><mml:mspace width="2em"/></mml:mtd><mml:mtd><mml:mspace width="2em"/></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12864_2019_6285_Article_Equa.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>For discrete random variable <italic>X</italic>∼<italic>P</italic>(<italic>x</italic>) (<italic>P</italic>(<italic>x</italic>) is the discrete probability distribution of <italic>X</italic>), the entropy of <italic>X</italic> is defined as <inline-formula id="IEq11"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$H(X) = \sum _{x} P(x) log\frac {1}{P(x)}$\end{document}</tex-math><mml:math id="M38"><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munder><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mtext mathvariant="italic">log</mml:mtext><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="12864_2019_6285_Article_IEq11.gif"/></alternatives></inline-formula>.</p>
        <p>The observed sample-feature matrix <inline-formula id="IEq12"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathbf {M} \in \mathbb {R}^{N \times p}$\end{document}</tex-math><mml:math id="M40"><mml:mi mathvariant="bold">M</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12864_2019_6285_Article_IEq12.gif"/></alternatives></inline-formula> can be used to measure the pair-wise mutual information scores between feature <italic>i</italic> and <italic>j</italic>: <italic>M</italic><italic>u</italic><italic>t</italic><italic>u</italic><italic>a</italic><italic>l</italic><italic>I</italic><italic>n</italic><italic>f</italic><italic>o</italic>(<bold>M</bold><sub>·,<italic>i</italic></sub>,<bold>M</bold><sub>·,<italic>j</italic></sub>). However, due to measurement noise and error, this may not be accurate.</p>
        <p>Ideally, the reconstructed signal with the proposed autoencoder model should reduce the noise in the data. Thus we can calculate pair-wise normalized mutual information scores using the reconstructed signal <bold>Z</bold> (Eq. <xref rid="Equ5" ref-type="">5</xref>):
<disp-formula id="Equb"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\mathbf{K} = PairwiseMutualInformation(\mathbf{Z}) \in \mathbb{R}^{p \times p} $$ \end{document}</tex-math><mml:math id="M42"><mml:mrow><mml:mi mathvariant="bold">K</mml:mi><mml:mo>=</mml:mo><mml:mtext mathvariant="italic">PairwiseMutualInformation</mml:mtext><mml:mo>(</mml:mo><mml:mi mathvariant="bold">Z</mml:mi><mml:mo>)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>×</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><graphic xlink:href="12864_2019_6285_Article_Equb.gif" position="anchor"/></alternatives></disp-formula></p>
        <p><bold>K</bold> can be regarded as a learned similarity matrix based on mutual information. Again we want to ensure that the learned similarity matrix is consistent with the known biological interaction network <bold>G</bold>. We can estimate the consistency between <bold>G</bold> and <bold>K</bold> as:
<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$  \left \lVert \mathbf{G} \odot \mathbf{K} \right \rVert^{2}_{F}  $$ \end{document}</tex-math><mml:math id="M44"><mml:msubsup><mml:mrow><mml:mfenced close="‖" open="‖" separators=""><mml:mrow><mml:mi mathvariant="bold">G</mml:mi><mml:mo>⊙</mml:mo><mml:mi mathvariant="bold">K</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><graphic xlink:href="12864_2019_6285_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>⊙ is element-wise matrix multiplication. As <bold>G</bold> and <bold>K</bold> are normalized feature interaction network and pairwise feature mutual information matrix, the norm of their element-wise multiplication can be an estimate of the consistency between <bold>G</bold> and <bold>K</bold>. We inject this mutual information regularization term into Eq. <xref rid="Equ9" ref-type="">9</xref>:
<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$  \begin{aligned} \underset{\mathbf{\Theta_{e}}, \mathbf{Y}}{\arg \min} &amp;\left\Vert \mathbf{M} - Encoder(\mathbf{M}, \mathbf{\Theta_{e}}) \cdot \mathbf{Y} \right\Vert^{2}_{F} \\ &amp;+ \alpha \ Trace\left(\mathbf{Y} \cdot \mathbf{L}_{G} \cdot \mathbf{Y}^{T}\right) \\ &amp;- \gamma \left \lVert \mathbf{G} \odot \mathbf{K} \right \rVert^{2}_{F} \end{aligned}  $$ \end{document}</tex-math><mml:math id="M46"><mml:mtable><mml:mtr><mml:mtd><mml:munder><mml:mrow><mml:mo>arg</mml:mo><mml:mo>min</mml:mo></mml:mrow><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow></mml:munder></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mfenced close="∥" open="∥" separators=""><mml:mrow><mml:mi mathvariant="bold">M</mml:mi><mml:mo>−</mml:mo><mml:mtext mathvariant="italic">Encoder</mml:mtext><mml:mo>(</mml:mo><mml:mi mathvariant="bold">M</mml:mi><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mo>)</mml:mo><mml:mo>·</mml:mo><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mspace width="1em"/><mml:mtext mathvariant="italic">Trace</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:msubsup><mml:mrow><mml:mfenced close="‖" open="‖" separators=""><mml:mrow><mml:mi mathvariant="bold">G</mml:mi><mml:mo>⊙</mml:mo><mml:mi mathvariant="bold">K</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12864_2019_6285_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula></p>
        <p><italic>α</italic>,<italic>γ</italic> are non-negative hyperparameters. There are numerical methods to measure the mutual information between two continuous high-dimensional random variables. The simplest approach is to divide the continuous space into small bins and discretize the variables with these bins. In order to estimate mutual information from data accurately, a large sample size is needed. Due to the difficulty in accurately calculating mutual information based on a limited number of data points, we do not include mutual information term in the following discussion and leave this for future work.</p>
      </sec>
    </sec>
    <sec id="Sec11">
      <title>Multi-view factorization AutoEncoder with network constraints</title>
      <p>We have given the objective function for a single view in Eq. <xref rid="Equ7" ref-type="">7</xref>. For multiple views, the objective can be formulated as follows:
<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$  \begin{aligned} \underset{\{\mathbf{\Theta_{e}}^{(v)}, \mathbf{Y}^{(v)}\}}{\arg \min} &amp; \sum_{v=1}^{V} \left(\left\Vert \mathbf{M}^{(v)} - Encoder\left(\mathbf{M}^{(v)}, \mathbf{\Theta_{e}}^{(v)}\right) \cdot \mathbf{Y}^{(v)} \right\Vert^{2}_{F} \right.\\ &amp;\left. + \alpha \ Trace\left(\mathbf{Y}^{(v)} \cdot \mathbf{L}_{G^{(v)}} \cdot \mathbf{Y}^{(v)^{T}}\right) \right) \end{aligned}  $$ \end{document}</tex-math><mml:math id="M48"><mml:mtable><mml:mtr><mml:mtd><mml:munder><mml:mrow><mml:mo>arg</mml:mo><mml:mo>min</mml:mo></mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>}</mml:mo></mml:mrow></mml:munder></mml:mtd><mml:mtd><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:munderover><mml:mfenced close="" open="(" separators=""><mml:mrow><mml:msubsup><mml:mrow><mml:mfenced close="∥" open="∥" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mtext mathvariant="italic">Encoder</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>·</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfenced close=")" open="" separators=""><mml:mrow><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mspace width="1em"/><mml:mtext mathvariant="italic">Trace</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12864_2019_6285_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>Note we use a separate autoencoder for each view. We try to minimize the reconstruction loss and feature interaction network regularizers for all views in Eq. <xref rid="Equ10" ref-type="">10</xref>.</p>
      <p>Here <italic>E</italic><italic>n</italic><italic>c</italic><italic>o</italic><italic>d</italic><italic>e</italic><italic>r</italic>(<bold>M</bold><sup>(<italic>v</italic>)</sup>,<italic>Θ</italic><sub><italic>e</italic></sub><sup>(<italic>v</italic>)</sup>)=<bold>X</bold><sup>(<italic>v</italic>)</sup> can be seen as the learned latent representation for <italic>N</italic> samples. We can derive patient similarity network <bold>S</bold><sup>(<italic>v</italic>)</sup> (which can also be used for clustering patients into groups) from <bold>X</bold><sup>(<italic>v</italic>)</sup>. Multiple approaches can be employed to calculate a patient similarity network. For example, we can use cosine similarity:
<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ S_{ij}=\frac{\lvert \mathbf{X}_{i, \cdot} \cdot \mathbf{X}_{j, \cdot} \rvert}{\lVert \mathbf{X}_{i, \cdot}\rVert \cdot \lVert \mathbf{X}_{j, \cdot} \rVert}  $$ \end{document}</tex-math><mml:math id="M50"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">ij</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mo>·</mml:mo></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mo>·</mml:mo></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mo>·</mml:mo></mml:mrow></mml:msub><mml:mo>∥</mml:mo><mml:mo>·</mml:mo><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mo>·</mml:mo></mml:mrow></mml:msub><mml:mo>∥</mml:mo></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="12864_2019_6285_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>We can get a patient similarity network <bold>S</bold><sup>(<italic>v</italic>)</sup> for each view <italic>v</italic> (Eq. <xref rid="Equ11" ref-type="">11</xref> omits the superscript for clarity). Moreover, the outputs of multiple encoders can be “fused” together for supervised learning.
<disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \mathbf{X} = \sum_{v=1}^{V} \mathbf{X}^{(v)} = \sum_{v=1}^{V}Encoder\left(\mathbf{M}^{(v)}, \mathbf{\Theta_{e}}^{(v)}\right)  $$ \end{document}</tex-math><mml:math id="M52"><mml:mi mathvariant="bold">X</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:munderover><mml:mtext mathvariant="italic">Encoder</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12864_2019_6285_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>This idea is similar to ResNet [<xref ref-type="bibr" rid="CR29">29</xref>]. Another approach is to concatenate all views together like DenseNet [<xref ref-type="bibr" rid="CR30">30</xref>]. We have tried using both in our experiments and the results are not significantly different.</p>
      <p>With the fused view <bold>X</bold>, we can again calculate the patient similarity network <bold>S</bold><sub><italic>X</italic></sub> using Eq. <xref rid="Equ11" ref-type="">11</xref>. Moreover, since <bold>S</bold><sub><italic>X</italic></sub>, and<bold>S</bold><sup>(<italic>v</italic>)</sup>,<italic>v</italic>=1,2,⋯,<italic>V</italic> are for the same set of patients, we can fuse them together using affinity network fusion [<xref ref-type="bibr" rid="CR14">14</xref>]:
<disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$  \mathbf{S} = \frac{1}{V+1} \left(\sum_{i=1}^{V} \mathbf{S}^{(v)} + \mathbf{S}_{X} \right)  $$ \end{document}</tex-math><mml:math id="M54"><mml:mi mathvariant="bold">S</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12864_2019_6285_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>Similar to the feature interaction network regularizer (Eq. <xref rid="Equ6" ref-type="">6</xref>), we also include a regularization term on the patient view similarity:
<disp-formula id="Equ14"><label>14</label><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$  Trace\left(\mathbf{X}^{(v)^{T}} \cdot \mathbf{L}_{S} \cdot \mathbf{X}^{(v)}\right)  $$ \end{document}</tex-math><mml:math id="M56"><mml:mtext mathvariant="italic">Trace</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12864_2019_6285_Article_Equ14.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>Here <bold>L</bold><sub><italic>S</italic></sub> is the graph Laplacian of <bold>S</bold>. Adding this term to Eq. <xref rid="Equ10" ref-type="">10</xref>, we get the new objective function:
<disp-formula id="Equ15"><label>15</label><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$  \begin{aligned} \underset{\{\mathbf{\Theta_{e}}^{(v)}, \mathbf{Y}^{(v)}\}}{\arg \min} \sum_{v=1}^{V} &amp;\left(\left\Vert \mathbf{M}^{(v)} - Encoder\left(\mathbf{M}^{(v)}, \mathbf{\Theta_{e}}^{(v)}\right) \cdot \mathbf{Y}^{(v)} \right\Vert^{2}_{F} \right.\\ &amp; + \alpha \ Trace\left(\mathbf{Y}^{(v)} \cdot \mathbf{L}_{G^{(v)}} \cdot \mathbf{Y}^{(v)^{T}}\right)\\ &amp; \left.+ \beta\ Trace\left(\mathbf{X}^{(v)^{T}} \cdot \mathbf{L}_{S} \cdot \mathbf{X}^{(v)}\right)\right) \end{aligned}  $$ \end{document}</tex-math><mml:math id="M58"><mml:mtable><mml:mtr><mml:mtd><mml:munder><mml:mrow><mml:mo>arg</mml:mo><mml:mo>min</mml:mo></mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>}</mml:mo></mml:mrow></mml:munder><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:munderover></mml:mtd><mml:mtd><mml:mfenced close="" open="(" separators=""><mml:mrow><mml:msubsup><mml:mrow><mml:mfenced close="∥" open="∥" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mtext mathvariant="italic">Encoder</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>·</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mspace width="1em"/><mml:mtext mathvariant="italic">Trace</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfenced close=")" open="" separators=""><mml:mrow><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mspace width="1em"/><mml:mtext mathvariant="italic">Trace</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12864_2019_6285_Article_Equ15.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>For each type of -omic data, there is one corresponding feature interaction network <bold>G</bold><sup>(<italic>v</italic>)</sup>. Different molecular interaction networks involve distinct feature sets and thus cannot be directly merged. However, patient similarity networks are about the same set of patients, and therefore can be combined to get a fused patient similarity network <bold>S</bold> using techniques such as affinity network fusion [<xref ref-type="bibr" rid="CR14">14</xref>]. Our framework uses both molecular interaction networks and patient similarity networks for regularized learning.</p>
    </sec>
    <sec id="Sec12">
      <title>Supervised learning with multi-view factorization autoencoder</title>
      <p>With multi-view data and feature interaction networks, our framework with the objective function Eq. <xref rid="Equ7" ref-type="">7</xref> can be used for unsupervised learning. When labeled data is available, we can use our model for supervised learning by adding another loss term to Eq. <xref rid="Equ7" ref-type="">7</xref>:
<disp-formula id="Equ16"><label>16</label><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$  \begin{aligned} &amp;\underset{\{\mathbf{\Theta_{e}}^{(v)}, \mathbf{Y}^{(v)}\}}{\arg \min} \mathcal{L}(\mathbf{T}, \left(\sum_{v=1}^{V}Encoder\left(\mathbf{M}^{(v)}, \mathbf{\Theta_{e}}^{(v)}\right)\right) \cdot \mathbf{C})\\ &amp;+ \eta \sum_{v=1}^{V} \left(\left\Vert \mathbf{M}^{(v)} - Encoder\left(\mathbf{M}^{(v)}, \mathbf{\Theta_{e}}^{(v)}\right) \cdot \mathbf{Y}^{(v)} \right\Vert^{2}_{F}\right)\\ &amp; + \alpha \sum_{v=1}^{V} Trace\left(\mathbf{Y}^{(v)} \cdot \mathbf{L}_{G^{(v)}} \cdot \mathbf{Y}^{(v)^{T}}\right) \\ &amp; + \beta\ \sum_{v=1}^{V} Trace\left(\mathbf{X}^{(v)^{T}} \cdot \mathbf{L}_{S} \cdot \mathbf{X}^{(v)}\right) \end{aligned}  $$ \end{document}</tex-math><mml:math id="M60"><mml:mtable><mml:mtr><mml:mtd><mml:munder><mml:mrow><mml:mo>arg</mml:mo><mml:mo>min</mml:mo></mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>}</mml:mo></mml:mrow></mml:munder><mml:mi mathvariant="script">ℒ</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">T</mml:mi><mml:mo>,</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:munderover><mml:mtext mathvariant="italic">Encoder</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>·</mml:mo><mml:mi mathvariant="bold">C</mml:mi><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:munderover><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msubsup><mml:mrow><mml:mfenced close="∥" open="∥" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mtext mathvariant="italic">Encoder</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>·</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:munderover><mml:mtext mathvariant="italic">Trace</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mspace width="1em"/><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:munderover><mml:mtext mathvariant="italic">Trace</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12864_2019_6285_Article_Equ16.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>The first term <inline-formula id="IEq13"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal {L}(\mathbf {T}, \left (\sum _{v=1}^{V}Encoder\left (\mathbf {M}^{(v)}, \mathbf {\Theta _{e}}^{(v)}\right)\right) \cdot \mathbf {C})$\end{document}</tex-math><mml:math id="M62"><mml:mi mathvariant="script">ℒ</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">T</mml:mi><mml:mo>,</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:munderover><mml:mtext mathvariant="italic">Encoder</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>·</mml:mo><mml:mi mathvariant="bold">C</mml:mi><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="12864_2019_6285_Article_IEq13.gif"/></alternatives></inline-formula> is the classification loss (e.g., cross entropy loss) or regression loss (e.g., mean squared error for continuous target variables) for supervised learning. <bold>T</bold> is the true class labels or other target variables available for training the model.</p>
      <p>As in Eq. <xref rid="Equ12" ref-type="">12</xref>, <inline-formula id="IEq14"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\sum _{v=1}^{V}Encoder\left (\mathbf {M}^{(v)}, \mathbf {\Theta _{e}}^{(v)}\right)$\end{document}</tex-math><mml:math id="M64"><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:munderover><mml:mtext mathvariant="italic">Encoder</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12864_2019_6285_Article_IEq14.gif"/></alternatives></inline-formula> is the sum of the last hidden layers of <italic>V</italic> autoencoders. This also represents the learned patient representations combining multiple views. <bold>C</bold> is the weights for the last fully connected layer typically used in neural network models for classification tasks. The second term <inline-formula id="IEq15"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\sum _{v=1}^{V} \left (\left \Vert \mathbf {M}^{(v)} - Encoder\left (\mathbf {M}^{(v)}, \mathbf {\Theta _{e}}^{(v)}\right) \cdot \mathbf {Y}^{(v)} \right \Vert ^{2}_{F}\right)$\end{document}</tex-math><mml:math id="M66"><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:munderover><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msubsup><mml:mrow><mml:mfenced close="∥" open="∥" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mtext mathvariant="italic">Encoder</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>·</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12864_2019_6285_Article_IEq15.gif"/></alternatives></inline-formula> is the reconstruction loss for all the submodule autoencoders. The third and four terms are the graph Laplacian constraints for molecular interaction networks and patient similarity networks as in Eq. <xref rid="Equ6" ref-type="">6</xref> and Eq. <xref rid="Equ14" ref-type="">14</xref>. <italic>η</italic>,<italic>α</italic>,<italic>β</italic> are non-negative hyperparameters adjusting the weights of the reconstruction loss, feature interaction network loss, and patient similarity network loss.</p>
      <p>A simple illustration of the whole framework combining two views with two-hidden-layer autoencoders is depicted in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. The whole framework is end-to-end differentiable. We implemented the model using PyTorch (<ext-link ext-link-type="uri" xlink:href="https://github.com/BeautyOfWeb/Multiview-AutoEncoder">https://github.com/BeautyOfWeb/Multiview-AutoEncoder</ext-link>).
<fig id="Fig1"><label>Fig. 1</label><caption><p>A simple illustration of the proposed framework with two data views, each with an encoder and a decoder. Different views are fused in the latent space and the fused view is used for supervised learning. Feature interaction networks are incorporated as regularizers into the training objective</p></caption><graphic xlink:href="12864_2019_6285_Fig1_HTML" id="MO1"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec13">
    <title>Results and discussion</title>
    <sec id="Sec14">
      <title>Datasets</title>
      <p>We downloaded and processed two datasets from The Cancer Genome Atlas (TCGA): Bladder Urothelial Carcinoma (BLCA) and Brain Lower Grade Glioma (LGG). 338 patients from the BLCA project and 423 patients from the LGG project were selected for downstream analysis, all of which have gene expression, miRNA expression, protein expression, and DNA methylation as well as clinical data available.</p>
      <sec id="Sec15">
        <title>Target clinical variable</title>
        <p>The main target variable is the Progression-Free Interval (PFI) event. PFI is a derived clinical (binary) outcome endpoint [<xref ref-type="bibr" rid="CR31">31</xref>], which is relatively accurate and is recommended to use for predictive tasks [<xref ref-type="bibr" rid="CR31">31</xref>]. PFI=1 implies the treatment outcome is unfavorable. For example, the patient had a new tumor event in a fixed period, such as a progression of disease, local recurrence, distant metastasis, new primary tumors, or died with cancer without a new tumor event. PFI=0 means the patient did not have a new tumor event or was censored in a fixed period. We are trying to predict the Progression-Free Interval (PFI) event using four types of -omics data (i.e., gene expression, miRNA expression, protein expression, and DNA methylation). As this is a binary classification problem, we used Average Precision and AUC (Area Under the ROC Curve) score as the main metrics to evaluate classification performances. The results using other metrics are similar.</p>
      </sec>
      <sec id="Sec16">
        <title>Data preprocessing</title>
        <p>We performed log transformation and removed outliers for gene features. Four thousand nine hundred forty two gene features were kept for downstream analysis after filtering out genes with either low mean or low variance. We removed features with low mean and variance for DNA methylation data. Four thousand seven hundred fifty three methylation features (i.e., beta values associated with CpG islands) were selected for analysis. We also performed log transformation and removed outliers for miRNA features. We removed nine protein expression features with NA values. In total, 10,546 features were selected for downstream analysis. For each type of features, we normalize it to have zero mean and standard deviation equal to 1.</p>
        <sec id="Sec17">
          <title>Molecular interaction networks</title>
          <p>We downloaded the protein-protein interaction network from the STRING (v10.5) database [<xref ref-type="bibr" rid="CR26">26</xref>] (<ext-link ext-link-type="uri" xlink:href="https://string-db.org/">https://string-db.org/</ext-link>), which contains more than ten million protein-protein interactions with confidence scores between 0 and 1000. We filtered out most interaction edges with low confidence scores and selected about 1.5 million interaction edges with confidence scores at least 400. We extracted a subnetwork from this PPI interaction network for gene and protein expression features. Since the gene-gene interaction network is too sparse, we performed a one-step random walk (i.e., multiplying the interaction network by itself), removed outliers and normalized it. For miRNA and methylation features, we first map to miRNA/methylation to gene (protein) features and then calculate a miRNA-miRNA and a methylation-methylation interaction network. Take miRNA data as an example. Let <bold>M</bold><sub><italic>m</italic><italic>i</italic><italic>R</italic>−<italic>p</italic><italic>r</italic><italic>o</italic></sub> be the adjacency matrix for the miRNA-protein mapping (this matrix is derived from miRDB (<ext-link ext-link-type="uri" xlink:href="http://www.mirdb.org">http://www.mirdb.org</ext-link>) miRNA target prediction scores), and <bold>M</bold><sub><italic>p</italic><italic>r</italic><italic>o</italic>−<italic>p</italic><italic>r</italic><italic>o</italic></sub> be the protein-protein interaction network, then the miRNA-miRNA interaction network <bold>M</bold><sub><italic>m</italic><italic>i</italic><italic>R</italic>−<italic>m</italic><italic>i</italic><italic>R</italic></sub> is calculated as follows:
<disp-formula id="Equc"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\mathbf{M}_{miR-miR} = \mathbf{M}_{miR-pro} \cdot \mathbf{M}_{pro-pro} \cdot \mathbf{M}_{miR-pro}^{T} $$ \end{document}</tex-math><mml:math id="M68"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">miR</mml:mtext><mml:mo>−</mml:mo><mml:mtext mathvariant="italic">miR</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">miR</mml:mtext><mml:mo>−</mml:mo><mml:mtext mathvariant="italic">pro</mml:mtext></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">pro</mml:mtext><mml:mo>−</mml:mo><mml:mtext mathvariant="italic">pro</mml:mtext></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">miR</mml:mtext><mml:mo>−</mml:mo><mml:mtext mathvariant="italic">pro</mml:mtext></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math><graphic xlink:href="12864_2019_6285_Article_Equc.gif" position="anchor"/></alternatives></disp-formula> All the four feature interaction matrices are normalized to have a Frobenius norm equal to 1.</p>
          <p>We randomly chose 70% of the dataset as the training set, 10% as the validation set, and the rest 20% as the test set. We trained different models on the training set and evaluated them on the validation set. We chose the model with the best validation accuracy to make predictions on the test set and reported the Average Precision and AUC score on the test set.</p>
        </sec>
      </sec>
    </sec>
    <sec id="Sec18">
      <title>Experimental results</title>
      <p>We compare our model with SVM, Decision Tree, Naive Bayes, Random Forest, and AdaBoost, as well as Variational AutoEncoder (VAE) and Adversarial AutoEncoder (AAE). Traditional models such as SVM only accept one feature matrix as input. So we used the concatenated feature matrix as model input. We used a linear kernel for SVM. We used 10 estimators in Random Forest and 50 estimators in AdaBoost.</p>
      <p>For the Multi-view AutoEncoder (MAE) model with a classification head, we used a three-layer neural network. The input layer has 10,546 units (features). Both the first and second hidden layers have 100 hidden units. The last layer also has 10,546 units (i.e., the reconstruction of the input). We added a classification head which is a linear layer with two hidden units corresponding to two classes.</p>
      <p>To facilitate fair comparisons, all of our proposed Multi-view Factorization AutoEncoder (MAE) models share the same model architecture(i.e., two hidden layers each with 100 hidden units for each of the four submodule autoencoders), but the training objectives are different. Since this dataset has four different data types, our model has four autoencoders as submodules, each of which encodes one type of data (one view). Figure <xref rid="Fig1" ref-type="fig">1</xref> shows our model structure (note in our experiments we have four views instead of only two shown in the figure). We combine the outputs of the four autoencoders (i.e., the outputs of the last hidden layers) by adding them together (Eq. <xref rid="Equ12" ref-type="">12</xref>) for classification tasks.</p>
      <p>The training objective for the Multiview Factorization AutoEncoder (MAE without graph constraints) includes only the first two terms in Eq. <xref rid="Equ16" ref-type="">16</xref>. The objective for the Multiview Factorization AutoEncoder with feature interaction network constraints (MAE + feat_int) includes the first three terms in Eq. <xref rid="Equ16" ref-type="">16</xref>. The objective for the Multiview Factorization AutoEncoder with patient view similarity network constraints (MAE + view_sim) includes the first two and the last terms in Eq. <xref rid="Equ16" ref-type="">16</xref>. And the objective for the Multiview Factorization AutoEncoder with both feature interaction and view similarity network (MAE + feat_int + view_sim) constraints includes all four terms in Eq. <xref rid="Equ16" ref-type="">16</xref>.</p>
      <p>As our proposed model with network constraints is end-to-end differentiable, we trained it with Adam [<xref ref-type="bibr" rid="CR32">32</xref>] with weight decay 10<sup>−4</sup>. The initial learning rate is 5×10<sup>−4</sup> for the first 500 iterations and then decreased by a factor of 10 (i.e., 5×10<sup>−5</sup>) for another 500 iterations. Models with the best validation accuracies are used for prediction on the test set.</p>
      <p>The Average Precision and AUC scores for Bladder Urothelial Carcinoma (BLCA) and Brain Lower Grade Glioma (LGG) using these models are shown in Tables <xref rid="Tab1" ref-type="table">1</xref> and <xref rid="Tab2" ref-type="table">2</xref>. Our proposed models (in bold font) achieved better Average Precision and AUC scores for predicting PFI on both datasets. Note that traditional methods such as Decision Tree do not perform as well as deep learning models. This may be due to the superior representation power of deep learning. The more recent Bayesian deep learning approach Variational AutoEncoder (VAE) did not achieve good results, while Adversarial AutoEncoder (AAE) achieved better results than other methods except our proposed method. Currently, the datasets contain a lot of noise (due to the nature of) and the feature interaction networks derived from public knowledgebases are incomplete and noisy, too. If a larger dataset consisting of hundreds of thousands of patients is available, we expect our proposed model with more network constraints to be able to generalize even better.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Results for bladder urothelial carcinoma (BLCA) dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model name</th><th align="left">Average precision</th><th align="left">AUC</th></tr></thead><tbody><tr><td align="left">SVM</td><td align="left">0.587</td><td align="left">0.688</td></tr><tr><td align="left">Decision tree</td><td align="left">0.590</td><td align="left">0.575</td></tr><tr><td align="left">Naive Bayes</td><td align="left">0.456</td><td align="left">0.635</td></tr><tr><td align="left">Random forest</td><td align="left">0.575</td><td align="left">0.670</td></tr><tr><td align="left">AdaBoost</td><td align="left">0.587</td><td align="left">0.662</td></tr><tr><td align="left">Variational AE</td><td align="left">0.528</td><td align="left">0.563</td></tr><tr><td align="left">Adversarial AE</td><td align="left">0.617</td><td align="left">0.693</td></tr><tr><td align="left">Multi-view AE</td><td align="left">0.595</td><td align="left">0.699</td></tr><tr><td align="left">MAE + feat_int</td><td align="left">0.650</td><td align="left">0.719</td></tr><tr><td align="left">MAE + view_sim</td><td align="left">0.652</td><td align="left">0.723</td></tr><tr><td align="left"><bold>MAE + feat_int + view_sim</bold></td><td align="left"><bold>0.664</bold></td><td align="left"><bold>0.740</bold></td></tr></tbody></table></table-wrap>
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Results for brain lower grade glioma (LGG) dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model name</th><th align="left">Average Precision</th><th align="left">AUC</th></tr></thead><tbody><tr><td align="left">SVM</td><td align="left">0.591</td><td align="left">0.713</td></tr><tr><td align="left">Decision tree</td><td align="left">0.518</td><td align="left">0.658</td></tr><tr><td align="left">Naive Bayes</td><td align="left">0.568</td><td align="left">0.742</td></tr><tr><td align="left">Random forest</td><td align="left">0.661</td><td align="left">0.670</td></tr><tr><td align="left">AdaBoost</td><td align="left">0.594</td><td align="left">0.673</td></tr><tr><td align="left">Variational AE</td><td align="left">0.628</td><td align="left">0.642</td></tr><tr><td align="left">Adversarial AE</td><td align="left">0.659</td><td align="left">0.702</td></tr><tr><td align="left">Multi-view AE</td><td align="left">0.551</td><td align="left">0.726</td></tr><tr><td align="left">MAE + feat_int</td><td align="left">0.576</td><td align="left">0.727</td></tr><tr><td align="left">MAE + view_sim</td><td align="left">0.737</td><td align="left">0.819</td></tr><tr><td align="left"><bold>MAE + feat_int + view_sim</bold></td><td align="left"><bold>0.746</bold></td><td align="left"><bold>0.825</bold></td></tr></tbody></table></table-wrap></p>
      <p><bold>Multi-omics outperforms single -omics</bold>We trained our autoencoder models on each type of -omics data (i.e., gene expression, miRNA expression, protein expression, and DNA methylation), and compared them with those trained using multi-omics data (all the four types combined) using our proposed Multi-view Factorization AutoEncoder model. The results on BLCA and LGG datasets are shown in Tables <xref rid="Tab3" ref-type="table">3</xref> and <xref rid="Tab4" ref-type="table">4</xref>, respectively. For both datasets, the results using multi-omics data (all four data types combined) significantly outperform those using a single type of -omics data.
<table-wrap id="Tab3"><label>Table 3</label><caption><p>Results using single -omics versus multi-omics on BLCA dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model name</th><th align="left">Average precision</th><th align="left">AUC</th></tr></thead><tbody><tr><td align="left">Gene (single -omics)</td><td align="left">0.532</td><td align="left">0.688</td></tr><tr><td align="left">miRNA (single -omics)</td><td align="left">0.368</td><td align="left">0.507</td></tr><tr><td align="left">Protein (single -omics)</td><td align="left">0.399</td><td align="left">0.567</td></tr><tr><td align="left">DNA Methylation (single -omics)</td><td align="left">0.601</td><td align="left">0.634</td></tr><tr><td align="left"><bold>Combined multi-omics</bold></td><td align="left"><bold>0.664</bold></td><td align="left"><bold>0.740</bold></td></tr></tbody></table></table-wrap>
<table-wrap id="Tab4"><label>Table 4</label><caption><p>Results using single -omics versus multi-omics on LGG dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model name</th><th align="left">Average precision</th><th align="left">AUC</th></tr></thead><tbody><tr><td align="left">Gene (single -omics)</td><td align="left">0.634</td><td align="left">0.728</td></tr><tr><td align="left">miRNA (single -omics)</td><td align="left">0.501</td><td align="left">0.686</td></tr><tr><td align="left">Protein (single -omics)</td><td align="left">0.575</td><td align="left">0.735</td></tr><tr><td align="left">DNA Methylation (single -omics)</td><td align="left">0.610</td><td align="left">0.698</td></tr><tr><td align="left"><bold>Combined multi-omics</bold></td><td align="left"><bold>0.746</bold></td><td align="left"><bold>0.825</bold></td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec20">
      <title>Results on the tCGA pan-cancer dataset</title>
      <p>We also performed experiments on the TCGA Pan-cancer dataset [<xref ref-type="bibr" rid="CR1">1</xref>] consisting of 6179 patients with 21 different cancer types. In addition to predicting Progression-Free Interval (PFI) event, we also predict Overall Survival (OS) event. Similar to PFI, OS is another derived clinical (binary) outcome endpoints [<xref ref-type="bibr" rid="CR31">31</xref>]. OS=1 means for patients who were dead from any cause based on the follow-up data; OS=0 for otherwise. There are 4460 patients with OS=0 and 1719 patients with OS=1. PFI and OS are the same for most cases (4941 out of 6179, or 80%). There are 4268 patients with PFI=0 and 1911 patients with PFI=1. PFI is preferred over OS given the relatively short follow-up time. Therefore, we mainly use PFI as a binary target.</p>
      <p>We used the same data processing procedure and experimental settings as described above for the BLCA and LGG datasets. The average AUC scores (10 runs) for predicting PFI and OS using these models are shown in Table <xref rid="Tab5" ref-type="table">5</xref>. Our proposed models (in bold font) achieved better AUC scores for both predicting PFI and OS than other traditional machine learning methods.
<table-wrap id="Tab5"><label>Table 5</label><caption><p>AUC scores for predicting PFI and OS on the TCGA pan-cancer dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model name</th><th align="left">AUC (OS)</th><th align="left">AUC (PFI)</th></tr></thead><tbody><tr><td align="left">SVM</td><td align="left">0.699</td><td align="left">0.625</td></tr><tr><td align="left">Decision Tree</td><td align="left">0.670</td><td align="left">0.634</td></tr><tr><td align="left">Naive Bayes</td><td align="left">0.655</td><td align="left">0.644</td></tr><tr><td align="left">kNN</td><td align="left">0.706</td><td align="left">0.659</td></tr><tr><td align="left">Random Forest</td><td align="left">0.720</td><td align="left">0.661</td></tr><tr><td align="left">AdaBoost</td><td align="left">0.716</td><td align="left">0.689</td></tr><tr><td align="left">MAE + feat_int</td><td align="left">0.765</td><td align="left">0.721</td></tr><tr><td align="left"><bold>MAE + view_sim</bold></td><td align="left">0.763</td><td align="left"><bold>0.724</bold></td></tr><tr><td align="left"><bold>MAE + feat_int + view_sim</bold></td><td align="left">0.766</td><td align="left"><bold>0.724</bold></td></tr></tbody></table></table-wrap></p>
      <p>In order to study if the model architecture would significantly affect the results, we change the number of hidden layers from one layer to three layers. The number of units for each hidden layer is shown in Table <xref rid="Tab6" ref-type="table">6</xref>. (Note we have omitted the input and output layers both of which have 10,546 hidden units). As shown in Table <xref rid="Tab6" ref-type="table">6</xref>, the results are not significantly different. In addition, we had tried to use DenseNet [<xref ref-type="bibr" rid="CR30">30</xref>] and ResNet [<xref ref-type="bibr" rid="CR29">29</xref>] as the backbone of the autoencoders instead of multi-layer perceptrons. The results are also not significantly different and thus not presented here.
<table-wrap id="Tab6"><label>Table 6</label><caption><p>AUC scores for PFI with different model architectures</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Number of hidden units</th><th align="left">AUC (PFI)</th></tr></thead><tbody><tr><td align="left">100</td><td align="left">0.723</td></tr><tr><td align="left">200</td><td align="left">0.725</td></tr><tr><td align="left">200-100</td><td align="left">0.726</td></tr><tr><td align="left">100-200</td><td align="left">0.727</td></tr><tr><td align="left">100-100</td><td align="left">0.724</td></tr><tr><td align="left">100-100-100</td><td align="left">0.725</td></tr><tr><td align="left">50-100-200</td><td align="left">0.726</td></tr><tr><td align="left">100-50-200</td><td align="left">0.726</td></tr><tr><td align="left">200-100-50</td><td align="left">0.722</td></tr></tbody></table></table-wrap></p>
      <sec id="Sec21">
        <title>Learned feature embeddings preserve interaction network structure</title>
        <p>Our proposed model learns patient representations and feature embeddings simultaneously. While patients are different from datasets to datasets, the genomic features (such as gene features) and their interaction networks are from domain knowledge, and thus are persistent regardless of which dataset we are using. Since we have a regularization term in the loss to ensure the learned feature embeddings are consistent with feature interaction networks, we would like to know if the model is able to learn an embedding that is “compatible” with the domain knowledge of interaction networks. We plotted the loss term <inline-formula id="IEq16"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\sum _{v=1}^{V} Trace\left (\mathbf {Y}^{(v)} \cdot \mathbf {L}_{G^{(v)}} \cdot \mathbf {Y}^{(v)^{T}}\right)$\end{document}</tex-math><mml:math id="M70"><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:munderover><mml:mtext mathvariant="italic">Trace</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12864_2019_6285_Article_IEq16.gif"/></alternatives></inline-formula> from one typical run of training our model with feature interaction network constraints in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. This regularization term decreased to nearly zero very fast, which means the information from feature interaction networks is fully assimilated into the model, or more specifically, the weights of the decoders in the model. We found that many independent runs show very similar loss curves, which means the model is able to robustly learn a feature embedding that preserves the feature interaction network information.
<fig id="Fig2"><label>Fig. 2</label><caption><p>A typical feature interaction network regularizer training loss curve. After about 100 iterations, the training loss (corresponding to the third term in Eq. <xref rid="Equ16" ref-type="">16</xref>) approaches almost zero, which means the learned molecular feature embeddings become consistent with the provided feature interaction networks</p></caption><graphic xlink:href="12864_2019_6285_Fig2_HTML" id="MO2"/></fig></p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec22" sec-type="conclusion">
    <title>Conclusion</title>
    <p>While it is challenging for applying machine learning to multi-omics data with the “big p, small n” problem, biological domain knowledge can be incorporated into the machine learning model as inductive biases to alleviate potential overfitting problems. A number of knowledgebases (e.g., STRING [<xref ref-type="bibr" rid="CR26">26</xref>], Reactome Pathways [<xref ref-type="bibr" rid="CR27">27</xref>], etc.) contain the information for extracting biological interaction networks, which can be incorporated into various machine learning models. In this paper, we presented the Multi-view Factorization AutoEncoder (MAE) model with network constraints that can effectively integrate domain knowledge such as molecular interaction networks with multi-omics data for accurately predicting clinical outcomes.</p>
    <p>The MAE model consists of multiple factorization autoencoders as submodules for individual data types (views) and combines multiple views with their high-level abstract representations for supervised learning. The factorization autoencoder employs a deep architecture for the encoder and a shallow architecture for the decoder. This increases the overall model representation power and provides a natural way to integrate graph constraints into the model. Our model learns molecular and patient embeddings simultaneously. With effective network regularization techniques, we can learn good feature representations and consistent patient similarity networks and feature interaction networks.</p>
    <p>The experimental results on the Bladder Urothelial Carcinoma (BLCA) and Brain Lower Grade Glioma (LGG) datasets and the TCGA pan-cancer dataset demonstrated that our proposed model with feature interaction network and patient similarity network constraints outperforms traditional methods and conventional deep learning models on predicting clinical target variables from multi-omics data. Our method can be applied to other large-scale multi-omics datasets to learn latent representations that are consistent with molecular interaction networks for various molecular entities. Besides multi-omics data, our proposed method can also be applied to any other multi-view data with feature interaction networks.</p>
    <p>The ultimate goal of multi-omics data integrative analysis is to disentangle complex factors and identify important factors that contribute to disease etiology. Our model learns distributed representations for various molecular entities and facilitates mining relationships among molecular features and clinical features. Essentially, learning good representations for both molecular and clinical features is fundamentally important to unravel the intricate relationships among them. Our work also provides a proof-of-concept framework for unifying data-driven and knowledge-driven approaches for mining multi-omics data with biological knowledge. We hope it can be applied to large-scale cancer genomics data and contribute to elucidating the etiology and mechanisms of cancer and other complex genetic diseases.</p>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>AAE</term>
        <def>
          <p>Adversarial AutoEncoder</p>
        </def>
      </def-item>
      <def-item>
        <term>ANF</term>
        <def>
          <p>Affinity network fusion</p>
        </def>
      </def-item>
      <def-item>
        <term>AUC</term>
        <def>
          <p>Area under the ROC curve</p>
        </def>
      </def-item>
      <def-item>
        <term>BLCA</term>
        <def>
          <p>Bladder urothelial carcinoma</p>
        </def>
      </def-item>
      <def-item>
        <term>LGG</term>
        <def>
          <p>Brain lower grade glioma</p>
        </def>
      </def-item>
      <def-item>
        <term>MAE</term>
        <def>
          <p>Multi-view factorization AutoEncoder</p>
        </def>
      </def-item>
      <def-item>
        <term>OS</term>
        <def>
          <p>Overall survival</p>
        </def>
      </def-item>
      <def-item>
        <term>PFI</term>
        <def>
          <p>Progression-free interval</p>
        </def>
      </def-item>
      <def-item>
        <term>SNF</term>
        <def>
          <p>Similarity network fusion</p>
        </def>
      </def-item>
      <def-item>
        <term>TCGA</term>
        <def>
          <p>The cancer genome atlas</p>
        </def>
      </def-item>
      <def-item>
        <term>VAE</term>
        <def>
          <p>Variational AutoEncoder</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>We thank The Cancer Genome Atlas (TCGA) network for making the high-quality multi-omics data freely available to the public.</p>
    <sec id="d29e4757">
      <title>About this supplement</title>
      <p>This article has been published as part of <italic>BMC Genomics Volume 20 Supplement 11, 2019: Selected articles from the IEEE BIBM International Conference on Bioinformatics &amp; Biomedicine (BIBM) 2018: genomics</italic>. The full contents of the supplement are available online at <ext-link ext-link-type="uri" xlink:href="https://bmcgenomics.biomedcentral.com/articles/supplements/volume-20-supplement-11">https://bmcgenomics.biomedcentral.com/articles/supplements/volume-20-supplement-11</ext-link>.</p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>TM and AZ conceived the idea, designed the study, and wrote the manuscript. TM implemented the method and performed the experiments. AZ supervised the study. All authors read and approved of the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>Publication costs have been funded by the US National Science Foundation under grant NSF IIS-1514204. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>All the data used in the experiments are publicly available at the GDC Data Portal website (<ext-link ext-link-type="uri" xlink:href="https://portal.gdc.cancer.gov/">https://portal.gdc.cancer.gov/</ext-link>).</p>
  </notes>
  <notes>
    <title>Ethics approval and consent to participate</title>
    <p>Not applicable.</p>
  </notes>
  <notes>
    <title>Consent for publication</title>
    <p>Not applicable.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Competing interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hutter</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Zenklusen</surname>
            <given-names>JC</given-names>
          </name>
        </person-group>
        <article-title>The cancer genome atlas: Creating lasting value beyond its data</article-title>
        <source>Cell</source>
        <year>2018</year>
        <volume>173</volume>
        <issue>2</issue>
        <fpage>283</fpage>
        <lpage>5</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2018.03.042</pub-id>
        <pub-id pub-id-type="pmid">29625045</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>LeCun</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Deep learning</article-title>
        <source>nature</source>
        <year>2015</year>
        <volume>521</volume>
        <issue>7553</issue>
        <fpage>436</fpage>
        <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>
        <pub-id pub-id-type="pmid">26017442</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3</label>
      <mixed-citation publication-type="other">Ma T, Zhang A. Multi-view factorization autoencoder with network constraints for multi-omic integrative analysis. In: IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2018, Madrid, Spain, December 3-6, 2018: 2018. p. 702–7. <pub-id pub-id-type="doi">10.1109/BIBM.2018.8621379</pub-id>. <ext-link ext-link-type="uri" xlink:href="http://doi.ieeecomputersociety.org/10.1109/BIBM.2018.8621379">http://doi.ieeecomputersociety.org/10.1109/BIBM.2018.8621379</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR4">
      <label>4</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Multi-view learning overview: Recent progress and new challenges</article-title>
        <source>Inf Fusion</source>
        <year>2017</year>
        <volume>38</volume>
        <fpage>43</fpage>
        <lpage>54</lpage>
        <pub-id pub-id-type="doi">10.1016/j.inffus.2017.02.007</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bell</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Koren</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Volinsky</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Matrix factorization techniques for recommender systems</article-title>
        <source>Computer</source>
        <year>2009</year>
        <volume>42</volume>
        <fpage>30</fpage>
        <lpage>37</lpage>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shen</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Shih</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hollern</surname>
            <given-names>DP</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Bowlby</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Tickoo</surname>
            <given-names>SK</given-names>
          </name>
          <name>
            <surname>Thorsson</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Mungall</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Newton</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hegde</surname>
            <given-names>AM</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Integrated molecular characterization of testicular germ cell tumors</article-title>
        <source>Cell Rep</source>
        <year>2018</year>
        <volume>23</volume>
        <issue>11</issue>
        <fpage>3392</fpage>
        <lpage>406</lpage>
        <pub-id pub-id-type="doi">10.1016/j.celrep.2018.05.039</pub-id>
        <pub-id pub-id-type="pmid">29898407</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Malta</surname>
            <given-names>TM</given-names>
          </name>
          <name>
            <surname>Sokolov</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gentles</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Burzykowski</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Poisson</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Weinstein</surname>
            <given-names>JN</given-names>
          </name>
          <name>
            <surname>Kamińska</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Huelsken</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Omberg</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Gevaert</surname>
            <given-names>O</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Machine learning identifies stemness features associated with oncogenic dedifferentiation</article-title>
        <source>Cell</source>
        <year>2018</year>
        <volume>173</volume>
        <issue>2</issue>
        <fpage>338</fpage>
        <lpage>54</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2018.03.034</pub-id>
        <pub-id pub-id-type="pmid">29625051</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Way</surname>
            <given-names>GP</given-names>
          </name>
          <name>
            <surname>Sanchez-Vega</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>La</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Armenia</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Chatila</surname>
            <given-names>WK</given-names>
          </name>
          <name>
            <surname>Luna</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Cherniack</surname>
            <given-names>AD</given-names>
          </name>
          <name>
            <surname>Mina</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ciriello</surname>
            <given-names>G</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Machine learning detects pan-cancer ras pathway activation in the cancer genome atlas</article-title>
        <source>Cell Rep</source>
        <year>2018</year>
        <volume>23</volume>
        <issue>1</issue>
        <fpage>172</fpage>
        <lpage>80</lpage>
        <pub-id pub-id-type="doi">10.1016/j.celrep.2018.03.046</pub-id>
        <pub-id pub-id-type="pmid">29617658</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Angione</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Conway</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lió</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Multiplex methods provide effective integration of multi-omic data in genome-scale models</article-title>
        <source>BMC Bioinformatics</source>
        <year>2016</year>
        <volume>17</volume>
        <issue>4</issue>
        <fpage>83</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-016-0912-1</pub-id>
        <pub-id pub-id-type="pmid">26961692</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ebrahim</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Brunk</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Tan</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>O’brien</surname>
            <given-names>EJ</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Szubin</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lerman</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Lechner</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sastry</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bordbar</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Multi-omic data integration enables discovery of hidden biological regularities</article-title>
        <source>Nat Commun</source>
        <year>2016</year>
        <volume>7</volume>
        <fpage>13091</fpage>
        <pub-id pub-id-type="doi">10.1038/ncomms13091</pub-id>
        <pub-id pub-id-type="pmid">27782110</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Henry</surname>
            <given-names>V. J.</given-names>
          </name>
          <name>
            <surname>Bandrowski</surname>
            <given-names>A. E.</given-names>
          </name>
          <name>
            <surname>Pepin</surname>
            <given-names>A.-S.</given-names>
          </name>
          <name>
            <surname>Gonzalez</surname>
            <given-names>B. J.</given-names>
          </name>
          <name>
            <surname>Desfeux</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>OMICtools: an informative directory for multi-omic data analysis</article-title>
        <source>Database</source>
        <year>2014</year>
        <volume>2014</volume>
        <issue>0</issue>
        <fpage>bau069</fpage>
        <lpage>bau069</lpage>
        <pub-id pub-id-type="doi">10.1093/database/bau069</pub-id>
        <pub-id pub-id-type="pmid">25024350</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shen</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Mo</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Schultz</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Seshan</surname>
            <given-names>VE</given-names>
          </name>
          <name>
            <surname>Olshen</surname>
            <given-names>AB</given-names>
          </name>
          <name>
            <surname>Huse</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ladanyi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Integrative subtype discovery in glioblastoma using icluster</article-title>
        <source>PLoS ONE</source>
        <year>2012</year>
        <volume>7</volume>
        <issue>4</issue>
        <fpage>35236</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0035236</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Mezlini</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Demir</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Fiume</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Tu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Brudno</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Haibe-Kains</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Goldenberg</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Similarity network fusion for aggregating data types on a genomic scale</article-title>
        <source>Nat Methods</source>
        <year>2014</year>
        <volume>11</volume>
        <issue>3</issue>
        <fpage>333</fpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.2810</pub-id>
        <pub-id pub-id-type="pmid">24464287</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14</label>
      <mixed-citation publication-type="other">Ma T, Zhang A. Integrate multi-omic data using affinity network fusion (anf) for cancer patient clustering. In: Bioinformatics and Biomedicine (BIBM), 2017 IEEE International Conference On. IEEE: 2017. p. 398–403. <pub-id pub-id-type="doi">10.1109/bibm.2017.8217682</pub-id>.</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hofree</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>JP</given-names>
          </name>
          <name>
            <surname>Carter</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Gross</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ideker</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Network-based stratification of tumor mutations</article-title>
        <source>Nat Methods</source>
        <year>2013</year>
        <volume>10</volume>
        <issue>11</issue>
        <fpage>1108</fpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.2651</pub-id>
        <pub-id pub-id-type="pmid">24037242</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Alipanahi</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Delong</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Weirauch</surname>
            <given-names>MT</given-names>
          </name>
          <name>
            <surname>Frey</surname>
            <given-names>BJ</given-names>
          </name>
        </person-group>
        <article-title>Predicting the sequence specificities of dna-and rna-binding proteins by deep learning</article-title>
        <source>Nat Biotechnol</source>
        <year>2015</year>
        <volume>33</volume>
        <issue>8</issue>
        <fpage>831</fpage>
        <pub-id pub-id-type="doi">10.1038/nbt.3300</pub-id>
        <pub-id pub-id-type="pmid">26213851</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Boža</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Brejová</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Vinař</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Deepnano: deep recurrent neural networks for base calling in minion nanopore reads</article-title>
        <source>PLoS ONE</source>
        <year>2017</year>
        <volume>12</volume>
        <issue>6</issue>
        <fpage>0178751</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0178751</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18</label>
      <mixed-citation publication-type="other">Wang D, Khosla A, Gargeya R, Irshad H, Beck AH. Deep learning for identifying metastatic breast cancer. 2016. arXiv preprint arXiv:1606.05718.</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Pham</surname>
            <given-names>Trang</given-names>
          </name>
          <name>
            <surname>Tran</surname>
            <given-names>Truyen</given-names>
          </name>
          <name>
            <surname>Phung</surname>
            <given-names>Dinh</given-names>
          </name>
          <name>
            <surname>Venkatesh</surname>
            <given-names>Svetha</given-names>
          </name>
        </person-group>
        <article-title>DeepCare: A Deep Dynamic Memory Model for Predictive Medicine</article-title>
        <source>Advances in Knowledge Discovery and Data Mining</source>
        <year>2016</year>
        <publisher-loc>Cham</publisher-loc>
        <publisher-name>Springer International Publishing</publisher-name>
        <fpage>30</fpage>
        <lpage>41</lpage>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20</label>
      <mixed-citation publication-type="other">Hu Z, Yang Z, Salakhutdinov R, Liang X, Qin L, Dong H, Xing E. Deep generative models with learnable knowledge constraints. 2018. arXiv preprint arXiv:1806.09764.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ma</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>MK</given-names>
          </name>
          <name>
            <surname>Fong</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ono</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Sage</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Demchak</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Sharan</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Ideker</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Using deep learning to model the hierarchical structure and function of a cell</article-title>
        <source>Nat Methods</source>
        <year>2018</year>
        <volume>15</volume>
        <issue>4</issue>
        <fpage>290</fpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.4627</pub-id>
        <pub-id pub-id-type="pmid">29505029</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Baltrusaitis</surname>
            <given-names>Tadas</given-names>
          </name>
          <name>
            <surname>Ahuja</surname>
            <given-names>Chaitanya</given-names>
          </name>
          <name>
            <surname>Morency</surname>
            <given-names>Louis-Philippe</given-names>
          </name>
        </person-group>
        <article-title>Multimodal Machine Learning: A Survey and Taxonomy</article-title>
        <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>
        <year>2019</year>
        <volume>41</volume>
        <issue>2</issue>
        <fpage>423</fpage>
        <lpage>443</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2018.2798607</pub-id>
        <pub-id pub-id-type="pmid">29994351</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Ngiam</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Khosla</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Nam</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Ng</surname>
            <given-names>AY</given-names>
          </name>
        </person-group>
        <article-title>Multimodal deep learning</article-title>
        <source>Proceedings of the 28th International Conference on Machine Learning (ICML-11)</source>
        <year>2011</year>
        <publisher-loc>Madison</publisher-loc>
        <publisher-name>Omnipress</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24</label>
      <mixed-citation publication-type="other">Wang W, Arora R, Livescu K, Bilmes J. On deep multi-view representation learning. In: Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37. ICML. JMLR.org: 2015. p. 1083–92. <ext-link ext-link-type="uri" xlink:href="http://dl.acm.org/citation.cfm?id=3045118.3045234">http://dl.acm.org/citation.cfm?id=3045118.3045234</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Yingming</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Ming</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Zhongfei</given-names>
          </name>
        </person-group>
        <article-title>A Survey of Multi-View Representation Learning</article-title>
        <source>IEEE Transactions on Knowledge and Data Engineering</source>
        <year>2019</year>
        <volume>31</volume>
        <issue>10</issue>
        <fpage>1863</fpage>
        <lpage>1883</lpage>
        <pub-id pub-id-type="doi">10.1109/TKDE.2018.2872063</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Szklarczyk</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Franceschini</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Wyder</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Forslund</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Heller</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Huerta-Cepas</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Simonovic</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Roth</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Santos</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Tsafou</surname>
            <given-names>KP</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>String v10: protein–protein interaction networks, integrated over the tree of life</article-title>
        <source>Nucleic Acids Res</source>
        <year>2014</year>
        <volume>43</volume>
        <issue>D1</issue>
        <fpage>447</fpage>
        <lpage>52</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gku1003</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Croft</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Mundo</surname>
            <given-names>AF</given-names>
          </name>
          <name>
            <surname>Haw</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Milacic</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Weiser</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Caudy</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Garapati</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Gillespie</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kamdar</surname>
            <given-names>MR</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The reactome pathway knowledgebase</article-title>
        <source>Nucleic Acids Res</source>
        <year>2013</year>
        <volume>42</volume>
        <issue>D1</issue>
        <fpage>472</fpage>
        <lpage>7</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkt1102</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>DD</given-names>
          </name>
          <name>
            <surname>Seung</surname>
            <given-names>HS</given-names>
          </name>
        </person-group>
        <article-title>Algorithms for non-negative matrix factorization</article-title>
        <source>Advances in Neural Information Processing Systems</source>
        <year>2001</year>
        <publisher-loc>Cambridge</publisher-loc>
        <publisher-name>MIT press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29</label>
      <mixed-citation publication-type="other">He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition: 2016. p. 770–8. <pub-id pub-id-type="doi">10.1109/cvpr.2016.90</pub-id>.</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30</label>
      <mixed-citation publication-type="other">Huang G, Liu Z, v. d. Maaten L., Weinberger KQ. Densely connected convolutional networks. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR): 2017. p. 2261–9. <pub-id pub-id-type="doi">10.1109/CVPR.2017.243</pub-id>.</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lichtenberg</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Hoadley</surname>
            <given-names>KA</given-names>
          </name>
          <name>
            <surname>Poisson</surname>
            <given-names>LM</given-names>
          </name>
          <name>
            <surname>Lazar</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Cherniack</surname>
            <given-names>AD</given-names>
          </name>
          <name>
            <surname>Kovatich</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Benz</surname>
            <given-names>CC</given-names>
          </name>
          <name>
            <surname>Levine</surname>
            <given-names>DA</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>AV</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>An integrated tcga pan-cancer clinical data resource to drive high-quality survival outcome analytics</article-title>
        <source>Cell</source>
        <year>2018</year>
        <volume>173</volume>
        <issue>2</issue>
        <fpage>400</fpage>
        <lpage>16</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2018.02.052</pub-id>
        <pub-id pub-id-type="pmid">29625055</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32</label>
      <mixed-citation publication-type="other">Kingma DP, Ba J. Adam: A method for stochastic optimization. 2014. arXiv preprint arXiv:1412.6980.</mixed-citation>
    </ref>
  </ref-list>
</back>
